<!doctype html><html><head><title>arXiv @ 2024.03.09</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/bootstrap.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/main.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/navbar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/plyr.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/flag-icon.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/custom/style.css><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM crossorigin=anonymous><script async src=https://cdn.jsdelivr.net/npm/es-module-shims@1/dist/es-module-shims.min.js crossorigin=anonymous></script><script type=importmap>
{
  "imports": {
    "@popperjs/core": "https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.8/dist/esm/popper.min.js",
    "bootstrap": "https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.esm.min.js"
  }
}
</script><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js integrity=sha384-geWF76RCwLtnZ8qwWowPQNguL3RmwHVBC9FhGdlKrxdiJJigb/j/68SIy3Te4Bkz crossorigin=anonymous></script><script type=module src=https://akitenkrad.github.io/akitenkrad-blog/js/custom/custom.js></script><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/fontawesome/css/all.min.css><link rel=icon type=image/png href=https://akitenkrad.github.io/akitenkrad-blog/images/favicons/favicon-96x96_huf1ee13f0caf27d1547f91fb46207d708_13005_42x0_resize_box_3.png><meta property="og:title" content="arXiv @ 2024.03.09"><meta property="og:description" content="Primary Categories astro-ph.CO (1) cs.AI (29) cs.AR (2) cs.CC (1) cs.CE (2) cs.CG (1) cs.CL (39) cs.CR (6) cs.CV (58) cs.CY (2) cs.DB (3) cs.DC (5) cs.DL (1) cs.DS (6) cs.GT (4) cs.HC (3) cs.IR (11) cs.IT (5) cs.LG (46) cs.MA (2) cs.MS (1) cs.NI (3) cs.OH (1) cs.PL (2) cs.RO (18) cs.SD (2) cs.SE (3) cs.SI (2) eess.IV (5) eess.SP (1) eess.SY (6) math.NA (8) math.OC (2) physics.bio-ph (1) physics."><meta property="og:type" content="article"><meta property="og:url" content="https://akitenkrad.github.io/akitenkrad-blog/posts/arxiv/202403/20240309000000/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-03-09T00:00:00+00:00"><meta property="article:modified_time" content="2024-03-09T00:00:00+00:00"><meta name=description content="arXiv @ 2024.03.09"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/single.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/sidebar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/style.css><script type=text/javascript src=//ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js></script><script language=JavaScript>$(document).ready(function(){$("a[href^='http']:not([href*='"+location.hostname+"'])").attr("target","_blank")})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-1MYYZQG0WE"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-1MYYZQG0WE",{anonymize_ip:!1})}</script></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow" style=position:fixed><div class=container><button class="navbar-toggler navbar-light navbar-for-sidebar" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span>
</button>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav mr-auto"><li class=nav-item><a class=navbar-brand href=/akitenkrad-blog><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png alt=Logo>
Akitenkrad's Blog</a></li></ul><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=/akitenkrad-blog#home>Home</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#about>About</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#recent-posts>Recent Posts</a></li><li class=nav-item><a class=nav-link id=blog-link href=https://akitenkrad.github.io/akitenkrad-blog/posts>Posts</a></li><li class=nav-item><a class=nav-link id=tag-link href=https://akitenkrad.github.io/akitenkrad-blog/tags>Tags</a></li></ul></div></div><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/akitenkrad-blog/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/>Papers</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202205/>2022.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202205/20220518224923/ title="A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks">A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220508162318/ title="A Deep Cascade Model for Multi-Document Reading Comprehension">A Deep Cascade Model for Multi-Document Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220506021208/ title="A Primer in BERTology: What We Know About How BERT Works">A Primer in BERTology: What We Know About How BERT Works</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220529131339/ title="Attention Is All You Need">Attention Is All You Need</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220503010000/ title="DeBERTa: Decoding-Enhanced BERT with Disentangled Attention">DeBERTa: Decoding-Enhanced BERT with Disentangled Attention</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220505222900/ title="Dense Passage Retrieval for Open-Domain Question Answering">Dense Passage Retrieval for Open-Domain Question Answering</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220511010217/ title="Multi-Style Generative Reading Comprehension">Multi-Style Generative Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220530102936/ title="Neural Machine Translation of Rare Words with Subword Units">Neural Machine Translation of Rare Words with Subword Units</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220523223206/ title="RoBERTa: A Robustly Optimized BERT Pretraining Approach">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220520124748/ title="Semi-Supervised Classification with Graph Convolutional Networks">Semi-Supervised Classification with Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220509110738/ title="Survey on graph embeddings and their applications to machine learning problems on graphs">Survey on graph embeddings and their applications to machine learning problems on graphs</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220514151839/ title="UnitedQA: A Hybrid Approach for Open Domain Question Answering">UnitedQA: A Hybrid Approach for Open Domain Question Answering</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202206/>2022.06</a><ul><li><a href=/akitenkrad-blog/posts/papers/202206/20220612105422/ title="Attributed Network Embedding for Learning in a Dynamic Environment">Attributed Network Embedding for Learning in a Dynamic Environment</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220608085622/ title="CodeBERT: A Pre-Trained Model for Programming and Natural Languages">CodeBERT: A Pre-Trained Model for Programming and Natural Languages</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220618223844/ title="High-order Proximity Preserved Embedding for Dynamic Networks">High-order Proximity Preserved Embedding for Dynamic Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220602171700/ title="S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension">S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202207/>2022.07</a><ul><li><a href=/akitenkrad-blog/posts/papers/202207/20220727145036/ title="Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions">Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions</a></li><li><a href=/akitenkrad-blog/posts/papers/202207/20220726163444/ title="Dynamic Network Embedding Survey">Dynamic Network Embedding Survey</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/papers/202208/20220802103319/ title="Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN">Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/papers/202209/20220909180827/ title="Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting">Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202305/>2023.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202305/20230513094804/ title="Improving Language Understanding by Generative Pre-Training">Improving Language Understanding by Generative Pre-Training</a></li><li><a href=/akitenkrad-blog/posts/papers/202305/20230514200241/ title="Language Models are Unsupervised Multitask Learners">Language Models are Unsupervised Multitask Learners</a></li></ul></li><li><a href=/akitenkrad-blog/posts/papers/202306/ title=2023.06>2023.06</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/>arXiv</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202307/>2023.07</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230701000000/ title="arXiv @ 2023.07.01">arXiv @ 2023.07.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230702000000/ title="arXiv @ 2023.07.02">arXiv @ 2023.07.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230703000000/ title="arXiv @ 2023.07.03">arXiv @ 2023.07.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230704000000/ title="arXiv @ 2023.07.04">arXiv @ 2023.07.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230705000000/ title="arXiv @ 2023.07.05">arXiv @ 2023.07.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230706000000/ title="arXiv @ 2023.07.06">arXiv @ 2023.07.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230707000000/ title="arXiv @ 2023.07.07">arXiv @ 2023.07.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230708000000/ title="arXiv @ 2023.07.08">arXiv @ 2023.07.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230709000000/ title="arXiv @ 2023.07.09">arXiv @ 2023.07.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230710000000/ title="arXiv @ 2023.07.10">arXiv @ 2023.07.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230711000000/ title="arXiv @ 2023.07.11">arXiv @ 2023.07.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230712000000/ title="arXiv @ 2023.07.12">arXiv @ 2023.07.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230713000000/ title="arXiv @ 2023.07.13">arXiv @ 2023.07.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230714000000/ title="arXiv @ 2023.07.14">arXiv @ 2023.07.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230715000000/ title="arXiv @ 2023.07.15">arXiv @ 2023.07.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230716000000/ title="arXiv @ 2023.07.16">arXiv @ 2023.07.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230717000000/ title="arXiv @ 2023.07.17">arXiv @ 2023.07.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230718000000/ title="arXiv @ 2023.07.18">arXiv @ 2023.07.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230719000000/ title="arXiv @ 2023.07.19">arXiv @ 2023.07.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230720000000/ title="arXiv @ 2023.07.20">arXiv @ 2023.07.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230721000000/ title="arXiv @ 2023.07.21">arXiv @ 2023.07.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230722000000/ title="arXiv @ 2023.07.22">arXiv @ 2023.07.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230723000000/ title="arXiv @ 2023.07.23">arXiv @ 2023.07.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230724000000/ title="arXiv @ 2023.07.24">arXiv @ 2023.07.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230725000000/ title="arXiv @ 2023.07.25">arXiv @ 2023.07.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230726000000/ title="arXiv @ 2023.07.26">arXiv @ 2023.07.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230727000000/ title="arXiv @ 2023.07.27">arXiv @ 2023.07.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230728000000/ title="arXiv @ 2023.07.28">arXiv @ 2023.07.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230729000000/ title="arXiv @ 2023.07.29">arXiv @ 2023.07.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230730000000/ title="arXiv @ 2023.07.30">arXiv @ 2023.07.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230731000000/ title="arXiv @ 2023.07.31">arXiv @ 2023.07.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202308/>2023.08</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230801000000/ title="arXiv @ 2023.08.01">arXiv @ 2023.08.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230802000000/ title="arXiv @ 2023.08.02">arXiv @ 2023.08.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230803000000/ title="arXiv @ 2023.08.03">arXiv @ 2023.08.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230804000000/ title="arXiv @ 2023.08.04">arXiv @ 2023.08.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230805000000/ title="arXiv @ 2023.08.05">arXiv @ 2023.08.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230806000000/ title="arXiv @ 2023.08.06">arXiv @ 2023.08.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230807000000/ title="arXiv @ 2023.08.07">arXiv @ 2023.08.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230808000000/ title="arXiv @ 2023.08.08">arXiv @ 2023.08.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230809000000/ title="arXiv @ 2023.08.09">arXiv @ 2023.08.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230810000000/ title="arXiv @ 2023.08.10">arXiv @ 2023.08.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230811000000/ title="arXiv @ 2023.08.11">arXiv @ 2023.08.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230812000000/ title="arXiv @ 2023.08.12">arXiv @ 2023.08.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230813000000/ title="arXiv @ 2023.08.13">arXiv @ 2023.08.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230814000000/ title="arXiv @ 2023.08.14">arXiv @ 2023.08.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230815000000/ title="arXiv @ 2023.08.15">arXiv @ 2023.08.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230816000000/ title="arXiv @ 2023.08.16">arXiv @ 2023.08.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230817000000/ title="arXiv @ 2023.08.17">arXiv @ 2023.08.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230818000000/ title="arXiv @ 2023.08.18">arXiv @ 2023.08.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230819000000/ title="arXiv @ 2023.08.19">arXiv @ 2023.08.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230820000000/ title="arXiv @ 2023.08.20">arXiv @ 2023.08.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230821000000/ title="arXiv @ 2023.08.21">arXiv @ 2023.08.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230822000000/ title="arXiv @ 2023.08.22">arXiv @ 2023.08.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230823000000/ title="arXiv @ 2023.08.23">arXiv @ 2023.08.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230824000000/ title="arXiv @ 2023.08.24">arXiv @ 2023.08.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230825000000/ title="arXiv @ 2023.08.25">arXiv @ 2023.08.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230826000000/ title="arXiv @ 2023.08.26">arXiv @ 2023.08.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230827000000/ title="arXiv @ 2023.08.27">arXiv @ 2023.08.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230828000000/ title="arXiv @ 2023.08.28">arXiv @ 2023.08.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230829000000/ title="arXiv @ 2023.08.29">arXiv @ 2023.08.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230830000000/ title="arXiv @ 2023.08.30">arXiv @ 2023.08.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230831000000/ title="arXiv @ 2023.08.31">arXiv @ 2023.08.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202309/>2023.09</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230901000000/ title="arXiv @ 2023.09.01">arXiv @ 2023.09.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230902000000/ title="arXiv @ 2023.09.02">arXiv @ 2023.09.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230903000000/ title="arXiv @ 2023.09.03">arXiv @ 2023.09.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230904000000/ title="arXiv @ 2023.09.04">arXiv @ 2023.09.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230905000000/ title="arXiv @ 2023.09.05">arXiv @ 2023.09.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230906000000/ title="arXiv @ 2023.09.06">arXiv @ 2023.09.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230907000000/ title="arXiv @ 2023.09.07">arXiv @ 2023.09.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230908000000/ title="arXiv @ 2023.09.08">arXiv @ 2023.09.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230909000000/ title="arXiv @ 2023.09.09">arXiv @ 2023.09.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230910000000/ title="arXiv @ 2023.09.10">arXiv @ 2023.09.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230911000000/ title="arXiv @ 2023.09.11">arXiv @ 2023.09.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230912000000/ title="arXiv @ 2023.09.12">arXiv @ 2023.09.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230913000000/ title="arXiv @ 2023.09.13">arXiv @ 2023.09.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230914000000/ title="arXiv @ 2023.09.14">arXiv @ 2023.09.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230915000000/ title="arXiv @ 2023.09.15">arXiv @ 2023.09.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230916000000/ title="arXiv @ 2023.09.16">arXiv @ 2023.09.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230917000000/ title="arXiv @ 2023.09.17">arXiv @ 2023.09.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230918000000/ title="arXiv @ 2023.09.18">arXiv @ 2023.09.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230919000000/ title="arXiv @ 2023.09.19">arXiv @ 2023.09.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230920000000/ title="arXiv @ 2023.09.20">arXiv @ 2023.09.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230921000000/ title="arXiv @ 2023.09.21">arXiv @ 2023.09.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230922000000/ title="arXiv @ 2023.09.22">arXiv @ 2023.09.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230923000000/ title="arXiv @ 2023.09.23">arXiv @ 2023.09.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230924000000/ title="arXiv @ 2023.09.24">arXiv @ 2023.09.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230925000000/ title="arXiv @ 2023.09.25">arXiv @ 2023.09.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230926000000/ title="arXiv @ 2023.09.26">arXiv @ 2023.09.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230927000000/ title="arXiv @ 2023.09.27">arXiv @ 2023.09.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230928000000/ title="arXiv @ 2023.09.28">arXiv @ 2023.09.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230929000000/ title="arXiv @ 2023.09.29">arXiv @ 2023.09.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230930000000/ title="arXiv @ 2023.09.30">arXiv @ 2023.09.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202310/>2023.1</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231001000000/ title="arXiv @ 2023.10.01">arXiv @ 2023.10.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231002000000/ title="arXiv @ 2023.10.02">arXiv @ 2023.10.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231003000000/ title="arXiv @ 2023.10.03">arXiv @ 2023.10.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231004000000/ title="arXiv @ 2023.10.04">arXiv @ 2023.10.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231005000000/ title="arXiv @ 2023.10.05">arXiv @ 2023.10.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231006000000/ title="arXiv @ 2023.10.06">arXiv @ 2023.10.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231007000000/ title="arXiv @ 2023.10.07">arXiv @ 2023.10.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231008000000/ title="arXiv @ 2023.10.08">arXiv @ 2023.10.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231009000000/ title="arXiv @ 2023.10.09">arXiv @ 2023.10.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231010000000/ title="arXiv @ 2023.10.10">arXiv @ 2023.10.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231011000000/ title="arXiv @ 2023.10.11">arXiv @ 2023.10.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231012000000/ title="arXiv @ 2023.10.12">arXiv @ 2023.10.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231013000000/ title="arXiv @ 2023.10.13">arXiv @ 2023.10.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231014000000/ title="arXiv @ 2023.10.14">arXiv @ 2023.10.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231015000000/ title="arXiv @ 2023.10.15">arXiv @ 2023.10.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231016000000/ title="arXiv @ 2023.10.16">arXiv @ 2023.10.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231017000000/ title="arXiv @ 2023.10.17">arXiv @ 2023.10.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231018000000/ title="arXiv @ 2023.10.18">arXiv @ 2023.10.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231019000000/ title="arXiv @ 2023.10.19">arXiv @ 2023.10.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231020000000/ title="arXiv @ 2023.10.20">arXiv @ 2023.10.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231021000000/ title="arXiv @ 2023.10.21">arXiv @ 2023.10.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231022000000/ title="arXiv @ 2023.10.22">arXiv @ 2023.10.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231023000000/ title="arXiv @ 2023.10.23">arXiv @ 2023.10.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231024000000/ title="arXiv @ 2023.10.24">arXiv @ 2023.10.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231025000000/ title="arXiv @ 2023.10.25">arXiv @ 2023.10.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231026000000/ title="arXiv @ 2023.10.26">arXiv @ 2023.10.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231027000000/ title="arXiv @ 2023.10.27">arXiv @ 2023.10.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231028000000/ title="arXiv @ 2023.10.28">arXiv @ 2023.10.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231029000000/ title="arXiv @ 2023.10.29">arXiv @ 2023.10.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231030000000/ title="arXiv @ 2023.10.30">arXiv @ 2023.10.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231031000000/ title="arXiv @ 2023.10.31">arXiv @ 2023.10.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202311/>2023.11</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231101000000/ title="arXiv @ 2023.11.01">arXiv @ 2023.11.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231102000000/ title="arXiv @ 2023.11.02">arXiv @ 2023.11.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231103000000/ title="arXiv @ 2023.11.03">arXiv @ 2023.11.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231104000000/ title="arXiv @ 2023.11.04">arXiv @ 2023.11.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231105000000/ title="arXiv @ 2023.11.05">arXiv @ 2023.11.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231106000000/ title="arXiv @ 2023.11.06">arXiv @ 2023.11.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231107000000/ title="arXiv @ 2023.11.07">arXiv @ 2023.11.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231108000000/ title="arXiv @ 2023.11.08">arXiv @ 2023.11.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231109000000/ title="arXiv @ 2023.11.09">arXiv @ 2023.11.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231110000000/ title="arXiv @ 2023.11.10">arXiv @ 2023.11.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231111000000/ title="arXiv @ 2023.11.11">arXiv @ 2023.11.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231112000000/ title="arXiv @ 2023.11.12">arXiv @ 2023.11.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231113000000/ title="arXiv @ 2023.11.13">arXiv @ 2023.11.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231114000000/ title="arXiv @ 2023.11.14">arXiv @ 2023.11.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231115000000/ title="arXiv @ 2023.11.15">arXiv @ 2023.11.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231116000000/ title="arXiv @ 2023.11.16">arXiv @ 2023.11.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231117000000/ title="arXiv @ 2023.11.17">arXiv @ 2023.11.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231118000000/ title="arXiv @ 2023.11.18">arXiv @ 2023.11.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231119000000/ title="arXiv @ 2023.11.19">arXiv @ 2023.11.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231120000000/ title="arXiv @ 2023.11.20">arXiv @ 2023.11.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231121000000/ title="arXiv @ 2023.11.21">arXiv @ 2023.11.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231122000000/ title="arXiv @ 2023.11.22">arXiv @ 2023.11.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231123000000/ title="arXiv @ 2023.11.23">arXiv @ 2023.11.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231124000000/ title="arXiv @ 2023.11.24">arXiv @ 2023.11.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231125000000/ title="arXiv @ 2023.11.25">arXiv @ 2023.11.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231126000000/ title="arXiv @ 2023.11.26">arXiv @ 2023.11.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231127000000/ title="arXiv @ 2023.11.27">arXiv @ 2023.11.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231128000000/ title="arXiv @ 2023.11.28">arXiv @ 2023.11.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231129000000/ title="arXiv @ 2023.11.29">arXiv @ 2023.11.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231130000000/ title="arXiv @ 2023.11.30">arXiv @ 2023.11.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202312/>2023.12</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231201000000/ title="arXiv @ 2023.12.01">arXiv @ 2023.12.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231202000000/ title="arXiv @ 2023.12.02">arXiv @ 2023.12.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231203000000/ title="arXiv @ 2023.12.03">arXiv @ 2023.12.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231204000000/ title="arXiv @ 2023.12.04">arXiv @ 2023.12.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231205000000/ title="arXiv @ 2023.12.05">arXiv @ 2023.12.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231206000000/ title="arXiv @ 2023.12.06">arXiv @ 2023.12.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231207000000/ title="arXiv @ 2023.12.07">arXiv @ 2023.12.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231208000000/ title="arXiv @ 2023.12.08">arXiv @ 2023.12.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231209000000/ title="arXiv @ 2023.12.09">arXiv @ 2023.12.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231210000000/ title="arXiv @ 2023.12.10">arXiv @ 2023.12.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231211000000/ title="arXiv @ 2023.12.11">arXiv @ 2023.12.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231212000000/ title="arXiv @ 2023.12.12">arXiv @ 2023.12.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231213000000/ title="arXiv @ 2023.12.13">arXiv @ 2023.12.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231214000000/ title="arXiv @ 2023.12.14">arXiv @ 2023.12.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231215000000/ title="arXiv @ 2023.12.15">arXiv @ 2023.12.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231216000000/ title="arXiv @ 2023.12.16">arXiv @ 2023.12.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231217000000/ title="arXiv @ 2023.12.17">arXiv @ 2023.12.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231218000000/ title="arXiv @ 2023.12.18">arXiv @ 2023.12.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231219000000/ title="arXiv @ 2023.12.19">arXiv @ 2023.12.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231220000000/ title="arXiv @ 2023.12.20">arXiv @ 2023.12.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231221000000/ title="arXiv @ 2023.12.21">arXiv @ 2023.12.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231222000000/ title="arXiv @ 2023.12.22">arXiv @ 2023.12.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231223000000/ title="arXiv @ 2023.12.23">arXiv @ 2023.12.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231224000000/ title="arXiv @ 2023.12.24">arXiv @ 2023.12.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231225000000/ title="arXiv @ 2023.12.25">arXiv @ 2023.12.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231226000000/ title="arXiv @ 2023.12.26">arXiv @ 2023.12.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231227000000/ title="arXiv @ 2023.12.27">arXiv @ 2023.12.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231228000000/ title="arXiv @ 2023.12.28">arXiv @ 2023.12.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231229000000/ title="arXiv @ 2023.12.29">arXiv @ 2023.12.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231230000000/ title="arXiv @ 2023.12.30">arXiv @ 2023.12.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231231000000/ title="arXiv @ 2023.12.31">arXiv @ 2023.12.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202401/>2024.01</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240101000000/ title="arXiv @ 2024.01.01">arXiv @ 2024.01.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240102000000/ title="arXiv @ 2024.01.02">arXiv @ 2024.01.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240103000000/ title="arXiv @ 2024.01.03">arXiv @ 2024.01.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240104000000/ title="arXiv @ 2024.01.04">arXiv @ 2024.01.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240105000000/ title="arXiv @ 2024.01.05">arXiv @ 2024.01.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240106000000/ title="arXiv @ 2024.01.06">arXiv @ 2024.01.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240107000000/ title="arXiv @ 2024.01.07">arXiv @ 2024.01.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240108000000/ title="arXiv @ 2024.01.08">arXiv @ 2024.01.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240109000000/ title="arXiv @ 2024.01.09">arXiv @ 2024.01.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240110000000/ title="arXiv @ 2024.01.10">arXiv @ 2024.01.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240111000000/ title="arXiv @ 2024.01.11">arXiv @ 2024.01.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240112000000/ title="arXiv @ 2024.01.12">arXiv @ 2024.01.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240113000000/ title="arXiv @ 2024.01.13">arXiv @ 2024.01.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240114000000/ title="arXiv @ 2024.01.14">arXiv @ 2024.01.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240115000000/ title="arXiv @ 2024.01.15">arXiv @ 2024.01.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240116000000/ title="arXiv @ 2024.01.16">arXiv @ 2024.01.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240117000000/ title="arXiv @ 2024.01.17">arXiv @ 2024.01.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240118000000/ title="arXiv @ 2024.01.18">arXiv @ 2024.01.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240119000000/ title="arXiv @ 2024.01.19">arXiv @ 2024.01.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240120000000/ title="arXiv @ 2024.01.20">arXiv @ 2024.01.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240121000000/ title="arXiv @ 2024.01.21">arXiv @ 2024.01.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240122000000/ title="arXiv @ 2024.01.22">arXiv @ 2024.01.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240123000000/ title="arXiv @ 2024.01.23">arXiv @ 2024.01.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240124000000/ title="arXiv @ 2024.01.24">arXiv @ 2024.01.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240125000000/ title="arXiv @ 2024.01.25">arXiv @ 2024.01.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240126000000/ title="arXiv @ 2024.01.26">arXiv @ 2024.01.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240127000000/ title="arXiv @ 2024.01.27">arXiv @ 2024.01.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240128000000/ title="arXiv @ 2024.01.28">arXiv @ 2024.01.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240129000000/ title="arXiv @ 2024.01.29">arXiv @ 2024.01.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240130000000/ title="arXiv @ 2024.01.30">arXiv @ 2024.01.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240131000000/ title="arXiv @ 2024.01.31">arXiv @ 2024.01.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202402/>2024.02</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240201000000/ title="arXiv @ 2024.02.01">arXiv @ 2024.02.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240202000000/ title="arXiv @ 2024.02.02">arXiv @ 2024.02.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240203000000/ title="arXiv @ 2024.02.03">arXiv @ 2024.02.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240204000000/ title="arXiv @ 2024.02.04">arXiv @ 2024.02.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240205000000/ title="arXiv @ 2024.02.05">arXiv @ 2024.02.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240206000000/ title="arXiv @ 2024.02.06">arXiv @ 2024.02.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/ title="arXiv @ 2024.02.07">arXiv @ 2024.02.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/ title="arXiv @ 2024.02.08">arXiv @ 2024.02.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/ title="arXiv @ 2024.02.09">arXiv @ 2024.02.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/ title="arXiv @ 2024.02.10">arXiv @ 2024.02.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240211000000/ title="arXiv @ 2024.02.11">arXiv @ 2024.02.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/ title="arXiv @ 2024.02.12">arXiv @ 2024.02.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/ title="arXiv @ 2024.02.13">arXiv @ 2024.02.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/ title="arXiv @ 2024.02.14">arXiv @ 2024.02.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/ title="arXiv @ 2024.02.15">arXiv @ 2024.02.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240216000000/ title="arXiv @ 2024.02.16">arXiv @ 2024.02.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/ title="arXiv @ 2024.02.17">arXiv @ 2024.02.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240218000000/ title="arXiv @ 2024.02.18">arXiv @ 2024.02.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/ title="arXiv @ 2024.02.19">arXiv @ 2024.02.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/ title="arXiv @ 2024.02.20">arXiv @ 2024.02.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/ title="arXiv @ 2024.02.21">arXiv @ 2024.02.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/ title="arXiv @ 2024.02.22">arXiv @ 2024.02.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/ title="arXiv @ 2024.02.23">arXiv @ 2024.02.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/ title="arXiv @ 2024.02.24">arXiv @ 2024.02.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/ title="arXiv @ 2024.02.25">arXiv @ 2024.02.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/ title="arXiv @ 2024.02.26">arXiv @ 2024.02.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/ title="arXiv @ 2024.02.27">arXiv @ 2024.02.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/ title="arXiv @ 2024.02.28">arXiv @ 2024.02.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/ title="arXiv @ 2024.02.29">arXiv @ 2024.02.29</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/202403/>2024.03</a><ul class=active><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/ title="arXiv @ 2024.03.01">arXiv @ 2024.03.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/ title="arXiv @ 2024.03.02">arXiv @ 2024.03.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/ title="arXiv @ 2024.03.03">arXiv @ 2024.03.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240304000000/ title="arXiv @ 2024.03.04">arXiv @ 2024.03.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/ title="arXiv @ 2024.03.05">arXiv @ 2024.03.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/ title="arXiv @ 2024.03.06">arXiv @ 2024.03.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/ title="arXiv @ 2024.03.07">arXiv @ 2024.03.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/ title="arXiv @ 2024.03.08">arXiv @ 2024.03.08</a></li><li><a class=active href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/ title="arXiv @ 2024.03.09">arXiv @ 2024.03.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/ title="arXiv @ 2024.03.10">arXiv @ 2024.03.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/ title="arXiv @ 2024.03.11">arXiv @ 2024.03.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/ title="arXiv @ 2024.03.12">arXiv @ 2024.03.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/ title="arXiv @ 2024.03.13">arXiv @ 2024.03.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/ title="arXiv @ 2024.03.14">arXiv @ 2024.03.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/ title="arXiv @ 2024.03.15">arXiv @ 2024.03.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/ title="arXiv @ 2024.03.16">arXiv @ 2024.03.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/ title="arXiv @ 2024.03.17">arXiv @ 2024.03.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/ title="arXiv @ 2024.03.18">arXiv @ 2024.03.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240319000000/ title="arXiv @ 2024.03.19">arXiv @ 2024.03.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/ title="arXiv @ 2024.03.20">arXiv @ 2024.03.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/ title="arXiv @ 2024.03.21">arXiv @ 2024.03.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/ title="arXiv @ 2024.03.22">arXiv @ 2024.03.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240323000000/ title="arXiv @ 2024.03.23">arXiv @ 2024.03.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/ title="arXiv @ 2024.03.24">arXiv @ 2024.03.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240325000000/ title="arXiv @ 2024.03.25">arXiv @ 2024.03.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240326000000/ title="arXiv @ 2024.03.26">arXiv @ 2024.03.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240327000000/ title="arXiv @ 2024.03.27">arXiv @ 2024.03.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240328000000/ title="arXiv @ 2024.03.28">arXiv @ 2024.03.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240329000000/ title="arXiv @ 2024.03.29">arXiv @ 2024.03.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240330000000/ title="arXiv @ 2024.03.30">arXiv @ 2024.03.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240331000000/ title="arXiv @ 2024.03.31">arXiv @ 2024.03.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202404/>2024.04</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240401000000/ title="arXiv @ 2024.04.01">arXiv @ 2024.04.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240402000000/ title="arXiv @ 2024.04.02">arXiv @ 2024.04.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240403000000/ title="arXiv @ 2024.04.03">arXiv @ 2024.04.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240404000000/ title="arXiv @ 2024.04.04">arXiv @ 2024.04.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240405000000/ title="arXiv @ 2024.04.05">arXiv @ 2024.04.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240406000000/ title="arXiv @ 2024.04.06">arXiv @ 2024.04.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240407000000/ title="arXiv @ 2024.04.07">arXiv @ 2024.04.07</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/algorithms/>Algorithms</a><ul><li><a href=/akitenkrad-blog/posts/algorithms/bandit-algorithm-basic/ title="Bandit Algorithm Basic">Bandit Algorithm Basic</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/conference/>Conference</a><ul><li><a href=/akitenkrad-blog/posts/conference/acl/ title="Annual Meeting of the Association for Computational Linguistics">Annual Meeting of the Association for Computational Linguistics</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/>Figures</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/figures/202208/20220822092748/ title=1ヶ月当たりの実労働時間の推移>1ヶ月当たりの実労働時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220813115200/ title=コンビニエンスストアの店舗数の推移>コンビニエンスストアの店舗数の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220821113322/ title=地域別1世帯当たり1ヶ月間の支出額の推移>地域別1世帯当たり1ヶ月間の支出額の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220828111836/ title="夫婦別子供の有無別生活時間 (2016)">夫婦別子供の有無別生活時間 (2016)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220827103615/ title=家族類型の変遷>家族類型の変遷</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220815131016/ title="年齢別大学院入学者数 (2021)">年齢別大学院入学者数 (2021)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220818122628/ title=情報サービス業における企業特殊的人的資本（名目）の推移>情報サービス業における企業特殊的人的資本（名目）の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220824095127/ title=有業者の社会生活における活動の変化>有業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220814175022/ title=東京23区の乗用車保有台数>東京23区の乗用車保有台数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220817121117/ title="業種別企業特殊的人的資本 (2018)">業種別企業特殊的人的資本 (2018)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220825094111/ title=無業者の社会生活における活動の変化>無業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220823104100/ title=睡眠時間の推移>睡眠時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220826102652/ title=総人口の推移>総人口の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220820225528/ title=都道府県別国公立別学校数・学生数>都道府県別国公立別学校数・学生数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220816120845/ title="都道府県別消費者物価指数/教育（全国平均=100）">都道府県別消費者物価指数/教育（全国平均=100）</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220812173814/ title=金沢市の夏の月平均気温の遷移>金沢市の夏の月平均気温の遷移</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/figures/202209/20220907122906/ title="ACL-2022 WordCloud">ACL-2022 WordCloud</a></li></ul></li></ul></li><li><a href=/akitenkrad-blog/posts/latex/ title="Latex Mathematics Syntax Guide">Latex Mathematics Syntax Guide</a></li><li><a href=/akitenkrad-blog/posts/markdown/ title="Markdown Sample">Markdown Sample</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/akitenkrad-blog/posts/arxiv/202403/20240309000000/hero.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_120x120_fit_box_3.png alt="Author Image"><h5 class=author-name></h5><p>Saturday, Mar 9, 2024</p></div><div class=title><h1>arXiv @ 2024.03.09</h1></div><div class=taxonomy-terms><ul><li class=rounded><a href=/akitenkrad-blog/tags/arxiv class="btn, btn-sm">arXiv</a></li><li class=rounded><a href=/akitenkrad-blog/tags/published2024 class="btn, btn-sm">2024</a></li></ul></div><div class=post-content id=post-content><figure style=border:none;width:100%;display:flex;justify-content:center><iframe src=pie.html width=900 height=620 style=border:none></iframe></figure><h2 id=primary-categories>Primary Categories</h2><ul><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/#astro-phco-1>astro-ph.CO (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/#csai-29>cs.AI (29)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/#csar-2>cs.AR (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/#cscc-1>cs.CC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/#csce-2>cs.CE (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/#cscg-1>cs.CG (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/#cscl-39>cs.CL (39)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/#cscr-6>cs.CR (6)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/#cscv-58>cs.CV (58)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/#cscy-2>cs.CY (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/#csdb-3>cs.DB (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/#csdc-5>cs.DC (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/#csdl-1>cs.DL (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/#csds-6>cs.DS (6)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/#csgt-4>cs.GT (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/#cshc-3>cs.HC (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/#csir-11>cs.IR (11)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/#csit-5>cs.IT (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/#cslg-46>cs.LG (46)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/#csma-2>cs.MA (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/#csms-1>cs.MS (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/#csni-3>cs.NI (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/#csoh-1>cs.OH (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/#cspl-2>cs.PL (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/#csro-18>cs.RO (18)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/#cssd-2>cs.SD (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/#csse-3>cs.SE (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/#cssi-2>cs.SI (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/#eessiv-5>eess.IV (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/#eesssp-1>eess.SP (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/#eesssy-6>eess.SY (6)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/#mathna-8>math.NA (8)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/#mathoc-2>math.OC (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/#physicsbio-ph-1>physics.bio-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/#physicsflu-dyn-1>physics.flu-dyn (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/#physicsmed-ph-1>physics.med-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/#q-biobm-1>q-bio.BM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/#q-biomn-1>q-bio.MN (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/#quant-ph-3>quant-ph (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/#statml-3>stat.ML (3)</a></li></ul><h2 id=keywords>Keywords</h2><table border=1 class=dataframe><thead><tr style=text-align:right><th>keyword</th><th>cs.AI</th><th>cs.CL</th><th>cs.CV</th><th>cs.IR</th><th>cs.LG</th><th>cs.RO</th></tr></thead><tbody><tr><td>Active Learning</td><td></td><td></td><td>1</td><td></td><td>1</td><td></td></tr><tr><td>Adversarial Attack</td><td></td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Adversarial Learning</td><td></td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Alpaca</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Anomaly Detection</td><td>1</td><td></td><td>3</td><td></td><td>1</td><td></td></tr><tr><td>Autoencoder</td><td></td><td></td><td>1</td><td></td><td>1</td><td></td></tr><tr><td>Automatic Speech Recognition</td><td>3</td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>BERT</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>BLOOM</td><td></td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Bandit Algorithm</td><td></td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Benchmarking</td><td>3</td><td>15</td><td>13</td><td>7</td><td>9</td><td></td></tr><tr><td>Black Box</td><td>1</td><td></td><td>1</td><td></td><td>3</td><td></td></tr><tr><td>ChatGPT</td><td>2</td><td></td><td></td><td>2</td><td></td><td></td></tr><tr><td>Chatbot</td><td>2</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Clustering</td><td></td><td>2</td><td>3</td><td></td><td>1</td><td>1</td></tr><tr><td>Common-sense Reasoning</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Continual Learning</td><td>1</td><td>2</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Continuous Time</td><td></td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Contrastive Learning</td><td>1</td><td>1</td><td>2</td><td></td><td>2</td><td></td></tr><tr><td>Convolution</td><td>1</td><td>1</td><td>7</td><td>1</td><td></td><td></td></tr><tr><td>Convolutional Neural Network</td><td></td><td></td><td>8</td><td>1</td><td></td><td></td></tr><tr><td>Counter-factual</td><td></td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Data Augmentation</td><td></td><td>1</td><td>3</td><td></td><td>1</td><td>1</td></tr><tr><td>Dependency Parsing</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Dialogue State Tracking</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Dialogue System</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Differential Privacy</td><td>1</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Diffusion Model</td><td></td><td></td><td>9</td><td></td><td>1</td><td></td></tr><tr><td>Direct Preference Optimization</td><td></td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Distribution Shift</td><td></td><td></td><td></td><td></td><td>2</td><td>2</td></tr><tr><td>Explainable AI</td><td></td><td></td><td>1</td><td></td><td>1</td><td></td></tr><tr><td>Face Recognition</td><td></td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Fact Verification</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Fairness</td><td></td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><td>Federated Learning</td><td></td><td></td><td></td><td></td><td>5</td><td></td></tr><tr><td>Few-shot</td><td>2</td><td>4</td><td>3</td><td></td><td></td><td></td></tr><tr><td>Few-shot Learning</td><td>1</td><td>2</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Fine-tuning</td><td>3</td><td>5</td><td>7</td><td>1</td><td>3</td><td>1</td></tr><tr><td>Foundation Model</td><td></td><td>1</td><td>2</td><td></td><td>2</td><td>1</td></tr><tr><td>GLUE</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>GPT</td><td>2</td><td>3</td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>GPT-3</td><td>1</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>GPT-3.5</td><td>1</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>GPT-4</td><td>1</td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>GPT-4 turbo</td><td>1</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Generative AI</td><td>2</td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><td>Geometry</td><td></td><td></td><td>3</td><td></td><td></td><td>1</td></tr><tr><td>Graph</td><td>7</td><td>2</td><td>2</td><td>2</td><td>7</td><td>2</td></tr><tr><td>Graph Attention Networks</td><td></td><td></td><td>1</td><td></td><td>1</td><td></td></tr><tr><td>Graph Convolutional Network</td><td></td><td></td><td>1</td><td>2</td><td></td><td></td></tr><tr><td>Graph Embedding</td><td></td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><td>Graph Neural Network</td><td>2</td><td>1</td><td></td><td></td><td>12</td><td></td></tr><tr><td>Grounding</td><td></td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Hallucination Detection</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Human Intervention</td><td></td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Image2text</td><td></td><td></td><td>2</td><td></td><td>1</td><td></td></tr><tr><td>In-context Learning</td><td>4</td><td>10</td><td></td><td></td><td></td><td></td></tr><tr><td>Information Retrieval</td><td></td><td>2</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Instruction Following</td><td>1</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Instruction Tuning</td><td>2</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Knowledge Distillation</td><td>4</td><td></td><td>3</td><td>3</td><td>1</td><td>1</td></tr><tr><td>Knowledge Graph</td><td>2</td><td>2</td><td></td><td></td><td>1</td><td></td></tr><tr><td>LLaMA</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>LSTM</td><td>2</td><td></td><td>1</td><td></td><td>1</td><td></td></tr><tr><td>Language Generation</td><td>1</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Large Language Model</td><td>16</td><td>42</td><td>5</td><td>5</td><td>6</td><td></td></tr><tr><td>Logistic Regression</td><td></td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Low-Resource</td><td></td><td>1</td><td></td><td></td><td>1</td><td></td></tr><tr><td>MNIST</td><td></td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Markov Decision Process</td><td></td><td>1</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Massive Multitask Language Understanding (MMLU)</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Mathematical Reasoning</td><td>1</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Message-Passing</td><td>2</td><td></td><td>1</td><td>1</td><td>3</td><td></td></tr><tr><td>Meta Learning</td><td></td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><td>Mistral</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Morphological Analysis</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Multi-modal</td><td>8</td><td>4</td><td>10</td><td>3</td><td>4</td><td>3</td></tr><tr><td>Named Entity Recognition</td><td>2</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Natural Language Generation</td><td>1</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Natural Language Inference</td><td></td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>Neural Machine Translation</td><td>2</td><td>3</td><td>1</td><td></td><td></td><td>1</td></tr><tr><td>Node Embedding</td><td></td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Object Detection</td><td></td><td></td><td>5</td><td></td><td></td><td></td></tr><tr><td>Online Reinforcement Learning</td><td></td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Optical Character Recognition</td><td></td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Out-of-distribution</td><td></td><td></td><td>1</td><td></td><td>2</td><td>1</td></tr><tr><td>Pre-trained Language Model</td><td>2</td><td>4</td><td></td><td>2</td><td></td><td></td></tr><tr><td>Probabilistic Model</td><td></td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Prompt</td><td>5</td><td>7</td><td>9</td><td>2</td><td></td><td></td></tr><tr><td>Pruning</td><td></td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Quantization</td><td></td><td>1</td><td>1</td><td></td><td>2</td><td></td></tr><tr><td>Question Answering</td><td>1</td><td>4</td><td>5</td><td></td><td></td><td></td></tr><tr><td>Reasoning</td><td>4</td><td>7</td><td>1</td><td>1</td><td>3</td><td></td></tr><tr><td>Recommendation</td><td>1</td><td>1</td><td></td><td>8</td><td>1</td><td></td></tr><tr><td>Recommender System</td><td>2</td><td>1</td><td></td><td>4</td><td></td><td></td></tr><tr><td>Recurrent Neural Network</td><td></td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Reinforcement Learning</td><td>2</td><td>1</td><td></td><td>1</td><td>8</td><td>4</td></tr><tr><td>Reinforcement Learning from Human Feedback</td><td></td><td>2</td><td></td><td></td><td>2</td><td></td></tr><tr><td>Relation Extraction</td><td>1</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Representation Learning</td><td></td><td></td><td>1</td><td></td><td>3</td><td></td></tr><tr><td>Retrieval Augmentation</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Retrieval-Augmented Generation</td><td></td><td>5</td><td></td><td>3</td><td></td><td></td></tr><tr><td>Sample Size</td><td></td><td></td><td></td><td></td><td></td><td>1</td></tr><tr><td>Self-Attention</td><td></td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Self-Distillation</td><td></td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Self-supervised Learning</td><td>2</td><td>1</td><td>3</td><td></td><td>4</td><td></td></tr><tr><td>Self-supervised Pre-training</td><td></td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Sentiment Analysis</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Simulation</td><td>1</td><td>1</td><td>3</td><td></td><td>2</td><td>9</td></tr><tr><td>Simulator</td><td>1</td><td>1</td><td>3</td><td></td><td>2</td><td>9</td></tr><tr><td>Speech-to-Speech Translation</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Stemming</td><td></td><td>1</td><td></td><td></td><td></td><td>1</td></tr><tr><td>Style Transfer</td><td></td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Summarization</td><td>2</td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>Supervised Learning</td><td>4</td><td></td><td>2</td><td></td><td>2</td><td></td></tr><tr><td>Text Analysis</td><td></td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Text Classification</td><td>2</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Text Generation</td><td>2</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Text Mining</td><td>1</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Text-to-speech</td><td></td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>Text2image</td><td></td><td></td><td>8</td><td></td><td></td><td></td></tr><tr><td>Topic Model</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Topic Modeling</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Transfer Learning</td><td></td><td>1</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Transformer</td><td></td><td>3</td><td>14</td><td>1</td><td>2</td><td></td></tr><tr><td>Unsupervised Learning</td><td>2</td><td></td><td>3</td><td></td><td>3</td><td></td></tr><tr><td>Vision Transformer</td><td></td><td>2</td><td>10</td><td></td><td></td><td></td></tr><tr><td>Vision-and-Language</td><td>1</td><td>1</td><td>3</td><td></td><td></td><td>1</td></tr><tr><td>Visual Question Answering</td><td></td><td></td><td>3</td><td></td><td></td><td></td></tr><tr><td>Weakly-supervised Learning</td><td></td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Zero-shot</td><td>1</td><td>2</td><td>4</td><td></td><td></td><td>1</td></tr><tr><td>Zero-shot Learning</td><td></td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>human-in-the-loop</td><td></td><td>1</td><td></td><td></td><td>1</td><td></td></tr></tbody></table><script>$(function(){$("table").addClass("keyword-table table-bordered border-success"),$("table thead").addClass("sticky-top"),$("table tbody td").css("text-align","")})</script><h2 id=cscl-39>cs.CL (39)</h2><h3 id=139--1292-halueval-wild-evaluating-hallucinations-of-language-models-in-the-wild-zhiying-zhu-et-al-2024>(1/39 | 1/292) HaluEval-Wild: Evaluating Hallucinations of Language Models in the Wild (Zhiying Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhiying Zhu, Zhiqing Sun, Yiming Yang. (2024)<br><strong>HaluEval-Wild: Evaluating Hallucinations of Language Models in the Wild</strong><br><button class=copy-to-clipboard title="HaluEval-Wild: Evaluating Hallucinations of Language Models in the Wild" index=1>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-1 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 113<br>Keywords: Benchmarking, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Alpaca, GPT, GPT-4, Question Answering, Question Answering, Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04307v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04307v1.pdf filename=2403.04307v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Hallucinations pose a significant challenge to the reliability of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> in critical domains. Recent <b>benchmarks</b> designed to assess <b>LLM</b> hallucinations within conventional NLP tasks, such as knowledge-intensive <b>question</b> <b>answering</b> <b>(QA)</b> and <b>summarization,</b> are insufficient for capturing the complexities of user-LLM interactions in dynamic, real-world settings. To address this gap, we introduce HaluEval-Wild, the first <b>benchmark</b> specifically designed to evaluate <b>LLM</b> hallucinations in the wild. We meticulously collect challenging (adversarially filtered by <b>Alpaca)</b> user queries from existing real-world user-LLM interaction datasets, including ShareGPT, to evaluate the hallucination rates of various <b>LLMs.</b> Upon analyzing the collected queries, we categorize them into five distinct types, which enables a fine-grained analysis of the types of hallucinations <b>LLMs</b> exhibit, and synthesize the reference answers with the powerful <b>GPT-4</b> model and <b>retrieval-augmented</b> <b>generation</b> <b>(RAG).</b> Our <b>benchmark</b> offers a novel approach towards enhancing our comprehension and improvement of <b>LLM</b> reliability in scenarios reflective of real-world interactions.</p></p class="citation"></blockquote><h3 id=239--2292-yi-open-foundation-models-by-01ai-01-ai-et-al-2024>(2/39 | 2/292) Yi: Open Foundation Models by 01.AI (01. AI et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><ol><li>AI, :, Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, Zonghong Dai. (2024)<br><strong>Yi: Open Foundation Models by 01.AI</strong><br><button class=copy-to-clipboard title="Yi: Open Foundation Models by 01.AI" index=2>
<span class=copy-to-clipboard-item>Copy Title<span></button></li></ol><div class="toast toast-copied toast-index-2 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 109<br>Keywords: Vision Transformer, Benchmarking, Fine-tuning, Fine-tuning, Foundation Model, Multi-modal, Multi-modal, Transformer, Chatbot, Massive Multitask Language Understanding (MMLU), Pre-trained Language Model, Vision Transformer, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04652v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04652v1.pdf filename=2403.04652v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce the Yi model family, a series of language and <b>multimodal</b> models that demonstrate strong multi-dimensional capabilities. The Yi model family is based on 6B and 34B <b>pretrained</b> <b>language</b> <b>models,</b> then we extend them to chat models, 200K long context models, depth-upscaled models, and <b>vision-language</b> <b>models.</b> Our base models achieve strong performance on a wide range of <b>benchmarks</b> like <b>MMLU,</b> and our <b>finetuned</b> chat models deliver strong human preference rate on major evaluation platforms like AlpacaEval and <b>Chatbot</b> Arena. Building upon our scalable super-computing infrastructure and the classical <b>transformer</b> architecture, we attribute the performance of Yi models primarily to its data quality resulting from our data-engineering efforts. For pretraining, we construct 3.1 trillion tokens of English and Chinese corpora using a cascaded data deduplication and quality filtering pipeline. For <b>finetuning,</b> we polish a small scale (less than 10K) instruction dataset over multiple iterations such that every single instance has been verified directly by our machine learning engineers. For <b>vision-language,</b> <b>we</b> combine the chat language model with a <b>vision</b> <b>transformer</b> encoder and train the model to align visual representations to the semantic space of the language model. We further extend the context length to 200K through lightweight continual pretraining and demonstrate strong needle-in-a-haystack retrieval performance. We show that extending the depth of the <b>pretrained</b> <b>checkpoint</b> <b>through</b> continual pretraining further improves performance. We believe that given our current results, continuing to scale up model parameters using thoroughly optimized data will lead to even stronger frontier models.</p></p class="citation"></blockquote><h3 id=339--3292-llms-in-the-imaginarium-tool-learning-through-simulated-trial-and-error-boshi-wang-et-al-2024>(3/39 | 3/292) LLMs in the Imaginarium: Tool Learning through Simulated Trial and Error (Boshi Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Boshi Wang, Hao Fang, Jason Eisner, Benjamin Van Durme, Yu Su. (2024)<br><strong>LLMs in the Imaginarium: Tool Learning through Simulated Trial and Error</strong><br><button class=copy-to-clipboard title="LLMs in the Imaginarium: Tool Learning through Simulated Trial and Error" index=3>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-3 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 100<br>Keywords: Continual Learning, Fine-tuning, Fine-tuning, GPT, GPT-4, Mistral, In-context Learning, In-context Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04746v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04746v1.pdf filename=2403.04746v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Tools are essential for <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to acquire up-to-date information and take consequential actions in external environments. Existing work on tool-augmented <b>LLMs</b> primarily focuses on the broad coverage of tools and the flexibility of adding new tools. However, a critical aspect that has surprisingly been understudied is simply how accurately an <b>LLM</b> uses tools for which it has been trained. We find that existing <b>LLMs,</b> including <b>GPT-4</b> and open-source <b>LLMs</b> specifically <b>fine-tuned</b> for tool use, only reach a correctness rate in the range of 30% to 60%, far from reliable use in practice. We propose a biologically inspired method for tool-augmented <b>LLMs,</b> simulated trial and error (STE), that orchestrates three key mechanisms for successful tool use behaviors in the biological system: trial and error, imagination, and memory. Specifically, STE leverages an <b>LLM&rsquo;s</b> &lsquo;imagination&rsquo; to simulate plausible scenarios for using a tool, after which the <b>LLM</b> interacts with the tool to learn from its execution feedback. Both short-term and long-term memory are employed to improve the depth and breadth of the exploration, respectively. Comprehensive experiments on ToolBench show that STE substantially improves tool learning for <b>LLMs</b> under both <b>in-context</b> <b>learning</b> and <b>fine-tuning</b> settings, bringing a boost of 46.7% to <b>Mistral-Instruct-7B</b> and enabling it to outperform <b>GPT-4.</b> We also show effective <b>continual</b> <b>learning</b> of tools via a simple experience replay strategy.</p></p class="citation"></blockquote><h3 id=439--4292-telecom-language-models-must-they-be-large-nicola-piovesan-et-al-2024>(4/39 | 4/292) Telecom Language Models: Must They Be Large? (Nicola Piovesan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nicola Piovesan, Antonio De Domenico, Fadhel Ayed. (2024)<br><strong>Telecom Language Models: Must They Be Large?</strong><br><button class=copy-to-clipboard title="Telecom Language Models: Must They Be Large?" index=4>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-4 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 90<br>Keywords: Retrieval-Augmented Generation, Retrieval-Augmented Generation, GPT, GPT-3, GPT-3.5, Common-sense Reasoning, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04666v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04666v1.pdf filename=2403.04666v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The increasing interest in <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> within the telecommunications sector underscores their potential to revolutionize operational efficiency. However, the deployment of these sophisticated models is often hampered by their substantial size and computational demands, raising concerns about their viability in resource-constrained environments. Addressing this challenge, recent advancements have seen the emergence of small language models that surprisingly exhibit performance comparable to their larger counterparts in many tasks, such as coding and <b>common-sense</b> <b>reasoning.</b> Phi-2, a compact yet powerful model, exemplifies this new wave of efficient small language models. This paper conducts a comprehensive evaluation of Phi-2&rsquo;s intrinsic understanding of the telecommunications domain. Recognizing the scale-related limitations, we enhance Phi-2&rsquo;s capabilities through a <b>Retrieval-Augmented</b> <b>Generation</b> <b>approach,</b> meticulously integrating an extensive knowledge base specifically curated with telecom standard specifications. The enhanced Phi-2 model demonstrates a profound improvement in accuracy, answering questions about telecom standards with a precision that closely rivals the more resource-intensive <b>GPT-3.5.</b> The paper further explores the refined capabilities of Phi-2 in addressing problem-solving scenarios within the telecom sector, highlighting its potential and limitations.</p></p class="citation"></blockquote><h3 id=539--5292-where-does-in-context-translation-happen-in-large-language-models-suzanna-sia-et-al-2024>(5/39 | 5/292) Where does In-context Translation Happen in Large Language Models (Suzanna Sia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Suzanna Sia, David Mueller, Kevin Duh. (2024)<br><strong>Where does In-context Translation Happen in Large Language Models</strong><br><button class=copy-to-clipboard title="Where does In-context Translation Happen in Large Language Models" index=5>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-5 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 90<br>Keywords: Fine-tuning, Self-supervised Learning, Neural Machine Translation, Neural Machine Translation, In-context Learning, In-context Learning, In-context Learning, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04510v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04510v1.pdf filename=2403.04510v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Self-supervised</b> <b>large</b> <b>language</b> <b>models</b> have demonstrated the ability to perform <b>Machine</b> <b>Translation</b> <b>(MT)</b> via <b>in-context</b> <b>learning,</b> but little is known about where the model performs the task with respect to <b>prompt</b> instructions and demonstration examples. In this work, we attempt to characterize the region where <b>large</b> <b>language</b> <b>models</b> transition from <b>in-context</b> <b>learners</b> to translation models. Through a series of layer-wise context-masking experiments on \textsc{GPTNeo2.7B}, \textsc{Bloom3B}, \textsc{Llama7b} and \textsc{Llama7b-chat}, we demonstrate evidence of a &ldquo;task recognition&rdquo; point where the translation task is encoded into the input representations and attention to context is no longer necessary. We further observe correspondence between the low performance when masking out entire layers, and the task recognition layers. Taking advantage of this redundancy results in 45% computational savings when <b>prompting</b> with 5 examples, and task recognition achieved at layer 14 / 32. Our layer-wise <b>fine-tuning</b> experiments indicate that the most effective layers for <b>MT</b> <b>fine-tuning</b> are the layers critical to task recognition.</p></p class="citation"></blockquote><h3 id=639--6292-few-shot-chain-of-thought-driven-reasoning-to-prompt-llms-for-open-ended-medical-question-answering-ojas-gramopadhye-et-al-2024>(6/39 | 6/292) Few shot chain-of-thought driven reasoning to prompt LLMs for open ended medical question answering (Ojas Gramopadhye et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ojas Gramopadhye, Saeel Sandeep Nachane, Prateek Chanda, Ganesh Ramakrishnan, Kshitij Sharad Jadhav, Yatin Nandwani, Dinesh Raghu, Sachindra Joshi. (2024)<br><strong>Few shot chain-of-thought driven reasoning to prompt LLMs for open ended medical question answering</strong><br><button class=copy-to-clipboard title="Few shot chain-of-thought driven reasoning to prompt LLMs for open ended medical question answering" index=6>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-6 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 80<br>Keywords: Few-shot, human-in-the-loop, Information Retrieval, Question Answering, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04890v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04890v1.pdf filename=2403.04890v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>models</b> <b>(LLMs)</b> have demonstrated significant potential in transforming healthcare by automating tasks such as clinical documentation, <b>information</b> <b>retrieval,</b> and decision support. In this aspect, carefully engineered <b>prompts</b> have emerged as a powerful tool for using <b>LLMs</b> for medical scenarios, e.g., patient clinical scenarios. In this paper, we propose a modified version of the MedQA-USMLE dataset, which is subjective, to mimic real-life clinical scenarios. We explore the Chain of Thought (CoT) <b>reasoning</b> based on subjective response generation for the modified MedQA-USMLE dataset with appropriate LM-driven forward <b>reasoning</b> for correct responses to the medical <b>questions.</b> <b>Keeping</b> in mind the importance of response verification in the medical setting, we utilize a reward training mechanism whereby the language model also provides an appropriate verified response for a particular response to a clinical <b>question.</b> <b>In</b> this regard, we also include <b>human-in-the-loop</b> for different evaluation aspects. We develop better in-contrast learning strategies by modifying the 5-shot-codex-CoT-prompt from arXiv:2207.08143 for the subjective MedQA dataset and developing our incremental-reasoning <b>prompt.</b> Our evaluations show that the incremental <b>reasoning</b> <b>prompt</b> performs better than the modified codex <b>prompt</b> in certain scenarios. We also show that greedy decoding with the incremental <b>reasoning</b> method performs better than other strategies, such as <b>prompt</b> chaining and eliminative <b>reasoning.</b></p></p class="citation"></blockquote><h3 id=739--7292-low-resource-court-judgment-summarization-for-common-law-systems-shuaiqi-liu-et-al-2024>(7/39 | 7/292) Low-Resource Court Judgment Summarization for Common Law Systems (Shuaiqi Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shuaiqi Liu, Jiannong Cao, Yicong Li, Ruosong Yang, Zhiyuan Wen. (2024)<br><strong>Low-Resource Court Judgment Summarization for Common Law Systems</strong><br><button class=copy-to-clipboard title="Low-Resource Court Judgment Summarization for Common Law Systems" index=7>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-7 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: I-2-7; I-7, cs-AI, cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: Data Augmentation, Few-shot, Low-Resource, Zero-shot, Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04454v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04454v1.pdf filename=2403.04454v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Common law courts need to refer to similar precedents&rsquo; judgments to inform their current decisions. Generating high-quality summaries of court judgment documents can facilitate legal practitioners to efficiently review previous cases and assist the general public in accessing how the courts operate and how the law is applied. Previous court judgment <b>summarization</b> research focuses on civil law or a particular jurisdiction&rsquo;s judgments. However, judges can refer to the judgments from all common law jurisdictions. Current <b>summarization</b> datasets are insufficient to satisfy the demands of summarizing precedents across multiple jurisdictions, especially when labeled <b>data</b> <b>are</b> scarce for many jurisdictions. To address the lack of datasets, we present CLSum, the first dataset for summarizing multi-jurisdictional common law court judgment documents. Besides, this is the first court judgment <b>summarization</b> work adopting <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> in <b>data</b> <b>augmentation,</b> summary generation, and evaluation. Specifically, we design an <b>LLM-based</b> <b>data</b> <b>augmentation</b> method incorporating legal knowledge. We also propose a legal knowledge enhanced evaluation metric based on <b>LLM</b> to assess the quality of generated judgment summaries. Our experimental results verify that the <b>LLM-based</b> <b>summarization</b> methods can perform well in the <b>few-shot</b> and <b>zero-shot</b> settings. Our <b>LLM-based</b> <b>data</b> <b>augmentation</b> method can mitigate the impact of low <b>data</b> <b>resources.</b> Furthermore, we carry out comprehensive comparative experiments to find essential model components and settings that are capable of enhancing <b>summarization</b> performance.</p></p class="citation"></blockquote><h3 id=839--8292-deep-icl-definition-enriched-experts-for-language-model-in-context-learning-xingwei-qu-et-al-2024>(8/39 | 8/292) DEEP-ICL: Definition-Enriched Experts for Language Model In-Context Learning (Xingwei Qu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xingwei Qu, Yiming Liang, Yucheng Wang, Tianyu Zheng, Tommy Yue, Lei Ma, Stephen W. Huang, Jiajun Zhang, Wenhu Chen, Chenghua Lin, Jie Fu, Ge Zhang. (2024)<br><strong>DEEP-ICL: Definition-Enriched Experts for Language Model In-Context Learning</strong><br><button class=copy-to-clipboard title="DEEP-ICL: Definition-Enriched Experts for Language Model In-Context Learning" index=8>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-8 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: Few-shot, Few-shot Learning, In-context Learning, In-context Learning, In-context Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04233v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04233v1.pdf filename=2403.04233v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>It has long been assumed that the sheer number of parameters in <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> drives <b>in-context</b> <b>learning</b> <b>(ICL)</b> capabilities, enabling remarkable performance improvements by leveraging task-specific demonstrations. Challenging this hypothesis, we introduce DEEP-ICL, a novel task Definition Enriched ExPert Ensembling methodology for <b>ICL.</b> DEEP-ICL explicitly extracts task definitions from given demonstrations and generates responses through learning task-specific examples. We argue that improvement from <b>ICL</b> does not directly rely on model size, but essentially stems from understanding task definitions and task-guided learning. Inspired by this, DEEP-ICL combines two 3B models with distinct roles (one for concluding task definitions and the other for learning task demonstrations) and achieves comparable performance to LLaMA2-13B. Furthermore, our framework outperforms conventional <b>ICL</b> by overcoming pretraining sequence length limitations, by supporting unlimited demonstrations. We contend that DEEP-ICL presents a novel alternative for achieving efficient <b>few-shot</b> <b>learning,</b> extending beyond the conventional <b>ICL.</b></p></p class="citation"></blockquote><h3 id=939--9292-proxy-rlhf-decoupling-generation-and-alignment-in-large-language-model-with-proxy-yu-zhu-et-al-2024>(9/39 | 9/292) Proxy-RLHF: Decoupling Generation and Alignment in Large Language Model with Proxy (Yu Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yu Zhu, Chuxiong Sun, Wenfei Yang, Wenqiang Wei, Bo Tang, Tianzhu Zhang, Zhiyu Li, Shifeng Zhang, Feiyu Xiong, Jie Hu, Mingchuan yang. (2024)<br><strong>Proxy-RLHF: Decoupling Generation and Alignment in Large Language Model with Proxy</strong><br><button class=copy-to-clipboard title="Proxy-RLHF: Decoupling Generation and Alignment in Large Language Model with Proxy" index=9>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-9 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 60<br>Keywords: Markov Decision Process, Reinforcement Learning, Reinforcement Learning from Human Feedback, Reinforcement Learning from Human Feedback, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04283v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04283v1.pdf filename=2403.04283v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Reinforcement</b> <b>Learning</b> <b>from</b> <b>Human</b> <b>Feedback</b> <b>(RLHF)</b> is the prevailing approach to ensure <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> align with human values. However, existing <b>RLHF</b> methods require a high computational cost, one main reason being that <b>RLHF</b> assigns both the generation and alignment tasks to the <b>LLM</b> simultaneously. In this paper, we introduce Proxy-RLHF, which decouples the generation and alignment processes of <b>LLMs,</b> achieving alignment with human values at a much lower computational cost. We start with a novel <b>Markov</b> <b>Decision</b> <b>Process</b> (MDP) designed for the alignment process and employ <b>Reinforcement</b> <b>Learning</b> <b>(RL)</b> <b>to</b> <b>train</b> a streamlined proxy model that oversees the token generation of the <b>LLM,</b> without altering the <b>LLM</b> itself. Experiments show that our method achieves a comparable level of alignment with only 1% of the training parameters of other methods.</p></p class="citation"></blockquote><h3 id=1039--10292-uncertainty-aware-relational-graph-neural-network-for-few-shot-knowledge-graph-completion-qian-li-et-al-2024>(10/39 | 10/292) Uncertainty-Aware Relational Graph Neural Network for Few-Shot Knowledge Graph Completion (Qian Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qian Li, Shu Guo, Yingjia Chen, Cheng Ji, Jiawei Sheng, Jianxin Li. (2024)<br><strong>Uncertainty-Aware Relational Graph Neural Network for Few-Shot Knowledge Graph Completion</strong><br><button class=copy-to-clipboard title="Uncertainty-Aware Relational Graph Neural Network for Few-Shot Knowledge Graph Completion" index=10>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-10 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 56<br>Keywords: Graph, Graph Neural Network, Benchmarking, Convolution, Few-shot, Few-shot Learning, Knowledge Graph, Knowledge Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04521v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04521v1.pdf filename=2403.04521v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Few-shot</b> <b>knowledge</b> <b>graph</b> <b>completion</b> <b>(FKGC)</b> aims to query the unseen facts of a relation given its <b>few-shot</b> <b>reference</b> entity pairs. The side effect of noises due to the uncertainty of entities and triples may limit the <b>few-shot</b> <b>learning,</b> but existing FKGC works neglect such uncertainty, which leads them more susceptible to limited reference samples with noises. In this paper, we propose a novel uncertainty-aware <b>few-shot</b> <b>KG</b> completion framework (UFKGC) to model uncertainty for a better understanding of the limited data by learning representations under Gaussian distribution. Uncertainty representation is first designed for estimating the uncertainty scope of the entity pairs after transferring feature representations into a Gaussian distribution. Further, to better integrate the neighbors with uncertainty characteristics for entity features, we design an uncertainty-aware relational <b>graph</b> <b>neural</b> <b>network</b> (UR-GNN) to conduct <b>convolution</b> operations between the Gaussian distributions. Then, multiple random samplings are conducted for reference triples within the Gaussian distribution to generate smooth reference representations during the optimization. The final completion score for each query instance is measured by the designed uncertainty optimization to make our approach more robust to the noises in <b>few-shot</b> <b>scenarios.</b> Experimental results show that our approach achieves excellent performance on two <b>benchmark</b> datasets compared to its competitors.</p></p class="citation"></blockquote><h3 id=1139--11292-pearl-a-review-driven-persona-knowledge-grounded-conversational-recommendation-dataset-minjin-kim-et-al-2024>(11/39 | 11/292) Pearl: A Review-driven Persona-Knowledge Grounded Conversational Recommendation Dataset (Minjin Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Minjin Kim, Minju Kim, Hana Kim, Beong-woo Kwak, Soyeon Chun, Hyunseo Kim, SeongKu Kang, Youngjae Yu, Jinyoung Yeo, Dongha Lee. (2024)<br><strong>Pearl: A Review-driven Persona-Knowledge Grounded Conversational Recommendation Dataset</strong><br><button class=copy-to-clipboard title="Pearl: A Review-driven Persona-Knowledge Grounded Conversational Recommendation Dataset" index=11>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-11 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Recommendation, Recommender System, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04460v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04460v2.pdf filename=2403.04460v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Conversational <b>recommender</b> <b>system</b> is an emerging area that has garnered an increasing interest in the community, especially with the advancements in <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> that enable diverse <b>reasoning</b> over conversational input. Despite the progress, the field has many aspects left to explore. The currently available public datasets for conversational <b>recommendation</b> lack specific user preferences and explanations for <b>recommendations,</b> hindering high-quality <b>recommendations.</b> To address such challenges, we present a novel conversational <b>recommendation</b> dataset named PEARL, synthesized with persona- and knowledge-augmented <b>LLM</b> simulators. We obtain detailed persona and knowledge from real-world reviews and construct a <b>large-scale</b> <b>dataset</b> <b>with</b> over 57k dialogues. Our experimental results demonstrate that utterances in PEARL include more specific user preferences, show expertise in the target domain, and provide <b>recommendations</b> more relevant to the dialogue context than those in prior datasets.</p></p class="citation"></blockquote><h3 id=1239--12292-nlpre-a-revised-approach-towards-language-centric-benchmarking-of-natural-language-preprocessing-systems-martyna-wiącek-et-al-2024>(12/39 | 12/292) NLPre: a revised approach towards language-centric benchmarking of Natural Language Preprocessing systems (Martyna Wiącek et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Martyna Wiącek, Piotr Rybak, Łukasz Pszenny, Alina Wróblewska. (2024)<br><strong>NLPre: a revised approach towards language-centric benchmarking of Natural Language Preprocessing systems</strong><br><button class=copy-to-clipboard title="NLPre: a revised approach towards language-centric benchmarking of Natural Language Preprocessing systems" index=12>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-12 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 46<br>Keywords: Benchmarking, Benchmarking, Transformer, Dependency Parsing, Morphological Analysis, GLUE<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04507v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04507v1.pdf filename=2403.04507v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the advancements of <b>transformer-based</b> architectures, we observe the rise of natural language preprocessing (NLPre) tools capable of solving preliminary NLP tasks (e.g. tokenisation, part-of-speech tagging, <b>dependency</b> <b>parsing,</b> or <b>morphological</b> <b>analysis)</b> without any external linguistic guidance. It is arduous to compare novel solutions to well-entrenched preprocessing toolkits, relying on rule-based <b>morphological</b> <b>analysers</b> or dictionaries. Aware of the shortcomings of existing NLPre evaluation approaches, we investigate a novel method of reliable and fair evaluation and performance reporting. Inspired by the <b>GLUE</b> <b>benchmark,</b> the proposed language-centric <b>benchmarking</b> system enables comprehensive ongoing evaluation of multiple NLPre tools, while credibly tracking their performance. The prototype application is configured for Polish and integrated with the thoroughly assembled NLPre-PL <b>benchmark.</b> Based on this <b>benchmark,</b> we conduct an extensive evaluation of a variety of Polish NLPre systems. To facilitate the construction of <b>benchmarking</b> environments for other languages, e.g. NLPre-GA for Irish or NLPre-ZH for Chinese, we ensure full customization of the publicly released source code of the <b>benchmarking</b> system. The links to all the resources (deployed platforms, source code, trained models, datasets etc.) can be found on the project website: <a href=https://sites.google.com/view/nlpre-benchmark>https://sites.google.com/view/nlpre-benchmark</a>.</p></p class="citation"></blockquote><h3 id=1339--13292-automating-the-information-extraction-from-semi-structured-interview-transcripts-angelina-parfenova-2024>(13/39 | 13/292) Automating the Information Extraction from Semi-Structured Interview Transcripts (Angelina Parfenova, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Angelina Parfenova. (2024)<br><strong>Automating the Information Extraction from Semi-Structured Interview Transcripts</strong><br><button class=copy-to-clipboard title="Automating the Information Extraction from Semi-Structured Interview Transcripts" index=13>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-13 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CY, cs-IR, cs-SI, cs.CL<br>Keyword Score: 43<br>Keywords: Clustering, Topic Model, BERT, Information Retrieval, Topic Modeling<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04819v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04819v1.pdf filename=2403.04819v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper explores the development and application of an automated system designed to extract <b>information</b> <b>from</b> semi-structured interview transcripts. Given the labor-intensive nature of traditional qualitative analysis methods, such as coding, there exists a significant demand for tools that can facilitate the analysis process. Our research investigates various <b>topic</b> <b>modeling</b> techniques and concludes that the best model for analyzing interview texts is a combination of <b>BERT</b> embeddings and HDBSCAN <b>clustering.</b> We present a user-friendly software prototype that enables researchers, including those without programming skills, to efficiently process and visualize the thematic structure of interview data. This tool not only facilitates the initial stages of qualitative analysis but also offers insights into the interconnectedness of <b>topics</b> <b>revealed,</b> thereby enhancing the depth of qualitative analysis.</p></p class="citation"></blockquote><h3 id=1439--14292-fact-checking-the-output-of-large-language-models-via-token-level-uncertainty-quantification-ekaterina-fadeeva-et-al-2024>(14/39 | 14/292) Fact-Checking the Output of Large Language Models via Token-Level Uncertainty Quantification (Ekaterina Fadeeva et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ekaterina Fadeeva, Aleksandr Rubashevskii, Artem Shelmanov, Sergey Petrakov, Haonan Li, Hamdy Mubarak, Evgenii Tsymbalov, Gleb Kuzmin, Alexander Panchenko, Timothy Baldwin, Preslav Nakov, Maxim Panov. (2024)<br><strong>Fact-Checking the Output of Large Language Models via Token-Level Uncertainty Quantification</strong><br><button class=copy-to-clipboard title="Fact-Checking the Output of Large Language Models via Token-Level Uncertainty Quantification" index=14>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-14 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 40<br>Keywords: Fact Verification, Hallucination Detection, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04696v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04696v1.pdf filename=2403.04696v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> are notorious for hallucinating, i.e., producing erroneous claims in their output. Such <b>hallucinations</b> <b>can</b> be dangerous, as occasional factual inaccuracies in the generated text might be obscured by the rest of the output being generally factual, making it extremely hard for the users to spot them. Current services that leverage <b>LLMs</b> usually do not provide any means for detecting unreliable generations. Here, we aim to bridge this gap. In particular, we propose a novel <b>fact-checking</b> <b>and</b> <b>hallucination</b> <b>detection</b> pipeline based on token-level uncertainty quantification. Uncertainty scores leverage information encapsulated in the output of a neural network or its layers to detect unreliable predictions, and we show that they can be used to <b>fact-check</b> <b>the</b> atomic claims in the <b>LLM</b> output. Moreover, we present a novel token-level uncertainty quantification method that removes the impact of uncertainty about what claim to generate on the current step and what surface form to use. Our method Claim Conditioned Probability (CCP) measures only the uncertainty of particular claim value expressed by the model. Experiments on the task of biography generation demonstrate strong improvements for CCP compared to the baselines for six different <b>LLMs</b> and three languages. Human evaluation reveals that the <b>fact-checking</b> <b>pipeline</b> based on uncertainty quantification is competitive with a <b>fact-checking</b> <b>tool</b> that leverages external knowledge.</p></p class="citation"></blockquote><h3 id=1539--15292-qaq-quality-adaptive-quantization-for-llm-kv-cache-shichen-dong-et-al-2024>(15/39 | 15/292) QAQ: Quality Adaptive Quantization for LLM KV Cache (Shichen Dong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shichen Dong, Wen Cheng, Jiayu Qin, Wei Wang. (2024)<br><strong>QAQ: Quality Adaptive Quantization for LLM KV Cache</strong><br><button class=copy-to-clipboard title="QAQ: Quality Adaptive Quantization for LLM KV Cache" index=15>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-15 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Quantization, Question Answering, Text Generation, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04643v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04643v1.pdf filename=2403.04643v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The emergence of <b>LLMs</b> has ignited a fresh surge of breakthroughs in NLP applications, particularly in domains such as <b>question-answering</b> <b>systems</b> and <b>text</b> <b>generation.</b> As the need for longer context grows, a significant bottleneck in model deployment emerges due to the linear expansion of the Key-Value (KV) cache with the context length. Existing methods primarily rely on various hypotheses, such as sorting the KV cache based on attention scores for replacement or eviction, to compress the KV cache and improve model throughput. However, heuristics used by these strategies may wrongly evict essential KV cache, which can significantly degrade model performance. In this paper, we propose QAQ, a Quality Adaptive <b>Quantization</b> scheme for the KV cache. We theoretically demonstrate that key cache and value cache exhibit distinct sensitivities to <b>quantization,</b> leading to the formulation of separate <b>quantization</b> strategies for their non-uniform <b>quantization.</b> Through the integration of dedicated outlier handling, as well as an improved attention-aware approach, QAQ achieves up to 10x the compression ratio of the KV cache size with a neglectable impact on model performance. QAQ significantly reduces the practical hurdles of deploying <b>LLMs,</b> opening up new possibilities for longer-context applications. The code is available at github.com/ClubieDong/KVCacheQuantization.</p></p class="citation"></blockquote><h3 id=1639--16292-ultrawiki-ultra-fine-grained-entity-set-expansion-with-negative-seed-entities-yangning-li-et-al-2024>(16/39 | 16/292) UltraWiki: Ultra-fine-grained Entity Set Expansion with Negative Seed Entities (Yangning Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yangning Li, Qingsong Lv, Tianyu Yu, Yinghui Li, Shulin Huang, Tingwei Lu, Xuming Hu, Wenhao JIang, Hai-Tao Zheng, Hui Wang. (2024)<br><strong>UltraWiki: Ultra-fine-grained Entity Set Expansion with Negative Seed Entities</strong><br><button class=copy-to-clipboard title="UltraWiki: Ultra-fine-grained Entity Set Expansion with Negative Seed Entities" index=16>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-16 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Contrastive Learning, Reasoning, Large Language Model, Retrieval Augmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04247v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04247v1.pdf filename=2403.04247v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Entity Set Expansion (ESE) aims to identify new entities belonging to the same semantic class as a given set of seed entities. Traditional methods primarily relied on positive seed entities to represent a target semantic class, which poses challenge for the representation of ultra-fine-grained semantic classes. Ultra-fine-grained semantic classes are defined based on fine-grained semantic classes with more specific attribute constraints. Describing it with positive seed entities alone cause two issues: (i) Ambiguity among ultra-fine-grained semantic classes. (ii) Inability to define &ldquo;unwanted&rdquo; semantic. Due to these inherent shortcomings, previous methods struggle to address the ultra-fine-grained ESE (Ultra-ESE). To solve this issue, we first introduce negative seed entities in the inputs, which belong to the same fine-grained semantic class as the positive seed entities but differ in certain attributes. Negative seed entities eliminate the semantic ambiguity by contrast between positive and negative attributes. Meanwhile, it provide a straightforward way to express &ldquo;unwanted&rdquo;. To assess model performance in Ultra-ESE, we constructed UltraWiki, the first <b>large-scale</b> <b>dataset</b> <b>tailored</b> for Ultra-ESE. UltraWiki encompasses 236 ultra-fine-grained semantic classes, where each query of them is represented with 3-5 positive and negative seed entities. A <b>retrieval-based</b> <b>framework</b> RetExpan and a generation-based framework GenExpan are proposed to comprehensively assess the efficacy of <b>large</b> <b>language</b> <b>models</b> from two different paradigms in Ultra-ESE. Moreover, we devised three strategies to enhance models&rsquo; comprehension of ultra-fine-grained entities semantics: <b>contrastive</b> <b>learning,</b> <b>retrieval</b> <b>augmentation,</b> and chain-of-thought <b>reasoning.</b> Extensive experiments confirm the effectiveness of our proposed strategies and also reveal that there remains a <b>large</b> <b>space</b> <b>for</b> improvement in Ultra-ESE.</p></p class="citation"></blockquote><h3 id=1739--17292-attempt-towards-stress-transfer-in-speech-to-speech-machine-translation-sai-akarsh-et-al-2024>(17/39 | 17/292) Attempt Towards Stress Transfer in Speech-to-Speech Machine Translation (Sai Akarsh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sai Akarsh, Vamshi Raghusimha, Anindita Mondal, Anil Vuppala. (2024)<br><strong>Attempt Towards Stress Transfer in Speech-to-Speech Machine Translation</strong><br><button class=copy-to-clipboard title="Attempt Towards Stress Transfer in Speech-to-Speech Machine Translation" index=17>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-17 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-SD, cs.CL, eess-AS<br>Keyword Score: 40<br>Keywords: Neural Machine Translation, Speech-to-Speech Translation, Text-to-speech, Text-to-speech<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04178v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04178v1.pdf filename=2403.04178v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The language diversity in India&rsquo;s education sector poses a significant challenge, hindering inclusivity. Despite the democratization of knowledge through online educational content, the dominance of English, as the internet&rsquo;s lingua franca, limits accessibility, emphasizing the crucial need for translation into Indian languages. Despite existing <b>Speech-to-Speech</b> <b>Machine</b> <b>Translation</b> (SSMT) technologies, the lack of intonation in these systems gives monotonous translations, leading to a loss of audience interest and disengagement from the content. To address this, our paper introduces a dataset with stress annotations in Indian English and also a <b>Text-to-Speech</b> <b>(TTS)</b> architecture capable of incorporating stress into synthesized speech. This dataset is used for training a stress detection model, which is then used in the SSMT system for detecting stress in the source speech and transferring it into the target language speech. The <b>TTS</b> architecture is based on FastPitch and can modify the variances based on stressed words given. We present an Indian English-to-Hindi SSMT system that can transfer stress and aim to enhance the overall quality and engagement of educational content.</p></p class="citation"></blockquote><h3 id=1839--18292-electrocardiogram-instruction-tuning-for-report-generation-zhongwei-wan-et-al-2024>(18/39 | 18/292) Electrocardiogram Instruction Tuning for Report Generation (Zhongwei Wan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhongwei Wan, Che Liu, Xin Wang, Chaofan Tao, Hui Shen, Zhenwu Peng, Jie Fu, Rossella Arcucci, Huaxiu Yao, Mi Zhang. (2024)<br><strong>Electrocardiogram Instruction Tuning for Report Generation</strong><br><button class=copy-to-clipboard title="Electrocardiogram Instruction Tuning for Report Generation" index=18>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-18 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL, eess-SP<br>Keyword Score: 39<br>Keywords: Benchmarking, Multi-modal, Multi-modal, Zero-shot, Instruction Tuning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04945v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04945v2.pdf filename=2403.04945v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Electrocardiogram (ECG) serves as the primary non-invasive diagnostic tool for cardiac conditions monitoring, are crucial in assisting clinicians. Recent studies have concentrated on classifying cardiac conditions using ECG data but have overlooked ECG report generation, which is not only time-consuming but also requires clinical expertise. To automate ECG report generation and ensure its versatility, we propose the <b>Multimodal</b> ECG <b>Instruction</b> <b>Tuning</b> (MEIT) framework, the \textit{first} attempt to tackle ECG report generation with <b>LLMs</b> and <b>multimodal</b> <b>instructions.</b> <b>To</b> facilitate future research, we establish a <b>benchmark</b> to evaluate MEIT with various <b>LLMs</b> backbones across two large-scale ECG datasets. Our approach uniquely aligns the representations of the ECG signal and the report, and we conduct extensive experiments to <b>benchmark</b> MEIT with nine open source <b>LLMs,</b> using more than 800,000 ECG reports. MEIT&rsquo;s results underscore the superior performance of <b>instruction-tuned</b> <b>LLMs,</b> showcasing their proficiency in quality report generation, <b>zero-shot</b> capabilities, and resilience to signal perturbation. These findings emphasize the efficacy of our MEIT framework and its potential for real-world clinical application.</p></p class="citation"></blockquote><h3 id=1939--19292-can-your-model-tell-a-negation-from-an-implicature-unravelling-challenges-with-intent-encoders-yuwei-zhang-et-al-2024>(19/39 | 19/292) Can Your Model Tell a Negation from an Implicature? Unravelling Challenges With Intent Encoders (Yuwei Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuwei Zhang, Siffi Singh, Sailik Sengupta, Igor Shalyminov, Hang Su, Hwanjun Song, Saab Mansour. (2024)<br><strong>Can Your Model Tell a Negation from an Implicature? Unravelling Challenges With Intent Encoders</strong><br><button class=copy-to-clipboard title="Can Your Model Tell a Negation from an Implicature? Unravelling Challenges With Intent Encoders" index=19>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-19 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 36<br>Keywords: Benchmarking, Clustering, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04314v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04314v1.pdf filename=2403.04314v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Conversational systems often rely on embedding models for intent classification and intent <b>clustering</b> tasks. The advent of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> which enable instructional embeddings allowing one to adjust semantics over the embedding space using <b>prompts,</b> are being viewed as a panacea for these downstream conversational tasks. However, traditional evaluation <b>benchmarks</b> rely solely on task metrics that don&rsquo;t particularly measure gaps related to semantic understanding. Thus, we propose an intent semantic toolkit that gives a more holistic view of intent embedding models by considering three tasks &ndash; (1) intent classification, (2) intent <b>clustering,</b> and (3) a novel triplet task. The triplet task gauges the model&rsquo;s understanding of two semantic concepts paramount in real-world conversational systems &ndash; negation and implicature. We observe that current embedding models fare poorly in semantic understanding of these concepts. To address this, we propose a pre-training approach to improve the embedding model by leveraging augmentation with data generated by an auto-regressive model and a contrastive loss term. Our approach improves the semantic understanding of the intent embedding model on the aforementioned linguistic dimensions while slightly effecting their performance on downstream task metrics.</p></p class="citation"></blockquote><h3 id=2039--20292-constitutionalexperts-training-a-mixture-of-principle-based-prompts-savvas-petridis-et-al-2024>(20/39 | 20/292) ConstitutionalExperts: Training a Mixture of Principle-based Prompts (Savvas Petridis et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Savvas Petridis, Ben Wedin, Ann Yuan, James Wexler, Nithum Thain. (2024)<br><strong>ConstitutionalExperts: Training a Mixture of Principle-based Prompts</strong><br><button class=copy-to-clipboard title="ConstitutionalExperts: Training a Mixture of Principle-based Prompts" index=20>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-20 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04894v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04894v1.pdf filename=2403.04894v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> are highly capable at a variety of tasks given the right <b>prompt,</b> but writing one is still a difficult and tedious process. In this work, we introduce ConstitutionalExperts, a method for learning a <b>prompt</b> consisting of constitutional principles (i.e. rules), given a training dataset. Unlike prior methods that optimize the <b>prompt</b> as a single entity, our method incrementally improves the <b>prompt</b> by surgically editing individual principles. We also show that we can improve overall performance by learning unique <b>prompts</b> for different semantic regions of the training data and using a mixture-of-experts (MoE) architecture to route inputs at inference time. We compare our method to other state of the art <b>prompt-optimization</b> techniques across six <b>benchmark</b> datasets. We also investigate whether MoE improves these other techniques. Our results suggest that ConstitutionalExperts outperforms other <b>prompt</b> optimization techniques by 10.9% (F1) and that mixture-of-experts improves all techniques, suggesting its broad applicability.</p></p class="citation"></blockquote><h3 id=2139--21292-exploring-continual-learning-of-compositional-generalization-in-nli-xiyan-fu-et-al-2024>(21/39 | 21/292) Exploring Continual Learning of Compositional Generalization in NLI (Xiyan Fu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiyan Fu, Anette Frank. (2024)<br><strong>Exploring Continual Learning of Compositional Generalization in NLI</strong><br><button class=copy-to-clipboard title="Exploring Continual Learning of Compositional Generalization in NLI" index=21>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-21 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, Continual Learning, Natural Language Inference, Natural Language Inference<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04400v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04400v1.pdf filename=2403.04400v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Compositional <b>Natural</b> <b>Language</b> <b>Inference</b> has been explored to assess the true abilities of neural models to perform <b>NLI.</b> Yet, current evaluations assume models to have full access to all primitive inferences in advance, in contrast to humans that continuously acquire inference knowledge. In this paper, we introduce the <b>Continual</b> <b>Compositional</b> Generalization in Inference (C2Gen <b>NLI)</b> challenge, where a model continuously acquires knowledge of constituting primitive inference tasks as a basis for compositional inferences. We explore how <b>continual</b> <b>learning</b> affects compositional generalization in <b>NLI,</b> by designing a <b>continual</b> <b>learning</b> setup for compositional <b>NLI</b> inference tasks. Our experiments demonstrate that models fail to compositionally generalize in a <b>continual</b> <b>scenario.</b> To address this problem, we first <b>benchmark</b> various <b>continual</b> <b>learning</b> algorithms and verify their efficacy. We then further analyze C2Gen, focusing on how to order primitives and compositional inference types and examining correlations between subtasks. Our analyses show that by learning subtasks continuously while observing their dependencies and increasing degrees of difficulty, <b>continual</b> <b>learning</b> can enhance composition generalization ability.</p></p class="citation"></blockquote><h3 id=2239--22292-evaluation-of-llms-on-syntax-aware-code-fill-in-the-middle-tasks-linyuan-gong-et-al-2024>(22/39 | 22/292) Evaluation of LLMs on Syntax-Aware Code Fill-in-the-Middle Tasks (Linyuan Gong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Linyuan Gong, Sida Wang, Mostafa Elhoushi, Alvin Cheung. (2024)<br><strong>Evaluation of LLMs on Syntax-Aware Code Fill-in-the-Middle Tasks</strong><br><button class=copy-to-clipboard title="Evaluation of LLMs on Syntax-Aware Code Fill-in-the-Middle Tasks" index=22>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-22 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs-SE, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04814v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04814v1.pdf filename=2403.04814v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce Syntax-Aware Fill-In-the-Middle (SAFIM), a new <b>benchmark</b> for evaluating <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> on the code Fill-in-the-Middle (FIM) task. This <b>benchmark</b> focuses on syntax-aware completions of program structures such as code blocks and conditional expressions, and includes 17,720 examples from multiple programming languages, sourced from recent code submissions after April 2022 to minimize data contamination. SAFIM provides a robust framework with various <b>prompt</b> designs and novel syntax-aware post-processing techniques, facilitating accurate and fair comparisons across <b>LLMs.</b> Our comprehensive evaluation of 15 <b>LLMs</b> shows that FIM pretraining not only enhances FIM proficiency but also improves Left-to-Right (L2R) inference using <b>LLMs.</b> Our findings challenge conventional beliefs and suggest that pretraining methods and data quality have more impact than model size. SAFIM thus serves as a foundational platform for future research in effective pretraining strategies for code <b>LLMs.</b> The evaluation toolkit and dataset are available at <a href=https://github.com/gonglinyuan/safim>https://github.com/gonglinyuan/safim</a>, and the leaderboard is available at <a href=https://safimbenchmark.com>https://safimbenchmark.com</a>.</p></p class="citation"></blockquote><h3 id=2339--23292-self-evaluation-of-large-language-model-based-on-glass-box-features-hui-huang-et-al-2024>(23/39 | 23/292) Self-Evaluation of Large Language Model based on Glass-box Features (Hui Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hui Huang, Yingqi Qu, Jing Liu, Muyun Yang, Tiejun Zhao. (2024)<br><strong>Self-Evaluation of Large Language Model based on Glass-box Features</strong><br><button class=copy-to-clipboard title="Self-Evaluation of Large Language Model based on Glass-box Features" index=23>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-23 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04222v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04222v1.pdf filename=2403.04222v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The proliferation of open-source <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> underscores the pressing need for evaluation methods. Existing works primarily rely on external evaluators, focusing on training and <b>prompting</b> strategies. However, a crucial aspect - model-aware glass-box features - is overlooked. In this study, we explore the utility of glass-box features under the scenario of self-evaluation, namely applying an <b>LLM</b> to evaluate its own output. We investigate various glass-box feature groups and discovered that the softmax distribution serves as a reliable indicator for quality evaluation. Furthermore, we propose two strategies to enhance the evaluation by incorporating features derived from references. Experimental results on public <b>benchmarks</b> validate the feasibility of self-evaluation of <b>LLMs</b> using glass-box features.</p></p class="citation"></blockquote><h3 id=2439--24292-large-language-models-are-in-context-molecule-learners-jiatong-li-et-al-2024>(24/39 | 24/292) Large Language Models are In-Context Molecule Learners (Jiatong Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiatong Li, Wei Liu, Zhihao Ding, Wenqi Fan, Yuqiang Li, Qing Li. (2024)<br><strong>Large Language Models are In-Context Molecule Learners</strong><br><button class=copy-to-clipboard title="Large Language Models are In-Context Molecule Learners" index=24>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-24 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 33<br>Keywords: Graph, In-context Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04197v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04197v1.pdf filename=2403.04197v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have demonstrated exceptional performance in biochemical tasks, especially the molecule caption translation task, which aims to bridge the gap between molecules and natural language texts. However, previous methods in adapting <b>LLMs</b> to the molecule-caption translation task required extra domain-specific pre-training stages, suffered weak alignment between molecular and textual spaces, or imposed stringent demands on the scale of <b>LLMs.</b> To resolve the challenges, we propose <b>In-Context</b> Molecule Adaptation (ICMA), as a new paradigm allowing <b>LLMs</b> to learn the molecule-text alignment from context examples via <b>In-Context</b> Molecule Tuning. Specifically, ICMA incorporates the following three stages: Cross-modal Retrieval, Post-retrieval Re-ranking, and <b>In-context</b> Molecule Tuning. Initially, Cross-modal Retrieval utilizes BM25 Caption Retrieval and Molecule <b>Graph</b> Retrieval to retrieve informative context examples. Additionally, we also propose Post-retrieval Re-ranking with Sequence Reversal and Random Walk to further improve the quality of retrieval results. Finally, <b>In-Context</b> Molecule Tuning unlocks the <b>in-context</b> molecule learning capability of <b>LLMs</b> with retrieved examples and adapts the parameters of <b>LLMs</b> for the molecule-caption translation task. Experimental results demonstrate that ICMT can empower <b>LLMs</b> to achieve state-of-the-art or comparable performance without extra training corpora and intricate structures, showing that <b>LLMs</b> are inherently <b>in-context</b> molecule learners.</p></p class="citation"></blockquote><h3 id=2539--25292-code-mixed-probes-show-how-pre-trained-models-generalise-on-code-switched-text-frances-a-laureano-de-leon-et-al-2024>(25/39 | 25/292) Code-Mixed Probes Show How Pre-Trained Models Generalise On Code-Switched Text (Frances A. Laureano De Leon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Frances A. Laureano De Leon, Harish Tayyar Madabushi, Mark Lee. (2024)<br><strong>Code-Mixed Probes Show How Pre-Trained Models Generalise On Code-Switched Text</strong><br><button class=copy-to-clipboard title="Code-Mixed Probes Show How Pre-Trained Models Generalise On Code-Switched Text" index=25>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-25 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Stemming, Pre-trained Language Model, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04872v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04872v1.pdf filename=2403.04872v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Code-switching is a prevalent linguistic phenomenon in which multilingual individuals seamlessly alternate between languages. Despite its widespread use online and recent research trends in this area, research in code-switching presents unique challenges, primarily <b>stemming</b> from the scarcity of labelled data and available resources. In this study we investigate how <b>pre-trained</b> <b>Language</b> <b>Models</b> handle code-switched text in three dimensions: a) the ability of <b>PLMs</b> to detect code-switched text, b) variations in the structural information that <b>PLMs</b> utilise to capture code-switched text, and c) the consistency of semantic information representation in code-switched text. To conduct a systematic and controlled evaluation of the language models in question, we create a novel dataset of well-formed naturalistic code-switched text along with parallel translations into the source languages. Our findings reveal that <b>pre-trained</b> <b>language</b> <b>models</b> are effective in generalising to code-switched text, shedding light on the abilities of these models to generalise representations to CS corpora. We release all our code and data including the novel corpus at <a href=https://github.com/francesita/code-mixed-probes>https://github.com/francesita/code-mixed-probes</a>.</p></p class="citation"></blockquote><h3 id=2639--26292-acceleron-a-tool-to-accelerate-research-ideation-harshit-nigam-et-al-2024>(26/39 | 26/292) Acceleron: A Tool to Accelerate Research Ideation (Harshit Nigam et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Harshit Nigam, Manasi Patwardhan, Lovekesh Vig, Gautam Shroff. (2024)<br><strong>Acceleron: A Tool to Accelerate Research Ideation</strong><br><button class=copy-to-clipboard title="Acceleron: A Tool to Accelerate Research Ideation" index=26>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-26 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04382v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04382v1.pdf filename=2403.04382v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Several tools have recently been proposed for assisting researchers during various stages of the research life-cycle. However, these primarily concentrate on tasks such as retrieving and recommending relevant literature, reviewing and critiquing the draft, and writing of research manuscripts. Our investigation reveals a significant gap in availability of tools specifically designed to assist researchers during the challenging ideation phase of the research life-cycle. To aid with research ideation, we propose `Acceleron&rsquo;, a research accelerator for different phases of the research life cycle, and which is specially designed to aid the ideation process. Acceleron guides researchers through the formulation of a comprehensive research proposal, encompassing a novel research problem. The proposals motivation is validated for novelty by identifying gaps in the existing literature and suggesting a plausible list of techniques to solve the proposed problem. We leverage the <b>reasoning</b> and domain-specific skills of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> to create an agent-based architecture incorporating colleague and mentor personas for <b>LLMs.</b> The <b>LLM</b> agents emulate the ideation process undertaken by researchers, engaging researchers in an interactive fashion to aid in the development of the research proposal. Notably, our tool addresses challenges inherent in <b>LLMs,</b> such as hallucinations, implements a two-stage aspect-based retrieval to manage precision-recall trade-offs, and tackles issues of unanswerability. As evaluation, we illustrate the execution of our motivation validation and method synthesis workflows on proposals from the ML and NLP domain, given by 3 distinct researchers. Our observations and evaluations provided by the researchers illustrate the efficacy of the tool in terms of assisting researchers with appropriate inputs at distinct stages and thus leading to improved time efficiency.</p></p class="citation"></blockquote><h3 id=2739--27292-aligners-decoupling-llms-and-alignment-lilian-ngweta-et-al-2024>(27/39 | 27/292) Aligners: Decoupling LLMs and Alignment (Lilian Ngweta et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lilian Ngweta, Mayank Agarwal, Subha Maity, Alex Gittens, Yuekai Sun, Mikhail Yurochkin. (2024)<br><strong>Aligners: Decoupling LLMs and Alignment</strong><br><button class=copy-to-clipboard title="Aligners: Decoupling LLMs and Alignment" index=27>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-27 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 30<br>Keywords: Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04224v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04224v2.pdf filename=2403.04224v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> need to be aligned with human expectations to ensure their safety and utility in most applications. Alignment is challenging, costly, and needs to be repeated for every <b>LLM</b> and alignment criterion. We propose to decouple <b>LLMs</b> and alignment by training aligner models that can be used to align any <b>LLM</b> for a given criteria on an as-needed basis, thus also reducing the potential negative impacts of alignment on performance. Our recipe for training the aligner models solely relies on synthetic data generated with a <b>(prompted)</b> <b>LLM</b> and can be easily adjusted for a variety of alignment criteria. We illustrate our method by training an &ldquo;ethical&rdquo; aligner and verify its efficacy empirically.</p></p class="citation"></blockquote><h3 id=2839--28292-common-7b-language-models-already-possess-strong-math-capabilities-chen-li-et-al-2024>(28/39 | 28/292) Common 7B Language Models Already Possess Strong Math Capabilities (Chen Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chen Li, Weiqi Wang, Jingcheng Hu, Yixuan Wei, Nanning Zheng, Han Hu, Zheng Zhang, Houwen Peng. (2024)<br><strong>Common 7B Language Models Already Possess Strong Math Capabilities</strong><br><button class=copy-to-clipboard title="Common 7B Language Models Already Possess Strong Math Capabilities" index=28>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-28 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 23<br>Keywords: Benchmarking, LLaMA, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04706v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04706v1.pdf filename=2403.04706v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Mathematical capabilities were previously believed to emerge in common language models only at a very large scale or require extensive math-related pre-training. This paper shows that the <b>LLaMA-2</b> 7B model with common pre-training already exhibits strong mathematical abilities, as evidenced by its impressive accuracy of 97.7% and 72.0% on the GSM8K and MATH <b>benchmarks,</b> respectively, when selecting the best response from 256 random generations. The primary issue with the current base model is the difficulty in consistently eliciting its inherent mathematical capabilities. Notably, the accuracy for the first answer drops to 49.5% and 7.9% on the GSM8K and MATH <b>benchmarks,</b> respectively. We find that simply scaling up the SFT data can significantly enhance the reliability of generating correct answers. However, the potential for extensive scaling is constrained by the scarcity of publicly available math questions. To overcome this limitation, we employ synthetic data, which proves to be nearly as effective as real data and shows no clear saturation when scaled up to approximately one million samples. This straightforward approach achieves an accuracy of 82.6% on GSM8K and 40.6% on MATH using <b>LLaMA-2</b> 7B models, surpassing previous models by 14.2% and 20.8%, respectively. We also provide insights into scaling behaviors across different <b>reasoning</b> complexities and error types.</p></p class="citation"></blockquote><h3 id=2939--29292-chain-of-thought-explanation-for-dialogue-state-tracking-lin-xu-et-al-2024>(29/39 | 29/292) Chain of Thought Explanation for Dialogue State Tracking (Lin Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lin Xu, Ningxin Peng, Daquan Zhou, See-Kiong Ng, Jinlan Fu. (2024)<br><strong>Chain of Thought Explanation for Dialogue State Tracking</strong><br><button class=copy-to-clipboard title="Chain of Thought Explanation for Dialogue State Tracking" index=29>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-29 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 23<br>Keywords: Benchmarking, Dialogue State Tracking, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04656v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04656v2.pdf filename=2403.04656v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Dialogue</b> <b>state</b> <b>tracking</b> (DST) aims to record user queries and goals during a conversational interaction achieved by maintaining a predefined set of slots and their corresponding values. Current approaches decide slot values opaquely, while humans usually adopt a more deliberate approach by collecting information from relevant <b>dialogue</b> <b>turns</b> <b>and</b> then <b>reasoning</b> the appropriate values. In this work, we focus on the steps needed to figure out slot values by proposing a model named Chain-of-Thought-Explanation (CoTE) for the DST task. CoTE, which is built on the generative DST framework, is designed to create detailed explanations step by step after determining the slot values. This process leads to more accurate and reliable slot values. More-over, to improve the <b>reasoning</b> ability of the CoTE, we further construct more fluent and high-quality explanations with automatic paraphrasing, leading the method CoTE-refined. Experimental results on three widely recognized DST <b>benchmarks-MultiWOZ</b> 2.2, WoZ 2.0, and M2M-demonstrate the remarkable effectiveness of the CoTE. Furthermore, through a meticulous fine-grained analysis, we observe significant benefits of our CoTE on samples characterized by longer <b>dialogue</b> <b>turns,</b> <b>user</b> responses, and <b>reasoning</b> steps.</p></p class="citation"></blockquote><h3 id=3039--30292-do-large-language-model-understand-multi-intent-spoken-language--shangjian-yin-et-al-2024>(30/39 | 30/292) Do Large Language Model Understand Multi-Intent Spoken Language ? (Shangjian Yin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shangjian Yin, Peijie Huang, Yuhong Xu, Haojing Huang, Jiatian Chen. (2024)<br><strong>Do Large Language Model Understand Multi-Intent Spoken Language ?</strong><br><button class=copy-to-clipboard title="Do Large Language Model Understand Multi-Intent Spoken Language ?" index=30>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-30 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 23<br>Keywords: Benchmarking, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04481v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04481v2.pdf filename=2403.04481v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study marks a significant advancement by harnessing <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> for multi-intent spoken language understanding (SLU), proposing a unique methodology that capitalizes on the generative power of <b>LLMs</b> within an SLU context. Our innovative technique reconfigures entity slots specifically for <b>LLM</b> application in multi-intent SLU environments and introduces the concept of Sub-Intent Instruction (SII), enhancing the dissection and interpretation of intricate, multi-intent communication within varied domains. The resultant datasets, dubbed LM-MixATIS and LM-MixSNIPS, are crafted from pre-existing <b>benchmarks.</b> Our research illustrates that <b>LLMs</b> can match and potentially excel beyond the capabilities of current state-of-the-art multi-intent SLU models. It further explores <b>LLM</b> efficacy across various intent configurations and dataset proportions. Moreover, we introduce two pioneering metrics, Entity Slot Accuracy (ESA) and Combined Semantic Accuracy (CSA), to provide an in-depth analysis of <b>LLM</b> proficiency in this complex field.</p></p class="citation"></blockquote><h3 id=3139--31292-metric-aware-llm-inference-michal-lukasik-et-al-2024>(31/39 | 31/292) Metric-aware LLM inference (Michal Lukasik et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michal Lukasik, Harikrishna Narasimhan, Aditya Krishna Menon, Felix Yu, Sanjiv Kumar. (2024)<br><strong>Metric-aware LLM inference</strong><br><button class=copy-to-clipboard title="Metric-aware LLM inference" index=31>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-31 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 23<br>Keywords: Benchmarking, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04182v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04182v1.pdf filename=2403.04182v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have demonstrated strong results on a range of NLP tasks. Typically, outputs are obtained via autoregressive sampling from the <b>LLM&rsquo;s</b> underlying distribution. We show that this inference strategy can be suboptimal for a range of tasks and associated evaluation metrics. As a remedy, we propose metric aware <b>LLM</b> inference: a decision theoretic approach optimizing for custom metrics at inference time. We report improvements over baselines on academic <b>benchmarks</b> and publicly available models.</p></p class="citation"></blockquote><h3 id=3239--32292-evaluating-biases-in-context-dependent-health-questions-sharon-levy-et-al-2024>(32/39 | 32/292) Evaluating Biases in Context-Dependent Health Questions (Sharon Levy et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sharon Levy, Tahilin Sanchez Karver, William D. Adler, Michelle R. Kaufman, Mark Dredze. (2024)<br><strong>Evaluating Biases in Context-Dependent Health Questions</strong><br><button class=copy-to-clipboard title="Evaluating Biases in Context-Dependent Health Questions" index=32>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-32 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: In-context Learning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04858v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04858v1.pdf filename=2403.04858v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Chat-based <b>large</b> <b>language</b> <b>models</b> have the opportunity to empower individuals lacking high-quality healthcare access to receive personalized information across a variety of topics. However, users may ask underspecified questions that require additional context for a model to correctly answer. We study how <b>large</b> <b>language</b> <b>model</b> biases are exhibited through these contextual questions in the healthcare domain. To accomplish this, we curate a dataset of sexual and reproductive healthcare questions that are dependent on age, sex, and location attributes. We compare models&rsquo; outputs with and without demographic context to determine group alignment among our contextual questions. Our experiments reveal biases in each of these attributes, where young adult female users are favored.</p></p class="citation"></blockquote><h3 id=3339--33292-classist-tools-social-class-correlates-with-performance-in-nlp-amanda-cercas-curry-et-al-2024>(33/39 | 33/292) Classist Tools: Social Class Correlates with Performance in NLP (Amanda Cercas Curry et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amanda Cercas Curry, Giuseppe Attanasio, Zeerak Talat, Dirk Hovy. (2024)<br><strong>Classist Tools: Social Class Correlates with Performance in NLP</strong><br><button class=copy-to-clipboard title="Classist Tools: Social Class Correlates with Performance in NLP" index=33>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-33 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Automatic Speech Recognition, Automatic Speech Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04445v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04445v1.pdf filename=2403.04445v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Since the foundational work of William Labov on the social stratification of language (Labov, 1964), linguistics has made concentrated efforts to explore the links between sociodemographic characteristics and language production and perception. But while there is strong evidence for socio-demographic characteristics in language, they are infrequently used in Natural Language Processing (NLP). Age and gender are somewhat well represented, but Labov&rsquo;s original target, socioeconomic status, is noticeably absent. And yet it matters. We show empirically that NLP disadvantages less-privileged socioeconomic groups. We annotate a corpus of 95K utterances from movies with social class, ethnicity and geographical language variety and measure the performance of NLP systems on three tasks: language modelling, <b>automatic</b> <b>speech</b> <b>recognition,</b> and grammar error correction. We find significant performance disparities that can be attributed to socioeconomic status as well as ethnicity and geographical differences. With NLP technologies becoming ever more ubiquitous and quotidian, they must accommodate all language varieties to avoid disadvantaging already marginalised groups. We argue for the inclusion of socioeconomic class in future language technologies.</p></p class="citation"></blockquote><h3 id=3439--34292-promising-and-worth-to-try-future-directions-for-advancing-state-of-the-art-surrogates-methods-of-agent-based-models-in-social-and-health-computational-sciences-atiyah-elsheikh-2024>(34/39 | 34/292) Promising and worth-to-try future directions for advancing state-of-the-art surrogates methods of agent-based models in social and health computational sciences (Atiyah Elsheikh, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Atiyah Elsheikh. (2024)<br><strong>Promising and worth-to-try future directions for advancing state-of-the-art surrogates methods of agent-based models in social and health computational sciences</strong><br><button class=copy-to-clipboard title="Promising and worth-to-try future directions for advancing state-of-the-art surrogates methods of agent-based models in social and health computational sciences" index=34>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-34 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-SY, cs.CL, eess-SY, math-DS<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04417v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04417v1.pdf filename=2403.04417v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The execution and runtime performance of model-based analysis tools for realistic large-scale ABMs (Agent-Based Models) can be excessively long. This due to the computational demand exponentially proportional to the model size (e.g. Population size) and the number of model parameters. Even the runtime of a single <b>simulation</b> of a realistic ABM may demand huge computational resources when attempting to employ realistic population size. The main aim of this ad-hoc brief report is to highlight some of surrogate models that were adequate and computationally less demanding for nonlinear dynamical models in various modeling application areas.To the author knowledge, these methods have been not, at least extensively, employed for ABMs within the field of (SHCS) Social Health Computational Sciences, yet. Thus, they might be, but not necessarily, useful in progressing state of the art for establishing surrogate models for ABMs in the field of SHCS.</p></p class="citation"></blockquote><h3 id=3539--35292-measuring-meaning-composition-in-the-human-brain-with-composition-scores-from-large-language-models-changjiang-gao-et-al-2024>(35/39 | 35/292) Measuring Meaning Composition in the Human Brain with Composition Scores from Large Language Models (Changjiang Gao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Changjiang Gao, Jixing Li, Jiajun Chen, Shujian Huang. (2024)<br><strong>Measuring Meaning Composition in the Human Brain with Composition Scores from Large Language Models</strong><br><button class=copy-to-clipboard title="Measuring Meaning Composition in the Human Brain with Composition Scores from Large Language Models" index=35>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-35 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Transformer, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04325v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04325v1.pdf filename=2403.04325v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The process of meaning composition, wherein smaller units like morphemes or words combine to form the meaning of phrases and sentences, is essential for human sentence comprehension. Despite extensive neurolinguistic research into the brain regions involved in meaning composition, a computational metric to quantify the extent of composition is still lacking. Drawing on the key-value memory interpretation of <b>transformer</b> feed-forward network blocks, we introduce the Composition Score, a novel model-based metric designed to quantify the degree of meaning composition during sentence comprehension. Experimental findings show that this metric correlates with brain clusters associated with word frequency, structural processing, and general sensitivity to words, suggesting the multifaceted nature of meaning composition during human sentence comprehension.</p></p class="citation"></blockquote><h3 id=3639--36292-macms-magahi-code-mixed-dataset-for-sentiment-analysis-priya-rani-et-al-2024>(36/39 | 36/292) MaCmS: Magahi Code-mixed Dataset for Sentiment Analysis (Priya Rani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Priya Rani, Gaurav Negi, Theodorus Fransen, John P. McCrae. (2024)<br><strong>MaCmS: Magahi Code-mixed Dataset for Sentiment Analysis</strong><br><button class=copy-to-clipboard title="MaCmS: Magahi Code-mixed Dataset for Sentiment Analysis" index=36>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-36 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Sentiment Analysis<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04639v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04639v1.pdf filename=2403.04639v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The present paper introduces new <b>sentiment</b> <b>data,</b> MaCMS, for Magahi-Hindi-English (MHE) code-mixed language, where Magahi is a less-resourced minority language. This dataset is the first Magahi-Hindi-English code-mixed dataset for <b>sentiment</b> <b>analysis</b> tasks. Further, we also provide a linguistics analysis of the dataset to understand the structure of code-mixing and a statistical study to understand the language preferences of speakers with different polarities. With these analyses, we also train baseline models to evaluate the dataset&rsquo;s quality.</p></p class="citation"></blockquote><h3 id=3739--37292-computational-modelling-of-plurality-and-definiteness-in-chinese-noun-phrases-yuqi-liu-et-al-2024>(37/39 | 37/292) Computational Modelling of Plurality and Definiteness in Chinese Noun Phrases (Yuqi Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuqi Liu, Guanyi Chen, Kees van Deemter. (2024)<br><strong>Computational Modelling of Plurality and Definiteness in Chinese Noun Phrases</strong><br><button class=copy-to-clipboard title="Computational Modelling of Plurality and Definiteness in Chinese Noun Phrases" index=37>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-37 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04376v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04376v1.pdf filename=2403.04376v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Theoretical linguists have suggested that some languages (e.g., Chinese and Japanese) are &ldquo;cooler&rdquo; than other languages based on the observation that the intended meaning of phrases in these languages depends more on their contexts. As a result, many expressions in these languages are shortened, and their meaning is inferred from the context. In this paper, we focus on the omission of the plurality and definiteness markers in Chinese noun phrases (NPs) to investigate the predictability of their intended meaning given the contexts. To this end, we built a corpus of Chinese NPs, each of which is accompanied by its corresponding context, and by labels indicating its singularity/plurality and definiteness/indefiniteness. We carried out corpus assessments and analyses. The results suggest that Chinese speakers indeed drop plurality and definiteness markers very frequently. Building on the corpus, we train a bank of computational models using both classic machine learning models and state-of-the-art <b>pre-trained</b> <b>language</b> <b>models</b> to predict the plurality and definiteness of each NP. We report on the performance of these models and analyse their behaviours.</p></p class="citation"></blockquote><h3 id=3839--38292-persona-extraction-through-semantic-similarity-for-emotional-support-conversation-generation-seunghee-han-et-al-2024>(38/39 | 38/292) Persona Extraction Through Semantic Similarity for Emotional Support Conversation Generation (Seunghee Han et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Seunghee Han, Se Jin Park, Chae Won Kim, Yong Man Ro. (2024)<br><strong>Persona Extraction Through Semantic Similarity for Emotional Support Conversation Generation</strong><br><button class=copy-to-clipboard title="Persona Extraction Through Semantic Similarity for Emotional Support Conversation Generation" index=38>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-38 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Dialogue System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04212v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04212v1.pdf filename=2403.04212v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Providing emotional support through <b>dialogue</b> <b>systems</b> is becoming increasingly important in today&rsquo;s world, as it can support both mental health and social interactions in many conversation scenarios. Previous works have shown that using persona is effective for generating empathetic and supportive responses. They have often relied on pre-provided persona rather than inferring them during conversations. However, it is not always possible to obtain a user persona before the conversation begins. To address this challenge, we propose PESS (Persona Extraction through Semantic Similarity), a novel framework that can automatically infer informative and consistent persona from <b>dialogues.</b> <b>We</b> devise completeness loss and consistency loss based on semantic similarity scores. The completeness loss encourages the model to generate missing persona information, and the consistency loss guides the model to distinguish between consistent and inconsistent persona. Our experimental results demonstrate that high-quality persona information inferred by PESS is effective in generating emotionally supportive responses.</p></p class="citation"></blockquote><h3 id=3939--39292-da-net-a-disentangled-and-adaptive-network-for-multi-source-cross-lingual-transfer-learning-ling-ge-et-al-2024>(39/39 | 39/292) DA-Net: A Disentangled and Adaptive Network for Multi-Source Cross-Lingual Transfer Learning (Ling Ge et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ling Ge, Chunming Hu, Guanghui Ma, Jihong Liu, Hong Zhang. (2024)<br><strong>DA-Net: A Disentangled and Adaptive Network for Multi-Source Cross-Lingual Transfer Learning</strong><br><button class=copy-to-clipboard title="DA-Net: A Disentangled and Adaptive Network for Multi-Source Cross-Lingual Transfer Learning" index=39>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-39 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04158v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04158v1.pdf filename=2403.04158v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multi-Source cross-lingual <b>transfer</b> <b>learning</b> deals with the <b>transfer</b> <b>of</b> task knowledge from multiple labelled source languages to an unlabeled target language under the language shift. Existing methods typically focus on weighting the predictions produced by language-specific classifiers of different sources that follow a shared encoder. However, all source languages share the same encoder, which is updated by all these languages. The extracted representations inevitably contain different source languages&rsquo; information, which may disturb the learning of the language-specific classifiers. Additionally, due to the language gap, language-specific classifiers trained with source labels are unable to make accurate predictions for the target language. Both facts impair the model&rsquo;s performance. To address these challenges, we propose a Disentangled and Adaptive Network (DA-Net). Firstly, we devise a feedback-guided collaborative disentanglement method that seeks to purify input representations of classifiers, thereby mitigating mutual interference from multiple sources. Secondly, we propose a class-aware parallel adaptation method that aligns class-level distributions for each source-target language pair, thereby alleviating the language pairs&rsquo; language gap. Experimental results on three different tasks involving 38 languages validate the effectiveness of our approach.</p></p class="citation"></blockquote><h2 id=cscv-58>cs.CV (58)</h2><h3 id=158--40292-masked-capsule-autoencoders-miles-everett-et-al-2024>(1/58 | 40/292) Masked Capsule Autoencoders (Miles Everett et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Miles Everett, Mingjun Zhong, Georgios Leontidis. (2024)<br><strong>Masked Capsule Autoencoders</strong><br><button class=copy-to-clipboard title="Masked Capsule Autoencoders" index=40>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-40 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 110<br>Keywords: Vision Transformer, Autoencoder, Convolution, Convolutional Neural Network, Convolutional Neural Network, Fine-tuning, Self-supervised Learning, Self-supervised Pre-training, Supervised Learning, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04724v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04724v1.pdf filename=2403.04724v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose Masked Capsule <b>Autoencoders</b> (MCAE), the first Capsule Network that utilises pretraining in a <b>self-supervised</b> <b>manner.</b> Capsule Networks have emerged as a powerful alternative to <b>Convolutional</b> <b>Neural</b> <b>Networks</b> <b>(CNNs),</b> and have shown favourable properties when compared to <b>Vision</b> <b>Transformers</b> (ViT), but have struggled to effectively learn when presented with more complex data, leading to Capsule Network models that do not scale to modern tasks. Our proposed MCAE model alleviates this issue by reformulating the Capsule Network to use masked image modelling as a pretraining stage before <b>finetuning</b> in a <b>supervised</b> manner. Across several experiments and ablations studies we demonstrate that similarly to <b>CNNs</b> and ViTs, Capsule Networks can also benefit from <b>self-supervised</b> <b>pretraining,</b> paving the way for further advancements in this neural network domain. For instance, pretraining on the Imagenette dataset, a dataset of 10 classes of Imagenet-sized images, we achieve not only state-of-the-art results for Capsule Networks but also a 9% improvement compared to purely <b>supervised</b> training. Thus we propose that Capsule Networks benefit from and should be trained within a masked image modelling framework, with a novel capsule decoder, to improve a Capsule Network&rsquo;s performance on realistic-sized images.</p></p class="citation"></blockquote><h3 id=258--41292-acc-vit--atrous-convolutions-comeback-in-vision-transformers-nabil-ibtehaz-et-al-2024>(2/58 | 41/292) ACC-ViT : Atrous Convolution&rsquo;s Comeback in Vision Transformers (Nabil Ibtehaz et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nabil Ibtehaz, Ning Yan, Masood Mortazavi, Daisuke Kihara. (2024)<br><strong>ACC-ViT : Atrous Convolution&rsquo;s Comeback in Vision Transformers</strong><br><button class=copy-to-clipboard title="ACC-ViT : Atrous Convolution's Comeback in Vision Transformers" index=41>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-41 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 90<br>Keywords: Vision Transformer, Object Detection, Contrastive Learning, Convolution, Fine-tuning, Zero-shot, Transformer, Vision Transformer, Zero-shot Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04200v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04200v1.pdf filename=2403.04200v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Transformers</b> have elevated to the state-of-the-art <b>vision</b> <b>architectures</b> through innovations in attention mechanism inspired from visual perception. At present two classes of attentions prevail in <b>vision</b> <b>transformers,</b> regional and sparse attention. The former bounds the pixel interactions within a region; the latter spreads them across sparse grids. The opposing natures of them have resulted in a dilemma between either preserving hierarchical relation or attaining a global context. In this work, taking inspiration from atrous <b>convolution,</b> we introduce Atrous Attention, a fusion of regional and sparse attention, which can adaptively consolidate both local and global information, while maintaining hierarchical relations. As a further tribute to atrous <b>convolution,</b> we redesign the ubiquitous inverted residual <b>convolution</b> blocks with atrous <b>convolution.</b> Finally, we propose a generalized, hybrid <b>vision</b> <b>transformer</b> backbone, named ACC-ViT, following conventional practices for standard <b>vision</b> <b>tasks.</b> Our tiny version model achieves $\sim 84 %$ accuracy on ImageNet-1K, with less than $28.5$ million parameters, which is $0.42%$ improvement over state-of-the-art MaxViT while having $8.4%$ less parameters. In addition, we have investigated the efficacy of ACC-ViT backbone under different evaluation settings, such as <b>finetuning,</b> linear probing, and <b>zero-shot</b> <b>learning</b> on tasks involving medical image analysis, <b>object</b> <b>detection,</b> and language-image <b>contrastive</b> <b>learning.</b> ACC-ViT is therefore a strong <b>vision</b> <b>backbone,</b> which is also competitive in mobile-scale versions, ideal for niche applications with small datasets.</p></p class="citation"></blockquote><h3 id=358--42292-effectiveness-assessment-of-recent-large-vision-language-models-yao-jiang-et-al-2024>(3/58 | 42/292) Effectiveness Assessment of Recent Large Vision-Language Models (Yao Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yao Jiang, Xinyu Yan, Ge-Peng Ji, Keren Fu, Meijun Sun, Huan Xiong, Deng-Ping Fan, Fahad Shahbaz Khan. (2024)<br><strong>Effectiveness Assessment of Recent Large Vision-Language Models</strong><br><button class=copy-to-clipboard title="Effectiveness Assessment of Recent Large Vision-Language Models" index=42>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-42 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 73<br>Keywords: Object Detection, Anomaly Detection, Multi-modal, GPT, Question Answering, Reasoning, Text2image, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04306v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04306v1.pdf filename=2403.04306v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The advent of large <b>vision-language</b> models (LVLMs) represents a noteworthy advancement towards the pursuit of artificial general intelligence. However, the extent of their efficacy across both specialized and general tasks warrants further investigation. This article endeavors to evaluate the competency of popular LVLMs in specialized and general tasks, respectively, aiming to offer a comprehensive comprehension of these innovative methodologies. To gauge their efficacy in specialized tasks, we tailor a comprehensive testbed comprising three distinct scenarios: natural, healthcare, and industrial, encompassing six challenging tasks. These tasks include salient, camouflaged, and transparent <b>object</b> <b>detection,</b> as well as polyp and skin lesion detection, alongside industrial <b>anomaly</b> <b>detection.</b> We examine the performance of three recent open-source LVLMs &ndash; MiniGPT-v2, LLaVA-1.5, and Shikra &ndash; in the realm of visual recognition and localization. Moreover, we conduct empirical investigations utilizing the aforementioned models alongside <b>GPT-4V,</b> assessing their <b>multi-modal</b> understanding capacities in general tasks such as <b>object</b> <b>counting,</b> absurd <b>question</b> <b>answering,</b> affordance <b>reasoning,</b> attribute recognition, and spatial relation <b>reasoning.</b> Our investigations reveal that these models demonstrate limited proficiency not only in specialized tasks but also in general tasks. We delve deeper into this inadequacy and suggest several potential factors, including limited cognition in specialized tasks, <b>object</b> <b>hallucination,</b> <b>text-to-image</b> interference, and decreased robustness in complex problems. We hope this study would provide valuable insights for the future development of LVLMs, augmenting their power in coping with both general and specialized applications.</p></p class="citation"></blockquote><h3 id=458--43292-textmonkey-an-ocr-free-large-multimodal-model-for-understanding-document-yuliang-liu-et-al-2024>(4/58 | 43/292) TextMonkey: An OCR-Free Large Multimodal Model for Understanding Document (Yuliang Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuliang Liu, Biao Yang, Qiang Liu, Zhang Li, Zhiyin Ma, Shuo Zhang, Xiang Bai. (2024)<br><strong>TextMonkey: An OCR-Free Large Multimodal Model for Understanding Document</strong><br><button class=copy-to-clipboard title="TextMonkey: An OCR-Free Large Multimodal Model for Understanding Document" index=43>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-43 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 69<br>Keywords: Optical Character Recognition, Benchmarking, Fine-tuning, Multi-modal, Multi-modal, Grounding, Question Answering, Text Analysis, Visual Question Answering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04473v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04473v1.pdf filename=2403.04473v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present TextMonkey, a large <b>multimodal</b> model (LMM) tailored for <b>text-centric</b> <b>tasks,</b> including document <b>question</b> <b>answering</b> (DocVQA) and scene <b>text</b> <b>analysis.</b> Our approach introduces enhancement across several dimensions: by adopting Shifted Window Attention with zero-initialization, we achieve cross-window connectivity at higher input resolutions and stabilize early training; We hypothesize that images may contain redundant tokens, and by using similarity to filter out significant tokens, we can not only streamline the token length but also enhance the model&rsquo;s performance. Moreover, by expanding our model&rsquo;s capabilities to encompass <b>text</b> <b>spotting</b> and <b>grounding,</b> and incorporating positional information into responses, we enhance interpretability and minimize hallucinations. Additionally, TextMonkey can be <b>finetuned</b> to gain the ability to comprehend commands for clicking screenshots. Overall, our method notably boosts performance across various <b>benchmark</b> datasets, achieving increases of 5.2%, 6.9%, and 2.8% in Scene <b>Text-Centric</b> <b>VQA,</b> Document Oriented <b>VQA,</b> and KIE, respectively, especially with a score of 561 on OCRBench, surpassing prior open-sourced large <b>multimodal</b> models for document understanding. Code will be released at <a href=https://github.com/Yuliang-Liu/Monkey>https://github.com/Yuliang-Liu/Monkey</a>.</p></p class="citation"></blockquote><h3 id=558--44292-t-tame-trainable-attention-mechanism-for-explaining-convolutional-networks-and-vision-transformers-mariano-v-ntrougkas-et-al-2024>(5/58 | 44/292) T-TAME: Trainable Attention Mechanism for Explaining Convolutional Networks and Vision Transformers (Mariano V. Ntrougkas et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mariano V. Ntrougkas, Nikolaos Gkalelis, Vasileios Mezaris. (2024)<br><strong>T-TAME: Trainable Attention Mechanism for Explaining Convolutional Networks and Vision Transformers</strong><br><button class=copy-to-clipboard title="T-TAME: Trainable Attention Mechanism for Explaining Convolutional Networks and Vision Transformers" index=44>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-44 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs-MM, cs.CV<br>Keyword Score: 65<br>Keywords: Vision Transformer, Black Box, Convolution, Convolutional Neural Network, Convolutional Neural Network, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04523v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04523v1.pdf filename=2403.04523v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The development and adoption of <b>Vision</b> <b>Transformers</b> and other deep-learning architectures for image classification tasks has been rapid. However, the <b>&ldquo;black</b> <b>box&rdquo;</b> nature of neural networks is a barrier to adoption in applications where explainability is essential. While some techniques for generating explanations have been proposed, primarily for <b>Convolutional</b> <b>Neural</b> <b>Networks,</b> adapting such techniques to the new paradigm of <b>Vision</b> <b>Transformers</b> is non-trivial. This paper presents T-TAME, <b>Transformer-compatible</b> Trainable Attention Mechanism for Explanations, a general methodology for explaining deep neural networks used in image classification tasks. The proposed architecture and training technique can be easily applied to any <b>convolutional</b> <b>or</b> <b>Vision</b> <b>Transformer-like</b> neural network, using a streamlined training approach. After training, explanation maps can be computed in a single forward pass; these explanation maps are comparable to or outperform the outputs of computationally expensive perturbation-based explainability techniques, achieving SOTA performance. We apply T-TAME to three popular deep learning classifier architectures, VGG-16, ResNet-50, and ViT-B-16, trained on the ImageNet dataset, and we demonstrate improvements over existing state-of-the-art explainability methods. A detailed analysis of the results and an ablation study provide insights into how the T-TAME design choices affect the quality of the generated explanation maps.</p></p class="citation"></blockquote><h3 id=658--45292-discriminative-probing-and-tuning-for-text-to-image-generation-leigang-qu-et-al-2024>(6/58 | 45/292) Discriminative Probing and Tuning for Text-to-Image Generation (Leigang Qu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Leigang Qu, Wenjie Wang, Yongqi Li, Hanwang Zhang, Liqiang Nie, Tat-Seng Chua. (2024)<br><strong>Discriminative Probing and Tuning for Text-to-Image Generation</strong><br><button class=copy-to-clipboard title="Discriminative Probing and Tuning for Text-to-Image Generation" index=45>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-45 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CL, cs-CV, cs-MM, cs.CV<br>Keyword Score: 63<br>Keywords: Benchmarking, Fine-tuning, Out-of-distribution, Text2image, Text2image, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04321v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04321v1.pdf filename=2403.04321v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite advancements in <b>text-to-image</b> generation (T2I), prior methods often face <b>text-image</b> misalignment problems such as relation confusion in generated images. Existing solutions involve cross-attention manipulation for better compositional understanding or integrating <b>large</b> <b>language</b> <b>models</b> for improved layout planning. However, the inherent alignment capabilities of T2I models are still inadequate. By reviewing the link between generative and discriminative modeling, we posit that T2I models&rsquo; discriminative abilities may reflect their <b>text-image</b> alignment proficiency during generation. In this light, we advocate bolstering the discriminative abilities of T2I models to achieve more precise <b>text-to-image</b> alignment for generation. We present a discriminative adapter built on T2I models to probe their discriminative abilities on two representative tasks and leverage discriminative <b>fine-tuning</b> to improve their <b>text-image</b> alignment. As a bonus of the discriminative adapter, a self-correction mechanism can leverage discriminative gradients to better align generated images to text <b>prompts</b> during inference. Comprehensive evaluations across three <b>benchmark</b> datasets, including both in-distribution and <b>out-of-distribution</b> scenarios, demonstrate our method&rsquo;s superior generation performance. Meanwhile, it achieves state-of-the-art discriminative performance on the two discriminative tasks compared to other generative models.</p></p class="citation"></blockquote><h3 id=758--46292-snapntell-enhancing-entity-centric-visual-question-answering-with-retrieval-augmented-multimodal-llm-jielin-qiu-et-al-2024>(7/58 | 46/292) SnapNTell: Enhancing Entity-Centric Visual Question Answering with Retrieval Augmented Multimodal LLM (Jielin Qiu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jielin Qiu, Andrea Madotto, Zhaojiang Lin, Paul A. Crook, Yifan Ethan Xu, Xin Luna Dong, Christos Faloutsos, Lei Li, Babak Damavandi, Seungwhan Moon. (2024)<br><strong>SnapNTell: Enhancing Entity-Centric Visual Question Answering with Retrieval Augmented Multimodal LLM</strong><br><button class=copy-to-clipboard title="SnapNTell: Enhancing Entity-Centric Visual Question Answering with Retrieval Augmented Multimodal LLM" index=46>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-46 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 59<br>Keywords: Benchmarking, Multi-modal, Multi-modal, Question Answering, Question Answering, Visual Question Answering, Visual Question Answering, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04735v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04735v1.pdf filename=2403.04735v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Vision-extended <b>LLMs</b> have made significant strides in <b>Visual</b> <b>Question</b> <b>Answering</b> <b>(VQA).</b> Despite these advancements, VLLMs still encounter substantial difficulties in handling queries involving long-tail entities, with a tendency to produce erroneous or hallucinated responses. In this work, we introduce a novel evaluative <b>benchmark</b> named \textbf{SnapNTell}, specifically tailored for entity-centric <b>VQA.</b> This task aims to test the models&rsquo; capabilities in identifying entities and providing detailed, entity-specific knowledge. We have developed the \textbf{SnapNTell Dataset}, distinct from traditional <b>VQA</b> datasets: (1) It encompasses a wide range of categorized entities, each represented by images and explicitly named in the answers; (2) It features <b>QA</b> pairs that require extensive knowledge for accurate responses. The dataset is organized into 22 major categories, containing 7,568 unique entities in total. For each entity, we curated 10 illustrative images and crafted 10 knowledge-intensive <b>QA</b> pairs. To address this novel task, we devised a scalable, efficient, and transparent retrieval-augmented <b>multimodal</b> <b>LLM.</b> Our approach markedly outperforms existing methods on the SnapNTell dataset, achieving a 66.5% improvement in the BELURT score. We will soon make the dataset and the source code publicly accessible.</p></p class="citation"></blockquote><h3 id=858--47292-objectcompose-evaluating-resilience-of-vision-based-models-on-object-to-background-compositional-changes-hashmat-shadab-malik-et-al-2024>(8/58 | 47/292) ObjectCompose: Evaluating Resilience of Vision-Based Models on Object-to-Background Compositional Changes (Hashmat Shadab Malik et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hashmat Shadab Malik, Muhammad Huzaifa, Muzammal Naseer, Salman Khan, Fahad Shahbaz Khan. (2024)<br><strong>ObjectCompose: Evaluating Resilience of Vision-Based Models on Object-to-Background Compositional Changes</strong><br><button class=copy-to-clipboard title="ObjectCompose: Evaluating Resilience of Vision-Based Models on Object-to-Background Compositional Changes" index=47>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-47 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 53<br>Keywords: Diffusion Model, Multi-modal, Image2text, Text2image, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04701v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04701v1.pdf filename=2403.04701v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Given the <b>large-scale</b> <b>multi-modal</b> <b>training</b> of recent vision-based models and their generalization capabilities, understanding the extent of their robustness is critical for their real-world deployment. In this work, we evaluate the resilience of current vision-based models against diverse object-to-background context variations. The majority of robustness evaluation methods have introduced synthetic datasets to induce changes to object characteristics (viewpoints, scale, color) or utilized image transformation techniques (adversarial changes, common corruptions) on real images to simulate shifts in distributions. Recent works have explored leveraging <b>large</b> <b>language</b> <b>models</b> and <b>diffusion</b> <b>models</b> to generate changes in the background. However, these methods either lack in offering control over the changes to be made or distort the object semantics, making them unsuitable for the task. Our method, on the other hand, can induce diverse object-to-background changes while preserving the original semantics and appearance of the object. To achieve this goal, we harness the generative capabilities of <b>text-to-image,</b> <b>image-to-text,</b> and image-to-segment models to automatically generate a broad spectrum of object-to-background changes. We induce both natural and adversarial background changes by either modifying the textual <b>prompts</b> or optimizing the latents and textual embedding of <b>text-to-image</b> models. This allows us to quantify the role of background context in understanding the robustness and generalization of deep neural networks. We produce various versions of standard vision datasets (ImageNet, COCO), incorporating either diverse and realistic backgrounds into the images or introducing color, texture, and adversarial changes in the background. We conduct extensive experiment to analyze the robustness of vision-based models against object-to-background context variations across diverse tasks.</p></p class="citation"></blockquote><h3 id=958--48292-afreeca-annotation-free-counting-for-all-adriano-dalessandro-et-al-2024>(9/58 | 48/292) AFreeCA: Annotation-Free Counting for All (Adriano D&rsquo;Alessandro et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Adriano D&rsquo;Alessandro, Ali Mahdavi-Amiri, Ghassan Hamarneh. (2024)<br><strong>AFreeCA: Annotation-Free Counting for All</strong><br><button class=copy-to-clipboard title="AFreeCA: Annotation-Free Counting for All" index=48>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-48 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Diffusion Model, Few-shot, Unsupervised Learning, Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04943v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04943v1.pdf filename=2403.04943v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Object counting methods typically rely on manually annotated datasets. The cost of creating such datasets has restricted the versatility of these networks to count objects from specific classes (such as humans or penguins), and counting objects from diverse categories remains a challenge. The availability of robust <b>text-to-image</b> latent <b>diffusion</b> <b>models</b> (LDMs) raises the question of whether these models can be utilized to generate counting datasets. However, LDMs struggle to create images with an exact number of objects based solely on text <b>prompts</b> but they can be used to offer a dependable \textit{sorting} signal by adding and removing objects within an image. Leveraging this data, we initially introduce an <b>unsupervised</b> sorting methodology to learn object-related features that are subsequently refined and anchored for counting purposes using counting data generated by LDMs. Further, we present a density classifier-guided method for dividing an image into patches containing objects that can be reliably counted. Consequently, we can generate counting data for any type of object and count them in an <b>unsupervised</b> manner. Our approach outperforms other <b>unsupervised</b> and <b>few-shot</b> alternatives and is not restricted to specific object classes for which counting data is available. Code to be released upon acceptance.</p></p class="citation"></blockquote><h3 id=1058--49292-self-adapting-large-visual-language-models-to-edge-devices-across-visual-modalities-kaiwen-cai-et-al-2024>(10/58 | 49/292) Self-Adapting Large Visual-Language Models to Edge Devices across Visual Modalities (Kaiwen Cai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kaiwen Cai, Zhekai Duan, Gaowen Liu, Charles Fleming, Chris Xiaoxuan Lu. (2024)<br><strong>Self-Adapting Large Visual-Language Models to Edge Devices across Visual Modalities</strong><br><button class=copy-to-clipboard title="Self-Adapting Large Visual-Language Models to Edge Devices across Visual Modalities" index=49>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-49 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Contrastive Learning, Knowledge Distillation, Knowledge Distillation, Quantization, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04908v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04908v1.pdf filename=2403.04908v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in <b>Vision-Language</b> (VL) models have sparked interest in their deployment on edge devices, yet challenges in handling diverse visual modalities, manual annotation, and computational constraints remain. We introduce EdgeVL, a novel framework that bridges this gap by seamlessly integrating dual-modality <b>knowledge</b> <b>distillation</b> and <b>quantization-aware</b> <b>contrastive</b> <b>learning.</b> This approach enables the adaptation of large VL models, like CLIP, for efficient use with both RGB and non-RGB images on resource-limited devices without the need for manual annotations. EdgeVL not only transfers visual language alignment capabilities to compact models but also maintains feature quality post-quantization, significantly enhancing open-vocabulary classification performance across various visual modalities. Our work represents the first systematic effort to adapt large VL models for edge deployment, showcasing up to 15.4% accuracy improvements on multiple datasets and up to 93-fold reduction in model size.</p></p class="citation"></blockquote><h3 id=1158--50292-optimizing-retinal-prosthetic-stimuli-with-conditional-invertible-neural-networks-yuli-wu-et-al-2024>(11/58 | 50/292) Optimizing Retinal Prosthetic Stimuli with Conditional Invertible Neural Networks (Yuli Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuli Wu, Julian Wittmann, Peter Walter, Johannes Stegmaier. (2024)<br><strong>Optimizing Retinal Prosthetic Stimuli with Conditional Invertible Neural Networks</strong><br><button class=copy-to-clipboard title="Optimizing Retinal Prosthetic Stimuli with Conditional Invertible Neural Networks" index=50>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-50 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 50<br>Keywords: Convolution, Convolutional Neural Network, Simulation, Simulator, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04884v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04884v1.pdf filename=2403.04884v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Implantable retinal prostheses offer a promising solution to restore partial vision by circumventing damaged photoreceptor cells in the retina and directly stimulating the remaining functional retinal cells. However, the information transmission between the camera and retinal cells is often limited by the low resolution of the electrode array and the lack of specificity for different ganglion cell types, resulting in suboptimal stimulations. In this work, we propose to utilize normalizing flow-based conditional invertible neural networks to optimize retinal implant stimulation in an <b>unsupervised</b> manner. The invertibility of these networks allows us to use them as a surrogate for the computational model of the visual system, while also encoding input camera signals into optimized electrical stimuli on the electrode array. Compared to other methods, such as trivial downsampling, linear models, and feed-forward <b>convolutional</b> <b>neural</b> <b>networks,</b> the flow-based invertible neural network and its conditional extension yield better visual reconstruction qualities w.r.t. various metrics using a physiologically validated <b>simulation</b> tool.</p></p class="citation"></blockquote><h3 id=1258--51292-scalable-and-robust-transformer-decoders-for-interpretable-image-classification-with-foundation-models-evelyn-mannix-et-al-2024>(12/58 | 51/292) Scalable and Robust Transformer Decoders for Interpretable Image Classification with Foundation Models (Evelyn Mannix et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Evelyn Mannix, Howard Bondell. (2024)<br><strong>Scalable and Robust Transformer Decoders for Interpretable Image Classification with Foundation Models</strong><br><button class=copy-to-clipboard title="Scalable and Robust Transformer Decoders for Interpretable Image Classification with Foundation Models" index=51>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-51 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 43<br>Keywords: Object Detection, Benchmarking, Foundation Model, Self-supervised Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04125v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04125v1.pdf filename=2403.04125v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Interpretable computer vision models can produce transparent predictions, where the features of an image are compared with prototypes from a training dataset and the similarity between them forms a basis for classification. Nevertheless these methods are computationally expensive to train, introduce additional complexity and may require domain knowledge to adapt hyper-parameters to a new dataset. Inspired by developments in <b>object</b> <b>detection,</b> segmentation and large-scale <b>self-supervised</b> <b>foundation</b> <b>vision</b> models, we introduce Component Features (ComFe), a novel explainable-by-design image classification approach using a <b>transformer-decoder</b> head and hierarchical mixture-modelling. With only global image labels and no segmentation or part annotations, ComFe can identify consistent image components, such as the head, body, wings and tail of a bird, and the image background, and determine which of these features are informative in making a prediction. We demonstrate that ComFe obtains higher accuracy compared to previous interpretable models across a range of fine-grained vision <b>benchmarks,</b> without the need to individually tune hyper-parameters for each dataset. We also show that ComFe outperforms a non-interpretable linear head across a range of datasets, including ImageNet, and improves performance on generalisation and robustness <b>benchmarks.</b></p></p class="citation"></blockquote><h3 id=1358--52292-auformer-vision-transformers-are-parameter-efficient-facial-action-unit-detectors-kaishen-yuan-et-al-2024>(13/58 | 52/292) AUFormer: Vision Transformers are Parameter-Efficient Facial Action Unit Detectors (Kaishen Yuan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kaishen Yuan, Zitong Yu, Xin Liu, Weicheng Xie, Huanjing Yue, Jingyu Yang. (2024)<br><strong>AUFormer: Vision Transformers are Parameter-Efficient Facial Action Unit Detectors</strong><br><button class=copy-to-clipboard title="AUFormer: Vision Transformers are Parameter-Efficient Facial Action Unit Detectors" index=52>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-52 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Vision Transformer, Transfer Learning, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04697v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04697v1.pdf filename=2403.04697v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Facial Action Units (AU) is a vital concept in the realm of affective computing, and AU detection has always been a hot research topic. Existing methods suffer from overfitting issues due to the utilization of a large number of learnable parameters on scarce AU-annotated datasets or heavy reliance on substantial additional relevant data. Parameter-Efficient <b>Transfer</b> <b>Learning</b> (PETL) provides a promising paradigm to address these challenges, whereas its existing methods lack design for AU characteristics. Therefore, we innovatively investigate PETL paradigm to AU detection, introducing AUFormer and proposing a novel Mixture-of-Knowledge Expert (MoKE) collaboration mechanism. An individual MoKE specific to a certain AU with minimal learnable parameters first integrates personalized multi-scale and correlation knowledge. Then the MoKE collaborates with other MoKEs in the expert group to obtain aggregated information and inject it into the frozen <b>Vision</b> <b>Transformer</b> (ViT) to achieve parameter-efficient AU detection. Additionally, we design a Margin-truncated Difficulty-aware Weighted Asymmetric Loss (MDWA-Loss), which can encourage the model to focus more on activated AUs, differentiate the difficulty of unactivated AUs, and discard potential mislabeled samples. Extensive experiments from various perspectives, including within-domain, cross-domain, data efficiency, and micro-expression domain, demonstrate AUFormer&rsquo;s state-of-the-art performance and robust generalization abilities without relying on additional relevant data. The code for AUFormer is available at <a href=https://github.com/yuankaishen2001/AUFormer>https://github.com/yuankaishen2001/AUFormer</a>.</p></p class="citation"></blockquote><h3 id=1458--53292-pixart-σ-weak-to-strong-training-of-diffusion-transformer-for-4k-text-to-image-generation-junsong-chen-et-al-2024>(14/58 | 53/292) PixArt-Σ: Weak-to-Strong Training of Diffusion Transformer for 4K Text-to-Image Generation (Junsong Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, Zhenguo Li. (2024)<br><strong>PixArt-Σ: Weak-to-Strong Training of Diffusion Transformer for 4K Text-to-Image Generation</strong><br><button class=copy-to-clipboard title="PixArt-Σ: Weak-to-Strong Training of Diffusion Transformer for 4K Text-to-Image Generation" index=53>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-53 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Diffusion Model, Transformer, Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04692v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04692v1.pdf filename=2403.04692v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we introduce PixArt-\Sigma, a <b>Diffusion</b> <b>Transformer</b> model~(DiT) capable of directly generating images at 4K resolution. PixArt-\Sigma represents a significant advancement over its predecessor, PixArt-\alpha, offering images of markedly higher fidelity and improved alignment with text <b>prompts.</b> A key feature of PixArt-\Sigma is its training efficiency. Leveraging the foundational pre-training of PixArt-\alpha, it evolves from the <code>weaker' baseline to a </code>stronger&rsquo; model via incorporating higher quality data, a process we term &ldquo;weak-to-strong training&rdquo;. The advancements in PixArt-\Sigma are twofold: (1) High-Quality Training Data: PixArt-\Sigma incorporates superior-quality image data, paired with more precise and detailed image captions. (2) Efficient Token Compression: we propose a novel attention module within the DiT framework that compresses both keys and values, significantly improving efficiency and facilitating ultra-high-resolution image generation. Thanks to these improvements, PixArt-\Sigma achieves superior image quality and user <b>prompt</b> adherence capabilities with significantly smaller model size (0.6B parameters) than existing <b>text-to-image</b> <b>diffusion</b> <b>models,</b> such as SDXL (2.6B parameters) and SD Cascade (5.1B parameters). Moreover, PixArt-\Sigma&rsquo;s capability to generate 4K images supports the creation of high-resolution posters and wallpapers, efficiently bolstering the production of high-quality visual content in industries such as film and gaming.</p></p class="citation"></blockquote><h3 id=1558--54292-pix2gif-motion-guided-diffusion-for-gif-generation-hitesh-kandala-et-al-2024>(15/58 | 54/292) Pix2Gif: Motion-Guided Diffusion for GIF Generation (Hitesh Kandala et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hitesh Kandala, Jianfeng Gao, Jianwei Yang. (2024)<br><strong>Pix2Gif: Motion-Guided Diffusion for GIF Generation</strong><br><button class=copy-to-clipboard title="Pix2Gif: Motion-Guided Diffusion for GIF Generation" index=54>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-54 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Diffusion Model, Zero-shot, Image2text, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04634v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04634v2.pdf filename=2403.04634v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present Pix2Gif, a motion-guided <b>diffusion</b> <b>model</b> for <b>image-to-GIF</b> <b>(video)</b> generation. We tackle this problem differently by formulating the task as an <b>image</b> <b>translation</b> problem steered by text and motion magnitude <b>prompts,</b> as shown in teaser fig. To ensure that the model adheres to motion guidance, we propose a new motion-guided warping module to spatially transform the features of the source <b>image</b> <b>conditioned</b> on the two types of <b>prompts.</b> Furthermore, we introduce a perceptual loss to ensure the transformed feature map remains within the same space as the target <b>image,</b> <b>ensuring</b> content consistency and coherence. In preparation for the model training, we meticulously curated data by extracting coherent <b>image</b> <b>frames</b> from the TGIF video-caption dataset, which provides rich information about the temporal changes of subjects. After pretraining, we apply our model in a <b>zero-shot</b> manner to a number of video datasets. Extensive qualitative and quantitative experiments demonstrate the effectiveness of our model &ndash; it not only captures the semantic <b>prompt</b> from text but also the spatial ones from motion guidance. We train all our models using a single node of 16xV100 GPUs. Code, dataset and models are made public at: <a href=https://hiteshk03.github.io/Pix2Gif/>https://hiteshk03.github.io/Pix2Gif/</a>.</p></p class="citation"></blockquote><h3 id=1658--55292-an-explainable-ai-framework-for-artificial-intelligence-of-medical-things-al-amin-et-al-2024>(16/58 | 55/292) An Explainable AI Framework for Artificial Intelligence of Medical Things (Al Amin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Al Amin, Kamrul Hasan, Saleh Zein-Sabatto, Deo Chimba, Imtiaz Ahmed, Tariqul Islam. (2024)<br><strong>An Explainable AI Framework for Artificial Intelligence of Medical Things</strong><br><button class=copy-to-clipboard title="An Explainable AI Framework for Artificial Intelligence of Medical Things" index=55>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-55 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, Explainable AI<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04130v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04130v1.pdf filename=2403.04130v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The healthcare industry has been revolutionized by the convergence of Artificial Intelligence of Medical Things (AIoMT), allowing advanced data-driven solutions to improve healthcare systems. With the increasing complexity of Artificial Intelligence (AI) models, the need for <b>Explainable</b> <b>Artificial</b> Intelligence (XAI) techniques become paramount, particularly in the medical domain, where transparent and interpretable decision-making becomes crucial. Therefore, in this work, we leverage a custom XAI framework, incorporating techniques such as Local Interpretable Model-Agnostic Explanations (LIME), SHapley Additive exPlanations (SHAP), and Gradient-weighted Class Activation Mapping (Grad-Cam), explicitly designed for the domain of AIoMT. The proposed framework enhances the effectiveness of strategic healthcare methods and aims to instill trust and promote understanding in AI-driven medical applications. Moreover, we utilize a majority voting technique that aggregates predictions from multiple <b>convolutional</b> <b>neural</b> <b>networks</b> <b>(CNNs)</b> and leverages their collective intelligence to make robust and accurate decisions in the healthcare system. Building upon this decision-making process, we apply the XAI framework to brain tumor detection as a use case demonstrating accurate and transparent diagnosis. Evaluation results underscore the exceptional performance of the XAI framework, achieving high precision, recall, and F1 scores with a training accuracy of 99% and a validation accuracy of 98%. Combining advanced XAI techniques with ensemble-based deep-learning (DL) methodologies allows for precise and reliable brain tumor diagnoses as an application of AIoMT.</p></p class="citation"></blockquote><h3 id=1758--56292-a-data-centric-approach-to-class-specific-bias-in-image-data-augmentation-athanasios-angelakis-et-al-2024>(17/58 | 56/292) A data-centric approach to class-specific bias in image data augmentation (Athanasios Angelakis et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Athanasios Angelakis, Andrey Rass. (2024)<br><strong>A data-centric approach to class-specific bias in image data augmentation</strong><br><button class=copy-to-clipboard title="A data-centric approach to class-specific bias in image data augmentation" index=56>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-56 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Vision Transformer, Data Augmentation, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04120v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04120v1.pdf filename=2403.04120v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Data</b> <b>augmentation</b> (DA) enhances model generalization in computer <b>vision</b> <b>but</b> may introduce biases, impacting class accuracy unevenly. Our study extends this inquiry, examining DA&rsquo;s class-specific bias across various datasets, including those distinct from ImageNet, through random cropping. We evaluated this phenomenon with ResNet50, EfficientNetV2S, and SWIN ViT, discovering that while residual models showed similar bias effects, <b>Vision</b> <b>Transformers</b> exhibited greater robustness or altered dynamics. This suggests a nuanced approach to model selection, emphasizing bias mitigation. We also refined a <b>&ldquo;data</b> <b>augmentation</b> robustness scouting&rdquo; method to manage DA-induced biases more efficiently, reducing computational demands significantly (training 112 models instead of 1860; a reduction of factor 16.2) while still capturing essential bias trends.</p></p class="citation"></blockquote><h3 id=1858--57292-cat-enhancing-multimodal-large-language-model-to-answer-questions-in-dynamic-audio-visual-scenarios-qilang-ye-et-al-2024>(18/58 | 57/292) CAT: Enhancing Multimodal Large Language Model to Answer Questions in Dynamic Audio-Visual Scenarios (Qilang Ye et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qilang Ye, Zitong Yu, Rui Shao, Xinyu Xie, Philip Torr, Xiaochun Cao. (2024)<br><strong>CAT: Enhancing Multimodal Large Language Model to Answer Questions in Dynamic Audio-Visual Scenarios</strong><br><button class=copy-to-clipboard title="CAT: Enhancing Multimodal Large Language Model to Answer Questions in Dynamic Audio-Visual Scenarios" index=57>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-57 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 36<br>Keywords: Direct Preference Optimization, Multi-modal, Multi-modal, Question Answering, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04640v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04640v1.pdf filename=2403.04640v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper focuses on the challenge of answering <b>questions</b> <b>in</b> scenarios that are composed of rich and complex dynamic audio-visual components. Although existing <b>Multimodal</b> <b>Large</b> <b>Language</b> <b>Models</b> (MLLMs) can respond to audio-visual content, these responses are sometimes ambiguous and fail to describe specific audio-visual events. To overcome this limitation, we introduce the CAT, which enhances MLLM in three ways: 1) besides straightforwardly bridging audio and video, we design a clue aggregator that aggregates <b>question-related</b> <b>clues</b> in dynamic audio-visual scenarios to enrich the detailed knowledge required for <b>large</b> <b>language</b> <b>models.</b> 2) CAT is trained on a mixed <b>multimodal</b> dataset, allowing <b>direct</b> <b>application</b> <b>in</b> audio-visual scenarios. Notably, we collect an audio-visual joint instruction dataset named AVinstruct, to further enhance the capacity of CAT to model cross-semantic correlations. 3) we propose AI-assisted ambiguity-aware <b>direct</b> <b>preference</b> <b>optimization,</b> a strategy specialized in retraining the model to favor the non-ambiguity response and improve the ability to localize specific audio-visual objects. Extensive experimental results demonstrate that CAT outperforms existing methods on <b>multimodal</b> tasks, especially in Audio-Visual <b>Question</b> <b>Answering</b> (AVQA) tasks. The codes and the collected instructions are released at <a href=https://github.com/rikeilong/Bay-CAT>https://github.com/rikeilong/Bay-CAT</a>.</p></p class="citation"></blockquote><h3 id=1958--58292-discriminative-sample-guided-and-parameter-efficient-feature-space-adaptation-for-cross-domain-few-shot-learning-rashindrie-perera-et-al-2024>(19/58 | 58/292) Discriminative Sample-Guided and Parameter-Efficient Feature Space Adaptation for Cross-Domain Few-Shot Learning (Rashindrie Perera et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rashindrie Perera, Saman Halgamuge. (2024)<br><strong>Discriminative Sample-Guided and Parameter-Efficient Feature Space Adaptation for Cross-Domain Few-Shot Learning</strong><br><button class=copy-to-clipboard title="Discriminative Sample-Guided and Parameter-Efficient Feature Space Adaptation for Cross-Domain Few-Shot Learning" index=58>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-58 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 36<br>Keywords: Benchmarking, Clustering, Few-shot, Few-shot Learning, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04492v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04492v2.pdf filename=2403.04492v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we look at cross-domain <b>few-shot</b> <b>classification</b> which presents the challenging task of learning new classes in unseen domains with few labelled examples. Existing methods, though somewhat effective, encounter several limitations, which we address in this work through two significant improvements. First, to address overfitting associated with <b>fine-tuning</b> a large number of parameters on small datasets, we introduce a lightweight parameter-efficient adaptation strategy. This strategy employs a linear transformation of pre-trained features, significantly reducing the trainable parameter count. Second, we replace the traditional nearest centroid classifier with a variance-aware loss function, enhancing the model&rsquo;s sensitivity to the inter- and intra-class variances within the training set for improved <b>clustering</b> in feature space. Empirical evaluations on the Meta-Dataset <b>benchmark</b> showcase that our approach not only improves accuracy up to 7.7% and 5.3% on seen and unseen datasets respectively but also achieves this performance while being at least ~3x more parameter-efficient than existing methods, establishing a new state-of-the-art in cross-domain <b>few-shot</b> <b>learning.</b> Our code can be found at <a href=https://github.com/rashindrie/DIPA>https://github.com/rashindrie/DIPA</a>.</p></p class="citation"></blockquote><h3 id=2058--59292-3dtexturetransformer-geometry-aware-texture-generation-for-arbitrary-mesh-topology-dharma-kc-et-al-2024>(20/58 | 59/292) 3DTextureTransformer: Geometry Aware Texture Generation for Arbitrary Mesh Topology (Dharma KC et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dharma KC, Clayton T. Morrison. (2024)<br><strong>3DTextureTransformer: Geometry Aware Texture Generation for Arbitrary Mesh Topology</strong><br><button class=copy-to-clipboard title="3DTextureTransformer: Geometry Aware Texture Generation for Arbitrary Mesh Topology" index=59>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-59 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 35<br>Keywords: Message-Passing, Geometry, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04225v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04225v1.pdf filename=2403.04225v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Learning to generate textures for a novel 3D mesh given a collection of 3D meshes and real-world 2D images is an important problem with applications in various domains such as 3D <b>simulation,</b> augmented and virtual reality, gaming, architecture, and design. Existing solutions either do not produce high-quality textures or deform the original high-resolution input mesh topology into a regular grid to make this generation easier but also lose the original mesh topology. In this paper, we present a novel framework called the 3DTextureTransformer that enables us to generate high-quality textures without deforming the original, high-resolution input mesh. Our solution, a hybrid of geometric deep learning and StyleGAN-like architecture, is flexible enough to work on arbitrary mesh topologies and also easily extensible to texture generation for point cloud representations. Our solution employs a <b>message-passing</b> framework in 3D in conjunction with a StyleGAN-like architecture for 3D texture generation. The architecture achieves state-of-the-art performance among a class of solutions that can learn from a collection of 3D <b>geometry</b> and real-world 2D images while working with any arbitrary mesh topology.</p></p class="citation"></blockquote><h3 id=2158--60292-textr2-bench-benchmarking-the-robustness-of-referring-perception-models-under-perturbations-xiang-li-et-al-2024>(21/58 | 60/292) $\text{R}^2$-Bench: Benchmarking the Robustness of Referring Perception Models under Perturbations (Xiang Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiang Li, Kai Qiu, Jinglu Wang, Xiaohao Xu, Rita Singh, Kashu Yamazak, Hao Chen, Xiaonan Huang, Bhiksha Raj. (2024)<br><strong>$\text{R}^2$-Bench: Benchmarking the Robustness of Referring Perception Models under Perturbations</strong><br><button class=copy-to-clipboard title="$\text{R}^2$-Bench: Benchmarking the Robustness of Referring Perception Models under Perturbations" index=60>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-60 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 32<br>Keywords: Benchmarking, Benchmarking, Multi-modal, Multi-modal, Grounding, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04924v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04924v1.pdf filename=2403.04924v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Referring perception, which aims at <b>grounding</b> visual objects with <b>multimodal</b> referring guidance, is essential for bridging the gap between humans, who provide instructions, and the environment where intelligent systems perceive. Despite progress in this field, the robustness of referring perception models (RPMs) against disruptive perturbations is not well explored. This work thoroughly assesses the resilience of RPMs against various perturbations in both general and specific contexts. Recognizing the complex nature of referring perception tasks, we present a comprehensive taxonomy of perturbations, and then develop a versatile toolbox for synthesizing and evaluating the effects of composite disturbances. Employing this toolbox, we construct $\text{R}^2$-Bench, a <b>benchmark</b> for assessing the Robustness of Referring perception models under noisy conditions across five key tasks. Moreover, we propose the $\text{R}^2$-Agent, an <b>LLM-based</b> agent that simplifies and automates model evaluation via natural language instructions. Our investigation uncovers the vulnerabilities of current RPMs to various perturbations and provides tools for assessing model robustness, potentially promoting the safe and resilient integration of intelligent systems into complex real-world scenarios.</p></p class="citation"></blockquote><h3 id=2258--61292-fooling-neural-networks-for-motion-forecasting-via-adversarial-attacks-edgar-medina-et-al-2024>(22/58 | 61/292) Fooling Neural Networks for Motion Forecasting via Adversarial Attacks (Edgar Medina et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Edgar Medina, Leyong Loh. (2024)<br><strong>Fooling Neural Networks for Motion Forecasting via Adversarial Attacks</strong><br><button class=copy-to-clipboard title="Fooling Neural Networks for Motion Forecasting via Adversarial Attacks" index=61>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-61 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Graph Convolutional Network, Convolutional Neural Network, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04954v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04954v2.pdf filename=2403.04954v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Human motion prediction is still an open problem, which is extremely important for autonomous driving and safety applications. Although there are great advances in this area, the widely studied topic of <b>adversarial</b> <b>attacks</b> has not been applied to multi-regression models such as <b>GCNs</b> and MLP-based architectures in human motion prediction. This work intends to reduce this gap using extensive quantitative and qualitative experiments in state-of-the-art architectures similar to the initial stages of <b>adversarial</b> <b>attacks</b> in image classification. The results suggest that models are susceptible to attacks even on low levels of perturbation. We also show experiments with 3D transformations that affect the model performance, in particular, we show that most models are sensitive to simple rotations and translations which do not alter joint distances. We conclude that similar to earlier <b>CNN</b> models, motion forecasting tasks are susceptible to small perturbations and simple 3D transformations.</p></p class="citation"></blockquote><h3 id=2358--62292-an-item-is-worth-a-prompt-versatile-image-editing-with-disentangled-control-aosong-feng-et-al-2024>(23/58 | 62/292) An Item is Worth a Prompt: Versatile Image Editing with Disentangled Control (Aosong Feng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aosong Feng, Weikang Qiu, Jinbin Bai, Kaicheng Zhou, Zhen Dong, Xiao Zhang, Rex Ying, Leandros Tassiulas. (2024)<br><strong>An Item is Worth a Prompt: Versatile Image Editing with Disentangled Control</strong><br><button class=copy-to-clipboard title="An Item is Worth a Prompt: Versatile Image Editing with Disentangled Control" index=62>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-62 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Diffusion Model, Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04880v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04880v1.pdf filename=2403.04880v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Building on the success of <b>text-to-image</b> <b>diffusion</b> <b>models</b> (DPMs), image editing is an important application to enable human interaction with AI-generated content. Among various editing methods, editing within the <b>prompt</b> space gains more attention due to its capacity and simplicity of controlling semantics. However, since <b>diffusion</b> <b>models</b> are commonly pretrained on descriptive text captions, direct editing of words in text <b>prompts</b> usually leads to completely different generated images, violating the requirements for image editing. On the other hand, existing editing methods usually consider introducing spatial masks to preserve the identity of unedited regions, which are usually ignored by DPMs and therefore lead to inharmonic editing results. Targeting these two challenges, in this work, we propose to disentangle the comprehensive image-prompt interaction into several item-prompt interactions, with each item linked to a special learned <b>prompt.</b> The resulting framework, named D-Edit, is based on pretrained <b>diffusion</b> <b>models</b> with cross-attention layers disentangled and adopts a two-step optimization to build item-prompt associations. Versatile image editing can then be applied to specific items by manipulating the corresponding <b>prompts.</b> We demonstrate state-of-the-art results in four types of editing operations including image-based, text-based, mask-based editing, and item removal, covering most types of editing applications, all within a single unified framework. Notably, D-Edit is the first framework that can (1) achieve item editing through mask editing and (2) combine image and text-based editing. We demonstrate the quality and versatility of the editing results for a diverse collection of images through both qualitative and quantitative evaluations.</p></p class="citation"></blockquote><h3 id=2458--63292-unitable-towards-a-unified-framework-for-table-structure-recognition-via-self-supervised-pretraining-shengyun-peng-et-al-2024>(24/58 | 63/292) UniTable: Towards a Unified Framework for Table Structure Recognition via Self-Supervised Pretraining (ShengYun Peng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>ShengYun Peng, Seongmin Lee, Xiaojing Wang, Rajarajeswari Balasubramaniyan, Duen Horng Chau. (2024)<br><strong>UniTable: Towards a Unified Framework for Table Structure Recognition via Self-Supervised Pretraining</strong><br><button class=copy-to-clipboard title="UniTable: Towards a Unified Framework for Table Structure Recognition via Self-Supervised Pretraining" index=63>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-63 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 30<br>Keywords: Fine-tuning, Self-supervised Learning, Self-supervised Pre-training<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04822v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04822v1.pdf filename=2403.04822v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Tables convey factual and quantitative data with implicit conventions created by humans that are often challenging for machines to parse. Prior work on table structure recognition (TSR) has mainly centered around complex task-specific combinations of available inputs and tools. We present UniTable, a training framework that unifies both the training paradigm and training objective of TSR. Its training paradigm combines the simplicity of purely pixel-level inputs with the effectiveness and scalability empowered by <b>self-supervised</b> <b>pretraining</b> (SSP) from diverse unannotated tabular images. Our framework unifies the training objectives of all three TSR tasks - extracting table structure, cell content, and cell bounding box (bbox) - into a unified task-agnostic training objective: language modeling. Extensive quantitative and qualitative analyses highlight UniTable&rsquo;s state-of-the-art (SOTA) performance on four of the largest TSR datasets. To promote reproducible research, enhance transparency, and SOTA innovations, we open-source our code at <a href=https://github.com/poloclub/unitable>https://github.com/poloclub/unitable</a> and release the first-of-its-kind Jupyter Notebook of the whole inference pipeline, <b>fine-tuned</b> across multiple TSR datasets, supporting all three TSR tasks.</p></p class="citation"></blockquote><h3 id=2558--64292-controllable-generation-with-text-to-image-diffusion-models-a-survey-pu-cao-et-al-2024>(25/58 | 64/292) Controllable Generation with Text-to-Image Diffusion Models: A Survey (Pu Cao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pu Cao, Feng Zhou, Qing Song, Lu Yang. (2024)<br><strong>Controllable Generation with Text-to-Image Diffusion Models: A Survey</strong><br><button class=copy-to-clipboard title="Controllable Generation with Text-to-Image Diffusion Models: A Survey" index=64>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-64 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Diffusion Model, Probabilistic Model, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04279v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04279v1.pdf filename=2403.04279v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the rapidly advancing realm of visual generation, <b>diffusion</b> <b>models</b> have revolutionized the landscape, marking a significant shift in capabilities with their impressive text-guided generative functions. However, relying solely on text for conditioning these models does not fully cater to the varied and complex requirements of different applications and scenarios. Acknowledging this shortfall, a variety of studies aim to control pre-trained <b>text-to-image</b> (T2I) models to support novel conditions. In this survey, we undertake a thorough review of the literature on controllable generation with T2I <b>diffusion</b> <b>models,</b> covering both the theoretical foundations and practical advancements in this domain. Our review begins with a brief introduction to the basics of denoising <b>diffusion</b> <b>probabilistic</b> <b>models</b> (DDPMs) and widely used T2I <b>diffusion</b> <b>models.</b> We then reveal the controlling mechanisms of <b>diffusion</b> <b>models,</b> theoretically analyzing how novel conditions are introduced into the denoising process for conditional generation. Additionally, we offer a detailed overview of research in this area, organizing it into distinct categories from the condition perspective: generation with specific conditions, generation with multiple conditions, and universal controllable generation. For an exhaustive list of the controllable generation literature surveyed, please refer to our curated repository at \url{https://github.com/PRIV-Creation/Awesome-Controllable-T2I-Diffusion-Models}.</p></p class="citation"></blockquote><h3 id=2658--65292-promise-promptable-medical-image-segmentation-using-sam-jinfeng-wang-et-al-2024>(26/58 | 65/292) ProMISe: Promptable Medical Image Segmentation using SAM (Jinfeng Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinfeng Wang, Sifan Song, Xinkun Wang, Yiyi Wang, Yiyi Miao, Jionglong Su, S. Kevin Zhou. (2024)<br><strong>ProMISe: Promptable Medical Image Segmentation using SAM</strong><br><button class=copy-to-clipboard title="ProMISe: Promptable Medical Image Segmentation using SAM" index=65>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-65 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Fine-tuning, Foundation Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04164v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04164v1.pdf filename=2403.04164v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the proposal of the Segment Anything Model (SAM), <b>fine-tuning</b> SAM for medical image segmentation (MIS) has become popular. However, due to the large size of the SAM model and the significant domain gap between natural and medical images, <b>fine-tuning-based</b> strategies are costly with potential risk of instability, feature damage and catastrophic forgetting. Furthermore, some methods of transferring SAM to a domain-specific MIS through <b>fine-tuning</b> strategies disable the model&rsquo;s <b>prompting</b> capability, severely limiting its utilization scenarios. In this paper, we propose an Auto-Prompting Module (APM), which provides SAM-based <b>foundation</b> <b>model</b> with Euclidean adaptive <b>prompts</b> in the target domain. Our experiments demonstrate that such adaptive <b>prompts</b> significantly improve SAM&rsquo;s non-fine-tuned performance in MIS. In addition, we propose a novel non-invasive method called Incremental Pattern Shifting (IPS) to adapt SAM to specific medical domains. Experimental results show that the IPS enables SAM to achieve state-of-the-art or competitive performance in MIS without the need for <b>fine-tuning.</b> By coupling these two methods, we propose ProMISe, an end-to-end non-fine-tuned framework for Promptable Medical Image Segmentation. Our experiments demonstrate that both using our methods individually or in combination achieves satisfactory performance in low-cost pattern shifting, with all of SAM&rsquo;s parameters frozen.</p></p class="citation"></blockquote><h3 id=2758--66292-dual-path-frequency-discriminators-for-few-shot-anomaly-detection-yuhu-bai-et-al-2024>(27/58 | 66/292) Dual-path Frequency Discriminators for Few-shot Anomaly Detection (Yuhu Bai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuhu Bai, Jiangning Zhang, Yuhang Dong, Guanzhong Tian, Liang Liu, Yunkang Cao, Yabiao Wang, Chengjie Wang. (2024)<br><strong>Dual-path Frequency Discriminators for Few-shot Anomaly Detection</strong><br><button class=copy-to-clipboard title="Dual-path Frequency Discriminators for Few-shot Anomaly Detection" index=66>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-66 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Anomaly Detection, Benchmarking, Few-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04151v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04151v2.pdf filename=2403.04151v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Few-shot</b> <b>anomaly</b> <b>detection</b> (FSAD) is essential in industrial manufacturing. However, existing FSAD methods struggle to effectively leverage a limited number of normal samples, and they may fail to detect and locate inconspicuous anomalies in the spatial domain. We further discover that these subtle anomalies would be more noticeable in the frequency domain. In this paper, we propose a Dual-Path Frequency Discriminators (DFD) network from a frequency perspective to tackle these issues. Specifically, we generate anomalies at both image-level and feature-level. Differential frequency components are extracted by the multi-frequency information construction module and supplied into the fine-grained feature construction module to provide adapted features. We consider <b>anomaly</b> <b>detection</b> as a discriminative classification problem, wherefore the dual-path feature discrimination module is employed to detect and locate the image-level and feature-level anomalies in the feature space. The discriminators aim to learn a joint representation of anomalous features and normal features in the latent space. Extensive experiments conducted on MVTec AD and VisA <b>benchmarks</b> demonstrate that our DFD surpasses current state-of-the-art methods. Source code will be available.</p></p class="citation"></blockquote><h3 id=2858--67292-towards-learning-based-planningthe-nuplan-benchmark-for-real-world-autonomous-driving-napat-karnchanachari-et-al-2024>(28/58 | 67/292) Towards learning-based planning:The nuPlan benchmark for real-world autonomous driving (Napat Karnchanachari et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Napat Karnchanachari, Dimitris Geromichalos, Kok Seang Tan, Nanxiang Li, Christopher Eriksen, Shakiba Yaghoubi, Noushin Mehdipour, Gianmarco Bernasconi, Whye Kit Fong, Yiluan Guo, Holger Caesar. (2024)<br><strong>Towards learning-based planning:The nuPlan benchmark for real-world autonomous driving</strong><br><button class=copy-to-clipboard title="Towards learning-based planning:The nuPlan benchmark for real-world autonomous driving" index=67>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-67 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV<br>Keyword Score: 23<br>Keywords: Benchmarking, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04133v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04133v1.pdf filename=2403.04133v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Machine Learning (ML) has replaced traditional handcrafted methods for perception and prediction in autonomous vehicles. Yet for the equally important planning task, the adoption of ML-based techniques is slow. We present nuPlan, the world&rsquo;s first real-world autonomous driving dataset, and <b>benchmark.</b> The <b>benchmark</b> is designed to test the ability of ML-based planners to handle diverse driving situations and to make safe and efficient decisions. To that end, we introduce a new large-scale dataset that consists of 1282 hours of diverse driving scenarios from 4 cities (Las Vegas, Boston, Pittsburgh, and Singapore) and includes high-quality auto-labeled object tracks and traffic light data. We exhaustively mine and taxonomize common and rare driving scenarios which are used during evaluation to get fine-grained insights into the performance and characteristics of a planner. Beyond the dataset, we provide a <b>simulation</b> and evaluation framework that enables a planner&rsquo;s actions to be simulated in closed-loop to account for interactions with other traffic participants. We present a detailed analysis of numerous baselines and investigate gaps between ML-based and traditional methods. Find the nuPlan dataset and code at nuplan.org.</p></p class="citation"></blockquote><h3 id=2958--68292-divide-and-conquer-high-resolution-industrial-anomaly-detection-via-memory-efficient-tiled-ensemble-blaž-rolih-et-al-2024>(29/58 | 68/292) Divide and Conquer: High-Resolution Industrial Anomaly Detection via Memory Efficient Tiled Ensemble (Blaž Rolih et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Blaž Rolih, Samet Akçay, Dick Ameln, Ashwin Vaidya. (2024)<br><strong>Divide and Conquer: High-Resolution Industrial Anomaly Detection via Memory Efficient Tiled Ensemble</strong><br><button class=copy-to-clipboard title="Divide and Conquer: High-Resolution Industrial Anomaly Detection via Memory Efficient Tiled Ensemble" index=68>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-68 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Anomaly Detection, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04932v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04932v1.pdf filename=2403.04932v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Industrial <b>anomaly</b> <b>detection</b> is an important task within computer vision with a wide range of practical use cases. The small size of anomalous regions in many real-world datasets necessitates processing the images at a high resolution. This frequently poses significant challenges concerning memory consumption during the model training and inference stages, leaving some existing methods impractical for widespread adoption. To overcome this challenge, we present the tiled ensemble approach, which reduces memory consumption by dividing the input images into a grid of tiles and training a dedicated model for each tile location. The tiled ensemble is compatible with any existing <b>anomaly</b> <b>detection</b> model without the need for any modification of the underlying architecture. By introducing overlapping tiles, we utilize the benefits of traditional stacking ensembles, leading to further improvements in <b>anomaly</b> <b>detection</b> capabilities beyond high resolution alone. We perform a comprehensive analysis using diverse underlying architectures, including Padim, PatchCore, FastFlow, and Reverse <b>Distillation,</b> on two standard <b>anomaly</b> <b>detection</b> datasets: MVTec and VisA. Our method demonstrates a notable improvement across setups while remaining within GPU memory constraints, consuming only as much GPU memory as a single model needs to process a single tile.</p></p class="citation"></blockquote><h3 id=3058--69292-delving-into-the-trajectory-long-tail-distribution-for-muti-object-tracking-sijia-chen-et-al-2024>(30/58 | 69/292) Delving into the Trajectory Long-tail Distribution for Muti-object Tracking (Sijia Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sijia Chen, En Yu, Jinyang Li, Wenbing Tao. (2024)<br><strong>Delving into the Trajectory Long-tail Distribution for Muti-object Tracking</strong><br><button class=copy-to-clipboard title="Delving into the Trajectory Long-tail Distribution for Muti-object Tracking" index=69>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-69 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Data Augmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04700v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04700v1.pdf filename=2403.04700v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multiple Object Tracking (MOT) is a critical area within computer vision, with a broad spectrum of practical implementations. Current research has primarily focused on the development of tracking algorithms and enhancement of post-processing techniques. Yet, there has been a lack of thorough examination concerning the nature of tracking <b>data</b> <b>it</b> self. In this study, we pioneer an exploration into the distribution patterns of tracking <b>data</b> <b>and</b> identify a pronounced long-tail distribution issue within existing MOT datasets. We note a significant imbalance in the distribution of trajectory lengths across different pedestrians, a phenomenon we refer to as &ldquo;pedestrians trajectory long-tail distribution&rdquo;. Addressing this challenge, we introduce a bespoke strategy designed to mitigate the effects of this skewed distribution. Specifically, we propose two <b>data</b> <b>augmentation</b> strategies, including Stationary Camera View <b>Data</b> <b>Augmentation</b> (SVA) and Dynamic Camera View <b>Data</b> <b>Augmentation</b> (DVA) , designed for viewpoint states and the Group Softmax (GS) module for Re-ID. SVA is to backtrack and predict the pedestrian trajectory of tail classes, and DVA is to use <b>diffusion</b> <b>model</b> to change the background of the scene. GS divides the pedestrians into unrelated groups and performs softmax operation on each group individually. Our proposed strategies can be integrated into numerous existing tracking systems, and extensive experimentation validates the efficacy of our method in reducing the influence of long-tail distribution on multi-object tracking performance. The code is available at <a href=https://github.com/chen-si-jia/Trajectory-Long-tail-Distribution-for-MOT>https://github.com/chen-si-jia/Trajectory-Long-tail-Distribution-for-MOT</a>.</p></p class="citation"></blockquote><h3 id=3158--70292-source-matters-source-dataset-impact-on-model-robustness-in-medical-imaging-dovile-juodelyte-et-al-2024>(31/58 | 70/292) Source Matters: Source Dataset Impact on Model Robustness in Medical Imaging (Dovile Juodelyte et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dovile Juodelyte, Yucheng Lu, Amelia Jiménez-Sánchez, Sabrina Bottazzi, Enzo Ferrante, Veronika Cheplygina. (2024)<br><strong>Source Matters: Source Dataset Impact on Model Robustness in Medical Imaging</strong><br><button class=copy-to-clipboard title="Source Matters: Source Dataset Impact on Model Robustness in Medical Imaging" index=70>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-70 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 20<br>Keywords: Transfer Learning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04484v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04484v1.pdf filename=2403.04484v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Transfer</b> <b>learning</b> has become an essential part of medical imaging classification algorithms, often leveraging ImageNet weights. However, the domain shift from natural to medical images has <b>prompted</b> alternatives such as RadImageNet, often demonstrating comparable classification performance. However, it remains unclear whether the performance gains from <b>transfer</b> <b>learning</b> stem from improved generalization or shortcut learning. To address this, we investigate potential confounders &ndash; whether synthetic or sampled from the data &ndash; across two publicly available chest X-ray and CT datasets. We show that ImageNet and RadImageNet achieve comparable classification performance, yet ImageNet is much more prone to overfitting to confounders. We recommend that researchers using ImageNet-pretrained models reexamine their model robustness by conducting similar experiments. Our code and experiments are available at <a href=https://github.com/DovileDo/source-matters>https://github.com/DovileDo/source-matters</a>.</p></p class="citation"></blockquote><h3 id=3258--71292-disentangled-diffusion-based-3d-human-pose-estimation-with-hierarchical-spatial-and-temporal-denoiser-qingyuan-cai-et-al-2024>(32/58 | 71/292) Disentangled Diffusion-Based 3D Human Pose Estimation with Hierarchical Spatial and Temporal Denoiser (Qingyuan Cai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qingyuan Cai, Xuecai Hu, Saihui Hou, Li Yao, Yongzhen Huang. (2024)<br><strong>Disentangled Diffusion-Based 3D Human Pose Estimation with Hierarchical Spatial and Temporal Denoiser</strong><br><button class=copy-to-clipboard title="Disentangled Diffusion-Based 3D Human Pose Estimation with Hierarchical Spatial and Temporal Denoiser" index=71>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-71 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04444v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04444v1.pdf filename=2403.04444v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, <b>diffusion-based</b> <b>methods</b> for monocular 3D human pose estimation have achieved state-of-the-art (SOTA) performance by directly regressing the 3D joint coordinates from the 2D pose sequence. Although some methods decompose the task into bone length and bone direction prediction based on the human anatomical skeleton to explicitly incorporate more human body prior constraints, the performance of these methods is significantly lower than that of the SOTA <b>diffusion-based</b> <b>methods.</b> This can be attributed to the tree structure of the human skeleton. Direct application of the disentangled method could amplify the accumulation of hierarchical errors, propagating through each hierarchy. Meanwhile, the hierarchical information has not been fully explored by the previous methods. To address these problems, a Disentangled <b>Diffusion-based</b> <b>3D</b> Human Pose Estimation method with Hierarchical Spatial and Temporal Denoiser is proposed, termed DDHPose. In our approach: (1) We disentangle the 3D pose and diffuse the bone length and bone direction during the forward process of the <b>diffusion</b> <b>model</b> to effectively model the human pose prior. A disentanglement loss is proposed to supervise <b>diffusion</b> <b>model</b> learning. (2) For the reverse process, we propose Hierarchical Spatial and Temporal Denoiser (HSTDenoiser) to improve the hierarchical modeling of each joint. Our HSTDenoiser comprises two components: the Hierarchical-Related Spatial <b>Transformer</b> (HRST) and the Hierarchical-Related Temporal <b>Transformer</b> (HRTT). HRST exploits joint spatial information and the influence of the parent joint on each joint for spatial modeling, while HRTT utilizes information from both the joint and its hierarchical adjacent joints to explore the hierarchical temporal correlations among joints.</p></p class="citation"></blockquote><h3 id=3358--72292-impacts-of-color-and-texture-distortions-on-earth-observation-data-in-deep-learning-martin-willbo-et-al-2024>(33/58 | 72/292) Impacts of Color and Texture Distortions on Earth Observation Data in Deep Learning (Martin Willbo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Martin Willbo, Aleksis Pirinen, John Martinsson, Edvin Listo Zec, Olof Mogren, Mikael Nilsson. (2024)<br><strong>Impacts of Color and Texture Distortions on Earth Observation Data in Deep Learning</strong><br><button class=copy-to-clipboard title="Impacts of Color and Texture Distortions on Earth Observation Data in Deep Learning" index=72>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-72 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 20<br>Keywords: Convolution, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04385v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04385v1.pdf filename=2403.04385v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Land cover classification and change detection are two important applications of remote sensing and Earth observation (EO) that have benefited greatly from the advances of deep learning. <b>Convolutional</b> and <b>transformer-based</b> U-net models are the state-of-the-art architectures for these tasks, and their performances have been boosted by an increased availability of large-scale annotated EO datasets. However, the influence of different visual characteristics of the input EO data on a model&rsquo;s predictions is not well understood. In this work we systematically examine model sensitivities with respect to several color- and texture-based distortions on the input EO data during inference, given models that have been trained without such distortions. We conduct experiments with multiple state-of-the-art segmentation networks for land cover classification and show that they are in general more sensitive to texture than to color distortions. Beyond revealing intriguing characteristics of widely used land cover classification models, our results can also be used to guide the development of more robust models within the EO domain.</p></p class="citation"></blockquote><h3 id=3458--73292-multi-step-temporal-modeling-for-uav-tracking-xiaoying-yuan-et-al-2024>(34/58 | 73/292) Multi-step Temporal Modeling for UAV Tracking (Xiaoying Yuan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaoying Yuan, Tingfa Xu, Xincong Liu, Ying Wang, Haolin Qin, Yuqiang Fang, Jianan Li. (2024)<br><strong>Multi-step Temporal Modeling for UAV Tracking</strong><br><button class=copy-to-clipboard title="Multi-step Temporal Modeling for UAV Tracking" index=73>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-73 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Transformer, Neural Machine Translation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04363v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04363v1.pdf filename=2403.04363v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the realm of unmanned aerial vehicle (UAV) tracking, Siamese-based approaches have gained traction due to their optimal balance between efficiency and precision. However, UAV scenarios often present challenges such as insufficient sampling resolution, fast motion and small objects with limited feature information. As a result, temporal context in UAV tracking tasks plays a pivotal role in target location, overshadowing the target&rsquo;s precise features. In this paper, we introduce <b>MT-Track,</b> a streamlined and efficient multi-step temporal modeling framework designed to harness the temporal context from historical frames for enhanced UAV tracking. This temporal integration occurs in two steps: correlation map generation and correlation map refinement. Specifically, we unveil a unique temporal correlation module that dynamically assesses the interplay between the template and search region features. This module leverages temporal information to refresh the template feature, yielding a more precise correlation map. Subsequently, we propose a mutual <b>transformer</b> module to refine the correlation maps of historical and current frames by modeling the temporal knowledge in the tracking sequence. This method significantly trims computational demands compared to the raw <b>transformer.</b> The compact yet potent nature of our tracking framework ensures commendable tracking outcomes, particularly in extended tracking scenarios.</p></p class="citation"></blockquote><h3 id=3558--74292-depth-aware-test-time-training-for-zero-shot-video-object-segmentation-weihuang-liu-et-al-2024>(35/58 | 74/292) Depth-aware Test-Time Training for Zero-shot Video Object Segmentation (Weihuang Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Weihuang Liu, Xi Shen, Haolun Li, Xiuli Bi, Bo Liu, Chi-Man Pun, Xiaodong Cun. (2024)<br><strong>Depth-aware Test-Time Training for Zero-shot Video Object Segmentation</strong><br><button class=copy-to-clipboard title="Depth-aware Test-Time Training for Zero-shot Video Object Segmentation" index=74>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-74 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Data Augmentation, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04258v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04258v1.pdf filename=2403.04258v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Zero-shot</b> Video Object Segmentation (ZSVOS) aims at segmenting the primary moving object without any human annotations. Mainstream solutions mainly focus on learning a single model on large-scale video datasets, which struggle to generalize to unseen videos. In this work, we introduce a test-time training (TTT) strategy to address the problem. Our key insight is to enforce the model to predict consistent depth during the TTT process. In detail, we first train a single network to perform both segmentation and depth prediction tasks. This can be effectively learned with our specifically designed depth modulation layer. Then, for the TTT process, the model is updated by predicting consistent depth maps for the same frame under different <b>data</b> <b>augmentations.</b> In addition, we explore different TTT weight updating strategies. Our empirical results suggest that the momentum-based weight initialization and looping-based training scheme lead to more stable improvements. Experiments show that the proposed method achieves clear improvements on ZSVOS. Our proposed video TTT strategy provides significant superiority over state-of-the-art TTT methods. Our code is available at: <a href=https://nifangbaage.github.io/DATTT>https://nifangbaage.github.io/DATTT</a>.</p></p class="citation"></blockquote><h3 id=3658--75292-sam-pd-how-far-can-sam-take-us-in-tracking-and-segmenting-anything-in-videos-by-prompt-denoising-tao-zhou-et-al-2024>(36/58 | 75/292) SAM-PD: How Far Can SAM Take Us in Tracking and Segmenting Anything in Videos by Prompt Denoising (Tao Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tao Zhou, Wenhan Luo, Qi Ye, Zhiguo Shi, Jiming Chen. (2024)<br><strong>SAM-PD: How Far Can SAM Take Us in Tracking and Segmenting Anything in Videos by Prompt Denoising</strong><br><button class=copy-to-clipboard title="SAM-PD: How Far Can SAM Take Us in Tracking and Segmenting Anything in Videos by Prompt Denoising" index=75>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-75 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Zero-shot, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04194v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04194v1.pdf filename=2403.04194v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, promptable segmentation models, such as the Segment Anything Model (SAM), have demonstrated robust <b>zero-shot</b> generalization capabilities on static images. These promptable models exhibit denoising abilities for imprecise <b>prompt</b> inputs, such as imprecise bounding boxes. In this paper, we explore the potential of applying SAM to track and segment objects in videos where we recognize the tracking task as a <b>prompt</b> denoising task. Specifically, we iteratively propagate the bounding box of each object&rsquo;s mask in the preceding frame as the <b>prompt</b> for the next frame. Furthermore, to enhance SAM&rsquo;s denoising capability against position and size variations, we propose a multi-prompt strategy where we provide multiple jittered and scaled box <b>prompts</b> for each object and preserve the mask prediction with the highest semantic similarity to the template mask. We also introduce a point-based refinement stage to handle occlusions and reduce cumulative errors. Without involving tracking modules, our approach demonstrates comparable performance in video object/instance segmentation tasks on three datasets: DAVIS2017, YouTubeVOS2018, and UVO, serving as a concise baseline and endowing SAM-based downstream applications with tracking capabilities.</p></p class="citation"></blockquote><h3 id=3758--76292-bags-blur-agnostic-gaussian-splatting-through-multi-scale-kernel-modeling-cheng-peng-et-al-2024>(37/58 | 76/292) BAGS: Blur Agnostic Gaussian Splatting through Multi-Scale Kernel Modeling (Cheng Peng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Cheng Peng, Yutao Tang, Yifan Zhou, Nengyu Wang, Xijun Liu, Deming Li, Rama Chellappa. (2024)<br><strong>BAGS: Blur Agnostic Gaussian Splatting through Multi-Scale Kernel Modeling</strong><br><button class=copy-to-clipboard title="BAGS: Blur Agnostic Gaussian Splatting through Multi-Scale Kernel Modeling" index=76>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-76 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 18<br>Keywords: Benchmarking, Convolution, Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04926v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04926v1.pdf filename=2403.04926v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent efforts in using 3D Gaussians for scene reconstruction and novel view synthesis can achieve impressive results on curated <b>benchmarks;</b> however, images captured in real life are often blurry. In this work, we analyze the robustness of Gaussian-Splatting-based methods against various image blur, such as motion blur, defocus blur, downscaling blur, \etc. Under these degradations, Gaussian-Splatting-based methods tend to overfit and produce worse results than Neural-Radiance-Field-based methods. To address this issue, we propose Blur Agnostic Gaussian Splatting (BAGS). BAGS introduces additional 2D modeling capacities such that a 3D-consistent and high quality scene can be reconstructed despite image-wise blur. Specifically, we model blur by estimating per-pixel <b>convolution</b> kernels from a Blur Proposal Network (BPN). BPN is designed to consider spatial, color, and depth variations of the scene to maximize modeling capacity. Additionally, BPN also proposes a quality-assessing mask, which indicates regions where blur occur. Finally, we introduce a coarse-to-fine kernel optimization scheme; this optimization scheme is fast and avoids sub-optimal solutions due to a sparse point cloud initialization, which often occurs when we apply Structure-from-Motion on blurry images. We demonstrate that BAGS achieves photorealistic renderings under various challenging blur conditions and imaging <b>geometry,</b> while significantly improving upon existing approaches.</p></p class="citation"></blockquote><h3 id=3858--77292-cn-rma-combined-network-with-ray-marching-aggregation-for-3d-indoors-object-detection-from-multi-view-images-guanlin-shen-et-al-2024>(38/58 | 77/292) CN-RMA: Combined Network with Ray Marching Aggregation for 3D Indoors Object Detection from Multi-view Images (Guanlin Shen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guanlin Shen, Jingwei Huang, Zhihua Hu, Bin Wang. (2024)<br><strong>CN-RMA: Combined Network with Ray Marching Aggregation for 3D Indoors Object Detection from Multi-view Images</strong><br><button class=copy-to-clipboard title="CN-RMA: Combined Network with Ray Marching Aggregation for 3D Indoors Object Detection from Multi-view Images" index=77>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-77 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 15<br>Keywords: Object Detection, Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04198v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04198v1.pdf filename=2403.04198v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces CN-RMA, a novel approach for 3D indoor <b>object</b> <b>detection</b> from multi-view images. We observe the key challenge as the ambiguity of image and 3D correspondence without explicit <b>geometry</b> to provide occlusion information. To address this issue, CN-RMA leverages the synergy of 3D reconstruction networks and 3D <b>object</b> <b>detection</b> networks, where the reconstruction network provides a rough Truncated Signed Distance Function (TSDF) and guides image features to vote to 3D space correctly in an end-to-end manner. Specifically, we associate weights to sampled points of each ray through ray marching, representing the contribution of a pixel in an image to corresponding 3D locations. Such weights are determined by the predicted signed distances so that image features vote only to regions near the reconstructed surface. Our method achieves state-of-the-art performance in 3D <b>object</b> <b>detection</b> from multi-view images, as measured by <a href=mailto:mAP@0.25>mAP@0.25</a> and <a href=mailto:mAP@0.5>mAP@0.5</a> on the ScanNet and ARKitScenes datasets. The code and models are released at <a href=https://github.com/SerCharles/CN-RMA>https://github.com/SerCharles/CN-RMA</a>.</p></p class="citation"></blockquote><h3 id=3958--78292-towards-scene-graph-anticipation-rohith-peddi-et-al-2024>(39/58 | 78/292) Towards Scene Graph Anticipation (Rohith Peddi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rohith Peddi, Saksham Singh, Saurabh, Parag Singla, Vibhav Gogate. (2024)<br><strong>Towards Scene Graph Anticipation</strong><br><button class=copy-to-clipboard title="Towards Scene Graph Anticipation" index=78>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-78 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Graph, Continuous Time, Continuous Time<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04899v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04899v1.pdf filename=2403.04899v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Spatio-temporal scene <b>graphs</b> represent interactions in a video by decomposing scenes into individual objects and their pair-wise temporal relationships. Long-term anticipation of the fine-grained pair-wise relationships between objects is a challenging problem. To this end, we introduce the task of Scene <b>Graph</b> Anticipation (SGA). We adapt state-of-the-art scene <b>graph</b> generation methods as baselines to anticipate future pair-wise relationships between objects and propose a novel approach SceneSayer. In SceneSayer, we leverage object-centric representations of relationships to reason about the observed video frames and model the evolution of relationships between objects. We take a <b>continuous</b> <b>time</b> perspective and model the latent dynamics of the evolution of object interactions using concepts of NeuralODE and NeuralSDE, respectively. We infer representations of future relationships by solving an Ordinary Differential Equation and a Stochastic Differential Equation, respectively. Extensive experimentation on the Action Genome dataset validates the efficacy of the proposed methods.</p></p class="citation"></blockquote><h3 id=4058--79292-embodied-understanding-of-driving-scenarios-yunsong-zhou-et-al-2024>(40/58 | 79/292) Embodied Understanding of Driving Scenarios (Yunsong Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yunsong Zhou, Linyan Huang, Qingwen Bu, Jia Zeng, Tianyu Li, Hang Qiu, Hongzi Zhu, Minyi Guo, Yu Qiao, Hongyang Li. (2024)<br><strong>Embodied Understanding of Driving Scenarios</strong><br><button class=copy-to-clipboard title="Embodied Understanding of Driving Scenarios" index=79>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-79 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04593v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04593v1.pdf filename=2403.04593v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Embodied scene understanding serves as the cornerstone for autonomous agents to perceive, interpret, and respond to open driving scenarios. Such understanding is typically founded upon <b>Vision-Language</b> Models (VLMs). Nevertheless, existing VLMs are restricted to the 2D domain, devoid of spatial awareness and long-horizon extrapolation proficiencies. We revisit the key aspects of autonomous driving and formulate appropriate rubrics. Hereby, we introduce the Embodied Language Model (ELM), a comprehensive framework tailored for agents&rsquo; understanding of driving scenes with large spatial and temporal spans. ELM incorporates space-aware pre-training to endow the agent with robust spatial localization capabilities. Besides, the model employs time-aware token selection to accurately inquire about temporal cues. We instantiate ELM on the reformulated multi-faced <b>benchmark,</b> and it surpasses previous state-of-the-art approaches in all aspects. All code, data, and models will be publicly shared.</p></p class="citation"></blockquote><h3 id=4158--80292-out-of-the-room-generalizing-event-based-dynamic-motion-segmentation-for-complex-scenes-stamatios-georgoulis-et-al-2024>(41/58 | 80/292) Out of the Room: Generalizing Event-Based Dynamic Motion Segmentation for Complex Scenes (Stamatios Georgoulis et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Stamatios Georgoulis, Weining Ren, Alfredo Bochicchio, Daniel Eckert, Yuanyou Li, Abel Gawel. (2024)<br><strong>Out of the Room: Generalizing Event-Based Dynamic Motion Segmentation for Complex Scenes</strong><br><button class=copy-to-clipboard title="Out of the Room: Generalizing Event-Based Dynamic Motion Segmentation for Complex Scenes" index=80>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-80 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04562v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04562v1.pdf filename=2403.04562v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Rapid and reliable identification of dynamic scene parts, also known as motion segmentation, is a key challenge for mobile sensors. Contemporary RGB camera-based methods rely on modeling camera and scene properties however, are often under-constrained and fall short in unknown categories. Event cameras have the potential to overcome these limitations, but corresponding methods have only been demonstrated in smaller-scale indoor environments with simplified dynamic objects. This work presents an event-based method for class-agnostic motion segmentation that can successfully be deployed across complex large-scale outdoor environments too. To this end, we introduce a novel divide-and-conquer pipeline that combines: (a) ego-motion compensated events, computed via a scene understanding module that predicts monocular depth and camera pose as auxiliary tasks, and (b) optical flow from a dedicated optical flow module. These intermediate representations are then fed into a segmentation module that predicts motion segmentation masks. A novel <b>transformer-based</b> temporal attention module in the segmentation module builds correlations across adjacent &lsquo;frames&rsquo; to get temporally consistent segmentation masks. Our method sets the new state-of-the-art on the classic EV-IMO <b>benchmark</b> (indoors), where we achieve improvements of 2.19 moving object IoU (2.22 mIoU) and 4.52 point IoU respectively, as well as on a newly-generated motion segmentation and tracking <b>benchmark</b> (outdoors) based on the DSEC event dataset, termed DSEC-MOTS, where we show improvement of 12.91 moving object IoU.</p></p class="citation"></blockquote><h3 id=4258--81292-active-generalized-category-discovery-shijie-ma-et-al-2024>(42/58 | 81/292) Active Generalized Category Discovery (Shijie Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shijie Ma, Fei Zhu, Zhun Zhong, Xu-Yao Zhang, Cheng-Lin Liu. (2024)<br><strong>Active Generalized Category Discovery</strong><br><button class=copy-to-clipboard title="Active Generalized Category Discovery" index=81>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-81 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Active Learning, Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04272v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04272v1.pdf filename=2403.04272v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Generalized Category Discovery (GCD) is a pragmatic and challenging open-world task, which endeavors to cluster unlabeled samples from both novel and old classes, leveraging some labeled data of old classes. Given that knowledge learned from old classes is not fully transferable to new classes, and that novel categories are fully unlabeled, GCD inherently faces intractable problems, including imbalanced classification performance and inconsistent confidence between old and new classes, especially in the low-labeling regime. Hence, some annotations of new classes are deemed necessary. However, labeling new classes is extremely costly. To address this issue, we take the spirit of <b>active</b> <b>learning</b> and propose a new setting called <b>Active</b> <b>Generalized</b> Category Discovery (AGCD). The goal is to improve the performance of GCD by actively selecting a limited amount of valuable samples for labeling from the oracle. To solve this problem, we devise an adaptive sampling strategy, which jointly considers novelty, informativeness and diversity to adaptively select novel samples with proper uncertainty. However, owing to the varied orderings of label indices caused by the <b>clustering</b> of novel classes, the queried labels are not directly applicable to subsequent training. To overcome this issue, we further propose a stable label mapping algorithm that transforms ground truth labels to the label space of the classifier, thereby ensuring consistent training across different <b>active</b> <b>selection</b> stages. Our method achieves state-of-the-art performance on both generic and fine-grained datasets. Our code is available at <a href=https://github.com/mashijie1028/ActiveGCD>https://github.com/mashijie1028/ActiveGCD</a></p></p class="citation"></blockquote><h3 id=4358--82292-a-spatiotemporal-style-transfer-algorithm-for-dynamic-visual-stimulus-generation-antonino-greco-et-al-2024>(43/58 | 82/292) A spatiotemporal style transfer algorithm for dynamic visual stimulus generation (Antonino Greco et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Antonino Greco, Markus Siegel. (2024)<br><strong>A spatiotemporal style transfer algorithm for dynamic visual stimulus generation</strong><br><button class=copy-to-clipboard title="A spatiotemporal style transfer algorithm for dynamic visual stimulus generation" index=82>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-82 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV, q-bio-NC<br>Keyword Score: 10<br>Keywords: Style Transfer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04940v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04940v1.pdf filename=2403.04940v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Understanding how visual information is encoded in biological and artificial systems often requires vision scientists to generate appropriate stimuli to test specific hypotheses. Although deep neural network models have revolutionized the field of image generation with methods such as image <b>style</b> <b>transfer,</b> available methods for video generation are scarce. Here, we introduce the Spatiotemporal <b>Style</b> <b>Transfer</b> (STST) algorithm, a dynamic visual stimulus generation framework that allows powerful manipulation and synthesis of video stimuli for vision research. It is based on a two-stream deep neural network model that factorizes spatial and temporal features to generate dynamic visual stimuli whose model layer activations are matched to those of input videos. As an example, we show that our algorithm enables the generation of model metamers, dynamic stimuli whose layer activations within our two-stream model are matched to those of natural videos. We show that these generated stimuli match the low-level spatiotemporal features of their natural counterparts but lack their high-level semantic features, making it a powerful paradigm to study object recognition. Late layer activations in deep vision models exhibited a lower similarity between natural and metameric stimuli compared to early layers, confirming the lack of high-level information in the generated stimuli. Finally, we use our generated stimuli to probe the representational capabilities of predictive coding deep networks. These results showcase potential applications of our algorithm as a versatile tool for dynamic stimulus generation in vision science.</p></p class="citation"></blockquote><h3 id=4458--83292-efficient-loftr-semi-dense-local-feature-matching-with-sparse-like-speed-yifan-wang-et-al-2024>(44/58 | 83/292) Efficient LoFTR: Semi-Dense Local Feature Matching with Sparse-Like Speed (Yifan Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yifan Wang, Xingyi He, Sida Peng, Dongli Tan, Xiaowei Zhou. (2024)<br><strong>Efficient LoFTR: Semi-Dense Local Feature Matching with Sparse-Like Speed</strong><br><button class=copy-to-clipboard title="Efficient LoFTR: Semi-Dense Local Feature Matching with Sparse-Like Speed" index=83>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-83 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04765v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04765v2.pdf filename=2403.04765v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a novel method for efficiently producing semi-dense matches across images. Previous detector-free matcher LoFTR has shown remarkable matching capability in handling large-viewpoint change and texture-poor scenarios but suffers from low efficiency. We revisit its design choices and derive multiple improvements for both efficiency and accuracy. One key observation is that performing the <b>transformer</b> over the entire feature map is redundant due to shared local information, therefore we propose an aggregated attention mechanism with adaptive token selection for efficiency. Furthermore, we find spatial variance exists in LoFTR&rsquo;s fine correlation module, which is adverse to matching accuracy. A novel two-stage correlation layer is proposed to achieve accurate subpixel correspondences for accuracy improvement. Our efficiency optimized model is $\sim 2.5\times$ faster than LoFTR which can even surpass state-of-the-art efficient sparse matching pipeline SuperPoint + LightGlue. Moreover, extensive experiments show that our method can achieve higher accuracy compared with competitive semi-dense matchers, with considerable efficiency benefits. This opens up exciting prospects for large-scale or latency-sensitive applications such as image retrieval and 3D reconstruction. Project page: <a href=https://zju3dv.github.io/efficientloftr>https://zju3dv.github.io/efficientloftr</a>.</p></p class="citation"></blockquote><h3 id=4558--84292-i-cant-believe-its-not-scene-flow-ishan-khatri-et-al-2024>(45/58 | 84/292) I Can&rsquo;t Believe It&rsquo;s Not Scene Flow! (Ishan Khatri et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ishan Khatri, Kyle Vedder, Neehar Peri, Deva Ramanan, James Hays. (2024)<br><strong>I Can&rsquo;t Believe It&rsquo;s Not Scene Flow!</strong><br><button class=copy-to-clipboard title="I Can't Believe It's Not Scene Flow!" index=84>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-84 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04739v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04739v1.pdf filename=2403.04739v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Current scene flow methods broadly fail to describe motion on small objects, and current scene flow evaluation protocols hide this failure by averaging over many points, with most drawn larger objects. To fix this evaluation failure, we propose a new evaluation protocol, Bucket Normalized EPE, which is class-aware and speed-normalized, enabling contextualized error comparisons between object types that move at vastly different speeds. To highlight current method failures, we propose a frustratingly simple <b>supervised</b> scene flow baseline, TrackFlow, built by bolting a high-quality pretrained detector (trained using many class rebalancing techniques) onto a simple tracker, that produces state-of-the-art performance on current standard evaluations and large improvements over prior art on our new evaluation. Our results make it clear that all scene flow evaluations must be class and speed aware, and <b>supervised</b> scene flow methods must address point class imbalances. We release the evaluation code publicly at <a href=https://github.com/kylevedder/BucketedSceneFlowEval>https://github.com/kylevedder/BucketedSceneFlowEval</a>.</p></p class="citation"></blockquote><h3 id=4658--85292-faster-neighborhood-attention-reducing-the-on2-cost-of-self-attention-at-the-threadblock-level-ali-hassani-et-al-2024>(46/58 | 85/292) Faster Neighborhood Attention: Reducing the O(n^2) Cost of Self Attention at the Threadblock Level (Ali Hassani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ali Hassani, Wen-Mei Hwu, Humphrey Shi. (2024)<br><strong>Faster Neighborhood Attention: Reducing the O(n^2) Cost of Self Attention at the Threadblock Level</strong><br><button class=copy-to-clipboard title="Faster Neighborhood Attention: Reducing the O(n^2) Cost of Self Attention at the Threadblock Level" index=85>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-85 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 10<br>Keywords: Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04690v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04690v1.pdf filename=2403.04690v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neighborhood attention reduces the cost of self attention by restricting each token&rsquo;s attention span to its nearest neighbors. This restriction, parameterized by a window size and dilation factor, draws a spectrum of possible attention patterns between linear projection and self attention. Neighborhood attention, and more generally sliding window attention patterns, have long been bounded by infrastructure, particularly in higher-rank spaces (2-D and 3-D), calling for the development of custom kernels, which have been limited in either functionality, or performance, if not both. In this work, we first show that neighborhood attention can be represented as a batched GEMM problem, similar to standard attention, and implement it for 1-D and 2-D neighborhood attention. These kernels on average provide 895% and 272% improvement in full precision latency compared to existing naive kernels for 1-D and 2-D neighborhood attention respectively. We find certain inherent inefficiencies in all unfused neighborhood attention kernels that bound their performance and lower-precision scalability. We also developed fused neighborhood attention; an adaptation of fused dot-product attention kernels that allow fine-grained control over attention across different spatial axes. Known for reducing the quadratic time complexity of self attention to a linear complexity, neighborhood attention can now enjoy a reduced and constant memory footprint, and record-breaking half precision latency. We observe that our fused kernels successfully circumvent some of the unavoidable inefficiencies in unfused implementations. While our unfused GEMM-based kernels only improve half precision performance compared to naive kernels by an average of 496% and 113% in 1-D and 2-D problems respectively, our fused kernels improve naive kernels by an average of 1607% and 581% in 1-D and 2-D problems respectively.</p></p class="citation"></blockquote><h3 id=4758--86292-dynamic-cross-attention-for-audio-visual-person-verification-r-gnana-praveen-et-al-2024>(47/58 | 86/292) Dynamic Cross Attention for Audio-Visual Person Verification (R. Gnana Praveen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>R. Gnana Praveen, Jahangir Alam. (2024)<br><strong>Dynamic Cross Attention for Audio-Visual Person Verification</strong><br><button class=copy-to-clipboard title="Dynamic Cross Attention for Audio-Visual Person Verification" index=86>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-86 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs-SD, cs.CV, eess-AS<br>Keyword Score: 10<br>Keywords: Graph Attention Networks<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04661v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04661v2.pdf filename=2403.04661v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Although person or identity verification has been predominantly explored using individual modalities such as face and voice, audio-visual fusion has recently shown immense potential to outperform unimodal approaches. Audio and visual modalities are often expected to pose strong complementary relationships, which plays a crucial role in effective audio-visual fusion. However, they may not always strongly complement each other, they may also exhibit weak complementary relationships, resulting in poor audio-visual feature representations. In this paper, we propose a Dynamic Cross-Attention (DCA) model that can dynamically select the cross-attended or unattended features on the fly based on the strong or weak complementary relationships, respectively, across audio and visual modalities. In particular, a conditional <b>gating</b> layer is designed to evaluate the contribution of the cross-attention mechanism and choose cross-attended features only when they exhibit strong complementary relationships, otherwise unattended features. Extensive experiments are conducted on the Voxceleb1 dataset to demonstrate the robustness of the proposed model. Results indicate that the proposed model consistently improves the performance on multiple variants of cross-attention while outperforming the state-of-the-art methods.</p></p class="citation"></blockquote><h3 id=4858--87292-explainable-face-verification-via-feature-guided-gradient-backpropagation-yuhang-lu-et-al-2024>(48/58 | 87/292) Explainable Face Verification via Feature-Guided Gradient Backpropagation (Yuhang Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuhang Lu, Zewei Xu, Touradj Ebrahimi. (2024)<br><strong>Explainable Face Verification via Feature-Guided Gradient Backpropagation</strong><br><button class=copy-to-clipboard title="Explainable Face Verification via Feature-Guided Gradient Backpropagation" index=87>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-87 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV<br>Keyword Score: 10<br>Keywords: Face Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04549v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04549v1.pdf filename=2403.04549v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent years have witnessed significant advancement in <b>face</b> <b>recognition</b> (FR) techniques, with their applications widely spread in people&rsquo;s lives and security-sensitive areas. There is a growing need for reliable interpretations of decisions of such systems. Existing studies relying on various mechanisms have investigated the usage of saliency maps as an explanation approach, but suffer from different limitations. This paper first explores the spatial relationship between <b>face</b> <b>image</b> and its deep representation via gradient backpropagation. Then a new explanation approach FGGB has been conceived, which provides precise and insightful similarity and dissimilarity saliency maps to explain the &ldquo;Accept&rdquo; and &ldquo;Reject&rdquo; decision of an FR system. Extensive visual presentation and quantitative measurement have shown that FGGB achieves superior performance in both similarity and dissimilarity maps when compared to current state-of-the-art explainable <b>face</b> <b>verification</b> approaches.</p></p class="citation"></blockquote><h3 id=4958--88292-friendnet-detection-friendly-dehazing-network-yihua-fan-et-al-2024>(49/58 | 88/292) FriendNet: Detection-Friendly Dehazing Network (Yihua Fan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yihua Fan, Yongzhen Wang, Mingqiang Wei, Fu Lee Wang, Haoran Xie. (2024)<br><strong>FriendNet: Detection-Friendly Dehazing Network</strong><br><button class=copy-to-clipboard title="FriendNet: Detection-Friendly Dehazing Network" index=88>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-88 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Object Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04443v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04443v1.pdf filename=2403.04443v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Adverse weather conditions often impair the quality of captured images, inevitably inducing cutting-edge <b>object</b> <b>detection</b> models for advanced driver assistance systems (ADAS) and autonomous driving. In this paper, we raise an intriguing question: can the combination of image restoration and <b>object</b> <b>detection</b> enhance detection performance in adverse weather conditions? To answer it, we propose an effective architecture that bridges image dehazing and <b>object</b> <b>detection</b> together via guidance information and task-driven learning to achieve detection-friendly dehazing, termed FriendNet. FriendNet aims to deliver both high-quality perception and high detection capacity. Different from existing efforts that intuitively treat image dehazing as pre-processing, FriendNet establishes a positive correlation between these two tasks. Clean features generated by the dehazing network potentially contribute to improvements in <b>object</b> <b>detection</b> performance. Conversely, <b>object</b> <b>detection</b> crucially guides the learning process of the image dehazing network under the task-driven learning scheme. We shed light on how downstream tasks can guide upstream dehazing processes, considering both network architecture and learning objectives. We design Guidance Fusion Block (GFB) and Guidance Attention Block (GAB) to facilitate the integration of detection information into the network. Furthermore, the incorporation of the detection task loss aids in refining the optimization process. Additionally, we introduce a new Physics-aware Feature Enhancement Block (PFEB), which integrates physics-based priors to enhance the feature extraction and representation capabilities. Extensive experiments on synthetic and real-world datasets demonstrate the superiority of our method over state-of-the-art methods on both image quality and detection precision. Our source code is available at <a href=https://github.com/fanyihua0309/FriendNet>https://github.com/fanyihua0309/FriendNet</a>.</p></p class="citation"></blockquote><h3 id=5058--89292-stabledrag-stable-dragging-for-point-based-image-editing-yutao-cui-et-al-2024>(50/58 | 89/292) StableDrag: Stable Dragging for Point-based Image Editing (Yutao Cui et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yutao Cui, Xiaotong Zhao, Guozhen Zhang, Shengming Cao, Kai Ma, Limin Wang. (2024)<br><strong>StableDrag: Stable Dragging for Point-based Image Editing</strong><br><button class=copy-to-clipboard title="StableDrag: Stable Dragging for Point-based Image Editing" index=89>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-89 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04437v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04437v1.pdf filename=2403.04437v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Point-based image editing has attracted remarkable attention since the emergence of DragGAN. Recently, DragDiffusion further pushes forward the generative quality via adapting this dragging technique to <b>diffusion</b> <b>models.</b> Despite these great success, this dragging scheme exhibits two major drawbacks, namely inaccurate point tracking and incomplete motion supervision, which may result in unsatisfactory dragging outcomes. To tackle these issues, we build a stable and precise drag-based editing framework, coined as StableDrag, by designing a discirminative point tracking method and a confidence-based latent enhancement strategy for motion supervision. The former allows us to precisely locate the updated handle points, thereby boosting the stability of long-range manipulation, while the latter is responsible for guaranteeing the optimized latent as high-quality as possible across all the manipulation steps. Thanks to these unique designs, we instantiate two types of image editing models including StableDrag-GAN and StableDrag-Diff, which attains more stable dragging performance, through extensive qualitative experiments and quantitative assessment on DragBench.</p></p class="citation"></blockquote><h3 id=5158--90292-single-to-dual-view-adaptation-for-egocentric-3d-hand-pose-estimation-ruicong-liu-et-al-2024>(51/58 | 90/292) Single-to-Dual-View Adaptation for Egocentric 3D Hand Pose Estimation (Ruicong Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruicong Liu, Takehiko Ohkawa, Mingfang Zhang, Yoichi Sato. (2024)<br><strong>Single-to-Dual-View Adaptation for Egocentric 3D Hand Pose Estimation</strong><br><button class=copy-to-clipboard title="Single-to-Dual-View Adaptation for Egocentric 3D Hand Pose Estimation" index=90>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-90 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04381v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04381v2.pdf filename=2403.04381v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The pursuit of accurate 3D hand pose estimation stands as a keystone for understanding human activity in the realm of egocentric vision. The majority of existing estimation methods still rely on single-view images as input, leading to potential limitations, e.g., limited field-of-view and ambiguity in depth. To address these problems, adding another camera to better capture the shape of hands is a practical direction. However, existing multi-view hand pose estimation methods suffer from two main drawbacks: 1) Requiring multi-view annotations for training, which are expensive. 2) During testing, the model becomes inapplicable if camera parameters/layout are not the same as those used in training. In this paper, we propose a novel Single-to-Dual-view adaptation (S2DHand) solution that adapts a pre-trained single-view estimator to dual views. Compared with existing multi-view training methods, 1) our adaptation process is <b>unsupervised,</b> eliminating the need for multi-view annotation. 2) Moreover, our method can handle arbitrary dual-view pairs with unknown camera parameters, making the model applicable to diverse camera settings. Specifically, S2DHand is built on certain stereo constraints, including pair-wise cross-view consensus and invariance of transformation between both views. These two stereo constraints are used in a complementary manner to generate pseudo-labels, allowing reliable adaptation. Evaluation results reveal that S2DHand achieves significant improvements on arbitrary camera pairs under both in-dataset and cross-dataset settings, and outperforms existing adaptation methods with leading performance. Project page: <a href=https://github.com/MickeyLLG/S2DHand>https://github.com/MickeyLLG/S2DHand</a>.</p></p class="citation"></blockquote><h3 id=5258--91292-video-driven-animation-of-neural-head-avatars-wolfgang-paier-et-al-2024>(52/58 | 91/292) Video-Driven Animation of Neural Head Avatars (Wolfgang Paier et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wolfgang Paier, Paul Hinzer, Anna Hilsmann, Peter Eisert. (2024)<br><strong>Video-Driven Animation of Neural Head Avatars</strong><br><button class=copy-to-clipboard title="Video-Driven Animation of Neural Head Avatars" index=91>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-91 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: LSTM<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04380v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04380v1.pdf filename=2403.04380v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a new approach for video-driven animation of high-quality neural 3D head models, addressing the challenge of person-independent animation from video input. Typically, high-quality generative models are learned for specific individuals from multi-view video footage, resulting in person-specific latent representations that drive the generation process. In order to achieve person-independent animation from video input, we introduce an <b>LSTM-based</b> animation network capable of translating person-independent expression features into personalized animation parameters of person-specific 3D head models. Our approach combines the advantages of personalized head models (high quality and realism) with the convenience of video-driven animation employing multi-person facial performance capture. We demonstrate the effectiveness of our approach on synthesized animations with high quality based on different source videos as well as an ablation study.</p></p class="citation"></blockquote><h3 id=5358--92292-spatiotemporal-pooling-on-appropriate-topological-maps-represented-as-two-dimensional-images-for-eeg-classification-takuto-fukushima-et-al-2024>(53/58 | 92/292) Spatiotemporal Pooling on Appropriate Topological Maps Represented as Two-Dimensional Images for EEG Classification (Takuto Fukushima et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Takuto Fukushima, Ryusuke Miyamoto. (2024)<br><strong>Spatiotemporal Pooling on Appropriate Topological Maps Represented as Two-Dimensional Images for EEG Classification</strong><br><button class=copy-to-clipboard title="Spatiotemporal Pooling on Appropriate Topological Maps Represented as Two-Dimensional Images for EEG Classification" index=92>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-92 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04353v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04353v1.pdf filename=2403.04353v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Motor imagery classification based on electroencephalography (EEG) signals is one of the most important brain-computer interface applications, although it needs further improvement. Several methods have attempted to obtain useful information from EEG signals by using recent deep learning techniques such as <b>transformers.</b> To improve the classification accuracy, this study proposes a novel EEG-based motor imagery classification method with three key features: generation of a topological map represented as a two-dimensional image from EEG signals with coordinate transformation based on t-SNE, use of the InternImage to extract spatial features, and use of spatiotemporal pooling inspired by PoolFormer to exploit spatiotemporal information concealed in a sequence of EEG images. Experimental results using the PhysioNet EEG Motor Movement/Imagery dataset showed that the proposed method achieved the best classification accuracy of 88.57%, 80.65%, and 70.17% on two-, three-, and four-class motor imagery tasks in cross-individual validation.</p></p class="citation"></blockquote><h3 id=5458--93292-lors-low-rank-residual-structure-for-parameter-efficient-network-stacking-jialin-li-et-al-2024>(54/58 | 93/292) LORS: Low-rank Residual Structure for Parameter-Efficient Network Stacking (Jialin Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jialin Li, Qiang Nie, Weifu Fu, Yuhuan Lin, Guangpin Tao, Yong Liu, Chengjie Wang. (2024)<br><strong>LORS: Low-rank Residual Structure for Parameter-Efficient Network Stacking</strong><br><button class=copy-to-clipboard title="LORS: Low-rank Residual Structure for Parameter-Efficient Network Stacking" index=93>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-93 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04303v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04303v1.pdf filename=2403.04303v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep learning models, particularly those based on <b>transformers,</b> often employ numerous stacked structures, which possess identical architectures and perform similar functions. While effective, this stacking paradigm leads to a substantial increase in the number of parameters, posing challenges for practical applications. In today&rsquo;s landscape of increasingly large models, stacking depth can even reach dozens, further exacerbating this issue. To mitigate this problem, we introduce LORS (LOw-rank Residual Structure). LORS allows stacked modules to share the majority of parameters, requiring a much smaller number of unique ones per module to match or even surpass the performance of using entirely distinct ones, thereby significantly reducing parameter usage. We validate our method by applying it to the stacked decoders of a query-based object detector, and conduct extensive experiments on the widely used MS COCO dataset. Experimental results demonstrate the effectiveness of our method, as even with a 70% reduction in the parameters of the decoder, our method still enables the model to achieve comparable or</p></p class="citation"></blockquote><h3 id=5558--94292-map-mask-pruning-for-source-free-model-intellectual-property-protection-boyang-peng-et-al-2024>(55/58 | 94/292) MAP: MAsk-Pruning for Source-Free Model Intellectual Property Protection (Boyang Peng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Boyang Peng, Sanqing Qu, Yong Wu, Tianpei Zou, Lianghua He, Alois Knoll, Guang Chen, changjun jiang. (2024)<br><strong>MAP: MAsk-Pruning for Source-Free Model Intellectual Property Protection</strong><br><button class=copy-to-clipboard title="MAP: MAsk-Pruning for Source-Free Model Intellectual Property Protection" index=94>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-94 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Pruning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04149v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04149v1.pdf filename=2403.04149v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep learning has achieved remarkable progress in various applications, heightening the importance of safeguarding the intellectual property (IP) of well-trained models. It entails not only authorizing usage but also ensuring the deployment of models in authorized data domains, i.e., making models exclusive to certain target domains. Previous methods necessitate concurrent access to source training data and target unauthorized data when performing IP protection, making them risky and inefficient for decentralized private data. In this paper, we target a practical setting where only a well-trained source model is available and investigate how we can realize IP protection. To achieve this, we propose a novel MAsk <b>Pruning</b> (MAP) framework. MAP stems from an intuitive hypothesis, i.e., there are target-related parameters in a well-trained model, locating and <b>pruning</b> them is the key to IP protection. Technically, MAP freezes the source model and learns a target-specific binary mask to prevent unauthorized data usage while minimizing performance degradation on authorized data. Moreover, we introduce a new metric aimed at achieving a better balance between source and target performance degradation. To verify the effectiveness and versatility, we have evaluated MAP in a variety of scenarios, including vanilla source-available, practical source-free, and challenging data-free. Extensive experiments indicate that MAP yields new state-of-the-art performance.</p></p class="citation"></blockquote><h3 id=5658--95292-sdpl-shifting-dense-partition-learning-for-uav-view-geo-localization-quan-chen-et-al-2024>(56/58 | 95/292) SDPL: Shifting-Dense Partition Learning for UAV-View Geo-Localization (Quan Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Quan Chen, Tingyu Wang, Zihao Yang, Haoran Li, Rongfeng Lu, Yaoqi Sun, Bolun Zheng, Chenggang Yan. (2024)<br><strong>SDPL: Shifting-Dense Partition Learning for UAV-View Geo-Localization</strong><br><button class=copy-to-clipboard title="SDPL: Shifting-Dense Partition Learning for UAV-View Geo-Localization" index=95>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-95 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 8<br>Keywords: Benchmarking, Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04172v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04172v1.pdf filename=2403.04172v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Cross-view geo-localization aims to match images of the same target from different platforms, e.g., drone and satellite. It is a challenging task due to the changing both appearance of targets and environmental content from different views. Existing methods mainly focus on digging more comprehensive information through feature maps segmentation, while inevitably destroy the image structure and are sensitive to the shifting and scale of the target in the query. To address the above issues, we introduce a simple yet effective part-based <b>representation</b> <b>learning,</b> called shifting-dense partition learning (SDPL). Specifically, we propose the dense partition strategy (DPS), which divides the image into multiple parts to explore contextual-information while explicitly maintain the global structure. To handle scenarios with non-centered targets, we further propose the shifting-fusion strategy, which generates multiple sets of parts in parallel based on various segmentation centers and then adaptively fuses all features to select the best partitions. Extensive experiments show that our SDPL is robust to position shifting and scale variations, and achieves competitive performance on two prevailing <b>benchmarks,</b> i.e., University-1652 and SUES-200.</p></p class="citation"></blockquote><h3 id=5758--96292-thats-my-point-compact-object-centric-lidar-pose-estimation-for-large-scale-outdoor-localisation-georgi-pramatarov-et-al-2024>(57/58 | 96/292) That&rsquo;s My Point: Compact Object-centric LiDAR Pose Estimation for Large-scale Outdoor Localisation (Georgi Pramatarov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Georgi Pramatarov, Matthew Gadd, Paul Newman, Daniele De Martini. (2024)<br><strong>That&rsquo;s My Point: Compact Object-centric LiDAR Pose Estimation for Large-scale Outdoor Localisation</strong><br><button class=copy-to-clipboard title="That's My Point: Compact Object-centric LiDAR Pose Estimation for Large-scale Outdoor Localisation" index=96>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-96 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV<br>Keyword Score: 3<br>Keywords: Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04755v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04755v1.pdf filename=2403.04755v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper is about 3D pose estimation on LiDAR scans with extremely minimal storage requirements to enable scalable mapping and localisation. We achieve this by <b>clustering</b> all points of segmented scans into semantic objects and representing them only with their respective centroid and semantic class. In this way, each LiDAR scan is reduced to a compact collection of four-number vectors. This abstracts away important structural information from the scenes, which is crucial for traditional registration approaches. To mitigate this, we introduce an object-matching network based on self- and cross-correlation that captures geometric and semantic relationships between entities. The respective matches allow us to recover the relative transformation between scans through weighted Singular Value Decomposition (SVD) and RANdom SAmple Consensus (RANSAC). We demonstrate that such representation is sufficient for metric localisation by registering point clouds taken under different viewpoints on the KITTI dataset, and at different periods of time localising between KITTI and KITTI-360. We achieve accurate metric estimates comparable with state-of-the-art methods with almost half the representation size, specifically 1.33 kB on average.</p></p class="citation"></blockquote><h3 id=5858--97292-magr-manifold-aligned-graph-regularization-for-continual-action-quality-assessment-kanglei-zhou-et-al-2024>(58/58 | 97/292) MAGR: Manifold-Aligned Graph Regularization for Continual Action Quality Assessment (Kanglei Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kanglei Zhou, Liyuan Wang, Xingxing Zhang, Hubert P. H. Shum, Frederick W. B. Li, Jianguo Li, Xiaohui Liang. (2024)<br><strong>MAGR: Manifold-Aligned Graph Regularization for Continual Action Quality Assessment</strong><br><button class=copy-to-clipboard title="MAGR: Manifold-Aligned Graph Regularization for Continual Action Quality Assessment" index=97>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-97 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04398v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04398v1.pdf filename=2403.04398v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Action Quality Assessment (AQA) evaluates diverse skills but models struggle with non-stationary data. We propose Continual AQA (CAQA) to refine models using sparse new data. Feature replay preserves memory without storing raw inputs. However, the misalignment between static old features and the dynamically changing feature manifold causes severe catastrophic forgetting. To address this novel problem, we propose Manifold-Aligned <b>Graph</b> Regularization (MAGR), which first aligns deviated old features to the current feature manifold, ensuring representation consistency. It then constructs a <b>graph</b> jointly arranging old and new features aligned with quality scores. Experiments show MAGR outperforms recent strong baselines with up to 6.56%, 5.66%, 15.64%, and 9.05% correlation gains on the MTL-AQA, FineDiving, UNLV-Dive, and JDM-MSA split datasets, respectively. This validates MAGR for continual assessment challenges arising from non-stationary skill variations.</p></p class="citation"></blockquote><h2 id=csir-11>cs.IR (11)</h2><h3 id=111--98292-can-small-language-models-be-good-reasoners-for-sequential-recommendation-yuling-wang-et-al-2024>(1/11 | 98/292) Can Small Language Models be Good Reasoners for Sequential Recommendation? (Yuling Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuling Wang, Changxin Tian, Binbin Hu, Yanhua Yu, Ziqi Liu, Zhiqiang Zhang, Jun Zhou, Liang Pang, Xiao Wang. (2024)<br><strong>Can Small Language Models be Good Reasoners for Sequential Recommendation?</strong><br><button class=copy-to-clipboard title="Can Small Language Models be Good Reasoners for Sequential Recommendation?" index=98>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-98 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-CL, cs-IR, cs-LG, cs.IR<br>Keyword Score: 100<br>Keywords: Knowledge Distillation, Knowledge Distillation, Knowledge Distillation, Recommendation, Recommender System, ChatGPT, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04260v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04260v1.pdf filename=2403.04260v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> open up new horizons for sequential <b>recommendations,</b> owing to their remarkable language comprehension and generation capabilities. However, there are still numerous challenges that should be addressed to successfully implement sequential <b>recommendations</b> empowered by <b>LLMs.</b> Firstly, user behavior patterns are often complex, and relying solely on one-step <b>reasoning</b> from <b>LLMs</b> may lead to incorrect or task-irrelevant responses. Secondly, the prohibitively resource requirements of <b>LLM</b> (e.g., <b>ChatGPT-175B)</b> are overwhelmingly high and impractical for real sequential <b>recommender</b> <b>systems.</b> In this paper, we propose a novel Step-by-step <b>knowLedge</b> <b>dIstillation</b> fraMework for <b>recommendation</b> (SLIM), paving a promising path for sequential <b>recommenders</b> <b>to</b> enjoy the exceptional <b>reasoning</b> capabilities of <b>LLMs</b> in a &ldquo;slim&rdquo; (i.e., resource-efficient) manner. We introduce CoT <b>prompting</b> based on user behavior sequences for the larger teacher model. The rationales generated by the teacher model are then utilized as labels to <b>distill</b> the downstream smaller student model (e.g., LLaMA2-7B). In this way, the student model acquires the step-by-step <b>reasoning</b> capabilities in <b>recommendation</b> tasks. We encode the generated rationales from the student model into a dense vector, which empowers <b>recommendation</b> in both ID-based and ID-agnostic scenarios. Extensive experiments demonstrate the effectiveness of SLIM over state-of-the-art baselines, and further analysis showcasing its ability to generate meaningful <b>recommendation</b> <b>reasoning</b> at affordable costs.</p></p class="citation"></blockquote><h3 id=211--99292-federated-recommendation-via-hybrid-retrieval-augmented-generation-huimin-zeng-et-al-2024>(2/11 | 99/292) Federated Recommendation via Hybrid Retrieval Augmented Generation (Huimin Zeng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Huimin Zeng, Zhenrui Yue, Qian Jiang, Dong Wang. (2024)<br><strong>Federated Recommendation via Hybrid Retrieval Augmented Generation</strong><br><button class=copy-to-clipboard title="Federated Recommendation via Hybrid Retrieval Augmented Generation" index=99>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-99 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-AI, cs-IR, cs.IR<br>Keyword Score: 93<br>Keywords: Benchmarking, Recommendation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, ChatGPT, GPT, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04256v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04256v1.pdf filename=2403.04256v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Federated <b>Recommendation</b> (FR) emerges as a novel paradigm that enables privacy-preserving <b>recommendations.</b> However, traditional FR systems usually represent users/items with discrete identities (IDs), suffering from performance degradation due to the data sparsity and heterogeneity in FR. On the other hand, <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> as recommenders have proven effective across various <b>recommendation</b> scenarios. Yet, <b>LLM-based</b> recommenders encounter challenges such as low inference efficiency and potential hallucination, compromising their performance in real-world scenarios. To this end, we propose <b>GPT-FedRec,</b> a federated <b>recommendation</b> framework leveraging <b>ChatGPT</b> and a novel hybrid <b>Retrieval</b> <b>Augmented</b> <b>Generation</b> <b>(RAG)</b> mechanism. <b>GPT-FedRec</b> is a two-stage solution. The first stage is a hybrid <b>retrieval</b> <b>process,</b> <b>mining</b> ID-based user patterns and text-based item features. Next, the retrieved results are converted into text <b>prompts</b> and fed into <b>GPT</b> for re-ranking. Our proposed hybrid <b>retrieval</b> <b>mechanism</b> <b>and</b> <b>LLM-based</b> re-rank aims to extract generalized features from data and exploit pretrained knowledge within <b>LLM,</b> overcoming data sparsity and heterogeneity in FR. In addition, the <b>RAG</b> approach also prevents <b>LLM</b> hallucination, improving the <b>recommendation</b> performance for real-world users. Experimental results on diverse <b>benchmark</b> datasets demonstrate the superior performance of <b>GPT-FedRec</b> against state-of-the-art baseline methods.</p></p class="citation"></blockquote><h3 id=311--100292-dgr-a-general-graph-desmoothing-framework-for-recommendation-via-global-and-local-perspectives-leilei-ding-et-al-2024>(3/11 | 100/292) DGR: A General Graph Desmoothing Framework for Recommendation via Global and Local Perspectives (Leilei Ding et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Leilei Ding, Dazhong Shen, Chao Wang, Tianfu Wang, Le Zhang, Hui Xiong, Yanyong Zhang. (2024)<br><strong>DGR: A General Graph Desmoothing Framework for Recommendation via Global and Local Perspectives</strong><br><button class=copy-to-clipboard title="DGR: A General Graph Desmoothing Framework for Recommendation via Global and Local Perspectives" index=100>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-100 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 76<br>Keywords: Graph Convolutional Network, Graph Convolutional Network, Message-Passing, Graph, Node Embedding, Benchmarking, Convolution, Convolutional Neural Network, Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04287v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04287v1.pdf filename=2403.04287v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>Convolutional</b> <b>Networks</b> <b>(GCNs)</b> have become pivotal in <b>recommendation</b> systems for learning user and item embeddings by leveraging the user-item interaction <b>graph&rsquo;s</b> <b>node</b> <b>information</b> and topology. However, these models often face the famous over-smoothing issue, leading to indistinct user and item embeddings and reduced personalization. Traditional desmoothing methods in <b>GCN-based</b> systems are model-specific, lacking a universal solution. This paper introduces a novel, model-agnostic approach named \textbf{D}esmoothing Framework for \textbf{G}CN-based \textbf{R}ecommendation Systems (\textbf{DGR}). It effectively addresses over-smoothing on general <b>GCN-based</b> <b>recommendation</b> models by considering both global and local perspectives. Specifically, we first introduce vector perturbations during each message passing layer to penalize the tendency of <b>node</b> <b>embeddings</b> approximating overly to be similar with the guidance of the global topological structure. Meanwhile, we further develop a tailored-design loss term for the readout embeddings to preserve the local collaborative relations between users and their neighboring items. In particular, items that exhibit a high correlation with neighboring items are also incorporated to enhance the local topological information. To validate our approach, we conduct extensive experiments on 5 <b>benchmark</b> datasets based on 5 well-known <b>GCN-based</b> <b>recommendation</b> models, demonstrating the effectiveness and generalization of our proposed framework.</p></p class="citation"></blockquote><h3 id=411--101292-aligning-gptrec-with-beyond-accuracy-goals-with-reinforcement-learning-aleksandr-petrov-et-al-2024>(4/11 | 101/292) Aligning GPTRec with Beyond-Accuracy Goals with Reinforcement Learning (Aleksandr Petrov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aleksandr Petrov, Craig Macdonald. (2024)<br><strong>Aligning GPTRec with Beyond-Accuracy Goals with Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Aligning GPTRec with Beyond-Accuracy Goals with Reinforcement Learning" index=101>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-101 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs-LG, cs.IR<br>Keyword Score: 40<br>Keywords: Recommendation, Recommender System, Reinforcement Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04875v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04875v1.pdf filename=2403.04875v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Adaptations of <b>Transformer</b> models, such as BERT4Rec and SASRec, achieve state-of-the-art performance in the sequential <b>recommendation</b> task according to accuracy-based metrics, such as NDCG. These models treat items as tokens and then utilise a score-and-rank approach (Top-K strategy), where the model first computes item scores and then ranks them according to this score. While this approach works well for accuracy-based metrics, it is hard to use it for optimising more complex beyond-accuracy metrics such as diversity. Recently, the GPTRec model, which uses a different Next-K strategy, has been proposed as an alternative to the Top-K models. In contrast with traditional Top-K <b>recommendations,</b> Next-K generates <b>recommendations</b> item-by-item and, therefore, can account for complex item-to-item interdependencies important for the beyond-accuracy measures. However, the original GPTRec paper focused only on accuracy in experiments and needed to address how to optimise the model for complex beyond-accuracy metrics. Indeed, training GPTRec for beyond-accuracy goals is challenging because the interaction training data available for training <b>recommender</b> <b>systems</b> typically needs to be aligned with beyond-accuracy <b>recommendation</b> goals. To solve the misalignment problem, we train GPTRec using a 2-stage approach: in the first stage, we use a teacher-student approach to train GPTRec, mimicking the behaviour of traditional Top-K models; in the second stage, we use <b>Reinforcement</b> <b>Learning</b> to align the model for beyond-accuracy goals. In particular, we experiment with increasing <b>recommendation</b> diversity and reducing popularity bias. Our experiments on two datasets show that in 3 out of 4 cases, GPTRec&rsquo;s Next-K generation approach offers a better tradeoff between accuracy and secondary metrics than classic greedy re-ranking techniques.</p></p class="citation"></blockquote><h3 id=511--102292-ducho-20-towards-a-more-up-to-date-feature-extraction-and-processing-framework-for-multimodal-recommendation-matteo-attimonelli-et-al-2024>(5/11 | 102/292) Ducho 2.0: Towards a More Up-to-Date Feature Extraction and Processing Framework for Multimodal Recommendation (Matteo Attimonelli et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Matteo Attimonelli, Danilo Danese, Daniele Malitesta, Claudio Pomo, Giuseppe Gassi, Tommaso Di Noia. (2024)<br><strong>Ducho 2.0: Towards a More Up-to-Date Feature Extraction and Processing Framework for Multimodal Recommendation</strong><br><button class=copy-to-clipboard title="Ducho 2.0: Towards a More Up-to-Date Feature Extraction and Processing Framework for Multimodal Recommendation" index=102>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-102 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 32<br>Keywords: Benchmarking, Benchmarking, Fine-tuning, Multi-modal, Multi-modal, Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04503v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04503v1.pdf filename=2403.04503v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we introduce Ducho 2.0, the latest stable version of our framework. Differently from Ducho, Ducho 2.0 offers a more personalized user experience with the definition and import of custom extraction models <b>fine-tuned</b> on specific tasks and datasets. Moreover, the new version is capable of extracting and processing features through <b>multimodal-by-design</b> large models. Notably, all these new features are supported by optimized data loading and storing to the local memory. To showcase the capabilities of Ducho 2.0, we demonstrate a complete <b>multimodal</b> <b>recommendation</b> pipeline, from the extraction/processing to the final <b>recommendation.</b> The idea is to provide practitioners and experienced scholars with a ready-to-use tool that, put on top of any <b>multimodal</b> <b>recommendation</b> framework, may permit them to run extensive <b>benchmarking</b> analyses. All materials are accessible at: \url{https://github.com/sisinflab/Ducho}.</p></p class="citation"></blockquote><h3 id=611--103292-benchmarking-news-recommendation-in-the-era-of-green-ai-qijiong-liu-et-al-2024>(6/11 | 103/292) Benchmarking News Recommendation in the Era of Green AI (Qijiong Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qijiong Liu, Jieming Zhu, Quanyu Dai, Xiao-Ming Wu. (2024)<br><strong>Benchmarking News Recommendation in the Era of Green AI</strong><br><button class=copy-to-clipboard title="Benchmarking News Recommendation in the Era of Green AI" index=103>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-103 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 26<br>Keywords: Benchmarking, Benchmarking, Recommendation, Recommender System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04736v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04736v1.pdf filename=2403.04736v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Over recent years, news <b>recommender</b> <b>systems</b> have gained significant attention in both academia and industry, emphasizing the need for a standardized <b>benchmark</b> to evaluate and compare the performance of these systems. Concurrently, Green AI advocates for reducing the energy consumption and environmental impact of machine learning. To address these concerns, we introduce the first Green AI <b>benchmarking</b> framework for news <b>recommendation,</b> known as GreenRec, and propose a metric for assessing the tradeoff between <b>recommendation</b> accuracy and efficiency. Our <b>benchmark</b> encompasses 30 base models and their variants, covering traditional end-to-end training paradigms as well as our proposed efficient only-encode-once (OLEO) paradigm. Through experiments consuming 2000 GPU hours, we observe that the OLEO paradigm achieves competitive accuracy compared to state-of-the-art end-to-end paradigms and delivers up to a 2992% improvement in sustainability metrics.</p></p class="citation"></blockquote><h3 id=711--104292-the-2nd-workshop-on-recommendation-with-generative-models-wenjie-wang-et-al-2024>(7/11 | 104/292) The 2nd Workshop on Recommendation with Generative Models (Wenjie Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenjie Wang, Yang Zhang, Xinyu Lin, Fuli Feng, Weiwen Liu, Yong Liu, Xiangyu Zhao, Wayne Xin Zhao, Yang Song, Xiangnan He. (2024)<br><strong>The 2nd Workshop on Recommendation with Generative Models</strong><br><button class=copy-to-clipboard title="The 2nd Workshop on Recommendation with Generative Models" index=104>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-104 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 20<br>Keywords: Recommendation, Recommender System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04399v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04399v1.pdf filename=2403.04399v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rise of generative models has driven significant advancements in <b>recommender</b> <b>systems,</b> leaving unique opportunities for enhancing users&rsquo; personalized <b>recommendations.</b> This workshop serves as a platform for researchers to explore and exchange innovative concepts related to the integration of generative models into <b>recommender</b> <b>systems.</b> It primarily focuses on five key perspectives: (i) improving <b>recommender</b> <b>algorithms,</b> (ii) generating personalized content, (iii) evolving the user-system interaction paradigm, (iv) enhancing trustworthiness checks, and (v) refining evaluation methodologies for generative <b>recommendations.</b> With generative models advancing rapidly, an increasing body of research is emerging in these domains, underscoring the timeliness and critical importance of this workshop. The related research will introduce innovative technologies to <b>recommender</b> <b>systems</b> and contribute to fresh challenges in both academia and industry. In the long term, this research direction has the potential to revolutionize the traditional <b>recommender</b> <b>paradigms</b> and foster the development of next-generation <b>recommender</b> <b>systems.</b></p></p class="citation"></blockquote><h3 id=811--105292-towards-robustness-analysis-of-e-commerce-ranking-system-ningfei-wang-et-al-2024>(8/11 | 105/292) Towards Robustness Analysis of E-Commerce Ranking System (Ningfei Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ningfei Wang, Yupin Huang, Han Cheng, Jiri Gesi, Xiaojie Wang, Vivek Mittal. (2024)<br><strong>Towards Robustness Analysis of E-Commerce Ranking System</strong><br><button class=copy-to-clipboard title="Towards Robustness Analysis of E-Commerce Ranking System" index=105>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-105 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 20<br>Keywords: Information Retrieval, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04257v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04257v1.pdf filename=2403.04257v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Information</b> <b>retrieval</b> (IR) is a pivotal component in various applications. Recent advances in machine learning (ML) have enabled the integration of ML algorithms into IR, particularly in ranking systems. While there is a plethora of research on the robustness of ML-based ranking systems, these studies largely neglect commercial e-commerce systems and fail to establish a connection between real-world and manipulated query relevance. In this paper, we present the first systematic measurement study on the robustness of e-commerce ranking systems. We define robustness as the consistency of ranking outcomes for semantically identical queries. To quantitatively analyze robustness, we propose a novel metric that considers both ranking position and item-specific <b>information</b> <b>that</b> are absent in existing metrics. Our <b>large-scale</b> <b>measurement</b> <b>study</b> with real-world data from e-commerce retailers reveals an open opportunity to measure and improve robustness since semantically identical queries often yield inconsistent ranking results. Based on our observations, we propose several solution directions to enhance robustness, such as the use of <b>Large</b> <b>Language</b> <b>Models.</b> Note that the issue of robustness discussed herein does not constitute an error or oversight. Rather, in scenarios where there exists a vast array of choices, it is feasible to present a multitude of products in various permutations, all of which could be equally appealing. However, this extensive selection may lead to customer confusion. As e-commerce retailers use various techniques to improve the quality of search results, we hope that this research offers valuable guidance for measuring the robustness of the ranking systems.</p></p class="citation"></blockquote><h3 id=911--106292-improving-retrieval-in-theme-specific-applications-using-a-corpus-topical-taxonomy-seongku-kang-et-al-2024>(9/11 | 106/292) Improving Retrieval in Theme-specific Applications using a Corpus Topical Taxonomy (SeongKu Kang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>SeongKu Kang, Shivam Agarwal, Bowen Jin, Dongha Lee, Hwanjo Yu, Jiawei Han. (2024)<br><strong>Improving Retrieval in Theme-specific Applications using a Corpus Topical Taxonomy</strong><br><button class=copy-to-clipboard title="Improving Retrieval in Theme-specific Applications using a Corpus Topical Taxonomy" index=106>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-106 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-AI, cs-IR, cs.IR<br>Keyword Score: 20<br>Keywords: Pre-trained Language Model, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04160v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04160v1.pdf filename=2403.04160v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Document retrieval has greatly benefited from the advancements of large-scale <b>pre-trained</b> <b>language</b> <b>models</b> <b>(PLMs).</b> However, their effectiveness is often limited in theme-specific applications for specialized areas or industries, due to unique terminologies, incomplete contexts of user queries, and specialized search intents. To capture the theme-specific information and improve retrieval, we propose to use a corpus topical taxonomy, which outlines the latent topic structure of the corpus while reflecting user-interested aspects. We introduce ToTER (Topical Taxonomy Enhanced Retrieval) framework, which identifies the central topics of queries and documents with the guidance of the taxonomy, and exploits their topical relatedness to supplement missing contexts. As a plug-and-play framework, ToTER can be flexibly employed to enhance various <b>PLM-based</b> retrievers. Through extensive quantitative, ablative, and exploratory experiments on two real-world datasets, we ascertain the benefits of using topical taxonomy for retrieval in theme-specific applications and demonstrate the effectiveness of ToTER.</p></p class="citation"></blockquote><h3 id=1011--107292-ssdrec-self-augmented-sequence-denoising-for-sequential-recommendation-chi-zhang-et-al-2024>(10/11 | 107/292) SSDRec: Self-Augmented Sequence Denoising for Sequential Recommendation (Chi Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chi Zhang, Qilong Han, Rui Chen, Xiangyu Zhao, Peng Tang, Hongtao Song. (2024)<br><strong>SSDRec: Self-Augmented Sequence Denoising for Sequential Recommendation</strong><br><button class=copy-to-clipboard title="SSDRec: Self-Augmented Sequence Denoising for Sequential Recommendation" index=107>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-107 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04278v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04278v1.pdf filename=2403.04278v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Traditional sequential <b>recommendation</b> methods assume that users&rsquo; sequence data is clean enough to learn accurate sequence representations to reflect user preferences. In practice, users&rsquo; sequences inevitably contain noise (e.g., accidental interactions), leading to incorrect reflections of user preferences. Consequently, some pioneer studies have explored modeling sequentiality and correlations in sequences to implicitly or explicitly reduce noise&rsquo;s influence. However, relying on only available intra-sequence information (i.e., sequentiality and correlations in a sequence) is insufficient and may result in over-denoising and under-denoising problems (OUPs), especially for short sequences. To improve reliability, we propose to augment sequences by inserting items before denoising. However, due to the data sparsity issue and computational costs, it is challenging to select proper items from the entire item universe to insert into proper positions in a target sequence. Motivated by the above observation, we propose a novel framework&ndash;Self-augmented Sequence Denoising for sequential <b>Recommendation</b> (SSDRec) with a three-stage learning paradigm to solve the above challenges. In the first stage, we empower SSDRec by a global relation encoder to learn multi-faceted inter-sequence relations in a data-driven manner. These relations serve as prior knowledge to guide subsequent stages. In the second stage, we devise a self-augmentation module to augment sequences to alleviate OUPs. Finally, we employ a hierarchical denoising module in the third stage to reduce the risk of false augmentations and pinpoint all noise in raw sequences. Extensive experiments on five real-world datasets demonstrate the superiority of \model over state-of-the-art denoising methods and its flexible applications to mainstream sequential <b>recommendation</b> models. The source code is available at <a href=https://github.com/zc-97/SSDRec>https://github.com/zc-97/SSDRec</a>.</p></p class="citation"></blockquote><h3 id=1111--108292-acorn-performant-and-predicate-agnostic-search-over-vector-embeddings-and-structured-data-liana-patel-et-al-2024>(11/11 | 108/292) ACORN: Performant and Predicate-Agnostic Search Over Vector Embeddings and Structured Data (Liana Patel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Liana Patel, Peter Kraft, Carlos Guestrin, Matei Zaharia. (2024)<br><strong>ACORN: Performant and Predicate-Agnostic Search Over Vector Embeddings and Structured Data</strong><br><button class=copy-to-clipboard title="ACORN: Performant and Predicate-Agnostic Search Over Vector Embeddings and Structured Data" index=108>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-108 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-DB, cs-IR, cs.IR<br>Keyword Score: 9<br>Keywords: Graph, Benchmarking, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04871v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04871v1.pdf filename=2403.04871v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Applications increasingly leverage mixed-modality data, and must jointly search over vector data, such as embedded images, text and video, as well as structured data, such as attributes and keywords. Proposed methods for this hybrid search setting either suffer from poor performance or support a severely restricted set of search predicates (e.g., only small sets of equality predicates), making them impractical for many applications. To address this, we present ACORN, an approach for performant and predicate-agnostic hybrid search. ACORN builds on Hierarchical Navigable Small Worlds (HNSW), a state-of-the-art <b>graph-based</b> approximate nearest neighbor index, and can be implemented efficiently by extending existing HNSW libraries. ACORN introduces the idea of predicate subgraph traversal to emulate a theoretically ideal, but impractical, hybrid search strategy. ACORN&rsquo;s predicate-agnostic construction algorithm is designed to enable this effective search strategy, while supporting a wide array of predicate sets and query semantics. We systematically evaluate ACORN on both prior <b>benchmark</b> datasets, with simple, low-cardinality predicate sets, and complex <b>multi-modal</b> datasets not supported by prior methods. We show that ACORN achieves state-of-the-art performance on all datasets, outperforming prior methods with 2-1,000x higher throughput at a fixed recall.</p></p class="citation"></blockquote><h2 id=csai-29>cs.AI (29)</h2><h3 id=129--109292-mkf-ads-a-multi-knowledge-fused-anomaly-detection-system-for-automotive-pengzhou-cheng-et-al-2024>(1/29 | 109/292) MKF-ADS: A Multi-Knowledge Fused Anomaly Detection System for Automotive (Pengzhou Cheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pengzhou Cheng, Zongru Wu, Gongshen Liu. (2024)<br><strong>MKF-ADS: A Multi-Knowledge Fused Anomaly Detection System for Automotive</strong><br><button class=copy-to-clipboard title="MKF-ADS: A Multi-Knowledge Fused Anomaly Detection System for Automotive" index=109>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-109 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CR, cs.AI<br>Keyword Score: 90<br>Keywords: Anomaly Detection, Convolution, Knowledge Distillation, Knowledge Distillation, Simulation, Simulator, Supervised Learning, LSTM, LSTM<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04293v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04293v1.pdf filename=2403.04293v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the requirements of Intelligent Transport Systems (ITSs) for extensive connectivity of Electronic Control Units (ECUs) to the outside world, safety and security have become stringent problems. Intrusion detection systems (IDSs) are a crucial safety component in remediating Controller Area Network (CAN) bus vulnerabilities. However, <b>supervised-based</b> IDSs fail to identify complexity attacks and <b>anomaly-based</b> <b>IDSs</b> have higher false alarms owing to capability bottleneck. In this paper, we propose a novel multi-knowledge fused <b>anomaly</b> <b>detection</b> model, called MKF-IDS. Specifically, the method designs an integration framework, including spatial-temporal correlation with an attention mechanism (STcAM) module and patch sparse-transformer module (PatchST). The STcAM with fine-pruning uses one-dimensional <b>convolution</b> (Conv1D) to extract spatial features and subsequently utilizes the Bidirectional <b>Long</b> <b>Short</b> <b>Term</b> <b>Memory</b> (Bi-LSTM) to extract the temporal features, where the attention mechanism will focus on the important time steps. Meanwhile, the PatchST captures the combined <b>long-time</b> <b>historical</b> <b>features</b> <b>from</b> independent univariate time series. Finally, the proposed method is based on <b>knowledge</b> <b>distillation</b> to STcAM as a student model for learning intrinsic <b>knowledge</b> <b>and</b> cross the ability to mimic PatchST. In the detection phase, the MKF-ADS only deploys STcAM to maintain efficiency in a resource-limited IVN environment. Moreover, the redundant noisy signal is reduced with bit flip rate and boundary decision estimation. We conduct extensive experiments on six <b>simulation</b> attack scenarios across various CAN IDs and time steps, and two real attack scenarios, which present a competitive prediction and detection performance. Compared with the baseline in the same paradigm, the error rate and FAR are 2.62% and 2.41% and achieve a promising F1-score of 97.3%.</p></p class="citation"></blockquote><h3 id=229--110292-advancing-biomedical-text-mining-with-community-challenges-hui-zong-et-al-2024>(2/29 | 110/292) Advancing Biomedical Text Mining with Community Challenges (Hui Zong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hui Zong, Rongrong Wu, Jiaxue Cha, Erman Wu, Jiakun Li, Liang Tao, Zuofeng Li, Buzhou Tang, Bairong Shen. (2024)<br><strong>Advancing Biomedical Text Mining with Community Challenges</strong><br><button class=copy-to-clipboard title="Advancing Biomedical Text Mining with Community Challenges" index=110>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-110 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs-LG, cs.AI<br>Keyword Score: 88<br>Keywords: Graph, Knowledge Graph, Named Entity Recognition, Question Answering, Relation Extraction, Text Classification, Text Generation, Text Mining, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04261v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04261v1.pdf filename=2403.04261v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The field of biomedical research has witnessed a significant increase in the accumulation of vast amounts of textual data from various sources such as scientific literatures, electronic health records, clinical trial reports, and social media. However, manually processing and analyzing these extensive and complex resources is time-consuming and inefficient. To address this challenge, biomedical <b>text</b> <b>mining,</b> also known as biomedical natural language processing, has garnered great attention. Community challenge evaluation competitions have played an important role in promoting technology innovation and interdisciplinary collaboration in biomedical <b>text</b> <b>mining</b> research. These challenges provide platforms for researchers to develop state-of-the-art solutions for data mining and information processing in biomedical research. In this article, we review the recent advances in community challenges specific to Chinese biomedical <b>text</b> <b>mining.</b> Firstly, we collect the information of these evaluation tasks, such as data sources and task types. Secondly, we conduct systematic summary and comparative analysis, including <b>named</b> <b>entity</b> <b>recognition,</b> entity normalization, attribute extraction, <b>relation</b> <b>extraction,</b> event extraction, <b>text</b> <b>classification,</b> <b>text</b> <b>similarity,</b> <b>knowledge</b> <b>graph</b> construction, <b>question</b> <b>answering,</b> <b>text</b> <b>generation,</b> and <b>large</b> <b>language</b> <b>model</b> evaluation. Then, we <b>summarize</b> the potential clinical applications of these community challenge tasks from translational informatics perspective. Finally, we discuss the contributions and limitations of these community challenges, while highlighting future directions in the era of <b>large</b> <b>language</b> <b>models.</b></p></p class="citation"></blockquote><h3 id=329--111292-privacy-preserving-fine-tuning-of-large-language-models-through-flatness-tiejin-chen-et-al-2024>(3/29 | 111/292) Privacy-preserving Fine-tuning of Large Language Models through Flatness (Tiejin Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tiejin Chen, Longchao Da, Huixue Zhou, Pingzhi Li, Kaixiong Zhou, Tianlong Chen, Hua Wei. (2024)<br><strong>Privacy-preserving Fine-tuning of Large Language Models through Flatness</strong><br><button class=copy-to-clipboard title="Privacy-preserving Fine-tuning of Large Language Models through Flatness" index=111>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-111 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: I-2, cs-AI, cs.AI<br>Keyword Score: 85<br>Keywords: Black Box, Fine-tuning, Knowledge Distillation, Knowledge Distillation, ChatGPT, Text Classification, Large Language Model, Large Language Model, Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04124v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04124v1.pdf filename=2403.04124v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The privacy concerns associated with the use of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have grown recently with the development of <b>LLMs</b> such as <b>ChatGPT.</b> <b>Differential</b> <b>Privacy</b> (DP) techniques are explored in existing work to mitigate their privacy risks at the cost of generalization degradation. Our paper reveals that the flatness of DP-trained models&rsquo; loss landscape plays an essential role in the trade-off between their privacy and generalization. We further propose a holistic framework to enforce appropriate weight flatness, which substantially improves model generalization with competitive privacy preservation. It innovates from three coarse-to-grained levels, including perturbation-aware min-max optimization on model weights within a layer, flatness-guided sparse prefix-tuning on weights across layers, and weight <b>knowledge</b> <b>distillation</b> between DP & non-DP weights copies. Comprehensive experiments of both <b>black-box</b> <b>and</b> white-box scenarios are conducted to demonstrate the effectiveness of our proposal in enhancing generalization and maintaining DP characteristics. For instance, on <b>text</b> <b>classification</b> dataset QNLI, DP-Flat achieves similar performance with non-private full <b>fine-tuning</b> but with DP guarantee under privacy budget $\epsilon=3$, and even better performance given higher privacy budgets. Codes are provided in the supplement.</p></p class="citation"></blockquote><h3 id=429--112292-feedback-generation-for-programming-exercises-with-gpt-4-imen-azaiz-et-al-2024>(4/29 | 112/292) Feedback-Generation for Programming Exercises With GPT-4 (Imen Azaiz et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Imen Azaiz, Natalie Kiesler, Sven Strickroth. (2024)<br><strong>Feedback-Generation for Programming Exercises With GPT-4</strong><br><button class=copy-to-clipboard title="Feedback-Generation for Programming Exercises With GPT-4" index=112>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-112 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 80<br>Keywords: GPT, GPT-3, GPT-3.5, GPT-4, GPT-4 turbo, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04449v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04449v1.pdf filename=2403.04449v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Ever since <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> and related applications have become broadly available, several studies investigated their potential for assisting educators and supporting students in higher education. <b>LLMs</b> such as Codex, <b>GPT-3.5,</b> and <b>GPT</b> 4 have shown promising results in the context of <b>large</b> <b>programming</b> <b>courses,</b> where students can benefit from feedback and hints if provided timely and at scale. This paper explores the quality of <b>GPT-4</b> <b>Turbo&rsquo;s</b> generated output for <b>prompts</b> containing both the programming task specification and a student&rsquo;s submission as input. Two assignments from an introductory programming course were selected, and <b>GPT-4</b> <b>was</b> asked to generate feedback for 55 randomly chosen, authentic student programming submissions. The output was qualitatively analyzed regarding correctness, personalization, fault localization, and other features identified in the material. Compared to prior work and analyses of <b>GPT-3.5,</b> <b>GPT-4</b> <b>Turbo</b> shows notable improvements. For example, the output is more structured and consistent. <b>GPT-4</b> <b>Turbo</b> can also accurately identify invalid casing in student programs&rsquo; output. In some cases, the feedback also includes the output of the student program. At the same time, inconsistent feedback was noted such as stating that the submission is correct but an error needs to be fixed. The present work increases our understanding of <b>LLMs&rsquo;</b> potential, limitations, and how to integrate them into e-assessment systems, pedagogical scenarios, and instructing students who are using applications based on <b>GPT-4.</b></p></p class="citation"></blockquote><h3 id=529--113292-how-far-are-we-from-intelligent-visual-deductive-reasoning-yizhe-zhang-et-al-2024>(5/29 | 113/292) How Far Are We from Intelligent Visual Deductive Reasoning? (Yizhe Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yizhe Zhang, He Bai, Ruixiang Zhang, Jiatao Gu, Shuangfei Zhai, Josh Susskind, Navdeep Jaitly. (2024)<br><strong>How Far Are We from Intelligent Visual Deductive Reasoning?</strong><br><button class=copy-to-clipboard title="How Far Are We from Intelligent Visual Deductive Reasoning?" index=113>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-113 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs-CV, cs.AI<br>Keyword Score: 60<br>Keywords: GPT, Reasoning, In-context Learning, In-context Learning, Large Language Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04732v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04732v2.pdf filename=2403.04732v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Vision-Language</b> Models (VLMs) such as <b>GPT-4V</b> have recently demonstrated incredible strides on diverse vision language tasks. We dig into vision-based deductive <b>reasoning,</b> a more sophisticated but less explored realm, and find previously unexposed blindspots in the current SOTA VLMs. Specifically, we leverage Raven&rsquo;s Progressive Matrices (RPMs), to assess VLMs&rsquo; abilities to perform multi-hop relational and deductive <b>reasoning</b> relying solely on visual clues. We perform comprehensive evaluations of several popular VLMs employing standard strategies such as <b>in-context</b> <b>learning,</b> self-consistency, and Chain-of-thoughts (CoT) on three diverse datasets, including the Mensa IQ test, IntelligenceTest, and RAVEN. The results reveal that despite the impressive capabilities of <b>LLMs</b> in text-based <b>reasoning,</b> we are still far from achieving comparable proficiency in visual deductive <b>reasoning.</b> We found that certain standard strategies that are effective when applied to <b>LLMs</b> do not seamlessly translate to the challenges presented by visual <b>reasoning</b> tasks. Moreover, a detailed analysis reveals that VLMs struggle to solve these tasks mainly because they are unable to perceive and comprehend multiple, confounding abstract patterns in RPM examples.</p></p class="citation"></blockquote><h3 id=629--114292-enhancing-court-view-generation-with-knowledge-injection-and-guidance-ang-li-et-al-2024>(6/29 | 114/292) Enhancing Court View Generation with Knowledge Injection and Guidance (Ang Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ang Li, Yiquan Wu, Yifei Liu, Fei Wu, Ming Cai, Kun Kuang. (2024)<br><strong>Enhancing Court View Generation with Knowledge Injection and Guidance</strong><br><button class=copy-to-clipboard title="Enhancing Court View Generation with Knowledge Injection and Guidance" index=114>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-114 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 60<br>Keywords: Language Generation, Natural Language Generation, Text Generation, Pre-trained Language Model, Pre-trained Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04366v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04366v1.pdf filename=2403.04366v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Court View Generation (CVG) is a challenging task in the field of Legal Artificial Intelligence (LegalAI), which aims to generate court views based on the plaintiff claims and the fact descriptions. While <b>Pretrained</b> <b>Language</b> <b>Models</b> <b>(PLMs)</b> have showcased their prowess in <b>natural</b> <b>language</b> <b>generation,</b> their application to the complex, knowledge-intensive domain of CVG often reveals inherent limitations. In this paper, we present a novel approach, named Knowledge Injection and Guidance (KIG), designed to bolster CVG using <b>PLMs.</b> To efficiently incorporate domain knowledge during the training stage, we introduce a knowledge-injected <b>prompt</b> encoder for <b>prompt</b> tuning, thereby reducing computational overhead. Moreover, to further enhance the model&rsquo;s ability to utilize domain knowledge, we employ a generating navigator, which dynamically guides the <b>text</b> <b>generation</b> process in the inference stage without altering the model&rsquo;s architecture, making it readily transferable. Comprehensive experiments on real-world data demonstrate the effectiveness of our approach compared to several established baselines, especially in the responsivity of claims, where it outperforms the best baseline by 11.87%.</p></p class="citation"></blockquote><h3 id=729--115292-on-the-essence-and-prospect-an-investigation-of-alignment-approaches-for-big-models-xinpeng-wang-et-al-2024>(7/29 | 115/292) On the Essence and Prospect: An Investigation of Alignment Approaches for Big Models (Xinpeng Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinpeng Wang, Shitong Duan, Xiaoyuan Yi, Jing Yao, Shanlin Zhou, Zhihua Wei, Peng Zhang, Dongkuan Xu, Maosong Sun, Xing Xie. (2024)<br><strong>On the Essence and Prospect: An Investigation of Alignment Approaches for Big Models</strong><br><button class=copy-to-clipboard title="On the Essence and Prospect: An Investigation of Alignment Approaches for Big Models" index=115>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-115 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs.AI<br>Keyword Score: 56<br>Keywords: Fine-tuning, Multi-modal, Multi-modal, Reinforcement Learning, Supervised Learning, In-context Learning, In-context Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04204v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04204v1.pdf filename=2403.04204v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Big models have achieved revolutionary breakthroughs in the field of AI, but they might also pose potential concerns. Addressing such concerns, alignment technologies were introduced to make these models conform to human preferences and values. Despite considerable advancements in the past year, various challenges lie in establishing the optimal alignment strategy, such as data cost and scalable oversight, and how to align remains an open question. In this survey paper, we comprehensively investigate value alignment approaches. We first unpack the historical context of alignment tracing back to the 1920s (where it comes from), then delve into the mathematical essence of alignment (what it is), shedding light on the inherent challenges. Following this foundation, we provide a detailed examination of existing alignment methods, which fall into three categories: <b>Reinforcement</b> <b>Learning,</b> <b>Supervised</b> <b>Fine-Tuning,</b> and <b>In-context</b> <b>Learning,</b> and demonstrate their intrinsic connections, strengths, and limitations, helping readers better understand this research area. In addition, two emerging topics, personal alignment, and <b>multimodal</b> alignment, are also discussed as novel frontiers in this field. Looking forward, we discuss potential alignment paradigms and how they could handle remaining challenges, prospecting where future alignment will go.</p></p class="citation"></blockquote><h3 id=829--116292-wiki-tabneradvancing-table-interpretation-through-named-entity-recognition-aneta-koleva-et-al-2024>(8/29 | 116/292) Wiki-TabNER:Advancing Table Interpretation Through Named Entity Recognition (Aneta Koleva et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aneta Koleva, Martin Ringsquandl, Ahmed Hatem, Thomas Runkler, Volker Tresp. (2024)<br><strong>Wiki-TabNER:Advancing Table Interpretation Through Named Entity Recognition</strong><br><button class=copy-to-clipboard title="Wiki-TabNER:Advancing Table Interpretation Through Named Entity Recognition" index=116>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-116 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs.AI<br>Keyword Score: 53<br>Keywords: Benchmarking, Few-shot, Named Entity Recognition, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04577v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04577v1.pdf filename=2403.04577v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Web tables contain a <b>large</b> <b>amount</b> <b>of</b> valuable knowledge and have inspired tabular language models aimed at tackling table interpretation (TI) tasks. In this paper, we analyse a widely used <b>benchmark</b> dataset for evaluation of TI tasks, particularly focusing on the entity linking task. Our analysis reveals that this dataset is overly simplified, potentially reducing its effectiveness for thorough evaluation and failing to accurately represent tables as they appear in the real-world. To overcome this drawback, we construct and annotate a new more challenging dataset. In addition to introducing the new dataset, we also introduce a novel problem aimed at addressing the entity linking task: <b>named</b> <b>entity</b> <b>recognition</b> within cells. Finally, we propose a <b>prompting</b> framework for evaluating the newly developed <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> on this novel TI task. We conduct experiments on <b>prompting</b> <b>LLMs</b> under various settings, where we use both random and similarity-based selection to choose the examples presented to the models. Our ablation study helps us gain insights into the impact of the <b>few-shot</b> examples. Additionally, we perform qualitative analysis to gain insights into the challenges encountered by the models and to understand the limitations of the proposed dataset.</p></p class="citation"></blockquote><h3 id=929--117292-zero-shot-cross-modal-transfer-of-reinforcement-learning-policies-through-a-global-workspace-léopold-maytié-et-al-2024>(9/29 | 117/292) Zero-shot cross-modal transfer of Reinforcement Learning policies through a Global Workspace (Léopold Maytié et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Léopold Maytié, Benjamin Devillers, Alexandre Arnold, Rufin VanRullen. (2024)<br><strong>Zero-shot cross-modal transfer of Reinforcement Learning policies through a Global Workspace</strong><br><button class=copy-to-clipboard title="Zero-shot cross-modal transfer of Reinforcement Learning policies through a Global Workspace" index=117>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-117 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 46<br>Keywords: Contrastive Learning, Fine-tuning, Multi-modal, Multi-modal, Reinforcement Learning, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04588v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04588v1.pdf filename=2403.04588v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Humans perceive the world through multiple senses, enabling them to create a comprehensive representation of their surroundings and to generalize information across domains. For instance, when a textual description of a scene is given, humans can mentally visualize it. In fields like robotics and <b>Reinforcement</b> <b>Learning</b> (RL), agents can also access information about the environment through multiple sensors; yet redundancy and complementarity between sensors is difficult to exploit as a source of robustness (e.g. against sensor failure) or generalization (e.g. transfer across domains). Prior research demonstrated that a robust and flexible <b>multimodal</b> representation can be efficiently constructed based on the cognitive science notion of a &lsquo;Global Workspace&rsquo;: a unique representation trained to combine information across modalities, and to broadcast its signal back to each modality. Here, we explore whether such a brain-inspired <b>multimodal</b> representation could be advantageous for RL agents. First, we train a &lsquo;Global Workspace&rsquo; to exploit information collected about the environment via two input modalities (a visual input, or an attribute vector representing the state of the agent and/or its environment). Then, we train a RL agent policy using this frozen Global Workspace. In two distinct environments and tasks, our results reveal the model&rsquo;s ability to perform <b>zero-shot</b> cross-modal transfer between input modalities, i.e. to apply to image inputs a policy previously trained on attribute vectors (and vice-versa), without additional training or <b>fine-tuning.</b> Variants and ablations of the full Global Workspace (including a CLIP-like <b>multimodal</b> representation trained via <b>contrastive</b> <b>learning)</b> did not display the same generalization abilities.</p></p class="citation"></blockquote><h3 id=1029--118292-graphinstruct-empowering-large-language-models-with-graph-understanding-and-reasoning-capability-zihan-luo-et-al-2024>(10/29 | 118/292) GraphInstruct: Empowering Large Language Models with Graph Understanding and Reasoning Capability (Zihan Luo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zihan Luo, Xiran Song, Hong Huang, Jianxun Lian, Chenhao Zhang, Jinqi Jiang, Xing Xie, Hai Jin. (2024)<br><strong>GraphInstruct: Empowering Large Language Models with Graph Understanding and Reasoning Capability</strong><br><button class=copy-to-clipboard title="GraphInstruct: Empowering Large Language Models with Graph Understanding and Reasoning Capability" index=118>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-118 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs.AI<br>Keyword Score: 46<br>Keywords: Graph, Benchmarking, Reasoning, Instruction Tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04483v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04483v1.pdf filename=2403.04483v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Evaluating and enhancing the general capabilities of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> has been an important research topic. <b>Graph</b> is a common data structure in the real world, and understanding <b>graph</b> data is a crucial part for advancing general intelligence. To evaluate and enhance the <b>graph</b> understanding abilities of <b>LLMs,</b> in this paper, we propose a <b>benchmark</b> named GraphInstruct, which comprehensively includes 21 classical <b>graph</b> <b>reasoning</b> tasks, providing diverse <b>graph</b> generation pipelines and detailed <b>reasoning</b> steps. Based on GraphInstruct, we further construct GraphLM through efficient <b>instruction-tuning,</b> <b>which</b> shows prominent <b>graph</b> understanding capability. In order to enhance the <b>LLM</b> with <b>graph</b> <b>reasoning</b> capability as well, we propose a step mask training strategy, and construct a model named GraphLM+. As one of the pioneering efforts to enhance the <b>graph</b> understanding and <b>reasoning</b> abilities of <b>LLMs,</b> extensive experiments have demonstrated the superiority of GraphLM and GraphLM+ over other <b>LLMs.</b> We look forward to more researchers exploring the potential of <b>LLMs</b> in the <b>graph</b> data mining domain through GraphInstruct. Our code for generating GraphInstruct is released publicly at: <a href=https://github.com/CGCL-codes/GraphInstruct>https://github.com/CGCL-codes/GraphInstruct</a>.</p></p class="citation"></blockquote><h3 id=1129--119292-improving-matrix-completion-by-exploiting-rating-ordinality-in-graph-neural-networks-jaehyun-lee-et-al-2024>(11/29 | 119/292) Improving Matrix Completion by Exploiting Rating Ordinality in Graph Neural Networks (Jaehyun Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jaehyun Lee, SeongKu Kang, Hwanjo Yu. (2024)<br><strong>Improving Matrix Completion by Exploiting Rating Ordinality in Graph Neural Networks</strong><br><button class=copy-to-clipboard title="Improving Matrix Completion by Exploiting Rating Ordinality in Graph Neural Networks" index=119>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-119 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 43<br>Keywords: Message-Passing, Graph, Graph Neural Network, Graph Neural Network, Recommender System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04504v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04504v1.pdf filename=2403.04504v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Matrix completion is an important area of research in <b>recommender</b> <b>systems.</b> Recent methods view a rating matrix as a user-item bi-partite <b>graph</b> <b>with</b> <b>labeled</b> edges denoting observed ratings and predict the edges between the user and item nodes by using the <b>graph</b> <b>neural</b> <b>network</b> <b>(GNN).</b> Despite their effectiveness, they treat each rating type as an independent relation type and thus cannot sufficiently consider the ordinal nature of the ratings. In this paper, we explore a new approach to exploit rating ordinality for <b>GNN,</b> which has not been studied well in the literature. We introduce a new method, called ROGMC, to leverage Rating Ordinality in <b>GNN-based</b> Matrix Completion. It uses cumulative preference propagation to directly incorporate rating ordinality in <b>GNN&rsquo;s</b> message passing, allowing for users&rsquo; stronger preferences to be more emphasized based on inherent orders of rating types. This process is complemented by interest regularization which facilitates preference learning using the underlying interest information. Our extensive experiments show that ROGMC consistently outperforms the existing strategies of using rating types for <b>GNN.</b> We expect that our attempt to explore the feasibility of utilizing rating ordinality for <b>GNN</b> may stimulate further research in this direction.</p></p class="citation"></blockquote><h3 id=1229--120292-unsupervised-learning-of-harmonic-analysis-based-on-neural-hsmm-with-code-quality-templates-yui-uehara-2024>(12/29 | 120/292) Unsupervised Learning of Harmonic Analysis Based on Neural HSMM with Code Quality Templates (Yui Uehara, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yui Uehara. (2024)<br><strong>Unsupervised Learning of Harmonic Analysis Based on Neural HSMM with Code Quality Templates</strong><br><button class=copy-to-clipboard title="Unsupervised Learning of Harmonic Analysis Based on Neural HSMM with Code Quality Templates" index=120>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-120 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 40<br>Keywords: Supervised Learning, Supervised Learning, Unsupervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04135v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04135v1.pdf filename=2403.04135v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a method of <b>unsupervised</b> <b>learning</b> of harmonic analysis based on a hidden semi-Markov model (HSMM). We introduce the chord quality templates, which specify the probability of pitch class emissions given a root note and a chord quality. Other probability distributions that comprise the HSMM are automatically learned via <b>unsupervised</b> <b>learning,</b> which has been a challenge in existing research. The results of the harmonic analysis of the proposed model were evaluated using existing labeled data. While our proposed method has yet to perform as well as existing models that used <b>supervised</b> <b>learning</b> and complex rule design, it has the advantage of not requiring expensive labeled data or rule elaboration. Furthermore, we also show how to recognize the tonic without prior knowledge, based on the transition probabilities of the Markov model.</p></p class="citation"></blockquote><h3 id=1329--121292-a-new-benchmark-for-evaluating-automatic-speech-recognition-in-the-arabic-call-domain-qusai-abo-obaidah-et-al-2024>(13/29 | 121/292) A New Benchmark for Evaluating Automatic Speech Recognition in the Arabic Call Domain (Qusai Abo Obaidah et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qusai Abo Obaidah, Muhy Eddin Zater, Adnan Jaljuli, Ali Mahboub, Asma Hakouz, Bashar Alfrou, Yazan Estaitia. (2024)<br><strong>A New Benchmark for Evaluating Automatic Speech Recognition in the Arabic Call Domain</strong><br><button class=copy-to-clipboard title="A New Benchmark for Evaluating Automatic Speech Recognition in the Arabic Call Domain" index=121>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-121 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs.AI<br>Keyword Score: 33<br>Keywords: Benchmarking, Automatic Speech Recognition, Automatic Speech Recognition, Automatic Speech Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04280v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04280v1.pdf filename=2403.04280v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work is an attempt to introduce a comprehensive <b>benchmark</b> for Arabic <b>speech</b> <b>recognition,</b> specifically tailored to address the challenges of telephone conversations in Arabic language. Arabic, characterized by its rich dialectal diversity and phonetic complexity, presents a number of unique challenges for <b>automatic</b> <b>speech</b> <b>recognition</b> <b>(ASR)</b> systems. These challenges are further amplified in the domain of telephone calls, where audio quality, background noise, and conversational <b>speech</b> <b>styles</b> negatively affect recognition accuracy. Our work aims to establish a robust <b>benchmark</b> that not only encompasses the broad spectrum of Arabic dialects but also emulates the real-world conditions of call-based communications. By incorporating diverse dialectical expressions and accounting for the variable quality of call recordings, this <b>benchmark</b> seeks to provide a rigorous testing ground for the development and evaluation of <b>ASR</b> systems capable of navigating the complexities of Arabic <b>speech</b> <b>in</b> telephonic contexts. This work also attempts to establish a baseline performance evaluation using state-of-the-art <b>ASR</b> technologies.</p></p class="citation"></blockquote><h3 id=1429--122292-contrastive-augmented-graph2graph-memory-interaction-for-few-shot-continual-learning-biqing-qi-et-al-2024>(14/29 | 122/292) Contrastive Augmented Graph2Graph Memory Interaction for Few Shot Continual Learning (Biqing Qi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Biqing Qi, Junqi Gao, Xingquan Chen, Dong Li, Jianxing Liu, Ligang Wu, Bowen Zhou. (2024)<br><strong>Contrastive Augmented Graph2Graph Memory Interaction for Few Shot Continual Learning</strong><br><button class=copy-to-clipboard title="Contrastive Augmented Graph2Graph Memory Interaction for Few Shot Continual Learning" index=122>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-122 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 33<br>Keywords: Graph, Continual Learning, Few-shot, Few-shot Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04140v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04140v1.pdf filename=2403.04140v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Few-Shot</b> <b>Class-Incremental</b> Learning (FSCIL) has gained considerable attention in recent years for its pivotal role in addressing continuously arriving classes. However, it encounters additional challenges. The scarcity of samples in new sessions intensifies overfitting, causing incompatibility between the output features of new and old classes, thereby escalating catastrophic forgetting. A prevalent strategy involves mitigating catastrophic forgetting through the Explicit Memory (EM), which comprise of class prototypes. However, current EM-based methods retrieves memory globally by performing Vector-to-Vector (V2V) interaction between features corresponding to the input and prototypes stored in EM, neglecting the geometric structure of local features. This hinders the accurate modeling of their positional relationships. To incorporate information of local geometric structure, we extend the V2V interaction to <b>Graph-to-Graph</b> (G2G) interaction. For enhancing local structures for better G2G alignment and the prevention of local feature collapse, we propose the Local <b>Graph</b> Preservation (LGP) mechanism. Additionally, to address sample scarcity in classes from new sessions, the Contrast-Augmented G2G (CAG2G) is introduced to promote the aggregation of same class features thus helps <b>few-shot</b> <b>learning.</b> Extensive comparisons on CIFAR100, CUB200, and the challenging ImageNet-R dataset demonstrate the superiority of our method over existing methods.</p></p class="citation"></blockquote><h3 id=1529--123292-automatic-and-universal-prompt-injection-attacks-against-large-language-models-xiaogeng-liu-et-al-2024>(15/29 | 123/292) Automatic and Universal Prompt Injection Attacks against Large Language Models (Xiaogeng Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaogeng Liu, Zhiyuan Yu, Yizhe Zhang, Ning Zhang, Chaowei Xiao. (2024)<br><strong>Automatic and Universal Prompt Injection Attacks against Large Language Models</strong><br><button class=copy-to-clipboard title="Automatic and Universal Prompt Injection Attacks against Large Language Models" index=123>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-123 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 30<br>Keywords: Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04957v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04957v1.pdf filename=2403.04957v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> excel in processing and generating human language, powered by their ability to interpret and follow instructions. However, their capabilities can be exploited through <b>prompt</b> injection attacks. These attacks manipulate <b>LLM-integrated</b> applications into producing responses aligned with the attacker&rsquo;s injected content, deviating from the user&rsquo;s actual requests. The substantial risks posed by these attacks underscore the need for a thorough understanding of the threats. Yet, research in this area faces challenges due to the lack of a unified goal for such attacks and their reliance on manually crafted <b>prompts,</b> complicating comprehensive assessments of <b>prompt</b> injection robustness. We introduce a unified framework for understanding the objectives of <b>prompt</b> injection attacks and present an automated gradient-based method for generating highly effective and universal <b>prompt</b> injection data, even in the face of defensive measures. With only five training samples (0.3% relative to the test data), our attack can achieve superior performance compared with baselines. Our findings emphasize the importance of gradient-based testing, which can avoid overestimation of robustness, especially for defense mechanisms.</p></p class="citation"></blockquote><h3 id=1629--124292-machine-learning-and-information-theory-concepts-towards-an-ai-mathematician-yoshua-bengio-et-al-2024>(16/29 | 124/292) Machine learning and information theory concepts towards an AI Mathematician (Yoshua Bengio et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yoshua Bengio, Nikolay Malkin. (2024)<br><strong>Machine learning and information theory concepts towards an AI Mathematician</strong><br><button class=copy-to-clipboard title="Machine learning and information theory concepts towards an AI Mathematician" index=124>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-124 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 30<br>Keywords: Mathematical Reasoning, Reasoning, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04571v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04571v1.pdf filename=2403.04571v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The current state-of-the-art in artificial intelligence is impressive, especially in terms of mastery of language, but not so much in terms of <b>mathematical</b> <b>reasoning.</b> What could be missing? Can we learn something useful about that gap from how the brains of mathematicians go about their craft? This essay builds on the idea that current deep learning mostly succeeds at system 1 abilities &ndash; which correspond to our intuition and habitual behaviors &ndash; but still lacks something important regarding system 2 abilities &ndash; which include <b>reasoning</b> and robust uncertainty estimation. It takes an information-theoretical posture to ask questions about what constitutes an interesting <b>mathematical</b> <b>statement,</b> which could guide future work in crafting an AI mathematician. The focus is not on proving a given theorem but on discovering new and interesting conjectures. The central hypothesis is that a desirable body of theorems better <b>summarizes</b> the set of all provable statements, for example by having a small description length while at the same time being close (in terms of number of derivation steps) to many provable statements.</p></p class="citation"></blockquote><h3 id=1729--125292-chatbot-arena-an-open-platform-for-evaluating-llms-by-human-preference-wei-lin-chiang-et-al-2024>(17/29 | 125/292) Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference (Wei-Lin Chiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E. Gonzalez, Ion Stoica. (2024)<br><strong>Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference</strong><br><button class=copy-to-clipboard title="Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference" index=125>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-125 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs.AI<br>Keyword Score: 30<br>Keywords: Chatbot, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04132v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04132v1.pdf filename=2403.04132v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have unlocked new capabilities and applications; however, evaluating the alignment with human preferences still poses significant challenges. To address this issue, we introduce <b>Chatbot</b> Arena, an open platform for evaluating <b>LLMs</b> based on human preferences. Our methodology employs a pairwise comparison approach and leverages input from a diverse user base through crowdsourcing. The platform has been operational for several months, amassing over 240K votes. This paper describes the platform, analyzes the data we have collected so far, and explains the tried-and-true statistical methods we are using for efficient and accurate evaluation and ranking of models. We confirm that the crowdsourced questions are sufficiently diverse and discriminating and that the crowdsourced human votes are in good agreement with those of expert raters. These analyses collectively establish a robust foundation for the credibility of <b>Chatbot</b> Arena. Because of its unique value and openness, <b>Chatbot</b> Arena has emerged as one of the most referenced <b>LLM</b> leaderboards, widely cited by leading <b>LLM</b> developers and companies. Our demo is publicly available at \url{https://chat.lmsys.org}.</p></p class="citation"></blockquote><h3 id=1829--126292-cotbal-comprehensive-task-balancing-for-multi-task-visual-instruction-tuning-yanqi-dai-et-al-2024>(18/29 | 126/292) CoTBal: Comprehensive Task Balancing for Multi-Task Visual Instruction Tuning (Yanqi Dai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yanqi Dai, Dong Jing, Nanyi Fei, Zhiwu Lu. (2024)<br><strong>CoTBal: Comprehensive Task Balancing for Multi-Task Visual Instruction Tuning</strong><br><button class=copy-to-clipboard title="CoTBal: Comprehensive Task Balancing for Multi-Task Visual Instruction Tuning" index=126>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-126 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 26<br>Keywords: Multi-modal, Multi-modal, Instruction Following, Instruction Tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04343v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04343v1.pdf filename=2403.04343v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Visual <b>instruction</b> <b>tuning</b> is a key training stage of large <b>multimodal</b> models (LMMs). Nevertheless, the common practice of indiscriminately mixing <b>instruction-following</b> <b>data</b> from various tasks may result in suboptimal overall performance due to different <b>instruction</b> <b>formats</b> and knowledge domains across tasks. To mitigate this issue, we propose a novel Comprehensive Task Balancing (CoTBal) algorithm for multi-task visual <b>instruction</b> <b>tuning</b> of LMMs. To our knowledge, this is the first work that explores multi-task optimization in visual <b>instruction</b> <b>tuning.</b> Specifically, we consider two key dimensions for task balancing: (1) Inter-Task Contribution, the phenomenon where learning one task potentially enhances the performance in other tasks, attributable to the overlapping knowledge domains, and (2) Intra-Task Difficulty, which refers to the learning difficulty within a single task. By quantifying these two dimensions with performance-based metrics, task balancing is thus enabled by assigning more weights to tasks that offer substantial contributions to others, receive minimal contributions from others, and also have great intra-task difficulties. Experiments show that our CoTBal leads to superior overall performance in multi-task visual <b>instruction</b> <b>tuning.</b></p></p class="citation"></blockquote><h3 id=1929--127292-towards-automatic-composition-of-asp-programs-from-natural-language-specifications-manuel-borroto-et-al-2024>(19/29 | 127/292) Towards Automatic Composition of ASP Programs from Natural Language Specifications (Manuel Borroto et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Manuel Borroto, Irfan Kareem, Francesco Ricca. (2024)<br><strong>Towards Automatic Composition of ASP Programs from Natural Language Specifications</strong><br><button class=copy-to-clipboard title="Towards Automatic Composition of ASP Programs from Natural Language Specifications" index=127>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-127 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 23<br>Keywords: Graph, Neural Machine Translation, Neural Machine Translation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04541v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04541v1.pdf filename=2403.04541v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper moves the first step towards automating the composition of Answer Set Programming (ASP) specifications. In particular, the following contributions are provided: (i) A dataset focused on <b>graph-related</b> problem specifications, designed to develop and assess tools for ASP automatic coding; (ii) A two-step architecture, implemented in the NL2ASP tool, for generating ASP programs from natural language specifications. NL2ASP uses <b>neural</b> <b>machine</b> <b>translation</b> to transform natural language into Controlled Natural Language (CNL) statements. Subsequently, CNL statements are converted into ASP code using the CNL2ASP tool. An experiment confirms the viability of the approach.</p></p class="citation"></blockquote><h3 id=2029--128292-self-supervision-in-time-for-satellite-imagess3-tss-a-novel-method-of-ssl-technique-in-satellite-images-akansh-maurya-et-al-2024>(20/29 | 128/292) Self-Supervision in Time for Satellite Images(S3-TSS): A novel method of SSL technique in Satellite images (Akansh Maurya et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Akansh Maurya, Hewan Shrestha, Mohammad Munem Shahriar. (2024)<br><strong>Self-Supervision in Time for Satellite Images(S3-TSS): A novel method of SSL technique in Satellite images</strong><br><button class=copy-to-clipboard title="Self-Supervision in Time for Satellite Images(S3-TSS): A novel method of SSL technique in Satellite images" index=128>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-128 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 20<br>Keywords: Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04859v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04859v2.pdf filename=2403.04859v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the limited availability of labeled data with various atmospheric conditions in remote sensing images, it seems useful to work with <b>self-supervised</b> <b>algorithms.</b> Few pretext-based algorithms, including from rotation, spatial context and jigsaw puzzles are not appropriate for satellite images. Often, satellite images have a higher temporal frequency. So, the temporal dimension of remote sensing data provides natural augmentation without requiring us to create artificial augmentation of images. Here, we propose S3-TSS, a novel method of <b>self-supervised</b> <b>learning</b> technique that leverages natural augmentation occurring in temporal dimension. We compare our results with current state-of-the-art methods and also perform various experiments. We observed that our method was able to perform better than baseline SeCo in four downstream datasets. Code for our work can be found here: <a href=https://github.com/hewanshrestha/Why-Self-Supervision-in-Time>https://github.com/hewanshrestha/Why-Self-Supervision-in-Time</a></p></p class="citation"></blockquote><h3 id=2129--129292-the-social-impact-of-generative-ai-an-analysis-on-chatgpt-maria-t-baldassarre-et-al-2024>(21/29 | 129/292) The Social Impact of Generative AI: An Analysis on ChatGPT (Maria T. Baldassarre et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Maria T. Baldassarre, Danilo Caivano, Berenice Fernandez Nieto, Domenico Gigante, Azzurra Ragone. (2024)<br><strong>The Social Impact of Generative AI: An Analysis on ChatGPT</strong><br><button class=copy-to-clipboard title="The Social Impact of Generative AI: An Analysis on ChatGPT" index=129>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-129 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CY, cs-ET, cs.AI<br>Keyword Score: 20<br>Keywords: Generative AI, ChatGPT<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04667v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04667v1.pdf filename=2403.04667v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent months, the social impact of Artificial Intelligence (AI) has gained considerable public interest, driven by the emergence of <b>Generative</b> <b>AI</b> models, <b>ChatGPT</b> in particular. The rapid development of these models has sparked heated discussions regarding their benefits, limitations, and associated risks. <b>Generative</b> <b>models</b> hold immense promise across multiple domains, such as healthcare, finance, and education, to cite a few, presenting diverse practical applications. Nevertheless, concerns about potential adverse effects have elicited divergent perspectives, ranging from privacy risks to escalating social inequality. This paper adopts a methodology to delve into the societal implications of <b>Generative</b> <b>AI</b> tools, focusing primarily on the case of <b>ChatGPT.</b> It evaluates the potential impact on several social sectors and illustrates the findings of a comprehensive literature review of both positive and negative effects, emerging trends, and areas of opportunity of <b>Generative</b> <b>AI</b> models. This analysis aims to facilitate an in-depth discussion by providing insights that can inspire policy, regulation, and responsible development practices to foster a human-centered AI.</p></p class="citation"></blockquote><h3 id=2229--130292-uncovering-the-deep-filter-bubble-narrow-exposure-in-short-video-recommendation-nicholas-sukiennik-et-al-2024>(22/29 | 130/292) Uncovering the Deep Filter Bubble: Narrow Exposure in Short-Video Recommendation (Nicholas Sukiennik et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nicholas Sukiennik, Chen Gao, Nian Li. (2024)<br><strong>Uncovering the Deep Filter Bubble: Narrow Exposure in Short-Video Recommendation</strong><br><button class=copy-to-clipboard title="Uncovering the Deep Filter Bubble: Narrow Exposure in Short-Video Recommendation" index=130>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-130 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: H-3-5, cs-AI, cs.AI<br>Keyword Score: 20<br>Keywords: Recommendation, Recommender System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04511v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04511v1.pdf filename=2403.04511v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Filter bubbles have been studied extensively within the context of online content platforms due to their potential to cause undesirable outcomes such as user dissatisfaction or polarization. With the rise of short-video platforms, the filter bubble has been given extra attention because these platforms rely on an unprecedented use of the <b>recommender</b> <b>system</b> to provide relevant content. In our work, we investigate the deep filter bubble, which refers to the user being exposed to narrow content within their broad interests. We accomplish this using one-year interaction data from a top short-video platform in China, which includes hierarchical data with three levels of categories for each video. We formalize our definition of a &ldquo;deep&rdquo; filter bubble within this context, and then explore various correlations within the data: first understanding the evolution of the deep filter bubble over time, and later revealing some of the factors that give rise to this phenomenon, such as specific categories, user demographics, and feedback type. We observe that while the overall proportion of users in a filter bubble remains largely constant over time, the depth composition of their filter bubble changes. In addition, we find that some demographic groups that have a higher likelihood of seeing narrower content and implicit feedback signals can lead to less bubble formation. Finally, we propose some ways in which <b>recommender</b> <b>systems</b> can be designed to reduce the risk of a user getting caught in a bubble.</p></p class="citation"></blockquote><h3 id=2329--131292-alto-an-efficient-network-orchestrator-for-compound-ai-systems-keshav-santhanam-et-al-2024>(23/29 | 131/292) ALTO: An Efficient Network Orchestrator for Compound AI Systems (Keshav Santhanam et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Keshav Santhanam, Deepti Raghavan, Muhammad Shahir Rahman, Thejas Venkatesh, Neha Kunjal, Pratiksha Thaker, Philip Levis, Matei Zaharia. (2024)<br><strong>ALTO: An Efficient Network Orchestrator for Compound AI Systems</strong><br><button class=copy-to-clipboard title="ALTO: An Efficient Network Orchestrator for Compound AI Systems" index=131>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-131 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs-DC, cs-IR, cs.AI<br>Keyword Score: 20<br>Keywords: Chatbot, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04311v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04311v1.pdf filename=2403.04311v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present ALTO, a network orchestrator for efficiently serving compound AI systems such as pipelines of language models. ALTO achieves high throughput and low latency by taking advantage of an optimization opportunity specific to generative language models: streaming intermediate outputs. As language models produce outputs token by token, ALTO exposes opportunities to stream intermediate outputs between stages when possible. We highlight two new challenges of correctness and load balancing which emerge when streaming intermediate data across distributed pipeline stage instances. We also motivate the need for an aggregation-aware routing interface and distributed <b>prompt-aware</b> scheduling to address these challenges. We demonstrate the impact of ALTO&rsquo;s partial output streaming on a complex <b>chatbot</b> verification pipeline, increasing throughput by up to 3x for a fixed latency target of 4 seconds / request while also reducing tail latency by 1.8x compared to a baseline serving approach.</p></p class="citation"></blockquote><h3 id=2429--132292-can-large-language-models-reason-and-plan-subbarao-kambhampati-2024>(24/29 | 132/292) Can Large Language Models Reason and Plan? (Subbarao Kambhampati, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Subbarao Kambhampati. (2024)<br><strong>Can Large Language Models Reason and Plan?</strong><br><button class=copy-to-clipboard title="Can Large Language Models Reason and Plan?" index=132>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-132 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs-LG, cs.AI<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04121v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04121v2.pdf filename=2403.04121v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While humans sometimes do show the capability of correcting their own erroneous guesses with self-critiquing, there seems to be no basis for that assumption in the case of <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=2529--133292-from-graph-to-word-bag-introducing-domain-knowledge-to-confusing-charge-prediction-ang-li-et-al-2024>(25/29 | 133/292) From Graph to Word Bag: Introducing Domain Knowledge to Confusing Charge Prediction (Ang Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ang Li, Qiangchao Chen, Yiquan Wu, Ming Cai, Xiang Zhou, Fei Wu, Kun Kuang. (2024)<br><strong>From Graph to Word Bag: Introducing Domain Knowledge to Confusing Charge Prediction</strong><br><button class=copy-to-clipboard title="From Graph to Word Bag: Introducing Domain Knowledge to Confusing Charge Prediction" index=133>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-133 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs.AI<br>Keyword Score: 18<br>Keywords: Graph, Knowledge Graph, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04369v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04369v1.pdf filename=2403.04369v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Confusing charge prediction is a challenging task in legal AI, which involves predicting confusing charges based on fact descriptions. While existing charge prediction methods have shown impressive performance, they face significant challenges when dealing with confusing charges, such as Snatch and Robbery. In the legal domain, constituent elements play a pivotal role in distinguishing confusing charges. Constituent elements are fundamental behaviors underlying criminal punishment and have subtle distinctions among charges. In this paper, we introduce a novel From <b>Graph</b> to Word Bag (FWGB) approach, which introduces domain <b>knowledge</b> <b>regarding</b> constituent elements to guide the model in making judgments on confusing charges, much like a judge&rsquo;s <b>reasoning</b> process. Specifically, we first construct a legal <b>knowledge</b> <b>graph</b> containing constituent elements to help select keywords for each charge, forming a word bag. Subsequently, to guide the model&rsquo;s attention towards the differentiating information for each charge within the context, we expand the attention mechanism and introduce a new loss function with attention supervision through words in the word bag. We construct the confusing charges dataset from real-world judicial documents. Experiments demonstrate the effectiveness of our method, especially in maintaining exceptional performance in imbalanced label distributions.</p></p class="citation"></blockquote><h3 id=2629--134292-a-safe-harbor-for-ai-evaluation-and-red-teaming-shayne-longpre-et-al-2024>(26/29 | 134/292) A Safe Harbor for AI Evaluation and Red Teaming (Shayne Longpre et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shayne Longpre, Sayash Kapoor, Kevin Klyman, Ashwin Ramaswami, Rishi Bommasani, Borhane Blili-Hamelin, Yangsibo Huang, Aviya Skowron, Zheng-Xin Yong, Suhas Kotha, Yi Zeng, Weiyan Shi, Xianjun Yang, Reid Southen, Alexander Robey, Patrick Chao, Diyi Yang, Ruoxi Jia, Daniel Kang, Sandy Pentland, Arvind Narayanan, Percy Liang, Peter Henderson. (2024)<br><strong>A Safe Harbor for AI Evaluation and Red Teaming</strong><br><button class=copy-to-clipboard title="A Safe Harbor for AI Evaluation and Red Teaming" index=134>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-134 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 10<br>Keywords: Generative AI<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04893v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04893v1.pdf filename=2403.04893v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Independent evaluation and red teaming are critical for identifying the risks posed by <b>generative</b> <b>AI</b> systems. However, the terms of service and enforcement strategies used by prominent AI companies to deter model misuse have disincentives on good faith safety evaluations. This causes some researchers to fear that conducting such research or releasing their findings will result in account suspensions or legal reprisal. Although some companies offer researcher access programs, they are an inadequate substitute for independent research access, as they have limited community representation, receive inadequate funding, and lack independence from corporate incentives. We propose that major AI developers commit to providing a legal and technical safe harbor, indemnifying public interest safety research and protecting it from the threat of account suspensions or legal reprisal. These proposals emerged from our collective experience conducting safety, privacy, and trustworthiness research on <b>generative</b> <b>AI</b> systems, where norms and incentives could be better aligned with public interests, without exacerbating model misuse. We believe these commitments are a necessary step towards more inclusive and unimpeded community efforts to tackle the risks of <b>generative</b> <b>AI.</b></p></p class="citation"></blockquote><h3 id=2729--135292-convergence-of-some-convex-message-passing-algorithms-to-a-fixed-point-vaclav-voracek-et-al-2024>(27/29 | 135/292) Convergence of Some Convex Message Passing Algorithms to a Fixed Point (Vaclav Voracek et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vaclav Voracek, Tomas Werner. (2024)<br><strong>Convergence of Some Convex Message Passing Algorithms to a Fixed Point</strong><br><button class=copy-to-clipboard title="Convergence of Some Convex Message Passing Algorithms to a Fixed Point" index=135>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-135 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LG, cs.AI, math-OC, stat-ML<br>Keyword Score: 10<br>Keywords: Message-Passing<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07004v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07004v1.pdf filename=2403.07004v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A popular approach to the MAP inference problem in graphical models is to minimize an upper bound obtained from a dual linear programming or Lagrangian relaxation by (block-)coordinate descent. Examples of such algorithms are max-sum diffusion and sequential tree-reweighted message passing. Convergence properties of these methods are currently not fully understood. They have been proved to converge to the set characterized by local consistency of active constraints, with unknown convergence rate; however, it was not clear if the iterates converge at all (to any single point). We prove a stronger result (which was conjectured before but never proved): the iterates converge to a fixed point of the algorithm. Moreover, we show that they achieve precision $\varepsilon>0$ in $\mathcal{O}(1/\varepsilon)$ iterations. We first prove this for a version of coordinate descent applied to a general piecewise-affine convex objective, using a novel proof technique. Then we demonstrate the generality of this approach by reducing some popular coordinate-descent algorithms to this problem. Finally we show that, in contrast to our main result, a similar version of coordinate descent applied to a constrained optimization problem need not converge.</p></p class="citation"></blockquote><h3 id=2829--136292-a-modular-end-to-end-multimodal-learning-method-for-structured-and-unstructured-data-marco-d-alessandro-et-al-2024>(28/29 | 136/292) A Modular End-to-End Multimodal Learning Method for Structured and Unstructured Data (Marco D Alessandro et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marco D Alessandro, Enrique Calabrés, Mikel Elkano. (2024)<br><strong>A Modular End-to-End Multimodal Learning Method for Structured and Unstructured Data</strong><br><button class=copy-to-clipboard title="A Modular End-to-End Multimodal Learning Method for Structured and Unstructured Data" index=136>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-136 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 6<br>Keywords: Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04866v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04866v1.pdf filename=2403.04866v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multimodal</b> learning is a rapidly growing research field that has revolutionized multitasking and generative modeling in AI. While much of the research has focused on dealing with unstructured data (e.g., language, images, audio, or video), structured data (e.g., tabular data, time series, or signals) has received less attention. However, many industry-relevant use cases involve or can be benefited from both types of data. In this work, we propose a modular, end-to-end <b>multimodal</b> learning method called MAGNUM, which can natively handle both structured and unstructured data. MAGNUM is flexible enough to employ any specialized unimodal module to extract, compress, and fuse information from all available modalities.</p></p class="citation"></blockquote><h3 id=2929--137292-identifying-causal-effects-under-functional-dependencies-yizuo-chen-et-al-2024>(29/29 | 137/292) Identifying Causal Effects Under Functional Dependencies (Yizuo Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yizuo Chen, Adnan Darwiche. (2024)<br><strong>Identifying Causal Effects Under Functional Dependencies</strong><br><button class=copy-to-clipboard title="Identifying Causal Effects Under Functional Dependencies" index=137>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-137 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LG, cs-SC, cs.AI, stat-ME<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04919v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04919v1.pdf filename=2403.04919v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the identification of causal effects, motivated by two improvements to identifiability which can be attained if one knows that some variables in a causal <b>graph</b> are functionally determined by their parents (without needing to know the specific functions). First, an unidentifiable causal effect may become identifiable when certain variables are functional. Second, certain functional variables can be excluded from being observed without affecting the identifiability of a causal effect, which may significantly reduce the number of needed variables in observational data. Our results are largely based on an elimination procedure which removes functional variables from a causal <b>graph</b> while preserving key properties in the resulting causal <b>graph,</b> including the identifiability of causal effects.</p></p class="citation"></blockquote><h2 id=cslg-46>cs.LG (46)</h2><h3 id=146--138292-teaching-large-language-models-to-reason-with-reinforcement-learning-alex-havrilla-et-al-2024>(1/46 | 138/292) Teaching Large Language Models to Reason with Reinforcement Learning (Alex Havrilla et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alex Havrilla, Yuqing Du, Sharath Chandra Raparthy, Christoforos Nalmpantis, Jane Dwivedi-Yu, Maksym Zhuravinskyi, Eric Hambro, Sainbayar Sukhbaatar, Roberta Raileanu. (2024)<br><strong>Teaching Large Language Models to Reason with Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Teaching Large Language Models to Reason with Reinforcement Learning" index=138>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-138 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 80<br>Keywords: Fine-tuning, Reinforcement Learning, Reinforcement Learning from Human Feedback, Reinforcement Learning from Human Feedback, Supervised Learning, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04642v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04642v1.pdf filename=2403.04642v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Reinforcement</b> <b>Learning</b> <b>from</b> <b>Human</b> <b>Feedback</b> (\textbf{RLHF}) has emerged as a dominant approach for aligning <b>LLM</b> outputs with human preferences. Inspired by the success of <b>RLHF,</b> we study the performance of multiple algorithms that learn from feedback (Expert Iteration, Proximal Policy Optimization (\textbf{PPO}), Return-Conditioned RL) on improving <b>LLM</b> <b>reasoning</b> capabilities. We investigate both sparse and dense rewards provided to the <b>LLM</b> both heuristically and via a learned reward model. We additionally start from multiple model sizes and initializations both with and without <b>supervised</b> <b>fine-tuning</b> (\textbf{SFT}) data. Overall, we find all algorithms perform comparably, with Expert Iteration performing best in most cases. Surprisingly, we find the sample complexity of Expert Iteration is similar to that of PPO, requiring at most on the order of $10^6$ samples to converge from a pretrained checkpoint. We investigate why this is the case, concluding that during RL training models fail to explore significantly beyond solutions already produced by SFT models. Additionally, we discuss a trade off between maj@1 and pass@96 metric performance during SFT training and how conversely RL training improves both simultaneously. We then conclude by discussing the implications of our findings for <b>RLHF</b> and the future role of RL in <b>LLM</b> <b>fine-tuning.</b></p></p class="citation"></blockquote><h3 id=246--139292-bloomgml-graph-machine-learning-through-the-lens-of-bilevel-optimization-amber-yijia-zheng-et-al-2024>(2/46 | 139/292) BloomGML: Graph Machine Learning through the Lens of Bilevel Optimization (Amber Yijia Zheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amber Yijia Zheng, Tong He, Yixuan Qiu, Minjie Wang, David Wipf. (2024)<br><strong>BloomGML: Graph Machine Learning through the Lens of Bilevel Optimization</strong><br><button class=copy-to-clipboard title="BloomGML: Graph Machine Learning through the Lens of Bilevel Optimization" index=139>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-139 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 58<br>Keywords: Message-Passing, Graph, Graph Embedding, Graph Neural Network, Graph Neural Network, Knowledge Graph, BLOOM<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04763v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04763v1.pdf filename=2403.04763v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Bilevel optimization refers to scenarios whereby the optimal solution of a lower-level energy function serves as input features to an upper-level objective of interest. These optimal features typically depend on tunable parameters of the lower-level energy in such a way that the entire bilevel pipeline can be trained end-to-end. Although not generally presented as such, this paper demonstrates how a variety of <b>graph</b> <b>learning</b> <b>techniques</b> can be recast as special cases of bilevel optimization or simplifications thereof. In brief, building on prior work we first derive a more flexible class of energy functions that, when paired with various descent steps (e.g., gradient descent, proximal methods, momentum, etc.), form <b>graph</b> <b>neural</b> <b>network</b> <b>(GNN)</b> <b>message-passing</b> layers; critically, we also carefully unpack where any residual approximation error lies with respect to the underlying constituent <b>message-passing</b> functions. We then probe several simplifications of this framework to derive close connections with non-GNN-based <b>graph</b> <b>learning</b> <b>approaches,</b> including <b>knowledge</b> <b>graph</b> <b>embeddings,</b> <b>various</b> forms of label propagation, and efficient <b>graph-regularized</b> <b>MLP</b> <b>models.</b> And finally, we present supporting empirical results that demonstrate the versatility of the proposed bilevel lens, which we refer to as BloomGML, referencing that BiLevel Optimization Offers More <b>Graph</b> <b>Machine</b> <b>Learning.</b> Our code is available at <a href=https://github.com/amberyzheng/BloomGML>https://github.com/amberyzheng/BloomGML</a>. Let <b>graph</b> <b>ML</b> <b>bloom.</b></p></p class="citation"></blockquote><h3 id=346--140292-reducing-self-supervised-learning-complexity-improves-weakly-supervised-classification-performance-in-computational-pathology-tim-lenz-et-al-2024>(3/46 | 140/292) Reducing self-supervised learning complexity improves weakly-supervised classification performance in computational pathology (Tim Lenz et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tim Lenz, Omar S. M. El Nahhas, Marta Ligero, Jakob Nikolas Kather. (2024)<br><strong>Reducing self-supervised learning complexity improves weakly-supervised classification performance in computational pathology</strong><br><button class=copy-to-clipboard title="Reducing self-supervised learning complexity improves weakly-supervised classification performance in computational pathology" index=140>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-140 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs.LG<br>Keyword Score: 50<br>Keywords: Foundation Model, Self-supervised Learning, Self-supervised Learning, Supervised Learning, Weakly-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04558v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04558v2.pdf filename=2403.04558v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep Learning models have been successfully utilized to extract clinically actionable insights from routinely available histology data. Generally, these models require annotations performed by clinicians, which are scarce and costly to generate. The emergence of <b>self-supervised</b> <b>learning</b> (SSL) methods remove this barrier, allowing for large-scale analyses on non-annotated data. However, recent SSL approaches apply increasingly expansive model architectures and larger datasets, causing the rapid escalation of data volumes, hardware prerequisites, and overall expenses, limiting access to these resources to few institutions. Therefore, we investigated the complexity of contrastive SSL in computational pathology in relation to classification performance with the utilization of consumer-grade hardware. Specifically, we analyzed the effects of adaptations in data volume, architecture, and algorithms on downstream classification tasks, emphasizing their impact on computational resources. We trained breast cancer <b>foundation</b> <b>models</b> on a large public patient cohort and validated them on various downstream classification tasks in a weakly <b>supervised</b> manner on two external public patient cohorts. Our experiments demonstrate that we can improve downstream classification performance whilst reducing SSL training duration by 90%. In summary, we propose a set of adaptations which enable the utilization of SSL in computational pathology in non-resource abundant environments.</p></p class="citation"></blockquote><h3 id=446--141292-control-based-graph-embeddings-with-data-augmentation-for-contrastive-learning-obaid-ullah-ahmad-et-al-2024>(4/46 | 141/292) Control-based Graph Embeddings with Data Augmentation for Contrastive Learning (Obaid Ullah Ahmad et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Obaid Ullah Ahmad, Anwar Said, Mudassir Shabbir, Waseem Abbas, Xenofon Koutsoukos. (2024)<br><strong>Control-based Graph Embeddings with Data Augmentation for Contrastive Learning</strong><br><button class=copy-to-clipboard title="Control-based Graph Embeddings with Data Augmentation for Contrastive Learning" index=141>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-141 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-MA, cs-SY, cs.LG, eess-SY<br>Keyword Score: 48<br>Keywords: Graph, Graph Embedding, Contrastive Learning, Data Augmentation, Representation Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04923v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04923v1.pdf filename=2403.04923v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we study the problem of <b>unsupervised</b> <b>graph</b> <b>representation</b> <b>learning</b> by harnessing the control properties of dynamical networks defined on <b>graphs.</b> <b>Our</b> approach introduces a novel framework for <b>contrastive</b> <b>learning,</b> a widely prevalent technique for <b>unsupervised</b> <b>representation</b> <b>learning.</b> A crucial step in <b>contrastive</b> <b>learning</b> is the creation of &lsquo;augmented&rsquo; <b>graphs</b> <b>from</b> the input <b>graphs.</b> <b>Though</b> different from the original <b>graphs,</b> <b>these</b> augmented <b>graphs</b> <b>retain</b> the original <b>graph&rsquo;s</b> <b>structural</b> characteristics. Here, we propose a unique method for generating these augmented <b>graphs</b> <b>by</b> leveraging the control properties of networks. The core concept revolves around perturbing the original <b>graph</b> <b>to</b> create a new one while preserving the controllability properties specific to networks and <b>graphs.</b> <b>Compared</b> to the existing methods, we demonstrate that this innovative approach enhances the effectiveness of <b>contrastive</b> <b>learning</b> frameworks, leading to superior results regarding the accuracy of the classification tasks. The key innovation lies in our ability to decode the network structure using these control properties, opening new avenues for <b>unsupervised</b> <b>graph</b> <b>representation</b> <b>learning.</b></p></p class="citation"></blockquote><h3 id=546--142292-contrastive-continual-learning-with-importance-sampling-and-prototype-instance-relation-distillation-jiyong-li-et-al-2024>(5/46 | 142/292) Contrastive Continual Learning with Importance Sampling and Prototype-Instance Relation Distillation (Jiyong Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiyong Li, Dilshod Azizov, Yang Li, Shangsong Liang. (2024)<br><strong>Contrastive Continual Learning with Importance Sampling and Prototype-Instance Relation Distillation</strong><br><button class=copy-to-clipboard title="Contrastive Continual Learning with Importance Sampling and Prototype-Instance Relation Distillation" index=142>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-142 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 48<br>Keywords: Benchmarking, Continual Learning, Contrastive Learning, Knowledge Distillation, Representation Learning, Self-Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04599v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04599v1.pdf filename=2403.04599v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, because of the high-quality <b>representations</b> <b>of</b> <b>contrastive</b> <b>learning</b> methods, rehearsal-based <b>contrastive</b> <b>continual</b> <b>learning</b> has been proposed to explore how to continually learn transferable <b>representation</b> <b>embeddings</b> to avoid the catastrophic forgetting issue in traditional <b>continual</b> <b>settings.</b> Based on this framework, we propose <b>Contrastive</b> <b>Continual</b> <b>Learning</b> via Importance Sampling (CCLIS) to preserve knowledge by recovering previous data distributions with a new strategy for Replay Buffer Selection (RBS), which minimize estimated variance to save hard negative samples for <b>representation</b> <b>learning</b> with high quality. Furthermore, we present the Prototype-instance Relation <b>Distillation</b> (PRD) loss, a technique designed to maintain the relationship between prototypes and sample <b>representations</b> <b>using</b> a <b>self-distillation</b> process. Experiments on standard <b>continual</b> <b>learning</b> <b>benchmarks</b> reveal that our method notably outperforms existing baselines in terms of knowledge preservation and thereby effectively counteracts catastrophic forgetting in online contexts. The code is available at <a href=https://github.com/lijy373/CCLIS>https://github.com/lijy373/CCLIS</a>.</p></p class="citation"></blockquote><h3 id=646--143292-on-demand-quantization-for-green-federated-generative-diffusion-in-mobile-edge-networks-bingkun-lai-et-al-2024>(6/46 | 143/292) On-demand Quantization for Green Federated Generative Diffusion in Mobile Edge Networks (Bingkun Lai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bingkun Lai, Jiayi He, Jiawen Kang, Gaolei Li, Minrui Xu, Tao zhang, Shengli Xie. (2024)<br><strong>On-demand Quantization for Green Federated Generative Diffusion in Mobile Edge Networks</strong><br><button class=copy-to-clipboard title="On-demand Quantization for Green Federated Generative Diffusion in Mobile Edge Networks" index=143>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-143 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-DC, cs-LG, cs-NI, cs.LG<br>Keyword Score: 40<br>Keywords: Diffusion Model, Federated Learning, Quantization, Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04430v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04430v1.pdf filename=2403.04430v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Generative Artificial Intelligence (GAI) shows remarkable productivity and creativity in Mobile Edge Networks, such as the metaverse and the Industrial Internet of Things. <b>Federated</b> <b>learning</b> is a promising technique for effectively training GAI models in mobile edge networks due to its data distribution. However, there is a notable issue with communication consumption when training large GAI models like generative <b>diffusion</b> <b>models</b> in mobile edge networks. Additionally, the substantial energy consumption associated with training <b>diffusion-based</b> <b>models,</b> along with the limited resources of edge devices and complexities of network environments, pose challenges for improving the training efficiency of GAI models. To address this challenge, we propose an on-demand <b>quantized</b> energy-efficient <b>federated</b> <b>diffusion</b> <b>approach</b> for mobile edge networks. Specifically, we first design a dynamic <b>quantized</b> <b>federated</b> <b>diffusion</b> <b>training</b> scheme considering various demands from the edge devices. Then, we study an energy efficiency problem based on specific <b>quantization</b> requirements. Numerical results show that our proposed method significantly reduces system energy consumption and transmitted model size compared to both baseline <b>federated</b> <b>diffusion</b> <b>and</b> fixed <b>quantized</b> <b>federated</b> <b>diffusion</b> <b>methods</b> while effectively maintaining reasonable quality and diversity of generated data.</p></p class="citation"></blockquote><h3 id=746--144292-generative-ai-for-synthetic-data-generation-methods-challenges-and-the-future-xu-guo-et-al-2024>(7/46 | 144/292) Generative AI for Synthetic Data Generation: Methods, Challenges and the Future (Xu Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xu Guo, Yiqiang Chen. (2024)<br><strong>Generative AI for Synthetic Data Generation: Methods, Challenges and the Future</strong><br><button class=copy-to-clipboard title="Generative AI for Synthetic Data Generation: Methods, Challenges and the Future" index=144>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-144 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: I-2-0, cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Generative AI, Low-Resource, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04190v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04190v1.pdf filename=2403.04190v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The recent surge in research focused on generating synthetic data from <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> especially for scenarios with limited data availability, marks a notable shift in <b>Generative</b> <b>Artificial</b> Intelligence (AI). Their ability to perform comparably to real-world data positions this approach as a compelling solution to <b>low-resource</b> challenges. This paper delves into advanced technologies that leverage these gigantic <b>LLMs</b> for the generation of task-specific training data. We outline methodologies, evaluation techniques, and practical applications, discuss the current limitations, and suggest potential pathways for future research.</p></p class="citation"></blockquote><h3 id=846--145292-clip-the-bias-how-useful-is-balancing-data-in-multimodal-learning-ibrahim-alabdulmohsin-et-al-2024>(8/46 | 145/292) CLIP the Bias: How Useful is Balancing Data in Multimodal Learning? (Ibrahim Alabdulmohsin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ibrahim Alabdulmohsin, Xiao Wang, Andreas Steiner, Priya Goyal, Alexander D&rsquo;Amour, Xiaohua Zhai. (2024)<br><strong>CLIP the Bias: How Useful is Balancing Data in Multimodal Learning?</strong><br><button class=copy-to-clipboard title="CLIP the Bias: How Useful is Balancing Data in Multimodal Learning?" index=145>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-145 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 36<br>Keywords: Fine-tuning, Multi-modal, Multi-modal, Recommendation, Image2text<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04547v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04547v1.pdf filename=2403.04547v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the effectiveness of data-balancing for mitigating biases in contrastive language-image pretraining (CLIP), identifying areas of strength and limitation. First, we reaffirm prior conclusions that CLIP models can inadvertently absorb societal stereotypes. To counter this, we present a novel algorithm, called <b>Multi-Modal</b> Moment Matching (M4), designed to reduce both representation and association biases (i.e. in first- and second-order statistics) in <b>multimodal</b> data. We use M4 to conduct an in-depth analysis taking into account various factors, such as the model, representation, and data size. Our study also explores the dynamic nature of how CLIP learns and unlearns biases. In particular, we find that <b>fine-tuning</b> is effective in countering representation biases, though its impact diminishes for association biases. Also, data balancing has a mixed impact on quality: it tends to improve classification but can hurt retrieval. Interestingly, data and architectural improvements seem to mitigate the negative impact of data balancing on performance; e.g. applying M4 to SigLIP-B/16 with data quality filters improves COCO <b>image-to-text</b> retrieval @5 from 86% (without data balancing) to 87% and ImageNet 0-shot classification from 77% to 77.5%! Finally, we conclude with <b>recommendations</b> for improving the efficacy of data balancing in <b>multimodal</b> systems.</p></p class="citation"></blockquote><h3 id=946--146292-on-the-topology-awareness-and-generalization-performance-of-graph-neural-networks-junwei-su-et-al-2024>(9/46 | 146/292) On the Topology Awareness and Generalization Performance of Graph Neural Networks (Junwei Su et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junwei Su, Chuan Wu. (2024)<br><strong>On the Topology Awareness and Generalization Performance of Graph Neural Networks</strong><br><button class=copy-to-clipboard title="On the Topology Awareness and Generalization Performance of Graph Neural Networks" index=146>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-146 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 36<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Active Learning, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04482v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04482v1.pdf filename=2403.04482v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Many computer vision and machine learning problems are modelled as learning tasks on <b>graphs,</b> <b>where</b> <b>graph</b> <b>neural</b> <b>networks</b> <b>(GNNs)</b> have emerged as a dominant tool for learning representations of <b>graph-structured</b> <b>data.</b> <b>A</b> key feature of <b>GNNs</b> is their use of <b>graph</b> <b>structures</b> <b>as</b> input, enabling them to exploit the <b>graphs&rsquo;</b> <b>inherent</b> <b>topological</b> properties-known as the topology awareness of <b>GNNs.</b> Despite the empirical successes of <b>GNNs,</b> the influence of topology awareness on generalization performance remains unexplored, particularly for node-level tasks that diverge from the assumption of data being independent and identically distributed (I.I.D.). The precise definition and characterization of the topology awareness of <b>GNNs,</b> especially concerning different topological features, are still unclear. This paper introduces a comprehensive framework to characterize the topology awareness of <b>GNNs</b> across any topological feature. Using this framework, we investigate the effects of topology awareness on <b>GNN</b> generalization performance. Contrary to the prevailing belief that enhancing the topology awareness of <b>GNNs</b> is always advantageous, our analysis reveals a critical insight: improving the topology awareness of <b>GNNs</b> may inadvertently lead to unfair generalization across structural groups, which might not be desired in some scenarios. Additionally, we conduct a case study using the intrinsic <b>graph</b> <b>metric,</b> <b>the</b> shortest path distance, on various <b>benchmark</b> datasets. The empirical results of this case study confirm our theoretical insights. Moreover, we demonstrate the practical applicability of our framework by using it to tackle the cold start problem in <b>graph</b> <b>active</b> <b>learning.</b></p></p class="citation"></blockquote><h3 id=1046--147292-gnn-vpa-a-variance-preserving-aggregation-strategy-for-graph-neural-networks-lisa-schneckenreiter-et-al-2024>(10/46 | 147/292) GNN-VPA: A Variance-Preserving Aggregation Strategy for Graph Neural Networks (Lisa Schneckenreiter et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lisa Schneckenreiter, Richard Freinschlag, Florian Sestak, Johannes Brandstetter, Günter Klambauer, Andreas Mayr. (2024)<br><strong>GNN-VPA: A Variance-Preserving Aggregation Strategy for Graph Neural Networks</strong><br><button class=copy-to-clipboard title="GNN-VPA: A Variance-Preserving Aggregation Strategy for Graph Neural Networks" index=147>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-147 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, stat-ML<br>Keyword Score: 33<br>Keywords: Message-Passing, Graph, Graph Neural Network, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04747v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04747v1.pdf filename=2403.04747v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>neural</b> <b>networks</b> <b>(GNNs),</b> and especially <b>message-passing</b> neural networks, excel in various domains such as physics, drug discovery, and molecular modeling. The expressivity of <b>GNNs</b> with respect to their ability to discriminate non-isomorphic <b>graphs</b> <b>critically</b> <b>depends</b> on the functions employed for message aggregation and <b>graph-level</b> <b>readout.</b> <b>By</b> applying signal propagation theory, we propose a variance-preserving aggregation function (VPA) that maintains expressivity, but yields improved forward and backward dynamics. Experiments demonstrate that VPA leads to increased predictive performance for popular <b>GNN</b> architectures as well as improved learning dynamics. Our results could pave the way towards normalizer-free or self-normalizing <b>GNNs.</b></p></p class="citation"></blockquote><h3 id=1146--148292-entropy-aware-message-passing-in-graph-neural-networks-philipp-nazari-et-al-2024>(11/46 | 148/292) Entropy Aware Message Passing in Graph Neural Networks (Philipp Nazari et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Philipp Nazari, Oliver Lemke, Davide Guidobene, Artiom Gesp. (2024)<br><strong>Entropy Aware Message Passing in Graph Neural Networks</strong><br><button class=copy-to-clipboard title="Entropy Aware Message Passing in Graph Neural Networks" index=148>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-148 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: I-2-6; I-5-1, cs-LG, cs.LG<br>Keyword Score: 33<br>Keywords: Message-Passing, Graph, Graph Neural Network, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04636v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04636v1.pdf filename=2403.04636v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep <b>Graph</b> <b>Neural</b> <b>Networks</b> struggle with oversmoothing. This paper introduces a novel, physics-inspired <b>GNN</b> model designed to mitigate this issue. Our approach integrates with existing <b>GNN</b> architectures, introducing an entropy-aware message passing term. This term performs gradient ascent on the entropy during node aggregation, thereby preserving a certain degree of entropy in the embeddings. We conduct a comparative analysis of our model against state-of-the-art <b>GNNs</b> across various common datasets.</p></p class="citation"></blockquote><h3 id=1246--149292-a-survey-of-graph-neural-networks-in-real-world-imbalance-noise-privacy-and-ood-challenges-wei-ju-et-al-2024>(12/46 | 149/292) A Survey of Graph Neural Networks in Real world: Imbalance, Noise, Privacy and OOD Challenges (Wei Ju et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wei Ju, Siyu Yi, Yifan Wang, Zhiping Xiao, Zhengyang Mao, Hourun Li, Yiyang Gu, Yifang Qin, Nan Yin, Senzhang Wang, Xinwang Liu, Xiao Luo, Philip S. Yu, Ming Zhang. (2024)<br><strong>A Survey of Graph Neural Networks in Real world: Imbalance, Noise, Privacy and OOD Challenges</strong><br><button class=copy-to-clipboard title="A Survey of Graph Neural Networks in Real world: Imbalance, Noise, Privacy and OOD Challenges" index=149>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-149 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-IR, cs-LG, cs-SI, cs.LG<br>Keyword Score: 33<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04468v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04468v1.pdf filename=2403.04468v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph-structured</b> <b>data</b> <b>exhibits</b> universality and widespread applicability across diverse domains, such as social network analysis, biochemistry, financial fraud detection, and network security. Significant strides have been made in leveraging <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> to achieve remarkable success in these areas. However, in real-world scenarios, the training environment for models is often far from ideal, leading to substantial performance degradation of <b>GNN</b> models due to various unfavorable factors, including imbalance in data distribution, the presence of noise in erroneous data, privacy protection of sensitive information, and generalization capability for <b>out-of-distribution</b> (OOD) scenarios. To tackle these issues, substantial efforts have been devoted to improving the performance of <b>GNN</b> models in practical real-world scenarios, as well as enhancing their reliability and robustness. In this paper, we present a comprehensive survey that systematically reviews existing <b>GNN</b> models, focusing on solutions to the four mentioned real-world challenges including imbalance, noise, privacy, and OOD in practical scenarios that many existing reviews have not considered. Specifically, we first highlight the four key challenges faced by existing <b>GNNs,</b> paving the way for our exploration of real-world <b>GNN</b> models. Subsequently, we provide detailed discussions on these four aspects, dissecting how these solutions contribute to enhancing the reliability and robustness of <b>GNN</b> models. Last but not least, we outline promising directions and offer future perspectives in the field.</p></p class="citation"></blockquote><h3 id=1346--150292-improved-algorithm-for-adversarial-linear-mixture-mdps-with-bandit-feedback-and-unknown-transition-long-fei-li-et-al-2024>(13/46 | 150/292) Improved Algorithm for Adversarial Linear Mixture MDPs with Bandit Feedback and Unknown Transition (Long-Fei Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Long-Fei Li, Peng Zhao, Zhi-Hua Zhou. (2024)<br><strong>Improved Algorithm for Adversarial Linear Mixture MDPs with Bandit Feedback and Unknown Transition</strong><br><button class=copy-to-clipboard title="Improved Algorithm for Adversarial Linear Mixture MDPs with Bandit Feedback and Unknown Transition" index=150>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-150 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 30<br>Keywords: Bandit Algorithm, Markov Decision Process, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04568v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04568v1.pdf filename=2403.04568v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study <b>reinforcement</b> <b>learning</b> with linear function approximation, unknown transition, and adversarial losses in the <b>bandit</b> feedback setting. Specifically, we focus on linear mixture <b>MDPs</b> whose transition kernel is a linear mixture model. We propose a new algorithm that attains an $\widetilde{O}(d\sqrt{HS^3K} + \sqrt{HSAK})$ regret with high probability, where $d$ is the dimension of feature mappings, $S$ is the size of state space, $A$ is the size of action space, $H$ is the episode length and $K$ is the number of episodes. Our result strictly improves the previous best-known $\widetilde{O}(dS^2 \sqrt{K} + \sqrt{HSAK})$ result in Zhao et al. (2023a) since $H \leq S$ holds by the layered MDP structure. Our advancements are primarily attributed to (i) a new least square estimator for the transition parameter that leverages the visit information of all states, as opposed to only one state in prior work, and (ii) a new self-normalized concentration tailored specifically to handle non-independent noises, originally proposed in the dynamic assortment area and firstly applied in <b>reinforcement</b> <b>learning</b> to handle correlations between different states.</p></p class="citation"></blockquote><h3 id=1446--151292-improve-generalization-ability-of-deep-wide-residual-network-with-a-suitable-scaling-factor-songtao-tian-et-al-2024>(14/46 | 151/292) Improve Generalization Ability of Deep Wide Residual Network with A Suitable Scaling Factor (Songtao Tian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Songtao Tian, Zixiong Yu. (2024)<br><strong>Improve Generalization Ability of Deep Wide Residual Network with A Suitable Scaling Factor</strong><br><button class=copy-to-clipboard title="Improve Generalization Ability of Deep Wide Residual Network with A Suitable Scaling Factor" index=151>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-151 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-ST, stat-TH<br>Keyword Score: 30<br>Keywords: MNIST, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04545v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04545v1.pdf filename=2403.04545v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep Residual Neural Networks (ResNets) have demonstrated remarkable success across a wide range of real-world applications. In this paper, we identify a suitable scaling factor (denoted by $\alpha$) on the residual branch of deep wide ResNets to achieve good generalization ability. We show that if $\alpha$ is a constant, the class of functions induced by Residual Neural Tangent Kernel (RNTK) is asymptotically not learnable, as the depth goes to infinity. We also highlight a surprising phenomenon: even if we allow $\alpha$ to decrease with increasing depth $L$, the degeneration phenomenon may still occur. However, when $\alpha$ decreases rapidly with $L$, the kernel regression with deep RNTK with early stopping can achieve the minimax rate provided that the target regression function falls in the reproducing kernel Hilbert space associated with the infinite-depth RNTK. Our <b>simulation</b> studies on synthetic data and real classification tasks such as <b>MNIST,</b> CIFAR10 and CIFAR100 support our theoretical criteria for choosing $\alpha$.</p></p class="citation"></blockquote><h3 id=1546--152292-storm-surge-modeling-in-the-ai-era-using-lstm-based-machine-learning-for-enhancing-forecasting-accuracy-stefanos-giaremis-et-al-2024>(15/46 | 152/292) Storm Surge Modeling in the AI ERA: Using LSTM-based Machine Learning for Enhancing Forecasting Accuracy (Stefanos Giaremis et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Stefanos Giaremis, Noujoud Nader, Clint Dawson, Hartmut Kaiser, Carola Kaiser, Efstratios Nikidis. (2024)<br><strong>Storm Surge Modeling in the AI ERA: Using LSTM-based Machine Learning for Enhancing Forecasting Accuracy</strong><br><button class=copy-to-clipboard title="Storm Surge Modeling in the AI ERA: Using LSTM-based Machine Learning for Enhancing Forecasting Accuracy" index=152>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-152 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, physics-ao-ph<br>Keyword Score: 30<br>Keywords: Simulation, Simulator, LSTM<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04818v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04818v1.pdf filename=2403.04818v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Physics <b>simulation</b> results of natural processes usually do not fully capture the real world. This is caused for instance by limits in what physical processes are simulated and to what accuracy. In this work we propose and analyze the use of an <b>LSTM-based</b> deep learning network machine learning (ML) architecture for capturing and predicting the behavior of the systemic error for storm surge forecast models with respect to real-world water height observations from gauge stations during hurricane events. The overall goal of this work is to predict the systemic error of the physics model and use it to improve the accuracy of the <b>simulation</b> results post factum. We trained our proposed ML model on a dataset of 61 historical storms in the coastal regions of the U.S. and we tested its performance in bias correcting modeled water level data predictions from hurricane Ian (2022). We show that our model can consistently improve the forecasting accuracy for hurricane Ian &ndash; unknown to the ML model &ndash; at all gauge station coordinates used for the initial data. Moreover, by examining the impact of using different subsets of the initial training dataset, containing a number of relatively similar or different hurricanes in terms of hurricane track, we found that we can obtain similar quality of bias correction by only using a subset of six hurricanes. This is an important result that implies the possibility to apply a pre-trained ML model to real-time hurricane forecasting results with the goal of bias correcting and improving the produced <b>simulation</b> accuracy. The presented work is an important first step in creating a bias correction system for real-time storm surge forecasting applicable to the full <b>simulation</b> area. It also presents a highly transferable and operationally applicable methodology for improving the accuracy in a wide range of physics <b>simulation</b> scenarios beyond storm surge forecasting.</p></p class="citation"></blockquote><h3 id=1646--153292-online-adaptation-of-language-models-with-a-memory-of-amortized-contexts-jihoon-tack-et-al-2024>(16/46 | 153/292) Online Adaptation of Language Models with a Memory of Amortized Contexts (Jihoon Tack et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jihoon Tack, Jaehyung Kim, Eric Mitchell, Jinwoo Shin, Yee Whye Teh, Jonathan Richard Schwarz. (2024)<br><strong>Online Adaptation of Language Models with a Memory of Amortized Contexts</strong><br><button class=copy-to-clipboard title="Online Adaptation of Language Models with a Memory of Amortized Contexts" index=153>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-153 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Meta Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04317v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04317v1.pdf filename=2403.04317v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Due to the rapid generation and dissemination of information, <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> quickly run out of date despite enormous development costs. Due to this crucial need to keep models updated, online learning has emerged as a critical necessity when utilizing <b>LLMs</b> for real-world applications. However, given the ever-expanding corpus of unseen documents and the <b>large</b> <b>parameter</b> <b>space</b> of modern <b>LLMs,</b> efficient adaptation is essential. To address these challenges, we propose Memory of Amortized Contexts (MAC), an efficient and effective online adaptation framework for <b>LLMs</b> with strong knowledge retention. We propose an amortized feature extraction and memory-augmentation approach to compress and extract information from new documents into compact modulations stored in a memory bank. When answering questions, our model attends to and extracts relevant knowledge from this memory bank. To learn informative modulations in an efficient manner, we utilize amortization-based <b>meta-learning,</b> <b>which</b> substitutes the optimization process with a single forward pass of the encoder. Subsequently, we learn to choose from and aggregate selected documents into a single modulation by conditioning on the question, allowing us to adapt a frozen language model during test time without requiring further gradient updates. Our experiment demonstrates the superiority of MAC in multiple aspects, including online adaptation performance, time, and memory efficiency. Code is available at: <a href=https://github.com/jihoontack/MAC>https://github.com/jihoontack/MAC</a>.</p></p class="citation"></blockquote><h3 id=1746--154292-why-online-reinforcement-learning-is-causal-oliver-schulte-et-al-2024>(17/46 | 154/292) Why Online Reinforcement Learning is Causal (Oliver Schulte et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Oliver Schulte, Pascal Poupart. (2024)<br><strong>Why Online Reinforcement Learning is Causal</strong><br><button class=copy-to-clipboard title="Why Online Reinforcement Learning is Causal" index=154>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-154 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: I-2-6, cs-AI, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Counter-factual, Online Reinforcement Learning, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04221v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04221v1.pdf filename=2403.04221v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Reinforcement</b> <b>learning</b> (RL) and causal modelling naturally complement each other. The goal of causal modelling is to predict the effects of interventions in an environment, while the goal of <b>reinforcement</b> <b>learning</b> is to select interventions that maximize the rewards the agent receives from the environment. <b>Reinforcement</b> <b>learning</b> includes the two most powerful sources of information for estimating causal relationships: temporal ordering and the ability to act on an environment. This paper examines which <b>reinforcement</b> <b>learning</b> settings we can expect to benefit from causal modelling, and how. In <b>online</b> <b>learning,</b> <b>the</b> agent has the ability to interact directly with their environment, and learn from exploring it. Our main argument is that in <b>online</b> <b>learning,</b> <b>conditional</b> probabilities are causal, and therefore offline RL is the setting where causal learning has the most potential to make a difference. Essentially, the reason is that when an agent learns from their {\em own} experience, there are no unobserved confounders that influence both the agent&rsquo;s own exploratory actions and the rewards they receive. Our paper formalizes this argument. For offline RL, where an agent may and typically does learn from the experience of {\em others}, we describe previous and new methods for leveraging a causal model, including support for <b>counterfactual</b> queries.</p></p class="citation"></blockquote><h3 id=1846--155292-context-based-multimodal-fusion-bilal-faye-et-al-2024>(18/46 | 155/292) Context-Based Multimodal Fusion (Bilal Faye et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bilal Faye, Hanane Azzag, Mustapha Lebbah, Djamel Bouchaffra. (2024)<br><strong>Context-Based Multimodal Fusion</strong><br><button class=copy-to-clipboard title="Context-Based Multimodal Fusion" index=155>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-155 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 26<br>Keywords: Multi-modal, Multi-modal, Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04650v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04650v2.pdf filename=2403.04650v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The fusion models, which effectively combine information from different sources, are widely used in solving <b>multimodal</b> tasks. However, they have significant limitations related to aligning data distributions across different modalities. This challenge can lead to inconsistencies and difficulties in learning robust representations. Alignment models, while specifically addressing this issue, often require training &ldquo;from scratch&rdquo; with large datasets to achieve optimal results, which can be costly in terms of resources and time. To overcome these limitations, we propose an innovative model called Context-Based <b>Multimodal</b> Fusion (CBMF), which combines both modality fusion and data distribution alignment. In CBMF, each modality is represented by a specific context vector, fused with the embedding of each modality. This enables the use of large pre-trained models that can be frozen, reducing the computational and training data requirements. Additionally, the network learns to differentiate embeddings of different modalities through fusion with context and aligns data distributions using a contrastive approach for <b>self-supervised</b> <b>learning.</b> Thus, CBMF offers an effective and economical solution for solving complex <b>multimodal</b> tasks.</p></p class="citation"></blockquote><h3 id=1946--156292-rethinking-of-encoder-based-warm-start-methods-in-hyperparameter-optimization-dawid-płudowski-et-al-2024>(19/46 | 156/292) Rethinking of Encoder-based Warm-start Methods in Hyperparameter Optimization (Dawid Płudowski et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dawid Płudowski, Antoni Zajko, Anna Kozak, Katarzyna Woźnica. (2024)<br><strong>Rethinking of Encoder-based Warm-start Methods in Hyperparameter Optimization</strong><br><button class=copy-to-clipboard title="Rethinking of Encoder-based Warm-start Methods in Hyperparameter Optimization" index=156>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-156 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 25<br>Keywords: Human Intervention, Meta Learning, Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04720v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04720v1.pdf filename=2403.04720v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Effectively representing heterogeneous tabular datasets for <b>meta-learning</b> <b>remains</b> an open problem. Previous approaches rely on predefined <b>meta-features,</b> <b>for</b> example, statistical measures or landmarkers. Encoder-based models, such as Dataset2Vec, allow us to extract significant <b>meta-features</b> <b>automatically</b> without <b>human</b> <b>intervention.</b> This research introduces a novel encoder-based <b>representation</b> <b>of</b> tabular datasets implemented within the liltab package available on GitHub <a href=https://github.com/azoz01/liltab>https://github.com/azoz01/liltab</a>. Our package is based on an established model for heterogeneous tabular data proposed in [Iwata and Kumagai, 2020]. The proposed approach employs a different model for encoding feature relationships, generating alternative <b>representations</b> <b>compared</b> to existing methods like Dataset2Vec. Both of them leverage the fundamental assumption of dataset similarity learning. In this work, we evaluate Dataset2Vec and liltab on two common <b>meta-tasks</b> <b>-</b> representing entire datasets and hyperparameter optimization warm-start. However, validation on an independent metaMIMIC dataset highlights the nuanced challenges in <b>representation</b> <b>learning.</b> We show that general <b>representations</b> <b>may</b> not suffice for some <b>meta-tasks</b> <b>where</b> requirements are not explicitly considered during extraction. [Iwata and Kumagai, 2020] Tomoharu Iwata and Atsutoshi Kumagai. <b>Meta-learning</b> <b>from</b> Tasks with Heterogeneous Attribute Spaces. In Advances in Neural Information Processing Systems, 2020.</p></p class="citation"></blockquote><h3 id=2046--157292-explaining-bayesian-optimization-by-shapley-values-facilitates-human-ai-collaboration-julian-rodemann-et-al-2024>(20/46 | 157/292) Explaining Bayesian Optimization by Shapley Values Facilitates Human-AI Collaboration (Julian Rodemann et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Julian Rodemann, Federico Croppi, Philipp Arens, Yusuf Sale, Julia Herbinger, Bernd Bischl, Eyke Hüllermeier, Thomas Augustin, Conor J. Walsh, Giuseppe Casalicchio. (2024)<br><strong>Explaining Bayesian Optimization by Shapley Values Facilitates Human-AI Collaboration</strong><br><button class=copy-to-clipboard title="Explaining Bayesian Optimization by Shapley Values Facilitates Human-AI Collaboration" index=157>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-157 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: I-2-6; I-2-9; F-2-2; J-6, cs-AI, cs-HC, cs-LG, cs-RO, cs.LG, stat-ML<br>Keyword Score: 25<br>Keywords: Black Box, human-in-the-loop, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04629v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04629v2.pdf filename=2403.04629v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Bayesian optimization (BO) with Gaussian processes (GP) has become an indispensable algorithm for <b>black</b> <b>box</b> optimization problems. Not without a dash of irony, BO is often considered a <b>black</b> <b>box</b> itself, lacking ways to provide reasons as to why certain parameters are proposed to be evaluated. This is particularly relevant in <b>human-in-the-loop</b> applications of BO, such as in robotics. We address this issue by proposing ShapleyBO, a framework for interpreting BO&rsquo;s proposals by game-theoretic Shapley values.They quantify each parameter&rsquo;s contribution to BO&rsquo;s acquisition function. Exploiting the linearity of Shapley values, we are further able to identify how strongly each parameter drives BO&rsquo;s exploration and exploitation for additive acquisition functions like the confidence bound. We also show that ShapleyBO can disentangle the contributions to exploration into those that explore aleatoric and epistemic uncertainty. Moreover, our method gives rise to a ShapleyBO-assisted human machine interface (HMI), allowing users to interfere with BO in case proposals do not align with human <b>reasoning.</b> We demonstrate this HMI&rsquo;s benefits for the use case of personalizing wearable robotic devices (assistive back exosuits) by <b>human-in-the-loop</b> BO. Results suggest human-BO teams with access to ShapleyBO can achieve lower regret than teams without.</p></p class="citation"></blockquote><h3 id=2146--158292-on-the-markov-property-of-neural-algorithmic-reasoning-analyses-and-methods-montgomery-bohde-et-al-2024>(21/46 | 158/292) On the Markov Property of Neural Algorithmic Reasoning: Analyses and Methods (Montgomery Bohde et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Montgomery Bohde, Meng Liu, Alexandra Saxton, Shuiwang Ji. (2024)<br><strong>On the Markov Property of Neural Algorithmic Reasoning: Analyses and Methods</strong><br><button class=copy-to-clipboard title="On the Markov Property of Neural Algorithmic Reasoning: Analyses and Methods" index=158>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-158 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-NE, cs.LG<br>Keyword Score: 23<br>Keywords: Graph Attention Networks, Benchmarking, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04929v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04929v1.pdf filename=2403.04929v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neural algorithmic <b>reasoning</b> is an emerging research direction that endows neural networks with the ability to mimic algorithmic executions step-by-step. A common paradigm in existing designs involves the use of historical embeddings in predicting the results of future execution steps. Our observation in this work is that such historical dependence intrinsically contradicts the Markov nature of algorithmic <b>reasoning</b> tasks. Based on this motivation, we present our ForgetNet, which does not use historical embeddings and thus is consistent with the Markov nature of the tasks. To address challenges in training ForgetNet at early stages, we further introduce G-ForgetNet, which uses a <b>gating</b> mechanism to allow for the selective integration of historical embeddings. Such an enhanced capability provides valuable computational pathways during the model&rsquo;s early training phase. Our extensive experiments, based on the CLRS-30 algorithmic <b>reasoning</b> <b>benchmark,</b> demonstrate that both ForgetNet and G-ForgetNet achieve better generalization capability than existing methods. Furthermore, we investigate the behavior of the <b>gating</b> mechanism, highlighting its degree of alignment with our intuitions and its effectiveness for robust performance.</p></p class="citation"></blockquote><h3 id=2246--159292-in-n-out-calibrating-graph-neural-networks-for-link-prediction-erik-nascimento-et-al-2024>(22/46 | 159/292) In-n-Out: Calibrating Graph Neural Networks for Link Prediction (Erik Nascimento et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Erik Nascimento, Diego Mesquita, Samuel Kaski, Amauri H Souza. (2024)<br><strong>In-n-Out: Calibrating Graph Neural Networks for Link Prediction</strong><br><button class=copy-to-clipboard title="In-n-Out: Calibrating Graph Neural Networks for Link Prediction" index=159>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-159 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Graph, Graph Neural Network, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04605v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04605v2.pdf filename=2403.04605v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep neural networks are notoriously miscalibrated, i.e., their outputs do not reflect the true probability of the event we aim to predict. While networks for tabular or image data are usually overconfident, recent works have shown that <b>graph</b> <b>neural</b> <b>networks</b> <b>(GNNs)</b> show the opposite behavior for node-level classification. But what happens when we are predicting links? We show that, in this case, <b>GNNs</b> often exhibit a mixed behavior. More specifically, they may be overconfident in negative predictions while being underconfident in positive ones. Based on this observation, we propose IN-N-OUT, the first-ever method to calibrate <b>GNNs</b> for link prediction. IN-N-OUT is based on two simple intuitions: i) attributing true/false labels to an edge while respecting a <b>GNNs</b> prediction should cause but small fluctuations in that edge&rsquo;s embedding; and, conversely, ii) if we label that same edge contradicting our <b>GNN,</b> embeddings should change more substantially. An extensive experimental campaign shows that IN-N-OUT significantly improves the calibration of <b>GNNs</b> in link prediction, consistently outperforming the baselines available &ndash; which are not designed for this specific task.</p></p class="citation"></blockquote><h3 id=2346--160292-enhancing-data-quality-in-federated-fine-tuning-of-foundation-models-wanru-zhao-et-al-2024>(23/46 | 160/292) Enhancing Data Quality in Federated Fine-Tuning of Foundation Models (Wanru Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wanru Zhao, Yaxin Du, Nicholas Donald Lane, Siheng Chen, Yanfeng Wang. (2024)<br><strong>Enhancing Data Quality in Federated Fine-Tuning of Foundation Models</strong><br><button class=copy-to-clipboard title="Enhancing Data Quality in Federated Fine-Tuning of Foundation Models" index=160>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-160 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-DC, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Fine-tuning, Foundation Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04529v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04529v1.pdf filename=2403.04529v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the current landscape of <b>foundation</b> <b>model</b> training, there is a significant reliance on public domain data, which is nearing exhaustion according to recent research. To further scale up, it is crucial to incorporate collaboration among multiple specialized and high-quality private domain data sources. However, the challenge of training models locally without sharing private data presents numerous obstacles in data quality control. To tackle this issue, we propose a data quality control pipeline for federated <b>fine-tuning</b> of <b>foundation</b> <b>models.</b> This pipeline computes scores reflecting the quality of training data and determines a global threshold for a unified standard, aiming for improved global performance. Our experiments show that the proposed quality control pipeline facilitates the effectiveness and reliability of the model training, leading to better performance.</p></p class="citation"></blockquote><h3 id=2446--161292-what-makes-an-image-realistic-lucas-theis-2024>(24/46 | 161/292) What makes an image realistic? (Lucas Theis, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lucas Theis. (2024)<br><strong>What makes an image realistic?</strong><br><button class=copy-to-clipboard title="What makes an image realistic?" index=161>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-161 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 20<br>Keywords: Adversarial Learning, Generative AI<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04493v3 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04493v3.pdf filename=2403.04493v3.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The last decade has seen tremendous progress in our ability to generate realistic-looking data, be it images, text, audio, or video. Here, we discuss the closely related problem of quantifying realism, that is, designing functions that can reliably tell realistic data from unrealistic data. This problem turns out to be significantly harder to solve and remains poorly understood, despite its prevalence in machine learning and recent breakthroughs in <b>generative</b> <b>AI.</b> Drawing on insights from algorithmic information theory, we discuss why this problem is challenging, why a good <b>generative</b> <b>model</b> alone is insufficient to solve it, and what a good solution would look like. In particular, we introduce the notion of a universal critic, which unlike <b>adversarial</b> <b>critics</b> does not require <b>adversarial</b> <b>training.</b> While universal critics are not immediately practical, they can serve both as a North Star for guiding practical implementations and as a tool for analyzing existing attempts to capture realism.</p></p class="citation"></blockquote><h3 id=2546--162292-boosting-fairness-and-robustness-in-over-the-air-federated-learning-halil-yigit-oksuz-et-al-2024>(25/46 | 162/292) Boosting Fairness and Robustness in Over-the-Air Federated Learning (Halil Yigit Oksuz et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Halil Yigit Oksuz, Fabio Molinari, Henning Sprekeler, Joerg Raisch. (2024)<br><strong>Boosting Fairness and Robustness in Over-the-Air Federated Learning</strong><br><button class=copy-to-clipboard title="Boosting Fairness and Robustness in Over-the-Air Federated Learning" index=162>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-162 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CY, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Fairness, Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04431v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04431v1.pdf filename=2403.04431v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Over-the-Air Computation is a beyond-5G communication strategy that has recently been shown to be useful for the decentralized training of machine learning models due to its efficiency. In this paper, we propose an Over-the-Air <b>federated</b> <b>learning</b> algorithm that aims to provide <b>fairness</b> and robustness through minmax optimization. By using the epigraph form of the problem at hand, we show that the proposed algorithm converges to the optimal solution of the minmax problem. Moreover, the proposed approach does not require reconstructing channel coefficients by complex encoding-decoding schemes as opposed to state-of-the-art approaches. This improves both efficiency and privacy.</p></p class="citation"></blockquote><h3 id=2646--163292-exploring-the-influence-of-dimensionality-reduction-on-anomaly-detection-performance-in-multivariate-time-series-mahsun-altin-et-al-2024>(26/46 | 163/292) Exploring the Influence of Dimensionality Reduction on Anomaly Detection Performance in Multivariate Time Series (Mahsun Altin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mahsun Altin, Altan Cakir. (2024)<br><strong>Exploring the Influence of Dimensionality Reduction on Anomaly Detection Performance in Multivariate Time Series</strong><br><button class=copy-to-clipboard title="Exploring the Influence of Dimensionality Reduction on Anomaly Detection Performance in Multivariate Time Series" index=163>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-163 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Anomaly Detection, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04429v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04429v1.pdf filename=2403.04429v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents an extensive empirical study on the integration of dimensionality reduction techniques with advanced <b>unsupervised</b> time series <b>anomaly</b> <b>detection</b> models, focusing on the MUTANT and <b>Anomaly-Transformer</b> <b>models.</b> The study involves a comprehensive evaluation across three different datasets: MSL, SMAP, and SWaT. Each dataset poses unique challenges, allowing for a robust assessment of the models&rsquo; capabilities in varied contexts. The dimensionality reduction techniques examined include PCA, UMAP, Random Projection, and t-SNE, each offering distinct advantages in simplifying high-dimensional data. Our findings reveal that dimensionality reduction not only aids in reducing computational complexity but also significantly enhances <b>anomaly</b> <b>detection</b> performance in certain scenarios. Moreover, a remarkable reduction in training times was observed, with reductions by approximately 300% and 650% when dimensionality was halved and minimized to the lowest dimensions, respectively. This efficiency gain underscores the dual benefit of dimensionality reduction in both performance enhancement and operational efficiency. The MUTANT model exhibits notable adaptability, especially with UMAP reduction, while the <b>Anomaly-Transformer</b> <b>demonstrates</b> versatility across various reduction techniques. These insights provide a deeper understanding of the synergistic effects of dimensionality reduction and <b>anomaly</b> <b>detection,</b> contributing valuable perspectives to the field of time series analysis. The study underscores the importance of selecting appropriate dimensionality reduction strategies based on specific model requirements and dataset characteristics, paving the way for more efficient, accurate, and scalable solutions in <b>anomaly</b> <b>detection.</b></p></p class="citation"></blockquote><h3 id=2746--164292-heteroswitch-characterizing-and-taming-system-induced-data-heterogeneity-in-federated-learning-gyudong-kim-et-al-2024>(27/46 | 164/292) HeteroSwitch: Characterizing and Taming System-Induced Data Heterogeneity in Federated Learning (Gyudong Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gyudong Kim, Mehdi Ghasemi, Soroush Heidari, Seungryong Kim, Young Geun Kim, Sarma Vrudhula, Carole-Jean Wu. (2024)<br><strong>HeteroSwitch: Characterizing and Taming System-Induced Data Heterogeneity in Federated Learning</strong><br><button class=copy-to-clipboard title="HeteroSwitch: Characterizing and Taming System-Induced Data Heterogeneity in Federated Learning" index=164>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-164 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-DC, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Fairness, Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04207v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04207v1.pdf filename=2403.04207v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Federated</b> <b>Learning</b> (FL) is a practical approach to train deep learning models collaboratively across user-end devices, protecting user privacy by retaining raw data on-device. In FL, participating user-end devices are highly fragmented in terms of hardware and software configurations. Such fragmentation introduces a new type of data heterogeneity in FL, namely \textit{system-induced data heterogeneity}, as each device generates distinct data depending on its hardware and software configurations. In this paper, we first characterize the impact of system-induced data heterogeneity on FL model performance. We collect a dataset using heterogeneous devices with variations across vendors and performance tiers. By using this dataset, we demonstrate that \textit{system-induced data heterogeneity} negatively impacts accuracy, and deteriorates <b>fairness</b> and domain generalization problems in FL. To address these challenges, we propose HeteroSwitch, which adaptively adopts generalization techniques (i.e., ISP transformation and SWAD) depending on the level of bias caused by varying HW and SW configurations. In our evaluation with a realistic FL dataset (FLAIR), HeteroSwitch reduces the variance of averaged precision by 6.3% across device types.</p></p class="citation"></blockquote><h3 id=2846--165292-ratsf-empowering-customer-service-volume-management-through-retrieval-augmented-time-series-forecasting-tianfeng-wang-et-al-2024>(28/46 | 165/292) RATSF: Empowering Customer Service Volume Management through Retrieval-Augmented Time-Series Forecasting (Tianfeng Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianfeng Wang, Gaojie Cui. (2024)<br><strong>RATSF: Empowering Customer Service Volume Management through Retrieval-Augmented Time-Series Forecasting</strong><br><button class=copy-to-clipboard title="RATSF: Empowering Customer Service Volume Management through Retrieval-Augmented Time-Series Forecasting" index=165>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-165 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-IR, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Recurrent Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04180v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04180v1.pdf filename=2403.04180v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>An efficient customer service management system hinges on precise forecasting of service volume. In this scenario, where data non-stationarity is pronounced, successful forecasting heavily relies on identifying and leveraging similar historical data rather than merely summarizing periodic patterns. Existing models based on <b>RNN</b> or <b>Transformer</b> architectures often struggle with this flexible and effective utilization. To address this challenge, we propose an efficient and adaptable cross-attention module termed RACA, which effectively leverages historical segments in forecasting task, and we devised a precise representation scheme for querying historical sequences, coupled with the design of a knowledge repository. These critical components collectively form our Retrieval-Augmented Temporal Sequence Forecasting framework (RATSF). RATSF not only significantly enhances performance in the context of Fliggy hotel service volume forecasting but, more crucially, can be seamlessly integrated into other <b>Transformer-based</b> time-series forecasting models across various application scenarios. Extensive experimentation has validated the effectiveness and generalizability of this system design across multiple diverse contexts.</p></p class="citation"></blockquote><h3 id=2946--166292-density-regression-efficient-and-distance-aware-deep-regressor-for-uncertainty-estimation-under-distribution-shifts-ha-manh-bui-et-al-2024>(29/46 | 166/292) Density-Regression: Efficient and Distance-Aware Deep Regressor for Uncertainty Estimation under Distribution Shifts (Ha Manh Bui et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ha Manh Bui, Anqi Liu. (2024)<br><strong>Density-Regression: Efficient and Distance-Aware Deep Regressor for Uncertainty Estimation under Distribution Shifts</strong><br><button class=copy-to-clipboard title="Density-Regression: Efficient and Distance-Aware Deep Regressor for Uncertainty Estimation under Distribution Shifts" index=166>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-166 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 13<br>Keywords: Benchmarking, Distribution Shift, Distribution Shift<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05600v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05600v1.pdf filename=2403.05600v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Morden deep ensembles technique achieves strong uncertainty estimation performance by going through multiple forward passes with different models. This is at the price of a high storage space and a slow speed in the inference (test) time. To address this issue, we propose Density-Regression, a method that leverages the density function in uncertainty estimation and achieves fast inference by a single forward pass. We prove it is distance aware on the feature space, which is a necessary condition for a neural network to produce high-quality uncertainty estimation under <b>distribution</b> <b>shifts.</b> Empirically, we conduct experiments on regression tasks with the cubic toy dataset, <b>benchmark</b> UCI, weather forecast with time series, and depth estimation under real-world shifted applications. We show that Density-Regression has competitive uncertainty estimation performance under <b>distribution</b> <b>shifts</b> with modern deep regressors while using a lower model size and a faster inference speed.</p></p class="citation"></blockquote><h3 id=3046--167292-lifelong-intelligence-beyond-the-edge-using-hyperdimensional-computing-xiaofan-yu-et-al-2024>(30/46 | 167/292) Lifelong Intelligence Beyond the Edge using Hyperdimensional Computing (Xiaofan Yu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaofan Yu, Anthony Thomas, Ivannia Gomez Moreno, Louis Gutierrez, Tajana Rosing. (2024)<br><strong>Lifelong Intelligence Beyond the Edge using Hyperdimensional Computing</strong><br><button class=copy-to-clipboard title="Lifelong Intelligence Beyond the Edge using Hyperdimensional Computing" index=167>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-167 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-NE, cs.LG<br>Keyword Score: 13<br>Keywords: Clustering, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04759v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04759v1.pdf filename=2403.04759v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>On-device learning has emerged as a prevailing trend that avoids the slow response time and costly communication of cloud-based learning. The ability to learn continuously and indefinitely in a changing environment, and with resource constraints, is critical for real sensor deployments. However, existing designs are inadequate for practical scenarios with (i) streaming data input, (ii) lack of supervision and (iii) limited on-board resources. In this paper, we design and deploy the first on-device lifelong learning system called LifeHD for general IoT applications with limited supervision. LifeHD is designed based on a novel neurally-inspired and lightweight learning paradigm called Hyperdimensional Computing (HDC). We utilize a two-tier associative memory organization to intelligently store and manage high-dimensional, low-precision vectors, which represent the historical patterns as cluster centroids. We additionally propose two variants of LifeHD to cope with scarce labeled inputs and power constraints. We implement LifeHD on off-the-shelf edge platforms and perform extensive evaluations across three scenarios. Our measurements show that LifeHD improves the <b>unsupervised</b> <b>clustering</b> accuracy by up to 74.8% compared to the state-of-the-art NN-based <b>unsupervised</b> lifelong learning baselines with as much as 34.3x better energy efficiency. Our code is available at <a href=https://github.com/Orienfish/LifeHD>https://github.com/Orienfish/LifeHD</a>.</p></p class="citation"></blockquote><h3 id=3146--168292-hyperspectral-unmixing-for-raman-spectroscopy-via-physics-constrained-autoencoders-dimitar-georgiev-et-al-2024>(31/46 | 168/292) Hyperspectral unmixing for Raman spectroscopy via physics-constrained autoencoders (Dimitar Georgiev et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dimitar Georgiev, Álvaro Fernández-Galiana, Simon Vilms Pedersen, Georgios Papadopoulos, Ruoxiao Xie, Molly M. Stevens, Mauricio Barahona. (2024)<br><strong>Hyperspectral unmixing for Raman spectroscopy via physics-constrained autoencoders</strong><br><button class=copy-to-clipboard title="Hyperspectral unmixing for Raman spectroscopy via physics-constrained autoencoders" index=168>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-168 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs.LG<br>Keyword Score: 13<br>Keywords: Autoencoder, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04526v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04526v1.pdf filename=2403.04526v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Raman spectroscopy is widely used across scientific domains to characterize the chemical composition of samples in a non-destructive, label-free manner. Many applications entail the unmixing of signals from mixtures of molecular species to identify the individual components present and their proportions, yet conventional methods for chemometrics often struggle with complex mixture scenarios encountered in practice. Here, we develop hyperspectral unmixing algorithms based on <b>autoencoder</b> neural networks, and we systematically validate them using both synthetic and experimental <b>benchmark</b> datasets created in-house. Our results demonstrate that unmixing <b>autoencoders</b> provide improved accuracy, robustness and efficiency compared to standard unmixing methods. We also showcase the applicability of <b>autoencoders</b> to complex biological settings by showing improved biochemical characterization of volumetric Raman imaging data from a monocytic cell.</p></p class="citation"></blockquote><h3 id=3246--169292-vlearn-off-policy-learning-with-efficient-state-value-function-estimation-fabian-otto-et-al-2024>(32/46 | 169/292) Vlearn: Off-Policy Learning with Efficient State-Value Function Estimation (Fabian Otto et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fabian Otto, Philipp Becker, Vien Ang Ngo, Gerhard Neumann. (2024)<br><strong>Vlearn: Off-Policy Learning with Efficient State-Value Function Estimation</strong><br><button class=copy-to-clipboard title="Vlearn: Off-Policy Learning with Efficient State-Value Function Estimation" index=169>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-169 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 13<br>Keywords: Benchmarking, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04453v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04453v1.pdf filename=2403.04453v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing off-policy <b>reinforcement</b> <b>learning</b> algorithms typically necessitate an explicit state-action-value function representation, which becomes problematic in high-dimensional action spaces. These algorithms often encounter challenges where they struggle with the curse of dimensionality, as maintaining a state-action-value function in such spaces becomes data-inefficient. In this work, we propose a novel off-policy trust region optimization approach, called Vlearn, that eliminates the requirement for an explicit state-action-value function. Instead, we demonstrate how to efficiently leverage just a state-value function as the critic, thus overcoming several limitations of existing methods. By doing so, Vlearn addresses the computational challenges posed by high-dimensional action spaces. Furthermore, Vlearn introduces an efficient approach to address the challenges associated with pure state-value function learning in the off-policy setting. This approach not only simplifies the implementation of off-policy policy gradient algorithms but also leads to consistent and robust performance across various <b>benchmark</b> tasks. Specifically, by removing the need for a state-action-value function Vlearn simplifies the learning process and allows for more efficient exploration and exploitation in complex environments</p></p class="citation"></blockquote><h3 id=3346--170292-gradient-free-neural-topology-optimization-gawel-kus-et-al-2024>(33/46 | 170/292) Gradient-free neural topology optimization (Gawel Kus et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gawel Kus, Miguel A. Bessa. (2024)<br><strong>Gradient-free neural topology optimization</strong><br><button class=copy-to-clipboard title="Gradient-free neural topology optimization" index=170>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-170 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-NA, cs.LG, math-NA<br>Keyword Score: 10<br>Keywords: Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04937v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04937v1.pdf filename=2403.04937v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Gradient-free optimizers allow for tackling problems regardless of the smoothness or differentiability of their objective function, but they require many more iterations to converge when compared to gradient-based algorithms. This has made them unviable for topology optimization due to the high computational cost per iteration and high dimensionality of these problems. We propose a pre-trained neural reparameterization strategy that leads to at least one order of magnitude decrease in iteration count when optimizing the designs in latent space, as opposed to the conventional approach without latent reparameterization. We demonstrate this via extensive computational experiments in- and <b>out-of-distribution</b> with the training data. Although gradient-based topology optimization is still more efficient for differentiable problems, such as compliance optimization of structures, we believe this work will open up a new path for problems where gradient information is not readily available (e.g. fracture).</p></p class="citation"></blockquote><h3 id=3446--171292-efficient-high-resolution-time-series-classification-via-attention-kronecker-decomposition-aosong-feng-et-al-2024>(34/46 | 171/292) Efficient High-Resolution Time Series Classification via Attention Kronecker Decomposition (Aosong Feng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aosong Feng, Jialin Chen, Juan Garza, Brooklyn Berry, Francisco Salazar, Yifeng Gao, Rex Ying, Leandros Tassiulas. (2024)<br><strong>Efficient High-Resolution Time Series Classification via Attention Kronecker Decomposition</strong><br><button class=copy-to-clipboard title="Efficient High-Resolution Time Series Classification via Attention Kronecker Decomposition" index=171>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-171 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04882v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04882v1.pdf filename=2403.04882v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The high-resolution time series classification problem is essential due to the increasing availability of detailed temporal data in various domains. To tackle this challenge effectively, it is imperative that the state-of-the-art attention model is scalable to accommodate the growing sequence lengths typically encountered in high-resolution time series data, while also demonstrating robustness in handling the inherent noise prevalent in such datasets. To address this, we propose to hierarchically encode the long time series into multiple levels based on the interaction ranges. By capturing relationships at different levels, we can build more robust, expressive, and efficient models that are capable of capturing both short-term fluctuations and long-term trends in the data. We then propose a new time series <b>transformer</b> backbone (KronTime) by introducing Kronecker-decomposed attention to process such multi-level time series, which sequentially calculates attention from the lower level to the upper level. Experiments on four long time series datasets demonstrate superior classification results with improved efficiency compared to baseline methods.</p></p class="citation"></blockquote><h3 id=3546--172292-end-to-end-conditional-robust-optimization-abhilash-chenreddy-et-al-2024>(35/46 | 172/292) End-to-end Conditional Robust Optimization (Abhilash Chenreddy et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Abhilash Chenreddy, Erick Delage. (2024)<br><strong>End-to-end Conditional Robust Optimization</strong><br><button class=copy-to-clipboard title="End-to-end Conditional Robust Optimization" index=172>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-172 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Logistic Regression<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04670v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04670v1.pdf filename=2403.04670v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The field of Contextual Optimization (CO) integrates machine learning and optimization to solve decision making problems under uncertainty. Recently, a risk sensitive variant of CO, known as Conditional Robust Optimization (CRO), combines uncertainty quantification with robust optimization in order to promote safety and reliability in high stake applications. Exploiting modern differentiable optimization methods, we propose a novel end-to-end approach to train a CRO model in a way that accounts for both the empirical risk of the prescribed decisions and the quality of conditional coverage of the contextual uncertainty set that supports them. While guarantees of success for the latter objective are impossible to obtain from the point of view of conformal prediction theory, high quality conditional coverage is achieved empirically by ingeniously employing a <b>logistic</b> <b>regression</b> differentiable layer within the calculation of coverage quality in our training loss. We show that the proposed training algorithms produce decisions that outperform the traditional estimate then optimize approaches.</p></p class="citation"></blockquote><h3 id=3646--173292-architectural-blueprint-for-heterogeneity-resilient-federated-learning-satwat-bashir-et-al-2024>(36/46 | 173/292) Architectural Blueprint For Heterogeneity-Resilient Federated Learning (Satwat Bashir et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Satwat Bashir, Tasos Dagiuklas, Kasra Kassai, Muddesar Iqbal. (2024)<br><strong>Architectural Blueprint For Heterogeneity-Resilient Federated Learning</strong><br><button class=copy-to-clipboard title="Architectural Blueprint For Heterogeneity-Resilient Federated Learning" index=173>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-173 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-DC, cs-LG, cs-NI, cs.LG<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04546v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04546v1.pdf filename=2403.04546v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper proposes a novel three tier architecture for <b>federated</b> <b>learning</b> to optimize edge computing environments. The proposed architecture addresses the challenges associated with client data heterogeneity and computational constraints. It introduces a scalable, privacy preserving framework that enhances the efficiency of distributed machine learning. Through experimentation, the paper demonstrates the architecture capability to manage non IID data sets more effectively than traditional <b>federated</b> <b>learning</b> models. Additionally, the paper highlights the potential of this innovative approach to significantly improve model accuracy, reduce communication overhead, and facilitate broader adoption of <b>federated</b> <b>learning</b> technologies.</p></p class="citation"></blockquote><h3 id=3746--174292-explainable-ai-for-embedded-systems-design-a-case-study-of-static-redundant-nvm-memory-write-prediction-abdoulaye-gamatié-et-al-2024>(37/46 | 174/292) Explainable AI for Embedded Systems Design: A Case Study of Static Redundant NVM Memory Write Prediction (Abdoulaye Gamatié et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Abdoulaye Gamatié, Yuyang Wang. (2024)<br><strong>Explainable AI for Embedded Systems Design: A Case Study of Static Redundant NVM Memory Write Prediction</strong><br><button class=copy-to-clipboard title="Explainable AI for Embedded Systems Design: A Case Study of Static Redundant NVM Memory Write Prediction" index=174>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-174 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-PL, cs-SE, cs.LG<br>Keyword Score: 10<br>Keywords: Explainable AI<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04337v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04337v1.pdf filename=2403.04337v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper investigates the application of <b>eXplainable</b> <b>Artificial</b> Intelligence (XAI) in the design of embedded systems using machine learning (ML). As a case study, it addresses the challenging problem of static silent store prediction. This involves identifying redundant memory writes based only on static program features. Eliminating such stores enhances performance and energy efficiency by reducing memory access and bus traffic, especially in the presence of emerging non-volatile memory technologies. To achieve this, we propose a methodology consisting of: 1) the development of relevant ML models for explaining silent store prediction, and 2) the application of XAI to explain these models. We employ two state-of-the-art model-agnostic XAI methods to analyze the causes of silent stores. Through the case study, we evaluate the effectiveness of the methods. We find that these methods provide explanations for silent store predictions, which are consistent with known causes of silent store occurrences from previous studies. Typically, this allows us to confirm the prevalence of silent stores in operations that write the zero constant into memory, or the absence of silent stores in operations involving loop induction variables. This suggests the potential relevance of XAI in analyzing ML models&rsquo; decision in embedded system design. From the case study, we share some valuable insights and pitfalls we encountered. More generally, this study aims to lay the groundwork for future research in the emerging field of XAI for embedded system design.</p></p class="citation"></blockquote><h3 id=3846--175292-mastering-memory-tasks-with-world-models-mohammad-reza-samsami-et-al-2024>(38/46 | 175/292) Mastering Memory Tasks with World Models (Mohammad Reza Samsami et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohammad Reza Samsami, Artem Zholus, Janarthanan Rajendran, Sarath Chandar. (2024)<br><strong>Mastering Memory Tasks with World Models</strong><br><button class=copy-to-clipboard title="Mastering Memory Tasks with World Models" index=175>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-175 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04253v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04253v1.pdf filename=2403.04253v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Current model-based <b>reinforcement</b> <b>learning</b> (MBRL) agents struggle with long-term dependencies. This limits their ability to effectively solve tasks involving extended time gaps between actions and outcomes, or tasks demanding the recalling of distant observations to inform current actions. To improve temporal coherence, we integrate a new family of state space models (SSMs) in world models of MBRL agents to present a new method, Recall to Imagine (R2I). This integration aims to enhance both long-term memory and long-horizon credit assignment. Through a diverse set of illustrative tasks, we systematically demonstrate that R2I not only establishes a new state-of-the-art for challenging memory and credit assignment RL tasks, such as BSuite and POPGym, but also showcases superhuman performance in the complex memory domain of Memory Maze. At the same time, it upholds comparable performance in classic RL tasks, such as Atari and DMC, suggesting the generality of our method. We also show that R2I is faster than the state-of-the-art MBRL method, DreamerV3, resulting in faster wall-time convergence.</p></p class="citation"></blockquote><h3 id=3946--176292-fill-and-spill-deep-reinforcement-learning-policy-gradient-methods-for-reservoir-operation-decision-and-control-sadegh-sadeghi-tabas-et-al-2024>(39/46 | 176/292) Fill-and-Spill: Deep Reinforcement Learning Policy Gradient Methods for Reservoir Operation Decision and Control (Sadegh Sadeghi Tabas et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sadegh Sadeghi Tabas, Vidya Samadi. (2024)<br><strong>Fill-and-Spill: Deep Reinforcement Learning Policy Gradient Methods for Reservoir Operation Decision and Control</strong><br><button class=copy-to-clipboard title="Fill-and-Spill: Deep Reinforcement Learning Policy Gradient Methods for Reservoir Operation Decision and Control" index=176>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-176 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-OC<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04195v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04195v1.pdf filename=2403.04195v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Changes in demand, various hydrological inputs, and environmental stressors are among the issues that water managers and policymakers face on a regular basis. These concerns have sparked interest in applying different techniques to determine reservoir operation policy decisions. As the resolution of the analysis increases, it becomes more difficult to effectively represent a real-world system using traditional methods such as Dynamic Programming (DP) and Stochastic Dynamic Programming (SDP) for determining the best reservoir operation policy. One of the challenges is the &ldquo;curse of dimensionality,&rdquo; which means the number of samples needed to estimate an arbitrary function with a given level of accuracy grows exponentially with respect to the number of input variables (i.e., dimensionality) of the function. Deep <b>Reinforcement</b> <b>Learning</b> (DRL) is an intelligent approach to overcome the curses of stochastic optimization problems for reservoir operation policy decisions. To our knowledge, this study is the first attempt that examine various novel DRL continuous-action policy gradient methods (PGMs), including Deep Deterministic Policy Gradients (DDPG), Twin Delayed DDPG (TD3), and two different versions of Soft Actor-Critic (SAC18 and SAC19) for optimizing reservoir operation policy. In this study, multiple DRL techniques were implemented in order to find the optimal operation policy of Folsom Reservoir in California, USA. The reservoir system supplies agricultural, municipal, hydropower, and environmental flow demands and flood control operations to the City of Sacramento. Analysis suggests that the TD3 and SAC are robust to meet the Folsom Reservoir&rsquo;s demands and optimize reservoir operation policies.</p></p class="citation"></blockquote><h3 id=4046--177292-noisy-spiking-actor-network-for-exploration-ding-chen-et-al-2024>(40/46 | 177/292) Noisy Spiking Actor Network for Exploration (Ding Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ding Chen, Peixi Peng, Tiejun Huang, Yonghong Tian. (2024)<br><strong>Noisy Spiking Actor Network for Exploration</strong><br><button class=copy-to-clipboard title="Noisy Spiking Actor Network for Exploration" index=177>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-177 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-NE, cs.LG<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04162v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04162v1.pdf filename=2403.04162v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As a general method for exploration in deep <b>reinforcement</b> <b>learning</b> (RL), NoisyNet can produce problem-specific exploration strategies. Spiking neural networks (SNNs), due to their binary firing mechanism, have strong robustness to noise, making it difficult to realize efficient exploration with local disturbances. To solve this exploration problem, we propose a noisy spiking actor network (NoisySAN) that introduces time-correlated noise during charging and transmission. Moreover, a noise reduction method is proposed to find a stable policy for the agent. Extensive experimental results demonstrate that our method outperforms the state-of-the-art performance on a wide range of continuous control tasks from OpenAI gym.</p></p class="citation"></blockquote><h3 id=4146--178292-stabilizing-policy-gradients-for-stochastic-differential-equations-via-consistency-with-perturbation-process-xiangxin-zhou-et-al-2024>(41/46 | 178/292) Stabilizing Policy Gradients for Stochastic Differential Equations via Consistency with Perturbation Process (Xiangxin Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiangxin Zhou, Liang Wang, Yichi Zhou. (2024)<br><strong>Stabilizing Policy Gradients for Stochastic Differential Equations via Consistency with Perturbation Process</strong><br><button class=copy-to-clipboard title="Stabilizing Policy Gradients for Stochastic Differential Equations via Consistency with Perturbation Process" index=178>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-178 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04154v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04154v1.pdf filename=2403.04154v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Considering generating samples with high rewards, we focus on optimizing deep neural networks parameterized stochastic differential equations (SDEs), the advanced generative models with high expressiveness, with policy gradient, the leading algorithm in <b>reinforcement</b> <b>learning.</b> Nevertheless, when applying policy gradients to SDEs, since the policy gradient is estimated on a finite set of trajectories, it can be ill-defined, and the policy behavior in data-scarce regions may be uncontrolled. This challenge compromises the stability of policy gradients and negatively impacts sample complexity. To address these issues, we propose constraining the SDE to be consistent with its associated perturbation process. Since the perturbation process covers the entire space and is easy to sample, we can mitigate the aforementioned problems. Our framework offers a general approach allowing for a versatile selection of policy gradient methods to effectively and efficiently train SDEs. We evaluate our algorithm on the task of structure-based drug design and optimize the binding affinity of generated ligand molecules. Our method achieves the best Vina score -9.07 on the CrossDocked2020 dataset.</p></p class="citation"></blockquote><h3 id=4246--179292-fl-guard-a-holistic-framework-for-run-time-detection-and-recovery-of-negative-federated-learning-hong-lin-et-al-2024>(42/46 | 179/292) FL-GUARD: A Holistic Framework for Run-Time Detection and Recovery of Negative Federated Learning (Hong Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hong Lin, Lidan Shou, Ke Chen, Gang Chen, Sai Wu. (2024)<br><strong>FL-GUARD: A Holistic Framework for Run-Time Detection and Recovery of Negative Federated Learning</strong><br><button class=copy-to-clipboard title="FL-GUARD: A Holistic Framework for Run-Time Detection and Recovery of Negative Federated Learning" index=179>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-179 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-DC, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04146v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04146v1.pdf filename=2403.04146v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Federated</b> <b>learning</b> (FL) is a promising approach for learning a model from data distributed on massive clients without exposing data privacy. It works effectively in the ideal federation where clients share homogeneous data distribution and learning behavior. However, FL may fail to function appropriately when the federation is not ideal, amid an unhealthy state called Negative <b>Federated</b> <b>Learning</b> (NFL), in which most clients gain no benefit from participating in FL. Many studies have tried to address NFL. However, their solutions either (1) predetermine to prevent NFL in the entire learning life-cycle or (2) tackle NFL in the aftermath of numerous learning rounds. Thus, they either (1) indiscriminately incur extra costs even if FL can perform well without such costs or (2) waste numerous learning rounds. Additionally, none of the previous work takes into account the clients who may be unwilling/unable to follow the proposed NFL solutions when using those solutions to upgrade an FL system in use. This paper introduces FL-GUARD, a holistic framework that can be employed on any FL system for tackling NFL in a run-time paradigm. That is, to dynamically detect NFL at the early stage (tens of rounds) of learning and then to activate recovery measures when necessary. Specifically, we devise a cost-effective NFL detection mechanism, which relies on an estimation of performance gain on clients. Only when NFL is detected, we activate the NFL recovery process, in which each client learns in parallel an adapted model when training the global model. Extensive experiment results confirm the effectiveness of FL-GUARD in detecting NFL and recovering from NFL to a healthy learning state. We also show that FL-GUARD is compatible with previous NFL solutions and robust against clients unwilling/unable to take any recovery measures.</p></p class="citation"></blockquote><h3 id=4346--180292-dissecting-sample-hardness-a-fine-grained-analysis-of-hardness-characterization-methods-for-data-centric-ai-nabeel-seedat-et-al-2024>(43/46 | 180/292) Dissecting Sample Hardness: A Fine-Grained Analysis of Hardness Characterization Methods for Data-Centric AI (Nabeel Seedat et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nabeel Seedat, Fergus Imrie, Mihaela van der Schaar. (2024)<br><strong>Dissecting Sample Hardness: A Fine-Grained Analysis of Hardness Characterization Methods for Data-Centric AI</strong><br><button class=copy-to-clipboard title="Dissecting Sample Hardness: A Fine-Grained Analysis of Hardness Characterization Methods for Data-Centric AI" index=180>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-180 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 6<br>Keywords: Benchmarking, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04551v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04551v1.pdf filename=2403.04551v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Characterizing samples that are difficult to learn from is crucial to developing highly performant ML models. This has led to numerous Hardness Characterization Methods (HCMs) that aim to identify &ldquo;hard&rdquo; samples. However, there is a lack of consensus regarding the definition and evaluation of &ldquo;hardness&rdquo;. Unfortunately, current HCMs have only been evaluated on specific types of hardness and often only qualitatively or with respect to downstream performance, overlooking the fundamental quantitative identification task. We address this gap by presenting a fine-grained taxonomy of hardness types. Additionally, we propose the Hardness Characterization Analysis Toolkit (H-CAT), which supports comprehensive and quantitative <b>benchmarking</b> of HCMs across the hardness taxonomy and can easily be extended to new HCMs, hardness types, and datasets. We use H-CAT to evaluate 13 different HCMs across 8 hardness types. This comprehensive evaluation encompassing over 14K setups uncovers strengths and weaknesses of different HCMs, leading to practical tips to guide HCM selection and future development. Our findings highlight the need for more comprehensive HCM evaluation, while we hope our hardness taxonomy and toolkit will advance the principled evaluation and uptake of data-centric AI methods.</p></p class="citation"></blockquote><h3 id=4446--181292-frri-a-novel-algorithm-for-fuzzy-rough-rule-induction-henri-bollaert-et-al-2024>(44/46 | 181/292) FRRI: a novel algorithm for fuzzy-rough rule induction (Henri Bollaert et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Henri Bollaert, Marko Palangetić, Chris Cornelis, Salvatore Greco, Roman Słowiński. (2024)<br><strong>FRRI: a novel algorithm for fuzzy-rough rule induction</strong><br><button class=copy-to-clipboard title="FRRI: a novel algorithm for fuzzy-rough rule induction" index=181>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-181 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 5<br>Keywords: Black Box<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04447v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04447v1.pdf filename=2403.04447v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Interpretability is the next frontier in machine learning research. In the search for white box models - as opposed to <b>black</b> <b>box</b> models, like random forests or neural networks - rule induction algorithms are a logical and promising option, since the rules can easily be understood by humans. Fuzzy and rough set theory have been successfully applied to this archetype, almost always separately. As both approaches to rule induction involve granular computing based on the concept of equivalence classes, it is natural to combine them. The QuickRules\cite{JensenCornelis2009} algorithm was a first attempt at using fuzzy rough set theory for rule induction. It is based on QuickReduct, a greedy algorithm for building decision reducts. QuickRules already showed an improvement over other rule induction methods. However, to evaluate the full potential of a fuzzy rough rule induction algorithm, one needs to start from the foundations. In this paper, we introduce a novel rule induction algorithm called Fuzzy Rough Rule Induction (FRRI). We provide background and explain the workings of our algorithm. Furthermore, we perform a computational experiment to evaluate the performance of our algorithm and compare it to other state-of-the-art rule induction approaches. We find that our algorithm is more accurate while creating small rulesets consisting of relatively short rules. We end the paper by outlining some directions for future work.</p></p class="citation"></blockquote><h3 id=4546--182292-cooperative-bayesian-optimization-for-imperfect-agents-ali-khoshvishkaie-et-al-2024>(45/46 | 182/292) Cooperative Bayesian Optimization for Imperfect Agents (Ali Khoshvishkaie et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ali Khoshvishkaie, Petrus Mikkola, Pierre-Alexandre Murena, Samuel Kaski. (2024)<br><strong>Cooperative Bayesian Optimization for Imperfect Agents</strong><br><button class=copy-to-clipboard title="Cooperative Bayesian Optimization for Imperfect Agents" index=182>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-182 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-MA, cs.LG<br>Keyword Score: 5<br>Keywords: Black Box<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04442v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04442v1.pdf filename=2403.04442v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce a cooperative Bayesian optimization problem for optimizing <b>black-box</b> <b>functions</b> of two variables where two agents choose together at which points to query the function but have only control over one variable each. This setting is inspired by human-AI teamwork, where an AI-assistant helps its human user solve a problem, in this simplest case, collaborative optimization. We formulate the solution as sequential decision-making, where the agent we control models the user as a computationally rational agent with prior knowledge about the function. We show that strategic planning of the queries enables better identification of the global maximum of the function as long as the user avoids excessive exploration. This planning is made possible by using Bayes Adaptive Monte Carlo planning and by endowing the agent with a user model that accounts for conservative belief updates and exploratory sampling of the points to query.</p></p class="citation"></blockquote><h3 id=4646--183292-minimizing-the-thompson-sampling-regret-to-sigma-ratio-ts-rsr-a-provably-efficient-algorithm-for-batch-bayesian-optimization-zhaolin-ren-et-al-2024>(46/46 | 183/292) Minimizing the Thompson Sampling Regret-to-Sigma Ratio (TS-RSR): a provably efficient algorithm for batch Bayesian Optimization (Zhaolin Ren et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhaolin Ren, Na Li. (2024)<br><strong>Minimizing the Thompson Sampling Regret-to-Sigma Ratio (TS-RSR): a provably efficient algorithm for batch Bayesian Optimization</strong><br><button class=copy-to-clipboard title="Minimizing the Thompson Sampling Regret-to-Sigma Ratio (TS-RSR): a provably efficient algorithm for batch Bayesian Optimization" index=183>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-183 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-OC, stat-ML<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04764v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04764v1.pdf filename=2403.04764v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a new approach for batch Bayesian Optimization (BO), where the sampling takes place by minimizing a Thompson Sampling approximation of a regret to uncertainty ratio. Our objective is able to coordinate the actions chosen in each batch in a way that minimizes redundancy between points whilst focusing on points with high predictive means or high uncertainty. We provide high-probability theoretical guarantees on the regret of our algorithm. Finally, numerically, we demonstrate that our method attains state-of-the-art performance on a range of nonconvex test functions, where it outperforms several competitive <b>benchmark</b> batch BO algorithms by an order of magnitude on average.</p></p class="citation"></blockquote><h2 id=eessiv-5>eess.IV (5)</h2><h3 id=15--184292-beyond-multiple-instance-learning-full-resolution-all-in-memory-end-to-end-pathology-slide-modeling-gabriele-campanella-et-al-2024>(1/5 | 184/292) Beyond Multiple Instance Learning: Full Resolution All-In-Memory End-To-End Pathology Slide Modeling (Gabriele Campanella et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gabriele Campanella, Eugene Fluder, Jennifer Zeng, Chad Vanderbilt, Thomas J. Fuchs. (2024)<br><strong>Beyond Multiple Instance Learning: Full Resolution All-In-Memory End-To-End Pathology Slide Modeling</strong><br><button class=copy-to-clipboard title="Beyond Multiple Instance Learning: Full Resolution All-In-Memory End-To-End Pathology Slide Modeling" index=184>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-184 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 70<br>Keywords: Foundation Model, Multiple Instance Learning, Supervised Learning, Supervised Learning, Weakly-supervised Learning, Weakly-supervised Learning, Weakly Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04865v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04865v1.pdf filename=2403.04865v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Artificial Intelligence (AI) has great potential to improve health outcomes by training systems on vast digitized clinical datasets. Computational Pathology, with its massive amounts of microscopy image data and impact on diagnostics and biomarkers, is at the forefront of this development. Gigapixel pathology slides pose a unique challenge due to their enormous size and are usually divided into tens of thousands of smaller tiles for analysis. This results in a discontinuity in the machine learning process by separating the training of tile-level encoders from slide-level aggregators and the need to adopt <b>weakly</b> <b>supervised</b> <b>learning</b> strategies. Training models from entire pathology slides end-to-end has been largely unexplored due to its computational challenges. To overcome this problem, we propose a novel approach to jointly train both a tile encoder and a slide-aggregator fully in memory and end-to-end at high-resolution, bridging the gap between input and slide-level supervision. While more computationally expensive, detailed quantitative validation shows promise for large-scale pre-training of pathology <b>foundation</b> <b>models.</b></p></p class="citation"></blockquote><h3 id=25--185292-medflip-medical-vision-and-language-self-supervised-fast-pre-training-with-masked-autoencoder-lei-li-et-al-2024>(2/5 | 185/292) MedFLIP: Medical Vision-and-Language Self-supervised Fast Pre-Training with Masked Autoencoder (Lei Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lei Li, Tianfang Zhang, Xinglin Zhang, Jiaqi Liu, Bingqi Ma, Yan Luo, Tao Chen. (2024)<br><strong>MedFLIP: Medical Vision-and-Language Self-supervised Fast Pre-Training with Masked Autoencoder</strong><br><button class=copy-to-clipboard title="MedFLIP: Medical Vision-and-Language Self-supervised Fast Pre-Training with Masked Autoencoder" index=185>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-185 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CL, cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 61<br>Keywords: Autoencoder, Multi-modal, Multi-modal, Representation Learning, Self-supervised Learning, Zero-shot, Vision-and-Language, Zero-shot Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04626v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04626v1.pdf filename=2403.04626v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Within the domain of medical analysis, extensive research has explored the potential of mutual learning between Masked Autoencoders(MAEs) and <b>multimodal</b> data. However, the impact of MAEs on intermodality remains a key challenge. We introduce MedFLIP, a Fast Language-Image Pre-training method for Medical analysis. We explore MAEs for <b>zero-shot</b> <b>learning</b> with crossed domains, which enhances the model ability to learn from limited data, a common scenario in medical diagnostics. We verify that masking an image does not affect intermodal learning. Furthermore, we propose the SVD loss to enhance the <b>representation</b> <b>learning</b> for characteristics of medical images, aiming to improve classification accuracy by leveraging the structural intricacies of such data. Lastly, we validate using language will improve the <b>zero-shot</b> <b>performance</b> for the medical image analysis. MedFLIP scaling of the masking process marks an advancement in the field, offering a pathway to rapid and precise medical image analysis without the traditional computational bottlenecks. Through experiments and validation, MedFLIP demonstrates efficient performance improvements, setting an explored standard for future research and application in medical diagnostics.</p></p class="citation"></blockquote><h3 id=35--186292-a-domain-translation-framework-with-an-adversarial-denoising-diffusion-model-to-generate-synthetic-datasets-of-echocardiography-images-cristiana-tiago-et-al-2024>(3/5 | 186/292) A Domain Translation Framework with an Adversarial Denoising Diffusion Model to Generate Synthetic Datasets of Echocardiography Images (Cristiana Tiago et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Cristiana Tiago, Sten Roar Snare, Jurica Sprem, Kristin McLeod. (2024)<br><strong>A Domain Translation Framework with an Adversarial Denoising Diffusion Model to Generate Synthetic Datasets of Echocardiography Images</strong><br><button class=copy-to-clipboard title="A Domain Translation Framework with an Adversarial Denoising Diffusion Model to Generate Synthetic Datasets of Echocardiography Images" index=186>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-186 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-AI, cs-CV, eess-IV, eess.IV<br>Keyword Score: 30<br>Keywords: Diffusion Model, Generative Adversarial Network, Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04612v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04612v1.pdf filename=2403.04612v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Currently, medical image domain translation operations show a high demand from researchers and clinicians. Amongst other capabilities, this task allows the generation of new medical images with sufficiently high image quality, making them clinically relevant. Deep Learning (DL) architectures, most specifically deep <b>generative</b> <b>models,</b> <b>are</b> widely used to generate and translate images from one domain to another. The proposed framework relies on an adversarial Denoising <b>Diffusion</b> <b>Model</b> (DDM) to synthesize echocardiography images and perform domain translation. Contrary to <b>Generative</b> <b>Adversarial</b> <b>Networks</b> <b>(GANs),</b> DDMs are able to generate high quality image samples with a large diversity. If a DDM is combined with a <b>GAN,</b> this ability to generate new data is completed at an even faster sampling time. In this work we trained an adversarial DDM combined with a <b>GAN</b> to learn the reverse denoising process, relying on a guide image, making sure relevant anatomical structures of each echocardiography image were kept and represented on the generated image samples. For several domain translation operations, the results verified that such <b>generative</b> <b>model</b> <b>was</b> able to synthesize high quality image samples: MSE: 11.50 +/- 3.69, PSNR (dB): 30.48 +/- 0.09, SSIM: 0.47 +/- 0.03. The proposed method showed high generalization ability, introducing a framework to create echocardiography images suitable to be used for clinical research purposes.</p></p class="citation"></blockquote><h3 id=45--187292-medm2g-unifying-medical-multi-modal-generation-via-cross-guided-diffusion-with-visual-invariant-chenlu-zhan-et-al-2024>(4/5 | 187/292) MedM2G: Unifying Medical Multi-Modal Generation via Cross-Guided Diffusion with Visual Invariant (Chenlu Zhan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chenlu Zhan, Yu Lin, Gaoang Wang, Hongwei Wang, Jian Wu. (2024)<br><strong>MedM2G: Unifying Medical Multi-Modal Generation via Cross-Guided Diffusion with Visual Invariant</strong><br><button class=copy-to-clipboard title="MedM2G: Unifying Medical Multi-Modal Generation via Cross-Guided Diffusion with Visual Invariant" index=187>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-187 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 23<br>Keywords: Multi-modal, Image2text, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04290v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04290v1.pdf filename=2403.04290v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Medical generative models, acknowledged for their high-quality sample generation ability, have accelerated the fast growth of medical applications. However, recent works concentrate on separate medical generation models for distinct medical tasks and are restricted to inadequate medical <b>multi-modal</b> knowledge, constraining medical comprehensive diagnosis. In this paper, we propose MedM2G, a Medical <b>Multi-Modal</b> Generative framework, with the key innovation to align, extract, and generate medical <b>multi-modal</b> within a unified model. Extending beyond single or two medical modalities, we efficiently align medical <b>multi-modal</b> through the central alignment approach in the unified space. Significantly, our framework extracts valuable clinical knowledge by preserving the medical visual invariant of each imaging modal, thereby enhancing specific medical information for <b>multi-modal</b> generation. By conditioning the adaptive cross-guided parameters into the multi-flow diffusion framework, our model promotes flexible interactions among medical <b>multi-modal</b> for generation. MedM2G is the first medical generative model that unifies medical generation tasks of <b>text-to-image,</b> <b>image-to-text,</b> and unified generation of medical modalities (CT, MRI, X-ray). It performs 5 medical generation tasks across 10 datasets, consistently outperforming various state-of-the-art works.</p></p class="citation"></blockquote><h3 id=55--188292-improved-focus-on-hard-samples-for-lung-nodule-detection-yujiang-chen-et-al-2024>(5/5 | 188/292) Improved Focus on Hard Samples for Lung Nodule Detection (Yujiang Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yujiang Chen, Mei Xie. (2024)<br><strong>Improved Focus on Hard Samples for Lung Nodule Detection</strong><br><button class=copy-to-clipboard title="Improved Focus on Hard Samples for Lung Nodule Detection" index=188>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-188 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04478v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04478v1.pdf filename=2403.04478v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, lung nodule detection methods based on deep learning have shown excellent performance in the medical image processing field. Considering that only a few public lung datasets are available and lung nodules are more difficult to detect in CT images than in natural images, the existing methods face many bottlenecks when detecting lung nodules, especially hard ones in CT images. In order to solve these problems, we plan to enhance the focus of our network. In this work, we present an improved detection network that pays more attention to hard samples and datasets to deal with lung nodules by introducing deformable <b>convolution</b> and self-paced learning. Experiments on the LUNA16 dataset demonstrate the effectiveness of our proposed components and show that our method has reached competitive performance.</p></p class="citation"></blockquote><h2 id=csdb-3>cs.DB (3)</h2><h3 id=13--189292-promoai-process-modeling-with-generative-ai-humam-kourani-et-al-2024>(1/3 | 189/292) ProMoAI: Process Modeling with Generative AI (Humam Kourani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Humam Kourani, Alessandro Berti, Daniel Schuster, Wil M. P. van der Aalst. (2024)<br><strong>ProMoAI: Process Modeling with Generative AI</strong><br><button class=copy-to-clipboard title="ProMoAI: Process Modeling with Generative AI" index=189>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-189 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DB<br>Categories: cs-CL, cs-DB, cs.DB<br>Keyword Score: 50<br>Keywords: Generative AI, Code Generation, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04327v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04327v1.pdf filename=2403.04327v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>ProMoAI is a novel tool that leverages <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> to automatically generate process models from textual descriptions, incorporating advanced <b>prompt</b> engineering, error handling, and <b>code</b> <b>generation</b> techniques. Beyond automating the generation of complex process models, ProMoAI also supports process model optimization. Users can interact with the tool by providing feedback on the generated model, which is then used for refining the process model. ProMoAI utilizes the capabilities <b>LLMs</b> to offer a novel, AI-driven approach to process modeling, significantly reducing the barrier to entry for users without deep technical knowledge in process modeling.</p></p class="citation"></blockquote><h3 id=23--190292-evaluation-of-nosql-in-the-energy-marketplace-with-graphql-optimization-michael-howard-2024>(2/3 | 190/292) Evaluation of NoSQL in the Energy Marketplace with GraphQL Optimization (Michael Howard, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michael Howard. (2024)<br><strong>Evaluation of NoSQL in the Energy Marketplace with GraphQL Optimization</strong><br><button class=copy-to-clipboard title="Evaluation of NoSQL in the Energy Marketplace with GraphQL Optimization" index=190>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-190 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DB<br>Categories: cs-DB, cs.DB<br>Keyword Score: 6<br>Keywords: Benchmarking, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04935v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04935v1.pdf filename=2403.04935v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The growing popularity of electric vehicles in the United States requires an ever-expanding infrastructure of commercial DC fast charging stations. The U.S. Department of Energy estimates 33,355 publicly available DC fast charging stations as of September 2023. Range anxiety is an important impediment to the adoption of electric vehicles and is even more relevant in underserved regions in the country. The peer-to-peer energy marketplace helps fill the demand by allowing private home and small business owners to rent their 240 Volt, level-2 charging facilities. The existing, publicly accessible outlets are wrapped with a Cloud-connected microcontroller managing security and charging sessions. These microcontrollers act as Edge devices communicating with a Cloud message broker, while both buyer and seller users interact with the framework via a web-based user interface. The database storage used by the marketplace framework is a key component in both the cost of development and the performance that contributes to the user experience. A traditional storage solution is the SQL database. However, difficulty in scaling across multiple nodes and cost of its server-based compute have resulted in a trend in the last 20 years towards other NoSQL, serverless approaches. In this study, we evaluate the NoSQL vs. SQL solutions through a comparison of Google Cloud Firestore and Cloud SQL MySQL offerings. The comparison pits Google&rsquo;s serverless, document-model, non-relational, NoSQL against the server-base, table-model, relational, SQL service. The evaluation is based on query latency, flexibility/scalability, and cost criteria. Through <b>benchmarking</b> and analysis of the architecture, we determine whether Firestore can support the energy marketplace storage needs and if the introduction of a GraphQL middleware layer can overcome its deficiencies.</p></p class="citation"></blockquote><h3 id=33--191292-mining-transactional-data-to-produce-extended-association-rules-using-collaborative-apriori-fsa-red-and-m5p-predictive-algorithm-as-a-basis-of-business-actions-feri-sulianta-et-al-2024>(3/3 | 191/292) Mining Transactional Data To Produce Extended Association Rules Using Collaborative Apriori, Fsa-Red And M5p Predictive Algorithm As A Basis Of Business Actions (Feri Sulianta et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Feri Sulianta, Laksana Eka Angga, Thee Houw Liong. (2024)<br><strong>Mining Transactional Data To Produce Extended Association Rules Using Collaborative Apriori, Fsa-Red And M5p Predictive Algorithm As A Basis Of Business Actions</strong><br><button class=copy-to-clipboard title="Mining Transactional Data To Produce Extended Association Rules Using Collaborative Apriori, Fsa-Red And M5p Predictive Algorithm As A Basis Of Business Actions" index=191>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-191 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DB<br>Categories: cs-DB, cs.DB<br>Keyword Score: 3<br>Keywords: Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04179v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04179v1.pdf filename=2403.04179v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>There are large amounts of transactional data which showed consumer shopping cart at a store that sells more than 150 types of products. In this case, the company is utilizing these data in making business action. In previous studies, the data that has a lot of attributes and record data reduction algorithms handled by the FSA Red (Feature Selection for Association Rules)are then mined using Apriori algorithm. The resulting association rules have high levels of accuracy and excellent test results, which rely more than 90%. In this study, the association rules generated in previous research will be updated by using prediction algorithms M5P, so that the association rules can be used within a period of several months in the future. Furthermore, some data mining technique such as: <b>clustering</b> and time series pattern will be implemented to examine the truth and extend the validity of association rules which were built. It can be concluded that the association rules were established after will generate strong association rules with confidence equal or higher than 70% and the rules established truth can be seen from the time series pattern on each group of goods which are then used as the basis of business actions.</p></p class="citation"></blockquote><h2 id=cscr-6>cs.CR (6)</h2><h3 id=16--192292-vaemax-open-set-intrusion-detection-based-on-openmax-and-variational-autoencoder-zhiyin-qiu-et-al-2024>(1/6 | 192/292) VAEMax: Open-Set Intrusion Detection based on OpenMax and Variational Autoencoder (Zhiyin Qiu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhiyin Qiu, Ding Zhou, Yahui Zhai, Bo Liu, Lei He, Jiuxin Cao. (2024)<br><strong>VAEMax: Open-Set Intrusion Detection based on OpenMax and Variational Autoencoder</strong><br><button class=copy-to-clipboard title="VAEMax: Open-Set Intrusion Detection based on OpenMax and Variational Autoencoder" index=192>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-192 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 50<br>Keywords: Autoencoder, Convolution, Convolutional Neural Network, Reconstruction Loss, Variational Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04193v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04193v1.pdf filename=2403.04193v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Promptly discovering unknown network attacks is critical for reducing the risk of major loss imposed on system or equipment. This paper aims to develop an open-set intrusion detection model to classify known attacks as well as inferring unknown ones. To achieve this, we employ OpenMax and <b>variational</b> <b>autoencoder</b> to propose a dual detection model, VAEMax. First, we extract flow payload feature based on one-dimensional <b>convolutional</b> <b>neural</b> <b>network.</b> Then, the OpenMax is used to classify flows, during which some unknown attacks can be detected, while the rest are misclassified into a certain class of known flows. Finally, use VAE to perform secondary detection on each class of flows, and determine whether the flow is an unknown attack based on the <b>reconstruction</b> <b>loss.</b> Experiments performed on dataset CIC-IDS2017 and CSE-CIC-IDS2018 show our approach is better than baseline models and can be effectively applied to realistic network environments.</p></p class="citation"></blockquote><h3 id=26--193292-membership-inference-attacks-and-privacy-in-topic-modeling-nico-manzonelli-et-al-2024>(2/6 | 193/292) Membership Inference Attacks and Privacy in Topic Modeling (Nico Manzonelli et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nico Manzonelli, Wanrong Zhang, Salil Vadhan. (2024)<br><strong>Membership Inference Attacks and Privacy in Topic Modeling</strong><br><button class=copy-to-clipboard title="Membership Inference Attacks and Privacy in Topic Modeling" index=193>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-193 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CL, cs-CR, cs-LG, cs.CR<br>Keyword Score: 30<br>Keywords: Topic Model, Large Language Model, Topic Modeling<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04451v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04451v1.pdf filename=2403.04451v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent research shows that <b>large</b> <b>language</b> <b>models</b> are susceptible to privacy attacks that infer aspects of the training data. However, it is unclear if simpler generative models, like <b>topic</b> <b>models,</b> share similar vulnerabilities. In this work, we propose an attack against <b>topic</b> <b>models</b> that can confidently identify members of the training data in Latent Dirichlet Allocation. Our results suggest that the privacy risks associated with generative modeling are not restricted to <b>large</b> <b>neural</b> <b>models.</b> Additionally, to mitigate these vulnerabilities, we explore differentially private (DP) <b>topic</b> <b>modeling.</b> We propose a framework for private <b>topic</b> <b>modeling</b> that incorporates DP vocabulary selection as a pre-processing step, and show that it improves privacy while having limited effects on practical utility.</p></p class="citation"></blockquote><h3 id=36--194292-secure-information-embedding-and-extraction-in-forensic-3d-fingerprinting-canran-wang-et-al-2024>(3/6 | 194/292) Secure Information Embedding and Extraction in Forensic 3D Fingerprinting (Canran Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Canran Wang, Jinwen Wang, Mi Zhou, Vinh Pham, Senyue Hao, Chao Zhou, Ning Zhang, Netanel Raviv. (2024)<br><strong>Secure Information Embedding and Extraction in Forensic 3D Fingerprinting</strong><br><button class=copy-to-clipboard title="Secure Information Embedding and Extraction in Forensic 3D Fingerprinting" index=194>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-194 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 10<br>Keywords: Information Retrieval<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04918v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04918v1.pdf filename=2403.04918v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The prevalence of 3D printing poses a significant risk to public safety, as any individual with internet access and a commodity printer is able to produce untraceable firearms, keys, counterfeit products, etc. To aid government authorities in combating these new security threats, several approaches have been taken to tag 3D-prints with identifying <b>information.</b> <b>Known</b> as fingerprints, this <b>information</b> <b>is</b> written into the object using various bit embedding techniques; examples include varying the height of the molten thermoplastic layers, and depositing metallic powder with different magnetic properties. Yet, the practicality of theses techniques in real-world forensic settings is hindered by the adversarial nature of this problem. That is, the 3D-printing process is out of reach of any law enforcement agencies; it is the adversary who controls all aspects of printing and possesses the printed object. To combat these threats, law enforcement agencies can regulate the manufacturing of 3D printers, on which they may enforce a fingerprinting scheme, and collect adversarially tampered remains (e.g., fragments of a broken 3D-printed firearm) during forensic investigation. Therefore, it is important to devise fingerprinting techniques so that the fingerprint could be extracted even if printing is carried out by the adversary. To this end, we present SIDE (Secure <b>Information</b> <b>Embedding</b> and Extraction), a fingerprinting framework that tackles the adversarial nature of forensic fingerprinting in 3D prints by offering both secure <b>information</b> <b>embedding</b> and secure <b>information</b> <b>extraction.</b></p></p class="citation"></blockquote><h3 id=46--195292-privacy-amplification-for-the-gaussian-mechanism-via-bounded-support-shengyuan-hu-et-al-2024>(4/6 | 195/292) Privacy Amplification for the Gaussian Mechanism via Bounded Support (Shengyuan Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shengyuan Hu, Saeed Mahloujifar, Virginia Smith, Kamalika Chaudhuri, Chuan Guo. (2024)<br><strong>Privacy Amplification for the Gaussian Mechanism via Bounded Support</strong><br><button class=copy-to-clipboard title="Privacy Amplification for the Gaussian Mechanism via Bounded Support" index=195>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-195 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-LG, cs.CR<br>Keyword Score: 10<br>Keywords: Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05598v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05598v1.pdf filename=2403.05598v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Data-dependent privacy accounting frameworks such as per-instance <b>differential</b> <b>privacy</b> (pDP) and Fisher information loss (FIL) confer fine-grained privacy guarantees for individuals in a fixed training dataset. These guarantees can be desirable compared to vanilla DP in real world settings as they tightly upper-bound the privacy leakage for a $\textit{specific}$ individual in an $\textit{actual}$ dataset, rather than considering worst-case datasets. While these frameworks are beginning to gain popularity, to date, there is a lack of private mechanisms that can fully leverage advantages of data-dependent accounting. To bridge this gap, we propose simple modifications of the Gaussian mechanism with bounded support, showing that they amplify privacy guarantees under data-dependent accounting. Experiments on model training with DP-SGD show that using bounded support Gaussian mechanisms can provide a reduction of the pDP bound $\epsilon$ by as much as 30% without negative effects on model utility.</p></p class="citation"></blockquote><h3 id=56--196292-group-privacy-amplification-and-unified-amplification-by-subsampling-for-rényi-differential-privacy-jan-schuchardt-et-al-2024>(5/6 | 196/292) Group Privacy Amplification and Unified Amplification by Subsampling for Rényi Differential Privacy (Jan Schuchardt et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jan Schuchardt, Mihail Stoian, Arthur Kosmala, Stephan Günnemann. (2024)<br><strong>Group Privacy Amplification and Unified Amplification by Subsampling for Rényi Differential Privacy</strong><br><button class=copy-to-clipboard title="Group Privacy Amplification and Unified Amplification by Subsampling for Rényi Differential Privacy" index=196>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-196 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-LG, cs.CR, stat-ML<br>Keyword Score: 10<br>Keywords: Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04867v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04867v1.pdf filename=2403.04867v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Differential</b> <b>privacy</b> (DP) has various desirable properties, such as robustness to post-processing, group privacy, and amplification by subsampling, which can be derived independently of each other. Our goal is to determine whether stronger privacy guarantees can be obtained by considering multiple of these properties jointly. To this end, we focus on the combination of group privacy and amplification by subsampling. To provide guarantees that are amenable to machine learning algorithms, we conduct our analysis in the framework of R'enyi-DP, which has more favorable composition properties than $(\epsilon,\delta)$-DP. As part of this analysis, we develop a unified framework for deriving amplification by subsampling guarantees for R'enyi-DP, which represents the first such framework for a privacy accounting method and is of independent interest. We find that it not only lets us improve upon and generalize existing amplification results for R'enyi-DP, but also derive provably tight group privacy amplification guarantees stronger than existing principles. These results establish the joint study of different DP properties as a promising research direction.</p></p class="citation"></blockquote><h3 id=66--197292-privacy-in-cloud-computing-through-immersion-based-coding-haleh-hayati-et-al-2024>(6/6 | 197/292) Privacy in Cloud Computing through Immersion-based Coding (Haleh Hayati et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haleh Hayati, Nathan van de Wouw, Carlos Murguia. (2024)<br><strong>Privacy in Cloud Computing through Immersion-based Coding</strong><br><button class=copy-to-clipboard title="Privacy in Cloud Computing through Immersion-based Coding" index=197>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-197 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 10<br>Keywords: Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04485v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04485v1.pdf filename=2403.04485v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Cloud computing enables users to process and store data remotely on high-performance computers and servers by sharing data over the Internet. However, transferring data to clouds causes unavoidable privacy concerns. Here, we present a synthesis framework to design coding mechanisms that allow sharing and processing data in a privacy-preserving manner without sacrificing data utility and algorithmic performance. We consider the setup where the user aims to run an algorithm in the cloud using private data. The cloud then returns some data utility back to the user (utility refers to the service that the algorithm provides, e.g., classification, prediction, AI models, etc.). To avoid privacy concerns, the proposed scheme provides tools to co-design: 1) coding mechanisms to distort the original data and guarantee a prescribed <b>differential</b> <b>privacy</b> level; 2) an equivalent-but-different algorithm (referred here to as the target algorithm) that runs on distorted data and produces distorted utility; and 3) a decoding function that extracts the true utility from the distorted one with a negligible error. Then, instead of sharing the original data and algorithm with the cloud, only the distorted data and target algorithm are disclosed, thereby avoiding privacy concerns. The proposed scheme is built on the synergy of <b>differential</b> <b>privacy</b> and system immersion tools from control theory. The key underlying idea is to design a higher-dimensional target algorithm that embeds all trajectories of the original algorithm and works on randomly encoded data to produce randomly encoded utility. We show that the proposed scheme can be designed to offer any level of <b>differential</b> <b>privacy</b> without degrading the algorithm&rsquo;s utility. We present two use cases to illustrate the performance of the developed tools: privacy in optimization/learning algorithms and a nonlinear networked control system.</p></p class="citation"></blockquote><h2 id=cshc-3>cs.HC (3)</h2><h3 id=13--198292-knowledgevis-interpreting-language-models-by-comparing-fill-in-the-blank-prompts-adam-coscia-et-al-2024>(1/3 | 198/292) KnowledgeVIS: Interpreting Language Models by Comparing Fill-in-the-Blank Prompts (Adam Coscia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Adam Coscia, Alex Endert. (2024)<br><strong>KnowledgeVIS: Interpreting Language Models by Comparing Fill-in-the-Blank Prompts</strong><br><button class=copy-to-clipboard title="KnowledgeVIS: Interpreting Language Models by Comparing Fill-in-the-Blank Prompts" index=198>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-198 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-AI, cs-CY, cs-HC, cs-LG, cs.HC<br>Keyword Score: 43<br>Keywords: Clustering, human-in-the-loop, Large Language Model, Prompt, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04758v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04758v1.pdf filename=2403.04758v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent growth in the popularity of <b>large</b> <b>language</b> <b>models</b> has led to their increased usage for summarizing, predicting, and generating text, making it vital to help researchers and engineers understand how and why they work. We present KnowledgeVis, a <b>human-in-the-loop</b> visual analytics system for interpreting language models using fill-in-the-blank sentences as <b>prompts.</b> By comparing predictions between sentences, KnowledgeVis reveals learned associations that intuitively connect what language models learn during training to natural language tasks downstream, helping users create and test multiple <b>prompt</b> variations, analyze predicted words using a novel semantic <b>clustering</b> technique, and discover insights using interactive visualizations. Collectively, these visualizations help users identify the likelihood and uniqueness of individual predictions, compare sets of predictions between <b>prompts,</b> and <b>summarize</b> patterns and relationships between predictions across all <b>prompts.</b> We demonstrate the capabilities of KnowledgeVis with feedback from six NLP experts as well as three different use cases: (1) probing biomedical knowledge in two domain-adapted models; and (2) evaluating harmful identity stereotypes and (3) discovering facts and relationships between three general-purpose models.</p></p class="citation"></blockquote><h3 id=23--199292-iscore-visual-analytics-for-interpreting-how-language-models-automatically-score-summaries-adam-coscia-et-al-2024>(2/3 | 199/292) iScore: Visual Analytics for Interpreting How Language Models Automatically Score Summaries (Adam Coscia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Adam Coscia, Langdon Holmes, Wesley Morris, Joon Suh Choi, Scott Crossley, Alex Endert. (2024)<br><strong>iScore: Visual Analytics for Interpreting How Language Models Automatically Score Summaries</strong><br><button class=copy-to-clipboard title="iScore: Visual Analytics for Interpreting How Language Models Automatically Score Summaries" index=199>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-199 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-AI, cs-CY, cs-HC, cs-LG, cs.HC<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04760v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04760v1.pdf filename=2403.04760v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The recent explosion in popularity of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> has inspired learning engineers to incorporate them into adaptive educational tools that automatically score summary writing. Understanding and evaluating <b>LLMs</b> is vital before deploying them in critical learning environments, yet their unprecedented size and expanding number of parameters inhibits transparency and impedes trust when they underperform. Through a collaborative user-centered design process with several learning engineers building and deploying summary scoring <b>LLMs,</b> we characterized fundamental design challenges and goals around interpreting their models, including aggregating <b>large</b> <b>text</b> <b>inputs,</b> tracking score provenance, and scaling <b>LLM</b> interpretability methods. To address their concerns, we developed iScore, an interactive visual analytics tool for learning engineers to upload, score, and compare multiple summaries simultaneously. Tightly integrated views allow users to iteratively revise the language in summaries, track changes in the resulting <b>LLM</b> scores, and visualize model weights at multiple levels of abstraction. To validate our approach, we deployed iScore with three learning engineers over the course of a month. We present a case study where interacting with iScore led a learning engineer to improve their <b>LLM&rsquo;s</b> score accuracy by three percentage points. Finally, we conducted qualitative interviews with the learning engineers that revealed how iScore enabled them to understand, evaluate, and build trust in their <b>LLMs</b> during deployment.</p></p class="citation"></blockquote><h3 id=33--200292-deepsee-multidimensional-visualizations-of-seabed-ecosystems-adam-coscia-et-al-2024>(3/3 | 200/292) DeepSee: Multidimensional Visualizations of Seabed Ecosystems (Adam Coscia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Adam Coscia, Haley M. Sapers, Noah Deutsch, Malika Khurana, John S. Magyar, Sergio A. Parra, Daniel R. Utter, Rebecca L. Wipfler, David W. Caress, Eric J. Martin, Jennifer B. Paduan, Maggie Hendrie, Santiago Lombeyda, Hillary Mushkin, Alex Endert, Scott Davidoff, Victoria J. Orphan. (2024)<br><strong>DeepSee: Multidimensional Visualizations of Seabed Ecosystems</strong><br><button class=copy-to-clipboard title="DeepSee: Multidimensional Visualizations of Seabed Ecosystems" index=200>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-200 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 13<br>Keywords: Sample Size, In-context Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04761v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04761v1.pdf filename=2403.04761v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Scientists studying deep ocean microbial ecosystems use limited numbers of sediment <b>samples</b> <b>collected</b> from the seafloor to characterize important life-sustaining biogeochemical cycles in the environment. Yet conducting fieldwork to <b>sample</b> <b>these</b> extreme remote environments is both expensive and time consuming, requiring tools that enable scientists to explore the sampling history of field sites and predict where taking new <b>samples</b> <b>is</b> likely to maximize scientific return. We conducted a collaborative, user-centered design study with a team of scientific researchers to develop DeepSee, an interactive data workspace that visualizes 2D and 3D interpolations of biogeochemical and microbial processes in context together with sediment sampling history overlaid on 2D seafloor maps. Based on a field deployment and qualitative interviews, we found that DeepSee increased the scientific return from limited <b>sample</b> <b>sizes,</b> catalyzed new research workflows, reduced long-term costs of sharing data, and supported teamwork and communication between team members with diverse research goals.</p></p class="citation"></blockquote><h2 id=csro-18>cs.RO (18)</h2><h3 id=118--201292-a-general-calibrated-regret-metric-for-detecting-and-mitigating-human-robot-interaction-failures-kensuke-nakamura-et-al-2024>(1/18 | 201/292) A General Calibrated Regret Metric for Detecting and Mitigating Human-Robot Interaction Failures (Kensuke Nakamura et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kensuke Nakamura, Ran Tian, Andrea Bajcsy. (2024)<br><strong>A General Calibrated Regret Metric for Detecting and Mitigating Human-Robot Interaction Failures</strong><br><button class=copy-to-clipboard title="A General Calibrated Regret Metric for Detecting and Mitigating Human-Robot Interaction Failures" index=201>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-201 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 40<br>Keywords: Fine-tuning, Out-of-distribution, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04745v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04745v1.pdf filename=2403.04745v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Robot decision-making increasingly relies on expressive data-driven human prediction models when operating around people. While these models are known to suffer from prediction errors in <b>out-of-distribution</b> interactions, not all prediction errors equally impact downstream robot performance. We identify that the mathematical notion of regret precisely characterizes the degree to which incorrect predictions of future interaction outcomes degraded closed-loop robot performance. However, canonical regret measures are poorly calibrated across diverse deployment interactions. We extend the canonical notion of regret by deriving a calibrated regret metric that generalizes from absolute reward space to probability space. With this transformation, our metric removes the need for explicit reward functions to calculate the robot&rsquo;s regret, enables fairer comparison of interaction anomalies across disparate deployment contexts, and facilitates targetted dataset construction of &ldquo;system-level&rdquo; prediction failures. We experimentally quantify the value of this high-regret interaction data for aiding the robot in improving its downstream decision-making. In a suite of closed-loop autonomous driving <b>simulations,</b> we find that <b>fine-tuning</b> ego-conditioned behavior predictors exclusively on high-regret human-robot interaction data can improve the robot&rsquo;s overall re-deployment performance with significantly (77%) less data.</p></p class="citation"></blockquote><h3 id=218--202292-learning-human-to-humanoid-real-time-whole-body-teleoperation-tairan-he-et-al-2024>(2/18 | 202/292) Learning Human-to-Humanoid Real-Time Whole-Body Teleoperation (Tairan He et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tairan He, Zhengyi Luo, Wenli Xiao, Chong Zhang, Kris Kitani, Changliu Liu, Guanya Shi. (2024)<br><strong>Learning Human-to-Humanoid Real-Time Whole-Body Teleoperation</strong><br><button class=copy-to-clipboard title="Learning Human-to-Humanoid Real-Time Whole-Body Teleoperation" index=202>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-202 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-LG, cs-RO, cs-SY, cs.RO, eess-SY<br>Keyword Score: 40<br>Keywords: Reinforcement Learning, Simulation, Simulator, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04436v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04436v1.pdf filename=2403.04436v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present Human to Humanoid (H2O), a <b>reinforcement</b> <b>learning</b> (RL) based framework that enables real-time whole-body teleoperation of a full-sized humanoid robot with only an RGB camera. To create a large-scale retargeted motion dataset of human movements for humanoid robots, we propose a scalable &ldquo;sim-to-data&rdquo; process to filter and pick feasible motions using a privileged motion imitator. Afterwards, we train a robust real-time humanoid motion imitator in <b>simulation</b> using these refined motions and transfer it to the real humanoid robot in a <b>zero-shot</b> manner. We successfully achieve teleoperation of dynamic whole-body motions in real-world scenarios, including walking, back jumping, kicking, turning, waving, pushing, boxing, etc. To the best of our knowledge, this is the first demonstration to achieve learning-based real-time whole-body humanoid teleoperation.</p></p class="citation"></blockquote><h3 id=318--203292-dnact-diffusion-guided-multi-task-3d-policy-learning-ge-yan-et-al-2024>(3/18 | 203/292) DNAct: Diffusion Guided Multi-Task 3D Policy Learning (Ge Yan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ge Yan, Yueh-Hua Wu, Xiaolong Wang. (2024)<br><strong>DNAct: Diffusion Guided Multi-Task 3D Policy Learning</strong><br><button class=copy-to-clipboard title="DNAct: Diffusion Guided Multi-Task 3D Policy Learning" index=203>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-203 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-CV, cs-RO, cs.RO<br>Keyword Score: 35<br>Keywords: Foundation Model, Geometry, Knowledge Distillation, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04115v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04115v2.pdf filename=2403.04115v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents DNAct, a language-conditioned multi-task policy framework that integrates neural rendering pre-training and diffusion training to enforce multi-modality learning in action sequence spaces. To learn a generalizable multi-task policy with few demonstrations, the pre-training phase of DNAct leverages neural rendering to <b>distill</b> 2D semantic features from <b>foundation</b> <b>models</b> such as Stable Diffusion to a 3D space, which provides a comprehensive semantic understanding regarding the scene. Consequently, it allows various applications to challenging robotic tasks requiring rich 3D semantics and accurate <b>geometry.</b> Furthermore, we introduce a novel approach utilizing diffusion training to learn a vision and language feature that encapsulates the inherent multi-modality in the multi-task demonstrations. By reconstructing the action sequences from different tasks via the diffusion process, the model is capable of distinguishing different modalities and thus improving the robustness and the generalizability of the learned representation. DNAct significantly surpasses SOTA NeRF-based multi-task manipulation approaches with over 30% improvement in success rate. Project website: dnact.github.io.</p></p class="citation"></blockquote><h3 id=418--204292-learning-agility-adaptation-for-flight-in-clutter-guangyu-zhao-et-al-2024>(4/18 | 204/292) Learning Agility Adaptation for Flight in Clutter (Guangyu Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guangyu Zhao, Tianyue Wu, Yeke Chen, Fei Gao. (2024)<br><strong>Learning Agility Adaptation for Flight in Clutter</strong><br><button class=copy-to-clipboard title="Learning Agility Adaptation for Flight in Clutter" index=204>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-204 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-LG, cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04586v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04586v1.pdf filename=2403.04586v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Animals learn to adapt agility of their movements to their capabilities and the environment they operate in. Mobile robots should also demonstrate this ability to combine agility and safety. The aim of this work is to endow flight vehicles with the ability of agility adaptation in prior unknown and partially observable cluttered environments. We propose a hierarchical learning and planning framework where we utilize both trial and error to comprehensively learn an agility policy with the vehicle&rsquo;s observation as the input, and well-established methods of model-based trajectory generation. Technically, we use online model-free <b>reinforcement</b> <b>learning</b> and a pre-training-fine-tuning reward scheme to obtain the deployable policy. The statistical results in <b>simulation</b> demonstrate the advantages of our method over the constant agility baselines and an alternative method in terms of flight efficiency and safety. In particular, the policy leads to intelligent behaviors, such as perception awareness, which distinguish it from other approaches. By deploying the policy to hardware, we verify that these advantages can be brought to the real world.</p></p class="citation"></blockquote><h3 id=518--205292-incremental-bayesian-learning-for-fail-operational-control-in-autonomous-driving-lei-zheng-et-al-2024>(5/18 | 205/292) Incremental Bayesian Learning for Fail-Operational Control in Autonomous Driving (Lei Zheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lei Zheng, Rui Yang, Zengqi Peng, Wei Yan, Michael Yu Wang, Jun Ma. (2024)<br><strong>Incremental Bayesian Learning for Fail-Operational Control in Autonomous Driving</strong><br><button class=copy-to-clipboard title="Incremental Bayesian Learning for Fail-Operational Control in Autonomous Driving" index=205>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-205 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Simulation, Simulator, Stemming<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04143v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04143v1.pdf filename=2403.04143v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Abrupt maneuvers by surrounding vehicles (SVs) can typically lead to safety concerns and affect the task efficiency of the ego vehicle (EV), especially with model uncertainties <b>stemming</b> from environmental disturbances. This paper presents a real-time fail-operational controller that ensures the asymptotic convergence of an uncertain EV to a safe state, while preserving task efficiency in dynamic environments. An incremental Bayesian learning approach is developed to facilitate online learning and inference of changing environmental disturbances. Leveraging disturbance quantification and constraint transformation, we develop a stochastic fail-operational barrier based on the control barrier function (CBF). With this development, the uncertain EV is able to converge asymptotically from an unsafe state to a defined safe state with probabilistic stability. Subsequently, the stochastic fail-operational barrier is integrated into an efficient fail-operational controller based on quadratic programming (QP). This controller is tailored for the EV operating under control constraints in the presence of environmental disturbances, with both safety and efficiency objectives taken into consideration. We validate the proposed framework in connected cruise control (CCC) tasks, where SVs perform aggressive driving maneuvers. The <b>simulation</b> results demonstrate that our method empowers the EV to swiftly return to a safe state while upholding task efficiency in real time, even under time-varying environmental disturbances.</p></p class="citation"></blockquote><h3 id=618--206292-real-time-planning-under-uncertainty-for-auvs-using-virtual-maps-ivana-collado-gonzalez-et-al-2024>(6/18 | 206/292) Real-Time Planning Under Uncertainty for AUVs Using Virtual Maps (Ivana Collado-Gonzalez et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ivana Collado-Gonzalez, John McConnell, Jinkun Wang, Paul Szenher, Brendan Englot. (2024)<br><strong>Real-Time Planning Under Uncertainty for AUVs Using Virtual Maps</strong><br><button class=copy-to-clipboard title="Real-Time Planning Under Uncertainty for AUVs Using Virtual Maps" index=206>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-206 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 23<br>Keywords: Graph, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04936v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04936v1.pdf filename=2403.04936v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Reliable localization is an essential capability for marine robots navigating in GPS-denied environments. SLAM, commonly used to mitigate dead reckoning errors, still fails in feature-sparse environments or with limited-range sensors. Pose estimation can be improved by incorporating the uncertainty prediction of future poses into the planning process and choosing actions that reduce uncertainty. However, performing belief propagation is computationally costly, especially when operating in large-scale environments. This work proposes a computationally efficient planning under uncertainty frame-work suitable for large-scale, feature-sparse environments. Our strategy leverages SLAM <b>graph</b> and occupancy map data obtained from a prior exploration phase to create a virtual map, describing the uncertainty of each map cell using a multivariate Gaussian. The virtual map is then used as a cost map in the planning phase, and performing belief propagation at each step is avoided. A receding horizon planning strategy is implemented, managing a goal-reaching and uncertainty-reduction tradeoff. <b>Simulation</b> experiments in a realistic underwater environment validate this approach. Experimental comparisons against a full belief propagation approach and a standard shortest-distance approach are conducted.</p></p class="citation"></blockquote><h3 id=718--207292-scalable-simulation-guided-compliant-tactile-finger-design-yuxiang-ma-et-al-2024>(7/18 | 207/292) Scalable, Simulation-Guided Compliant Tactile Finger Design (Yuxiang Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuxiang Ma, Arpit Agarwal, Sandra Q. Liu, Wenzhen Yuan, Edward H. Adelson. (2024)<br><strong>Scalable, Simulation-Guided Compliant Tactile Finger Design</strong><br><button class=copy-to-clipboard title="Scalable, Simulation-Guided Compliant Tactile Finger Design" index=207>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-207 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04638v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04638v1.pdf filename=2403.04638v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Compliant grippers enable robots to work with humans in unstructured environments. In general, these grippers can improve with tactile sensing to estimate the state of objects around them to precisely manipulate objects. However, co-designing compliant structures with high-resolution tactile sensing is a challenging task. We propose a <b>simulation</b> framework for the end-to-end forward design of GelSight Fin Ray sensors. Our <b>simulation</b> framework consists of mechanical <b>simulation</b> using the finite element method (FEM) and optical <b>simulation</b> including physically based rendering (PBR). To simulate the fluorescent paint used in these GelSight Fin Rays, we propose an efficient method that can be directly integrated in PBR. Using the <b>simulation</b> framework, we investigate design choices available in the compliant grippers, namely gel pad shapes, illumination conditions, Fin Ray gripper sizes, and Fin Ray stiffness. This infrastructure enables faster design and prototype time frames of new Fin Ray sensors that have various sensing areas, ranging from 48 mm $\times$ \18 mm to 70 mm $\times$ 35 mm. Given the parameters we choose, we can thus optimize different Fin Ray designs and show their utility in grasping day-to-day objects.</p></p class="citation"></blockquote><h3 id=818--208292-a-magnetic-millirobot-walks-on-slippery-biological-surfaces-for-targeted-cargo-delivery-moonkwang-jeong-et-al-2024>(8/18 | 208/292) A Magnetic Millirobot Walks on Slippery Biological Surfaces for Targeted Cargo Delivery (Moonkwang Jeong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Moonkwang Jeong, Xiangzhou Tan, Felix Fischer, Tian Qiu. (2024)<br><strong>A Magnetic Millirobot Walks on Slippery Biological Surfaces for Targeted Cargo Delivery</strong><br><button class=copy-to-clipboard title="A Magnetic Millirobot Walks on Slippery Biological Surfaces for Targeted Cargo Delivery" index=208>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-208 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: J-3, cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04467v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04467v1.pdf filename=2403.04467v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Small-scale robots hold great potential for targeted cargo delivery in minimally-inv asive medicine. However, current robots often face challenges to locomote efficiently on slip pery biological tissue surfaces, especially when loaded with heavy cargos. Here, we report a magnetic millirobot that can walk on rough and slippery biological tissues by anchoring itself on the soft tissue surface alternatingly with two feet and reciprocally rotating the body to mov e forward. We experimentally studied the locomotion, validated it with numerical <b>simulations</b> and optimized the actuation parameters to fit various terrains and loading conditions. Further more, we developed a permanent magnet set-up to enable wireless actuation within a huma n-scale volume which allows precise control of the millirobot to follow complex trajectories, cl imb vertical walls, and carry cargo up to four times of its own weight. Upon reaching the targ et location, it performs a deployment sequence to release the liquid drug into tissues. The ro bust gait of our millirobot on rough biological terrains, combined with its heavy load capacity, make it a versatile and effective miniaturized vehicle for targeted cargo delivery.</p></p class="citation"></blockquote><h3 id=918--209292-symmetry-considerations-for-learning-task-symmetric-robot-policies-mayank-mittal-et-al-2024>(9/18 | 209/292) Symmetry Considerations for Learning Task Symmetric Robot Policies (Mayank Mittal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mayank Mittal, Nikita Rudin, Victor Klemm, Arthur Allshire, Marco Hutter. (2024)<br><strong>Symmetry Considerations for Learning Task Symmetric Robot Policies</strong><br><button class=copy-to-clipboard title="Symmetry Considerations for Learning Task Symmetric Robot Policies" index=209>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-209 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Data Augmentation, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04359v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04359v1.pdf filename=2403.04359v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Symmetry is a fundamental aspect of many real-world robotic tasks. However, current deep <b>reinforcement</b> <b>learning</b> (DRL) approaches can seldom harness and exploit symmetry effectively. Often, the learned behaviors fail to achieve the desired transformation invariances and suffer from motion artifacts. For instance, a quadruped may exhibit different gaits when commanded to move forward or backward, even though it is symmetrical about its torso. This issue becomes further pronounced in high-dimensional or complex environments, where DRL methods are prone to local optima and fail to explore regions of the state space equally. Past methods on encouraging symmetry for robotic tasks have studied this topic mainly in a single-task setting, where symmetry usually refers to symmetry in the motion, such as the gait patterns. In this paper, we revisit this topic for goal-conditioned tasks in robotics, where symmetry lies mainly in task execution and not necessarily in the learned motions themselves. In particular, we investigate two approaches to incorporate symmetry invariance into DRL &ndash; <b>data</b> <b>augmentation</b> and mirror loss function. We provide a theoretical foundation for using augmented samples in an on-policy setting. Based on this, we show that the corresponding approach achieves faster convergence and improves the learned behaviors in various challenging robotic tasks, from climbing boxes with a quadruped to dexterous manipulation.</p></p class="citation"></blockquote><h3 id=1018--210292-litsim-conflict-aware-policy-for-long-term-interactive-traffic-simulation-haojie-xin-et-al-2024>(10/18 | 210/292) LitSim: Conflict-aware Policy for Long-term Interactive Traffic Simulation (Haojie Xin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haojie Xin, Xiaodong Zhang, Renzhi Tang, Songyang Yan, Qianrui Zhao, Chunze Yang, Zijiang Yang. (2024)<br><strong>LitSim: Conflict-aware Policy for Long-term Interactive Traffic Simulation</strong><br><button class=copy-to-clipboard title="LitSim: Conflict-aware Policy for Long-term Interactive Traffic Simulation" index=210>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-210 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04299v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04299v1.pdf filename=2403.04299v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Simulation</b> is pivotal in evaluating the performance of autonomous driving systems due to the advantages in efficiency and cost compared to on-road testing. Realistic multi-agent behavior~(e.g., interactive and long-term) is needed to narrow the gap between the <b>simulation</b> and the reality. The existing work has the following shortcomings in achieving this goal:~(1) log replay offers realistic scenarios but leads to unrealistic collisions due to lacking dynamic interactions, and~(2) model-based and learning-based solutions encourage interactions but often deviate from real-world data in long horizons. In this work, we propose LitSim, a long-term interactive <b>simulation</b> approach that maximizes realism while avoiding unrealistic collisions. Specifically, we replay the log for most scenarios and intervene only when LitSim predicts unrealistic conflicts. We then encourage interactions among the agents and resolve the conflicts, thereby reducing the likelihood of unrealistic collisions. We train and validate our model on the real-world dataset NGSIM, and the experimental results demonstrate that LitSim outperforms the current popular approaches in realism and reactivity.</p></p class="citation"></blockquote><h3 id=1118--211292-globally-stable-neural-imitation-policies-amin-abyaneh-et-al-2024>(11/18 | 211/292) Globally Stable Neural Imitation Policies (Amin Abyaneh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amin Abyaneh, Mariana Sosa Guzmán, Hsiu-Chin Lin. (2024)<br><strong>Globally Stable Neural Imitation Policies</strong><br><button class=copy-to-clipboard title="Globally Stable Neural Imitation Policies" index=211>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-211 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-LG, cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04118v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04118v1.pdf filename=2403.04118v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Imitation learning presents an effective approach to alleviate the resource-intensive and time-consuming nature of policy learning from scratch in the solution space. Even though the resulting policy can mimic expert demonstrations reliably, it often lacks predictability in unexplored regions of the state-space, giving rise to significant safety concerns in the face of perturbations. To address these challenges, we introduce the Stable Neural Dynamical System (SNDS), an imitation learning regime which produces a policy with formal stability guarantees. We deploy a neural policy architecture that facilitates the representation of stability based on Lyapunov theorem, and jointly train the policy and its corresponding Lyapunov candidate to ensure global stability. We validate our approach by conducting extensive experiments in <b>simulation</b> and successfully deploying the trained policies on a real-world manipulator arm. The experimental results demonstrate that our method overcomes the instability, accuracy, and computational intensity problems associated with previous imitation learning methods, making our method a promising solution for stable policy learning in complex planning scenarios.</p></p class="citation"></blockquote><h3 id=1218--212292-a-mixed-integer-conic-program-for-the-moving-target-traveling-salesman-problem-based-on-a-graph-of-convex-sets-allen-george-philip-et-al-2024>(12/18 | 212/292) A Mixed-Integer Conic Program for the Moving-Target Traveling Salesman Problem based on a Graph of Convex Sets (Allen George Philip et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Allen George Philip, Zhongqiang Ren, Sivakumar Rathinam, Howie Choset. (2024)<br><strong>A Mixed-Integer Conic Program for the Moving-Target Traveling Salesman Problem based on a Graph of Convex Sets</strong><br><button class=copy-to-clipboard title="A Mixed-Integer Conic Program for the Moving-Target Traveling Salesman Problem based on a Graph of Convex Sets" index=212>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-212 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-DS, cs-RO, cs.RO<br>Keyword Score: 13<br>Keywords: Graph, Neural Machine Translation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04917v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04917v2.pdf filename=2403.04917v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces a new formulation that finds the optimum for the Moving-Target Traveling Salesman Problem <b>(MT-TSP),</b> which seeks to find a shortest path for an agent, that starts at a depot, visits a set of moving targets exactly once within their assigned time-windows, and returns to the depot. The formulation relies on the key idea that when the targets move along lines, their trajectories become convex sets within the space-time coordinate system. The problem then reduces to finding the shortest path within a <b>graph</b> of convex sets, subject to some speed constraints. We compare our formulation with the current state-of-the-art Mixed Integer Conic Program (MICP) solver for the <b>MT-TSP.</b> The experimental results show that our formulation outperforms the MICP for instances with up to 20 targets, with up to two orders of magnitude reduction in runtime, and up to a 60% tighter optimality gap. We also show that the solution cost from the convex relaxation of our formulation provides significantly tighter lower bounds for the <b>MT-TSP</b> than the ones from the MICP.</p></p class="citation"></blockquote><h3 id=1318--213292-generalizing-cooperative-eco-driving-via-multi-residual-task-learning-vindula-jayawardana-et-al-2024>(13/18 | 213/292) Generalizing Cooperative Eco-driving via Multi-residual Task Learning (Vindula Jayawardana et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vindula Jayawardana, Sirui Li, Cathy Wu, Yashar Farid, Kentaro Oguchi. (2024)<br><strong>Generalizing Cooperative Eco-driving via Multi-residual Task Learning</strong><br><button class=copy-to-clipboard title="Generalizing Cooperative Eco-driving via Multi-residual Task Learning" index=213>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-213 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-LG, cs-MA, cs-RO, cs-SY, cs.RO, eess-SY<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04232v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04232v1.pdf filename=2403.04232v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Conventional control, such as model-based control, is commonly utilized in autonomous driving due to its efficiency and reliability. However, real-world autonomous driving contends with a multitude of diverse traffic scenarios that are challenging for these planning algorithms. Model-free Deep <b>Reinforcement</b> <b>Learning</b> (DRL) presents a promising avenue in this direction, but learning DRL control policies that generalize to multiple traffic scenarios is still a challenge. To address this, we introduce Multi-residual Task Learning (MRTL), a generic learning framework based on multi-task learning that, for a set of task scenarios, decomposes the control into nominal components that are effectively solved by conventional control methods and residual terms which are solved using learning. We employ MRTL for fleet-level emission reduction in mixed traffic using autonomous vehicles as a means of system control. By analyzing the performance of MRTL across nearly 600 signalized intersections and 1200 traffic scenarios, we demonstrate that it emerges as a promising approach to synergize the strengths of DRL and conventional methods in generalizable control.</p></p class="citation"></blockquote><h3 id=1418--214292-closing-the-visual-sim-to-real-gap-with-object-composable-nerfs-nikhil-mishra-et-al-2024>(14/18 | 214/292) Closing the Visual Sim-to-Real Gap with Object-Composable NeRFs (Nikhil Mishra et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nikhil Mishra, Maximilian Sieb, Pieter Abbeel, Xi Chen. (2024)<br><strong>Closing the Visual Sim-to-Real Gap with Object-Composable NeRFs</strong><br><button class=copy-to-clipboard title="Closing the Visual Sim-to-Real Gap with Object-Composable NeRFs" index=214>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-214 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-CV, cs-LG, cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Distribution Shift, Distribution Shift<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04114v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04114v1.pdf filename=2403.04114v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep learning methods for perception are the cornerstone of many robotic systems. Despite their potential for impressive performance, obtaining real-world training data is expensive, and can be impractically difficult for some tasks. Sim-to-real transfer with domain randomization offers a potential workaround, but often requires extensive manual tuning and results in models that are brittle to <b>distribution</b> <b>shift</b> between sim and real. In this work, we introduce Composable Object Volume NeRF (COV-NeRF), an object-composable NeRF model that is the centerpiece of a real-to-sim pipeline for synthesizing training data targeted to scenes and objects from the real world. COV-NeRF extracts objects from real images and composes them into new scenes, generating photorealistic renderings and many types of 2D and 3D supervision, including depth maps, segmentation masks, and meshes. We show that COV-NeRF matches the rendering quality of modern NeRF methods, and can be used to rapidly close the sim-to-real gap across a variety of perceptual modalities.</p></p class="citation"></blockquote><h3 id=1518--215292-ogmp-oracle-guided-multimodal-policies-for-agile-and-versatile-robot-control-lokesh-krishna-et-al-2024>(15/18 | 215/292) OGMP: Oracle Guided Multimodal Policies for Agile and Versatile Robot Control (Lokesh Krishna et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lokesh Krishna, Nikhil Sobanbabu, Quan Nguyen. (2024)<br><strong>OGMP: Oracle Guided Multimodal Policies for Agile and Versatile Robot Control</strong><br><button class=copy-to-clipboard title="OGMP: Oracle Guided Multimodal Policies for Agile and Versatile Robot Control" index=215>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-215 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 6<br>Keywords: Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04205v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04205v1.pdf filename=2403.04205v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Amidst task-specific learning-based control synthesis frameworks that achieve impressive empirical results, a unified framework that systematically constructs an optimal policy for sufficiently solving a general notion of a task is absent. Hence, we propose a theoretical framework for a task-centered control synthesis leveraging two critical ideas: 1) oracle-guided policy optimization for the non-limiting integration of sub-optimal task-based priors to guide the policy optimization and 2) task-vital multimodality to break down solving a task into executing a sequence of behavioral modes. The proposed approach results in highly agile parkour and diving on a 16-DoF dynamic bipedal robot. The obtained policy advances indefinitely on a track, performing leaps and jumps of varying lengths and heights for the parkour task. Corresponding to the dive task, the policy demonstrates front, back, and side flips from various initial heights. Finally, we introduce a novel latent mode space reachability analysis to study our policies&rsquo; versatility and generalization by computing a feasible mode set function through which we certify a set of failure-free modes for our policy to perform at any given state.</p></p class="citation"></blockquote><h3 id=1618--216292-robokube-establishing-a-new-foundation-for-the-cloud-native-evolution-in-robotics-yu-liu-et-al-2024>(16/18 | 216/292) RoboKube: Establishing a New Foundation for the Cloud Native Evolution in Robotics (Yu Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yu Liu, Aitor Hernandez Herranz, Roberto C. Sundin. (2024)<br><strong>RoboKube: Establishing a New Foundation for the Cloud Native Evolution in Robotics</strong><br><button class=copy-to-clipboard title="RoboKube: Establishing a New Foundation for the Cloud Native Evolution in Robotics" index=216>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-216 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 3<br>Keywords: Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04440v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04440v1.pdf filename=2403.04440v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Cloud native technologies have been observed to expand into the realm of Internet of Things (IoT) and Cyber-physical Systems, of which an important application domain is robotics. In this paper, we review the cloudification practice in the robotics domain from both literature and industrial perspectives. We propose RoboKube, an adaptive framework that is based on the Kubernetes (K8s) ecosystem to set up a common platform across the device-cloud continuum for the deployment of cloudified Robotic Operating System (ROS) powered applications, to facilitate the cloud native evolution in robotics. We examine the process of modernizing ROS applications using cloud-native technologies, focusing on both the platform and application perspectives. In addition, we address the challenges of networking setups for heterogeneous environments. This paper intends to serves as a guide for developers and researchers, offering insights into containerization strategies, ROS node distribution and <b>clustering,</b> and deployment options. To demonstrate the feasibility of our approach, we present a case study involving the cloudification of a teleoperation testbed.</p></p class="citation"></blockquote><h3 id=1718--217292-social-robots-for-sleep-health-a-scoping-review-victor-nikhil-antony-et-al-2024>(17/18 | 217/292) Social Robots for Sleep Health: A Scoping Review (Victor Nikhil Antony et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Victor Nikhil Antony, Mengchi Li, Shu-Han Lin, Junxin Li, Chein-Ming Huang. (2024)<br><strong>Social Robots for Sleep Health: A Scoping Review</strong><br><button class=copy-to-clipboard title="Social Robots for Sleep Health: A Scoping Review" index=217>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-217 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-HC, cs-RO, cs.RO<br>Keyword Score: 3<br>Keywords: Sample Size<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04169v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04169v1.pdf filename=2403.04169v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Poor sleep health is an increasingly concerning public healthcare crisis, especially when coupled with a dwindling number of health professionals qualified to combat it. However, there is a growing body of scientific literature on the use of digital technologies in supporting and sustaining individuals&rsquo; healthy sleep habits. Social robots are a relatively recent technology that has been used to facilitate health care interventions and may have potential in improving sleep health outcomes, as well. Social robots&rsquo; unique characteristics &ndash; such as anthropomorphic physical embodiment or effective communication methods &ndash; help to engage users and motivate them to comply with specific interventions, thus improving the interventions&rsquo; outcomes. This scoping review aims to evaluate current scientific evidence for employing social robots in sleep health interventions, identify critical research gaps, and suggest future directions for developing and using social robots to improve people&rsquo;s sleep health. Our analysis of the reviewed studies found them limited due to a singular focus on the older adult population, use of small <b>sample</b> <b>sizes,</b> limited intervention durations, and other compounding factors. Nevertheless, the reviewed studies reported several positive outcomes, highlighting the potential social robots hold in this field. Although our review found limited clinical evidence for the efficacy of social robots as purveyors of sleep health interventions, it did elucidate the potential for a successful future in this domain if current limitations are addressed and more research is conducted.</p></p class="citation"></blockquote><h3 id=1818--218292-an-adaptable-safe-and-portable-robot-assisted-feeding-system-ethan-kroll-gordon-et-al-2024>(18/18 | 218/292) An Adaptable, Safe, and Portable Robot-Assisted Feeding System (Ethan Kroll Gordon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ethan Kroll Gordon, Rajat Kumar Jenamani, Amal Nanavati, Ziang Liu, Haya Bolotski, Raida Karim, Daniel Stabile, Atharva Kashyap, Bernie Hao Zhu, Xilai Dai, Tyler Schrenk, Jonathan Ko, Taylor Kessler Faulkner, Tapomayukh Bhattacharjee, Siddhartha Srinivasa. (2024)<br><strong>An Adaptable, Safe, and Portable Robot-Assisted Feeding System</strong><br><button class=copy-to-clipboard title="An Adaptable, Safe, and Portable Robot-Assisted Feeding System" index=218>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-218 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 3<br>Keywords: Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04134v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04134v1.pdf filename=2403.04134v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We demonstrate a robot-assisted feeding system that enables people with mobility impairments to feed themselves. Our system design embodies Safety, Portability, and User Control, with comprehensive full-stack safety checks, the ability to be mounted on and powered by any powered wheelchair, and a custom web-app allowing care-recipients to leverage their own assistive devices for robot control. For bite acquisition, we leverage <b>multi-modal</b> online learning to tractably adapt to unseen food types. For bite transfer, we leverage real-time mouth perception and interaction-aware control. Co-designed with community researchers, our system has been validated through multiple end-user studies.</p></p class="citation"></blockquote><h2 id=mathna-8>math.NA (8)</h2><h3 id=18--219292-accelerating-multigrid-solver-with-generative-super-resolution-francisco-holguin-et-al-2024>(1/8 | 219/292) Accelerating multigrid solver with generative super-resolution (Francisco Holguin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Francisco Holguin, GS Sidharth, Gavin Portwood. (2024)<br><strong>Accelerating multigrid solver with generative super-resolution</strong><br><button class=copy-to-clipboard title="Accelerating multigrid solver with generative super-resolution" index=219>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-219 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA<br>Keyword Score: 40<br>Keywords: Generative Adversarial Network, Generative Adversarial Network, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07936v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07936v1.pdf filename=2403.07936v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The geometric multigrid algorithm is an efficient numerical method for solving a variety of elliptic partial differential equations (PDEs). The method damps errors at progressively finer grid scales, resulting in faster convergence compared to iterative methods such as Gauss-Seidel. The prolongation or coarse-to-fine interpolation operator within the multigrid algorithm, lends itself to a data-driven treatment with deep learning super-resolution, commonly used to increase the resolution of images. We (i) propose the integration of a super-resolution <b>generative</b> <b>adversarial</b> <b>network</b> <b>(GAN)</b> model with the multigrid algorithm as the prolongation operator and (ii) show that the <b>GAN-interpolation</b> can improve the convergence properties of multigrid in comparison to cubic spline interpolation on a class of multiscale PDEs typically solved in fluid mechanics and engineering <b>simulations.</b> We also highlight the importance of characterizing hybrid (machine learning/traditional) algorithm parameters.</p></p class="citation"></blockquote><h3 id=28--220292-a-structure-preserving-semi-implicit-imex-finite-volume-scheme-for-ideal-magnetohydrodynamics-at-all-mach-and-alfvén-numbers-walter-boscheri-et-al-2024>(2/8 | 220/292) A structure-preserving semi-implicit IMEX finite volume scheme for ideal magnetohydrodynamics at all Mach and Alfvén numbers (Walter Boscheri et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Walter Boscheri, Andrea Thomann. (2024)<br><strong>A structure-preserving semi-implicit IMEX finite volume scheme for ideal magnetohydrodynamics at all Mach and Alfvén numbers</strong><br><button class=copy-to-clipboard title="A structure-preserving semi-implicit IMEX finite volume scheme for ideal magnetohydrodynamics at all Mach and Alfvén numbers" index=220>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-220 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA<br>Keyword Score: 23<br>Keywords: Benchmarking, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04517v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04517v1.pdf filename=2403.04517v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a divergence-free semi-implicit finite volume scheme for the <b>simulation</b> of the ideal magnetohydrodynamics (MHD) equations which is stable for large time steps controlled by the local transport speed at all Mach and Alfv'en numbers. An operator splitting technique allows to treat the convective terms explicitly while the hydrodynamic pressure and the magnetic field contributions are integrated implicitly, yielding two decoupled linear implicit systems. The linearity of the implicit part is achieved by means of a semi-implicit time linearization. This structure is favorable as second-order accuracy in time can be achieved relying on the class of semi-implicit IMplicit-EXplicit Runge-Kutta (IMEX-RK) methods. In space, implicit cell-centered finite difference operators are designed to discretely preserve the divergence-free property of the magnetic field on three-dimensional Cartesian meshes. The new scheme is also particularly well suited for low Mach number flows and for the incompressible limit of the MHD equations, since no explicit numerical dissipation is added to the implicit contribution and the time step is scale independent. Likewise, highly magnetized flows can benefit from the implicit treatment of the magnetic fluxes, hence improving the computational efficiency of the novel method. The convective terms undergo a shock-capturing second order finite volume discretization to guarantee the effectiveness of the proposed method even for high Mach number flows. The new scheme is <b>benchmarked</b> against a series of test cases for the ideal MHD equations addressing different acoustic and Alfv'en Mach number regimes where the performance and the stability of the new scheme is assessed.</p></p class="citation"></blockquote><h3 id=38--221292-a-mechanism-informed-reinforcement-learning-framework-for-shape-optimization-of-airfoils-jingfeng-wang-et-al-2024>(3/8 | 221/292) A mechanism-informed reinforcement learning framework for shape optimization of airfoils (Jingfeng Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jingfeng Wang, Guanghui Hu. (2024)<br><strong>A mechanism-informed reinforcement learning framework for shape optimization of airfoils</strong><br><button class=copy-to-clipboard title="A mechanism-informed reinforcement learning framework for shape optimization of airfoils" index=221>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-221 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-CE, cs-LG, cs-NA, math-NA, math.NA<br>Keyword Score: 15<br>Keywords: Geometry, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04329v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04329v1.pdf filename=2403.04329v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this study, we present the mechanism-informed <b>reinforcement</b> <b>learning</b> framework for airfoil shape optimization. By leveraging the twin delayed deep deterministic policy gradient algorithm for its notable stability, our approach addresses the complexities of optimizing shapes governed by fluid dynamics. The PDEs-based solver is adopted for its accuracy even when the configurations and geometries are extraordinarily changed during the exploration. Dual-weighted residual-based mesh refinement strategy is applied to ensure the accurate calculation of target functionals. To streamline the iterative optimization process and handle geometric deformations, our approach integrates Laplacian smoothing, adaptive refinement, and a B'ezier fitting strategy. This combination not only remits mesh tangling but also guarantees a precise manipulation of the airfoil <b>geometry.</b> Our neural network architecture leverages B'ezier curves for efficient dimensionality reduction, thereby enhancing the learning process and ensuring the geometric accuracy of the airfoil shapes. An attention mechanism is embedded within the network to calculate potential action on the state as well. Furthermore, we have introduced different reward and penalty mechanisms tailored to the specific challenges of airfoil optimization. This algorithm is designed to support the optimization task, facilitating a more targeted and effective approach for airfoil shape optimization.</p></p class="citation"></blockquote><h3 id=48--222292-scalable-approximation-and-solvers-for-ionic-electrodiffusion-in-cellular-geometries-pietro-benedusi-et-al-2024>(4/8 | 222/292) Scalable approximation and solvers for ionic electrodiffusion in cellular geometries (Pietro Benedusi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pietro Benedusi, Ada J. Ellingsrud, Halvor Herlyng, Marie E. Rognes. (2024)<br><strong>Scalable approximation and solvers for ionic electrodiffusion in cellular geometries</strong><br><button class=copy-to-clipboard title="Scalable approximation and solvers for ionic electrodiffusion in cellular geometries" index=222>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-222 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: 65F10, 65N30, 65N55, 68U20, 68W10, 92-08, 92C20, cs-CE, cs-NA, math-NA, math.NA<br>Keyword Score: 6<br>Keywords: Benchmarking, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04491v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04491v1.pdf filename=2403.04491v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The activity and dynamics of excitable cells are fundamentally regulated and moderated by extracellular and intracellular ion concentrations and their electric potentials. The increasing availability of dense reconstructions of excitable tissue at extreme geometric detail pose a new and clear scientific computing challenge for computational modelling of ion dynamics and transport. In this paper, we design, develop and evaluate a scalable numerical algorithm for solving the time-dependent and nonlinear KNP-EMI equations describing ionic electrodiffusion for excitable cells with an explicit geometric representation of intracellular and extracellular compartments and interior interfaces. We also introduce and specify a set of model scenarios of increasing complexity suitable for <b>benchmarking.</b> Our solution strategy is based on an implicit-explicit discretization and linearization in time, a mixed finite element discretization of ion concentrations and electric potentials in intracellular and extracellular domains, and an algebraic multigrid-based, inexact block-diagonal preconditioner for GMRES. Numerical experiments with up to $10^8$ unknowns per time step and up to 256 cores demonstrate that this solution strategy is robust and scalable with respect to the problem size, time discretization and number of cores.</p></p class="citation"></blockquote><h3 id=58--223292-towards-robust-data-driven-automated-recovery-of-symbolic-conservation-laws-from-limited-data-tracey-oellerich-et-al-2024>(5/8 | 223/292) Towards Robust Data-Driven Automated Recovery of Symbolic Conservation Laws from Limited Data (Tracey Oellerich et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tracey Oellerich, Maria Emelianenko. (2024)<br><strong>Towards Robust Data-Driven Automated Recovery of Symbolic Conservation Laws from Limited Data</strong><br><button class=copy-to-clipboard title="Towards Robust Data-Driven Automated Recovery of Symbolic Conservation Laws from Limited Data" index=223>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-223 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04889v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04889v1.pdf filename=2403.04889v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Conservation laws are an inherent feature in many systems modeling real world phenomena, in particular, those modeling biological and chemical systems. If the form of the underlying dynamical system is known, linear algebra and algebraic <b>geometry</b> methods can be used to identify the conservation laws. Our work focuses on using data-driven methods to identify the conservation law(s) in the absence of the knowledge of system dynamics. Building in part upon the ideas proposed in [arXiv:1811.00961], we develop a robust data-driven computational framework that automates the process of identifying the number and type of the conservation law(s) while keeping the amount of required data to a minimum. We demonstrate that due to relative stability of singular vectors to noise we are able to reconstruct correct conservation laws without the need for excessive parameter tuning. While we focus primarily on biological examples, the framework proposed herein is suitable for a variety of data science applications and can be coupled with other machine learning approaches.</p></p class="citation"></blockquote><h3 id=68--224292-a-finite-element-contour-integral-method-for-computing-the-resonances-of-metallic-grating-structures-with-subwavelength-holes-yingxia-xi-et-al-2024>(6/8 | 224/292) A finite element contour integral method for computing the resonances of metallic grating structures with subwavelength holes (Yingxia Xi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yingxia Xi, Junshan Lin, Jiguang Sun. (2024)<br><strong>A finite element contour integral method for computing the resonances of metallic grating structures with subwavelength holes</strong><br><button class=copy-to-clipboard title="A finite element contour integral method for computing the resonances of metallic grating structures with subwavelength holes" index=224>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-224 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: 35B34, 68U01, 68W25, cs-NA, math-NA, math.NA<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04514v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04514v1.pdf filename=2403.04514v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider the numerical computation of resonances for metallic grating structures with dispersive media and small slit holes. The underlying eigenvalue problem is nonlinear and the mathematical model is multiscale due to the existence of several length scales in problem <b>geometry</b> and material contrast. We discretize the partial differential equation model over the truncated domain using the finite element method and develop a multi-step contour integral eigensolver to compute the resonances. The eigensolver first locates eigenvalues using a spectral indicator and then computes eigenvalues by a subspace projection scheme. The proposed numerical method is robust and scalable, and does not require initial guess as the iteration methods. Numerical examples are presented to demonstrate its effectiveness.</p></p class="citation"></blockquote><h3 id=78--225292-a-lagrangian-approach-for-solving-an-axisymmetric-thermo-electromagnetic-problem-application-to-time-varying-geometry-processes-marta-benítez-et-al-2024>(7/8 | 225/292) A Lagrangian approach for solving an axisymmetric thermo-electromagnetic problem. Application to time-varying geometry processes (Marta Benítez et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marta Benítez, Alfredo Bermúdez, Pedro Fontán, Iván Martínez, Pilar Salgado. (2024)<br><strong>A Lagrangian approach for solving an axisymmetric thermo-electromagnetic problem. Application to time-varying geometry processes</strong><br><button class=copy-to-clipboard title="A Lagrangian approach for solving an axisymmetric thermo-electromagnetic problem. Application to time-varying geometry processes" index=225>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-225 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: 65N30, 65M22, 35Q74, 35Q60, cs-NA, math-NA, math.NA<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04377v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04377v1.pdf filename=2403.04377v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The aim of this work is to introduce a thermo-electromagnetic model for calculating the temperature and the power dissipated in cylindrical pieces whose <b>geometry</b> var'ies with time and undergoes large deformations; the motion will be a known data. The work will be a first step towards building a complete thermoelectromagnetic-mechanical model suitable for simulating electrically assisted forming processes, which is the main motivation of the work. The electromagnetic model will be obtained from the time-harmonic eddy current problem with an inplane current; the source will be given in terms of currents or voltages defined at sorne parts of the boundary. Finite element methods based on a Lagrangian weak formulation will be used for the numerical solution. This approach will avoid the need to compute and remesh the thermo-electromagnetic domain along the time. The numerical tools will be implemented in FEniCS and validated by using a suitable test also solved in Eulerian coordinates.</p></p class="citation"></blockquote><h3 id=88--226292-a-robust-shifted-proper-orthogonal-decomposition-proximal-methods-for-decomposing-flows-with-multiple-transports-philipp-krah-et-al-2024>(8/8 | 226/292) A robust shifted proper orthogonal decomposition: Proximal methods for decomposing flows with multiple transports (Philipp Krah et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Philipp Krah, Arthur Marmin, Beata Zorawski, Julius Reiss, Kai Schneider. (2024)<br><strong>A robust shifted proper orthogonal decomposition: Proximal methods for decomposing flows with multiple transports</strong><br><button class=copy-to-clipboard title="A robust shifted proper orthogonal decomposition: Proximal methods for decomposing flows with multiple transports" index=226>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-226 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA, physics-flu-dyn<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04313v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04313v1.pdf filename=2403.04313v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a new methodology for decomposing flows with multiple transports that further extends the shifted proper orthogonal decomposition (sPOD). The sPOD tries to approximate transport-dominated flows by a sum of co-moving data fields. The proposed methods stem from sPOD but optimize the co-moving fields directly and penalize their nuclear norm to promote low rank of the individual data in the decomposition. Furthermore, we add a robustness term to the decomposition that can deal with interpolation error and data noises. Leveraging tools from convex optimization, we derive three proximal algorithms to solve the decomposition problem. We report a numerical comparison with existing methods against synthetic data <b>benchmarks</b> and then show the separation ability of our methods on 1D and 2D incompressible and reactive flows. The resulting methodology is the basis of a new analysis paradigm that results in the same interpretability as the POD for the individual co-moving fields.</p></p class="citation"></blockquote><h2 id=quant-ph-3>quant-ph (3)</h2><h3 id=13--227292-online-maximum-likelihood-parameter-estimation-for-continuously-monitored-quantum-systems-henrik-glavind-clausen-et-al-2024>(1/3 | 227/292) Online Maximum Likelihood Parameter Estimation for Continuously-Monitored Quantum Systems (Henrik Glavind Clausen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Henrik Glavind Clausen, Pierre Rouchon, Rafal Wisniewski. (2024)<br><strong>Online Maximum Likelihood Parameter Estimation for Continuously-Monitored Quantum Systems</strong><br><button class=copy-to-clipboard title="Online Maximum Likelihood Parameter Estimation for Continuously-Monitored Quantum Systems" index=227>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-227 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-SY, eess-SY, quant-ph, quant-ph<br>Keyword Score: 40<br>Keywords: Continuous Time, Continuous Time, Discrete Time, Discrete Time, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04648v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04648v1.pdf filename=2403.04648v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we consider the problem of online (real-time, single-shot) estimation of static or slow-varying parameters along quantum trajectories in quantum dynamical systems. Based on the measurement signal of a continuously-monitored quantum system, we propose a recursive algorithm for computing the maximum likelihood estimate of unknown parameters using an approach based on stochastic gradient ascent on the log-likelihood function. We formulate the algorithm in both <b>discrete-time</b> <b>and</b> <b>continuous-time</b> <b>and</b> illustrate the performance of the algorithm through <b>simulations</b> of a simple two-level system undergoing homodyne measurement from which we are able to track multiple parameters simultaneously.</p></p class="citation"></blockquote><h3 id=23--228292-qubit-wise-architecture-search-method-for-variational-quantum-circuits-jialin-chen-et-al-2024>(2/3 | 228/292) Qubit-Wise Architecture Search Method for Variational Quantum Circuits (Jialin Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jialin Chen, Zhiqiang Cai, Ke Xu, Di Wu, Wei Cao. (2024)<br><strong>Qubit-Wise Architecture Search Method for Variational Quantum Circuits</strong><br><button class=copy-to-clipboard title="Qubit-Wise Architecture Search Method for Variational Quantum Circuits" index=228>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-228 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-LG, quant-ph, quant-ph<br>Keyword Score: 30<br>Keywords: MNIST, Reinforcement Learning, Question Answering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04268v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04268v1.pdf filename=2403.04268v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Considering the noise level limit, one crucial aspect for quantum machine learning is to design a high-performing variational quantum circuit architecture with small number of quantum gates. As the classical neural architecture search (NAS), quantum architecture search methods <b>(QAS)</b> employ methods like <b>reinforcement</b> <b>learning,</b> evolutionary algorithms and supernet optimiza-tion to improve the search efficiency. In this paper, we propose a novel qubit-wise architec-ture search (QWAS) method, which progres-sively search one-qubit configuration per stage, and combine with Monte Carlo Tree Search al-gorithm to find good quantum architectures by partitioning the search space into several good and bad subregions. The numerical experimental results indicate that our proposed method can balance the exploration and exploitation of cir-cuit performance and size in some real-world tasks, such as <b>MNIST,</b> Fashion and MOSI. As far as we know, QWAS achieves the state-of-art re-sults of all tasks in the terms of accuracy and circuit size.</p></p class="citation"></blockquote><h3 id=33--229292-optimal-scheduling-of-graph-states-via-path-decompositions-samuel-j-elman-et-al-2024>(3/3 | 229/292) Optimal Scheduling of Graph States via Path Decompositions (Samuel J. Elman et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Samuel J. Elman, Jason Gavriel, Ryan L. Mann. (2024)<br><strong>Optimal Scheduling of Graph States via Path Decompositions</strong><br><button class=copy-to-clipboard title="Optimal Scheduling of Graph States via Path Decompositions" index=229>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-229 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-CC, cs-DS, quant-ph, quant-ph<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04126v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04126v1.pdf filename=2403.04126v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the optimal scheduling of <b>graph</b> states in measurement-based quantum computation, establishing an equivalence between measurement schedules and path decompositions of <b>graphs.</b> We define the spatial cost of a measurement schedule based on the number of simultaneously active qubits and prove that an optimal measurement schedule corresponds to a path decomposition of minimal width. Our analysis shows that approximating the spatial cost of a <b>graph</b> is $\textsf{NP}$-hard, while for <b>graphs</b> with bounded spatial cost, we establish an efficient algorithm for computing an optimal measurement schedule.</p></p class="citation"></blockquote><h2 id=csse-3>cs.SE (3)</h2><h3 id=13--230292-exploring-llm-based-agents-for-root-cause-analysis-devjeet-roy-et-al-2024>(1/3 | 230/292) Exploring LLM-based Agents for Root Cause Analysis (Devjeet Roy et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Devjeet Roy, Xuchao Zhang, Rashi Bhave, Chetan Bansal, Pedro Las-Casas, Rodrigo Fonseca, Saravan Rajmohan. (2024)<br><strong>Exploring LLM-based Agents for Root Cause Analysis</strong><br><button class=copy-to-clipboard title="Exploring LLM-based Agents for Root Cause Analysis" index=230>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-230 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-CL, cs-LG, cs-SE, cs.SE<br>Keyword Score: 40<br>Keywords: Out-of-distribution, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04123v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04123v1.pdf filename=2403.04123v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The growing complexity of cloud based software systems has resulted in incident management becoming an integral part of the software development lifecycle. Root cause analysis (RCA), a critical part of the incident management process, is a demanding task for on-call engineers, requiring deep domain knowledge and extensive experience with a team&rsquo;s specific services. Automation of RCA can result in significant savings of time, and ease the burden of incident management on on-call engineers. Recently, researchers have utilized <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> to perform RCA, and have demonstrated promising results. However, these approaches are not able to dynamically collect additional diagnostic information such as incident related logs, metrics or databases, severely restricting their ability to diagnose root causes. In this work, we explore the use of <b>LLM</b> based agents for RCA to address this limitation. We present a thorough empirical evaluation of a ReAct agent equipped with retrieval tools, on an <b>out-of-distribution</b> dataset of production incidents collected at Microsoft. Results show that ReAct performs competitively with strong retrieval and <b>reasoning</b> baselines, but with highly increased factual accuracy. We then extend this evaluation by incorporating discussions associated with incident reports as additional inputs for the models, which surprisingly does not yield significant performance improvements. Lastly, we conduct a case study with a team at Microsoft to equip the ReAct agent with tools that give it access to external diagnostic services that are used by the team for manual RCA. Our results show how agents can overcome the limitations of prior work, and practical considerations for implementing such a system in practice.</p></p class="citation"></blockquote><h3 id=23--231292-unveiling-a-hidden-risk-exposing-educational-but-malicious-repositories-in-github-md-rayhanul-masud-et-al-2024>(2/3 | 231/292) Unveiling A Hidden Risk: Exposing Educational but Malicious Repositories in GitHub (Md Rayhanul Masud et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Md Rayhanul Masud, Michalis Faloutsos. (2024)<br><strong>Unveiling A Hidden Risk: Exposing Educational but Malicious Repositories in GitHub</strong><br><button class=copy-to-clipboard title="Unveiling A Hidden Risk: Exposing Educational but Malicious Repositories in GitHub" index=231>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-231 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-CR, cs-SE, cs.SE<br>Keyword Score: 20<br>Keywords: ChatGPT, Ransomware<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04419v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04419v1.pdf filename=2403.04419v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Are malicious repositories hiding under the educational label in GitHub? Recent studies have identified collections of GitHub repositories hosting malware source code with notable collaboration among the developers. Thus, analyzing GitHub repositories deserves inevitable attention due to its open-source nature providing easy access to malicious software code and artifacts. Here we leverage the capabilities of <b>ChatGPT</b> in a qualitative study to annotate an educational GitHub repository based on maliciousness of its metadata contents. Our contribution is twofold. First, we demonstrate the employment of <b>ChatGPT</b> to understand and annotate the content published in software repositories. Second, we provide evidence of hidden risk in educational repositories contributing to the opportunities of potential threats and malicious intents. We carry out a systematic study on a collection of 35.2K GitHub repositories claimed to be created for educational purposes only. First, our study finds an increasing trend in the number of such repositories published every year. Second, 9294 of them are labeled by <b>ChatGPT</b> as malicious, and further categorization of the malicious ones detects 14 different malware families including DDoS, keylogger, <b>ransomware</b> and so on. Overall, this exploratory study flags a wake-up call for the community for better understanding and analysis of software platforms.</p></p class="citation"></blockquote><h3 id=33--232292-shufflebench-a-benchmark-for-large-scale-data-shuffling-operations-with-distributed-stream-processing-frameworks-sören-henning-et-al-2024>(3/3 | 232/292) ShuffleBench: A Benchmark for Large-Scale Data Shuffling Operations with Distributed Stream Processing Frameworks (Sören Henning et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sören Henning, Adriano Vogel, Michael Leichtfried, Otmar Ertl, Rick Rabiser. (2024)<br><strong>ShuffleBench: A Benchmark for Large-Scale Data Shuffling Operations with Distributed Stream Processing Frameworks</strong><br><button class=copy-to-clipboard title="ShuffleBench: A Benchmark for Large-Scale Data Shuffling Operations with Distributed Stream Processing Frameworks" index=232>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-232 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-DC, cs-SE, cs.SE<br>Keyword Score: 11<br>Keywords: Benchmarking, Benchmarking, Black Box<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04570v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04570v1.pdf filename=2403.04570v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Distributed stream processing frameworks help building scalable and reliable applications that perform transformations and aggregations on continuous data streams. This paper introduces ShuffleBench, a novel <b>benchmark</b> to evaluate the performance of modern stream processing frameworks. In contrast to other <b>benchmarks,</b> it focuses on use cases where stream processing frameworks are mainly employed for shuffling (i.e., re-distributing) data records to perform state-local aggregations, while the actual aggregation logic is considered as <b>black-box</b> <b>software</b> components. ShuffleBench is inspired by requirements for near real-time analytics of a large cloud observability platform and takes up <b>benchmarking</b> metrics and methods for latency, throughput, and scalability established in the performance engineering research community. Although inspired by a real-world observability use case, it is highly configurable to allow domain-independent evaluations. ShuffleBench comes as a ready-to-use open-source software utilizing existing Kubernetes tooling and providing implementations for four state-of-the-art frameworks. Therefore, we expect ShuffleBench to be a valuable contribution to both industrial practitioners building stream processing applications and researchers working on new stream processing approaches. We complement this paper with an experimental performance evaluation that employs ShuffleBench with various configurations on Flink, Hazelcast, Kafka Streams, and Spark in a cloud-native environment. Our results show that Flink achieves the highest throughput while Hazelcast processes data streams with the lowest latency.</p></p class="citation"></blockquote><h2 id=cssd-2>cs.SD (2)</h2><h3 id=12--233292-a-study-of-dropout-induced-modality-bias-on-robustness-to-missing-video-frames-for-audio-visual-speech-recognition-yusheng-dai-et-al-2024>(1/2 | 233/292) A Study of Dropout-Induced Modality Bias on Robustness to Missing Video Frames for Audio-Visual Speech Recognition (Yusheng Dai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yusheng Dai, Hang Chen, Jun Du, Ruoyu Wang, Shihao Chen, Jiefeng Ma, Haotian Wang, Chin-Hui Lee. (2024)<br><strong>A Study of Dropout-Induced Modality Bias on Robustness to Missing Video Frames for Audio-Visual Speech Recognition</strong><br><button class=copy-to-clipboard title="A Study of Dropout-Induced Modality Bias on Robustness to Missing Video Frames for Audio-Visual Speech Recognition" index=233>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-233 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-CV, cs-LG, cs-MM, cs-SD, cs.SD, eess-AS<br>Keyword Score: 36<br>Keywords: Knowledge Distillation, Knowledge Distillation, Multi-modal, Multi-modal, Automatic Speech Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04245v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04245v1.pdf filename=2403.04245v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Advanced Audio-Visual <b>Speech</b> <b>Recognition</b> (AVSR) systems have been observed to be sensitive to missing video frames, performing even worse than single-modality models. While applying the dropout technique to the video modality enhances robustness to missing frames, it simultaneously results in a performance loss when dealing with complete data input. In this paper, we investigate this contrasting phenomenon from the perspective of modality bias and reveal that an excessive modality bias on the audio caused by dropout is the underlying reason. Moreover, we present the Modality Bias Hypothesis (MBH) to systematically describe the relationship between modality bias and robustness against missing modality in <b>multimodal</b> systems. Building on these findings, we propose a novel <b>Multimodal</b> Distribution Approximation with <b>Knowledge</b> <b>Distillation</b> (MDA-KD) framework to reduce over-reliance on the audio modality and to maintain performance and robustness simultaneously. Finally, to address an entirely missing modality, we adopt adapters to dynamically switch decision strategies. The effectiveness of our proposed approach is evaluated and validated through a series of comprehensive experiments using the MISP2021 and MISP2022 datasets. Our code is available at <a href=https://github.com/dalision/ModalBiasAVSR>https://github.com/dalision/ModalBiasAVSR</a></p></p class="citation"></blockquote><h3 id=22--234292-a-detailed-audio-text-data-simulation-pipeline-using-single-event-sounds-xuenan-xu-et-al-2024>(2/2 | 234/292) A Detailed Audio-Text Data Simulation Pipeline using Single-Event Sounds (Xuenan Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xuenan Xu, Xiaohang Xu, Zeyu Xie, Pingyue Zhang, Mengyue Wu, Kai Yu. (2024)<br><strong>A Detailed Audio-Text Data Simulation Pipeline using Single-Event Sounds</strong><br><button class=copy-to-clipboard title="A Detailed Audio-Text Data Simulation Pipeline using Single-Event Sounds" index=234>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-234 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-SD, cs.SD, eess-AS<br>Keyword Score: 30<br>Keywords: Simulation, Simulator, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04594v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04594v1.pdf filename=2403.04594v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, there has been an increasing focus on audio-text cross-modal learning. However, most of the existing audio-text datasets contain only simple descriptions of sound events. Compared with classification labels, the advantages of such descriptions are significantly limited. In this paper, we first analyze the detailed information that human descriptions of audio may contain beyond sound event labels. Based on the analysis, we propose an automatic pipeline for curating audio-text pairs with rich details. Leveraging the property that sounds can be mixed and concatenated in the time domain, we control details in four aspects: temporal relationship, loudness, speaker identity, and occurrence number, in simulating audio mixtures. Corresponding details are transformed into captions by <b>large</b> <b>language</b> <b>models.</b> Audio-text pairs with rich details in text descriptions are thereby obtained. We validate the effectiveness of our pipeline with a small amount of simulated data, demonstrating that the simulated data enables models to learn detailed audio captioning.</p></p class="citation"></blockquote><h2 id=csni-3>cs.NI (3)</h2><h3 id=13--235292-itrpl-an-intelligent-and-trusted-rpl-protocol-based-on-multi-agent-reinforcement-learning-debasmita-dey-et-al-2024>(1/3 | 235/292) iTRPL: An Intelligent and Trusted RPL Protocol based on Multi-Agent Reinforcement Learning (Debasmita Dey et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Debasmita Dey, Nirnay Ghosh. (2024)<br><strong>iTRPL: An Intelligent and Trusted RPL Protocol based on Multi-Agent Reinforcement Learning</strong><br><button class=copy-to-clipboard title="iTRPL: An Intelligent and Trusted RPL Protocol based on Multi-Agent Reinforcement Learning" index=235>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-235 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI<br>Keyword Score: 33<br>Keywords: Graph, Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04416v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04416v1.pdf filename=2403.04416v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Routing Protocol for Low Power and Lossy Networks (RPL) is the de-facto routing standard in IoT networks. It enables nodes to collaborate and autonomously build ad-hoc networks modeled by tree-like destination-oriented direct acyclic <b>graphs</b> (DODAG). Despite its widespread usage in industry and healthcare domains, RPL is susceptible to insider attacks. Although the state-of-the-art RPL ensures that only authenticated nodes participate in DODAG, such hard security measures are still inadequate to prevent insider threats. This entails a need to integrate soft security mechanisms to support decision-making. This paper proposes iTRPL, an intelligent and behavior-based framework that incorporates trust to segregate honest and malicious nodes within a DODAG. It also leverages multi-agent <b>reinforcement</b> <b>learning</b> (MARL) to make autonomous decisions concerning the DODAG. The framework enables a parent node to compute the trust for its child and decide if the latter can join the DODAG. It tracks the behavior of the child node, updates the trust, computes the rewards (or penalties), and shares with the root. The root aggregates the rewards/penalties of all nodes, computes the overall return, and decides via its $\epsilon$-Greedy MARL module if the DODAG will be retained or modified for the future. A <b>simulation-based</b> performance evaluation demonstrates that iTRPL learns to make optimal decisions with time.</p></p class="citation"></blockquote><h3 id=23--236292-performance-evaluation-of-conditional-handover-in-5g-systems-under-fading-scenario-souvik-deb-et-al-2024>(2/3 | 236/292) Performance evaluation of conditional handover in 5G systems under fading scenario (Souvik Deb et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Souvik Deb, Megh Rathod, Rishi Balamurugan, Shankar K. Ghosh, Rajeev K. Singh, Samriddha Sanyal. (2024)<br><strong>Performance evaluation of conditional handover in 5G systems under fading scenario</strong><br><button class=copy-to-clipboard title="Performance evaluation of conditional handover in 5G systems under fading scenario" index=236>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-236 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04379v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04379v1.pdf filename=2403.04379v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To enhance the handover performance in fifth generation (5G) cellular systems, conditional handover (CHO) has been evolved as a promising solution. Unlike A3 based handover where handover execution is certain after receiving handover command from the serving access network, in CHO, handover execution is conditional on the RSRP measurements from both current and target access networks, as well as on mobility parameters such as preparation and execution offsets. Analytic evaluation of conditional handover performance is unprecedented in literature. In this work, handover performance of CHO has been carried out in terms of handover latency, handover packet loss and handover failure probability. A Markov model accounting the effect of different mobility parameters (e.g., execution offset, preparation offset, time-to-preparation and time-to-execution), UE velocity and channel fading characteristics; has been proposed to characterize handover failure. Results obtained from the analytic model has been validated against extensive <b>simulation</b> results. Our study reveal that optimal configuration of $O_{exec}$, $O_{prep}$, $T_{exec}$ and $T_{prep}$ is actually conditional on underlying UE velocity and fading characteristics. This study will be helpful for the mobile operators to choose appropriate thresholds of the mobility parameters under different channel condition and UE velocities.</p></p class="citation"></blockquote><h3 id=33--237292-super-resolution-on-network-telemetry-time-series-fengchen-gong-et-al-2024>(3/3 | 237/292) Super-resolution on network telemetry time series (Fengchen Gong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fengchen Gong, Divya Raghunathan, Aarti Gupta, Maria Apostolaki. (2024)<br><strong>Super-resolution on network telemetry time series</strong><br><button class=copy-to-clipboard title="Super-resolution on network telemetry time series" index=237>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-237 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04165v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04165v1.pdf filename=2403.04165v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Fine-grained monitoring is crucial for multiple data-driven tasks such as debugging, provisioning, and securing networks. Yet, practical constraints in collecting, extracting, and storing data often force operators to use coarse-grained sampled monitoring, degrading the performance of the various tasks. In this work, we explore the feasibility of leveraging the correlations among coarse-grained time series to impute their fine-grained counterparts in software. We present Zoom2Net, a <b>transformer-based</b> model for network imputation that incorporates domain knowledge through operational and measurement constraints, ensuring that the imputed network telemetry time series are not only realistic but also align with existing measurements and are plausible. This approach enhances the capabilities of current monitoring infrastructures, allowing operators to gain more insights into system behaviors without the need for hardware upgrades. We evaluate Zoom2Net on four diverse datasets (e.g. cloud telemetry and Internet data transfer) and use cases (such as bursts analysis and traffic classification). We demonstrate that Zoom2Net consistently achieves high imputation accuracy with a zoom-in factor of up to 100 and performs better on downstream tasks compared to baselines by an average of 38%.</p></p class="citation"></blockquote><h2 id=csdc-5>cs.DC (5)</h2><h3 id=15--238292-optimizing-cnn-using-hpc-tools-shahrin-rahman-2024>(1/5 | 238/292) Optimizing CNN Using HPC Tools (Shahrin Rahman, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shahrin Rahman. (2024)<br><strong>Optimizing CNN Using HPC Tools</strong><br><button class=copy-to-clipboard title="Optimizing CNN Using HPC Tools" index=238>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-238 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: N/A, cs-DC, cs.DC<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04870v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04870v1.pdf filename=2403.04870v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper optimizes the <b>Convolutional</b> <b>Neural</b> <b>Network</b> <b>(CNN)</b> algorithm using high-performance computing (HPC) technologies. It uses multi-core processors, GPUs, and parallel computing frameworks like OpenMPI and CUDA to speed up <b>CNN</b> model training. The approach improves performance and training time and is superior to alternative strategies. The study demonstrates how HPC technologies can refine the <b>CNN</b> method, resulting in faster and more accurate training of large-scale <b>CNN</b> models.</p></p class="citation"></blockquote><h3 id=25--239292-greenbytes-intelligent-energy-estimation-for-edge-cloud-kasra-kassai-et-al-2024>(2/5 | 239/292) GreenBytes: Intelligent Energy Estimation for Edge-Cloud (Kasra Kassai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kasra Kassai, Tasos Dagiuklas, Satwat Bashir, Muddesar Iqbal. (2024)<br><strong>GreenBytes: Intelligent Energy Estimation for Edge-Cloud</strong><br><button class=copy-to-clipboard title="GreenBytes: Intelligent Energy Estimation for Edge-Cloud" index=239>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-239 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs-ET, cs-NI, cs.DC<br>Keyword Score: 30<br>Keywords: LSTM, LSTM, LSTM<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04665v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04665v1.pdf filename=2403.04665v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study investigates the application of advanced machine learning models, specifically <b>Long</b> <b>Short-Term</b> <b>Memory</b> <b>(LSTM)</b> networks and Gradient Booster models, for accurate energy consumption estimation within a Kubernetes cluster environment. It aims to enhance sustainable computing practices by providing precise predictions of energy usage across various computing nodes. Through meticulous analysis of model performance on both master and worker nodes, the research reveals the strengths and potential applications of these models in promoting energy efficiency. The <b>LSTM</b> model demonstrates remarkable predictive accuracy, particularly in capturing dynamic computing workloads over time, evidenced by low mean squared error (MSE) rates and the ability to closely track actual energy consumption trends. Conversely, the Gradient Booster model showcases robustness and adaptability across different computational environments, despite slightly higher MSE values. The study underscores the complementary nature of these models in advancing sustainable computing practices, suggesting their integration into energy management systems could significantly enhance environmental sustainability in technology operations.</p></p class="citation"></blockquote><h3 id=35--240292-parendi-thousand-way-parallel-rtl-simulation-mahyar-emami-et-al-2024>(3/5 | 240/292) Parendi: Thousand-Way Parallel RTL Simulation (Mahyar Emami et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mahyar Emami, Thomas Bourgeat, James Larus. (2024)<br><strong>Parendi: Thousand-Way Parallel RTL Simulation</strong><br><button class=copy-to-clipboard title="Parendi: Thousand-Way Parallel RTL Simulation" index=240>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-240 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-AR, cs-DC, cs.DC<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04714v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04714v1.pdf filename=2403.04714v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Hardware development relies on <b>simulations,</b> particularly cycle-accurate RTL (Register Transfer Level) <b>simulations,</b> which consume significant time. As single-processor performance grows only slowly, conventional, single-threaded RTL <b>simulation</b> is becoming less practical for increasingly complex chips and systems. A solution is parallel RTL <b>simulation,</b> where ideally, simulators could run on thousands of parallel cores. However, existing simulators can only exploit tens of cores. This paper studies the challenges inherent in running parallel RTL <b>simulation</b> on a multi-thousand-core machine (the Graphcore IPU, a 1472-core machine). <b>Simulation</b> performance requires balancing three factors: synchronization, communication, and computation. We experimentally evaluate each metric and analyze how it affects parallel <b>simulation</b> speed, drawing on contrasts between the large-scale IPU and smaller but faster x86 systems. Using this analysis, we build Parendi, an RTL simulator for the IPU. It distributes RTL <b>simulation</b> across 5888 cores on 4 IPU sockets. Parendi runs large RTL designs up to 4x faster than a powerful, state-of-the-art x86 multicore system.</p></p class="citation"></blockquote><h3 id=45--241292-fedclust-optimizing-federated-learning-on-non-iid-data-through-weight-driven-client-clustering-md-sirajul-islam-et-al-2024>(4/5 | 241/292) FedClust: Optimizing Federated Learning on Non-IID Data through Weight-Driven Client Clustering (Md Sirajul Islam et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Md Sirajul Islam, Simin Javaherian, Fei Xu, Xu Yuan, Li Chen, Nian-Feng Tzeng. (2024)<br><strong>FedClust: Optimizing Federated Learning on Non-IID Data through Weight-Driven Client Clustering</strong><br><button class=copy-to-clipboard title="FedClust: Optimizing Federated Learning on Non-IID Data through Weight-Driven Client Clustering" index=241>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-241 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs-LG, cs.DC<br>Keyword Score: 13<br>Keywords: Clustering, Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04144v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04144v1.pdf filename=2403.04144v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Federated</b> <b>learning</b> (FL) is an emerging distributed machine learning paradigm enabling collaborative model training on decentralized devices without exposing their local data. A key challenge in FL is the uneven data distribution across client devices, violating the well-known assumption of independent-and-identically-distributed (IID) training samples in conventional machine learning. Clustered <b>federated</b> <b>learning</b> (CFL) addresses this challenge by grouping clients based on the similarity of their data distributions. However, existing CFL approaches require a large number of communication rounds for stable cluster formation and rely on a predefined number of clusters, thus limiting their flexibility and adaptability. This paper proposes FedClust, a novel CFL approach leveraging correlations between local model weights and client data distributions. FedClust groups clients into clusters in a one-shot manner using strategically selected partial model weights and dynamically accommodates newcomers in real-time. Experimental results demonstrate FedClust outperforms baseline approaches in terms of accuracy and communication costs.</p></p class="citation"></blockquote><h3 id=55--242292-improvements--evaluations-on-the-mlcommons-cloudmask-benchmark-varshitha-chennamsetti-et-al-2024>(5/5 | 242/292) Improvements & Evaluations on the MLCommons CloudMask Benchmark (Varshitha Chennamsetti et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Varshitha Chennamsetti, Laiba Mehnaz, Dan Zhao, Banani Ghosh, Sergey V. Samsonau. (2024)<br><strong>Improvements & Evaluations on the MLCommons CloudMask Benchmark</strong><br><button class=copy-to-clipboard title="Improvements & Evaluations on the MLCommons CloudMask Benchmark" index=242>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-242 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs-LG, cs.DC<br>Keyword Score: 6<br>Keywords: Benchmarking, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04553v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04553v1.pdf filename=2403.04553v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we report the performance <b>benchmarking</b> results of deep learning models on MLCommons&rsquo; Science cloud-masking <b>benchmark</b> using a high-performance computing cluster at New York University (NYU): NYU Greene. MLCommons is a consortium that develops and maintains several scientific <b>benchmarks</b> that can benefit from developments in AI. We provide a description of the cloud-masking <b>benchmark</b> task, updated code, and the best model for this <b>benchmark</b> when using our selected hyperparameter settings. Our <b>benchmarking</b> results include the highest accuracy achieved on the NYU system as well as the average time taken for both training and inference on the <b>benchmark</b> across several runs/seeds. Our code can be found on GitHub. MLCommons team has been kept informed about our progress and may use the developed code for their future work.</p></p class="citation"></blockquote><h2 id=csce-2>cs.CE (2)</h2><h3 id=12--243292-effect-of-turbulent-diffusion-in-modeling-anaerobic-digestion-jeremy-z-yan-et-al-2024>(1/2 | 243/292) Effect of turbulent diffusion in modeling anaerobic digestion (Jeremy Z. Yan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jeremy Z. Yan, Prashant Kumar, Wolfgang Rauch. (2024)<br><strong>Effect of turbulent diffusion in modeling anaerobic digestion</strong><br><button class=copy-to-clipboard title="Effect of turbulent diffusion in modeling anaerobic digestion" index=243>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-243 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CE<br>Categories: cs-CE, cs.CE<br>Keyword Score: 30<br>Keywords: Diffusion Model, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04457v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04457v1.pdf filename=2403.04457v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this study, the impact of turbulent <b>diffusion</b> <b>on</b> mixing of biochemical reaction models is explored by implementing and validating different models. An original codebase called CHAD (Coupled Hydrodynamics and Anaerobic Digestion) is extended to incorporate turbulent <b>diffusion</b> <b>and</b> validate it against results from OpenFOAM with 2D Rayleigh-Taylor Instability and lid-driven cavity <b>simulations.</b> The models are then tested for the applications with Anaerobic Digestion - a widely used wastewater treatment method. The findings demonstrate that the implemented models accurately capture turbulent <b>diffusion</b> <b>when</b> provided with an accurate flow field. Specifically, a minor effect of chemical turbulent <b>diffusion</b> <b>on</b> biochemical reactions within the anaerobic digestion tank is observed, while thermal turbulent <b>diffusion</b> <b>significantly</b> influences mixing. By successfully implementing turbulent <b>diffusion</b> <b>models</b> in CHAD, its capabilities for more accurate anaerobic digestion <b>simulations</b> are enhanced, aiding in optimizing the design and operation of anaerobic digestion reactors in real-world wastewater treatment applications.</p></p class="citation"></blockquote><h3 id=22--244292-sentiment-driven-prediction-of-financial-returns-a-bayesian-enhanced-finbert-approach-raffaele-giuseppe-cestari-et-al-2024>(2/2 | 244/292) Sentiment-driven prediction of financial returns: a Bayesian-enhanced FinBERT approach (Raffaele Giuseppe Cestari et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Raffaele Giuseppe Cestari, Simone Formentin. (2024)<br><strong>Sentiment-driven prediction of financial returns: a Bayesian-enhanced FinBERT approach</strong><br><button class=copy-to-clipboard title="Sentiment-driven prediction of financial returns: a Bayesian-enhanced FinBERT approach" index=244>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-244 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CE<br>Categories: cs-AI, cs-CE, cs.CE<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04427v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04427v1.pdf filename=2403.04427v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Predicting financial returns accurately poses a significant challenge due to the inherent uncertainty in financial time series data. Enhancing prediction models&rsquo; performance hinges on effectively capturing both social and financial sentiment. In this study, we showcase the efficacy of leveraging sentiment information extracted from tweets using the FinBERT <b>large</b> <b>language</b> <b>model.</b> By meticulously curating an optimal feature set through correlation analysis and employing Bayesian-optimized Recursive Feature Elimination for automatic feature selection, we surpass existing methodologies, achieving an F1-score exceeding 70% on the test set. This success translates into demonstrably higher cumulative profits during backtested trading. Our investigation focuses on real-world SPY ETF data alongside corresponding tweets sourced from the StockTwits platform.</p></p class="citation"></blockquote><h2 id=eesssy-6>eess.SY (6)</h2><h3 id=16--245292-model-free-load-frequency-control-of-nonlinear-power-systems-based-on-deep-reinforcement-learning-xiaodi-chen-et-al-2024>(1/6 | 245/292) Model-Free Load Frequency Control of Nonlinear Power Systems Based on Deep Reinforcement Learning (Xiaodi Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaodi Chen, Meng Zhang, Zhengguang Wu, Ligang Wu, Xiaohong Guan. (2024)<br><strong>Model-Free Load Frequency Control of Nonlinear Power Systems Based on Deep Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Model-Free Load Frequency Control of Nonlinear Power Systems Based on Deep Reinforcement Learning" index=245>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-245 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-AI, cs-SY, eess-SY, eess.SY<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04374v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04374v1.pdf filename=2403.04374v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Load frequency control (LFC) is widely employed in power systems to stabilize frequency fluctuation and guarantee power quality. However, most existing LFC methods rely on accurate power system modeling and usually ignore the nonlinear characteristics of the system, limiting controllers&rsquo; performance. To solve these problems, this paper proposes a model-free LFC method for nonlinear power systems based on deep deterministic policy gradient (DDPG) framework. The proposed method establishes an emulator network to emulate power system dynamics. After defining the action-value function, the emulator network is applied for control actions evaluation instead of the critic network. Then the actor network controller is effectively optimized by estimating the policy gradient based on zeroth-order optimization (ZOO) and backpropagation algorithm. <b>Simulation</b> results and corresponding comparisons demonstrate the designed controller can generate appropriate control actions and has strong adaptability for nonlinear power systems.</p></p class="citation"></blockquote><h3 id=26--246292-controller-adaptation-via-learning-solutions-of-contextual-bayesian-optimization-viet-anh-le-et-al-2024>(2/6 | 246/292) Controller Adaptation via Learning Solutions of Contextual Bayesian Optimization (Viet-Anh Le et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Viet-Anh Le, Andreas A. Malikopoulos. (2024)<br><strong>Controller Adaptation via Learning Solutions of Contextual Bayesian Optimization</strong><br><button class=copy-to-clipboard title="Controller Adaptation via Learning Solutions of Contextual Bayesian Optimization" index=246>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-246 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 25<br>Keywords: Black Box, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04881v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04881v1.pdf filename=2403.04881v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this letter, we propose a framework for adapting the controller&rsquo;s parameters based on learning optimal solutions from contextual <b>black-box</b> <b>optimization</b> problems. We consider a class of control design problems for dynamical systems operating in different environments or conditions represented by contextual parameters. The overarching goal is to identify the controller parameters that maximize the controlled system&rsquo;s performance, given different realizations of the contextual parameters. We formulate a contextual Bayesian optimization problem in which the solution is actively learned using Gaussian processes to approximate the controller adaptation strategy. We demonstrate the efficacy of the proposed framework with a <b>simulation-to-real</b> example. We learn the optimal weighting strategy of a model predictive control for connected and automated vehicles interacting with human-driven vehicles from <b>simulations</b> and then deploy it in a real-time experiment.</p></p class="citation"></blockquote><h3 id=36--247292-tensor-power-flow-formulations-for-multidimensional-analyses-in-distribution-systems-edgar-mauricio-salazar-duque-et-al-2024>(3/6 | 247/292) Tensor Power Flow Formulations for Multidimensional Analyses in Distribution Systems (Edgar Mauricio Salazar Duque et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Edgar Mauricio Salazar Duque, Juan S. Giraldo, Pedro P. Vergara, Phuong H. Nguyen, Han, Slootweg. (2024)<br><strong>Tensor Power Flow Formulations for Multidimensional Analyses in Distribution Systems</strong><br><button class=copy-to-clipboard title="Tensor Power Flow Formulations for Multidimensional Analyses in Distribution Systems" index=247>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-247 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 23<br>Keywords: Benchmarking, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04578v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04578v1.pdf filename=2403.04578v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we present two multidimensional power flow formulations based on a fixed-point iteration (FPI) algorithm to efficiently solve hundreds of thousands of power flows in distribution systems. The presented algorithms are the base for a new TensorPowerFlow (TPF) tool and shine for their simplicity, benefiting from multicore \gls{cpu} and \gls{gpu} parallelization. We also focus on the mathematical convergence properties of the algorithm, showing that its unique solution is at the practical operational point, which is the solution of high-voltage and low-current. The proof is validated using numerical <b>simulations</b> showing the robustness of the FPI algorithm compared to the classical \gls{nr} approach. In the case study, a <b>benchmark</b> with different PF solution methods is performed, showing that for applications requiring a yearly <b>simulation</b> at 1-minute resolution the computation time is decreased by a factor of 164, compared to the NR in its sparse formulation.</p></p class="citation"></blockquote><h3 id=46--248292-closed-loop-performance-optimization-of-model-predictive-control-with-robustness-guarantees-riccardo-zuliani-et-al-2024>(4/6 | 248/292) Closed-loop Performance Optimization of Model Predictive Control with Robustness Guarantees (Riccardo Zuliani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Riccardo Zuliani, Efe C. Balta, John Lygeros. (2024)<br><strong>Closed-loop Performance Optimization of Model Predictive Control with Robustness Guarantees</strong><br><button class=copy-to-clipboard title="Closed-loop Performance Optimization of Model Predictive Control with Robustness Guarantees" index=248>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-248 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY, math-OC<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04655v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04655v1.pdf filename=2403.04655v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Model mismatch and process noise are two frequently occurring phenomena that can drastically affect the performance of model predictive control (MPC) in practical applications. We propose a principled way to tune the cost function and the constraints of linear MPC schemes to achieve good performance and robust constraint satisfaction on uncertain nonlinear dynamics with additive noise. The tuning is performed using a novel MPC tuning algorithm based on backpropagation developed in our earlier work. Using the scenario approach, we provide probabilistic bounds on the likelihood of closed-loop constraint violation over a finite horizon. We showcase the effectiveness of the proposed method on linear and nonlinear <b>simulation</b> examples.</p></p class="citation"></blockquote><h3 id=56--249292-control-barrier-functions-for-linear-continuous-time-input-delay-systems-with-limited-horizon-previewable-disturbances-tarun-pati-et-al-2024>(5/6 | 249/292) Control Barrier Functions for Linear Continuous-Time Input-Delay Systems with Limited-Horizon Previewable Disturbances (Tarun Pati et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tarun Pati, Seunghoon Hwang, Sze Zheng Yong. (2024)<br><strong>Control Barrier Functions for Linear Continuous-Time Input-Delay Systems with Limited-Horizon Previewable Disturbances</strong><br><button class=copy-to-clipboard title="Control Barrier Functions for Linear Continuous-Time Input-Delay Systems with Limited-Horizon Previewable Disturbances" index=249>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-249 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 10<br>Keywords: Continuous Time, Continuous Time<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04243v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04243v1.pdf filename=2403.04243v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Cyber-physical and autonomous systems are often equipped with mechanisms that provide predictions/projections of future disturbances, e.g., road curvatures, commonly referred to as preview or lookahead, but this preview information is typically not leveraged in the context of deriving control barrier functions (CBFs) for safety. This paper proposes a novel limited preview control barrier function (LPrev-CBF) that avoids both ends of the spectrum, where on one end, the standard CBF approach treats the (previewable) disturbances simply as worst-case adversarial signals and on the other end, a recent Prev-CBF approach assumes that the disturbances are previewable and known for the entire future. Moreover, our approach applies to input-delay systems and has recursive feasibility guarantees since we explicitly take input constraints/bounds into consideration. Thus, our approach provides strong safety guarantees in a less conservative manner than standard CBF approaches while considering a more realistic setting with limited preview and input delays.</p></p class="citation"></blockquote><h3 id=66--250292-a-crosstalk-aware-timing-prediction-method-in-routing-leilei-jin-et-al-2024>(6/6 | 250/292) A Crosstalk-Aware Timing Prediction Method in Routing (Leilei Jin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Leilei Jin, Jiajie Xu, Wenjie Fu, Hao Yan, Longxing Shi. (2024)<br><strong>A Crosstalk-Aware Timing Prediction Method in Routing</strong><br><button class=copy-to-clipboard title="A Crosstalk-Aware Timing Prediction Method in Routing" index=250>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-250 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: I-6-4; B-7-3, cs-SY, eess-SY, eess.SY<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04145v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04145v1.pdf filename=2403.04145v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With shrinking interconnect spacing in advanced technology nodes, existing timing predictions become less precise due to the challenging quantification of crosstalk-induced delay. During the routing, the crosstalk effect is typically modeled by predicting coupling capacitance with congestion information. However, the timing estimation tends to be overly pessimistic, as the crosstalk-induced delay depends not only on the coupling capacitance but also on the signal arrival time. This work presents a crosstalk-aware timing estimation method using a two-step machine learning approach. Interconnects that are physically adjacent and overlap in signal timing windows are filtered first. Crosstalk delay is predicted by integrating physical topology and timing features without relying on post-routing results and the parasitic extraction. Experimental results show a match rate of over 99% for identifying crosstalk nets compared to the commercial tool on the OpenCores <b>benchmarks,</b> with prediction results being more accurate than those of other state-of-the-art methods.</p></p class="citation"></blockquote><h2 id=csgt-4>cs.GT (4)</h2><h3 id=14--251292-rl-cfr-improving-action-abstraction-for-imperfect-information-extensive-form-games-with-reinforcement-learning-boning-li-et-al-2024>(1/4 | 251/292) RL-CFR: Improving Action Abstraction for Imperfect Information Extensive-Form Games with Reinforcement Learning (Boning Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Boning Li, Zhixuan Fang, Longbo Huang. (2024)<br><strong>RL-CFR: Improving Action Abstraction for Imperfect Information Extensive-Form Games with Reinforcement Learning</strong><br><button class=copy-to-clipboard title="RL-CFR: Improving Action Abstraction for Imperfect Information Extensive-Form Games with Reinforcement Learning" index=251>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-251 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-GT, cs-LG, cs.GT<br>Keyword Score: 30<br>Keywords: Counter-factual, Markov Decision Process, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04344v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04344v1.pdf filename=2403.04344v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Effective action abstraction is crucial in tackling challenges associated with large action spaces in Imperfect Information Extensive-Form Games (IIEFGs). However, due to the vast state space and computational complexity in IIEFGs, existing methods often rely on fixed abstractions, resulting in sub-optimal performance. In response, we introduce RL-CFR, a novel <b>reinforcement</b> <b>learning</b> (RL) approach for dynamic action abstraction. RL-CFR builds upon our innovative <b>Markov</b> <b>Decision</b> <b>Process</b> (MDP) formulation, with states corresponding to public information and actions represented as feature vectors indicating specific action abstractions. The reward is defined as the expected payoff difference between the selected and default action abstractions. RL-CFR constructs a game tree with RL-guided action abstractions and utilizes <b>counterfactual</b> regret minimization (CFR) for strategy derivation. Impressively, it can be trained from scratch, achieving higher expected payoff without increased CFR solving time. In experiments on Heads-up No-limit Texas Hold&rsquo;em, RL-CFR outperforms ReBeL&rsquo;s replication and Slumbot, demonstrating significant win-rate margins of $64\pm 11$ and $84\pm 17$ mbb/hand, respectively.</p></p class="citation"></blockquote><h3 id=24--252292-mechanism-for-decision-aware-collaborative-federated-learning-a-pitfall-of-shapley-values-meng-qi-et-al-2024>(2/4 | 252/292) Mechanism for Decision-aware Collaborative Federated Learning: A Pitfall of Shapley Values (Meng Qi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Meng Qi, Mingxi Zhu. (2024)<br><strong>Mechanism for Decision-aware Collaborative Federated Learning: A Pitfall of Shapley Values</strong><br><button class=copy-to-clipboard title="Mechanism for Decision-aware Collaborative Federated Learning: A Pitfall of Shapley Values" index=252>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-252 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-GT, cs.GT<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04753v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04753v1.pdf filename=2403.04753v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper investigates mechanism design for decision-aware collaboration via <b>federated</b> <b>learning</b> (FL) platforms. Our framework consists of a digital platform and multiple decision-aware agents, each endowed with proprietary data sets. The platform offers an infrastructure that enables access to the data, creates incentives for collaborative learning aimed at operational decision-making, and conducts FL to avoid direct raw data sharing. The computation and communication efficiency of the FL process is inherently influenced by the agent participation equilibrium induced by the mechanism. Therefore, assessing the system&rsquo;s efficiency involves two critical factors: the surplus created by coalition formation and the communication costs incurred across the coalition during FL. To evaluate the system efficiency under the intricate interplay between mechanism design, agent participation, operational decision-making, and the performance of FL algorithms, we introduce a multi-action collaborative <b>federated</b> <b>learning</b> (MCFL) framework for decision-aware agents. Under this framework, we further analyze the equilibrium for the renowned Shapley value based mechanisms. Specifically, we examine the issue of false-name manipulation, a form of dishonest behavior where participating agents create duplicate fake identities to split their original data among these identities. By solving the agent participation equilibrium, we demonstrate that while Shapley value effectively maximizes coalition-generated surplus by encouraging full participation, it inadvertently promotes false-name manipulation. This further significantly increases the communication costs when the platform conducts FL. Thus, we highlight a significant pitfall of Shapley value based mechanisms, which implicitly incentivizes data splitting and identity duplication, ultimately impairing the overall efficiency in FL systems.</p></p class="citation"></blockquote><h3 id=34--253292-conflict-and-fairness-in-resource-allocation-susobhan-bandopadhyay-et-al-2024>(3/4 | 253/292) Conflict and Fairness in Resource Allocation (Susobhan Bandopadhyay et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Susobhan Bandopadhyay, Aritra Banik, Sushmita Gupta, Pallavi Jain, Abhishek Sahu, Saket Saurabh, Prafullkumar Tale. (2024)<br><strong>Conflict and Fairness in Resource Allocation</strong><br><button class=copy-to-clipboard title="Conflict and Fairness in Resource Allocation" index=253>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-253 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-DS, cs-GT, cs.GT<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04265v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04265v1.pdf filename=2403.04265v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the standard model of fair allocation of resources to agents, every agent has some utility for every resource, and the goal is to assign resources to agents so that the agents&rsquo; welfare is maximized. Motivated by job scheduling, interest in this problem dates back to the work of Deuermeyer et al. [SIAM J. on Algebraic Discrete Methods'82]. Recent works consider the compatibility between resources and assign only mutually compatible resources to an agent. We study a fair allocation problem in which we are given a set of agents, a set of resources, a utility function for every agent over a set of resources, and a {\it conflict graph} on the set of resources (where an edge denotes incompatibility). The goal is to assign resources to the agents such that $(i)$ the set of resources allocated to an agent are compatible with each other, and $(ii)$ the minimum satisfaction of an agent is maximized, where the satisfaction of an agent is the sum of the utility of the assigned resources. Chiarelli et al. [Algorithmica'22] explore this problem from the classical complexity perspective to draw the boundary between the cases that are polynomial-time solvable and those that are \NP-hard. In this article, we study the parameterized complexity of the problem (and its variants) by considering several natural and structural parameters.</p></p class="citation"></blockquote><h3 id=44--254292-extensive-form-game-solving-via-blackwell-approachability-on-treeplexes-darshan-chakrabarti-et-al-2024>(4/4 | 254/292) Extensive-Form Game Solving via Blackwell Approachability on Treeplexes (Darshan Chakrabarti et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Darshan Chakrabarti, Julien Grand-Clément, Christian Kroer. (2024)<br><strong>Extensive-Form Game Solving via Blackwell Approachability on Treeplexes</strong><br><button class=copy-to-clipboard title="Extensive-Form Game Solving via Blackwell Approachability on Treeplexes" index=254>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-254 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-GT, cs.GT<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04680v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04680v1.pdf filename=2403.04680v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we introduce the first algorithmic framework for Blackwell approachability on the sequence-form polytope, the class of convex polytopes capturing the strategies of players in extensive-form games (EFGs). This leads to a new class of regret-minimization algorithms that are stepsize-invariant, in the same sense as the Regret Matching and Regret Matching$^+$ algorithms for the simplex. Our modular framework can be combined with any existing regret minimizer over cones to compute a Nash equilibrium in two-player zero-sum EFGs with perfect recall, through the self-play framework. Leveraging predictive online mirror descent, we introduce Predictive Treeplex Blackwell$^+$ (PTB$^+$), and show a $O(1/\sqrt{T})$ convergence rate to Nash equilibrium in self-play. We then show how to stabilize PTB$^+$ with a stepsize, resulting in an algorithm with a state-of-the-art $O(1/T)$ convergence rate. We provide an extensive set of experiments to compare our framework with several algorithmic <b>benchmarks,</b> including CFR$^+$ and its predictive variant, and we highlight interesting connections between practical performance and the stepsize-dependence or stepsize-invariance properties of classical algorithms.</p></p class="citation"></blockquote><h2 id=physicsbio-ph-1>physics.bio-ph (1)</h2><h3 id=11--255292-preference-optimization-of-protein-language-models-as-a-multi-objective-binder-design-paradigm-pouria-mistani-et-al-2024>(1/1 | 255/292) Preference optimization of protein language models as a multi-objective binder design paradigm (Pouria Mistani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pouria Mistani, Venkatesh Mysore. (2024)<br><strong>Preference optimization of protein language models as a multi-objective binder design paradigm</strong><br><button class=copy-to-clipboard title="Preference optimization of protein language models as a multi-objective binder design paradigm" index=255>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-255 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.bio-ph<br>Categories: cs-AI, cs-CE, physics-bio-ph, physics.bio-ph, q-bio-BM<br>Keyword Score: 30<br>Keywords: Direct Preference Optimization, Fine-tuning, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04187v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04187v1.pdf filename=2403.04187v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a multi-objective binder design paradigm based on instruction <b>fine-tuning</b> and <b>direct</b> <b>preference</b> <b>optimization</b> (DPO) of autoregressive protein language models <b>(pLMs).</b> Multiple design objectives are encoded in the language model through <b>direct</b> <b>optimization</b> <b>on</b> expert curated preference sequence datasets comprising preferred and dispreferred distributions. We show the proposed alignment strategy enables ProtGPT2 to effectively design binders conditioned on specified receptors and a drug developability criterion. Generated binder samples demonstrate median isoelectric point (pI) improvements by $17%-60%$.</p></p class="citation"></blockquote><h2 id=cssi-2>cs.SI (2)</h2><h3 id=12--256292-generating-insights-about-financial-asks-from-reddit-posts-and-user-interactions-sachin-thukral-et-al-2024>(1/2 | 256/292) Generating insights about financial asks from Reddit posts and user interactions (Sachin Thukral et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sachin Thukral, Suyash Sangwan, Vipul Chauhan, Arnab Chatterjee, Lipika Dey. (2024)<br><strong>Generating insights about financial asks from Reddit posts and user interactions</strong><br><button class=copy-to-clipboard title="Generating insights about financial asks from Reddit posts and user interactions" index=256>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-256 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-SI, cs.SI<br>Keyword Score: 23<br>Keywords: Clustering, ChatGPT, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04308v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04308v2.pdf filename=2403.04308v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As an increasingly large number of people turn to platforms like Reddit, YouTube, Twitter, Instagram, etc. for financial advice, generating insights about the content generated and interactions taking place within these platforms have become a key research question. This study proposes content and interaction analysis techniques for a large repository created from social media content, where people interactions are centered around financial information exchange. We propose methods for content analysis that can generate human-interpretable insights using topic-centered <b>clustering</b> and multi-document abstractive <b>summarization.</b> We share details of insights generated from our experiments with a large repository of data gathered from subreddit for personal finance. We have also explored the use of <b>ChatGPT</b> and Vicuna for generating responses to queries and compared them with human responses. The methods proposed in this work are generic and applicable to all large social media platforms.</p></p class="citation"></blockquote><h3 id=22--257292-improving-link-prediction-accuracy-of-network-embedding-algorithms-via-rich-node-attribute-information-weiwei-gu-et-al-2024>(2/2 | 257/292) Improving link prediction accuracy of network embedding algorithms via rich node attribute information (Weiwei Gu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Weiwei Gu, Jinqiang Hou, Weiyi Gu. (2024)<br><strong>Improving link prediction accuracy of network embedding algorithms via rich node attribute information</strong><br><button class=copy-to-clipboard title="Improving link prediction accuracy of network embedding algorithms via rich node attribute information" index=257>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-257 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-SI, cs.SI<br>Keyword Score: 13<br>Keywords: Graph, Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04282v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04282v1.pdf filename=2403.04282v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Complex networks are widely used to represent an abundance of real-world relations ranging from social networks to brain networks. Inferring missing links or predicting future ones based on the currently observed network is known as the link prediction task.Recent network embedding based link prediction algorithms have demonstrated ground-breaking performance on link prediction accuracy. Those algorithms usually apply node attributes as the initial feature input to accelerate the convergence speed during the training process. However, they do not take full advantage of node feature information. In this paper,besides applying feature attributes as the initial input, we make better utilization of node attribute information by building attributable networks and plugging attributable networks into some typical link prediction algorithms and naming this algorithm Attributive <b>Graph</b> Enhanced Embedding (AGEE). AGEE is able to automatically learn the weighting trades-off between the structure and the attributive networks. Numerical experiments show that AGEE can improve the link prediction accuracy by around 3% compared with link prediction framework SEAL, Variational <b>Graph</b> <b>AutoEncoder</b> (VGAE), and Node2vec.</p></p class="citation"></blockquote><h2 id=physicsflu-dyn-1>physics.flu-dyn (1)</h2><h3 id=11--258292-jax-sph-a-differentiable-smoothed-particle-hydrodynamics-framework-artur-p-toshev-et-al-2024>(1/1 | 258/292) JAX-SPH: A Differentiable Smoothed Particle Hydrodynamics Framework (Artur P. Toshev et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Artur P. Toshev, Harish Ramachandran, Jonas A. Erbesdobler, Gianluca Galletti, Johannes Brandstetter, Nikolaus A. Adams. (2024)<br><strong>JAX-SPH: A Differentiable Smoothed Particle Hydrodynamics Framework</strong><br><button class=copy-to-clipboard title="JAX-SPH: A Differentiable Smoothed Particle Hydrodynamics Framework" index=258>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-258 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.flu-dyn<br>Categories: cs-LG, physics-flu-dyn, physics.flu-dyn<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04750v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04750v1.pdf filename=2403.04750v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Particle-based fluid <b>simulations</b> have emerged as a powerful tool for solving the Navier-Stokes equations, especially in cases that include intricate physics and free surfaces. The recent addition of machine learning methods to the toolbox for solving such problems is pushing the boundary of the quality vs. speed tradeoff of such numerical <b>simulations.</b> In this work, we lead the way to Lagrangian fluid simulators compatible with deep learning frameworks, and propose JAX-SPH - a Smoothed Particle Hydrodynamics (SPH) framework implemented in JAX. JAX-SPH builds on the code for dataset generation from the LagrangeBench project (Toshev et al., 2023) and extends this code in multiple ways: (a) integration of further key SPH algorithms, (b) restructuring the code toward a Python library, (c) verification of the gradients through the solver, and (d) demonstration of the utility of the gradients for solving inverse problems as well as a Solver-in-the-Loop application. Our code is available at <a href=https://github.com/tumaer/jax-sph>https://github.com/tumaer/jax-sph</a>.</p></p class="citation"></blockquote><h2 id=astro-phco-1>astro-ph.CO (1)</h2><h3 id=11--259292-testing-an-entropy-estimator-related-to-the-dynamical-state-of-galaxy-clusters-j-m-zúniga-et-al-2024>(1/1 | 259/292) Testing an entropy estimator related to the dynamical state of galaxy clusters (J. M. Zúniga et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>J. M. Zúniga, C. A. Caretta, A. P. González, E. García-Manzanárez. (2024)<br><strong>Testing an entropy estimator related to the dynamical state of galaxy clusters</strong><br><button class=copy-to-clipboard title="Testing an entropy estimator related to the dynamical state of galaxy clusters" index=259>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-259 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: astro-ph.CO<br>Categories: astro-ph-CO, astro-ph.CO, cs-IT, math-DS, math-IT, physics-data-an, stat-AP<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04723v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04723v1.pdf filename=2403.04723v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose the entropy estimator $H_Z$, calculated from global dynamical parameters, in an attempt to capture the degree of evolution of galaxy systems. We assume that the observed (spatial and velocity) distributions of member galaxies in these systems evolve over time towards states of higher dynamical relaxation (higher entropy), becoming more random and homogeneous in virial equilibrium. Thus, the $H_Z$-entropy should correspond to the gravitacional assembly state of the systems. This was tested in a sample of 70 well sampled clusters in the Local Universe whose gravitational assembly state, classified from optical and X-ray analysis of substructures, shows clear statistical correlation with $H_Z$. This estimator was also tested on a sample of clusters (halos) from the IllustrisTNG <b>simulations,</b> obtaining results in agreement with the observational ones.</p></p class="citation"></blockquote><h2 id=csit-5>cs.IT (5)</h2><h3 id=15--260292-molecular-arithmetic-coding-mac-for-internet-of-bio-nano-things-iobnt-melih-şahin-et-al-2024>(1/5 | 260/292) Molecular Arithmetic Coding (MAC) for Internet of Bio-Nano Things (IoBNT) (Melih Şahin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Melih Şahin, Beyza E. Ortlek, Ozgur B. Akan. (2024)<br><strong>Molecular Arithmetic Coding (MAC) for Internet of Bio-Nano Things (IoBNT)</strong><br><button class=copy-to-clipboard title="Molecular Arithmetic Coding (MAC) for Internet of Bio-Nano Things (IoBNT)" index=260>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-260 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04672v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04672v1.pdf filename=2403.04672v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Molecular Communication (MC) has emerged as a promising paradigm employing molecules to transfer information at the nano-scale. Unlike MC channel coding, MC source coding has remained mostly an unexplored area of research. In a recent paper, prefix source coding was introduced into the field, through an MC-adapted version of the Huffman Coding. In the context of MC source coding, this paper proposes the Molecular Arithmetic Coding (MAC) whose algorithmic implementation and code-structure is non-arbitrarily different than that of the widely-known classical arithmetic coding. MAC is designed to mitigate Inter-Symbol Interference (ISI) for alphabets with known symbol probabilities through, in a highly efficient way, avoiding consecutive 1-bits. However, due to bit precision limitations any arithmetic coding method faces, without any assumption made on the structure of the symbol alphabet, unique-decodability of MAC is not guaranteed. Accordingly, a uniquely-decodable new coding scheme named Molecular Arithmetic with Prefix Coding (MAPC) is also introduced. Across multiple alphabets, we show that MAPC provides a better compression performance compared to the optimal MC-adapted prefix coding. <b>Simulation</b> results of an exemplary alphabet demonstrates the superior symbol and word error rate performance of MAPC compared to the optimal MC-adapted prefix coding and to the uncoded BCSK schemes.</p></p class="citation"></blockquote><h3 id=25--261292-matched-filter-precoded-rate-splitting-multiple-access-a-simple-and-energy-efficient-design-hui-zhao-et-al-2024>(2/5 | 261/292) Matched-filter Precoded Rate Splitting Multiple Access: A Simple and Energy-efficient Design (Hui Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hui Zhao, Dirk Slock. (2024)<br><strong>Matched-filter Precoded Rate Splitting Multiple Access: A Simple and Energy-efficient Design</strong><br><button class=copy-to-clipboard title="Matched-filter Precoded Rate Splitting Multiple Access: A Simple and Energy-efficient Design" index=261>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-261 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04502v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04502v1.pdf filename=2403.04502v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce an energy-efficient downlink rate splitting multiple access (RSMA) scheme, employing a simple matched filter (MF) for precoding. We consider a transmitter equipped with multiple antennas, serving several single-antenna users at the same frequency-time resource, each with distinct message requests. Within the conventional 1-layer RSMA framework, requested messages undergo splitting into common and private streams, which are then precoded separately before transmission. In contrast, we propose a novel strategy where only an MF is employed to precode both the common and private streams in RSMA, promising significantly improved energy efficiency and reduced complexity. We demonstrate that this MF-precoded RSMA achieves the same delivery performance as conventional RSMA, where the common stream is beamformed using maximal ratio transmission (MRT) and the private streams are precoded by MF. Taking into account imperfect channel state information at the transmitter, we proceed to analyze the delivery performance of the MF-precoded RSMA. We derive the ergodic rates for decoding the common and private streams at a target user respectively in the massive MIMO regime. Finally, numerical <b>simulations</b> validate the accuracy of our analytical models, as well as demonstrate the advantages over conventional RSMA.</p></p class="citation"></blockquote><h3 id=35--262292-secure-mimo-communication-relying-on-movable-antennas-jun-tang-et-al-2024>(3/5 | 262/292) Secure MIMO Communication Relying on Movable Antennas (Jun Tang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jun Tang, Cunhua Pan, Yang Zhang, Hong Ren, Kezhi Wang. (2024)<br><strong>Secure MIMO Communication Relying on Movable Antennas</strong><br><button class=copy-to-clipboard title="Secure MIMO Communication Relying on Movable Antennas" index=262>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-262 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04269v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04269v1.pdf filename=2403.04269v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper considers a movable antenna (MA)-aided secure multiple-input multiple-output (MIMO) communication system consisting of a base station (BS), a legitimate information receiver (IR) and an eavesdropper (Eve), where the BS is equipped with MAs to enhance the system&rsquo;s physical layer security (PLS). Specifically, we aim to maximize the secrecy rate (SR) by jointly optimizing the transmit precoding (TPC) matrix, the artificial noise (AN) covariance matrix and the MAs&rsquo; positions under the constraints of the maximum transmit power and the minimum distance between MAs. To solve this non-convex problem with highly coupled optimization variables, the block coordinate descent (BCD) method is applied to alternately update the variables. Specifically, we first reformulate the SR into a tractable form by utilizing the minimum mean square error (MMSE) method, and derive the optimal TPC matrix and the AN covariance matrix with fixed MAs&rsquo; positions by applying the Lagrangian multiplier method in semi-closed forms. Then, the majorization-minimization (MM) algorithm is employed to iteratively optimize each MA&rsquo;s position while keeping others fixed. Finally, <b>simulation</b> results are provided to demonstrate the effectiveness of the proposed algorithms and the significant advantages of the MA-aided system over conventional fixed position antenna (FPA)-based system in enhancing system&rsquo;s security.</p></p class="citation"></blockquote><h3 id=45--263292-rectangular-rotational-invariant-estimator-for-high-rank-matrix-estimation-farzad-pourkamali-et-al-2024>(4/5 | 263/292) Rectangular Rotational Invariant Estimator for High-Rank Matrix Estimation (Farzad Pourkamali et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Farzad Pourkamali, Nicolas Macris. (2024)<br><strong>Rectangular Rotational Invariant Estimator for High-Rank Matrix Estimation</strong><br><button class=copy-to-clipboard title="Rectangular Rotational Invariant Estimator for High-Rank Matrix Estimation" index=263>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-263 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, math-IT<br>Keyword Score: 10<br>Keywords: Mutual Information<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04615v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04615v1.pdf filename=2403.04615v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider estimating a matrix from noisy observations coming from an arbitrary additive bi-rotational invariant perturbation. We propose an estimator which is optimal among the class of rectangular rotational invariant estimators and can be applied irrespective of the prior on the signal. For the particular case of Gaussian noise, we prove the optimality of the proposed estimator, and we find an explicit expression for the MMSE in terms of the limiting singular value distribution of the observation matrix. Moreover, we prove a formula linking the asymptotic <b>mutual</b> <b>information</b> and the limit of a log-spherical integral of rectangular matrices. We also provide numerical checks for our results for general bi-rotational invariant noise, as well as Gaussian noise, which match our theoretical predictions.</p></p class="citation"></blockquote><h3 id=55--264292-optimal-denial-of-service-attacks-against-status-updating-saad-kriouile-et-al-2024>(5/5 | 264/292) Optimal Denial-of-Service Attacks Against Status Updating (Saad Kriouile et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Saad Kriouile, Mohamad Assaad, Deniz Gündüz, Touraj Soleymani. (2024)<br><strong>Optimal Denial-of-Service Attacks Against Status Updating</strong><br><button class=copy-to-clipboard title="Optimal Denial-of-Service Attacks Against Status Updating" index=264>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-264 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, math-IT, math-OC<br>Keyword Score: 10<br>Keywords: Markov Decision Process<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04489v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04489v1.pdf filename=2403.04489v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we investigate denial-of-service attacks against status updating. The target system is modeled by a <b>Markov</b> <b>chain</b> <b>and</b> an unreliable wireless channel, and the performance of status updating in the target system is measured based on two metrics: age of information and age of incorrect information. Our objective is to devise optimal attack policies that strike a balance between the deterioration of the system&rsquo;s performance and the adversary&rsquo;s energy. We model the optimal problem as a <b>Markov</b> <b>decision</b> <b>process</b> and prove rigorously that the optimal jamming policy is a threshold-based policy under both metrics. In addition, we provide a low-complexity algorithm to obtain the optimal threshold value of the jamming policy. Our numerical results show that the networked system with the age-of-incorrect-information metric is less sensitive to jamming attacks than with the age-of-information metric. Index Terms-age of incorrect information, age of information, cyber-physical systems, status updating, remote monitoring.</p></p class="citation"></blockquote><h2 id=csar-2>cs.AR (2)</h2><h3 id=12--265292-virtuoso-an-open-source-comprehensive-and-modular-simulation-framework-for-virtual-memory-research-konstantinos-kanellopoulos-et-al-2024>(1/2 | 265/292) Virtuoso: An Open-Source, Comprehensive and Modular Simulation Framework for Virtual Memory Research (Konstantinos Kanellopoulos et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Konstantinos Kanellopoulos, Konstantinos Sgouras, Onur Mutlu. (2024)<br><strong>Virtuoso: An Open-Source, Comprehensive and Modular Simulation Framework for Virtual Memory Research</strong><br><button class=copy-to-clipboard title="Virtuoso: An Open-Source, Comprehensive and Modular Simulation Framework for Virtual Memory Research" index=265>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-265 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AR<br>Categories: cs-AR, cs-OS, cs.AR<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04635v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04635v1.pdf filename=2403.04635v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Virtual memory is a cornerstone of modern computing systems.Introduced as one of the earliest instances of hardware-software co-design, VM facilitates programmer-transparent memory man agement, data sharing, process isolation and memory protection. Evaluating the efficiency of various virtual memory (VM) designs is crucial (i) given their significant impact on the system, including the CPU caches, the main memory, and the storage device and (ii) given that different system architectures might benefit from various VM techniques. Such an evaluation is not straightforward, as it heavily hinges on modeling the interplay between different VM techniques and the interactions of VM with the system architecture. Modern simulators, however, struggle to keep up with the rapid VM research developments, lacking the capability to model a wide range of contemporary VM techniques and their interactions. To this end, we present Virtuoso, an open-source, comprehensive and modular <b>simulation</b> framework that models various VM designs to establish a common ground for virtual memory research. We demonstrate the versatility and the potential of Virtuoso with four new case studies. Virtuoso is freely open-source and can be found at <a href=https://github.com/CMU-SAFARI/Virtuoso>https://github.com/CMU-SAFARI/Virtuoso</a>.</p></p class="citation"></blockquote><h3 id=22--266292-a-methodology-to-automatically-optimize-dynamic-memory-managers-applying-grammatical-evolution-josé-l-risco-martín-et-al-2024>(2/2 | 266/292) A methodology to automatically optimize dynamic memory managers applying grammatical evolution (José L. Risco-Martín et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>José L. Risco-Martín, J. Manuel Colmenar, J. Ignacio Hidalgo, Juan Lanchares, Josefa Díaz. (2024)<br><strong>A methodology to automatically optimize dynamic memory managers applying grammatical evolution</strong><br><button class=copy-to-clipboard title="A methodology to automatically optimize dynamic memory managers applying grammatical evolution" index=266>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-266 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AR<br>Categories: cs-AR, cs.AR<br>Keyword Score: 10<br>Keywords: High-Resource<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04414v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04414v1.pdf filename=2403.04414v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Modern consumer devices must execute multimedia applications that exhibit high resource utilization. In order to efficiently execute these applications, the dynamic memory subsystem needs to be optimized. This complex task can be tackled in two complementary ways: optimizing the application source code or designing custom dynamic memory management mechanisms. Currently, the first approach has been well established, and several automatic methodologies have been proposed. Regarding the second approach, software engineers often write custom dynamic memory managers from scratch, which is a difficult and error-prone work. This paper presents a novel way to automatically generate custom dynamic memory managers optimizing both performance and memory usage of the target application. The design space is pruned using grammatical evolution converging to the best dynamic memory manager implementation for the target application. Our methodology achieves important improvements (62.55% and 30.62% better on average in performance and memory usage, respectively) when its results are compared to five different general-purpose dynamic memory managers.</p></p class="citation"></blockquote><h2 id=csds-6>cs.DS (6)</h2><h3 id=16--267292-optimizing-inventory-placement-for-a-downstream-online-matching-problem-boris-epstein-et-al-2024>(1/6 | 267/292) Optimizing Inventory Placement for a Downstream Online Matching Problem (Boris Epstein et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Boris Epstein, Will Ma. (2024)<br><strong>Optimizing Inventory Placement for a Downstream Online Matching Problem</strong><br><button class=copy-to-clipboard title="Optimizing Inventory Placement for a Downstream Online Matching Problem" index=267>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-267 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs.DS<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04598v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04598v1.pdf filename=2403.04598v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the inventory placement problem of splitting $Q$ units of a single item across warehouses, in advance of a downstream online matching problem that represents the dynamic fulfillment decisions of an e-commerce retailer. This is a challenging problem both in theory, because the downstream matching problem itself is computationally hard, and in practice, because the fulfillment team is constantly updating its algorithm and the placement team cannot directly evaluate how a placement decision would perform. We compare the performance of three placement procedures based on optimizing surrogate functions that have been studied and applied: Offline, Myopic, and Fluid placement. On the theory side, we show that optimizing inventory placement for the Offline surrogate leads to a $(1-(1-1/d)^d)/2$-approximation for the joint placement and fulfillment problem. We assume $d$ is an upper bound on how many warehouses can serve any demand location and that stochastic arrivals satisfy either temporal or spatial independence. The crux of our theoretical contribution is to use randomized rounding to derive a tight $(1-(1-1/d)^d)$-approximation for the integer programming problem of optimizing the Offline surrogate. We use statistical learning to show that rounding after optimizing a sample-average Offline surrogate, which is necessary due to the exponentially-sized support, does indeed have vanishing loss. On the experimental side, we extract real-world sequences of customer orders from publicly-available JD.com data and evaluate different combinations of placement and fulfillment procedures. Optimizing the Offline surrogate performs best overall, even compared to <b>simulation</b> procedures, corroborating our theory.</p></p class="citation"></blockquote><h3 id=26--268292-algorithms-and-complexity-for-path-covers-of-temporal-dags-when-is-dilworth-dynamic-dibyayan-chakraborty-et-al-2024>(2/6 | 268/292) Algorithms and complexity for path covers of temporal DAGs: when is Dilworth dynamic? (Dibyayan Chakraborty et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dibyayan Chakraborty, Antoine Dailly, Florent Foucaud, Ralf Klasing. (2024)<br><strong>Algorithms and complexity for path covers of temporal DAGs: when is Dilworth dynamic?</strong><br><button class=copy-to-clipboard title="Algorithms and complexity for path covers of temporal DAGs: when is Dilworth dynamic?" index=268>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-268 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DM, cs-DS, cs.DS, math-CO<br>Keyword Score: 13<br>Keywords: Graph, Discrete Time, Discrete Time<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04589v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04589v1.pdf filename=2403.04589v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we study a dynamic analogue of the Path Cover problem, which can be solved in polynomial-time in directed acyclic <b>graphs.</b> A temporal digraph has an arc set that changes over <b>discrete</b> <b>time-steps,</b> if the underlying digraph (the union of all the arc sets) is acyclic, then we have a temporal DAG. A temporal path is a directed path in the underlying digraph, such that the time-steps of arcs are strictly increasing along the path. Two temporal paths are temporally disjoint if they do not occupy any vertex at the same time. A temporal (resp. temporally disjoint) path cover is a collection of (resp. temporally disjoint) temporal paths that covers all vertices. In this paper, we study the computational complexities of the problems of finding a temporal (disjoint) path cover with minimum cardinality, denoted as Temporal Path Cover (TPC) and Temporally Disjoint Path Cover (TD-PC). We show that both problems are NP-hard even when the underlying DAG is planar, bipartite, subcubic, and there are only two arc-disjoint time-steps. Moreover, TD-PC remains NP-hard even on temporal oriented trees. In contrast, we show that TPC is polynomial-time solvable on temporal oriented trees by a reduction to Clique Cover for (static undirected) weakly chordal <b>graphs</b> (a subclass of perfect <b>graphs</b> for which Clique Cover admits an efficient algorithm). This highlights an interesting algorithmic difference between the two problems. Although it is NP-hard on temporal oriented trees, TD-PC becomes polynomial-time solvable on temporal oriented lines and temporal rooted directed trees. We also show that TPC (resp. TD-PC) admits an XP (resp. FPT) time algorithm with respect to parameter tmax + tw, where tmax is the maximum time-step, and tw is the treewidth of the underlying static undirected <b>graph.</b></p></p class="citation"></blockquote><h3 id=36--269292-np-completeness-for-the-space-optimality-of-double-array-tries-hideo-bannai-et-al-2024>(3/6 | 269/292) NP-Completeness for the Space-Optimality of Double-Array Tries (Hideo Bannai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hideo Bannai, Keisuke Goto, Shunsuke Kanda, Dominik Köppl. (2024)<br><strong>NP-Completeness for the Space-Optimality of Double-Array Tries</strong><br><button class=copy-to-clipboard title="NP-Completeness for the Space-Optimality of Double-Array Tries" index=269>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-269 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs.DS<br>Keyword Score: 10<br>Keywords: Information Retrieval<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04951v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04951v1.pdf filename=2403.04951v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Indexing a set of strings for prefix search or membership queries is a fundamental task with many applications such as <b>information</b> <b>retrieval</b> or database systems. A classic abstract data type for modelling such an index is a trie. Due to the fundamental nature of this problem, it has sparked much interest, leading to a variety of trie implementations with different characteristics. A trie implementation that has been well-used in practice is the double-array (trie) consisting of merely two integer arrays. While a traversal takes constant time per node visit, the needed space consumption in computer words can be as large as the product of the number of nodes and the alphabet size. Despite that several heuristics have been proposed on lowering the space requirements, we are unaware of any theoretical guarantees. In this paper, we study the decision problem whether there exists a double-array of a given size. To this end, we first draw a connection to the sparse matrix compression problem, which makes our problem NP-complete for alphabet sizes linear to the number of nodes. We further propose a reduction from the restricted directed Hamiltonian path problem, leading to NP-completeness even for logarithmic-sized alphabets.</p></p class="citation"></blockquote><h3 id=46--270292-time-aware-projections-truly-node-private-graph-statistics-under-continual-observation-palak-jain-et-al-2024>(4/6 | 270/292) Time-Aware Projections: Truly Node-Private Graph Statistics under Continual Observation (Palak Jain et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Palak Jain, Adam Smith, Connor Wagaman. (2024)<br><strong>Time-Aware Projections: Truly Node-Private Graph Statistics under Continual Observation</strong><br><button class=copy-to-clipboard title="Time-Aware Projections: Truly Node-Private Graph Statistics under Continual Observation" index=270>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-270 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-CR, cs-DS, cs.DS<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04630v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04630v1.pdf filename=2403.04630v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We describe the first algorithms that satisfy the standard notion of node-differential privacy in the continual release setting (i.e., without an assumed promise on input streams). Previous work addresses node-private continual release by assuming an unenforced promise on the maximum degree in a <b>graph;</b> indeed, the algorithms from these works exhibit blatant privacy violations when the degree bound is not met. Our algorithms are accurate on sparse <b>graphs,</b> for several fundamental <b>graph</b> problems: counting edges, triangles, other subgraphs, and connected components; and releasing degree histograms. Our unconditionally private algorithms generally have optimal error, up to polylogarithmic factors and lower-order terms. We provide general transformations that take a base algorithm for the continual release setting, which need only be private for streams satisfying a promised degree bound, and produce an algorithm that is unconditionally private yet mimics the base algorithm when the stream meets the degree bound (and adds only linear overhead to the time and space complexity of the base algorithm). To do so, we design new projection algorithms for <b>graph</b> streams, based on the batch-model techniques of Day et al. 2016 and Blocki et al. 2013, which modify the stream to limit its degree. Our main technical innovation is to show that the projections are stable &ndash; meaning that similar input <b>graphs</b> have similar projections &ndash; when the input stream satisfies a privately testable safety condition. Our transformation then follows a novel online variant of the Propose-Test-Release framework (Dwork and Lei, 2009), privately testing the safety condition before releasing output at each step.</p></p class="citation"></blockquote><h3 id=56--271292-a-simple-and-near-optimal-algorithm-for-directed-expander-decompositions-aurelio-l-sulser-et-al-2024>(5/6 | 271/292) A Simple and Near-Optimal Algorithm for Directed Expander Decompositions (Aurelio L. Sulser et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aurelio L. Sulser, Maximilian Probst Gutenberg. (2024)<br><strong>A Simple and Near-Optimal Algorithm for Directed Expander Decompositions</strong><br><button class=copy-to-clipboard title="A Simple and Near-Optimal Algorithm for Directed Expander Decompositions" index=271>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-271 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs.DS<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04542v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04542v1.pdf filename=2403.04542v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we present the first algorithm to compute expander decompositions in an $m$-edge directed <b>graph</b> with near-optimal time $\tilde{O}(m)$. Further, our algorithm can maintain such a decomposition in a dynamic <b>graph</b> and again obtains near-optimal update times. Our result improves over previous algorithms of Bernstein-Probst Gutenberg-Saranurak (FOCS 2020), Hua-Kyng-Probst Gutenberg-Wu (SODA 2023) that only obtained algorithms optimal up to subpolynomial factors. At the same time, our algorithm is much simpler and more accessible than previous work. In order to obtain our new algorithm, we present a new push-pull-relabel flow framework that generalizes the classic push-relabel flow algorithm of Goldberg-Tarjan (JACM 1988), which was later dynamized for computing expander decompositions in undirected <b>graphs</b> by Henzinger-Rao-Wang (SIAM J. Comput. 2020), Saranurak-Wang (SODA 2019). We then show that the flow problems formulated in recent work of Hua-Kyng-Probst Gutenberg-Wu (SODA 2023) to decompose directed <b>graphs</b> can be solved much more efficiently in the push-pull-relabel flow framework.</p></p class="citation"></blockquote><h3 id=66--272292-switching-classes-characterization-and-computation-dhanyamol-antony-et-al-2024>(6/6 | 272/292) Switching Classes: Characterization and Computation (Dhanyamol Antony et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dhanyamol Antony, Yixin Cao, Sagartanu Pal, R. B. Sandeep. (2024)<br><strong>Switching Classes: Characterization and Computation</strong><br><button class=copy-to-clipboard title="Switching Classes: Characterization and Computation" index=272>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-272 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs.DS, math-CO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04263v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04263v1.pdf filename=2403.04263v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In a <b>graph,</b> the switching operation reverses adjacencies between a subset of vertices and the others. For a hereditary <b>graph</b> class $\mathcal{G}$, we are concerned with the maximum subclass and the minimum superclass of $\mathcal{G}$ that are closed under switching. We characterize the maximum subclass for many important classes $\mathcal{G}$, and prove that it is finite when $\mathcal{G}$ is minor-closed and omits at least one <b>graph.</b> For several <b>graph</b> classes, we develop polynomial-time algorithms to recognize the minimum superclass. We also show that the recognition of the superclass is NP-complete for $H$-free <b>graphs</b> when $H$ is a sufficiently long path or cycle, and it cannot be solved in subexponential time assuming the Exponential Time Hypothesis.</p></p class="citation"></blockquote><h2 id=eesssp-1>eess.SP (1)</h2><h3 id=11--273292-comparison-of-gait-phase-detection-using-traditional-machine-learning-and-deep-learning-techniques-farhad-nazari-et-al-2024>(1/1 | 273/292) Comparison of gait phase detection using traditional machine learning and deep learning techniques (Farhad Nazari et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Farhad Nazari, Navid Mohajer, Darius Nahavandi, Abbas Khosravi. (2024)<br><strong>Comparison of gait phase detection using traditional machine learning and deep learning techniques</strong><br><button class=copy-to-clipboard title="Comparison of gait phase detection using traditional machine learning and deep learning techniques" index=273>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-273 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SP<br>Categories: cs-CV, cs-HC, cs-LG, eess-SP, eess.SP<br>Keyword Score: 20<br>Keywords: Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05595v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05595v1.pdf filename=2403.05595v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Human walking is a complex activity with a high level of cooperation and interaction between different systems in the body. Accurate detection of the phases of the gait in real-time is crucial to control lower-limb assistive devices like exoskeletons and prostheses. There are several ways to detect the walking gait phase, ranging from cameras and depth sensors to the sensors attached to the device itself or the human body. Electromyography (EMG) is one of the input methods that has captured lots of attention due to its precision and time delay between neuromuscular activity and muscle movement. This study proposes a few Machine Learning (ML) based models on lower-limb EMG data for human walking. The proposed models are based on Gaussian Naive Bayes (NB), Decision Tree (DT), Random Forest (RF), Linear Discriminant Analysis (LDA) and Deep <b>Convolutional</b> <b>Neural</b> <b>Networks</b> (DCNN). The traditional ML models are trained on hand-crafted features or their reduced components using Principal Component Analysis (PCA). On the contrary, the DCNN model utilises <b>convolutional</b> <b>layers</b> <b>to</b> extract features from raw data. The results show up to 75% average accuracy for traditional ML models and 79% for Deep Learning (DL) model. The highest achieved accuracy in 50 trials of the training DL model is 89.5%.</p></p class="citation"></blockquote><h2 id=mathoc-2>math.OC (2)</h2><h3 id=12--274292-locodl-communication-efficient-distributed-learning-with-local-training-and-compression-laurent-condat-et-al-2024>(1/2 | 274/292) LoCoDL: Communication-Efficient Distributed Learning with Local Training and Compression (Laurent Condat et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Laurent Condat, Artavazd Maranjyan, Peter Richtárik. (2024)<br><strong>LoCoDL: Communication-Efficient Distributed Learning with Local Training and Compression</strong><br><button class=copy-to-clipboard title="LoCoDL: Communication-Efficient Distributed Learning with Local Training and Compression" index=274>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-274 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-DC, cs-LG, math-OC, math.OC<br>Keyword Score: 20<br>Keywords: Federated Learning, Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04348v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04348v1.pdf filename=2403.04348v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In Distributed optimization and Learning, and even more in the modern framework of <b>federated</b> <b>learning,</b> communication, which is slow and costly, is critical. We introduce LoCoDL, a communication-efficient algorithm that leverages the two popular and effective techniques of Local training, which reduces the communication frequency, and Compression, in which short bitstreams are sent instead of full-dimensional vectors of floats. LoCoDL works with a large class of unbiased compressors that includes widely-used sparsification and <b>quantization</b> methods. LoCoDL provably benefits from local training and compression and enjoys a doubly-accelerated communication complexity, with respect to the condition number of the functions and the model dimension, in the general heterogenous regime with strongly convex functions. This is confirmed in practice, with LoCoDL outperforming existing algorithms.</p></p class="citation"></blockquote><h3 id=22--275292-memetic-differential-evolution-methods-for-semi-supervised-clustering-pierluigi-mansueto-et-al-2024>(2/2 | 275/292) Memetic Differential Evolution Methods for Semi-Supervised Clustering (Pierluigi Mansueto et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pierluigi Mansueto, Fabio Schoen. (2024)<br><strong>Memetic Differential Evolution Methods for Semi-Supervised Clustering</strong><br><button class=copy-to-clipboard title="Memetic Differential Evolution Methods for Semi-Supervised Clustering" index=275>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-275 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: 90C11, 90C30, 90C59, cs-LG, cs-NE, math-OC, math.OC<br>Keyword Score: 13<br>Keywords: Clustering, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04322v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04322v1.pdf filename=2403.04322v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we deal with semi-supervised Minimum Sum-of-Squares <b>Clustering</b> (MSSC) problems where background knowledge is given in the form of instance-level constraints. In particular, we take into account &ldquo;must-link&rdquo; and &ldquo;cannot-link&rdquo; constraints, each of which indicates if two dataset points should be associated to the same or to a different cluster. The presence of such constraints makes the problem at least as hard as its <b>unsupervised</b> version: it is no more true that each point is associated to its nearest cluster center, thus requiring some modifications in crucial operations, such as the assignment step. In this scenario, we propose a novel memetic strategy based on the Differential Evolution paradigm, directly extending a state-of-the-art framework recently proposed in the <b>unsupervised</b> <b>clustering</b> literature. As far as we know, our contribution represents the first attempt to define a memetic methodology designed to generate a (hopefully) optimal feasible solution for the semi-supervised MSSC problem. The proposal is compared with some state-of-the-art algorithms from the literature on a set of well-known datasets, highlighting its effectiveness and efficiency in finding good quality <b>clustering</b> solutions.</p></p class="citation"></blockquote><h2 id=csms-1>cs.MS (1)</h2><h3 id=11--276292-genml-a-python-library-to-generate-the-mittag-leffler-correlated-noise-xiang-qu-et-al-2024>(1/1 | 276/292) GenML: A Python Library to Generate the Mittag-Leffler Correlated Noise (Xiang Qu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiang Qu, Hui Zhao, Wenjie Cai, Gongyi Wang, Zihan Huang. (2024)<br><strong>GenML: A Python Library to Generate the Mittag-Leffler Correlated Noise</strong><br><button class=copy-to-clipboard title="GenML: A Python Library to Generate the Mittag-Leffler Correlated Noise" index=276>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-276 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MS<br>Categories: cond-mat-stat-mech, cs-MS, cs.MS, physics-comp-ph<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04273v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04273v1.pdf filename=2403.04273v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Mittag-Leffler correlated noise (M-L noise) plays a crucial role in the dynamics of complex systems, yet the scientific community has lacked tools for its direct generation. Addressing this gap, our work introduces GenML, a Python library specifically designed for generating M-L noise. We detail the architecture and functionalities of GenML and its underlying algorithmic approach, which enables the precise <b>simulation</b> of M-L noise. The effectiveness of GenML is validated through quantitative analyses of autocorrelation functions and diffusion behaviors, showcasing its capability to accurately replicate theoretical noise properties. Our contribution with GenML enables the effective application of M-L noise data in numerical <b>simulation</b> and data-driven methods for describing complex systems, moving beyond mere theoretical modeling.</p></p class="citation"></blockquote><h2 id=statml-3>stat.ML (3)</h2><h3 id=13--277292-efficient-cnn-lstm-based-parameter-estimation-of-levy-driven-stochastic-differential-equations-shuaiyu-li-et-al-2024>(1/3 | 277/292) Efficient CNN-LSTM based Parameter Estimation of Levy Driven Stochastic Differential Equations (Shuaiyu Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shuaiyu Li, Yang Ruan, Changzhou Long, Yuzhong Cheng. (2024)<br><strong>Efficient CNN-LSTM based Parameter Estimation of Levy Driven Stochastic Differential Equations</strong><br><button class=copy-to-clipboard title="Efficient CNN-LSTM based Parameter Estimation of Levy Driven Stochastic Differential Equations" index=277>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-277 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-AI, cs-LG, stat-ML, stat.ML<br>Keyword Score: 20<br>Keywords: Convolutional Neural Network, LSTM<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04246v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04246v1.pdf filename=2403.04246v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study addresses the challenges in parameter estimation of stochastic differential equations driven by non-Gaussian noises, which are critical in understanding dynamic phenomena such as price fluctuations and the spread of infectious diseases. Previous research highlighted the potential of <b>LSTM</b> networks in estimating parameters of alpha stable Levy driven SDEs but faced limitations including high time complexity and constraints of the <b>LSTM</b> chaining property. To mitigate these issues, we introduce the PEnet, a novel <b>CNN-LSTM-based</b> three-stage model that offers an end to end approach with superior accuracy and adaptability to varying data structures, enhanced inference speed for long sequence observations through initial data feature condensation by <b>CNN,</b> and high generalization capability, allowing its application to various complex SDE scenarios. Experiments on synthetic datasets confirm PEnet significant advantage in estimating SDE parameters associated with noise characteristics, establishing it as a competitive method for SDE parameter estimation in the presence of Levy noise.</p></p class="citation"></blockquote><h3 id=23--278292-signature-isolation-forest-guillaume-staerman-et-al-2024>(2/3 | 278/292) Signature Isolation Forest (Guillaume Staerman et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guillaume Staerman, Marta Campi, Gareth W. Peters. (2024)<br><strong>Signature Isolation Forest</strong><br><button class=copy-to-clipboard title="Signature Isolation Forest" index=278>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-278 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 13<br>Keywords: Anomaly Detection, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04405v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04405v1.pdf filename=2403.04405v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Functional Isolation Forest (FIF) is a recent state-of-the-art <b>Anomaly</b> <b>Detection</b> (AD) algorithm designed for functional data. It relies on a tree partition procedure where an abnormality score is computed by projecting each curve observation on a drawn dictionary through a linear inner product. Such linear inner product and the dictionary are a priori choices that highly influence the algorithm&rsquo;s performances and might lead to unreliable results, particularly with complex datasets. This work addresses these challenges by introducing \textit{Signature Isolation Forest}, a novel AD algorithm class leveraging the rough path theory&rsquo;s signature transform. Our objective is to remove the constraints imposed by FIF through the proposition of two algorithms which specifically target the linearity of the FIF inner product and the choice of the dictionary. We provide several numerical experiments, including a real-world applications <b>benchmark</b> showing the relevance of our methods.</p></p class="citation"></blockquote><h3 id=33--279292-fundamental-limits-of-non-linear-low-rank-matrix-estimation-pierre-mergny-et-al-2024>(3/3 | 279/292) Fundamental limits of Non-Linear Low-Rank Matrix Estimation (Pierre Mergny et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pierre Mergny, Justin Ko, Florent Krzakala, Lenka Zdeborová. (2024)<br><strong>Fundamental limits of Non-Linear Low-Rank Matrix Estimation</strong><br><button class=copy-to-clipboard title="Fundamental limits of Non-Linear Low-Rank Matrix Estimation" index=279>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-279 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 10<br>Keywords: Message-Passing<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04234v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04234v1.pdf filename=2403.04234v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider the task of estimating a low-rank matrix from non-linear and noisy observations. We prove a strong universality result showing that Bayes-optimal performances are characterized by an equivalent Gaussian model with an effective prior, whose parameters are entirely determined by an expansion of the non-linear function. In particular, we show that to reconstruct the signal accurately, one requires a signal-to-noise ratio growing as $N^{\frac 12 (1-1/k_F)}$, where $k_F$ is the first non-zero Fisher information coefficient of the function. We provide asymptotic characterization for the minimal achievable mean squared error (MMSE) and an approximate <b>message-passing</b> algorithm that reaches the MMSE under conditions analogous to the linear version of the problem. We also provide asymptotic errors achieved by methods such as principal component analysis combined with Bayesian denoising, and compare them with Bayes-optimal MMSE.</p></p class="citation"></blockquote><h2 id=physicsmed-ph-1>physics.med-ph (1)</h2><h3 id=11--280292-understanding-the-pulsar-effect-in-combined-radiotherapy-and-immunotherapy-through-attention-mechanisms-with-a-transformer-model-hao-peng-et-al-2024>(1/1 | 280/292) Understanding the PULSAR Effect in Combined Radiotherapy and Immunotherapy through Attention Mechanisms with a Transformer Model (Hao Peng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hao Peng, Casey Moore, Debabrata Saha, Steve Jiang, Robert Timmerman. (2024)<br><strong>Understanding the PULSAR Effect in Combined Radiotherapy and Immunotherapy through Attention Mechanisms with a Transformer Model</strong><br><button class=copy-to-clipboard title="Understanding the PULSAR Effect in Combined Radiotherapy and Immunotherapy through Attention Mechanisms with a Transformer Model" index=280>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-280 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.med-ph<br>Categories: cs-AI, physics-med-ph, physics.med-ph<br>Keyword Score: 20<br>Keywords: Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04175v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04175v1.pdf filename=2403.04175v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>PULSAR (personalized, ultra-fractionated stereotactic adaptive radiotherapy) is the adaptation of stereotactic ablative radiotherapy towards personalized cancer management. For the first time, we applied a <b>transformer-based</b> attention mechanism to investigate the underlying interactions between combined PULSAR and PD-L1 blockade immunotherapy based on a murine cancer model (Lewis Lung Carcinoma, LLC). The proposed approach is able to predict the trend of tumor volume change semi-quantitatively, and excels in identifying the potential causal relationships through both <b>self-attention</b> and cross-attention scores.</p></p class="citation"></blockquote><h2 id=csdl-1>cs.DL (1)</h2><h3 id=11--281292-brainknow----extracting-linking-and-associating-neuroscience-knowledge-cunqing-huangfu-et-al-2024>(1/1 | 281/292) BrainKnow &ndash; Extracting, Linking, and Associating Neuroscience Knowledge (Cunqing Huangfu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Cunqing Huangfu, Yi Zeng, Yuwei Wang, Dongsheng Wang, Zizhe Ruan. (2024)<br><strong>BrainKnow &ndash; Extracting, Linking, and Associating Neuroscience Knowledge</strong><br><button class=copy-to-clipboard title="BrainKnow -- Extracting, Linking, and Associating Neuroscience Knowledge" index=281>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-281 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DL<br>Categories: 92-04, J-3, cs-DL, cs.DL, q-bio-NC<br>Keyword Score: 18<br>Keywords: Graph, Knowledge Graph, Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04346v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04346v2.pdf filename=2403.04346v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The vast accumulation of neuroscience <b>knowledge</b> <b>presents</b> a challenge for researchers to timely and accurately locate the specific information they require. Constructing a <b>knowledge</b> <b>engine</b> that automatically extracts and organizes information from academic papers can provide researchers with timely and accurate informational services. We present the Brain <b>Knowledge</b> <b>Engine</b> (BrainKnow), which extracts and integrates neuroscience <b>knowledge</b> <b>from</b> published papers from PubMed. BrainKnow comprises a substantial repository, containing 3,626,931 relations spanning a broad spectrum of 37,011 neuroscience concepts extracted from 1817744 articles. The relations in BrainKnow can be accessed and navigated through a user-friendly web interface. Additionally, BrainKnow employs <b>graph</b> network algorithms for the <b>recommendation</b> and visualization of <b>knowledge.</b> <b>BrainKnow</b> is capable of automatic real-time updates. BrainKnow represents the first neuroscience <b>knowledge</b> <b>graph</b> that not only integrates <b>knowledge</b> <b>in-depth</b> but also facilitates fully automated updates.</p></p class="citation"></blockquote><h2 id=cspl-2>cs.PL (2)</h2><h3 id=12--282292-conjugate-operators-for-transparent-explorable-research-outputs-joseph-bond-et-al-2024>(1/2 | 282/292) Conjugate operators for transparent, explorable research outputs (Joseph Bond et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Joseph Bond, Cristina David, Minh Nguyen, Dominic Orchard, Roly Perera. (2024)<br><strong>Conjugate operators for transparent, explorable research outputs</strong><br><button class=copy-to-clipboard title="Conjugate operators for transparent, explorable research outputs" index=282>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-282 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.PL<br>Categories: cs-PL, cs.PL<br>Keyword Score: 13<br>Keywords: Graph, Fact Verification<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04403v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04403v2.pdf filename=2403.04403v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Charts, figures, and text derived from data play an important role in decision making, from data-driven policy development to day-to-day choices informed by online articles. Making sense of, or <b>fact-checking,</b> <b>outputs</b> means understanding how they relate to the underlying data. Even for domain experts with access to the source code and data sets, this poses a significant challenge. In this paper we introduce a new program analysis framework which supports interactive exploration of fine-grained I/O relationships directly through computed outputs, making use of dynamic dependence <b>graphs.</b> Our main contribution is a novel notion in data provenance which we call related inputs, a relation of mutual relevance or &ldquo;cognacy&rdquo; which arises between inputs when they contribute to common features of the output. Queries of this form allow readers to ask questions like &ldquo;What outputs use this data element, and what other data elements are used along with it?&rdquo;. We show how Jonsson and Tarski&rsquo;s concept of conjugate operators on Boolean algebras appropriately characterises the notion of cognacy in a dependence <b>graph,</b> and give a procedure for computing related inputs over such a <b>graph.</b></p></p class="citation"></blockquote><h3 id=22--283292-message-observing-sessions-ryan-kavanagh-et-al-2024>(2/2 | 283/292) Message-Observing Sessions (Ryan Kavanagh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ryan Kavanagh, Brigitte Pientka. (2024)<br><strong>Message-Observing Sessions</strong><br><button class=copy-to-clipboard title="Message-Observing Sessions" index=283>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-283 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.PL<br>Categories: D-3-3; F-3-2, cs-PL, cs.PL<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04633v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04633v1.pdf filename=2403.04633v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present Most, a process language with message-observing session types. Message-observing session types extend binary session types with type-level computation to specify communication protocols that vary based on messages observed on other channels. Hence, Most allows us to express global invariants about processes, rather than just local invariants, in a bottom-up, compositional way. We give Most a semantic foundation using traces with binding, a semantic approach for compositionally <b>reasoning</b> about traces in the presence of name generation. We use this semantics to prove type soundness and compositionality for Most processes. We see this as a significant step towards capturing message-dependencies and providing more precise guarantees about processes.</p></p class="citation"></blockquote><h2 id=q-biomn-1>q-bio.MN (1)</h2><h3 id=11--284292-cell-reprogramming-design-by-transfer-learning-of-functional-transcriptional-networks-thomas-p-wytock-et-al-2024>(1/1 | 284/292) Cell reprogramming design by transfer learning of functional transcriptional networks (Thomas P. Wytock et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Thomas P. Wytock, Adilson E. Motter. (2024)<br><strong>Cell reprogramming design by transfer learning of functional transcriptional networks</strong><br><button class=copy-to-clipboard title="Cell reprogramming design by transfer learning of functional transcriptional networks" index=284>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-284 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.MN<br>Categories: cond-mat-dis-nn, cs-LG, q-bio-GN, q-bio-MN, q-bio.MN<br>Keyword Score: 10<br>Keywords: Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04837v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04837v1.pdf filename=2403.04837v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent developments in synthetic biology, next-generation sequencing, and machine learning provide an unprecedented opportunity to rationally design new disease treatments based on measured responses to gene perturbations and drugs to reprogram cells. The main challenges to seizing this opportunity are the incomplete knowledge of the cellular network and the combinatorial explosion of possible interventions, both of which are insurmountable by experiments. To address these challenges, we develop a <b>transfer</b> <b>learning</b> approach to control cell behavior that is pre-trained on transcriptomic data associated with human cell fates, thereby generating a model of the network dynamics that can be transferred to specific reprogramming goals. The approach combines transcriptional responses to gene perturbations to minimize the difference between a given pair of initial and target transcriptional states. We demonstrate our approach&rsquo;s versatility by applying it to a microarray dataset comprising >9,000 microarrays across 54 cell types and 227 unique perturbations, and an RNASeq dataset consisting of >10,000 sequencing runs across 36 cell types and 138 perturbations. Our approach reproduces known reprogramming protocols with an AUROC of 0.91 while innovating over existing methods by pre-training an adaptable model that can be tailored to specific reprogramming transitions. We show that the number of gene perturbations required to steer from one fate to another increases with decreasing developmental relatedness and that fewer genes are needed to progress along developmental paths than to regress. These findings establish a proof-of-concept for our approach to computationally design control strategies and provide insights into how gene regulatory networks govern phenotype.</p></p class="citation"></blockquote><h2 id=cscy-2>cs.CY (2)</h2><h3 id=12--285292-literature-review-of-current-sustainability-assessment-frameworks-and-approaches-for-organizations-sarah-farahdel-et-al-2024>(1/2 | 285/292) Literature Review of Current Sustainability Assessment Frameworks and Approaches for Organizations (Sarah Farahdel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sarah Farahdel, Chun Wang, Anjali Awasthi. (2024)<br><strong>Literature Review of Current Sustainability Assessment Frameworks and Approaches for Organizations</strong><br><button class=copy-to-clipboard title="Literature Review of Current Sustainability Assessment Frameworks and Approaches for Organizations" index=285>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-285 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CY, cs.CY<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04717v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04717v1.pdf filename=2403.04717v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This systematic literature review explores sustainability assessment frameworks (SAFs) across diverse industries. The review focuses on SAF design approaches including the methods used for Sustainability Indicator (SI) selection, relative importance assessment, and interdependency analysis. Various methods, including literature reviews, stakeholder interviews, questionnaires, Pareto analysis, SMART approach, and adherence to sustainability standards, contribute to the complex SI selection process. Fuzzy-AHP stands out as a robust technique for assessing relative SI importance. While dynamic sustainability and performance indices are essential, methods like DEMATEL, VIKOR, correlation analysis, and causal models for interdependency assessment exhibit static limitations. The review presents strengths and limitations of SAFs, addressing gaps in design approaches and contributing to a comprehensive understanding. The insights of this review aim to benefit policymakers, administrators, leaders, and researchers, fostering sustainability practices. Future research <b>recommendations</b> include exploring multi-criteria decision-making models and hybrid approaches, extending sustainability evaluation across organizational levels and supply chains. Emphasizing adaptability to industry specifics and dynamic global adjustments is proposed for holistic sustainability practices, further enhancing organizational sustainability.</p></p class="citation"></blockquote><h3 id=22--286292-disciplining-deliberation-a-sociotechnical-perspective-on-machine-learning-trade-offs-sina-fazelpour-2024>(2/2 | 286/292) Disciplining deliberation: a sociotechnical perspective on machine learning trade-offs (Sina Fazelpour, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sina Fazelpour. (2024)<br><strong>Disciplining deliberation: a sociotechnical perspective on machine learning trade-offs</strong><br><button class=copy-to-clipboard title="Disciplining deliberation: a sociotechnical perspective on machine learning trade-offs" index=286>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-286 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CY, cs.CY<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04226v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04226v1.pdf filename=2403.04226v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper focuses on two highly publicized formal trade-offs in the field of responsible artificial intelligence (AI) &ndash; between predictive accuracy and <b>fairness</b> and between predictive accuracy and interpretability. These formal trade-offs are often taken by researchers, practitioners, and policy-makers to directly imply corresponding tensions between underlying values. Thus interpreted, the trade-offs have formed a core focus of normative engagement in AI governance, accompanied by a particular division of labor along disciplinary lines. This paper argues against this prevalent interpretation by drawing attention to three sets of considerations that are critical for bridging the gap between these formal trade-offs and their practical impacts on relevant values. I show how neglecting these considerations can distort our normative deliberations, and result in costly and misaligned interventions and justifications. Taken together, these considerations form a sociotechnical framework that could guide those involved in AI governance to assess how, in many cases, we can and should have higher aspirations than the prevalent interpretation of the trade-offs would suggest. I end by drawing out the normative opportunities and challenges that emerge out of these considerations, and highlighting the imperative of interdisciplinary collaboration in fostering responsible AI.</p></p class="citation"></blockquote><h2 id=csoh-1>cs.OH (1)</h2><h3 id=11--287292-new-algorithms-for-the-simplification-of-multiple-trajectories-under-bandwidth-constraints-gilles-dejaegere-et-al-2024>(1/1 | 287/292) New algorithms for the simplification of multiple trajectories under bandwidth constraints (Gilles Dejaegere et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gilles Dejaegere, Mahmoud Sakr. (2024)<br><strong>New algorithms for the simplification of multiple trajectories under bandwidth constraints</strong><br><button class=copy-to-clipboard title="New algorithms for the simplification of multiple trajectories under bandwidth constraints" index=287>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-287 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.OH<br>Categories: cs-OH, cs.OH<br>Keyword Score: 10<br>Keywords: In-context Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04821v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04821v1.pdf filename=2403.04821v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study introduces time-windowed variations of three established trajectory simplification algorithms. These new algorithms are specifically designed to be used in contexts with bandwidth limitations. We present the details of these algorithms and highlight the differences compared to their classical counterparts. To evaluate their performance, we conduct accuracy assessments for varying sizes of time windows, utilizing two different datasets and exploring different compression ratios. The accuracies of the proposed algorithms are compared with those of existing methods. Our findings demonstrate that, for larger time windows, the enhanced version of the bandwidth-constrained STTrace outperforms other algorithms, with the bandwidth-constrained improved version of \squish also yielding satisfactory results at a lower computational cost. Conversely, for short time windows, only the bandwidth-constrained version of Dead Reckoning remains satisfactory.</p></p class="citation"></blockquote><h2 id=csma-2>cs.MA (2)</h2><h3 id=12--288292-dynamics-of-moral-behavior-in-heterogeneous-populations-of-learning-agents-elizaveta-tennant-et-al-2024>(1/2 | 288/292) Dynamics of Moral Behavior in Heterogeneous Populations of Learning Agents (Elizaveta Tennant et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Elizaveta Tennant, Stephen Hailes, Mirco Musolesi. (2024)<br><strong>Dynamics of Moral Behavior in Heterogeneous Populations of Learning Agents</strong><br><button class=copy-to-clipboard title="Dynamics of Moral Behavior in Heterogeneous Populations of Learning Agents" index=288>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-288 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MA<br>Categories: cs-AI, cs-CY, cs-LG, cs-MA, cs.MA<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04202v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04202v2.pdf filename=2403.04202v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Growing concerns about safety and alignment of AI systems highlight the importance of embedding moral capabilities in artificial agents. A promising solution is the use of learning from experience, i.e., <b>Reinforcement</b> <b>Learning.</b> In multi-agent (social) environments, complex population-level phenomena may emerge from interactions between individual learning agents. Many of the existing studies rely on simulated social dilemma environments to study the interactions of independent learning agents. However, they tend to ignore the moral heterogeneity that is likely to be present in societies of agents in practice. For example, at different points in time a single learning agent may face opponents who are consequentialist (i.e., caring about maximizing some outcome over time) or norm-based (i.e., focusing on conforming to a specific norm here and now). The extent to which agents&rsquo; co-development may be impacted by such moral heterogeneity in populations is not well understood. In this paper, we present a study of the learning dynamics of morally heterogeneous populations interacting in a social dilemma setting. Using a Prisoner&rsquo;s Dilemma environment with a partner selection mechanism, we investigate the extent to which the prevalence of diverse moral agents in populations affects individual agents&rsquo; learning behaviors and emergent population-level outcomes. We observe several types of non-trivial interactions between pro-social and anti-social agents, and find that certain classes of moral agents are able to steer selfish agents towards more cooperative behavior.</p></p class="citation"></blockquote><h3 id=22--289292-distributed-multi-objective-optimization-in-cyber-physical-energy-systems-sanja-stark-et-al-2024>(2/2 | 289/292) Distributed Multi-objective Optimization in Cyber-Physical Energy Systems (Sanja Stark et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sanja Stark, Emilie Frost, Marvin Nebel-Wenner. (2024)<br><strong>Distributed Multi-objective Optimization in Cyber-Physical Energy Systems</strong><br><button class=copy-to-clipboard title="Distributed Multi-objective Optimization in Cyber-Physical Energy Systems" index=289>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-289 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MA<br>Categories: cs-MA, cs.MA<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04627v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04627v1.pdf filename=2403.04627v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Managing complex Cyber-Physical Energy Systems (CPES) requires solving various optimization problems with multiple objectives and constraints. As distributed control architectures are becoming more popular in CPES for certain tasks due to their flexibility, robustness, and privacy protection, multi-objective optimization must also be distributed. For this purpose, we present MO-COHDA, a fully distributed, agent-based algorithm, for solving multi-objective optimization problems of CPES. MO-COHDA allows an easy and flexible adaptation to different use cases and integration of custom functionality. To evaluate the effectiveness of MO-COHDA, we compare it to a central NSGA-2 algorithm using multi-objective <b>benchmark</b> functions from the ZDT problem suite. The results show that MO-COHDA can approximate the reference front of the <b>benchmark</b> problems well and is suitable for solving multi-objective optimization problems. In addition, an example use case of scheduling a group of generation units while optimizing three different objectives was evaluated to show how MO-COHDA can be easily applied to real-world optimization problems in CPES.</p></p class="citation"></blockquote><h2 id=cscg-1>cs.CG (1)</h2><h3 id=11--290292-a-clique-based-separator-for-intersection-graphs-of-geodesic-disks-in-mathbbr2-boris-aronov-et-al-2024>(1/1 | 290/292) A Clique-Based Separator for Intersection Graphs of Geodesic Disks in $\mathbb{R}^2$ (Boris Aronov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Boris Aronov, Mark de Berg, Leonidas Theocharous. (2024)<br><strong>A Clique-Based Separator for Intersection Graphs of Geodesic Disks in $\mathbb{R}^2$</strong><br><button class=copy-to-clipboard title="A Clique-Based Separator for Intersection Graphs of Geodesic Disks in $\mathbb{R}^2$" index=290>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-290 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CG<br>Categories: F-2-2, cs-CG, cs.CG<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04905v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04905v1.pdf filename=2403.04905v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Let $d$ be a (well-behaved) shortest-path metric defined on a path-connected subset of $\mathbb{R}^2$ and let $\mathcal{D}={D_1,\ldots,D_n}$ be a set of geodesic disks with respect to the metric $d$. We prove that $\mathcal{G}^{\times}(\mathcal{D})$, the intersection <b>graph</b> of the disks in $\mathcal{D}$, has a clique-based separator consisting of $O(n^{3/4+\varepsilon})$ cliques. This significantly extends the class of objects whose intersection <b>graphs</b> have small clique-based separators. Our clique-based separator yields an algorithm for $q$-COLORING that runs in time $2^{O(n^{3/4+\varepsilon})}$, assuming the boundaries of the disks $D_i$ can be computed in polynomial time. We also use our clique-based separator to obtain a simple, efficient, and almost exact distance oracle for intersection <b>graphs</b> of geodesic disks. Our distance oracle uses $O(n^{7/4+\varepsilon})$ storage and can report the hop distance between any two nodes in $\mathcal{G}^{\times}(\mathcal{D})$ in $O(n^{3/4+\varepsilon})$ time, up to an additive error of one. So far, distance oracles with an additive error of one that use subquadratic storage and sublinear query time were not known for such general <b>graph</b> classes.</p></p class="citation"></blockquote><h2 id=cscc-1>cs.CC (1)</h2><h3 id=11--291292-on-12-domination-in-interval-and-circle-graphs-mohsen-alambardar-meybodi-et-al-2024>(1/1 | 291/292) On $[1,2]$-Domination in Interval and Circle Graphs (Mohsen Alambardar Meybodi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohsen Alambardar Meybodi, Abolfazl Poureidi. (2024)<br><strong>On $[1,2]$-Domination in Interval and Circle Graphs</strong><br><button class=copy-to-clipboard title="On $[1,2]$-Domination in Interval and Circle Graphs" index=291>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-291 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CC<br>Categories: cs-CC, cs-DS, cs.CC, math-CO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04694v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04694v1.pdf filename=2403.04694v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A subset $S$ of vertices in a <b>graph</b> $G=(V, E)$ is Dominating Set if each vertex in $V(G)\setminus S$ is adjacent to at least one vertex in $S$. Chellali et al. in 2013, by restricting the number of neighbors in $S$ of a vertex outside $S$, introduced the concept of $[1,j]$-dominating set. A set $D \subseteq V$ of a <b>graph</b> $G = (V, E)$ is called $[1,j]$-Dominating Set of $G$ if every vertex not in $D$ has at least one neighbor and at most $j$ neighbors in $D$. The Minimum $[1,j]$-Domination problem is the problem of finding the minimum set $D$. Given a positive integer $k$ and a <b>graph</b> $G = (V, E)$, the $[1,j]$-Domination Decision problem is to decide whether $G$ has $[1,j]$-dominating set of cardinality at most $k$. A polynomial-time algorithm was obtained in split <b>graphs</b> for a constant $j$ in contrast to the classic Dominating Set problem which is NP-hard in split <b>graphs.</b> This result motivates us to investigate the effect of restriction $j$ on the complexity of $[1,j]$-domination problem on various classes of <b>graphs.</b> Although for $j\geq 3$, it has been proved that the minimum of classical domination is equal to minimum $[1,j]$-domination in interval <b>graphs,</b> the complexity of finding the minimum $[1,2]$-domination in interval <b>graphs</b> is still outstanding. In this paper, we propose a polynomial-time algorithm for computing a minimum $[1,2]$ on non-proper interval <b>graphs</b> by a dynamic programming technique. Next, on the negative side, we show that the minimum $[1,2]$-dominating set problem on circle <b>graphs</b> is $NP$-complete.</p></p class="citation"></blockquote><h2 id=q-biobm-1>q-bio.BM (1)</h2><h3 id=11--292292-sgnet-folding-symmetrical-protein-complex-with-deep-learning-zhaoqun-li-et-al-2024>(1/1 | 292/292) SGNet: Folding Symmetrical Protein Complex with Deep Learning (Zhaoqun Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhaoqun Li, Jingcheng Yu, Qiwei Ye. (2024)<br><strong>SGNet: Folding Symmetrical Protein Complex with Deep Learning</strong><br><button class=copy-to-clipboard title="SGNet: Folding Symmetrical Protein Complex with Deep Learning" index=292>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-292 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.BM<br>Categories: cs-CL, q-bio-BM, q-bio.BM<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04395v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04395v1.pdf filename=2403.04395v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep learning has made significant progress in protein structure prediction, advancing the development of computational biology. However, despite the high accuracy achieved in predicting single-chain structures, a significant number of large homo-oligomeric assemblies exhibit internal symmetry, posing a major challenge in structure determination. The performances of existing deep learning methods are limited since the symmetrical protein assembly usually has a long sequence, making structural computation infeasible. In addition, multiple identical subunits in symmetrical protein complex cause the issue of supervision ambiguity in label assignment, requiring a consistent structure modeling for the training. To tackle these problems, we propose a protein folding framework called SGNet to model protein-protein interactions in symmetrical assemblies. SGNet conducts feature extraction on a single subunit and generates the whole assembly using our proposed symmetry module, which largely mitigates computational problems caused by sequence length. Thanks to the elaborate design of modeling symmetry consistently, we can model all global symmetry types in quaternary protein structure prediction. Extensive experimental results on a <b>benchmark</b> of symmetrical protein complexes further demonstrate the effectiveness of our method.</p></p class="citation"></blockquote></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/ title="arXiv @ 2024.03.08" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Prev</div><div class=next-prev-text>arXiv @ 2024.03.08</div></a></div><div class="col-md-6 next-article"><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/ title="arXiv @ 2024.03.10" class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>arXiv @ 2024.03.10</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#primary-categories>Primary Categories</a></li><li><a href=#keywords>Keywords</a></li><li><a href=#cscl-39>cs.CL (39)</a><ul><li><a href=#139--1292-halueval-wild-evaluating-hallucinations-of-language-models-in-the-wild-zhiying-zhu-et-al-2024>(1/39 | 1/292) HaluEval-Wild: Evaluating Hallucinations of Language Models in the Wild (Zhiying Zhu et al., 2024)</a></li><li><a href=#239--2292-yi-open-foundation-models-by-01ai-01-ai-et-al-2024>(2/39 | 2/292) Yi: Open Foundation Models by 01.AI (01. AI et al., 2024)</a></li><li><a href=#339--3292-llms-in-the-imaginarium-tool-learning-through-simulated-trial-and-error-boshi-wang-et-al-2024>(3/39 | 3/292) LLMs in the Imaginarium: Tool Learning through Simulated Trial and Error (Boshi Wang et al., 2024)</a></li><li><a href=#439--4292-telecom-language-models-must-they-be-large-nicola-piovesan-et-al-2024>(4/39 | 4/292) Telecom Language Models: Must They Be Large? (Nicola Piovesan et al., 2024)</a></li><li><a href=#539--5292-where-does-in-context-translation-happen-in-large-language-models-suzanna-sia-et-al-2024>(5/39 | 5/292) Where does In-context Translation Happen in Large Language Models (Suzanna Sia et al., 2024)</a></li><li><a href=#639--6292-few-shot-chain-of-thought-driven-reasoning-to-prompt-llms-for-open-ended-medical-question-answering-ojas-gramopadhye-et-al-2024>(6/39 | 6/292) Few shot chain-of-thought driven reasoning to prompt LLMs for open ended medical question answering (Ojas Gramopadhye et al., 2024)</a></li><li><a href=#739--7292-low-resource-court-judgment-summarization-for-common-law-systems-shuaiqi-liu-et-al-2024>(7/39 | 7/292) Low-Resource Court Judgment Summarization for Common Law Systems (Shuaiqi Liu et al., 2024)</a></li><li><a href=#839--8292-deep-icl-definition-enriched-experts-for-language-model-in-context-learning-xingwei-qu-et-al-2024>(8/39 | 8/292) DEEP-ICL: Definition-Enriched Experts for Language Model In-Context Learning (Xingwei Qu et al., 2024)</a></li><li><a href=#939--9292-proxy-rlhf-decoupling-generation-and-alignment-in-large-language-model-with-proxy-yu-zhu-et-al-2024>(9/39 | 9/292) Proxy-RLHF: Decoupling Generation and Alignment in Large Language Model with Proxy (Yu Zhu et al., 2024)</a></li><li><a href=#1039--10292-uncertainty-aware-relational-graph-neural-network-for-few-shot-knowledge-graph-completion-qian-li-et-al-2024>(10/39 | 10/292) Uncertainty-Aware Relational Graph Neural Network for Few-Shot Knowledge Graph Completion (Qian Li et al., 2024)</a></li><li><a href=#1139--11292-pearl-a-review-driven-persona-knowledge-grounded-conversational-recommendation-dataset-minjin-kim-et-al-2024>(11/39 | 11/292) Pearl: A Review-driven Persona-Knowledge Grounded Conversational Recommendation Dataset (Minjin Kim et al., 2024)</a></li><li><a href=#1239--12292-nlpre-a-revised-approach-towards-language-centric-benchmarking-of-natural-language-preprocessing-systems-martyna-wiącek-et-al-2024>(12/39 | 12/292) NLPre: a revised approach towards language-centric benchmarking of Natural Language Preprocessing systems (Martyna Wiącek et al., 2024)</a></li><li><a href=#1339--13292-automating-the-information-extraction-from-semi-structured-interview-transcripts-angelina-parfenova-2024>(13/39 | 13/292) Automating the Information Extraction from Semi-Structured Interview Transcripts (Angelina Parfenova, 2024)</a></li><li><a href=#1439--14292-fact-checking-the-output-of-large-language-models-via-token-level-uncertainty-quantification-ekaterina-fadeeva-et-al-2024>(14/39 | 14/292) Fact-Checking the Output of Large Language Models via Token-Level Uncertainty Quantification (Ekaterina Fadeeva et al., 2024)</a></li><li><a href=#1539--15292-qaq-quality-adaptive-quantization-for-llm-kv-cache-shichen-dong-et-al-2024>(15/39 | 15/292) QAQ: Quality Adaptive Quantization for LLM KV Cache (Shichen Dong et al., 2024)</a></li><li><a href=#1639--16292-ultrawiki-ultra-fine-grained-entity-set-expansion-with-negative-seed-entities-yangning-li-et-al-2024>(16/39 | 16/292) UltraWiki: Ultra-fine-grained Entity Set Expansion with Negative Seed Entities (Yangning Li et al., 2024)</a></li><li><a href=#1739--17292-attempt-towards-stress-transfer-in-speech-to-speech-machine-translation-sai-akarsh-et-al-2024>(17/39 | 17/292) Attempt Towards Stress Transfer in Speech-to-Speech Machine Translation (Sai Akarsh et al., 2024)</a></li><li><a href=#1839--18292-electrocardiogram-instruction-tuning-for-report-generation-zhongwei-wan-et-al-2024>(18/39 | 18/292) Electrocardiogram Instruction Tuning for Report Generation (Zhongwei Wan et al., 2024)</a></li><li><a href=#1939--19292-can-your-model-tell-a-negation-from-an-implicature-unravelling-challenges-with-intent-encoders-yuwei-zhang-et-al-2024>(19/39 | 19/292) Can Your Model Tell a Negation from an Implicature? Unravelling Challenges With Intent Encoders (Yuwei Zhang et al., 2024)</a></li><li><a href=#2039--20292-constitutionalexperts-training-a-mixture-of-principle-based-prompts-savvas-petridis-et-al-2024>(20/39 | 20/292) ConstitutionalExperts: Training a Mixture of Principle-based Prompts (Savvas Petridis et al., 2024)</a></li><li><a href=#2139--21292-exploring-continual-learning-of-compositional-generalization-in-nli-xiyan-fu-et-al-2024>(21/39 | 21/292) Exploring Continual Learning of Compositional Generalization in NLI (Xiyan Fu et al., 2024)</a></li><li><a href=#2239--22292-evaluation-of-llms-on-syntax-aware-code-fill-in-the-middle-tasks-linyuan-gong-et-al-2024>(22/39 | 22/292) Evaluation of LLMs on Syntax-Aware Code Fill-in-the-Middle Tasks (Linyuan Gong et al., 2024)</a></li><li><a href=#2339--23292-self-evaluation-of-large-language-model-based-on-glass-box-features-hui-huang-et-al-2024>(23/39 | 23/292) Self-Evaluation of Large Language Model based on Glass-box Features (Hui Huang et al., 2024)</a></li><li><a href=#2439--24292-large-language-models-are-in-context-molecule-learners-jiatong-li-et-al-2024>(24/39 | 24/292) Large Language Models are In-Context Molecule Learners (Jiatong Li et al., 2024)</a></li><li><a href=#2539--25292-code-mixed-probes-show-how-pre-trained-models-generalise-on-code-switched-text-frances-a-laureano-de-leon-et-al-2024>(25/39 | 25/292) Code-Mixed Probes Show How Pre-Trained Models Generalise On Code-Switched Text (Frances A. Laureano De Leon et al., 2024)</a></li><li><a href=#2639--26292-acceleron-a-tool-to-accelerate-research-ideation-harshit-nigam-et-al-2024>(26/39 | 26/292) Acceleron: A Tool to Accelerate Research Ideation (Harshit Nigam et al., 2024)</a></li><li><a href=#2739--27292-aligners-decoupling-llms-and-alignment-lilian-ngweta-et-al-2024>(27/39 | 27/292) Aligners: Decoupling LLMs and Alignment (Lilian Ngweta et al., 2024)</a></li><li><a href=#2839--28292-common-7b-language-models-already-possess-strong-math-capabilities-chen-li-et-al-2024>(28/39 | 28/292) Common 7B Language Models Already Possess Strong Math Capabilities (Chen Li et al., 2024)</a></li><li><a href=#2939--29292-chain-of-thought-explanation-for-dialogue-state-tracking-lin-xu-et-al-2024>(29/39 | 29/292) Chain of Thought Explanation for Dialogue State Tracking (Lin Xu et al., 2024)</a></li><li><a href=#3039--30292-do-large-language-model-understand-multi-intent-spoken-language--shangjian-yin-et-al-2024>(30/39 | 30/292) Do Large Language Model Understand Multi-Intent Spoken Language ? (Shangjian Yin et al., 2024)</a></li><li><a href=#3139--31292-metric-aware-llm-inference-michal-lukasik-et-al-2024>(31/39 | 31/292) Metric-aware LLM inference (Michal Lukasik et al., 2024)</a></li><li><a href=#3239--32292-evaluating-biases-in-context-dependent-health-questions-sharon-levy-et-al-2024>(32/39 | 32/292) Evaluating Biases in Context-Dependent Health Questions (Sharon Levy et al., 2024)</a></li><li><a href=#3339--33292-classist-tools-social-class-correlates-with-performance-in-nlp-amanda-cercas-curry-et-al-2024>(33/39 | 33/292) Classist Tools: Social Class Correlates with Performance in NLP (Amanda Cercas Curry et al., 2024)</a></li><li><a href=#3439--34292-promising-and-worth-to-try-future-directions-for-advancing-state-of-the-art-surrogates-methods-of-agent-based-models-in-social-and-health-computational-sciences-atiyah-elsheikh-2024>(34/39 | 34/292) Promising and worth-to-try future directions for advancing state-of-the-art surrogates methods of agent-based models in social and health computational sciences (Atiyah Elsheikh, 2024)</a></li><li><a href=#3539--35292-measuring-meaning-composition-in-the-human-brain-with-composition-scores-from-large-language-models-changjiang-gao-et-al-2024>(35/39 | 35/292) Measuring Meaning Composition in the Human Brain with Composition Scores from Large Language Models (Changjiang Gao et al., 2024)</a></li><li><a href=#3639--36292-macms-magahi-code-mixed-dataset-for-sentiment-analysis-priya-rani-et-al-2024>(36/39 | 36/292) MaCmS: Magahi Code-mixed Dataset for Sentiment Analysis (Priya Rani et al., 2024)</a></li><li><a href=#3739--37292-computational-modelling-of-plurality-and-definiteness-in-chinese-noun-phrases-yuqi-liu-et-al-2024>(37/39 | 37/292) Computational Modelling of Plurality and Definiteness in Chinese Noun Phrases (Yuqi Liu et al., 2024)</a></li><li><a href=#3839--38292-persona-extraction-through-semantic-similarity-for-emotional-support-conversation-generation-seunghee-han-et-al-2024>(38/39 | 38/292) Persona Extraction Through Semantic Similarity for Emotional Support Conversation Generation (Seunghee Han et al., 2024)</a></li><li><a href=#3939--39292-da-net-a-disentangled-and-adaptive-network-for-multi-source-cross-lingual-transfer-learning-ling-ge-et-al-2024>(39/39 | 39/292) DA-Net: A Disentangled and Adaptive Network for Multi-Source Cross-Lingual Transfer Learning (Ling Ge et al., 2024)</a></li></ul></li><li><a href=#cscv-58>cs.CV (58)</a><ul><li><a href=#158--40292-masked-capsule-autoencoders-miles-everett-et-al-2024>(1/58 | 40/292) Masked Capsule Autoencoders (Miles Everett et al., 2024)</a></li><li><a href=#258--41292-acc-vit--atrous-convolutions-comeback-in-vision-transformers-nabil-ibtehaz-et-al-2024>(2/58 | 41/292) ACC-ViT : Atrous Convolution&rsquo;s Comeback in Vision Transformers (Nabil Ibtehaz et al., 2024)</a></li><li><a href=#358--42292-effectiveness-assessment-of-recent-large-vision-language-models-yao-jiang-et-al-2024>(3/58 | 42/292) Effectiveness Assessment of Recent Large Vision-Language Models (Yao Jiang et al., 2024)</a></li><li><a href=#458--43292-textmonkey-an-ocr-free-large-multimodal-model-for-understanding-document-yuliang-liu-et-al-2024>(4/58 | 43/292) TextMonkey: An OCR-Free Large Multimodal Model for Understanding Document (Yuliang Liu et al., 2024)</a></li><li><a href=#558--44292-t-tame-trainable-attention-mechanism-for-explaining-convolutional-networks-and-vision-transformers-mariano-v-ntrougkas-et-al-2024>(5/58 | 44/292) T-TAME: Trainable Attention Mechanism for Explaining Convolutional Networks and Vision Transformers (Mariano V. Ntrougkas et al., 2024)</a></li><li><a href=#658--45292-discriminative-probing-and-tuning-for-text-to-image-generation-leigang-qu-et-al-2024>(6/58 | 45/292) Discriminative Probing and Tuning for Text-to-Image Generation (Leigang Qu et al., 2024)</a></li><li><a href=#758--46292-snapntell-enhancing-entity-centric-visual-question-answering-with-retrieval-augmented-multimodal-llm-jielin-qiu-et-al-2024>(7/58 | 46/292) SnapNTell: Enhancing Entity-Centric Visual Question Answering with Retrieval Augmented Multimodal LLM (Jielin Qiu et al., 2024)</a></li><li><a href=#858--47292-objectcompose-evaluating-resilience-of-vision-based-models-on-object-to-background-compositional-changes-hashmat-shadab-malik-et-al-2024>(8/58 | 47/292) ObjectCompose: Evaluating Resilience of Vision-Based Models on Object-to-Background Compositional Changes (Hashmat Shadab Malik et al., 2024)</a></li><li><a href=#958--48292-afreeca-annotation-free-counting-for-all-adriano-dalessandro-et-al-2024>(9/58 | 48/292) AFreeCA: Annotation-Free Counting for All (Adriano D&rsquo;Alessandro et al., 2024)</a></li><li><a href=#1058--49292-self-adapting-large-visual-language-models-to-edge-devices-across-visual-modalities-kaiwen-cai-et-al-2024>(10/58 | 49/292) Self-Adapting Large Visual-Language Models to Edge Devices across Visual Modalities (Kaiwen Cai et al., 2024)</a></li><li><a href=#1158--50292-optimizing-retinal-prosthetic-stimuli-with-conditional-invertible-neural-networks-yuli-wu-et-al-2024>(11/58 | 50/292) Optimizing Retinal Prosthetic Stimuli with Conditional Invertible Neural Networks (Yuli Wu et al., 2024)</a></li><li><a href=#1258--51292-scalable-and-robust-transformer-decoders-for-interpretable-image-classification-with-foundation-models-evelyn-mannix-et-al-2024>(12/58 | 51/292) Scalable and Robust Transformer Decoders for Interpretable Image Classification with Foundation Models (Evelyn Mannix et al., 2024)</a></li><li><a href=#1358--52292-auformer-vision-transformers-are-parameter-efficient-facial-action-unit-detectors-kaishen-yuan-et-al-2024>(13/58 | 52/292) AUFormer: Vision Transformers are Parameter-Efficient Facial Action Unit Detectors (Kaishen Yuan et al., 2024)</a></li><li><a href=#1458--53292-pixart-σ-weak-to-strong-training-of-diffusion-transformer-for-4k-text-to-image-generation-junsong-chen-et-al-2024>(14/58 | 53/292) PixArt-Σ: Weak-to-Strong Training of Diffusion Transformer for 4K Text-to-Image Generation (Junsong Chen et al., 2024)</a></li><li><a href=#1558--54292-pix2gif-motion-guided-diffusion-for-gif-generation-hitesh-kandala-et-al-2024>(15/58 | 54/292) Pix2Gif: Motion-Guided Diffusion for GIF Generation (Hitesh Kandala et al., 2024)</a></li><li><a href=#1658--55292-an-explainable-ai-framework-for-artificial-intelligence-of-medical-things-al-amin-et-al-2024>(16/58 | 55/292) An Explainable AI Framework for Artificial Intelligence of Medical Things (Al Amin et al., 2024)</a></li><li><a href=#1758--56292-a-data-centric-approach-to-class-specific-bias-in-image-data-augmentation-athanasios-angelakis-et-al-2024>(17/58 | 56/292) A data-centric approach to class-specific bias in image data augmentation (Athanasios Angelakis et al., 2024)</a></li><li><a href=#1858--57292-cat-enhancing-multimodal-large-language-model-to-answer-questions-in-dynamic-audio-visual-scenarios-qilang-ye-et-al-2024>(18/58 | 57/292) CAT: Enhancing Multimodal Large Language Model to Answer Questions in Dynamic Audio-Visual Scenarios (Qilang Ye et al., 2024)</a></li><li><a href=#1958--58292-discriminative-sample-guided-and-parameter-efficient-feature-space-adaptation-for-cross-domain-few-shot-learning-rashindrie-perera-et-al-2024>(19/58 | 58/292) Discriminative Sample-Guided and Parameter-Efficient Feature Space Adaptation for Cross-Domain Few-Shot Learning (Rashindrie Perera et al., 2024)</a></li><li><a href=#2058--59292-3dtexturetransformer-geometry-aware-texture-generation-for-arbitrary-mesh-topology-dharma-kc-et-al-2024>(20/58 | 59/292) 3DTextureTransformer: Geometry Aware Texture Generation for Arbitrary Mesh Topology (Dharma KC et al., 2024)</a></li><li><a href=#2158--60292-textr2-bench-benchmarking-the-robustness-of-referring-perception-models-under-perturbations-xiang-li-et-al-2024>(21/58 | 60/292) $\text{R}^2$-Bench: Benchmarking the Robustness of Referring Perception Models under Perturbations (Xiang Li et al., 2024)</a></li><li><a href=#2258--61292-fooling-neural-networks-for-motion-forecasting-via-adversarial-attacks-edgar-medina-et-al-2024>(22/58 | 61/292) Fooling Neural Networks for Motion Forecasting via Adversarial Attacks (Edgar Medina et al., 2024)</a></li><li><a href=#2358--62292-an-item-is-worth-a-prompt-versatile-image-editing-with-disentangled-control-aosong-feng-et-al-2024>(23/58 | 62/292) An Item is Worth a Prompt: Versatile Image Editing with Disentangled Control (Aosong Feng et al., 2024)</a></li><li><a href=#2458--63292-unitable-towards-a-unified-framework-for-table-structure-recognition-via-self-supervised-pretraining-shengyun-peng-et-al-2024>(24/58 | 63/292) UniTable: Towards a Unified Framework for Table Structure Recognition via Self-Supervised Pretraining (ShengYun Peng et al., 2024)</a></li><li><a href=#2558--64292-controllable-generation-with-text-to-image-diffusion-models-a-survey-pu-cao-et-al-2024>(25/58 | 64/292) Controllable Generation with Text-to-Image Diffusion Models: A Survey (Pu Cao et al., 2024)</a></li><li><a href=#2658--65292-promise-promptable-medical-image-segmentation-using-sam-jinfeng-wang-et-al-2024>(26/58 | 65/292) ProMISe: Promptable Medical Image Segmentation using SAM (Jinfeng Wang et al., 2024)</a></li><li><a href=#2758--66292-dual-path-frequency-discriminators-for-few-shot-anomaly-detection-yuhu-bai-et-al-2024>(27/58 | 66/292) Dual-path Frequency Discriminators for Few-shot Anomaly Detection (Yuhu Bai et al., 2024)</a></li><li><a href=#2858--67292-towards-learning-based-planningthe-nuplan-benchmark-for-real-world-autonomous-driving-napat-karnchanachari-et-al-2024>(28/58 | 67/292) Towards learning-based planning:The nuPlan benchmark for real-world autonomous driving (Napat Karnchanachari et al., 2024)</a></li><li><a href=#2958--68292-divide-and-conquer-high-resolution-industrial-anomaly-detection-via-memory-efficient-tiled-ensemble-blaž-rolih-et-al-2024>(29/58 | 68/292) Divide and Conquer: High-Resolution Industrial Anomaly Detection via Memory Efficient Tiled Ensemble (Blaž Rolih et al., 2024)</a></li><li><a href=#3058--69292-delving-into-the-trajectory-long-tail-distribution-for-muti-object-tracking-sijia-chen-et-al-2024>(30/58 | 69/292) Delving into the Trajectory Long-tail Distribution for Muti-object Tracking (Sijia Chen et al., 2024)</a></li><li><a href=#3158--70292-source-matters-source-dataset-impact-on-model-robustness-in-medical-imaging-dovile-juodelyte-et-al-2024>(31/58 | 70/292) Source Matters: Source Dataset Impact on Model Robustness in Medical Imaging (Dovile Juodelyte et al., 2024)</a></li><li><a href=#3258--71292-disentangled-diffusion-based-3d-human-pose-estimation-with-hierarchical-spatial-and-temporal-denoiser-qingyuan-cai-et-al-2024>(32/58 | 71/292) Disentangled Diffusion-Based 3D Human Pose Estimation with Hierarchical Spatial and Temporal Denoiser (Qingyuan Cai et al., 2024)</a></li><li><a href=#3358--72292-impacts-of-color-and-texture-distortions-on-earth-observation-data-in-deep-learning-martin-willbo-et-al-2024>(33/58 | 72/292) Impacts of Color and Texture Distortions on Earth Observation Data in Deep Learning (Martin Willbo et al., 2024)</a></li><li><a href=#3458--73292-multi-step-temporal-modeling-for-uav-tracking-xiaoying-yuan-et-al-2024>(34/58 | 73/292) Multi-step Temporal Modeling for UAV Tracking (Xiaoying Yuan et al., 2024)</a></li><li><a href=#3558--74292-depth-aware-test-time-training-for-zero-shot-video-object-segmentation-weihuang-liu-et-al-2024>(35/58 | 74/292) Depth-aware Test-Time Training for Zero-shot Video Object Segmentation (Weihuang Liu et al., 2024)</a></li><li><a href=#3658--75292-sam-pd-how-far-can-sam-take-us-in-tracking-and-segmenting-anything-in-videos-by-prompt-denoising-tao-zhou-et-al-2024>(36/58 | 75/292) SAM-PD: How Far Can SAM Take Us in Tracking and Segmenting Anything in Videos by Prompt Denoising (Tao Zhou et al., 2024)</a></li><li><a href=#3758--76292-bags-blur-agnostic-gaussian-splatting-through-multi-scale-kernel-modeling-cheng-peng-et-al-2024>(37/58 | 76/292) BAGS: Blur Agnostic Gaussian Splatting through Multi-Scale Kernel Modeling (Cheng Peng et al., 2024)</a></li><li><a href=#3858--77292-cn-rma-combined-network-with-ray-marching-aggregation-for-3d-indoors-object-detection-from-multi-view-images-guanlin-shen-et-al-2024>(38/58 | 77/292) CN-RMA: Combined Network with Ray Marching Aggregation for 3D Indoors Object Detection from Multi-view Images (Guanlin Shen et al., 2024)</a></li><li><a href=#3958--78292-towards-scene-graph-anticipation-rohith-peddi-et-al-2024>(39/58 | 78/292) Towards Scene Graph Anticipation (Rohith Peddi et al., 2024)</a></li><li><a href=#4058--79292-embodied-understanding-of-driving-scenarios-yunsong-zhou-et-al-2024>(40/58 | 79/292) Embodied Understanding of Driving Scenarios (Yunsong Zhou et al., 2024)</a></li><li><a href=#4158--80292-out-of-the-room-generalizing-event-based-dynamic-motion-segmentation-for-complex-scenes-stamatios-georgoulis-et-al-2024>(41/58 | 80/292) Out of the Room: Generalizing Event-Based Dynamic Motion Segmentation for Complex Scenes (Stamatios Georgoulis et al., 2024)</a></li><li><a href=#4258--81292-active-generalized-category-discovery-shijie-ma-et-al-2024>(42/58 | 81/292) Active Generalized Category Discovery (Shijie Ma et al., 2024)</a></li><li><a href=#4358--82292-a-spatiotemporal-style-transfer-algorithm-for-dynamic-visual-stimulus-generation-antonino-greco-et-al-2024>(43/58 | 82/292) A spatiotemporal style transfer algorithm for dynamic visual stimulus generation (Antonino Greco et al., 2024)</a></li><li><a href=#4458--83292-efficient-loftr-semi-dense-local-feature-matching-with-sparse-like-speed-yifan-wang-et-al-2024>(44/58 | 83/292) Efficient LoFTR: Semi-Dense Local Feature Matching with Sparse-Like Speed (Yifan Wang et al., 2024)</a></li><li><a href=#4558--84292-i-cant-believe-its-not-scene-flow-ishan-khatri-et-al-2024>(45/58 | 84/292) I Can&rsquo;t Believe It&rsquo;s Not Scene Flow! (Ishan Khatri et al., 2024)</a></li><li><a href=#4658--85292-faster-neighborhood-attention-reducing-the-on2-cost-of-self-attention-at-the-threadblock-level-ali-hassani-et-al-2024>(46/58 | 85/292) Faster Neighborhood Attention: Reducing the O(n^2) Cost of Self Attention at the Threadblock Level (Ali Hassani et al., 2024)</a></li><li><a href=#4758--86292-dynamic-cross-attention-for-audio-visual-person-verification-r-gnana-praveen-et-al-2024>(47/58 | 86/292) Dynamic Cross Attention for Audio-Visual Person Verification (R. Gnana Praveen et al., 2024)</a></li><li><a href=#4858--87292-explainable-face-verification-via-feature-guided-gradient-backpropagation-yuhang-lu-et-al-2024>(48/58 | 87/292) Explainable Face Verification via Feature-Guided Gradient Backpropagation (Yuhang Lu et al., 2024)</a></li><li><a href=#4958--88292-friendnet-detection-friendly-dehazing-network-yihua-fan-et-al-2024>(49/58 | 88/292) FriendNet: Detection-Friendly Dehazing Network (Yihua Fan et al., 2024)</a></li><li><a href=#5058--89292-stabledrag-stable-dragging-for-point-based-image-editing-yutao-cui-et-al-2024>(50/58 | 89/292) StableDrag: Stable Dragging for Point-based Image Editing (Yutao Cui et al., 2024)</a></li><li><a href=#5158--90292-single-to-dual-view-adaptation-for-egocentric-3d-hand-pose-estimation-ruicong-liu-et-al-2024>(51/58 | 90/292) Single-to-Dual-View Adaptation for Egocentric 3D Hand Pose Estimation (Ruicong Liu et al., 2024)</a></li><li><a href=#5258--91292-video-driven-animation-of-neural-head-avatars-wolfgang-paier-et-al-2024>(52/58 | 91/292) Video-Driven Animation of Neural Head Avatars (Wolfgang Paier et al., 2024)</a></li><li><a href=#5358--92292-spatiotemporal-pooling-on-appropriate-topological-maps-represented-as-two-dimensional-images-for-eeg-classification-takuto-fukushima-et-al-2024>(53/58 | 92/292) Spatiotemporal Pooling on Appropriate Topological Maps Represented as Two-Dimensional Images for EEG Classification (Takuto Fukushima et al., 2024)</a></li><li><a href=#5458--93292-lors-low-rank-residual-structure-for-parameter-efficient-network-stacking-jialin-li-et-al-2024>(54/58 | 93/292) LORS: Low-rank Residual Structure for Parameter-Efficient Network Stacking (Jialin Li et al., 2024)</a></li><li><a href=#5558--94292-map-mask-pruning-for-source-free-model-intellectual-property-protection-boyang-peng-et-al-2024>(55/58 | 94/292) MAP: MAsk-Pruning for Source-Free Model Intellectual Property Protection (Boyang Peng et al., 2024)</a></li><li><a href=#5658--95292-sdpl-shifting-dense-partition-learning-for-uav-view-geo-localization-quan-chen-et-al-2024>(56/58 | 95/292) SDPL: Shifting-Dense Partition Learning for UAV-View Geo-Localization (Quan Chen et al., 2024)</a></li><li><a href=#5758--96292-thats-my-point-compact-object-centric-lidar-pose-estimation-for-large-scale-outdoor-localisation-georgi-pramatarov-et-al-2024>(57/58 | 96/292) That&rsquo;s My Point: Compact Object-centric LiDAR Pose Estimation for Large-scale Outdoor Localisation (Georgi Pramatarov et al., 2024)</a></li><li><a href=#5858--97292-magr-manifold-aligned-graph-regularization-for-continual-action-quality-assessment-kanglei-zhou-et-al-2024>(58/58 | 97/292) MAGR: Manifold-Aligned Graph Regularization for Continual Action Quality Assessment (Kanglei Zhou et al., 2024)</a></li></ul></li><li><a href=#csir-11>cs.IR (11)</a><ul><li><a href=#111--98292-can-small-language-models-be-good-reasoners-for-sequential-recommendation-yuling-wang-et-al-2024>(1/11 | 98/292) Can Small Language Models be Good Reasoners for Sequential Recommendation? (Yuling Wang et al., 2024)</a></li><li><a href=#211--99292-federated-recommendation-via-hybrid-retrieval-augmented-generation-huimin-zeng-et-al-2024>(2/11 | 99/292) Federated Recommendation via Hybrid Retrieval Augmented Generation (Huimin Zeng et al., 2024)</a></li><li><a href=#311--100292-dgr-a-general-graph-desmoothing-framework-for-recommendation-via-global-and-local-perspectives-leilei-ding-et-al-2024>(3/11 | 100/292) DGR: A General Graph Desmoothing Framework for Recommendation via Global and Local Perspectives (Leilei Ding et al., 2024)</a></li><li><a href=#411--101292-aligning-gptrec-with-beyond-accuracy-goals-with-reinforcement-learning-aleksandr-petrov-et-al-2024>(4/11 | 101/292) Aligning GPTRec with Beyond-Accuracy Goals with Reinforcement Learning (Aleksandr Petrov et al., 2024)</a></li><li><a href=#511--102292-ducho-20-towards-a-more-up-to-date-feature-extraction-and-processing-framework-for-multimodal-recommendation-matteo-attimonelli-et-al-2024>(5/11 | 102/292) Ducho 2.0: Towards a More Up-to-Date Feature Extraction and Processing Framework for Multimodal Recommendation (Matteo Attimonelli et al., 2024)</a></li><li><a href=#611--103292-benchmarking-news-recommendation-in-the-era-of-green-ai-qijiong-liu-et-al-2024>(6/11 | 103/292) Benchmarking News Recommendation in the Era of Green AI (Qijiong Liu et al., 2024)</a></li><li><a href=#711--104292-the-2nd-workshop-on-recommendation-with-generative-models-wenjie-wang-et-al-2024>(7/11 | 104/292) The 2nd Workshop on Recommendation with Generative Models (Wenjie Wang et al., 2024)</a></li><li><a href=#811--105292-towards-robustness-analysis-of-e-commerce-ranking-system-ningfei-wang-et-al-2024>(8/11 | 105/292) Towards Robustness Analysis of E-Commerce Ranking System (Ningfei Wang et al., 2024)</a></li><li><a href=#911--106292-improving-retrieval-in-theme-specific-applications-using-a-corpus-topical-taxonomy-seongku-kang-et-al-2024>(9/11 | 106/292) Improving Retrieval in Theme-specific Applications using a Corpus Topical Taxonomy (SeongKu Kang et al., 2024)</a></li><li><a href=#1011--107292-ssdrec-self-augmented-sequence-denoising-for-sequential-recommendation-chi-zhang-et-al-2024>(10/11 | 107/292) SSDRec: Self-Augmented Sequence Denoising for Sequential Recommendation (Chi Zhang et al., 2024)</a></li><li><a href=#1111--108292-acorn-performant-and-predicate-agnostic-search-over-vector-embeddings-and-structured-data-liana-patel-et-al-2024>(11/11 | 108/292) ACORN: Performant and Predicate-Agnostic Search Over Vector Embeddings and Structured Data (Liana Patel et al., 2024)</a></li></ul></li><li><a href=#csai-29>cs.AI (29)</a><ul><li><a href=#129--109292-mkf-ads-a-multi-knowledge-fused-anomaly-detection-system-for-automotive-pengzhou-cheng-et-al-2024>(1/29 | 109/292) MKF-ADS: A Multi-Knowledge Fused Anomaly Detection System for Automotive (Pengzhou Cheng et al., 2024)</a></li><li><a href=#229--110292-advancing-biomedical-text-mining-with-community-challenges-hui-zong-et-al-2024>(2/29 | 110/292) Advancing Biomedical Text Mining with Community Challenges (Hui Zong et al., 2024)</a></li><li><a href=#329--111292-privacy-preserving-fine-tuning-of-large-language-models-through-flatness-tiejin-chen-et-al-2024>(3/29 | 111/292) Privacy-preserving Fine-tuning of Large Language Models through Flatness (Tiejin Chen et al., 2024)</a></li><li><a href=#429--112292-feedback-generation-for-programming-exercises-with-gpt-4-imen-azaiz-et-al-2024>(4/29 | 112/292) Feedback-Generation for Programming Exercises With GPT-4 (Imen Azaiz et al., 2024)</a></li><li><a href=#529--113292-how-far-are-we-from-intelligent-visual-deductive-reasoning-yizhe-zhang-et-al-2024>(5/29 | 113/292) How Far Are We from Intelligent Visual Deductive Reasoning? (Yizhe Zhang et al., 2024)</a></li><li><a href=#629--114292-enhancing-court-view-generation-with-knowledge-injection-and-guidance-ang-li-et-al-2024>(6/29 | 114/292) Enhancing Court View Generation with Knowledge Injection and Guidance (Ang Li et al., 2024)</a></li><li><a href=#729--115292-on-the-essence-and-prospect-an-investigation-of-alignment-approaches-for-big-models-xinpeng-wang-et-al-2024>(7/29 | 115/292) On the Essence and Prospect: An Investigation of Alignment Approaches for Big Models (Xinpeng Wang et al., 2024)</a></li><li><a href=#829--116292-wiki-tabneradvancing-table-interpretation-through-named-entity-recognition-aneta-koleva-et-al-2024>(8/29 | 116/292) Wiki-TabNER:Advancing Table Interpretation Through Named Entity Recognition (Aneta Koleva et al., 2024)</a></li><li><a href=#929--117292-zero-shot-cross-modal-transfer-of-reinforcement-learning-policies-through-a-global-workspace-léopold-maytié-et-al-2024>(9/29 | 117/292) Zero-shot cross-modal transfer of Reinforcement Learning policies through a Global Workspace (Léopold Maytié et al., 2024)</a></li><li><a href=#1029--118292-graphinstruct-empowering-large-language-models-with-graph-understanding-and-reasoning-capability-zihan-luo-et-al-2024>(10/29 | 118/292) GraphInstruct: Empowering Large Language Models with Graph Understanding and Reasoning Capability (Zihan Luo et al., 2024)</a></li><li><a href=#1129--119292-improving-matrix-completion-by-exploiting-rating-ordinality-in-graph-neural-networks-jaehyun-lee-et-al-2024>(11/29 | 119/292) Improving Matrix Completion by Exploiting Rating Ordinality in Graph Neural Networks (Jaehyun Lee et al., 2024)</a></li><li><a href=#1229--120292-unsupervised-learning-of-harmonic-analysis-based-on-neural-hsmm-with-code-quality-templates-yui-uehara-2024>(12/29 | 120/292) Unsupervised Learning of Harmonic Analysis Based on Neural HSMM with Code Quality Templates (Yui Uehara, 2024)</a></li><li><a href=#1329--121292-a-new-benchmark-for-evaluating-automatic-speech-recognition-in-the-arabic-call-domain-qusai-abo-obaidah-et-al-2024>(13/29 | 121/292) A New Benchmark for Evaluating Automatic Speech Recognition in the Arabic Call Domain (Qusai Abo Obaidah et al., 2024)</a></li><li><a href=#1429--122292-contrastive-augmented-graph2graph-memory-interaction-for-few-shot-continual-learning-biqing-qi-et-al-2024>(14/29 | 122/292) Contrastive Augmented Graph2Graph Memory Interaction for Few Shot Continual Learning (Biqing Qi et al., 2024)</a></li><li><a href=#1529--123292-automatic-and-universal-prompt-injection-attacks-against-large-language-models-xiaogeng-liu-et-al-2024>(15/29 | 123/292) Automatic and Universal Prompt Injection Attacks against Large Language Models (Xiaogeng Liu et al., 2024)</a></li><li><a href=#1629--124292-machine-learning-and-information-theory-concepts-towards-an-ai-mathematician-yoshua-bengio-et-al-2024>(16/29 | 124/292) Machine learning and information theory concepts towards an AI Mathematician (Yoshua Bengio et al., 2024)</a></li><li><a href=#1729--125292-chatbot-arena-an-open-platform-for-evaluating-llms-by-human-preference-wei-lin-chiang-et-al-2024>(17/29 | 125/292) Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference (Wei-Lin Chiang et al., 2024)</a></li><li><a href=#1829--126292-cotbal-comprehensive-task-balancing-for-multi-task-visual-instruction-tuning-yanqi-dai-et-al-2024>(18/29 | 126/292) CoTBal: Comprehensive Task Balancing for Multi-Task Visual Instruction Tuning (Yanqi Dai et al., 2024)</a></li><li><a href=#1929--127292-towards-automatic-composition-of-asp-programs-from-natural-language-specifications-manuel-borroto-et-al-2024>(19/29 | 127/292) Towards Automatic Composition of ASP Programs from Natural Language Specifications (Manuel Borroto et al., 2024)</a></li><li><a href=#2029--128292-self-supervision-in-time-for-satellite-imagess3-tss-a-novel-method-of-ssl-technique-in-satellite-images-akansh-maurya-et-al-2024>(20/29 | 128/292) Self-Supervision in Time for Satellite Images(S3-TSS): A novel method of SSL technique in Satellite images (Akansh Maurya et al., 2024)</a></li><li><a href=#2129--129292-the-social-impact-of-generative-ai-an-analysis-on-chatgpt-maria-t-baldassarre-et-al-2024>(21/29 | 129/292) The Social Impact of Generative AI: An Analysis on ChatGPT (Maria T. Baldassarre et al., 2024)</a></li><li><a href=#2229--130292-uncovering-the-deep-filter-bubble-narrow-exposure-in-short-video-recommendation-nicholas-sukiennik-et-al-2024>(22/29 | 130/292) Uncovering the Deep Filter Bubble: Narrow Exposure in Short-Video Recommendation (Nicholas Sukiennik et al., 2024)</a></li><li><a href=#2329--131292-alto-an-efficient-network-orchestrator-for-compound-ai-systems-keshav-santhanam-et-al-2024>(23/29 | 131/292) ALTO: An Efficient Network Orchestrator for Compound AI Systems (Keshav Santhanam et al., 2024)</a></li><li><a href=#2429--132292-can-large-language-models-reason-and-plan-subbarao-kambhampati-2024>(24/29 | 132/292) Can Large Language Models Reason and Plan? (Subbarao Kambhampati, 2024)</a></li><li><a href=#2529--133292-from-graph-to-word-bag-introducing-domain-knowledge-to-confusing-charge-prediction-ang-li-et-al-2024>(25/29 | 133/292) From Graph to Word Bag: Introducing Domain Knowledge to Confusing Charge Prediction (Ang Li et al., 2024)</a></li><li><a href=#2629--134292-a-safe-harbor-for-ai-evaluation-and-red-teaming-shayne-longpre-et-al-2024>(26/29 | 134/292) A Safe Harbor for AI Evaluation and Red Teaming (Shayne Longpre et al., 2024)</a></li><li><a href=#2729--135292-convergence-of-some-convex-message-passing-algorithms-to-a-fixed-point-vaclav-voracek-et-al-2024>(27/29 | 135/292) Convergence of Some Convex Message Passing Algorithms to a Fixed Point (Vaclav Voracek et al., 2024)</a></li><li><a href=#2829--136292-a-modular-end-to-end-multimodal-learning-method-for-structured-and-unstructured-data-marco-d-alessandro-et-al-2024>(28/29 | 136/292) A Modular End-to-End Multimodal Learning Method for Structured and Unstructured Data (Marco D Alessandro et al., 2024)</a></li><li><a href=#2929--137292-identifying-causal-effects-under-functional-dependencies-yizuo-chen-et-al-2024>(29/29 | 137/292) Identifying Causal Effects Under Functional Dependencies (Yizuo Chen et al., 2024)</a></li></ul></li><li><a href=#cslg-46>cs.LG (46)</a><ul><li><a href=#146--138292-teaching-large-language-models-to-reason-with-reinforcement-learning-alex-havrilla-et-al-2024>(1/46 | 138/292) Teaching Large Language Models to Reason with Reinforcement Learning (Alex Havrilla et al., 2024)</a></li><li><a href=#246--139292-bloomgml-graph-machine-learning-through-the-lens-of-bilevel-optimization-amber-yijia-zheng-et-al-2024>(2/46 | 139/292) BloomGML: Graph Machine Learning through the Lens of Bilevel Optimization (Amber Yijia Zheng et al., 2024)</a></li><li><a href=#346--140292-reducing-self-supervised-learning-complexity-improves-weakly-supervised-classification-performance-in-computational-pathology-tim-lenz-et-al-2024>(3/46 | 140/292) Reducing self-supervised learning complexity improves weakly-supervised classification performance in computational pathology (Tim Lenz et al., 2024)</a></li><li><a href=#446--141292-control-based-graph-embeddings-with-data-augmentation-for-contrastive-learning-obaid-ullah-ahmad-et-al-2024>(4/46 | 141/292) Control-based Graph Embeddings with Data Augmentation for Contrastive Learning (Obaid Ullah Ahmad et al., 2024)</a></li><li><a href=#546--142292-contrastive-continual-learning-with-importance-sampling-and-prototype-instance-relation-distillation-jiyong-li-et-al-2024>(5/46 | 142/292) Contrastive Continual Learning with Importance Sampling and Prototype-Instance Relation Distillation (Jiyong Li et al., 2024)</a></li><li><a href=#646--143292-on-demand-quantization-for-green-federated-generative-diffusion-in-mobile-edge-networks-bingkun-lai-et-al-2024>(6/46 | 143/292) On-demand Quantization for Green Federated Generative Diffusion in Mobile Edge Networks (Bingkun Lai et al., 2024)</a></li><li><a href=#746--144292-generative-ai-for-synthetic-data-generation-methods-challenges-and-the-future-xu-guo-et-al-2024>(7/46 | 144/292) Generative AI for Synthetic Data Generation: Methods, Challenges and the Future (Xu Guo et al., 2024)</a></li><li><a href=#846--145292-clip-the-bias-how-useful-is-balancing-data-in-multimodal-learning-ibrahim-alabdulmohsin-et-al-2024>(8/46 | 145/292) CLIP the Bias: How Useful is Balancing Data in Multimodal Learning? (Ibrahim Alabdulmohsin et al., 2024)</a></li><li><a href=#946--146292-on-the-topology-awareness-and-generalization-performance-of-graph-neural-networks-junwei-su-et-al-2024>(9/46 | 146/292) On the Topology Awareness and Generalization Performance of Graph Neural Networks (Junwei Su et al., 2024)</a></li><li><a href=#1046--147292-gnn-vpa-a-variance-preserving-aggregation-strategy-for-graph-neural-networks-lisa-schneckenreiter-et-al-2024>(10/46 | 147/292) GNN-VPA: A Variance-Preserving Aggregation Strategy for Graph Neural Networks (Lisa Schneckenreiter et al., 2024)</a></li><li><a href=#1146--148292-entropy-aware-message-passing-in-graph-neural-networks-philipp-nazari-et-al-2024>(11/46 | 148/292) Entropy Aware Message Passing in Graph Neural Networks (Philipp Nazari et al., 2024)</a></li><li><a href=#1246--149292-a-survey-of-graph-neural-networks-in-real-world-imbalance-noise-privacy-and-ood-challenges-wei-ju-et-al-2024>(12/46 | 149/292) A Survey of Graph Neural Networks in Real world: Imbalance, Noise, Privacy and OOD Challenges (Wei Ju et al., 2024)</a></li><li><a href=#1346--150292-improved-algorithm-for-adversarial-linear-mixture-mdps-with-bandit-feedback-and-unknown-transition-long-fei-li-et-al-2024>(13/46 | 150/292) Improved Algorithm for Adversarial Linear Mixture MDPs with Bandit Feedback and Unknown Transition (Long-Fei Li et al., 2024)</a></li><li><a href=#1446--151292-improve-generalization-ability-of-deep-wide-residual-network-with-a-suitable-scaling-factor-songtao-tian-et-al-2024>(14/46 | 151/292) Improve Generalization Ability of Deep Wide Residual Network with A Suitable Scaling Factor (Songtao Tian et al., 2024)</a></li><li><a href=#1546--152292-storm-surge-modeling-in-the-ai-era-using-lstm-based-machine-learning-for-enhancing-forecasting-accuracy-stefanos-giaremis-et-al-2024>(15/46 | 152/292) Storm Surge Modeling in the AI ERA: Using LSTM-based Machine Learning for Enhancing Forecasting Accuracy (Stefanos Giaremis et al., 2024)</a></li><li><a href=#1646--153292-online-adaptation-of-language-models-with-a-memory-of-amortized-contexts-jihoon-tack-et-al-2024>(16/46 | 153/292) Online Adaptation of Language Models with a Memory of Amortized Contexts (Jihoon Tack et al., 2024)</a></li><li><a href=#1746--154292-why-online-reinforcement-learning-is-causal-oliver-schulte-et-al-2024>(17/46 | 154/292) Why Online Reinforcement Learning is Causal (Oliver Schulte et al., 2024)</a></li><li><a href=#1846--155292-context-based-multimodal-fusion-bilal-faye-et-al-2024>(18/46 | 155/292) Context-Based Multimodal Fusion (Bilal Faye et al., 2024)</a></li><li><a href=#1946--156292-rethinking-of-encoder-based-warm-start-methods-in-hyperparameter-optimization-dawid-płudowski-et-al-2024>(19/46 | 156/292) Rethinking of Encoder-based Warm-start Methods in Hyperparameter Optimization (Dawid Płudowski et al., 2024)</a></li><li><a href=#2046--157292-explaining-bayesian-optimization-by-shapley-values-facilitates-human-ai-collaboration-julian-rodemann-et-al-2024>(20/46 | 157/292) Explaining Bayesian Optimization by Shapley Values Facilitates Human-AI Collaboration (Julian Rodemann et al., 2024)</a></li><li><a href=#2146--158292-on-the-markov-property-of-neural-algorithmic-reasoning-analyses-and-methods-montgomery-bohde-et-al-2024>(21/46 | 158/292) On the Markov Property of Neural Algorithmic Reasoning: Analyses and Methods (Montgomery Bohde et al., 2024)</a></li><li><a href=#2246--159292-in-n-out-calibrating-graph-neural-networks-for-link-prediction-erik-nascimento-et-al-2024>(22/46 | 159/292) In-n-Out: Calibrating Graph Neural Networks for Link Prediction (Erik Nascimento et al., 2024)</a></li><li><a href=#2346--160292-enhancing-data-quality-in-federated-fine-tuning-of-foundation-models-wanru-zhao-et-al-2024>(23/46 | 160/292) Enhancing Data Quality in Federated Fine-Tuning of Foundation Models (Wanru Zhao et al., 2024)</a></li><li><a href=#2446--161292-what-makes-an-image-realistic-lucas-theis-2024>(24/46 | 161/292) What makes an image realistic? (Lucas Theis, 2024)</a></li><li><a href=#2546--162292-boosting-fairness-and-robustness-in-over-the-air-federated-learning-halil-yigit-oksuz-et-al-2024>(25/46 | 162/292) Boosting Fairness and Robustness in Over-the-Air Federated Learning (Halil Yigit Oksuz et al., 2024)</a></li><li><a href=#2646--163292-exploring-the-influence-of-dimensionality-reduction-on-anomaly-detection-performance-in-multivariate-time-series-mahsun-altin-et-al-2024>(26/46 | 163/292) Exploring the Influence of Dimensionality Reduction on Anomaly Detection Performance in Multivariate Time Series (Mahsun Altin et al., 2024)</a></li><li><a href=#2746--164292-heteroswitch-characterizing-and-taming-system-induced-data-heterogeneity-in-federated-learning-gyudong-kim-et-al-2024>(27/46 | 164/292) HeteroSwitch: Characterizing and Taming System-Induced Data Heterogeneity in Federated Learning (Gyudong Kim et al., 2024)</a></li><li><a href=#2846--165292-ratsf-empowering-customer-service-volume-management-through-retrieval-augmented-time-series-forecasting-tianfeng-wang-et-al-2024>(28/46 | 165/292) RATSF: Empowering Customer Service Volume Management through Retrieval-Augmented Time-Series Forecasting (Tianfeng Wang et al., 2024)</a></li><li><a href=#2946--166292-density-regression-efficient-and-distance-aware-deep-regressor-for-uncertainty-estimation-under-distribution-shifts-ha-manh-bui-et-al-2024>(29/46 | 166/292) Density-Regression: Efficient and Distance-Aware Deep Regressor for Uncertainty Estimation under Distribution Shifts (Ha Manh Bui et al., 2024)</a></li><li><a href=#3046--167292-lifelong-intelligence-beyond-the-edge-using-hyperdimensional-computing-xiaofan-yu-et-al-2024>(30/46 | 167/292) Lifelong Intelligence Beyond the Edge using Hyperdimensional Computing (Xiaofan Yu et al., 2024)</a></li><li><a href=#3146--168292-hyperspectral-unmixing-for-raman-spectroscopy-via-physics-constrained-autoencoders-dimitar-georgiev-et-al-2024>(31/46 | 168/292) Hyperspectral unmixing for Raman spectroscopy via physics-constrained autoencoders (Dimitar Georgiev et al., 2024)</a></li><li><a href=#3246--169292-vlearn-off-policy-learning-with-efficient-state-value-function-estimation-fabian-otto-et-al-2024>(32/46 | 169/292) Vlearn: Off-Policy Learning with Efficient State-Value Function Estimation (Fabian Otto et al., 2024)</a></li><li><a href=#3346--170292-gradient-free-neural-topology-optimization-gawel-kus-et-al-2024>(33/46 | 170/292) Gradient-free neural topology optimization (Gawel Kus et al., 2024)</a></li><li><a href=#3446--171292-efficient-high-resolution-time-series-classification-via-attention-kronecker-decomposition-aosong-feng-et-al-2024>(34/46 | 171/292) Efficient High-Resolution Time Series Classification via Attention Kronecker Decomposition (Aosong Feng et al., 2024)</a></li><li><a href=#3546--172292-end-to-end-conditional-robust-optimization-abhilash-chenreddy-et-al-2024>(35/46 | 172/292) End-to-end Conditional Robust Optimization (Abhilash Chenreddy et al., 2024)</a></li><li><a href=#3646--173292-architectural-blueprint-for-heterogeneity-resilient-federated-learning-satwat-bashir-et-al-2024>(36/46 | 173/292) Architectural Blueprint For Heterogeneity-Resilient Federated Learning (Satwat Bashir et al., 2024)</a></li><li><a href=#3746--174292-explainable-ai-for-embedded-systems-design-a-case-study-of-static-redundant-nvm-memory-write-prediction-abdoulaye-gamatié-et-al-2024>(37/46 | 174/292) Explainable AI for Embedded Systems Design: A Case Study of Static Redundant NVM Memory Write Prediction (Abdoulaye Gamatié et al., 2024)</a></li><li><a href=#3846--175292-mastering-memory-tasks-with-world-models-mohammad-reza-samsami-et-al-2024>(38/46 | 175/292) Mastering Memory Tasks with World Models (Mohammad Reza Samsami et al., 2024)</a></li><li><a href=#3946--176292-fill-and-spill-deep-reinforcement-learning-policy-gradient-methods-for-reservoir-operation-decision-and-control-sadegh-sadeghi-tabas-et-al-2024>(39/46 | 176/292) Fill-and-Spill: Deep Reinforcement Learning Policy Gradient Methods for Reservoir Operation Decision and Control (Sadegh Sadeghi Tabas et al., 2024)</a></li><li><a href=#4046--177292-noisy-spiking-actor-network-for-exploration-ding-chen-et-al-2024>(40/46 | 177/292) Noisy Spiking Actor Network for Exploration (Ding Chen et al., 2024)</a></li><li><a href=#4146--178292-stabilizing-policy-gradients-for-stochastic-differential-equations-via-consistency-with-perturbation-process-xiangxin-zhou-et-al-2024>(41/46 | 178/292) Stabilizing Policy Gradients for Stochastic Differential Equations via Consistency with Perturbation Process (Xiangxin Zhou et al., 2024)</a></li><li><a href=#4246--179292-fl-guard-a-holistic-framework-for-run-time-detection-and-recovery-of-negative-federated-learning-hong-lin-et-al-2024>(42/46 | 179/292) FL-GUARD: A Holistic Framework for Run-Time Detection and Recovery of Negative Federated Learning (Hong Lin et al., 2024)</a></li><li><a href=#4346--180292-dissecting-sample-hardness-a-fine-grained-analysis-of-hardness-characterization-methods-for-data-centric-ai-nabeel-seedat-et-al-2024>(43/46 | 180/292) Dissecting Sample Hardness: A Fine-Grained Analysis of Hardness Characterization Methods for Data-Centric AI (Nabeel Seedat et al., 2024)</a></li><li><a href=#4446--181292-frri-a-novel-algorithm-for-fuzzy-rough-rule-induction-henri-bollaert-et-al-2024>(44/46 | 181/292) FRRI: a novel algorithm for fuzzy-rough rule induction (Henri Bollaert et al., 2024)</a></li><li><a href=#4546--182292-cooperative-bayesian-optimization-for-imperfect-agents-ali-khoshvishkaie-et-al-2024>(45/46 | 182/292) Cooperative Bayesian Optimization for Imperfect Agents (Ali Khoshvishkaie et al., 2024)</a></li><li><a href=#4646--183292-minimizing-the-thompson-sampling-regret-to-sigma-ratio-ts-rsr-a-provably-efficient-algorithm-for-batch-bayesian-optimization-zhaolin-ren-et-al-2024>(46/46 | 183/292) Minimizing the Thompson Sampling Regret-to-Sigma Ratio (TS-RSR): a provably efficient algorithm for batch Bayesian Optimization (Zhaolin Ren et al., 2024)</a></li></ul></li><li><a href=#eessiv-5>eess.IV (5)</a><ul><li><a href=#15--184292-beyond-multiple-instance-learning-full-resolution-all-in-memory-end-to-end-pathology-slide-modeling-gabriele-campanella-et-al-2024>(1/5 | 184/292) Beyond Multiple Instance Learning: Full Resolution All-In-Memory End-To-End Pathology Slide Modeling (Gabriele Campanella et al., 2024)</a></li><li><a href=#25--185292-medflip-medical-vision-and-language-self-supervised-fast-pre-training-with-masked-autoencoder-lei-li-et-al-2024>(2/5 | 185/292) MedFLIP: Medical Vision-and-Language Self-supervised Fast Pre-Training with Masked Autoencoder (Lei Li et al., 2024)</a></li><li><a href=#35--186292-a-domain-translation-framework-with-an-adversarial-denoising-diffusion-model-to-generate-synthetic-datasets-of-echocardiography-images-cristiana-tiago-et-al-2024>(3/5 | 186/292) A Domain Translation Framework with an Adversarial Denoising Diffusion Model to Generate Synthetic Datasets of Echocardiography Images (Cristiana Tiago et al., 2024)</a></li><li><a href=#45--187292-medm2g-unifying-medical-multi-modal-generation-via-cross-guided-diffusion-with-visual-invariant-chenlu-zhan-et-al-2024>(4/5 | 187/292) MedM2G: Unifying Medical Multi-Modal Generation via Cross-Guided Diffusion with Visual Invariant (Chenlu Zhan et al., 2024)</a></li><li><a href=#55--188292-improved-focus-on-hard-samples-for-lung-nodule-detection-yujiang-chen-et-al-2024>(5/5 | 188/292) Improved Focus on Hard Samples for Lung Nodule Detection (Yujiang Chen et al., 2024)</a></li></ul></li><li><a href=#csdb-3>cs.DB (3)</a><ul><li><a href=#13--189292-promoai-process-modeling-with-generative-ai-humam-kourani-et-al-2024>(1/3 | 189/292) ProMoAI: Process Modeling with Generative AI (Humam Kourani et al., 2024)</a></li><li><a href=#23--190292-evaluation-of-nosql-in-the-energy-marketplace-with-graphql-optimization-michael-howard-2024>(2/3 | 190/292) Evaluation of NoSQL in the Energy Marketplace with GraphQL Optimization (Michael Howard, 2024)</a></li><li><a href=#33--191292-mining-transactional-data-to-produce-extended-association-rules-using-collaborative-apriori-fsa-red-and-m5p-predictive-algorithm-as-a-basis-of-business-actions-feri-sulianta-et-al-2024>(3/3 | 191/292) Mining Transactional Data To Produce Extended Association Rules Using Collaborative Apriori, Fsa-Red And M5p Predictive Algorithm As A Basis Of Business Actions (Feri Sulianta et al., 2024)</a></li></ul></li><li><a href=#cscr-6>cs.CR (6)</a><ul><li><a href=#16--192292-vaemax-open-set-intrusion-detection-based-on-openmax-and-variational-autoencoder-zhiyin-qiu-et-al-2024>(1/6 | 192/292) VAEMax: Open-Set Intrusion Detection based on OpenMax and Variational Autoencoder (Zhiyin Qiu et al., 2024)</a></li><li><a href=#26--193292-membership-inference-attacks-and-privacy-in-topic-modeling-nico-manzonelli-et-al-2024>(2/6 | 193/292) Membership Inference Attacks and Privacy in Topic Modeling (Nico Manzonelli et al., 2024)</a></li><li><a href=#36--194292-secure-information-embedding-and-extraction-in-forensic-3d-fingerprinting-canran-wang-et-al-2024>(3/6 | 194/292) Secure Information Embedding and Extraction in Forensic 3D Fingerprinting (Canran Wang et al., 2024)</a></li><li><a href=#46--195292-privacy-amplification-for-the-gaussian-mechanism-via-bounded-support-shengyuan-hu-et-al-2024>(4/6 | 195/292) Privacy Amplification for the Gaussian Mechanism via Bounded Support (Shengyuan Hu et al., 2024)</a></li><li><a href=#56--196292-group-privacy-amplification-and-unified-amplification-by-subsampling-for-rényi-differential-privacy-jan-schuchardt-et-al-2024>(5/6 | 196/292) Group Privacy Amplification and Unified Amplification by Subsampling for Rényi Differential Privacy (Jan Schuchardt et al., 2024)</a></li><li><a href=#66--197292-privacy-in-cloud-computing-through-immersion-based-coding-haleh-hayati-et-al-2024>(6/6 | 197/292) Privacy in Cloud Computing through Immersion-based Coding (Haleh Hayati et al., 2024)</a></li></ul></li><li><a href=#cshc-3>cs.HC (3)</a><ul><li><a href=#13--198292-knowledgevis-interpreting-language-models-by-comparing-fill-in-the-blank-prompts-adam-coscia-et-al-2024>(1/3 | 198/292) KnowledgeVIS: Interpreting Language Models by Comparing Fill-in-the-Blank Prompts (Adam Coscia et al., 2024)</a></li><li><a href=#23--199292-iscore-visual-analytics-for-interpreting-how-language-models-automatically-score-summaries-adam-coscia-et-al-2024>(2/3 | 199/292) iScore: Visual Analytics for Interpreting How Language Models Automatically Score Summaries (Adam Coscia et al., 2024)</a></li><li><a href=#33--200292-deepsee-multidimensional-visualizations-of-seabed-ecosystems-adam-coscia-et-al-2024>(3/3 | 200/292) DeepSee: Multidimensional Visualizations of Seabed Ecosystems (Adam Coscia et al., 2024)</a></li></ul></li><li><a href=#csro-18>cs.RO (18)</a><ul><li><a href=#118--201292-a-general-calibrated-regret-metric-for-detecting-and-mitigating-human-robot-interaction-failures-kensuke-nakamura-et-al-2024>(1/18 | 201/292) A General Calibrated Regret Metric for Detecting and Mitigating Human-Robot Interaction Failures (Kensuke Nakamura et al., 2024)</a></li><li><a href=#218--202292-learning-human-to-humanoid-real-time-whole-body-teleoperation-tairan-he-et-al-2024>(2/18 | 202/292) Learning Human-to-Humanoid Real-Time Whole-Body Teleoperation (Tairan He et al., 2024)</a></li><li><a href=#318--203292-dnact-diffusion-guided-multi-task-3d-policy-learning-ge-yan-et-al-2024>(3/18 | 203/292) DNAct: Diffusion Guided Multi-Task 3D Policy Learning (Ge Yan et al., 2024)</a></li><li><a href=#418--204292-learning-agility-adaptation-for-flight-in-clutter-guangyu-zhao-et-al-2024>(4/18 | 204/292) Learning Agility Adaptation for Flight in Clutter (Guangyu Zhao et al., 2024)</a></li><li><a href=#518--205292-incremental-bayesian-learning-for-fail-operational-control-in-autonomous-driving-lei-zheng-et-al-2024>(5/18 | 205/292) Incremental Bayesian Learning for Fail-Operational Control in Autonomous Driving (Lei Zheng et al., 2024)</a></li><li><a href=#618--206292-real-time-planning-under-uncertainty-for-auvs-using-virtual-maps-ivana-collado-gonzalez-et-al-2024>(6/18 | 206/292) Real-Time Planning Under Uncertainty for AUVs Using Virtual Maps (Ivana Collado-Gonzalez et al., 2024)</a></li><li><a href=#718--207292-scalable-simulation-guided-compliant-tactile-finger-design-yuxiang-ma-et-al-2024>(7/18 | 207/292) Scalable, Simulation-Guided Compliant Tactile Finger Design (Yuxiang Ma et al., 2024)</a></li><li><a href=#818--208292-a-magnetic-millirobot-walks-on-slippery-biological-surfaces-for-targeted-cargo-delivery-moonkwang-jeong-et-al-2024>(8/18 | 208/292) A Magnetic Millirobot Walks on Slippery Biological Surfaces for Targeted Cargo Delivery (Moonkwang Jeong et al., 2024)</a></li><li><a href=#918--209292-symmetry-considerations-for-learning-task-symmetric-robot-policies-mayank-mittal-et-al-2024>(9/18 | 209/292) Symmetry Considerations for Learning Task Symmetric Robot Policies (Mayank Mittal et al., 2024)</a></li><li><a href=#1018--210292-litsim-conflict-aware-policy-for-long-term-interactive-traffic-simulation-haojie-xin-et-al-2024>(10/18 | 210/292) LitSim: Conflict-aware Policy for Long-term Interactive Traffic Simulation (Haojie Xin et al., 2024)</a></li><li><a href=#1118--211292-globally-stable-neural-imitation-policies-amin-abyaneh-et-al-2024>(11/18 | 211/292) Globally Stable Neural Imitation Policies (Amin Abyaneh et al., 2024)</a></li><li><a href=#1218--212292-a-mixed-integer-conic-program-for-the-moving-target-traveling-salesman-problem-based-on-a-graph-of-convex-sets-allen-george-philip-et-al-2024>(12/18 | 212/292) A Mixed-Integer Conic Program for the Moving-Target Traveling Salesman Problem based on a Graph of Convex Sets (Allen George Philip et al., 2024)</a></li><li><a href=#1318--213292-generalizing-cooperative-eco-driving-via-multi-residual-task-learning-vindula-jayawardana-et-al-2024>(13/18 | 213/292) Generalizing Cooperative Eco-driving via Multi-residual Task Learning (Vindula Jayawardana et al., 2024)</a></li><li><a href=#1418--214292-closing-the-visual-sim-to-real-gap-with-object-composable-nerfs-nikhil-mishra-et-al-2024>(14/18 | 214/292) Closing the Visual Sim-to-Real Gap with Object-Composable NeRFs (Nikhil Mishra et al., 2024)</a></li><li><a href=#1518--215292-ogmp-oracle-guided-multimodal-policies-for-agile-and-versatile-robot-control-lokesh-krishna-et-al-2024>(15/18 | 215/292) OGMP: Oracle Guided Multimodal Policies for Agile and Versatile Robot Control (Lokesh Krishna et al., 2024)</a></li><li><a href=#1618--216292-robokube-establishing-a-new-foundation-for-the-cloud-native-evolution-in-robotics-yu-liu-et-al-2024>(16/18 | 216/292) RoboKube: Establishing a New Foundation for the Cloud Native Evolution in Robotics (Yu Liu et al., 2024)</a></li><li><a href=#1718--217292-social-robots-for-sleep-health-a-scoping-review-victor-nikhil-antony-et-al-2024>(17/18 | 217/292) Social Robots for Sleep Health: A Scoping Review (Victor Nikhil Antony et al., 2024)</a></li><li><a href=#1818--218292-an-adaptable-safe-and-portable-robot-assisted-feeding-system-ethan-kroll-gordon-et-al-2024>(18/18 | 218/292) An Adaptable, Safe, and Portable Robot-Assisted Feeding System (Ethan Kroll Gordon et al., 2024)</a></li></ul></li><li><a href=#mathna-8>math.NA (8)</a><ul><li><a href=#18--219292-accelerating-multigrid-solver-with-generative-super-resolution-francisco-holguin-et-al-2024>(1/8 | 219/292) Accelerating multigrid solver with generative super-resolution (Francisco Holguin et al., 2024)</a></li><li><a href=#28--220292-a-structure-preserving-semi-implicit-imex-finite-volume-scheme-for-ideal-magnetohydrodynamics-at-all-mach-and-alfvén-numbers-walter-boscheri-et-al-2024>(2/8 | 220/292) A structure-preserving semi-implicit IMEX finite volume scheme for ideal magnetohydrodynamics at all Mach and Alfvén numbers (Walter Boscheri et al., 2024)</a></li><li><a href=#38--221292-a-mechanism-informed-reinforcement-learning-framework-for-shape-optimization-of-airfoils-jingfeng-wang-et-al-2024>(3/8 | 221/292) A mechanism-informed reinforcement learning framework for shape optimization of airfoils (Jingfeng Wang et al., 2024)</a></li><li><a href=#48--222292-scalable-approximation-and-solvers-for-ionic-electrodiffusion-in-cellular-geometries-pietro-benedusi-et-al-2024>(4/8 | 222/292) Scalable approximation and solvers for ionic electrodiffusion in cellular geometries (Pietro Benedusi et al., 2024)</a></li><li><a href=#58--223292-towards-robust-data-driven-automated-recovery-of-symbolic-conservation-laws-from-limited-data-tracey-oellerich-et-al-2024>(5/8 | 223/292) Towards Robust Data-Driven Automated Recovery of Symbolic Conservation Laws from Limited Data (Tracey Oellerich et al., 2024)</a></li><li><a href=#68--224292-a-finite-element-contour-integral-method-for-computing-the-resonances-of-metallic-grating-structures-with-subwavelength-holes-yingxia-xi-et-al-2024>(6/8 | 224/292) A finite element contour integral method for computing the resonances of metallic grating structures with subwavelength holes (Yingxia Xi et al., 2024)</a></li><li><a href=#78--225292-a-lagrangian-approach-for-solving-an-axisymmetric-thermo-electromagnetic-problem-application-to-time-varying-geometry-processes-marta-benítez-et-al-2024>(7/8 | 225/292) A Lagrangian approach for solving an axisymmetric thermo-electromagnetic problem. Application to time-varying geometry processes (Marta Benítez et al., 2024)</a></li><li><a href=#88--226292-a-robust-shifted-proper-orthogonal-decomposition-proximal-methods-for-decomposing-flows-with-multiple-transports-philipp-krah-et-al-2024>(8/8 | 226/292) A robust shifted proper orthogonal decomposition: Proximal methods for decomposing flows with multiple transports (Philipp Krah et al., 2024)</a></li></ul></li><li><a href=#quant-ph-3>quant-ph (3)</a><ul><li><a href=#13--227292-online-maximum-likelihood-parameter-estimation-for-continuously-monitored-quantum-systems-henrik-glavind-clausen-et-al-2024>(1/3 | 227/292) Online Maximum Likelihood Parameter Estimation for Continuously-Monitored Quantum Systems (Henrik Glavind Clausen et al., 2024)</a></li><li><a href=#23--228292-qubit-wise-architecture-search-method-for-variational-quantum-circuits-jialin-chen-et-al-2024>(2/3 | 228/292) Qubit-Wise Architecture Search Method for Variational Quantum Circuits (Jialin Chen et al., 2024)</a></li><li><a href=#33--229292-optimal-scheduling-of-graph-states-via-path-decompositions-samuel-j-elman-et-al-2024>(3/3 | 229/292) Optimal Scheduling of Graph States via Path Decompositions (Samuel J. Elman et al., 2024)</a></li></ul></li><li><a href=#csse-3>cs.SE (3)</a><ul><li><a href=#13--230292-exploring-llm-based-agents-for-root-cause-analysis-devjeet-roy-et-al-2024>(1/3 | 230/292) Exploring LLM-based Agents for Root Cause Analysis (Devjeet Roy et al., 2024)</a></li><li><a href=#23--231292-unveiling-a-hidden-risk-exposing-educational-but-malicious-repositories-in-github-md-rayhanul-masud-et-al-2024>(2/3 | 231/292) Unveiling A Hidden Risk: Exposing Educational but Malicious Repositories in GitHub (Md Rayhanul Masud et al., 2024)</a></li><li><a href=#33--232292-shufflebench-a-benchmark-for-large-scale-data-shuffling-operations-with-distributed-stream-processing-frameworks-sören-henning-et-al-2024>(3/3 | 232/292) ShuffleBench: A Benchmark for Large-Scale Data Shuffling Operations with Distributed Stream Processing Frameworks (Sören Henning et al., 2024)</a></li></ul></li><li><a href=#cssd-2>cs.SD (2)</a><ul><li><a href=#12--233292-a-study-of-dropout-induced-modality-bias-on-robustness-to-missing-video-frames-for-audio-visual-speech-recognition-yusheng-dai-et-al-2024>(1/2 | 233/292) A Study of Dropout-Induced Modality Bias on Robustness to Missing Video Frames for Audio-Visual Speech Recognition (Yusheng Dai et al., 2024)</a></li><li><a href=#22--234292-a-detailed-audio-text-data-simulation-pipeline-using-single-event-sounds-xuenan-xu-et-al-2024>(2/2 | 234/292) A Detailed Audio-Text Data Simulation Pipeline using Single-Event Sounds (Xuenan Xu et al., 2024)</a></li></ul></li><li><a href=#csni-3>cs.NI (3)</a><ul><li><a href=#13--235292-itrpl-an-intelligent-and-trusted-rpl-protocol-based-on-multi-agent-reinforcement-learning-debasmita-dey-et-al-2024>(1/3 | 235/292) iTRPL: An Intelligent and Trusted RPL Protocol based on Multi-Agent Reinforcement Learning (Debasmita Dey et al., 2024)</a></li><li><a href=#23--236292-performance-evaluation-of-conditional-handover-in-5g-systems-under-fading-scenario-souvik-deb-et-al-2024>(2/3 | 236/292) Performance evaluation of conditional handover in 5G systems under fading scenario (Souvik Deb et al., 2024)</a></li><li><a href=#33--237292-super-resolution-on-network-telemetry-time-series-fengchen-gong-et-al-2024>(3/3 | 237/292) Super-resolution on network telemetry time series (Fengchen Gong et al., 2024)</a></li></ul></li><li><a href=#csdc-5>cs.DC (5)</a><ul><li><a href=#15--238292-optimizing-cnn-using-hpc-tools-shahrin-rahman-2024>(1/5 | 238/292) Optimizing CNN Using HPC Tools (Shahrin Rahman, 2024)</a></li><li><a href=#25--239292-greenbytes-intelligent-energy-estimation-for-edge-cloud-kasra-kassai-et-al-2024>(2/5 | 239/292) GreenBytes: Intelligent Energy Estimation for Edge-Cloud (Kasra Kassai et al., 2024)</a></li><li><a href=#35--240292-parendi-thousand-way-parallel-rtl-simulation-mahyar-emami-et-al-2024>(3/5 | 240/292) Parendi: Thousand-Way Parallel RTL Simulation (Mahyar Emami et al., 2024)</a></li><li><a href=#45--241292-fedclust-optimizing-federated-learning-on-non-iid-data-through-weight-driven-client-clustering-md-sirajul-islam-et-al-2024>(4/5 | 241/292) FedClust: Optimizing Federated Learning on Non-IID Data through Weight-Driven Client Clustering (Md Sirajul Islam et al., 2024)</a></li><li><a href=#55--242292-improvements--evaluations-on-the-mlcommons-cloudmask-benchmark-varshitha-chennamsetti-et-al-2024>(5/5 | 242/292) Improvements & Evaluations on the MLCommons CloudMask Benchmark (Varshitha Chennamsetti et al., 2024)</a></li></ul></li><li><a href=#csce-2>cs.CE (2)</a><ul><li><a href=#12--243292-effect-of-turbulent-diffusion-in-modeling-anaerobic-digestion-jeremy-z-yan-et-al-2024>(1/2 | 243/292) Effect of turbulent diffusion in modeling anaerobic digestion (Jeremy Z. Yan et al., 2024)</a></li><li><a href=#22--244292-sentiment-driven-prediction-of-financial-returns-a-bayesian-enhanced-finbert-approach-raffaele-giuseppe-cestari-et-al-2024>(2/2 | 244/292) Sentiment-driven prediction of financial returns: a Bayesian-enhanced FinBERT approach (Raffaele Giuseppe Cestari et al., 2024)</a></li></ul></li><li><a href=#eesssy-6>eess.SY (6)</a><ul><li><a href=#16--245292-model-free-load-frequency-control-of-nonlinear-power-systems-based-on-deep-reinforcement-learning-xiaodi-chen-et-al-2024>(1/6 | 245/292) Model-Free Load Frequency Control of Nonlinear Power Systems Based on Deep Reinforcement Learning (Xiaodi Chen et al., 2024)</a></li><li><a href=#26--246292-controller-adaptation-via-learning-solutions-of-contextual-bayesian-optimization-viet-anh-le-et-al-2024>(2/6 | 246/292) Controller Adaptation via Learning Solutions of Contextual Bayesian Optimization (Viet-Anh Le et al., 2024)</a></li><li><a href=#36--247292-tensor-power-flow-formulations-for-multidimensional-analyses-in-distribution-systems-edgar-mauricio-salazar-duque-et-al-2024>(3/6 | 247/292) Tensor Power Flow Formulations for Multidimensional Analyses in Distribution Systems (Edgar Mauricio Salazar Duque et al., 2024)</a></li><li><a href=#46--248292-closed-loop-performance-optimization-of-model-predictive-control-with-robustness-guarantees-riccardo-zuliani-et-al-2024>(4/6 | 248/292) Closed-loop Performance Optimization of Model Predictive Control with Robustness Guarantees (Riccardo Zuliani et al., 2024)</a></li><li><a href=#56--249292-control-barrier-functions-for-linear-continuous-time-input-delay-systems-with-limited-horizon-previewable-disturbances-tarun-pati-et-al-2024>(5/6 | 249/292) Control Barrier Functions for Linear Continuous-Time Input-Delay Systems with Limited-Horizon Previewable Disturbances (Tarun Pati et al., 2024)</a></li><li><a href=#66--250292-a-crosstalk-aware-timing-prediction-method-in-routing-leilei-jin-et-al-2024>(6/6 | 250/292) A Crosstalk-Aware Timing Prediction Method in Routing (Leilei Jin et al., 2024)</a></li></ul></li><li><a href=#csgt-4>cs.GT (4)</a><ul><li><a href=#14--251292-rl-cfr-improving-action-abstraction-for-imperfect-information-extensive-form-games-with-reinforcement-learning-boning-li-et-al-2024>(1/4 | 251/292) RL-CFR: Improving Action Abstraction for Imperfect Information Extensive-Form Games with Reinforcement Learning (Boning Li et al., 2024)</a></li><li><a href=#24--252292-mechanism-for-decision-aware-collaborative-federated-learning-a-pitfall-of-shapley-values-meng-qi-et-al-2024>(2/4 | 252/292) Mechanism for Decision-aware Collaborative Federated Learning: A Pitfall of Shapley Values (Meng Qi et al., 2024)</a></li><li><a href=#34--253292-conflict-and-fairness-in-resource-allocation-susobhan-bandopadhyay-et-al-2024>(3/4 | 253/292) Conflict and Fairness in Resource Allocation (Susobhan Bandopadhyay et al., 2024)</a></li><li><a href=#44--254292-extensive-form-game-solving-via-blackwell-approachability-on-treeplexes-darshan-chakrabarti-et-al-2024>(4/4 | 254/292) Extensive-Form Game Solving via Blackwell Approachability on Treeplexes (Darshan Chakrabarti et al., 2024)</a></li></ul></li><li><a href=#physicsbio-ph-1>physics.bio-ph (1)</a><ul><li><a href=#11--255292-preference-optimization-of-protein-language-models-as-a-multi-objective-binder-design-paradigm-pouria-mistani-et-al-2024>(1/1 | 255/292) Preference optimization of protein language models as a multi-objective binder design paradigm (Pouria Mistani et al., 2024)</a></li></ul></li><li><a href=#cssi-2>cs.SI (2)</a><ul><li><a href=#12--256292-generating-insights-about-financial-asks-from-reddit-posts-and-user-interactions-sachin-thukral-et-al-2024>(1/2 | 256/292) Generating insights about financial asks from Reddit posts and user interactions (Sachin Thukral et al., 2024)</a></li><li><a href=#22--257292-improving-link-prediction-accuracy-of-network-embedding-algorithms-via-rich-node-attribute-information-weiwei-gu-et-al-2024>(2/2 | 257/292) Improving link prediction accuracy of network embedding algorithms via rich node attribute information (Weiwei Gu et al., 2024)</a></li></ul></li><li><a href=#physicsflu-dyn-1>physics.flu-dyn (1)</a><ul><li><a href=#11--258292-jax-sph-a-differentiable-smoothed-particle-hydrodynamics-framework-artur-p-toshev-et-al-2024>(1/1 | 258/292) JAX-SPH: A Differentiable Smoothed Particle Hydrodynamics Framework (Artur P. Toshev et al., 2024)</a></li></ul></li><li><a href=#astro-phco-1>astro-ph.CO (1)</a><ul><li><a href=#11--259292-testing-an-entropy-estimator-related-to-the-dynamical-state-of-galaxy-clusters-j-m-zúniga-et-al-2024>(1/1 | 259/292) Testing an entropy estimator related to the dynamical state of galaxy clusters (J. M. Zúniga et al., 2024)</a></li></ul></li><li><a href=#csit-5>cs.IT (5)</a><ul><li><a href=#15--260292-molecular-arithmetic-coding-mac-for-internet-of-bio-nano-things-iobnt-melih-şahin-et-al-2024>(1/5 | 260/292) Molecular Arithmetic Coding (MAC) for Internet of Bio-Nano Things (IoBNT) (Melih Şahin et al., 2024)</a></li><li><a href=#25--261292-matched-filter-precoded-rate-splitting-multiple-access-a-simple-and-energy-efficient-design-hui-zhao-et-al-2024>(2/5 | 261/292) Matched-filter Precoded Rate Splitting Multiple Access: A Simple and Energy-efficient Design (Hui Zhao et al., 2024)</a></li><li><a href=#35--262292-secure-mimo-communication-relying-on-movable-antennas-jun-tang-et-al-2024>(3/5 | 262/292) Secure MIMO Communication Relying on Movable Antennas (Jun Tang et al., 2024)</a></li><li><a href=#45--263292-rectangular-rotational-invariant-estimator-for-high-rank-matrix-estimation-farzad-pourkamali-et-al-2024>(4/5 | 263/292) Rectangular Rotational Invariant Estimator for High-Rank Matrix Estimation (Farzad Pourkamali et al., 2024)</a></li><li><a href=#55--264292-optimal-denial-of-service-attacks-against-status-updating-saad-kriouile-et-al-2024>(5/5 | 264/292) Optimal Denial-of-Service Attacks Against Status Updating (Saad Kriouile et al., 2024)</a></li></ul></li><li><a href=#csar-2>cs.AR (2)</a><ul><li><a href=#12--265292-virtuoso-an-open-source-comprehensive-and-modular-simulation-framework-for-virtual-memory-research-konstantinos-kanellopoulos-et-al-2024>(1/2 | 265/292) Virtuoso: An Open-Source, Comprehensive and Modular Simulation Framework for Virtual Memory Research (Konstantinos Kanellopoulos et al., 2024)</a></li><li><a href=#22--266292-a-methodology-to-automatically-optimize-dynamic-memory-managers-applying-grammatical-evolution-josé-l-risco-martín-et-al-2024>(2/2 | 266/292) A methodology to automatically optimize dynamic memory managers applying grammatical evolution (José L. Risco-Martín et al., 2024)</a></li></ul></li><li><a href=#csds-6>cs.DS (6)</a><ul><li><a href=#16--267292-optimizing-inventory-placement-for-a-downstream-online-matching-problem-boris-epstein-et-al-2024>(1/6 | 267/292) Optimizing Inventory Placement for a Downstream Online Matching Problem (Boris Epstein et al., 2024)</a></li><li><a href=#26--268292-algorithms-and-complexity-for-path-covers-of-temporal-dags-when-is-dilworth-dynamic-dibyayan-chakraborty-et-al-2024>(2/6 | 268/292) Algorithms and complexity for path covers of temporal DAGs: when is Dilworth dynamic? (Dibyayan Chakraborty et al., 2024)</a></li><li><a href=#36--269292-np-completeness-for-the-space-optimality-of-double-array-tries-hideo-bannai-et-al-2024>(3/6 | 269/292) NP-Completeness for the Space-Optimality of Double-Array Tries (Hideo Bannai et al., 2024)</a></li><li><a href=#46--270292-time-aware-projections-truly-node-private-graph-statistics-under-continual-observation-palak-jain-et-al-2024>(4/6 | 270/292) Time-Aware Projections: Truly Node-Private Graph Statistics under Continual Observation (Palak Jain et al., 2024)</a></li><li><a href=#56--271292-a-simple-and-near-optimal-algorithm-for-directed-expander-decompositions-aurelio-l-sulser-et-al-2024>(5/6 | 271/292) A Simple and Near-Optimal Algorithm for Directed Expander Decompositions (Aurelio L. Sulser et al., 2024)</a></li><li><a href=#66--272292-switching-classes-characterization-and-computation-dhanyamol-antony-et-al-2024>(6/6 | 272/292) Switching Classes: Characterization and Computation (Dhanyamol Antony et al., 2024)</a></li></ul></li><li><a href=#eesssp-1>eess.SP (1)</a><ul><li><a href=#11--273292-comparison-of-gait-phase-detection-using-traditional-machine-learning-and-deep-learning-techniques-farhad-nazari-et-al-2024>(1/1 | 273/292) Comparison of gait phase detection using traditional machine learning and deep learning techniques (Farhad Nazari et al., 2024)</a></li></ul></li><li><a href=#mathoc-2>math.OC (2)</a><ul><li><a href=#12--274292-locodl-communication-efficient-distributed-learning-with-local-training-and-compression-laurent-condat-et-al-2024>(1/2 | 274/292) LoCoDL: Communication-Efficient Distributed Learning with Local Training and Compression (Laurent Condat et al., 2024)</a></li><li><a href=#22--275292-memetic-differential-evolution-methods-for-semi-supervised-clustering-pierluigi-mansueto-et-al-2024>(2/2 | 275/292) Memetic Differential Evolution Methods for Semi-Supervised Clustering (Pierluigi Mansueto et al., 2024)</a></li></ul></li><li><a href=#csms-1>cs.MS (1)</a><ul><li><a href=#11--276292-genml-a-python-library-to-generate-the-mittag-leffler-correlated-noise-xiang-qu-et-al-2024>(1/1 | 276/292) GenML: A Python Library to Generate the Mittag-Leffler Correlated Noise (Xiang Qu et al., 2024)</a></li></ul></li><li><a href=#statml-3>stat.ML (3)</a><ul><li><a href=#13--277292-efficient-cnn-lstm-based-parameter-estimation-of-levy-driven-stochastic-differential-equations-shuaiyu-li-et-al-2024>(1/3 | 277/292) Efficient CNN-LSTM based Parameter Estimation of Levy Driven Stochastic Differential Equations (Shuaiyu Li et al., 2024)</a></li><li><a href=#23--278292-signature-isolation-forest-guillaume-staerman-et-al-2024>(2/3 | 278/292) Signature Isolation Forest (Guillaume Staerman et al., 2024)</a></li><li><a href=#33--279292-fundamental-limits-of-non-linear-low-rank-matrix-estimation-pierre-mergny-et-al-2024>(3/3 | 279/292) Fundamental limits of Non-Linear Low-Rank Matrix Estimation (Pierre Mergny et al., 2024)</a></li></ul></li><li><a href=#physicsmed-ph-1>physics.med-ph (1)</a><ul><li><a href=#11--280292-understanding-the-pulsar-effect-in-combined-radiotherapy-and-immunotherapy-through-attention-mechanisms-with-a-transformer-model-hao-peng-et-al-2024>(1/1 | 280/292) Understanding the PULSAR Effect in Combined Radiotherapy and Immunotherapy through Attention Mechanisms with a Transformer Model (Hao Peng et al., 2024)</a></li></ul></li><li><a href=#csdl-1>cs.DL (1)</a><ul><li><a href=#11--281292-brainknow----extracting-linking-and-associating-neuroscience-knowledge-cunqing-huangfu-et-al-2024>(1/1 | 281/292) BrainKnow &ndash; Extracting, Linking, and Associating Neuroscience Knowledge (Cunqing Huangfu et al., 2024)</a></li></ul></li><li><a href=#cspl-2>cs.PL (2)</a><ul><li><a href=#12--282292-conjugate-operators-for-transparent-explorable-research-outputs-joseph-bond-et-al-2024>(1/2 | 282/292) Conjugate operators for transparent, explorable research outputs (Joseph Bond et al., 2024)</a></li><li><a href=#22--283292-message-observing-sessions-ryan-kavanagh-et-al-2024>(2/2 | 283/292) Message-Observing Sessions (Ryan Kavanagh et al., 2024)</a></li></ul></li><li><a href=#q-biomn-1>q-bio.MN (1)</a><ul><li><a href=#11--284292-cell-reprogramming-design-by-transfer-learning-of-functional-transcriptional-networks-thomas-p-wytock-et-al-2024>(1/1 | 284/292) Cell reprogramming design by transfer learning of functional transcriptional networks (Thomas P. Wytock et al., 2024)</a></li></ul></li><li><a href=#cscy-2>cs.CY (2)</a><ul><li><a href=#12--285292-literature-review-of-current-sustainability-assessment-frameworks-and-approaches-for-organizations-sarah-farahdel-et-al-2024>(1/2 | 285/292) Literature Review of Current Sustainability Assessment Frameworks and Approaches for Organizations (Sarah Farahdel et al., 2024)</a></li><li><a href=#22--286292-disciplining-deliberation-a-sociotechnical-perspective-on-machine-learning-trade-offs-sina-fazelpour-2024>(2/2 | 286/292) Disciplining deliberation: a sociotechnical perspective on machine learning trade-offs (Sina Fazelpour, 2024)</a></li></ul></li><li><a href=#csoh-1>cs.OH (1)</a><ul><li><a href=#11--287292-new-algorithms-for-the-simplification-of-multiple-trajectories-under-bandwidth-constraints-gilles-dejaegere-et-al-2024>(1/1 | 287/292) New algorithms for the simplification of multiple trajectories under bandwidth constraints (Gilles Dejaegere et al., 2024)</a></li></ul></li><li><a href=#csma-2>cs.MA (2)</a><ul><li><a href=#12--288292-dynamics-of-moral-behavior-in-heterogeneous-populations-of-learning-agents-elizaveta-tennant-et-al-2024>(1/2 | 288/292) Dynamics of Moral Behavior in Heterogeneous Populations of Learning Agents (Elizaveta Tennant et al., 2024)</a></li><li><a href=#22--289292-distributed-multi-objective-optimization-in-cyber-physical-energy-systems-sanja-stark-et-al-2024>(2/2 | 289/292) Distributed Multi-objective Optimization in Cyber-Physical Energy Systems (Sanja Stark et al., 2024)</a></li></ul></li><li><a href=#cscg-1>cs.CG (1)</a><ul><li><a href=#11--290292-a-clique-based-separator-for-intersection-graphs-of-geodesic-disks-in-mathbbr2-boris-aronov-et-al-2024>(1/1 | 290/292) A Clique-Based Separator for Intersection Graphs of Geodesic Disks in $\mathbb{R}^2$ (Boris Aronov et al., 2024)</a></li></ul></li><li><a href=#cscc-1>cs.CC (1)</a><ul><li><a href=#11--291292-on-12-domination-in-interval-and-circle-graphs-mohsen-alambardar-meybodi-et-al-2024>(1/1 | 291/292) On $[1,2]$-Domination in Interval and Circle Graphs (Mohsen Alambardar Meybodi et al., 2024)</a></li></ul></li><li><a href=#q-biobm-1>q-bio.BM (1)</a><ul><li><a href=#11--292292-sgnet-folding-symmetrical-protein-complex-with-deep-learning-zhaoqun-li-et-al-2024>(1/1 | 292/292) SGNet: Folding Symmetrical Protein Complex with Deep Learning (Zhaoqun Li et al., 2024)</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:contact.to.akitenkrad@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>contact.to.akitenkrad@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/akitenkrad-blog/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2020 Akitenkrad.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/akitenkrad-blog/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/jquery-3.4.1.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/popper.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/bootstrap.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/navbar.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/plyr.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=https://akitenkrad.github.io/akitenkrad-blog/js/single.js></script><script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.css><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.js></script><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!0})})</script></body></html>