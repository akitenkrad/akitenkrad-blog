<!doctype html><html><head><title>arXiv @ 2024.03.03</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/bootstrap.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/main.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/navbar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/plyr.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/flag-icon.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/custom/style.css><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM crossorigin=anonymous><script async src=https://cdn.jsdelivr.net/npm/es-module-shims@1/dist/es-module-shims.min.js crossorigin=anonymous></script><script type=importmap>
{
  "imports": {
    "@popperjs/core": "https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.8/dist/esm/popper.min.js",
    "bootstrap": "https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.esm.min.js"
  }
}
</script><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js integrity=sha384-geWF76RCwLtnZ8qwWowPQNguL3RmwHVBC9FhGdlKrxdiJJigb/j/68SIy3Te4Bkz crossorigin=anonymous></script><script type=module src=https://akitenkrad.github.io/akitenkrad-blog/js/custom/custom.js></script><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/fontawesome/css/all.min.css><link rel=icon type=image/png href=https://akitenkrad.github.io/akitenkrad-blog/images/favicons/favicon-96x96_huf1ee13f0caf27d1547f91fb46207d708_13005_42x0_resize_box_3.png><meta property="og:title" content="arXiv @ 2024.03.03"><meta property="og:description" content="Primary Categories astro-ph.IM (1) cond-mat.mtrl-sci (1) cs.AI (7) cs.AR (1) cs.CC (1) cs.CG (1) cs.CL (37) cs.CR (9) cs.CV (46) cs.CY (1) cs.DB (2) cs.DC (3) cs.DS (4) cs.GR (1) cs.GT (4) cs.HC (8) cs.IR (7) cs.IT (9) cs.LG (40) cs.LO (2) cs.MA (1) cs.NE (1) cs.NI (2) cs.RO (16) cs.SD (2) cs.SE (4) cs.SI (2) eess.AS (2) eess.IV (3) eess.SP (2) eess.SY (7) math.CO (1) math.MG (1) math.NA (4) math."><meta property="og:type" content="article"><meta property="og:url" content="https://akitenkrad.github.io/akitenkrad-blog/posts/arxiv/202403/20240303000000/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-03-03T00:00:00+00:00"><meta property="article:modified_time" content="2024-03-03T00:00:00+00:00"><meta name=description content="arXiv @ 2024.03.03"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/single.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/sidebar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/style.css><script type=text/javascript src=//ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js></script><script language=JavaScript>$(document).ready(function(){$("a[href^='http']:not([href*='"+location.hostname+"'])").attr("target","_blank")})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-1MYYZQG0WE"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-1MYYZQG0WE",{anonymize_ip:!1})}</script></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow" style=position:fixed><div class=container><button class="navbar-toggler navbar-light navbar-for-sidebar" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span>
</button>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav mr-auto"><li class=nav-item><a class=navbar-brand href=/akitenkrad-blog><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png alt=Logo>
Akitenkrad's Blog</a></li></ul><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=/akitenkrad-blog#home>Home</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#about>About</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#recent-posts>Recent Posts</a></li><li class=nav-item><a class=nav-link id=blog-link href=https://akitenkrad.github.io/akitenkrad-blog/posts>Posts</a></li><li class=nav-item><a class=nav-link id=tag-link href=https://akitenkrad.github.io/akitenkrad-blog/tags>Tags</a></li></ul></div></div><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/akitenkrad-blog/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/>Papers</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202205/>2022.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202205/20220518224923/ title="A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks">A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220508162318/ title="A Deep Cascade Model for Multi-Document Reading Comprehension">A Deep Cascade Model for Multi-Document Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220506021208/ title="A Primer in BERTology: What We Know About How BERT Works">A Primer in BERTology: What We Know About How BERT Works</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220529131339/ title="Attention Is All You Need">Attention Is All You Need</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220503010000/ title="DeBERTa: Decoding-Enhanced BERT with Disentangled Attention">DeBERTa: Decoding-Enhanced BERT with Disentangled Attention</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220505222900/ title="Dense Passage Retrieval for Open-Domain Question Answering">Dense Passage Retrieval for Open-Domain Question Answering</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220511010217/ title="Multi-Style Generative Reading Comprehension">Multi-Style Generative Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220530102936/ title="Neural Machine Translation of Rare Words with Subword Units">Neural Machine Translation of Rare Words with Subword Units</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220523223206/ title="RoBERTa: A Robustly Optimized BERT Pretraining Approach">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220520124748/ title="Semi-Supervised Classification with Graph Convolutional Networks">Semi-Supervised Classification with Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220509110738/ title="Survey on graph embeddings and their applications to machine learning problems on graphs">Survey on graph embeddings and their applications to machine learning problems on graphs</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220514151839/ title="UnitedQA: A Hybrid Approach for Open Domain Question Answering">UnitedQA: A Hybrid Approach for Open Domain Question Answering</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202206/>2022.06</a><ul><li><a href=/akitenkrad-blog/posts/papers/202206/20220612105422/ title="Attributed Network Embedding for Learning in a Dynamic Environment">Attributed Network Embedding for Learning in a Dynamic Environment</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220608085622/ title="CodeBERT: A Pre-Trained Model for Programming and Natural Languages">CodeBERT: A Pre-Trained Model for Programming and Natural Languages</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220618223844/ title="High-order Proximity Preserved Embedding for Dynamic Networks">High-order Proximity Preserved Embedding for Dynamic Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220602171700/ title="S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension">S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202207/>2022.07</a><ul><li><a href=/akitenkrad-blog/posts/papers/202207/20220727145036/ title="Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions">Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions</a></li><li><a href=/akitenkrad-blog/posts/papers/202207/20220726163444/ title="Dynamic Network Embedding Survey">Dynamic Network Embedding Survey</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/papers/202208/20220802103319/ title="Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN">Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/papers/202209/20220909180827/ title="Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting">Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202305/>2023.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202305/20230513094804/ title="Improving Language Understanding by Generative Pre-Training">Improving Language Understanding by Generative Pre-Training</a></li><li><a href=/akitenkrad-blog/posts/papers/202305/20230514200241/ title="Language Models are Unsupervised Multitask Learners">Language Models are Unsupervised Multitask Learners</a></li></ul></li><li><a href=/akitenkrad-blog/posts/papers/202306/ title=2023.06>2023.06</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/>arXiv</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202307/>2023.07</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230701000000/ title="arXiv @ 2023.07.01">arXiv @ 2023.07.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230702000000/ title="arXiv @ 2023.07.02">arXiv @ 2023.07.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230703000000/ title="arXiv @ 2023.07.03">arXiv @ 2023.07.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230704000000/ title="arXiv @ 2023.07.04">arXiv @ 2023.07.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230705000000/ title="arXiv @ 2023.07.05">arXiv @ 2023.07.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230706000000/ title="arXiv @ 2023.07.06">arXiv @ 2023.07.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230707000000/ title="arXiv @ 2023.07.07">arXiv @ 2023.07.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230708000000/ title="arXiv @ 2023.07.08">arXiv @ 2023.07.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230709000000/ title="arXiv @ 2023.07.09">arXiv @ 2023.07.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230710000000/ title="arXiv @ 2023.07.10">arXiv @ 2023.07.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230711000000/ title="arXiv @ 2023.07.11">arXiv @ 2023.07.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230712000000/ title="arXiv @ 2023.07.12">arXiv @ 2023.07.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230713000000/ title="arXiv @ 2023.07.13">arXiv @ 2023.07.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230714000000/ title="arXiv @ 2023.07.14">arXiv @ 2023.07.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230715000000/ title="arXiv @ 2023.07.15">arXiv @ 2023.07.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230716000000/ title="arXiv @ 2023.07.16">arXiv @ 2023.07.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230717000000/ title="arXiv @ 2023.07.17">arXiv @ 2023.07.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230718000000/ title="arXiv @ 2023.07.18">arXiv @ 2023.07.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230719000000/ title="arXiv @ 2023.07.19">arXiv @ 2023.07.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230720000000/ title="arXiv @ 2023.07.20">arXiv @ 2023.07.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230721000000/ title="arXiv @ 2023.07.21">arXiv @ 2023.07.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230722000000/ title="arXiv @ 2023.07.22">arXiv @ 2023.07.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230723000000/ title="arXiv @ 2023.07.23">arXiv @ 2023.07.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230724000000/ title="arXiv @ 2023.07.24">arXiv @ 2023.07.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230725000000/ title="arXiv @ 2023.07.25">arXiv @ 2023.07.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230726000000/ title="arXiv @ 2023.07.26">arXiv @ 2023.07.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230727000000/ title="arXiv @ 2023.07.27">arXiv @ 2023.07.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230728000000/ title="arXiv @ 2023.07.28">arXiv @ 2023.07.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230729000000/ title="arXiv @ 2023.07.29">arXiv @ 2023.07.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230730000000/ title="arXiv @ 2023.07.30">arXiv @ 2023.07.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230731000000/ title="arXiv @ 2023.07.31">arXiv @ 2023.07.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202308/>2023.08</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230801000000/ title="arXiv @ 2023.08.01">arXiv @ 2023.08.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230802000000/ title="arXiv @ 2023.08.02">arXiv @ 2023.08.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230803000000/ title="arXiv @ 2023.08.03">arXiv @ 2023.08.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230804000000/ title="arXiv @ 2023.08.04">arXiv @ 2023.08.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230805000000/ title="arXiv @ 2023.08.05">arXiv @ 2023.08.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230806000000/ title="arXiv @ 2023.08.06">arXiv @ 2023.08.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230807000000/ title="arXiv @ 2023.08.07">arXiv @ 2023.08.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230808000000/ title="arXiv @ 2023.08.08">arXiv @ 2023.08.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230809000000/ title="arXiv @ 2023.08.09">arXiv @ 2023.08.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230810000000/ title="arXiv @ 2023.08.10">arXiv @ 2023.08.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230811000000/ title="arXiv @ 2023.08.11">arXiv @ 2023.08.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230812000000/ title="arXiv @ 2023.08.12">arXiv @ 2023.08.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230813000000/ title="arXiv @ 2023.08.13">arXiv @ 2023.08.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230814000000/ title="arXiv @ 2023.08.14">arXiv @ 2023.08.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230815000000/ title="arXiv @ 2023.08.15">arXiv @ 2023.08.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230816000000/ title="arXiv @ 2023.08.16">arXiv @ 2023.08.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230817000000/ title="arXiv @ 2023.08.17">arXiv @ 2023.08.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230818000000/ title="arXiv @ 2023.08.18">arXiv @ 2023.08.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230819000000/ title="arXiv @ 2023.08.19">arXiv @ 2023.08.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230820000000/ title="arXiv @ 2023.08.20">arXiv @ 2023.08.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230821000000/ title="arXiv @ 2023.08.21">arXiv @ 2023.08.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230822000000/ title="arXiv @ 2023.08.22">arXiv @ 2023.08.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230823000000/ title="arXiv @ 2023.08.23">arXiv @ 2023.08.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230824000000/ title="arXiv @ 2023.08.24">arXiv @ 2023.08.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230825000000/ title="arXiv @ 2023.08.25">arXiv @ 2023.08.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230826000000/ title="arXiv @ 2023.08.26">arXiv @ 2023.08.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230827000000/ title="arXiv @ 2023.08.27">arXiv @ 2023.08.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230828000000/ title="arXiv @ 2023.08.28">arXiv @ 2023.08.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230829000000/ title="arXiv @ 2023.08.29">arXiv @ 2023.08.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230830000000/ title="arXiv @ 2023.08.30">arXiv @ 2023.08.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230831000000/ title="arXiv @ 2023.08.31">arXiv @ 2023.08.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202309/>2023.09</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230901000000/ title="arXiv @ 2023.09.01">arXiv @ 2023.09.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230902000000/ title="arXiv @ 2023.09.02">arXiv @ 2023.09.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230903000000/ title="arXiv @ 2023.09.03">arXiv @ 2023.09.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230904000000/ title="arXiv @ 2023.09.04">arXiv @ 2023.09.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230905000000/ title="arXiv @ 2023.09.05">arXiv @ 2023.09.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230906000000/ title="arXiv @ 2023.09.06">arXiv @ 2023.09.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230907000000/ title="arXiv @ 2023.09.07">arXiv @ 2023.09.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230908000000/ title="arXiv @ 2023.09.08">arXiv @ 2023.09.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230909000000/ title="arXiv @ 2023.09.09">arXiv @ 2023.09.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230910000000/ title="arXiv @ 2023.09.10">arXiv @ 2023.09.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230911000000/ title="arXiv @ 2023.09.11">arXiv @ 2023.09.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230912000000/ title="arXiv @ 2023.09.12">arXiv @ 2023.09.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230913000000/ title="arXiv @ 2023.09.13">arXiv @ 2023.09.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230914000000/ title="arXiv @ 2023.09.14">arXiv @ 2023.09.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230915000000/ title="arXiv @ 2023.09.15">arXiv @ 2023.09.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230916000000/ title="arXiv @ 2023.09.16">arXiv @ 2023.09.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230917000000/ title="arXiv @ 2023.09.17">arXiv @ 2023.09.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230918000000/ title="arXiv @ 2023.09.18">arXiv @ 2023.09.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230919000000/ title="arXiv @ 2023.09.19">arXiv @ 2023.09.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230920000000/ title="arXiv @ 2023.09.20">arXiv @ 2023.09.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230921000000/ title="arXiv @ 2023.09.21">arXiv @ 2023.09.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230922000000/ title="arXiv @ 2023.09.22">arXiv @ 2023.09.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230923000000/ title="arXiv @ 2023.09.23">arXiv @ 2023.09.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230924000000/ title="arXiv @ 2023.09.24">arXiv @ 2023.09.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230925000000/ title="arXiv @ 2023.09.25">arXiv @ 2023.09.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230926000000/ title="arXiv @ 2023.09.26">arXiv @ 2023.09.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230927000000/ title="arXiv @ 2023.09.27">arXiv @ 2023.09.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230928000000/ title="arXiv @ 2023.09.28">arXiv @ 2023.09.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230929000000/ title="arXiv @ 2023.09.29">arXiv @ 2023.09.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230930000000/ title="arXiv @ 2023.09.30">arXiv @ 2023.09.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202310/>2023.1</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231001000000/ title="arXiv @ 2023.10.01">arXiv @ 2023.10.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231002000000/ title="arXiv @ 2023.10.02">arXiv @ 2023.10.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231003000000/ title="arXiv @ 2023.10.03">arXiv @ 2023.10.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231004000000/ title="arXiv @ 2023.10.04">arXiv @ 2023.10.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231005000000/ title="arXiv @ 2023.10.05">arXiv @ 2023.10.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231006000000/ title="arXiv @ 2023.10.06">arXiv @ 2023.10.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231007000000/ title="arXiv @ 2023.10.07">arXiv @ 2023.10.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231008000000/ title="arXiv @ 2023.10.08">arXiv @ 2023.10.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231009000000/ title="arXiv @ 2023.10.09">arXiv @ 2023.10.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231010000000/ title="arXiv @ 2023.10.10">arXiv @ 2023.10.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231011000000/ title="arXiv @ 2023.10.11">arXiv @ 2023.10.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231012000000/ title="arXiv @ 2023.10.12">arXiv @ 2023.10.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231013000000/ title="arXiv @ 2023.10.13">arXiv @ 2023.10.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231014000000/ title="arXiv @ 2023.10.14">arXiv @ 2023.10.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231015000000/ title="arXiv @ 2023.10.15">arXiv @ 2023.10.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231016000000/ title="arXiv @ 2023.10.16">arXiv @ 2023.10.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231017000000/ title="arXiv @ 2023.10.17">arXiv @ 2023.10.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231018000000/ title="arXiv @ 2023.10.18">arXiv @ 2023.10.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231019000000/ title="arXiv @ 2023.10.19">arXiv @ 2023.10.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231020000000/ title="arXiv @ 2023.10.20">arXiv @ 2023.10.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231021000000/ title="arXiv @ 2023.10.21">arXiv @ 2023.10.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231022000000/ title="arXiv @ 2023.10.22">arXiv @ 2023.10.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231023000000/ title="arXiv @ 2023.10.23">arXiv @ 2023.10.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231024000000/ title="arXiv @ 2023.10.24">arXiv @ 2023.10.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231025000000/ title="arXiv @ 2023.10.25">arXiv @ 2023.10.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231026000000/ title="arXiv @ 2023.10.26">arXiv @ 2023.10.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231027000000/ title="arXiv @ 2023.10.27">arXiv @ 2023.10.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231028000000/ title="arXiv @ 2023.10.28">arXiv @ 2023.10.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231029000000/ title="arXiv @ 2023.10.29">arXiv @ 2023.10.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231030000000/ title="arXiv @ 2023.10.30">arXiv @ 2023.10.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231031000000/ title="arXiv @ 2023.10.31">arXiv @ 2023.10.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202311/>2023.11</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231101000000/ title="arXiv @ 2023.11.01">arXiv @ 2023.11.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231102000000/ title="arXiv @ 2023.11.02">arXiv @ 2023.11.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231103000000/ title="arXiv @ 2023.11.03">arXiv @ 2023.11.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231104000000/ title="arXiv @ 2023.11.04">arXiv @ 2023.11.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231105000000/ title="arXiv @ 2023.11.05">arXiv @ 2023.11.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231106000000/ title="arXiv @ 2023.11.06">arXiv @ 2023.11.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231107000000/ title="arXiv @ 2023.11.07">arXiv @ 2023.11.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231108000000/ title="arXiv @ 2023.11.08">arXiv @ 2023.11.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231109000000/ title="arXiv @ 2023.11.09">arXiv @ 2023.11.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231110000000/ title="arXiv @ 2023.11.10">arXiv @ 2023.11.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231111000000/ title="arXiv @ 2023.11.11">arXiv @ 2023.11.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231112000000/ title="arXiv @ 2023.11.12">arXiv @ 2023.11.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231113000000/ title="arXiv @ 2023.11.13">arXiv @ 2023.11.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231114000000/ title="arXiv @ 2023.11.14">arXiv @ 2023.11.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231115000000/ title="arXiv @ 2023.11.15">arXiv @ 2023.11.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231116000000/ title="arXiv @ 2023.11.16">arXiv @ 2023.11.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231117000000/ title="arXiv @ 2023.11.17">arXiv @ 2023.11.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231118000000/ title="arXiv @ 2023.11.18">arXiv @ 2023.11.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231119000000/ title="arXiv @ 2023.11.19">arXiv @ 2023.11.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231120000000/ title="arXiv @ 2023.11.20">arXiv @ 2023.11.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231121000000/ title="arXiv @ 2023.11.21">arXiv @ 2023.11.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231122000000/ title="arXiv @ 2023.11.22">arXiv @ 2023.11.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231123000000/ title="arXiv @ 2023.11.23">arXiv @ 2023.11.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231124000000/ title="arXiv @ 2023.11.24">arXiv @ 2023.11.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231125000000/ title="arXiv @ 2023.11.25">arXiv @ 2023.11.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231126000000/ title="arXiv @ 2023.11.26">arXiv @ 2023.11.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231127000000/ title="arXiv @ 2023.11.27">arXiv @ 2023.11.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231128000000/ title="arXiv @ 2023.11.28">arXiv @ 2023.11.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231129000000/ title="arXiv @ 2023.11.29">arXiv @ 2023.11.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231130000000/ title="arXiv @ 2023.11.30">arXiv @ 2023.11.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202312/>2023.12</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231201000000/ title="arXiv @ 2023.12.01">arXiv @ 2023.12.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231202000000/ title="arXiv @ 2023.12.02">arXiv @ 2023.12.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231203000000/ title="arXiv @ 2023.12.03">arXiv @ 2023.12.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231204000000/ title="arXiv @ 2023.12.04">arXiv @ 2023.12.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231205000000/ title="arXiv @ 2023.12.05">arXiv @ 2023.12.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231206000000/ title="arXiv @ 2023.12.06">arXiv @ 2023.12.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231207000000/ title="arXiv @ 2023.12.07">arXiv @ 2023.12.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231208000000/ title="arXiv @ 2023.12.08">arXiv @ 2023.12.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231209000000/ title="arXiv @ 2023.12.09">arXiv @ 2023.12.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231210000000/ title="arXiv @ 2023.12.10">arXiv @ 2023.12.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231211000000/ title="arXiv @ 2023.12.11">arXiv @ 2023.12.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231212000000/ title="arXiv @ 2023.12.12">arXiv @ 2023.12.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231213000000/ title="arXiv @ 2023.12.13">arXiv @ 2023.12.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231214000000/ title="arXiv @ 2023.12.14">arXiv @ 2023.12.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231215000000/ title="arXiv @ 2023.12.15">arXiv @ 2023.12.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231216000000/ title="arXiv @ 2023.12.16">arXiv @ 2023.12.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231217000000/ title="arXiv @ 2023.12.17">arXiv @ 2023.12.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231218000000/ title="arXiv @ 2023.12.18">arXiv @ 2023.12.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231219000000/ title="arXiv @ 2023.12.19">arXiv @ 2023.12.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231220000000/ title="arXiv @ 2023.12.20">arXiv @ 2023.12.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231221000000/ title="arXiv @ 2023.12.21">arXiv @ 2023.12.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231222000000/ title="arXiv @ 2023.12.22">arXiv @ 2023.12.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231223000000/ title="arXiv @ 2023.12.23">arXiv @ 2023.12.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231224000000/ title="arXiv @ 2023.12.24">arXiv @ 2023.12.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231225000000/ title="arXiv @ 2023.12.25">arXiv @ 2023.12.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231226000000/ title="arXiv @ 2023.12.26">arXiv @ 2023.12.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231227000000/ title="arXiv @ 2023.12.27">arXiv @ 2023.12.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231228000000/ title="arXiv @ 2023.12.28">arXiv @ 2023.12.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231229000000/ title="arXiv @ 2023.12.29">arXiv @ 2023.12.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231230000000/ title="arXiv @ 2023.12.30">arXiv @ 2023.12.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231231000000/ title="arXiv @ 2023.12.31">arXiv @ 2023.12.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202401/>2024.01</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240101000000/ title="arXiv @ 2024.01.01">arXiv @ 2024.01.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240102000000/ title="arXiv @ 2024.01.02">arXiv @ 2024.01.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240103000000/ title="arXiv @ 2024.01.03">arXiv @ 2024.01.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240104000000/ title="arXiv @ 2024.01.04">arXiv @ 2024.01.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240105000000/ title="arXiv @ 2024.01.05">arXiv @ 2024.01.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240106000000/ title="arXiv @ 2024.01.06">arXiv @ 2024.01.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240107000000/ title="arXiv @ 2024.01.07">arXiv @ 2024.01.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240108000000/ title="arXiv @ 2024.01.08">arXiv @ 2024.01.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240109000000/ title="arXiv @ 2024.01.09">arXiv @ 2024.01.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240110000000/ title="arXiv @ 2024.01.10">arXiv @ 2024.01.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240111000000/ title="arXiv @ 2024.01.11">arXiv @ 2024.01.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240112000000/ title="arXiv @ 2024.01.12">arXiv @ 2024.01.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240113000000/ title="arXiv @ 2024.01.13">arXiv @ 2024.01.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240114000000/ title="arXiv @ 2024.01.14">arXiv @ 2024.01.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240115000000/ title="arXiv @ 2024.01.15">arXiv @ 2024.01.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240116000000/ title="arXiv @ 2024.01.16">arXiv @ 2024.01.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240117000000/ title="arXiv @ 2024.01.17">arXiv @ 2024.01.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240118000000/ title="arXiv @ 2024.01.18">arXiv @ 2024.01.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240119000000/ title="arXiv @ 2024.01.19">arXiv @ 2024.01.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240120000000/ title="arXiv @ 2024.01.20">arXiv @ 2024.01.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240121000000/ title="arXiv @ 2024.01.21">arXiv @ 2024.01.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240122000000/ title="arXiv @ 2024.01.22">arXiv @ 2024.01.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240123000000/ title="arXiv @ 2024.01.23">arXiv @ 2024.01.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240124000000/ title="arXiv @ 2024.01.24">arXiv @ 2024.01.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240125000000/ title="arXiv @ 2024.01.25">arXiv @ 2024.01.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240126000000/ title="arXiv @ 2024.01.26">arXiv @ 2024.01.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240127000000/ title="arXiv @ 2024.01.27">arXiv @ 2024.01.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240128000000/ title="arXiv @ 2024.01.28">arXiv @ 2024.01.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240129000000/ title="arXiv @ 2024.01.29">arXiv @ 2024.01.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240130000000/ title="arXiv @ 2024.01.30">arXiv @ 2024.01.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240131000000/ title="arXiv @ 2024.01.31">arXiv @ 2024.01.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202402/>2024.02</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240201000000/ title="arXiv @ 2024.02.01">arXiv @ 2024.02.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240202000000/ title="arXiv @ 2024.02.02">arXiv @ 2024.02.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240203000000/ title="arXiv @ 2024.02.03">arXiv @ 2024.02.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240204000000/ title="arXiv @ 2024.02.04">arXiv @ 2024.02.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240205000000/ title="arXiv @ 2024.02.05">arXiv @ 2024.02.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240206000000/ title="arXiv @ 2024.02.06">arXiv @ 2024.02.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/ title="arXiv @ 2024.02.07">arXiv @ 2024.02.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/ title="arXiv @ 2024.02.08">arXiv @ 2024.02.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/ title="arXiv @ 2024.02.09">arXiv @ 2024.02.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/ title="arXiv @ 2024.02.10">arXiv @ 2024.02.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240211000000/ title="arXiv @ 2024.02.11">arXiv @ 2024.02.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/ title="arXiv @ 2024.02.12">arXiv @ 2024.02.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/ title="arXiv @ 2024.02.13">arXiv @ 2024.02.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/ title="arXiv @ 2024.02.14">arXiv @ 2024.02.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/ title="arXiv @ 2024.02.15">arXiv @ 2024.02.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240216000000/ title="arXiv @ 2024.02.16">arXiv @ 2024.02.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/ title="arXiv @ 2024.02.17">arXiv @ 2024.02.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240218000000/ title="arXiv @ 2024.02.18">arXiv @ 2024.02.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/ title="arXiv @ 2024.02.19">arXiv @ 2024.02.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/ title="arXiv @ 2024.02.20">arXiv @ 2024.02.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/ title="arXiv @ 2024.02.21">arXiv @ 2024.02.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/ title="arXiv @ 2024.02.22">arXiv @ 2024.02.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/ title="arXiv @ 2024.02.23">arXiv @ 2024.02.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/ title="arXiv @ 2024.02.24">arXiv @ 2024.02.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/ title="arXiv @ 2024.02.25">arXiv @ 2024.02.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/ title="arXiv @ 2024.02.26">arXiv @ 2024.02.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/ title="arXiv @ 2024.02.27">arXiv @ 2024.02.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/ title="arXiv @ 2024.02.28">arXiv @ 2024.02.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/ title="arXiv @ 2024.02.29">arXiv @ 2024.02.29</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/202403/>2024.03</a><ul class=active><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/ title="arXiv @ 2024.03.01">arXiv @ 2024.03.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/ title="arXiv @ 2024.03.02">arXiv @ 2024.03.02</a></li><li><a class=active href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/ title="arXiv @ 2024.03.03">arXiv @ 2024.03.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240304000000/ title="arXiv @ 2024.03.04">arXiv @ 2024.03.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/ title="arXiv @ 2024.03.05">arXiv @ 2024.03.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/ title="arXiv @ 2024.03.06">arXiv @ 2024.03.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/ title="arXiv @ 2024.03.07">arXiv @ 2024.03.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/ title="arXiv @ 2024.03.08">arXiv @ 2024.03.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/ title="arXiv @ 2024.03.09">arXiv @ 2024.03.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/ title="arXiv @ 2024.03.10">arXiv @ 2024.03.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/ title="arXiv @ 2024.03.11">arXiv @ 2024.03.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/ title="arXiv @ 2024.03.12">arXiv @ 2024.03.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/ title="arXiv @ 2024.03.13">arXiv @ 2024.03.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/ title="arXiv @ 2024.03.14">arXiv @ 2024.03.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/ title="arXiv @ 2024.03.15">arXiv @ 2024.03.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/ title="arXiv @ 2024.03.16">arXiv @ 2024.03.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/ title="arXiv @ 2024.03.17">arXiv @ 2024.03.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/ title="arXiv @ 2024.03.18">arXiv @ 2024.03.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240319000000/ title="arXiv @ 2024.03.19">arXiv @ 2024.03.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/ title="arXiv @ 2024.03.20">arXiv @ 2024.03.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/ title="arXiv @ 2024.03.21">arXiv @ 2024.03.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/ title="arXiv @ 2024.03.22">arXiv @ 2024.03.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240323000000/ title="arXiv @ 2024.03.23">arXiv @ 2024.03.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/ title="arXiv @ 2024.03.24">arXiv @ 2024.03.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240325000000/ title="arXiv @ 2024.03.25">arXiv @ 2024.03.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240326000000/ title="arXiv @ 2024.03.26">arXiv @ 2024.03.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240327000000/ title="arXiv @ 2024.03.27">arXiv @ 2024.03.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240328000000/ title="arXiv @ 2024.03.28">arXiv @ 2024.03.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240329000000/ title="arXiv @ 2024.03.29">arXiv @ 2024.03.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240330000000/ title="arXiv @ 2024.03.30">arXiv @ 2024.03.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240331000000/ title="arXiv @ 2024.03.31">arXiv @ 2024.03.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202404/>2024.04</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240401000000/ title="arXiv @ 2024.04.01">arXiv @ 2024.04.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240402000000/ title="arXiv @ 2024.04.02">arXiv @ 2024.04.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240403000000/ title="arXiv @ 2024.04.03">arXiv @ 2024.04.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240404000000/ title="arXiv @ 2024.04.04">arXiv @ 2024.04.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240405000000/ title="arXiv @ 2024.04.05">arXiv @ 2024.04.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240406000000/ title="arXiv @ 2024.04.06">arXiv @ 2024.04.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240407000000/ title="arXiv @ 2024.04.07">arXiv @ 2024.04.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240408000000/ title="arXiv @ 2024.04.08">arXiv @ 2024.04.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240409000000/ title="arXiv @ 2024.04.09">arXiv @ 2024.04.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240410000000/ title="arXiv @ 2024.04.10">arXiv @ 2024.04.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240411000000/ title="arXiv @ 2024.04.11">arXiv @ 2024.04.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240412000000/ title="arXiv @ 2024.04.12">arXiv @ 2024.04.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240413000000/ title="arXiv @ 2024.04.13">arXiv @ 2024.04.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240414000000/ title="arXiv @ 2024.04.14">arXiv @ 2024.04.14</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/algorithms/>Algorithms</a><ul><li><a href=/akitenkrad-blog/posts/algorithms/bandit-algorithm-basic/ title="Bandit Algorithm Basic">Bandit Algorithm Basic</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/conference/>Conference</a><ul><li><a href=/akitenkrad-blog/posts/conference/acl/ title="Annual Meeting of the Association for Computational Linguistics">Annual Meeting of the Association for Computational Linguistics</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/>Figures</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/figures/202208/20220822092748/ title=1ヶ月当たりの実労働時間の推移>1ヶ月当たりの実労働時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220813115200/ title=コンビニエンスストアの店舗数の推移>コンビニエンスストアの店舗数の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220821113322/ title=地域別1世帯当たり1ヶ月間の支出額の推移>地域別1世帯当たり1ヶ月間の支出額の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220828111836/ title="夫婦別子供の有無別生活時間 (2016)">夫婦別子供の有無別生活時間 (2016)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220827103615/ title=家族類型の変遷>家族類型の変遷</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220815131016/ title="年齢別大学院入学者数 (2021)">年齢別大学院入学者数 (2021)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220818122628/ title=情報サービス業における企業特殊的人的資本（名目）の推移>情報サービス業における企業特殊的人的資本（名目）の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220824095127/ title=有業者の社会生活における活動の変化>有業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220814175022/ title=東京23区の乗用車保有台数>東京23区の乗用車保有台数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220817121117/ title="業種別企業特殊的人的資本 (2018)">業種別企業特殊的人的資本 (2018)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220825094111/ title=無業者の社会生活における活動の変化>無業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220823104100/ title=睡眠時間の推移>睡眠時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220826102652/ title=総人口の推移>総人口の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220820225528/ title=都道府県別国公立別学校数・学生数>都道府県別国公立別学校数・学生数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220816120845/ title="都道府県別消費者物価指数/教育（全国平均=100）">都道府県別消費者物価指数/教育（全国平均=100）</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220812173814/ title=金沢市の夏の月平均気温の遷移>金沢市の夏の月平均気温の遷移</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/figures/202209/20220907122906/ title="ACL-2022 WordCloud">ACL-2022 WordCloud</a></li></ul></li></ul></li><li><a href=/akitenkrad-blog/posts/latex/ title="Latex Mathematics Syntax Guide">Latex Mathematics Syntax Guide</a></li><li><a href=/akitenkrad-blog/posts/markdown/ title="Markdown Sample">Markdown Sample</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/akitenkrad-blog/posts/arxiv/202403/20240303000000/hero.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_120x120_fit_box_3.png alt="Author Image"><h5 class=author-name></h5><p>Sunday, Mar 3, 2024</p></div><div class=title><h1>arXiv @ 2024.03.03</h1></div><div class=taxonomy-terms><ul><li class=rounded><a href=/akitenkrad-blog/tags/arxiv class="btn, btn-sm">arXiv</a></li><li class=rounded><a href=/akitenkrad-blog/tags/published2024 class="btn, btn-sm">2024</a></li></ul></div><div class=post-content id=post-content><figure style=border:none;width:100%;display:flex;justify-content:center><iframe src=pie.html width=900 height=620 style=border:none></iframe></figure><h2 id=primary-categories>Primary Categories</h2><ul><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/#astro-phim-1>astro-ph.IM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/#cond-matmtrl-sci-1>cond-mat.mtrl-sci (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/#csai-7>cs.AI (7)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/#csar-1>cs.AR (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/#cscc-1>cs.CC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/#cscg-1>cs.CG (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/#cscl-37>cs.CL (37)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/#cscr-9>cs.CR (9)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/#cscv-46>cs.CV (46)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/#cscy-1>cs.CY (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/#csdb-2>cs.DB (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/#csdc-3>cs.DC (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/#csds-4>cs.DS (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/#csgr-1>cs.GR (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/#csgt-4>cs.GT (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/#cshc-8>cs.HC (8)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/#csir-7>cs.IR (7)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/#csit-9>cs.IT (9)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/#cslg-40>cs.LG (40)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/#cslo-2>cs.LO (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/#csma-1>cs.MA (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/#csne-1>cs.NE (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/#csni-2>cs.NI (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/#csro-16>cs.RO (16)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/#cssd-2>cs.SD (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/#csse-4>cs.SE (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/#cssi-2>cs.SI (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/#eessas-2>eess.AS (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/#eessiv-3>eess.IV (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/#eesssp-2>eess.SP (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/#eesssy-7>eess.SY (7)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/#mathco-1>math.CO (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/#mathmg-1>math.MG (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/#mathna-4>math.NA (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/#mathoc-1>math.OC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/#physicsmed-ph-1>physics.med-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/#q-bioqm-1>q-bio.QM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/#q-fincp-1>q-fin.CP (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/#q-fingn-1>q-fin.GN (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/#statap-1>stat.AP (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/#statme-1>stat.ME (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/#statml-3>stat.ML (3)</a></li></ul><h2 id=keywords>Keywords</h2><table border=1 class=dataframe><thead><tr style=text-align:right><th>keyword</th><th>cs.CL</th><th>cs.CV</th><th>cs.LG</th><th>cs.RO</th></tr></thead><tbody><tr><td>Adversarial Attack</td><td></td><td></td><td>2</td><td></td></tr><tr><td>Adversarial Learning</td><td>1</td><td></td><td>2</td><td></td></tr><tr><td>Autoencoder</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Automatic Speech Recognition</td><td>4</td><td></td><td></td><td></td></tr><tr><td>BERT</td><td>1</td><td></td><td>1</td><td></td></tr><tr><td>BERTScore</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Bandit Algorithm</td><td></td><td></td><td>2</td><td></td></tr><tr><td>Benchmarking</td><td>14</td><td>12</td><td>8</td><td>2</td></tr><tr><td>Black Box</td><td></td><td></td><td></td><td>1</td></tr><tr><td>Chatbot</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Clustering</td><td></td><td>2</td><td>1</td><td></td></tr><tr><td>Contrastive Learning</td><td>1</td><td></td><td></td><td></td></tr><tr><td>ControlNet</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Convolution</td><td></td><td>6</td><td>2</td><td></td></tr><tr><td>Convolutional Neural Network</td><td></td><td>7</td><td>1</td><td></td></tr><tr><td>Curriculum Learning</td><td></td><td></td><td></td><td>1</td></tr><tr><td>Data Augmentation</td><td>2</td><td></td><td>1</td><td>1</td></tr><tr><td>Differential Privacy</td><td></td><td></td><td>2</td><td></td></tr><tr><td>Diffusion Model</td><td></td><td>7</td><td>1</td><td></td></tr><tr><td>Direct Preference Optimization</td><td>1</td><td></td><td>1</td><td></td></tr><tr><td>Discrete Time</td><td></td><td></td><td>2</td><td></td></tr><tr><td>Domain Adaptation</td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>Fact Verification</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Fairness</td><td>1</td><td></td><td>1</td><td></td></tr><tr><td>Federated Learning</td><td></td><td></td><td>2</td><td></td></tr><tr><td>Few-shot</td><td>4</td><td>3</td><td>1</td><td></td></tr><tr><td>Few-shot Learning</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Fine-tuning</td><td>10</td><td>9</td><td>2</td><td>1</td></tr><tr><td>Foundation Model</td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>GPT</td><td>4</td><td>1</td><td></td><td></td></tr><tr><td>GPT-3</td><td>1</td><td></td><td></td><td></td></tr><tr><td>GPT-3.5</td><td>1</td><td></td><td></td><td></td></tr><tr><td>GPT-4</td><td>3</td><td></td><td></td><td></td></tr><tr><td>GPT-4 turbo</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Gaussian Process</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Gemini</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Geometry</td><td></td><td>5</td><td>2</td><td></td></tr><tr><td>Graph</td><td>1</td><td></td><td>7</td><td>1</td></tr><tr><td>Graph Attention Networks</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Graph Neural Network</td><td></td><td></td><td>6</td><td></td></tr><tr><td>Grounding</td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>Hallucination Detection</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Human Intervention</td><td></td><td></td><td></td><td>1</td></tr><tr><td>Image2text</td><td></td><td>1</td><td></td><td></td></tr><tr><td>In-context Learning</td><td>4</td><td></td><td></td><td></td></tr><tr><td>Information Retrieval</td><td>1</td><td>1</td><td>1</td><td></td></tr><tr><td>Instruction Tuning</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Knowledge Distillation</td><td></td><td>3</td><td>5</td><td>1</td></tr><tr><td>Knowledge Graph</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Knowledge Transfer</td><td></td><td></td><td>1</td><td></td></tr><tr><td>LLaMA</td><td>2</td><td>1</td><td>1</td><td></td></tr><tr><td>LSTM</td><td></td><td></td><td>4</td><td>3</td></tr><tr><td>Label Smoothing</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Language Generation</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Large Language Model</td><td>43</td><td>3</td><td>6</td><td>2</td></tr><tr><td>Logistic Regression</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Low-Resource</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Markov Decision Process</td><td></td><td></td><td>2</td><td></td></tr><tr><td>Masked Language Model</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Mathematical Reasoning</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Message-Passing</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Model Compression</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Multi-modal</td><td>3</td><td>7</td><td></td><td>2</td></tr><tr><td>Mutual Information</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Natural Language Generation</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Natural Language Inference</td><td>4</td><td></td><td></td><td></td></tr><tr><td>Object Detection</td><td></td><td>4</td><td></td><td></td></tr><tr><td>Opinion Summarization</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Optical Character Recognition</td><td>2</td><td>2</td><td></td><td></td></tr><tr><td>Out-of-distribution</td><td>1</td><td></td><td>1</td><td></td></tr><tr><td>Parameter Sharing</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Perplexity</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Pre-trained Language Model</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Prompt</td><td>12</td><td>8</td><td></td><td>1</td></tr><tr><td>Pruning</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Question Answering</td><td>2</td><td>2</td><td></td><td></td></tr><tr><td>Reasoning</td><td>3</td><td>2</td><td></td><td></td></tr><tr><td>Recommendation</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Reinforcement Learning</td><td>1</td><td></td><td>12</td><td>4</td></tr><tr><td>Relation Extraction</td><td>3</td><td></td><td></td><td></td></tr><tr><td>Representation Learning</td><td></td><td>5</td><td></td><td></td></tr><tr><td>Rouge</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Self-Attention</td><td></td><td>3</td><td></td><td></td></tr><tr><td>Self-supervised Learning</td><td>1</td><td>6</td><td>2</td><td></td></tr><tr><td>Self-supervised Pre-training</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Sentiment Analysis</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Simulation</td><td></td><td></td><td>2</td><td>9</td></tr><tr><td>Simulator</td><td></td><td></td><td>2</td><td>9</td></tr><tr><td>Stance Detection</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Stochastic Gradient Descent</td><td></td><td></td><td>2</td><td></td></tr><tr><td>Summarization</td><td>2</td><td></td><td>2</td><td></td></tr><tr><td>Supervised Learning</td><td>3</td><td>1</td><td>1</td><td></td></tr><tr><td>Text Analysis</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Text Classification</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Text Generation</td><td></td><td>1</td><td>1</td><td></td></tr><tr><td>Text Summarization</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Text2image</td><td></td><td>4</td><td></td><td></td></tr><tr><td>Tokenization</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Transfer Learning</td><td>1</td><td></td><td>2</td><td></td></tr><tr><td>Transformer</td><td>3</td><td>6</td><td></td><td></td></tr><tr><td>Unsupervised Learning</td><td>1</td><td>3</td><td></td><td></td></tr><tr><td>Vision Transformer</td><td></td><td>4</td><td></td><td></td></tr><tr><td>Vision-and-Language</td><td>1</td><td>6</td><td></td><td></td></tr><tr><td>Word Embedding</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Yolo</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Zero-shot</td><td>5</td><td>1</td><td></td><td></td></tr><tr><td>Zero-shot Learning</td><td>1</td><td></td><td></td><td></td></tr></tbody></table><script>$(function(){$("table").addClass("keyword-table table-bordered border-success"),$("table thead").addClass("sticky-top"),$("table tbody td").css("text-align","")})</script><h2 id=cscl-37>cs.CL (37)</h2><h3 id=137--1243-softtiger-a-clinical-foundation-model-for-healthcare-workflows-ye-chen-et-al-2024>(1/37 | 1/243) SoftTiger: A Clinical Foundation Model for Healthcare Workflows (Ye Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ye Chen, Igor Couto, Wei Cai, Cong Fu, Bruno Dorneles. (2024)<br><strong>SoftTiger: A Clinical Foundation Model for Healthcare Workflows</strong><br><button class=copy-to-clipboard title="SoftTiger: A Clinical Foundation Model for Healthcare Workflows" index=1>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-1 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 110<br>Keywords: Fine-tuning, Foundation Model, Supervised Learning, GPT, GPT-3, GPT-3.5, GPT-4, Gemini, Information Retrieval, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00868v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00868v1.pdf filename=2403.00868v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We release and introduce SoftTiger, a clinical <b>large</b> <b>language</b> <b>model</b> (CLaM) designed as a <b>foundation</b> <b>model</b> for healthcare workflows. The narrative and unstructured nature of clinical notes is a major obstacle for healthcare intelligentization. We address a critical problem of structuring clinical notes into clinical data, according to international interoperability standards. We collect and annotate data for three critical subtasks, namely, international patient summary, clinical impression and medical encounter. We then <b>supervised</b> <b>fine-tuned</b> a state-of-the-art <b>LLM</b> using public and credentialed clinical data. The training is orchestrated in a way that the target model can first support basic clinical tasks such as abbreviation expansion and temporal <b>information</b> <b>extraction,</b> and then learn to perform more complex downstream clinical tasks such as impression and encounter summary. Moreover, we address, several modeling challenges in the healthcare context, e.g., extra long context window. Our blind pairwise evaluation shows that SoftTiger outperforms other popular open-source models and <b>GPT-3.5,</b> comparable to <b>Gemini-pro,</b> and only has a mild gap from <b>GPT-4.</b> We believe that <b>LLMs</b> may become a step-stone towards healthcare digitalization and democratization. Therefore, we publicly release SoftTiger models at scales of 13 billion and 70 billion parameters, as well as datasets and code for our innovative scalable evaluation, hopefully, making a significant contribution to the healthcare industry.</p></p class="citation"></blockquote><h3 id=237--2243-malto-at-semeval-2024-task-6-leveraging-synthetic-data-for-llm-hallucination-detection-federico-borra-et-al-2024>(2/37 | 2/243) MALTO at SemEval-2024 Task 6: Leveraging Synthetic Data for LLM Hallucination Detection (Federico Borra et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Federico Borra, Claudio Savelli, Giacomo Rosso, Alkis Koudounas, Flavio Giobergia. (2024)<br><strong>MALTO at SemEval-2024 Task 6: Leveraging Synthetic Data for LLM Hallucination Detection</strong><br><button class=copy-to-clipboard title="MALTO at SemEval-2024 Task 6: Leveraging Synthetic Data for LLM Hallucination Detection" index=2>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-2 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 100<br>Keywords: Data Augmentation, Fine-tuning, Hallucination Detection, Language Generation, Natural Language Generation, Natural Language Generation, Natural Language Inference, Natural Language Inference, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00964v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00964v1.pdf filename=2403.00964v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In <b>Natural</b> <b>Language</b> <b>Generation</b> <b>(NLG),</b> contemporary <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> face several challenges, such as generating fluent yet inaccurate outputs and reliance on fluency-centric metrics. This often leads to neural networks exhibiting <b>&ldquo;hallucinations&rdquo;.</b> <b>The</b> SHROOM challenge focuses on automatically identifying these <b>hallucinations</b> <b>in</b> the generated text. To tackle these issues, we introduce two key components, a <b>data</b> <b>augmentation</b> pipeline incorporating <b>LLM-assisted</b> pseudo-labelling and sentence rephrasing, and a voting ensemble from three models pre-trained on <b>Natural</b> <b>Language</b> <b>Inference</b> <b>(NLI)</b> tasks and <b>fine-tuned</b> on diverse datasets.</p></p class="citation"></blockquote><h3 id=337--3243-surveying-the-dead-minds-historical-psychological-text-analysis-with-contextualized-construct-representation-ccr-for-classical-chinese-yuqi-chen-et-al-2024>(3/37 | 3/243) Surveying the Dead Minds: Historical-Psychological Text Analysis with Contextualized Construct Representation (CCR) for Classical Chinese (Yuqi Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuqi Chen, Sixuan Li, Ying Li, Mohammad Atari. (2024)<br><strong>Surveying the Dead Minds: Historical-Psychological Text Analysis with Contextualized Construct Representation (CCR) for Classical Chinese</strong><br><button class=copy-to-clipboard title="Surveying the Dead Minds: Historical-Psychological Text Analysis with Contextualized Construct Representation (CCR) for Classical Chinese" index=3>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-3 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-CY, cs.CL<br>Keyword Score: 93<br>Keywords: Benchmarking, Contrastive Learning, Fine-tuning, Supervised Learning, GPT, GPT-4, Transformer, Text Analysis, Prompt, Word Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00509v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00509v1.pdf filename=2403.00509v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we develop a pipeline for historical-psychological <b>text</b> <b>analysis</b> in classical Chinese. Humans have produced <b>texts</b> <b>in</b> various languages for thousands of years; however, most of the computational literature is focused on contemporary languages and corpora. The emerging field of historical psychology relies on computational techniques to extract aspects of psychology from historical corpora using new methods developed in natural language processing (NLP). The present pipeline, called Contextualized Construct Representations (CCR), combines expert knowledge in psychometrics (i.e., psychological surveys) with <b>text</b> <b>representations</b> generated via <b>transformer-based</b> language models to measure psychological constructs such as traditionalism, norm strength, and collectivism in classical Chinese corpora. Considering the scarcity of available data, we propose an indirect <b>supervised</b> <b>contrastive</b> <b>learning</b> approach and build the first Chinese historical psychology corpus (C-HI-PSY) to <b>fine-tune</b> pre-trained models. We evaluate the pipeline to demonstrate its superior performance compared with other approaches. The CCR method outperforms <b>word-embedding-based</b> <b>approaches</b> across all of our tasks and exceeds <b>prompting</b> with <b>GPT-4</b> in most tasks. Finally, we <b>benchmark</b> the pipeline against objective, external data to further verify its validity.</p></p class="citation"></blockquote><h3 id=437--4243-cross-lingual-learning-vs-low-resource-fine-tuning-a-case-study-with-fact-checking-in-turkish-recep-firat-cekinel-et-al-2024>(4/37 | 4/243) Cross-Lingual Learning vs. Low-Resource Fine-Tuning: A Case Study with Fact-Checking in Turkish (Recep Firat Cekinel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Recep Firat Cekinel, Pinar Karagoz, Cagri Coltekin. (2024)<br><strong>Cross-Lingual Learning vs. Low-Resource Fine-Tuning: A Case Study with Fact-Checking in Turkish</strong><br><button class=copy-to-clipboard title="Cross-Lingual Learning vs. Low-Resource Fine-Tuning: A Case Study with Fact-Checking in Turkish" index=4>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-4 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 90<br>Keywords: Few-shot, Fine-tuning, Low-Resource, Transfer Learning, Zero-shot, Fact Verification, In-context Learning, In-context Learning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00411v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00411v1.pdf filename=2403.00411v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rapid spread of misinformation through social media platforms has raised concerns regarding its impact on public opinion. While misinformation is prevalent in other languages, the majority of research in this field has concentrated on the English language. Hence, there is a scarcity of datasets for other languages, including Turkish. To address this concern, we have introduced the FCTR dataset, consisting of 3238 real-world claims. This dataset spans multiple domains and incorporates evidence collected from three Turkish <b>fact-checking</b> <b>organizations.</b> Additionally, we aim to assess the effectiveness of cross-lingual <b>transfer</b> <b>learning</b> for <b>low-resource</b> languages, with a particular focus on Turkish. We demonstrate <b>in-context</b> <b>learning</b> <b>(zero-shot</b> and <b>few-shot)</b> performance of <b>large</b> <b>language</b> <b>models</b> in this context. The experimental results indicate that the dataset has the potential to advance research in the Turkish language.</p></p class="citation"></blockquote><h3 id=537--5243-llms-for-targeted-sentiment-in-news-headlines-exploring-different-levels-of-prompt-prescriptiveness-jana-juroš-et-al-2024>(5/37 | 5/243) LLMs for Targeted Sentiment in News Headlines: Exploring Different Levels of Prompt Prescriptiveness (Jana Juroš et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jana Juroš, Laura Majer, Jan Šnajder. (2024)<br><strong>LLMs for Targeted Sentiment in News Headlines: Exploring Different Levels of Prompt Prescriptiveness</strong><br><button class=copy-to-clipboard title="LLMs for Targeted Sentiment in News Headlines: Exploring Different Levels of Prompt Prescriptiveness" index=5>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-5 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 80<br>Keywords: Few-shot, Fine-tuning, Zero-shot, Sentiment Analysis, In-context Learning, In-context Learning, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00418v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00418v1.pdf filename=2403.00418v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>News headlines often evoke <b>sentiment</b> <b>by</b> intentionally portraying entities in particular ways, making targeted <b>sentiment</b> <b>analysis</b> (TSA) of headlines a worthwhile but difficult task. <b>Fine-tuned</b> encoder models show satisfactory TSA performance, but their background knowledge is limited, and they require a labeled dataset. <b>LLMs</b> offer a potentially universal solution for TSA due to their broad linguistic and world knowledge along with <b>in-context</b> <b>learning</b> abilities, yet their performance is heavily influenced by <b>prompt</b> design. Drawing parallels with annotation paradigms for subjective tasks, we explore the influence of <b>prompt</b> design on the performance of <b>LLMs</b> for TSA of news headlines. We evaluate the predictive accuracy of state-of-the-art <b>LLMs</b> using <b>prompts</b> with different levels of prescriptiveness, ranging from plain <b>zero-shot</b> to elaborate <b>few-shot</b> <b>prompts</b> matching annotation guidelines. Recognizing the subjective nature of TSA, we evaluate the ability of <b>LLMs</b> to quantify predictive uncertainty via calibration error and correlation to human inter-annotator agreement. We find that, except for <b>few-shot</b> <b>prompting,</b> calibration and F1-score improve with increased prescriptiveness, but the optimal level depends on the model.</p></p class="citation"></blockquote><h3 id=637--6243-improving-socratic-question-generation-using-data-augmentation-and-preference-optimization-nischal-ashok-kumar-et-al-2024>(6/37 | 6/243) Improving Socratic Question Generation using Data Augmentation and Preference Optimization (Nischal Ashok Kumar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nischal Ashok Kumar, Andrew Lan. (2024)<br><strong>Improving Socratic Question Generation using Data Augmentation and Preference Optimization</strong><br><button class=copy-to-clipboard title="Improving Socratic Question Generation using Data Augmentation and Preference Optimization" index=6>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-6 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CY, cs-LG, cs.CL<br>Keyword Score: 70<br>Keywords: Data Augmentation, Direct Preference Optimization, Reinforcement Learning, LLaMA, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00199v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00199v1.pdf filename=2403.00199v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Socratic method is a way of guiding students toward solving a problem independently without directly revealing the solution to the problem. Although this method has been shown to significantly improve student learning outcomes, it remains a complex labor-intensive task for instructors. <b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> can be used to augment human effort by automatically generating Socratic questions for students. However, existing methods that involve <b>prompting</b> these <b>LLMs</b> sometimes produce invalid outputs, e.g., those that directly reveal the solution to the problem or provide irrelevant or premature questions. To alleviate this problem, inspired by <b>reinforcement</b> <b>learning</b> with AI feedback (RLAIF), we first propose a <b>data</b> <b>augmentation</b> method to enrich existing Socratic questioning datasets with questions that are invalid in specific ways. Next, we propose a method to optimize open-source <b>LLMs</b> such as <b>LLama</b> 2 to prefer ground-truth questions over generated invalid ones, using <b>direct</b> <b>preference</b> <b>optimization</b> (DPO). Our experiments on a Socratic questions dataset for student code debugging show that a DPO-optimized 7B <b>LLama</b> 2 model can effectively avoid generating invalid questions, and as a result, outperforms existing state-of-the-art <b>prompting</b> methods.</p></p class="citation"></blockquote><h3 id=737--7243-benchmarking-zero-shot-stance-detection-with-flant5-xxl-insights-from-training-data-prompting-and-decoding-strategies-into-its-near-sota-performance-rachith-aiyappa-et-al-2024>(7/37 | 7/243) Benchmarking zero-shot stance detection with FlanT5-XXL: Insights from training data, prompting, and decoding strategies into its near-SoTA performance (Rachith Aiyappa et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rachith Aiyappa, Shruthi Senthilmani, Jisun An, Haewoon Kwak, Yong-Yeol Ahn. (2024)<br><strong>Benchmarking zero-shot stance detection with FlanT5-XXL: Insights from training data, prompting, and decoding strategies into its near-SoTA performance</strong><br><button class=copy-to-clipboard title="Benchmarking zero-shot stance detection with FlanT5-XXL: Insights from training data, prompting, and decoding strategies into its near-SoTA performance" index=7>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-7 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 66<br>Keywords: Benchmarking, Benchmarking, Fine-tuning, Zero-shot, Stance Detection, Large Language Model, Perplexity, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00236v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00236v1.pdf filename=2403.00236v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We investigate the performance of <b>LLM-based</b> <b>zero-shot</b> <b>stance</b> <b>detection</b> on tweets. Using FlanT5-XXL, an instruction-tuned open-source <b>LLM,</b> with the SemEval 2016 Tasks 6A, 6B, and P-Stance datasets, we study the performance and its variations under different <b>prompts</b> and decoding strategies, as well as the potential biases of the model. We show that the <b>zero-shot</b> approach can match or outperform state-of-the-art <b>benchmarks,</b> including <b>fine-tuned</b> models. We provide various insights into its performance including the sensitivity to instructions and <b>prompts,</b> the decoding strategies, the <b>perplexity</b> of the <b>prompts,</b> and to negations and oppositions present in <b>prompts.</b> Finally, we ensure that the <b>LLM</b> has not been trained on test datasets, and identify a positivity bias which may partially explain the performance differences across decoding strategie</p></p class="citation"></blockquote><h3 id=837--8243-attribute-structuring-improves-llm-based-evaluation-of-clinical-text-summaries-zelalem-gero-et-al-2024>(8/37 | 8/243) Attribute Structuring Improves LLM-Based Evaluation of Clinical Text Summaries (Zelalem Gero et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zelalem Gero, Chandan Singh, Yiqing Xie, Sheng Zhang, Tristan Naumann, Jianfeng Gao, Hoifung Poon. (2024)<br><strong>Attribute Structuring Improves LLM-Based Evaluation of Clinical Text Summaries</strong><br><button class=copy-to-clipboard title="Attribute Structuring Improves LLM-Based Evaluation of Clinical Text Summaries" index=8>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-8 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 63<br>Keywords: Benchmarking, Grounding, Text Summarization, Large Language Model, Large Language Model, Prompt, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01002v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01002v1.pdf filename=2403.01002v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Summarizing clinical <b>text</b> <b>is</b> crucial in health decision-support and clinical research. <b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have shown the potential to generate accurate clinical <b>text</b> <b>summaries,</b> but still struggle with issues regarding <b>grounding</b> and evaluation, especially in safety-critical domains such as health. Holistically evaluating <b>text</b> <b>summaries</b> is challenging because they may contain unsubstantiated information. Here, we explore a general mitigation framework using Attribute Structuring (AS), which structures the summary evaluation process. It decomposes the evaluation process into a grounded procedure that uses an <b>LLM</b> for relatively simple structuring and scoring tasks, rather than the full task of holistic summary evaluation. Experiments show that AS consistently improves the correspondence between human annotations and automated metrics in clinical <b>text</b> <b>summarization.</b> Additionally, AS yields interpretations in the form of a short <b>text</b> <b>span</b> corresponding to each output, which enables efficient human auditing, paving the way towards trustworthy evaluation of clinical information in resource-constrained scenarios. We release our code, <b>prompts,</b> and an open-source <b>benchmark</b> at <a href=https://github.com/microsoft/attribute-structuring>https://github.com/microsoft/attribute-structuring</a>.</p></p class="citation"></blockquote><h3 id=937--9243-localrqa-from-generating-data-to-locally-training-testing-and-deploying-retrieval-augmented-qa-systems-xiao-yu-et-al-2024>(9/37 | 9/243) LocalRQA: From Generating Data to Locally Training, Testing, and Deploying Retrieval-Augmented QA Systems (Xiao Yu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiao Yu, Yunan Lu, Zhou Yu. (2024)<br><strong>LocalRQA: From Generating Data to Locally Training, Testing, and Deploying Retrieval-Augmented QA Systems</strong><br><button class=copy-to-clipboard title="LocalRQA: From Generating Data to Locally Training, Testing, and Deploying Retrieval-Augmented QA Systems" index=9>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-9 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: GPT, GPT-4, GPT-4 turbo, Question Answering, Question Answering, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00982v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00982v1.pdf filename=2403.00982v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Retrieval-augmented <b>question-answering</b> <b>systems</b> combine retrieval techniques with <b>large</b> <b>language</b> <b>models</b> to provide answers that are more accurate and informative. Many existing toolkits allow users to quickly build such systems using off-the-shelf models, but they fall short in supporting researchers and developers to customize the model training, testing, and deployment process. We propose LocalRQA, an open-source toolkit that features a wide selection of model training algorithms, evaluation methods, and deployment tools curated from the latest research. As a showcase, we build <b>QA</b> systems using online documentation obtained from Databricks and Faire&rsquo;s websites. We find 7B-models trained and deployed using LocalRQA reach a similar performance compared to using OpenAI&rsquo;s text-ada-002 and <b>GPT-4-turbo.</b></p></p class="citation"></blockquote><h3 id=1037--10243-large-language-models-for-simultaneous-named-entity-extraction-and-spelling-correction-edward-whittaker-et-al-2024>(10/37 | 10/243) Large Language Models for Simultaneous Named Entity Extraction and Spelling Correction (Edward Whittaker et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Edward Whittaker, Ikuo Kitagishi. (2024)<br><strong>Large Language Models for Simultaneous Named Entity Extraction and Spelling Correction</strong><br><button class=copy-to-clipboard title="Large Language Models for Simultaneous Named Entity Extraction and Spelling Correction" index=10>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-10 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: H-3-3; H-3-4; I-2-7; I-7-1; I-7-5, cs-CL, cs-CV, cs.CL<br>Keyword Score: 60<br>Keywords: Optical Character Recognition, Optical Character Recognition, Fine-tuning, BERT, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00528v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00528v1.pdf filename=2403.00528v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Language Models (LMs) such as <b>BERT,</b> have been shown to perform well on the task of identifying Named Entities (NE) in text. A <b>BERT</b> LM is typically used as a classifier to classify individual tokens in the input text, or to classify spans of tokens, as belonging to one of a set of possible NE categories. In this paper, we hypothesise that decoder-only <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> can also be used generatively to extract both the NE, as well as potentially recover the correct surface form of the NE, where any spelling errors that were present in the input text get automatically corrected. We <b>fine-tune</b> two <b>BERT</b> LMs as baselines, as well as eight open-source <b>LLMs,</b> on the task of producing NEs from text that was obtained by applying <b>Optical</b> <b>Character</b> <b>Recognition</b> <b>(OCR)</b> to images of Japanese shop receipts; in this work, we do not attempt to find or evaluate the location of NEs in the text. We show that the best <b>fine-tuned</b> <b>LLM</b> performs as well as, or slightly better than, the best <b>fine-tuned</b> <b>BERT</b> LM, although the differences are not significant. However, the best <b>LLM</b> is also shown to correct <b>OCR</b> errors in some cases, as initially hypothesised.</p></p class="citation"></blockquote><h3 id=1137--11243-formulation-comparison-for-timeline-construction-using-llms-kimihiro-hasegawa-et-al-2024>(11/37 | 11/243) Formulation Comparison for Timeline Construction using LLMs (Kimihiro Hasegawa et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kimihiro Hasegawa, Nikhil Kandukuri, Susan Holm, Yukari Yamakawa, Teruko Mitamura. (2024)<br><strong>Formulation Comparison for Timeline Construction using LLMs</strong><br><button class=copy-to-clipboard title="Formulation Comparison for Timeline Construction using LLMs" index=11>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-11 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 53<br>Keywords: Benchmarking, Few-shot, LLaMA, Natural Language Inference, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00990v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00990v1.pdf filename=2403.00990v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Constructing a timeline requires identifying the chronological order of events in an article. In prior timeline construction datasets, temporal orders are typically annotated by either event-to-time anchoring or event-to-event pairwise ordering, both of which suffer from missing temporal information. To mitigate the issue, we develop a new evaluation dataset, TimeSET, consisting of single-document timelines with document-level order annotation. TimeSET features saliency-based event selection and partial ordering, which enable a practical annotation workload. Aiming to build better automatic timeline construction systems, we propose a novel evaluation framework to compare multiple task formulations with TimeSET by <b>prompting</b> open <b>LLMs,</b> i.e., <b>Llama</b> 2 and Flan-T5. Considering that identifying temporal orders of events is a core subtask in timeline construction, we further <b>benchmark</b> open <b>LLMs</b> on existing event temporal ordering datasets to gain a robust understanding of their capabilities. Our experiments show that (1) <b>NLI</b> formulation with Flan-T5 demonstrates a strong performance among others, while (2) timeline construction and event temporal ordering are still challenging tasks for <b>few-shot</b> <b>LLMs.</b> Our code and data are available at <a href=https://github.com/kimihiroh/timeset>https://github.com/kimihiroh/timeset</a>.</p></p class="citation"></blockquote><h3 id=1237--12243-extracting-polymer-nanocomposite-samples-from-full-length-documents-ghazal-khalighinejad-et-al-2024>(12/37 | 12/243) Extracting Polymer Nanocomposite Samples from Full-Length Documents (Ghazal Khalighinejad et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ghazal Khalighinejad, Defne Circi, L. C. Brinson, Bhuwan Dhingra. (2024)<br><strong>Extracting Polymer Nanocomposite Samples from Full-Length Documents</strong><br><button class=copy-to-clipboard title="Extracting Polymer Nanocomposite Samples from Full-Length Documents" index=12>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-12 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 53<br>Keywords: Benchmarking, Zero-shot, Relation Extraction, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00260v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00260v1.pdf filename=2403.00260v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper investigates the use of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> for extracting sample lists of polymer nanocomposites (PNCs) from full-length materials science research papers. The challenge lies in the complex nature of PNC samples, which have numerous attributes scattered throughout the text. The complexity of annotating detailed information on PNCs limits the availability of data, making conventional document-level <b>relation</b> <b>extraction</b> techniques impractical due to the challenge in creating comprehensive named entity span annotations. To address this, we introduce a new <b>benchmark</b> and an evaluation technique for this task and explore different <b>prompting</b> strategies in a <b>zero-shot</b> manner. We also incorporate self-consistency to improve the performance. Our findings show that even advanced <b>LLMs</b> struggle to extract all of the samples from an article. Finally, we analyze the errors encountered in this process, categorizing them into three main challenges, and discuss potential strategies for future research to overcome them.</p></p class="citation"></blockquote><h3 id=1337--13243-hierarchical-indexing-for-retrieval-augmented-opinion-summarization-tom-hosking-et-al-2024>(13/37 | 13/243) Hierarchical Indexing for Retrieval-Augmented Opinion Summarization (Tom Hosking et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tom Hosking, Hao Tang, Mirella Lapata. (2024)<br><strong>Hierarchical Indexing for Retrieval-Augmented Opinion Summarization</strong><br><button class=copy-to-clipboard title="Hierarchical Indexing for Retrieval-Augmented Opinion Summarization" index=13>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-13 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Unsupervised Learning, Opinion Summarization, Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00435v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00435v1.pdf filename=2403.00435v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose a method for <b>unsupervised</b> abstractive <b>opinion</b> <b>summarization,</b> that combines the attributability and scalability of extractive approaches with the coherence and fluency of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> Our method, HIRO, learns an index structure that maps sentences to a path through a semantically organized discrete hierarchy. At inference time, we populate the index and use it to identify and retrieve clusters of sentences containing popular <b>opinions</b> <b>from</b> input reviews. Then, we use a pretrained <b>LLM</b> to generate a readable summary that is grounded in these extracted evidential clusters. The modularity of our approach allows us to evaluate its efficacy at each stage. We show that HIRO learns an encoding space that is more semantically structured than prior work, and generates summaries that are more representative of the <b>opinions</b> <b>in</b> the input reviews. Human evaluation confirms that HIRO generates more coherent, detailed and accurate summaries that are significantly preferred by annotators compared to prior work.</p></p class="citation"></blockquote><h3 id=1437--14243-axolotl-fairness-through-assisted-self-debiasing-of-large-language-model-outputs-sana-ebrahimi-et-al-2024>(14/37 | 14/243) AXOLOTL: Fairness through Assisted Self-Debiasing of Large Language Model Outputs (Sana Ebrahimi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sana Ebrahimi, Kaiwen Chen, Abolfazl Asudeh, Gautam Das, Nick Koudas. (2024)<br><strong>AXOLOTL: Fairness through Assisted Self-Debiasing of Large Language Model Outputs</strong><br><button class=copy-to-clipboard title="AXOLOTL: Fairness through Assisted Self-Debiasing of Large Language Model Outputs" index=14>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-14 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-CY, cs-LG, cs.CL<br>Keyword Score: 50<br>Keywords: Fairness, Zero-shot, Large Language Model, Large Language Model, Zero-shot Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00198v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00198v1.pdf filename=2403.00198v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Pre-trained <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have significantly advanced natural language processing capabilities but are susceptible to biases present in their training data, leading to unfair outcomes in various applications. While numerous strategies have been proposed to mitigate bias, they often require extensive computational resources and may compromise model performance. In this work, we introduce AXOLOTL, a novel post-processing framework, which operates agnostically across tasks and models, leveraging public APIs to interact with <b>LLMs</b> without direct access to internal parameters. Through a three-step process resembling <b>zero-shot</b> <b>learning,</b> AXOLOTL identifies biases, proposes resolutions, and guides the model to self-debias its outputs. This approach minimizes computational costs and preserves model performance, making AXOLOTL a promising tool for debiasing <b>LLM</b> outputs with broad applicability and ease of use.</p></p class="citation"></blockquote><h3 id=1537--15243-mediswift-efficient-sparse-pre-trained-biomedical-language-models-vithursan-thangarasa-et-al-2024>(15/37 | 15/243) MediSwift: Efficient Sparse Pre-trained Biomedical Language Models (Vithursan Thangarasa et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vithursan Thangarasa, Mahmoud Salem, Shreyas Saxena, Kevin Leong, Joel Hestness, Sean Lie. (2024)<br><strong>MediSwift: Efficient Sparse Pre-trained Biomedical Language Models</strong><br><button class=copy-to-clipboard title="MediSwift: Efficient Sparse Pre-trained Biomedical Language Models" index=15>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-15 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 43<br>Keywords: Benchmarking, Fine-tuning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00952v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00952v1.pdf filename=2403.00952v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> are typically trained on general source data for various domains, but a recent surge in domain-specific <b>LLMs</b> has shown their potential to outperform general-purpose models in domain-specific tasks (e.g., biomedicine). Although domain-specific pre-training enhances efficiency and leads to smaller models, the computational costs of training these <b>LLMs</b> remain high, posing budgeting challenges. We introduce MediSwift, a suite of biomedical LMs that leverage sparse pre-training on domain-specific biomedical text data. By inducing up to 75% weight sparsity during the pre-training phase, MediSwift achieves a 2-2.5x reduction in training FLOPs. Notably, all sparse pre-training was performed on the Cerebras CS-2 system, which is specifically designed to realize the acceleration benefits from unstructured weight sparsity, thereby significantly enhancing the efficiency of the MediSwift models. Through subsequent dense <b>fine-tuning</b> and strategic soft <b>prompting,</b> MediSwift models outperform existing <b>LLMs</b> up to 7B parameters on biomedical tasks, setting new <b>benchmarks</b> w.r.t efficiency-accuracy on tasks such as PubMedQA. Our results show that sparse pre-training, along with dense <b>fine-tuning</b> and soft <b>prompting,</b> offers an effective method for creating high-performing, computationally efficient models in specialized domains.</p></p class="citation"></blockquote><h3 id=1637--16243-standardizing-the-measurement-of-text-diversity-a-tool-and-a-comparative-analysis-of-scores-chantal-shaib-et-al-2024>(16/37 | 16/243) Standardizing the Measurement of Text Diversity: A Tool and a Comparative Analysis of Scores (Chantal Shaib et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chantal Shaib, Joe Barrow, Jiuding Sun, Alexa F. Siu, Byron C. Wallace, Ani Nenkova. (2024)<br><strong>Standardizing the Measurement of Text Diversity: A Tool and a Comparative Analysis of Scores</strong><br><button class=copy-to-clipboard title="Standardizing the Measurement of Text Diversity: A Tool and a Comparative Analysis of Scores" index=16>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-16 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: BERTScore, Instruction Tuning, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00553v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00553v1.pdf filename=2403.00553v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The diversity across outputs generated by <b>large</b> <b>language</b> <b>models</b> shapes the perception of their quality and utility. <b>Prompt</b> leaks, templated answer structure, and canned responses across different interactions are readily noticed by people, but there is no standard score to measure this aspect of model behavior. In this work we empirically investigate diversity scores on English texts. We find that computationally efficient compression algorithms capture information similar to what is measured by slow to compute $n$-gram overlap homogeneity scores. Further, a combination of measures &ndash; compression ratios, self-repetition of long $n$-grams and Self-BLEU and <b>BERTScore</b> &ndash; are sufficient to report, as they have low mutual correlation with each other. The applicability of scores extends beyond analysis of generative models; for example, we highlight applications on <b>instruction-tuning</b> <b>datasets</b> and human-produced texts. We release a diversity score package to facilitate research and invite consistency across reports.</p></p class="citation"></blockquote><h3 id=1737--17243-lucid-llm-generated-utterances-for-complex-and-interesting-dialogues-joe-stacey-et-al-2024>(17/37 | 17/243) LUCID: LLM-Generated Utterances for Complex and Interesting Dialogues (Joe Stacey et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Joe Stacey, Jianpeng Cheng, John Torr, Tristan Guigue, Joris Driesen, Alexandru Coca, Mark Gaynor, Anders Johannsen. (2024)<br><strong>LUCID: LLM-Generated Utterances for Complex and Interesting Dialogues</strong><br><button class=copy-to-clipboard title="LUCID: LLM-Generated Utterances for Complex and Interesting Dialogues" index=17>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-17 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: I-2-7, cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Out-of-distribution, Transformer, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00462v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00462v1.pdf filename=2403.00462v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Virtual assistants are poised to take a dramatic leap forward in terms of their dialogue capabilities, spurred by recent advances in <b>transformer-based</b> <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> Yet a major bottleneck to achieving genuinely transformative task-oriented dialogue capabilities remains the scarcity of high quality and linguistically sophisticated data. Existing datasets, while impressive in scale, have limited domain coverage and contain few genuinely challenging conversational phenomena; those which are present are typically unlabelled, making it difficult to assess the strengths and weaknesses of models without time-consuming and costly human evaluation. Moreover, creating high quality dialogue data has until now required considerable human input, limiting both the scale of these datasets and the ability to rapidly bootstrap data for a new target domain. We aim to overcome these issues with LUCID, a modularised and highly automated <b>LLM-driven</b> data generation system that produces realistic, diverse and challenging dialogues. We use LUCID to generate a seed dataset of 4,277 multi-domain, multi-intent conversations across 100 intents to demonstrate its capabilities. The generated conversations include a wide range of challenging phenomena and diverse user behaviour, conveniently identifiable via a set of turn-level tags. Finally, we provide separate test sets for seen and unseen intents, allowing for convenient <b>out-of-distribution</b> evaluation. We release both the data generation code and the dataset itself.</p></p class="citation"></blockquote><h3 id=1837--18243-peacock-a-family-of-arabic-multimodal-large-language-models-and-benchmarks-fakhraddin-alwajih-et-al-2024>(18/37 | 18/243) Peacock: A Family of Arabic Multimodal Large Language Models and Benchmarks (Fakhraddin Alwajih et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fakhraddin Alwajih, El Moatez Billah Nagoudi, Gagan Bhatia, Abdelrahman Mohamed, Muhammad Abdul-Mageed. (2024)<br><strong>Peacock: A Family of Arabic Multimodal Large Language Models and Benchmarks</strong><br><button class=copy-to-clipboard title="Peacock: A Family of Arabic Multimodal Large Language Models and Benchmarks" index=18>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-18 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 39<br>Keywords: Benchmarking, Multi-modal, Multi-modal, Reasoning, Large Language Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01031v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01031v1.pdf filename=2403.01031v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multimodal</b> <b>large</b> <b>language</b> <b>models</b> (MLLMs) have proven effective in a wide range of tasks requiring complex <b>reasoning</b> and linguistic comprehension. However, due to a lack of high-quality <b>multimodal</b> resources in languages other than English, success of MLLMs remains relatively limited to English-based settings. This poses significant challenges in developing comparable models for other languages, including even those with <b>large</b> <b>speaker</b> <b>populations</b> such as Arabic. To alleviate this challenge, we introduce a comprehensive family of Arabic MLLMs, dubbed \textit{Peacock}, with strong vision and language capabilities. Through comprehensive qualitative and quantitative analysis, we demonstrate the solid performance of our models on various visual <b>reasoning</b> tasks and further show their emerging dialectal potential. Additionally, we introduce ~\textit{Henna}, a new <b>benchmark</b> specifically designed for assessing MLLMs on aspects related to Arabic culture, setting the first stone for culturally-aware Arabic MLLMs.The GitHub repository for the \textit{Peacock} project is available at \url{https://github.com/UBC-NLP/peacock}.</p></p class="citation"></blockquote><h3 id=1937--19243-autord-an-automatic-and-end-to-end-system-for-rare-disease-knowledge-graph-construction-based-on-ontologies-enhanced-large-language-models-lang-cao-et-al-2024>(19/37 | 19/243) AutoRD: An Automatic and End-to-End System for Rare Disease Knowledge Graph Construction Based on Ontologies-enhanced Large Language Models (Lang Cao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lang Cao, Jimeng Sun, Adam Cross. (2024)<br><strong>AutoRD: An Automatic and End-to-End System for Rare Disease Knowledge Graph Construction Based on Ontologies-enhanced Large Language Models</strong><br><button class=copy-to-clipboard title="AutoRD: An Automatic and End-to-End System for Rare Disease Knowledge Graph Construction Based on Ontologies-enhanced Large Language Models" index=19>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-19 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 38<br>Keywords: Graph, Knowledge Graph, Relation Extraction, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00953v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00953v1.pdf filename=2403.00953v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Objectives: Our objective is to create an end-to-end system called AutoRD, which automates extracting information from clinical text about rare diseases. We have conducted various tests to evaluate the performance of AutoRD and highlighted its strengths and limitations in this paper. Materials and Methods: Our system, AutoRD, is a software pipeline involving data preprocessing, entity extraction, <b>relation</b> <b>extraction,</b> entity calibration, and <b>knowledge</b> <b>graph</b> construction. We implement this using <b>large</b> <b>language</b> <b>models</b> and medical <b>knowledge</b> <b>graphs</b> developed from open-source medical ontologies. We quantitatively evaluate our system on entity extraction, <b>relation</b> <b>extraction,</b> and the performance of <b>knowledge</b> <b>graph</b> construction. Results: AutoRD achieves an overall F1 score of 47.3%, a 14.4% improvement compared to the base <b>LLM.</b> In detail, AutoRD achieves an overall entity extraction F1 score of 56.1% (rare_disease: 83.5%, disease: 35.8%, symptom_and_sign: 46.1%, anaphor: 67.5%) and an overall <b>relation</b> <b>extraction</b> F1 score of 38.6% (produces: 34.7%, increases_risk_of: 12.4%, is_a: 37.4%, is_acronym: 44.1%, is_synonym: 16.3%, anaphora: 57.5%). Our qualitative experiment also demonstrates that the performance in constructing the <b>knowledge</b> <b>graph</b> is commendable. Discussion: AutoRD demonstrates the potential of <b>LLM</b> applications in rare disease detection. This improvement is attributed to several design, including the integration of ontologies-enhanced <b>LLMs.</b> Conclusion: AutoRD is an automated end-to-end system for extracting rare disease information from text to build <b>knowledge</b> <b>graphs.</b> It uses ontologies-enhanced <b>LLMs</b> for a robust medical <b>knowledge</b> <b>base.</b> The superior performance of AutoRD is validated by experimental evaluations, demonstrating the potential of <b>LLMs</b> in healthcare.</p></p class="citation"></blockquote><h3 id=2037--20243-merging-text-transformer-models-from-different-initializations-neha-verma-et-al-2024>(20/37 | 20/243) Merging Text Transformer Models from Different Initializations (Neha Verma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Neha Verma, Maha Elbayad. (2024)<br><strong>Merging Text Transformer Models from Different Initializations</strong><br><button class=copy-to-clipboard title="Merging Text Transformer Models from Different Initializations" index=20>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-20 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, Fine-tuning, Transformer, Masked Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00986v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00986v2.pdf filename=2403.00986v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent work on one-shot permutation-based model merging has shown impressive low- or zero-barrier mode connectivity between models from completely different initializations. However, this line of work has not yet extended to the <b>Transformer</b> architecture, despite its dominant popularity in the language domain. Therefore, in this work, we investigate the extent to which separate <b>Transformer</b> minima learn similar features, and propose a model merging technique to investigate the relationship between these minima in the loss landscape. The specifics of the architecture, like its residual connections, multi-headed attention, and discrete, sequential input, require specific interventions in order to compute model permutations that remain within the same functional equivalence class. In merging these models with our method, we consistently find lower loss barriers between minima compared to model averaging for several models trained on a <b>masked-language</b> <b>modeling</b> <b>task</b> or <b>fine-tuned</b> on a language understanding <b>benchmark.</b> Our results show that the minima of these models are less sharp and isolated than previously understood, and provide a basis for future work on merging separately trained <b>Transformer</b> models.</p></p class="citation"></blockquote><h3 id=2137--21243-self-consistent-decoding-for-more-factual-open-responses-christopher-malon-et-al-2024>(21/37 | 21/243) Self-Consistent Decoding for More Factual Open Responses (Christopher Malon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Christopher Malon, Xiaodan Zhu. (2024)<br><strong>Self-Consistent Decoding for More Factual Open Responses</strong><br><button class=copy-to-clipboard title="Self-Consistent Decoding for More Factual Open Responses" index=21>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-21 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, Natural Language Inference, Large Language Model, Rouge<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00696v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00696v1.pdf filename=2403.00696v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Self-consistency has emerged as a powerful method for improving the accuracy of short answers generated by <b>large</b> <b>language</b> <b>models.</b> As previously defined, it only concerns the accuracy of a final answer parsed from generated text. In this work, we extend the idea to open response generation, by integrating voting into the decoding method. Each output sentence is selected from among multiple samples, conditioning on the previous selections, based on a simple token overlap score. We compare this &ldquo;Sample & Select&rdquo; method to greedy decoding, beam search, nucleus sampling, and the recently introduced hallucination avoiding decoders of DoLA, P-CRR, and S-CRR. We show that Sample & Select improves factuality by a 30% relative margin against these decoders in <b>NLI-based</b> evaluation on the subsets of CNN/DM and XSum used in the FRANK <b>benchmark,</b> while maintaining comparable <b>ROUGE-1</b> F1 scores against reference summaries. We collect human verifications of the generated summaries, confirming the factual superiority of our method.</p></p class="citation"></blockquote><h3 id=2237--22243-diahalu-a-dialogue-level-hallucination-evaluation-benchmark-for-large-language-models-kedi-chen-et-al-2024>(22/37 | 22/243) DiaHalu: A Dialogue-level Hallucination Evaluation Benchmark for Large Language Models (Kedi Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kedi Chen, Qin Chen, Jie Zhou, Yishen He, Liang He. (2024)<br><strong>DiaHalu: A Dialogue-level Hallucination Evaluation Benchmark for Large Language Models</strong><br><button class=copy-to-clipboard title="DiaHalu: A Dialogue-level Hallucination Evaluation Benchmark for Large Language Models" index=22>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-22 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00896v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00896v1.pdf filename=2403.00896v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Since <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> achieve significant success in recent years, the hallucination issue remains a challenge, numerous <b>benchmarks</b> are proposed to detect the hallucination. Nevertheless, some of these <b>benchmarks</b> are not naturally generated by <b>LLMs</b> but are intentionally induced. Also, many merely focus on the factuality hallucination while ignoring the faithfulness hallucination. Additionally, although dialogue pattern is more widely utilized in the era of <b>LLMs,</b> current <b>benchmarks</b> only concentrate on sentence-level and passage-level hallucination. In this study, we propose DiaHalu, the first dialogue-level hallucination evaluation <b>benchmark</b> to our knowledge. Initially, we integrate the collected topics into system <b>prompts</b> and facilitate a dialogue between two ChatGPT3.5. Subsequently, we manually modify the contents that do not adhere to human language conventions and then have <b>LLMs</b> re-generate, simulating authentic human-machine interaction scenarios. Finally, professional scholars annotate all the samples in the dataset. DiaHalu covers four common multi-turn dialogue domains and five hallucination subtypes, extended from factuality and faithfulness hallucination. Experiments through some well-known <b>LLMs</b> and detection methods on the dataset show that DiaHalu is a challenging <b>benchmark,</b> holding significant value for further research.</p></p class="citation"></blockquote><h3 id=2337--23243-margin-discrepancy-based-adversarial-training-for-multi-domain-text-classification-yuan-wu-2024>(23/37 | 23/243) Margin Discrepancy-based Adversarial Training for Multi-Domain Text Classification (Yuan Wu, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuan Wu. (2024)<br><strong>Margin Discrepancy-based Adversarial Training for Multi-Domain Text Classification</strong><br><button class=copy-to-clipboard title="Margin Discrepancy-based Adversarial Training for Multi-Domain Text Classification" index=23>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-23 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 33<br>Keywords: Adversarial Learning, Benchmarking, Text Classification, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00888v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00888v1.pdf filename=2403.00888v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multi-domain <b>text</b> <b>classification</b> (MDTC) endeavors to harness available resources from correlated <b>domains</b> <b>to</b> enhance the classification accuracy of the target <b>domain.</b> <b>Presently,</b> most MDTC approaches that embrace <b>adversarial</b> <b>training</b> and the shared-private paradigm exhibit cutting-edge performance. Unfortunately, these methods face a non-negligible challenge: the absence of theoretical guarantees in the design of MDTC algorithms. The dearth of theoretical underpinning poses a substantial impediment to the advancement of MDTC algorithms. To tackle this problem, we first provide a theoretical analysis of MDTC by decomposing the MDTC task into multiple <b>domain</b> <b>adaptation</b> tasks. We incorporate the margin discrepancy as the measure of <b>domain</b> <b>divergence</b> and establish a new generalization bound based on Rademacher complexity. Subsequently, we propose a margin discrepancy-based <b>adversarial</b> <b>training</b> (MDAT) approach for MDTC, in accordance with our theoretical analysis. To validate the efficacy of the proposed MDAT method, we conduct empirical studies on two MDTC <b>benchmarks.</b> The experimental results demonstrate that our MDAT approach surpasses state-of-the-art baselines on both datasets.</p></p class="citation"></blockquote><h3 id=2437--24243-mitigating-reversal-curse-via-semantic-aware-permutation-training-qingyan-guo-et-al-2024>(24/37 | 24/243) Mitigating Reversal Curse via Semantic-aware Permutation Training (Qingyan Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qingyan Guo, Rui Wang, Junliang Guo, Xu Tan, Jiang Bian, Yujiu Yang. (2024)<br><strong>Mitigating Reversal Curse via Semantic-aware Permutation Training</strong><br><button class=copy-to-clipboard title="Mitigating Reversal Curse via Semantic-aware Permutation Training" index=24>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-24 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 30<br>Keywords: Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00758v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00758v2.pdf filename=2403.00758v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have achieved impressive performance across diverse tasks, recent studies showcase that causal <b>LLMs</b> suffer from the &ldquo;reversal curse&rdquo;. It is a typical example that the model knows &ldquo;A&rsquo;s father is B&rdquo;, but is unable to reason &ldquo;B&rsquo;s child is A&rdquo;. This limitation poses a challenge to the advancement of artificial general intelligence (AGI), as it suggests a gap in the models&rsquo; ability to comprehend and apply bidirectional <b>reasoning.</b> In this paper, we first conduct substantial evaluation and identify that the root cause of the reversal curse lies in the different word order between the training and inference stage, namely, the poor ability of causal language models to predict antecedent words within the training data. Accordingly, permutation on the training data is considered as a potential solution, since this can make the model predict antecedent words or tokens. However, previous permutation methods may disrupt complete phrases or entities, thereby posing challenges for the model to comprehend and learn from training data. To address this issue, we propose Semantic-aware Permutation Training (SPT), which addresses this issue by segmenting the training sentences into semantic units (i.e., entities or phrases) with an assistant language model and permuting these units before feeding into the model. Extensive experiments demonstrate that SPT effectively mitigates the reversal curse since the performance on reversed questions approximates that on the forward ones, and significantly advances the performance of existing works.</p></p class="citation"></blockquote><h3 id=2537--25243-rethinking-tokenization-crafting-better-tokenizers-for-large-language-models-jinbiao-yang-2024>(25/37 | 25/243) Rethinking Tokenization: Crafting Better Tokenizers for Large Language Models (Jinbiao Yang, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinbiao Yang. (2024)<br><strong>Rethinking Tokenization: Crafting Better Tokenizers for Large Language Models</strong><br><button class=copy-to-clipboard title="Rethinking Tokenization: Crafting Better Tokenizers for Large Language Models" index=25>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-25 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Tokenization, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00417v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00417v1.pdf filename=2403.00417v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Tokenization</b> significantly influences language models(LMs)&rsquo; performance. This paper traces the evolution of tokenizers from word-level to subword-level, analyzing how they balance tokens and types to enhance model adaptability while controlling complexity. Despite subword tokenizers like Byte Pair Encoding (BPE) overcoming many word tokenizer limitations, they encounter difficulties in handling non-Latin languages and depend heavily on extensive training data and computational resources to grasp the nuances of multiword expressions (MWEs). This article argues that tokenizers, more than mere technical tools, should drawing inspiration from the cognitive science about human language processing. This study then introduces the &ldquo;Principle of Least Effort&rdquo; from cognitive science, that humans naturally seek to reduce cognitive effort, and discusses the benefits of this principle for tokenizer development. Based on this principle, the paper proposes that the Less-is-Better (LiB) model could be a new approach for <b>LLM</b> tokenizer. The LiB model can autonomously learn an integrated vocabulary consisting of subwords, words, and MWEs, which effectively reduces both the numbers of tokens and types. Comparative evaluations show that the LiB tokenizer outperforms existing word and BPE tokenizers, presenting an innovative method for tokenizer development, and hinting at the possibility of future cognitive science-based tokenizers being more efficient.</p></p class="citation"></blockquote><h3 id=2637--26243-post-decoder-biasing-for-end-to-end-speech-recognition-of-multi-turn-medical-interview-heyang-liu-et-al-2024>(26/37 | 26/243) Post-decoder Biasing for End-to-End Speech Recognition of Multi-turn Medical Interview (Heyang Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Heyang Liu, Yu Wang, Yanfeng Wang. (2024)<br><strong>Post-decoder Biasing for End-to-End Speech Recognition of Multi-turn Medical Interview</strong><br><button class=copy-to-clipboard title="Post-decoder Biasing for End-to-End Speech Recognition of Multi-turn Medical Interview" index=26>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-26 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-SD, cs.CL, eess-AS<br>Keyword Score: 30<br>Keywords: Automatic Speech Recognition, Automatic Speech Recognition, Automatic Speech Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00370v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00370v1.pdf filename=2403.00370v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>End-to-end (E2E) approach is gradually replacing hybrid models for <b>automatic</b> <b>speech</b> <b>recognition</b> <b>(ASR)</b> tasks. However, the optimization of E2E models lacks an intuitive method for handling decoding shifts, especially in scenarios with a large number of domain-specific rare words that hold specific important meanings. Furthermore, the absence of knowledge-intensive <b>speech</b> <b>datasets</b> in academia has been a significant limiting factor, and the commonly used <b>speech</b> <b>corpora</b> exhibit significant disparities with realistic conversation. To address these challenges, we present Medical Interview (MED-IT), a multi-turn consultation <b>speech</b> <b>dataset</b> that contains a substantial number of knowledge-intensive named entities. We also explore methods to enhance the recognition performance of rare words for E2E models. We propose a novel approach, post-decoder biasing, which constructs a transform probability matrix based on the distribution of training transcriptions. This guides the model to prioritize recognizing words in the biasing list. In our experiments, for subsets of rare words appearing in the training <b>speech</b> <b>between</b> 10 and 20 times, and between 1 and 5 times, the proposed method achieves a relative improvement of 9.3% and 5.1%, respectively.</p></p class="citation"></blockquote><h3 id=2737--27243-semi-instruct-bridging-natural-instruct-and-self-instruct-for-code-large-language-models-xianzhen-luo-et-al-2024>(27/37 | 27/243) Semi-Instruct: Bridging Natural-Instruct and Self-Instruct for Code Large Language Models (Xianzhen Luo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xianzhen Luo, Qingfu Zhu, Zhiming Zhang, Xu Wang, Qing Yang, Dongliang Xu, Wanxiang Che. (2024)<br><strong>Semi-Instruct: Bridging Natural-Instruct and Self-Instruct for Code Large Language Models</strong><br><button class=copy-to-clipboard title="Semi-Instruct: Bridging Natural-Instruct and Self-Instruct for Code Large Language Models" index=27>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-27 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Instruction Tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00338v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00338v1.pdf filename=2403.00338v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Instruction</b> <b>tuning</b> plays a pivotal role in Code <b>Large</b> <b>Language</b> <b>Models</b> (Code <b>LLMs)</b> for the task of program synthesis. Presently, two dominant paradigms for collecting tuning data are natural-instruct (human-written) and self-instruct (automatically generated). Natural-instruct includes diverse and correct codes but lacks <b>instruction-code</b> <b>pairs,</b> and exists improper code formats like nested single-line codes. In contrast, self-instruct automatically generates proper paired data. However, it suffers from low diversity due to generating duplicates and cannot ensure the correctness of codes. To bridge the both paradigms, we propose \textbf{Semi-Instruct}. It first converts diverse but improper codes from natural-instruct into proper <b>instruction-code</b> <b>pairs</b> through a method similar to self-instruct. To verify the correctness of generated codes, we design a novel way to construct test cases by generating cases&rsquo; inputs and executing correct codes from natural-instruct to get outputs. Finally, diverse and correct <b>instruction-code</b> <b>pairs</b> are retained for <b>instruction</b> <b>tuning.</b> Experiments show that semi-instruct is significantly better than natural-instruct and self-instruct. Furthermore, the performance steadily improves as data scale increases.</p></p class="citation"></blockquote><h3 id=2837--28243-dpp-based-adversarial-prompt-searching-for-lanugage-models-xu-zhang-et-al-2024>(28/37 | 28/243) DPP-Based Adversarial Prompt Searching for Lanugage Models (Xu Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xu Zhang, Xiaojun Wan. (2024)<br><strong>DPP-Based Adversarial Prompt Searching for Lanugage Models</strong><br><button class=copy-to-clipboard title="DPP-Based Adversarial Prompt Searching for Lanugage Models" index=28>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-28 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Perplexity, Pre-trained Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00292v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00292v1.pdf filename=2403.00292v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Language models risk generating mindless and offensive content, which hinders their safe deployment. Therefore, it is crucial to discover and modify potential toxic outputs of <b>pre-trained</b> <b>language</b> <b>models</b> before deployment. In this work, we elicit toxic content by automatically searching for a <b>prompt</b> that directs <b>pre-trained</b> <b>language</b> <b>models</b> towards the generation of a specific target output. The problem is challenging due to the discrete nature of textual data and the considerable computational resources required for a single forward pass of the language model. To combat these challenges, we introduce Auto-regressive Selective Replacement Ascent (ASRA), a discrete optimization algorithm that selects <b>prompts</b> based on both quality and similarity with determinantal point process (DPP). Experimental results on six different <b>pre-trained</b> <b>language</b> <b>models</b> demonstrate the efficacy of ASRA for eliciting toxic content. Furthermore, our analysis reveals a strong correlation between the success rate of ASRA attacks and the <b>perplexity</b> of target outputs, while indicating limited association with the quantity of model parameters.</p></p class="citation"></blockquote><h3 id=2937--29243-gender-bias-in-large-language-models-across-multiple-languages-jinman-zhao-et-al-2024>(29/37 | 29/243) Gender Bias in Large Language Models across Multiple Languages (Jinman Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinman Zhao, Yitian Ding, Chen Jia, Yining Wang, Zifan Qian. (2024)<br><strong>Gender Bias in Large Language Models across Multiple Languages</strong><br><button class=copy-to-clipboard title="Gender Bias in Large Language Models across Multiple Languages" index=29>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-29 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: GPT, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00277v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00277v1.pdf filename=2403.00277v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the growing deployment of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> across various applications, assessing the influence of gender biases embedded in <b>LLMs</b> becomes crucial. The topic of gender bias within the realm of natural language processing (NLP) has gained considerable focus, particularly in the context of English. Nonetheless, the investigation of gender bias in languages other than English is still relatively under-explored and insufficiently analyzed. In this work, We examine gender bias in <b>LLMs-generated</b> outputs for different languages. We use three measurements: 1) gender bias in selecting descriptive words given the gender-related context. 2) gender bias in selecting gender-related pronouns (she/he) given the descriptive words. 3) gender bias in the topics of <b>LLM-generated</b> dialogues. We investigate the outputs of the <b>GPT</b> series of <b>LLMs</b> in various languages using our three measurement methods. Our findings revealed significant gender biases across all the languages we examined.</p></p class="citation"></blockquote><h3 id=3037--30243-transcription-and-translation-of-videos-using-fine-tuned-xlsr-wav2vec2-on-custom-dataset-and-mbart-aniket-tathe-et-al-2024>(30/37 | 30/243) Transcription and translation of videos using fine-tuned XLSR Wav2Vec2 on custom dataset and mBART (Aniket Tathe et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aniket Tathe, Anand Kamble, Suyash Kumbharkar, Atharva Bhandare, Anirban C. Mitra. (2024)<br><strong>Transcription and translation of videos using fine-tuned XLSR Wav2Vec2 on custom dataset and mBART</strong><br><button class=copy-to-clipboard title="Transcription and translation of videos using fine-tuned XLSR Wav2Vec2 on custom dataset and mBART" index=30>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-30 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CV, cs-LG, cs-SD, cs.CL, eess-AS<br>Keyword Score: 30<br>Keywords: Fine-tuning, Self-supervised Learning, Automatic Speech Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00212v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00212v1.pdf filename=2403.00212v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This research addresses the challenge of training an <b>ASR</b> model for personalized voices with minimal data. Utilizing just 14 minutes of custom audio from a YouTube video, we employ Retrieval-Based Voice Conversion (RVC) to create a custom Common Voice 16.0 corpus. Subsequently, a Cross-lingual <b>Self-supervised</b> Representations (XLSR) Wav2Vec2 model is <b>fine-tuned</b> on this dataset. The developed web-based GUI efficiently transcribes and translates input Hindi videos. By integrating XLSR Wav2Vec2 and mBART, the system aligns the translated text with the video timeline, delivering an accessible solution for multilingual video content transcription and translation for personalized voice.</p></p class="citation"></blockquote><h3 id=3137--31243-few-shot-relation-extraction-with-hybrid-visual-evidence-jiaying-gong-et-al-2024>(31/37 | 31/243) Few-Shot Relation Extraction with Hybrid Visual Evidence (Jiaying Gong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiaying Gong, Hoda Eldardiry. (2024)<br><strong>Few-Shot Relation Extraction with Hybrid Visual Evidence</strong><br><button class=copy-to-clipboard title="Few-Shot Relation Extraction with Hybrid Visual Evidence" index=31>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-31 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CV, cs.CL<br>Keyword Score: 23<br>Keywords: Few-shot, Multi-modal, Relation Extraction<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00724v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00724v1.pdf filename=2403.00724v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The goal of <b>few-shot</b> <b>relation</b> <b>extraction</b> is to predict <b>relations</b> <b>between</b> name entities in a sentence when only a few labeled instances are available for training. Existing <b>few-shot</b> <b>relation</b> <b>extraction</b> methods focus on uni-modal information such as text only. This reduces performance when there are no clear contexts between the name entities described in text. We propose a <b>multi-modal</b> <b>few-shot</b> <b>relation</b> <b>extraction</b> model (MFS-HVE) that leverages both textual and visual semantic information to learn a <b>multi-modal</b> representation jointly. The MFS-HVE includes semantic feature extractors and <b>multi-modal</b> fusion components. The MFS-HVE semantic feature extractors are developed to extract both textual and visual features. The visual features include global image features and local object features within the image. The MFS-HVE <b>multi-modal</b> fusion unit integrates information from various modalities using image-guided attention, object-guided attention, and hybrid feature attention to fully capture the semantic interaction between visual regions of images and relevant texts. Extensive experiments conducted on two public datasets demonstrate that semantic visual information significantly improves the performance of <b>few-shot</b> <b>relation</b> <b>prediction.</b></p></p class="citation"></blockquote><h3 id=3237--32243-do-zombies-understand-a-choose-your-own-adventure-exploration-of-machine-cognition-ariel-goldstein-et-al-2024>(32/37 | 32/243) Do Zombies Understand? A Choose-Your-Own-Adventure Exploration of Machine Cognition (Ariel Goldstein et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ariel Goldstein, Gabriel Stanovsky. (2024)<br><strong>Do Zombies Understand? A Choose-Your-Own-Adventure Exploration of Machine Cognition</strong><br><button class=copy-to-clipboard title="Do Zombies Understand? A Choose-Your-Own-Adventure Exploration of Machine Cognition" index=32>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-32 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 23<br>Keywords: Benchmarking, Chatbot, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00499v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00499v1.pdf filename=2403.00499v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advances in <b>LLMs</b> have sparked a debate on whether they understand text. In this position paper, we argue that opponents in this debate hold different definitions for understanding, and particularly differ in their view on the role of consciousness. To substantiate this claim, we propose a thought experiment involving an open-source <b>chatbot</b> $Z$ which excels on every possible <b>benchmark,</b> seemingly without subjective experience. We ask whether $Z$ is capable of understanding, and show that different schools of thought within seminal AI research seem to answer this question differently, uncovering their terminological disagreement. Moving forward, we propose two distinct working definitions for understanding which explicitly acknowledge the question of consciousness, and draw connections with a rich literature in philosophy, psychology and neuroscience.</p></p class="citation"></blockquote><h3 id=3337--33243-rome-memorization-insights-from-text-probability-and-hidden-state-in-large-language-models-bo-li-et-al-2024>(33/37 | 33/243) ROME: Memorization Insights from Text, Probability and Hidden State in Large Language Models (Bo Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bo Li, Qinghua Zhao, Lijie Wen. (2024)<br><strong>ROME: Memorization Insights from Text, Probability and Hidden State in Large Language Models</strong><br><button class=copy-to-clipboard title="ROME: Memorization Insights from Text, Probability and Hidden State in Large Language Models" index=33>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-33 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00510v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00510v2.pdf filename=2403.00510v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Probing the memorization of <b>large</b> <b>language</b> <b>models</b> holds significant importance. Previous works have established metrics for quantifying memorization, explored various influencing factors, such as data duplication, model size, and <b>prompt</b> length, and evaluated memorization by comparing model outputs with training corpora. However, the training corpora are of enormous scale and its pre-processing is time-consuming. To explore memorization without accessing training data, we propose a novel approach, named ROME, wherein memorization is explored by comparing disparities across memorized and non-memorized. Specifically, models firstly categorize the selected samples into memorized and non-memorized groups, and then comparing the demonstrations in the two groups from the insights of text, probability, and hidden state. Experimental findings show the disparities in factors including word length, part-of-speech, word frequency, mean and variance, just to name a few.</p></p class="citation"></blockquote><h3 id=3437--34243-a-semantic-distance-metric-learning-approach-for-lexical-semantic-change-detection-taichi-aida-et-al-2024>(34/37 | 34/243) A Semantic Distance Metric Learning approach for Lexical Semantic Change Detection (Taichi Aida et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Taichi Aida, Danushka Bollegala. (2024)<br><strong>A Semantic Distance Metric Learning approach for Lexical Semantic Change Detection</strong><br><button class=copy-to-clipboard title="A Semantic Distance Metric Learning approach for Lexical Semantic Change Detection" index=34>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-34 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 13<br>Keywords: Benchmarking, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00226v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00226v1.pdf filename=2403.00226v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Detecting temporal semantic changes of words is an important task for various NLP applications that must make time-sensitive predictions. Lexical Semantic Change Detection (SCD) task considers the problem of predicting whether a given target word, $w$, changes its meaning between two different text corpora, $C_1$ and $C_2$. For this purpose, we propose a <b>supervised</b> two-staged SCD method that uses existing Word-in-Context (WiC) datasets. In the first stage, for a target word $w$, we learn two sense-aware encoder that represents the meaning of $w$ in a given sentence selected from a corpus. Next, in the second stage, we learn a sense-aware distance metric that compares the semantic representations of a target word across all of its occurrences in $C_1$ and $C_2$. Experimental results on multiple <b>benchmark</b> datasets for SCD show that our proposed method consistently outperforms all previously proposed SCD methods for multiple languages, establishing a novel state-of-the-art for SCD. Interestingly, our findings imply that there are specialised dimensions that carry information related to semantic changes of words in the sense-aware embedding space. Source code is available at <a href=https://github.com/a1da4/svp-sdml>https://github.com/a1da4/svp-sdml</a> .</p></p class="citation"></blockquote><h3 id=3537--35243-predictions-from-language-models-for-multiple-choice-tasks-are-not-robust-under-variation-of-scoring-methods-polina-tsvilodub-et-al-2024>(35/37 | 35/243) Predictions from language models for multiple-choice tasks are not robust under variation of scoring methods (Polina Tsvilodub et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Polina Tsvilodub, Hening Wang, Sharon Grosch, Michael Franke. (2024)<br><strong>Predictions from language models for multiple-choice tasks are not robust under variation of scoring methods</strong><br><button class=copy-to-clipboard title="Predictions from language models for multiple-choice tasks are not robust under variation of scoring methods" index=35>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-35 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00998v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00998v1.pdf filename=2403.00998v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper systematically compares different methods of deriving item-level predictions of language models for multiple-choice tasks. It compares scoring methods for answer options based on free generation of responses, various probability-based scores, a Likert-scale style rating method, and embedding similarity. In a case study on pragmatic language interpretation, we find that <b>LLM</b> predictions are not robust under variation of method choice, both within a single <b>LLM</b> and across different <b>LLMs.</b> As this variability entails pronounced researcher degrees of freedom in reporting results, knowledge of the variability is crucial to secure robustness of results and research integrity.</p></p class="citation"></blockquote><h3 id=3637--36243-self-consistent-reasoning-based-aspect-sentiment-quad-prediction-with-extract-then-assign-strategy-jieyong-kim-et-al-2024>(36/37 | 36/243) Self-Consistent Reasoning-based Aspect-Sentiment Quad Prediction with Extract-Then-Assign Strategy (Jieyong Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jieyong Kim, Ryang Heo, Yongsik Seo, SeongKu Kang, Jinyoung Yeo, Dongha Lee. (2024)<br><strong>Self-Consistent Reasoning-based Aspect-Sentiment Quad Prediction with Extract-Then-Assign Strategy</strong><br><button class=copy-to-clipboard title="Self-Consistent Reasoning-based Aspect-Sentiment Quad Prediction with Extract-Then-Assign Strategy" index=36>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-36 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00354v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00354v1.pdf filename=2403.00354v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the task of aspect sentiment quad prediction (ASQP), generative methods for predicting sentiment quads have shown promising results. However, they still suffer from imprecise predictions and limited interpretability, caused by data scarcity and inadequate modeling of the quadruplet composition process. In this paper, we propose Self-Consistent <b>Reasoning-based</b> Aspect-sentiment quadruple Prediction (SCRAP), optimizing its model to generate <b>reasonings</b> and the corresponding sentiment quadruplets in sequence. SCRAP adopts the Extract-Then-Assign <b>reasoning</b> strategy, which closely mimics human cognition. In the end, SCRAP significantly improves the model&rsquo;s ability to handle complex <b>reasoning</b> tasks and correctly predict quadruplets through consistency voting, resulting in enhanced interpretability and accuracy in ASQP.</p></p class="citation"></blockquote><h3 id=3737--37243-word-order-and-world-knowledge-qinghua-zhao-et-al-2024>(37/37 | 37/243) Word Order and World Knowledge (Qinghua Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qinghua Zhao, Vinit Ravishankar, Nicolas Garneau, Anders Søgaard. (2024)<br><strong>Word Order and World Knowledge</strong><br><button class=copy-to-clipboard title="Word Order and World Knowledge" index=37>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-37 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00876v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00876v1.pdf filename=2403.00876v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Word order is an important concept in natural language, and in this work, we study how word order affects the induction of world knowledge from raw text using language models. We use word analogies to probe for such knowledge. Specifically, in addition to the natural word order, we first respectively extract texts of six fixed word orders from five languages and then pretrain the language models on these texts. Finally, we analyze the experimental results of the fixed word orders on word analogies and show that i) certain fixed word orders consistently outperform or underperform others, though the specifics vary across languages, and ii) the Wov2Lex hypothesis is not hold in <b>pre-trained</b> <b>language</b> <b>models,</b> and the natural word order typically yields mediocre results. The source code will be made publicly available at <a href=https://github.com/lshowway/probing_by_analogy>https://github.com/lshowway/probing_by_analogy</a>.</p></p class="citation"></blockquote><h2 id=csdb-2>cs.DB (2)</h2><h3 id=12--38243-dfin-sql-integrating-focused-schema-with-din-sql-for-superior-accuracy-in-large-scale-databases-shai-volvovsky-et-al-2024>(1/2 | 38/243) DFIN-SQL: Integrating Focused Schema with DIN-SQL for Superior Accuracy in Large-Scale Databases (Shai Volvovsky et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shai Volvovsky, Marco Marcassa, Mustafa Panbiharwala. (2024)<br><strong>DFIN-SQL: Integrating Focused Schema with DIN-SQL for Superior Accuracy in Large-Scale Databases</strong><br><button class=copy-to-clipboard title="DFIN-SQL: Integrating Focused Schema with DIN-SQL for Superior Accuracy in Large-Scale Databases" index=38>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-38 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DB<br>Categories: cs-AI, cs-DB, cs.DB<br>Keyword Score: 103<br>Keywords: Benchmarking, Fine-tuning, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, GPT, GPT-4, Text2SQL, In-context Learning, In-context Learning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00872v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00872v1.pdf filename=2403.00872v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The task of converting natural language queries into SQL queries is intricate, necessitating a blend of precise techniques for an accurate translation. The DIN-SQL (Decomposed-In-Context SQL) methodology represents a significant development in this domain. This paper introduces DFIN (Decomposed Focused-In-Context), an innovative extension of DIN-SQL that enhances <b>Text-to-SQL</b> conversion by addressing schema linking errors, which are a major source of inaccuracies. DFIN uniquely alternates between <b>prompting</b> techniques and <b>Retrieval-Augmented</b> <b>Generation</b> <b>(RAG),</b> adapting to the size and complexity of the database schema. A preprocessing phase embeds database definitions and leverages annotated files, akin to those in the BIRD dataset, facilitating the runtime <b>retrieval</b> <b>of</b> <b>pertinent</b> schema information. This strategy significantly reduces the token count for schema linking <b>prompts,</b> enabling the use of a standard <b>GPT-4</b> model over its larger context variant, thus handling large-scale databases more effectively and economically. Our evaluation on the BIRD dataset, a challenging real-world <b>benchmark,</b> demonstrates that DFIN not only scales efficiently but also improves accuracy, achieving a score of 51.69. This improvement surpasses DIN-SQL method (the current third-place), which is the highest-ranked model employing <b>in-context</b> <b>learning</b> rather than <b>fine-tuning,</b> previously scoring 50.72. The advancement of DFIN underscores the evolving capabilities of <b>in-context</b> <b>learning</b> methodologies combined with advanced language models, offering a promising avenue for future research in complex <b>Text-to-SQL</b> conversion tasks.</p></p class="citation"></blockquote><h3 id=22--39243-text-classification-of-column-headers-with-a-controlled-vocabulary-leveraging-llms-for-metadata-enrichment-margherita-martorana-et-al-2024>(2/2 | 39/243) Text classification of column headers with a controlled vocabulary: leveraging LLMs for metadata enrichment (Margherita Martorana et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Margherita Martorana, Tobias Kuhn, Lise Stork, Jacco van Ossenbruggen. (2024)<br><strong>Text classification of column headers with a controlled vocabulary: leveraging LLMs for metadata enrichment</strong><br><button class=copy-to-clipboard title="Text classification of column headers with a controlled vocabulary: leveraging LLMs for metadata enrichment" index=39>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-39 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DB<br>Categories: cs-AI, cs-DB, cs-IR, cs.DB<br>Keyword Score: 40<br>Keywords: ChatGPT, Text Classification, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00884v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00884v2.pdf filename=2403.00884v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Traditional dataset retrieval systems index on metadata information rather than on the data values. Thus relying primarily on manual annotations and high-quality metadata, processes known to be labour-intensive and challenging to automate. We propose a method to support metadata enrichment with topic annotations of column headers using three <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs):</b> <b>ChatGPT-3.5,</b> GoogleBard and GoogleGemini. We investigate the <b>LLMs</b> ability to classify column headers based on domain-specific topics from a controlled vocabulary. We evaluate our approach by assessing the internal consistency of the <b>LLMs,</b> the inter-machine alignment, and the human-machine agreement for the topic classification task. Additionally, we investigate the impact of contextual information (i.e. dataset description) on the classification outcomes. Our results suggest that <b>ChatGPT</b> and GoogleGemini outperform GoogleBard for internal consistency as well as <b>LLM-human-alignment.</b> Interestingly, we found that context had no impact on the <b>LLMs</b> performances. This work proposes a novel approach that leverages <b>LLMs</b> for <b>text</b> <b>classification</b> using a controlled topic vocabulary, which has the potential to facilitate automated metadata enrichment, thereby enhancing dataset retrieval and the Findability, Accessibility, Interoperability and Reusability (FAIR) of research data on the Web.</p></p class="citation"></blockquote><h2 id=cslg-40>cs.LG (40)</h2><h3 id=140--40243-differentially-private-knowledge-distillation-via-synthetic-text-generation-james-flemings-et-al-2024>(1/40 | 40/243) Differentially Private Knowledge Distillation via Synthetic Text Generation (James Flemings et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>James Flemings, Murali Annavaram. (2024)<br><strong>Differentially Private Knowledge Distillation via Synthetic Text Generation</strong><br><button class=copy-to-clipboard title="Differentially Private Knowledge Distillation via Synthetic Text Generation" index=40>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-40 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-CR, cs-LG, cs.LG<br>Keyword Score: 80<br>Keywords: Knowledge Distillation, Knowledge Distillation, Knowledge Distillation, Model Compression, Text Generation, Large Language Model, Large Language Model, Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00932v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00932v1.pdf filename=2403.00932v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>models</b> <b>(LLMs)</b> are achieving state-of-the-art performance in many different downstream tasks. However, the increasing urgency of data privacy requires <b>LLMs</b> to train with <b>Differential</b> <b>Privacy</b> (DP) on private data. Concurrently it is also necessary to compress <b>LLMs</b> for real-life deployments on resource-constrained devices or latency-sensitive applications. <b>Differential</b> <b>privacy</b> and <b>model</b> <b>compression</b> generally must trade off utility loss to achieve their objectives. Moreover, concurrently achieving both can result in even more utility loss. To this end, we propose a novel differentially private <b>knowledge</b> <b>distillation</b> algorithm that exploits synthetic data generated by a differentially private <b>LLM.</b> The <b>knowledge</b> <b>of</b> a teacher <b>model</b> <b>is</b> transferred onto the student in two ways: one way from the synthetic data itself, the hard labels, and the other way by the output distribution of the teacher <b>model</b> <b>evaluated</b> on the synthetic data, the soft labels. Furthermore, if the teacher and student share a similar architectural structure, we can further <b>distill</b> <b>knowledge</b> <b>by</b> exploiting hidden representations. Our results show that our framework substantially improves the utility over existing baselines with strong privacy parameters, {\epsilon} = 2, validating that we can successfully compress autoregressive <b>LLMs</b> while preserving the privacy of training data.</p></p class="citation"></blockquote><h3 id=240--41243-a-survey-of-geometric-graph-neural-networks-data-structures-models-and-applications-jiaqi-han-et-al-2024>(2/40 | 41/243) A Survey of Geometric Graph Neural Networks: Data Structures, Models and Applications (Jiaqi Han et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiaqi Han, Jiacheng Cen, Liming Wu, Zongzhao Li, Xiangzhe Kong, Rui Jiao, Ziyang Yu, Tingyang Xu, Fandi Wu, Zihe Wang, Hongteng Xu, Zhewei Wei, Yang Liu, Yu Rong, Wenbing Huang. (2024)<br><strong>A Survey of Geometric Graph Neural Networks: Data Structures, Models and Applications</strong><br><button class=copy-to-clipboard title="A Survey of Geometric Graph Neural Networks: Data Structures, Models and Applications" index=41>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-41 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 48<br>Keywords: Message-Passing, Graph, Graph Neural Network, Graph Neural Network, Geometry, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00485v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00485v1.pdf filename=2403.00485v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Geometric <b>graph</b> <b>is</b> <b>a</b> special kind of <b>graph</b> <b>with</b> <b>geometric</b> features, which is vital to model many scientific problems. Unlike generic <b>graphs,</b> <b>geometric</b> <b>graphs</b> <b>often</b> <b>exhibit</b> physical symmetries of translations, rotations, and reflections, making them ineffectively processed by current <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs).</b> To tackle this issue, researchers proposed a variety of Geometric <b>Graph</b> <b>Neural</b> <b>Networks</b> equipped with invariant/equivariant properties to better characterize the <b>geometry</b> and topology of geometric <b>graphs.</b> <b>Given</b> <b>the</b> current progress in this field, it is imperative to conduct a comprehensive survey of data structures, models, and applications related to geometric <b>GNNs.</b> In this paper, based on the necessary but concise mathematical preliminaries, we provide a unified view of existing models from the geometric message passing perspective. Additionally, we <b>summarize</b> the applications as well as the related datasets to facilitate later research for methodology development and experimental evaluation. We also discuss the challenges and future potential directions of Geometric <b>GNNs</b> at the end of this survey.</p></p class="citation"></blockquote><h3 id=340--42243-cloud-based-federated-learning-framework-for-mri-segmentation-rukesh-prajapati-et-al-2024>(3/40 | 42/243) Cloud-based Federated Learning Framework for MRI Segmentation (Rukesh Prajapati et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rukesh Prajapati, Amr S. El-Wakeel. (2024)<br><strong>Cloud-based Federated Learning Framework for MRI Segmentation</strong><br><button class=copy-to-clipboard title="Cloud-based Federated Learning Framework for MRI Segmentation" index=42>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-42 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Convolution, Convolutional Neural Network, Federated Learning, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00254v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00254v1.pdf filename=2403.00254v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In contemporary rural healthcare settings, the principal challenge in diagnosing brain images is the scarcity of available data, given that most of the existing deep learning models demand extensive training data to optimize their performance, necessitating centralized processing methods that potentially compromise data privacy. This paper proposes a novel framework tailored for brain tissue segmentation in rural healthcare facilities. The framework employs a deep <b>reinforcement</b> <b>learning</b> (DRL) environment in tandem with a refinement model (RM) deployed locally at rural healthcare sites. The proposed DRL model has a reduced parameter count and practicality for implementation across distributed rural sites. To uphold data privacy and enhance model generalization without transgressing privacy constraints, we employ <b>federated</b> <b>learning</b> (FL) for cooperative model training. We demonstrate the efficacy of our approach by training the network with a limited data set and observing a substantial performance enhancement, mitigating inaccuracies and irregularities in segmentation across diverse sites. Remarkably, the DRL model attains an accuracy of up to 80%, surpassing the capabilities of conventional <b>convolutional</b> <b>neural</b> <b>networks</b> when confronted with data insufficiency. Incorporating our RM results in an additional accuracy improvement of at least 10%, while FL contributes to a further accuracy enhancement of up to 5%. Collectively, the framework achieves an average 92% accuracy rate within rural healthcare settings characterized by data constraints.</p></p class="citation"></blockquote><h3 id=440--43243-overestimation-overfitting-and-plasticity-in-actor-critic-the-bitter-lesson-of-reinforcement-learning-michal-nauman-et-al-2024>(4/40 | 43/243) Overestimation, Overfitting, and Plasticity in Actor-Critic: the Bitter Lesson of Reinforcement Learning (Michal Nauman et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michal Nauman, Michał Bortkiewicz, Mateusz Ostaszewski, Piotr Miłoś, Tomasz Trzciński, Marek Cygan. (2024)<br><strong>Overestimation, Overfitting, and Plasticity in Actor-Critic: the Bitter Lesson of Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Overestimation, Overfitting, and Plasticity in Actor-Critic: the Bitter Lesson of Reinforcement Learning" index=43>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-43 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 33<br>Keywords: Benchmarking, Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00514v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00514v1.pdf filename=2403.00514v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in off-policy <b>Reinforcement</b> <b>Learning</b> (RL) have significantly improved sample efficiency, primarily due to the incorporation of various forms of regularization that enable more gradient update steps than traditional agents. However, many of these techniques have been tested in limited settings, often on tasks from single <b>simulation</b> <b>benchmarks</b> and against well-known algorithms rather than a range of regularization approaches. This limits our understanding of the specific mechanisms driving RL improvements. To address this, we implemented over 60 different off-policy agents, each integrating established regularization techniques from recent state-of-the-art algorithms. We tested these agents across 14 diverse tasks from 2 <b>simulation</b> <b>benchmarks.</b> Our findings reveal that while the effectiveness of a specific regularization setup varies with the task, certain combinations consistently demonstrate robust and superior performance. Notably, a simple Soft Actor-Critic agent, appropriately regularized, reliably solves dog tasks, which were previously solved mainly through model-based approaches.</p></p class="citation"></blockquote><h3 id=540--44243-a-regularization-based-transfer-learning-method-for-information-extraction-via-instructed-graph-decoder-kedi-chen-et-al-2024>(5/40 | 44/243) A Regularization-based Transfer Learning Method for Information Extraction via Instructed Graph Decoder (Kedi Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kedi Chen, Jie Zhou, Qin Chen, Shunyu Liu, Liang He. (2024)<br><strong>A Regularization-based Transfer Learning Method for Information Extraction via Instructed Graph Decoder</strong><br><button class=copy-to-clipboard title="A Regularization-based Transfer Learning Method for Information Extraction via Instructed Graph Decoder" index=44>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-44 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 33<br>Keywords: Graph, Knowledge Transfer, Transfer Learning, Information Retrieval<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00891v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00891v1.pdf filename=2403.00891v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Information</b> <b>extraction</b> (IE) aims to extract complex structured <b>information</b> <b>from</b> the text. Numerous datasets have been constructed for various IE tasks, leading to time-consuming and labor-intensive data annotations. Nevertheless, most prevailing methods focus on training task-specific models, while the common <b>knowledge</b> <b>among</b> different IE tasks is not explicitly modeled. Moreover, the same phrase may have inconsistent labels in different tasks, which poses a big challenge for <b>knowledge</b> <b>transfer</b> <b>using</b> a unified model. In this study, we propose a regularization-based <b>transfer</b> <b>learning</b> method for IE (TIE) via an instructed <b>graph</b> decoder. Specifically, we first construct an instruction pool for datasets from all well-known IE tasks, and then present an instructed <b>graph</b> decoder, which decodes various complex structures into a <b>graph</b> uniformly based on corresponding instructions. In this way, the common <b>knowledge</b> <b>shared</b> with existing datasets can be learned and transferred to a new dataset with new labels. Furthermore, to alleviate the label inconsistency problem among various IE tasks, we introduce a task-specific regularization strategy, which does not update the gradients of two tasks with &lsquo;opposite direction&rsquo;. We conduct extensive experiments on 12 datasets spanning four IE tasks, and the results demonstrate the great advantages of our proposed method</p></p class="citation"></blockquote><h3 id=640--45243-equipment-health-assessment-time-series-analysis-for-wind-turbine-performance-jana-backhus-et-al-2024>(6/40 | 45/243) Equipment Health Assessment: Time Series Analysis for Wind Turbine Performance (Jana Backhus et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jana Backhus, Aniruddha Rajendra Rao, Chandrasekar Venkatraman, Abhishek Padmanabhan, A. Vinoth Kumar, Chetan Gupta. (2024)<br><strong>Equipment Health Assessment: Time Series Analysis for Wind Turbine Performance</strong><br><button class=copy-to-clipboard title="Equipment Health Assessment: Time Series Analysis for Wind Turbine Performance" index=45>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-45 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, math-FA, stat-AP<br>Keyword Score: 30<br>Keywords: LSTM, LSTM, LSTM<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00975v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00975v1.pdf filename=2403.00975v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this study, we leverage SCADA data from diverse wind turbines to predict power output, employing advanced time series methods, specifically Functional Neural Networks (FNN) and <b>Long</b> <b>Short-Term</b> <b>Memory</b> <b>(LSTM)</b> networks. A key innovation lies in the ensemble of FNN and <b>LSTM</b> models, capitalizing on their collective learning. This ensemble approach outperforms individual models, ensuring stable and accurate power output predictions. Additionally, machine learning techniques are applied to detect wind turbine performance deterioration, enabling proactive maintenance strategies and health assessment. Crucially, our analysis reveals the uniqueness of each wind turbine, necessitating tailored models for optimal predictions. These insight underscores the importance of providing automatized customization for different turbines to keep human modeling effort low. Importantly, the methodologies developed in this analysis are not limited to wind turbines; they can be extended to predict and optimize performance in various machinery, highlighting the versatility and applicability of our research across diverse industrial contexts.</p></p class="citation"></blockquote><h3 id=740--46243-atp-enabling-fast-llm-serving-via-attention-on-top-principal-keys-yue-niu-et-al-2024>(7/40 | 46/243) ATP: Enabling Fast LLM Serving via Attention on Top Principal Keys (Yue Niu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yue Niu, Saurav Prakash, Salman Avestimehr. (2024)<br><strong>ATP: Enabling Fast LLM Serving via Attention on Top Principal Keys</strong><br><button class=copy-to-clipboard title="ATP: Enabling Fast LLM Serving via Attention on Top Principal Keys" index=46>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-46 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: BERT, LLaMA, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.02352v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.02352v1.pdf filename=2403.02352v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose a new attention mechanism with linear complexity, ATP, that fixates \textbf{A}ttention on \textbf{T}op \textbf{P}rincipal keys, rather than on each individual token. Particularly, ATP is driven by an important observation that input sequences are typically low-rank, i.e., input sequences can be represented by a few principal bases. Therefore, instead of directly iterating over all the input tokens, ATP transforms inputs into an orthogonal space and computes attention only on the top principal bases (keys). Owing to the observed low-rank structure in input sequences, ATP is able to capture semantic relationships in input sequences with a few principal keys. Furthermore, the attention complexity is reduced from \emph{quadratic} to \emph{linear} without incurring a noticeable performance drop. ATP further reduces complexity for other linear layers with low-rank inputs, leading to more speedup compared to prior works that solely target the attention module. Our evaluations on various models (e.g., <b>BERT</b> and <b>Llama)</b> demonstrate that ATP achieves comparable accuracy with much lower computation and memory complexity than the standard attention mechanism. In particular, ATP barely loses accuracy with only $1/2$ principal keys, and only incurs around $2%$ accuracy drops with $1/4$ principal keys.</p></p class="citation"></blockquote><h3 id=840--47243-scale-free-adversarial-reinforcement-learning-mingyu-chen-et-al-2024>(8/40 | 47/243) Scale-free Adversarial Reinforcement Learning (Mingyu Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mingyu Chen, Xuezhou Zhang. (2024)<br><strong>Scale-free Adversarial Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Scale-free Adversarial Reinforcement Learning" index=47>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-47 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Bandit Algorithm, Markov Decision Process, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00930v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00930v1.pdf filename=2403.00930v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper initiates the study of scale-free learning in Markov Decision Processes <b>(MDPs),</b> where the scale of rewards/losses is unknown to the learner. We design a generic algorithmic framework, \underline{S}cale \underline{C}lipping \underline{B}ound (\texttt{SCB}), and instantiate this framework in both the adversarial Multi-armed <b>Bandit</b> (MAB) setting and the adversarial MDP setting. Through this framework, we achieve the first minimax optimal expected regret bound and the first high-probability regret bound in scale-free adversarial MABs, resolving an open problem raised in \cite{hadiji2023adaptation}. On adversarial <b>MDPs,</b> our framework also give birth to the first scale-free RL algorithm with a $\tilde{\mathcal{O}}(\sqrt{T})$ high-probability regret guarantee.</p></p class="citation"></blockquote><h3 id=940--48243-bias-mitigation-in-fine-tuning-pre-trained-models-for-enhanced-fairness-and-efficiency-yixuan-zhang-et-al-2024>(9/40 | 48/243) Bias Mitigation in Fine-tuning Pre-trained Models for Enhanced Fairness and Efficiency (Yixuan Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yixuan Zhang, Feng Zhou. (2024)<br><strong>Bias Mitigation in Fine-tuning Pre-trained Models for Enhanced Fairness and Efficiency</strong><br><button class=copy-to-clipboard title="Bias Mitigation in Fine-tuning Pre-trained Models for Enhanced Fairness and Efficiency" index=48>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-48 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CY, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Fairness, Fine-tuning, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00625v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00625v1.pdf filename=2403.00625v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Fine-tuning</b> pre-trained models is a widely employed technique in numerous real-world applications. However, <b>fine-tuning</b> these models on new tasks can lead to unfair outcomes. This is due to the absence of generalization guarantees for <b>fairness</b> properties, regardless of whether the original pre-trained model was developed with <b>fairness</b> considerations. To tackle this issue, we introduce an efficient and robust <b>fine-tuning</b> framework specifically designed to mitigate biases in new tasks. Our empirical analysis shows that the parameters in the pre-trained model that affect predictions for different demographic groups are different, so based on this observation, we employ a <b>transfer</b> <b>learning</b> strategy that neutralizes the importance of these influential weights, determined using Fisher information across demographic groups. Additionally, we integrate this weight importance neutralization strategy with a matrix factorization technique, which provides a low-rank approximation of the weight matrix using fewer parameters, reducing the computational demands. Experiments on multiple pre-trained models and new tasks demonstrate the effectiveness of our method.</p></p class="citation"></blockquote><h3 id=1040--49243-robust-deep-reinforcement-learning-through-adversarial-attacks-and-training--a-survey-lucas-schott-et-al-2024>(10/40 | 49/243) Robust Deep Reinforcement Learning Through Adversarial Attacks and Training : A Survey (Lucas Schott et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lucas Schott, Josephine Delas, Hatem Hajri, Elies Gherbi, Reda Yaich, Nora Boulahia-Cuppens, Frederic Cuppens, Sylvain Lamprier. (2024)<br><strong>Robust Deep Reinforcement Learning Through Adversarial Attacks and Training : A Survey</strong><br><button class=copy-to-clipboard title="Robust Deep Reinforcement Learning Through Adversarial Attacks and Training : A Survey" index=49>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-49 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Adversarial Learning, Reinforcement Learning, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00420v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00420v1.pdf filename=2403.00420v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep <b>Reinforcement</b> <b>Learning</b> (DRL) is an approach for training autonomous agents across various complex environments. Despite its significant performance in well known environments, it remains susceptible to minor conditions variations, raising concerns about its reliability in real-world applications. To improve usability, DRL must demonstrate trustworthiness and robustness. A way to improve robustness of DRL to unknown changes in the conditions is through <b>Adversarial</b> <b>Training,</b> by training the agent against well suited <b>adversarial</b> <b>attacks</b> on the dynamics of the environment. Addressing this critical issue, our work presents an in-depth analysis of contemporary <b>adversarial</b> <b>attack</b> methodologies, systematically categorizing them and comparing their objectives and operational mechanisms. This classification offers a detailed insight into how <b>adversarial</b> <b>attacks</b> effectively act for evaluating the resilience of DRL agents, thereby paving the way for enhancing their robustness.</p></p class="citation"></blockquote><h3 id=1140--50243-robust-policy-learning-via-offline-skill-diffusion-woo-kyung-kim-et-al-2024>(11/40 | 50/243) Robust Policy Learning via Offline Skill Diffusion (Woo Kyung Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Woo Kyung Kim, Minjong Yoo, Honguk Woo. (2024)<br><strong>Robust Policy Learning via Offline Skill Diffusion</strong><br><button class=copy-to-clipboard title="Robust Policy Learning via Offline Skill Diffusion" index=50>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-50 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Diffusion Model, Few-shot, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00225v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00225v2.pdf filename=2403.00225v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Skill-based <b>reinforcement</b> <b>learning</b> (RL) approaches have shown considerable promise, especially in solving long-horizon tasks via hierarchical structures. These skills, learned task-agnostically from offline datasets, can accelerate the policy learning process for new tasks. Yet, the application of these skills in different domains remains restricted due to their inherent dependency on the datasets, which poses a challenge when attempting to learn a skill-based policy via RL for a target domain different from the datasets&rsquo; domains. In this paper, we present a novel offline skill learning framework DuSkill which employs a guided <b>Diffusion</b> <b>model</b> to generate versatile skills extended from the limited skills in datasets, thereby enhancing the robustness of policy learning for tasks in different domains. Specifically, we devise a guided <b>diffusion-based</b> <b>skill</b> decoder in conjunction with the hierarchical encoding to disentangle the skill embedding space into two distinct representations, one for encapsulating domain-invariant behaviors and the other for delineating the factors that induce domain variations in the behaviors. Our DuSkill framework enhances the diversity of skills learned offline, thus enabling to accelerate the learning procedure of high-level policies for different domains. Through experiments, we show that DuSkill outperforms other skill-based imitation learning and RL algorithms for several long-horizon tasks, demonstrating its benefits in <b>few-shot</b> imitation and online RL.</p></p class="citation"></blockquote><h3 id=1240--51243-efficient-reinforcement-learning-for-global-decision-making-in-the-presence-of-local-agents-at-scale-emile-anand-et-al-2024>(12/40 | 51/243) Efficient Reinforcement Learning for Global Decision Making in the Presence of Local Agents at Scale (Emile Anand et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Emile Anand, Guannan Qu. (2024)<br><strong>Efficient Reinforcement Learning for Global Decision Making in the Presence of Local Agents at Scale</strong><br><button class=copy-to-clipboard title="Efficient Reinforcement Learning for Global Decision Making in the Presence of Local Agents at Scale" index=51>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-51 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: I-2-6, cs-LG, cs-MA, cs.LG<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00222v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00222v1.pdf filename=2403.00222v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study <b>reinforcement</b> <b>learning</b> for global decision-making in the presence of many local agents, where the global decision-maker makes decisions affecting all local agents, and the objective is to learn a policy that maximizes the rewards of both the global and the local agents. Such problems find many applications, e.g. demand response, EV charging, queueing, etc. In this setting, scalability has been a long-standing challenge due to the size of the state/action space which can be exponential in the number of agents. This work proposes the SUB-SAMPLE-Q algorithm where the global agent subsamples $k\leq n$ local agents to compute an optimal policy in time that is only exponential in $k$, providing an exponential speedup from standard methods that are exponential in $n$. We show that the learned policy converges to the optimal policy in the order of $\tilde{O}(1/\sqrt{k}+\epsilon_{k,m})$ as the number of sub-sampled agents $k$ increases, where $\epsilon_{k,m}$ is the Bellman noise. We also conduct numerical <b>simulations</b> in a demand-response setting and a queueing setting.</p></p class="citation"></blockquote><h3 id=1340--52243-beyond-single-model-views-for-deep-learning-optimization-versus-generalizability-of-stochastic-optimization-algorithms-toki-tahmid-inan-et-al-2024>(13/40 | 52/243) Beyond Single-Model Views for Deep Learning: Optimization versus Generalizability of Stochastic Optimization Algorithms (Toki Tahmid Inan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Toki Tahmid Inan, Mingrui Liu, Amarda Shehu. (2024)<br><strong>Beyond Single-Model Views for Deep Learning: Optimization versus Generalizability of Stochastic Optimization Algorithms</strong><br><button class=copy-to-clipboard title="Beyond Single-Model Views for Deep Learning: Optimization versus Generalizability of Stochastic Optimization Algorithms" index=52>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-52 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 26<br>Keywords: Benchmarking, Benchmarking, Stochastic Gradient Descent, Stochastic Gradient Descent<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00574v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00574v1.pdf filename=2403.00574v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite an extensive body of literature on deep learning optimization, our current understanding of what makes an optimization algorithm effective is fragmented. In particular, we do not understand well whether enhanced optimization translates to improved generalizability. Current research overlooks the inherent <b>stochastic</b> <b>nature</b> <b>of</b> <b>stochastic</b> <b>gradient</b> <b>descent</b> <b>(SGD)</b> and its variants, resulting in a lack of comprehensive <b>benchmarking</b> and insight into their statistical performance. This paper aims to address this gap by adopting a novel approach. Rather than solely evaluating the endpoint of individual optimization trajectories, we draw from an ensemble of trajectories to estimate the stationary distribution of <b>stochastic</b> <b>optimizers.</b> <b>Our</b> investigation encompasses a wide array of techniques, including <b>SGD</b> and its variants, flat-minima optimizers, and new algorithms we propose under the Basin Hopping framework. Through our evaluation, which encompasses synthetic functions with known minima and real-world problems in computer vision and natural language processing, we emphasize fair <b>benchmarking</b> under a statistical framework, comparing stationary distributions and establishing statistical significance. Our study uncovers several key findings regarding the relationship between training loss and hold-out accuracy, as well as the comparable performance of <b>SGD,</b> noise-enabled variants, and novel optimizers utilizing the BH framework. Notably, these algorithms demonstrate performance on par with flat-minima optimizers like SAM, albeit with half the gradient evaluations. We anticipate that our work will catalyze further exploration in deep learning optimization, encouraging a shift away from single-model approaches towards methodologies that acknowledge and leverage the <b>stochastic</b> <b>nature</b> <b>of</b> optimizers.</p></p class="citation"></blockquote><h3 id=1440--53243-graph-construction-with-flexible-nodes-for-traffic-demand-prediction-jinyan-hou-et-al-2024>(14/40 | 53/243) Graph Construction with Flexible Nodes for Traffic Demand Prediction (Jinyan Hou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinyan Hou, Shan Liu, Ya Zhang, Haotong Qin. (2024)<br><strong>Graph Construction with Flexible Nodes for Traffic Demand Prediction</strong><br><button class=copy-to-clipboard title="Graph Construction with Flexible Nodes for Traffic Demand Prediction" index=53>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-53 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 26<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00276v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00276v1.pdf filename=2403.00276v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>neural</b> <b>networks</b> <b>(GNNs)</b> have been widely applied in traffic demand prediction, and transportation modes can be divided into station-based mode and free-floating traffic mode. Existing research in traffic <b>graph</b> <b>construction</b> <b>primarily</b> relies on map matching to construct <b>graphs</b> <b>based</b> <b>on</b> the road network. However, the complexity and inhomogeneity of data distribution in free-floating traffic demand forecasting make road network matching inflexible. To tackle these challenges, this paper introduces a novel <b>graph</b> <b>construction</b> <b>method</b> tailored to free-floating traffic mode. We propose a novel density-based <b>clustering</b> algorithm (HDPC-L) to determine the flexible positioning of nodes in the <b>graph,</b> <b>overcoming</b> <b>the</b> computational bottlenecks of traditional <b>clustering</b> algorithms and enabling effective handling of large-scale datasets. Furthermore, we extract valuable information from ridership data to initialize the edge weights of <b>GNNs.</b> Comprehensive experiments on two real-world datasets, the Shenzhen bike-sharing dataset and the Haikou ride-hailing dataset, show that the method significantly improves the performance of the model. On average, our models show an improvement in accuracy of around 25% and 19.5% on the two datasets. Additionally, it significantly enhances computational efficiency, reducing training time by approximately 12% and 32.5% on the two datasets. We make our code available at <a href=https://github.com/houjinyan/HDPC-L-ODInit>https://github.com/houjinyan/HDPC-L-ODInit</a>.</p></p class="citation"></blockquote><h3 id=1540--54243-on-the-role-of-information-structure-in-reinforcement-learning-for-partially-observable-sequential-teams-and-games-awni-altabaa-et-al-2024>(15/40 | 54/243) On the Role of Information Structure in Reinforcement Learning for Partially-Observable Sequential Teams and Games (Awni Altabaa et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Awni Altabaa, Zhuoran Yang. (2024)<br><strong>On the Role of Information Structure in Reinforcement Learning for Partially-Observable Sequential Teams and Games</strong><br><button class=copy-to-clipboard title="On the Role of Information Structure in Reinforcement Learning for Partially-Observable Sequential Teams and Games" index=54>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-54 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, stat-ML<br>Keyword Score: 23<br>Keywords: Graph, Markov Decision Process, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00993v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00993v1.pdf filename=2403.00993v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In a sequential decision-making problem, the information structure is the description of how events in the system occurring at different points in time affect each other. Classical models of <b>reinforcement</b> <b>learning</b> (e.g., <b>MDPs,</b> POMDPs, Dec-POMDPs, and POMGs) assume a very simple and highly regular information structure, while more general models like predictive state representations do not explicitly model the information structure. By contrast, real-world sequential decision-making problems typically involve a complex and time-varying interdependence of system variables, requiring a rich and flexible representation of information structure. In this paper, we argue for the perspective that explicit representation of information structures is an important component of analyzing and solving <b>reinforcement</b> <b>learning</b> problems. We propose novel <b>reinforcement</b> <b>learning</b> models with an explicit representation of information structure, capturing classical models as special cases. We show that this leads to a richer analysis of sequential decision-making problems and enables more tailored algorithm design. In particular, we characterize the &ldquo;complexity&rdquo; of the observable dynamics of any sequential decision-making problem through a <b>graph-theoretic</b> analysis of the DAG representation of its information structure. The central quantity in this analysis is the minimal set of variables that $d$-separates the past observations from future observations. Furthermore, through constructing a generalization of predictive state representations, we propose tailored <b>reinforcement</b> <b>learning</b> algorithms and prove that the sample complexity is in part determined by the information structure. This recovers known tractability results and gives a novel perspective on <b>reinforcement</b> <b>learning</b> in general sequential decision-making problems, providing a systematic way of identifying new tractable classes of problems.</p></p class="citation"></blockquote><h3 id=1640--55243-subhomogeneous-deep-equilibrium-models-pietro-sittoni-et-al-2024>(16/40 | 55/243) Subhomogeneous Deep Equilibrium Models (Pietro Sittoni et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pietro Sittoni, Francesco Tudisco. (2024)<br><strong>Subhomogeneous Deep Equilibrium Models</strong><br><button class=copy-to-clipboard title="Subhomogeneous Deep Equilibrium Models" index=55>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-55 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-NA, cs.LG, math-NA, math-OC<br>Keyword Score: 23<br>Keywords: Graph, Graph Neural Network, Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00720v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00720v1.pdf filename=2403.00720v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Implicit-depth neural networks have grown as powerful alternatives to traditional networks in various applications in recent years. However, these models often lack guarantees of existence and uniqueness, raising stability, performance, and reproducibility issues. In this paper, we present a new analysis of the existence and uniqueness of fixed points for implicit-depth neural networks based on the concept of subhomogeneous operators and the nonlinear Perron-Frobenius theory. Compared to previous similar analyses, our theory allows for weaker assumptions on the parameter matrices, thus yielding a more flexible framework for well-defined implicit networks. We illustrate the performance of the resulting subhomogeneous networks on feed-forward, <b>convolutional,</b> and <b>graph</b> <b>neural</b> <b>network</b> examples.</p></p class="citation"></blockquote><h3 id=1740--56243-nonlinear-sheaf-diffusion-in-graph-neural-networks-olga-zaghen-2024>(17/40 | 56/243) Nonlinear Sheaf Diffusion in Graph Neural Networks (Olga Zaghen, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Olga Zaghen. (2024)<br><strong>Nonlinear Sheaf Diffusion in Graph Neural Networks</strong><br><button class=copy-to-clipboard title="Nonlinear Sheaf Diffusion in Graph Neural Networks" index=56>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-56 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Graph, Graph Neural Network, Discrete Time, Discrete Time<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00337v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00337v1.pdf filename=2403.00337v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work focuses on exploring the potential benefits of introducing a nonlinear Laplacian in Sheaf Neural Networks for <b>graph-related</b> <b>tasks.</b> <b>The</b> primary aim is to understand the impact of such nonlinearity on diffusion dynamics, signal propagation, and performance of neural network architectures in <b>discrete-time</b> <b>settings.</b> The study primarily emphasizes experimental analysis, using real-world and synthetic datasets to validate the practical effectiveness of different versions of the model. This approach shifts the focus from an initial theoretical exploration to demonstrating the practical utility of the proposed model.</p></p class="citation"></blockquote><h3 id=1840--57243-distributional-dataset-distillation-with-subtask-decomposition-tian-qin-et-al-2024>(18/40 | 57/243) Distributional Dataset Distillation with Subtask Decomposition (Tian Qin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tian Qin, Zhiwei Deng, David Alvarez-Melis. (2024)<br><strong>Distributional Dataset Distillation with Subtask Decomposition</strong><br><button class=copy-to-clipboard title="Distributional Dataset Distillation with Subtask Decomposition" index=57>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-57 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Knowledge Distillation, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00999v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00999v1.pdf filename=2403.00999v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>What does a neural network learn when training from a task-specific dataset? Synthesizing this knowledge is the central idea behind Dataset <b>Distillation,</b> which recent work has shown can be used to compress large datasets into a small set of input-label pairs ($\textit{prototypes}$) that capture essential aspects of the original dataset. In this paper, we make the key observation that existing methods <b>distilling</b> into explicit prototypes are very often suboptimal, incurring in unexpected storage cost from <b>distilled</b> labels. In response, we propose $\textit{Distributional Dataset Distillation}$ (D3), which encodes the data using minimal sufficient per-class statistics and paired with a decoder, we <b>distill</b> dataset into a compact distributional representation that is more memory-efficient compared to prototype-based methods. To scale up the process of learning these representations, we propose $\textit{Federated distillation}$, which decomposes the dataset into subsets, <b>distills</b> them in parallel using sub-task experts and then re-aggregates them. We thoroughly evaluate our algorithm on a three-dimensional metric and show that our method achieves state-of-the-art results on TinyImageNet and ImageNet-1K. Specifically, we outperform the prior art by $6.9%$ on ImageNet-1K under the storage budget of 2 images per class.</p></p class="citation"></blockquote><h3 id=1940--58243-fine-tuning-with-very-large-dropout-jianyu-zhang-et-al-2024>(19/40 | 58/243) Fine-tuning with Very Large Dropout (Jianyu Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianyu Zhang, Léon Bottou. (2024)<br><strong>Fine-tuning with Very Large Dropout</strong><br><button class=copy-to-clipboard title="Fine-tuning with Very Large Dropout" index=58>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-58 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Fine-tuning, Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00946v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00946v1.pdf filename=2403.00946v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>It is impossible today to pretend that the practice of machine learning is compatible with the idea that training and testing data follow the same distribution. Several authors have recently used ensemble techniques to show how scenarios involving multiple data distributions are best served by representations that are both richer than those obtained by regularizing for the best in-distribution performance, and richer than those obtained under the influence of the implicit sparsity bias of common stochastic gradient procedures. This contribution investigates the use of very high dropout rates instead of ensembles to obtain such rich representations. Although training a deep network from scratch using such dropout rates is virtually impossible, <b>fine-tuning</b> a large pre-trained model under such conditions is not only possible but also achieves <b>out-of-distribution</b> performances that exceed those of both ensembles and weight averaging methods such as model soups. This result has practical significance because the importance of the <b>fine-tuning</b> scenario has considerably grown in recent years. This result also provides interesting insights on the nature of rich representations and on the intrinsically linear nature of <b>fine-tuning</b> a large network using a comparatively small dataset.</p></p class="citation"></blockquote><h3 id=2040--59243-resilience-of-entropy-model-in-distributed-neural-networks-milin-zhang-et-al-2024>(20/40 | 59/243) Resilience of Entropy Model in Distributed Neural Networks (Milin Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Milin Zhang, Mohammad Abdi, Shahriar Rifat, Francesco Restuccia. (2024)<br><strong>Resilience of Entropy Model in Distributed Neural Networks</strong><br><button class=copy-to-clipboard title="Resilience of Entropy Model in Distributed Neural Networks" index=59>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-59 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CR, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Adversarial Learning, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00942v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00942v1.pdf filename=2403.00942v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Distributed deep neural networks (DNNs) have emerged as a key technique to reduce communication overhead without sacrificing performance in edge computing systems. Recently, entropy coding has been introduced to further reduce the communication overhead. The key idea is to train the distributed DNN jointly with an entropy model, which is used as side information during inference time to adaptively encode latent representations into bit streams with variable length. To the best of our knowledge, the resilience of entropy models is yet to be investigated. As such, in this paper we formulate and investigate the resilience of entropy models to intentional interference (e.g., <b>adversarial</b> <b>attacks)</b> and unintentional interference (e.g., weather changes and motion blur). Through an extensive experimental campaign with 3 different DNN architectures, 2 entropy models and 4 rate-distortion trade-off factors, we demonstrate that the entropy attacks can increase the communication overhead by up to 95%. By separating compression features in frequency and spatial domain, we propose a new defense mechanism that can reduce the transmission overhead of the attacked input by about 9% compared to unperturbed data, with only about 2% accuracy loss. Importantly, the proposed defense mechanism is a standalone approach which can be applied in conjunction with approaches such as <b>adversarial</b> <b>training</b> to further improve robustness. Code will be shared for reproducibility.</p></p class="citation"></blockquote><h3 id=2140--60243-atp-an-efficient-and-scalable-method-for-localizing-llm-behaviour-to-components-jános-kramár-et-al-2024>(21/40 | 60/243) AtP*: An efficient and scalable method for localizing LLM behaviour to components (János Kramár et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>János Kramár, Tom Lieberum, Rohin Shah, Neel Nanda. (2024)<br><em><em>AtP</em>: An efficient and scalable method for localizing LLM behaviour to components</em>*<br><button class=copy-to-clipboard title="AtP*: An efficient and scalable method for localizing LLM behaviour to components" index=60>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-60 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00745v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00745v1.pdf filename=2403.00745v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Activation Patching is a method of directly computing causal attributions of behavior to model components. However, applying it exhaustively requires a sweep with cost scaling linearly in the number of model components, which can be prohibitively expensive for SoTA <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> We investigate Attribution Patching (AtP), a fast gradient-based approximation to Activation Patching and find two classes of failure modes of AtP which lead to significant false negatives. We propose a variant of AtP called AtP*, with two changes to address these failure modes while retaining scalability. We present the first systematic study of AtP and alternative methods for faster activation patching and show that AtP significantly outperforms all other investigated methods, with AtP* providing further significant improvement. Finally, we provide a method to bound the probability of remaining false negatives of AtP* estimates.</p></p class="citation"></blockquote><h3 id=2240--61243-rethinking-the-uniformity-metric-in-self-supervised-learning-xianghong-fang-et-al-2024>(22/40 | 61/243) Rethinking The Uniformity Metric in Self-Supervised Learning (Xianghong Fang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xianghong Fang, Jian Li, Qiang Sun, Benyou Wang. (2024)<br><strong>Rethinking The Uniformity Metric in Self-Supervised Learning</strong><br><button class=copy-to-clipboard title="Rethinking The Uniformity Metric in Self-Supervised Learning" index=61>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-61 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00642v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00642v1.pdf filename=2403.00642v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Uniformity plays a crucial role in the assessment of learned representations, contributing to a deeper comprehension of <b>self-supervised</b> <b>learning.</b> The seminal work by \citet{Wang2020UnderstandingCR} introduced a uniformity metric that quantitatively measures the collapse degree of learned representations. Directly optimizing this metric together with alignment proves to be effective in preventing constant collapse. However, we present both theoretical and empirical evidence revealing that this metric lacks sensitivity to dimensional collapse, highlighting its limitations. To address this limitation and design a more effective uniformity metric, this paper identifies five fundamental properties, some of which the existing uniformity metric fails to meet. We subsequently introduce a novel uniformity metric that satisfies all of these desiderata and exhibits sensitivity to dimensional collapse. When applied as an auxiliary loss in various established <b>self-supervised</b> <b>methods,</b> our proposed uniformity metric consistently enhances their performance in downstream tasks.Our code was released at <a href=https://github.com/sunset-clouds/WassersteinUniformityMetric>https://github.com/sunset-clouds/WassersteinUniformityMetric</a>.</p></p class="citation"></blockquote><h3 id=2340--62243-epsilon-greedy-thompson-sampling-to-bayesian-optimization-bach-do-et-al-2024>(23/40 | 62/243) Epsilon-Greedy Thompson Sampling to Bayesian Optimization (Bach Do et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bach Do, Ruda Zhang. (2024)<br><strong>Epsilon-Greedy Thompson Sampling to Bayesian Optimization</strong><br><button class=copy-to-clipboard title="Epsilon-Greedy Thompson Sampling to Bayesian Optimization" index=62>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-62 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-OC, stat-ML<br>Keyword Score: 20<br>Keywords: Gaussian Process, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00540v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00540v1.pdf filename=2403.00540v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Thompson sampling (TS) serves as a solution for addressing the exploitation-exploration dilemma in Bayesian optimization (BO). While it prioritizes exploration by randomly generating and maximizing sample paths of <b>Gaussian</b> <b>process</b> (GP) posteriors, TS weakly manages its exploitation by gathering information about the true objective function after each exploration is performed. In this study, we incorporate the epsilon-greedy ($\varepsilon$-greedy) policy, a well-established selection strategy in <b>reinforcement</b> <b>learning,</b> into TS to improve its exploitation. We first delineate two extremes of TS applied for BO, namely the generic TS and a sample-average TS. The former and latter promote exploration and exploitation, respectively. We then use $\varepsilon$-greedy policy to randomly switch between the two extremes. A small value of $\varepsilon \in (0,1)$ prioritizes exploitation, and vice versa. We empirically show that $\varepsilon$-greedy TS with an appropriate $\varepsilon$ is better than one of its two extremes and competes with the other.</p></p class="citation"></blockquote><h3 id=2440--63243-fractal-interpolation-in-the-context-of-prediction-accuracy-optimization-alexandra-baicoianu-et-al-2024>(24/40 | 63/243) Fractal interpolation in the context of prediction accuracy optimization (Alexandra Baicoianu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alexandra Baicoianu, Cristina Gabriela Gavrilă, Cristina Maria Pacurar, Victor Dan Pacurar. (2024)<br><strong>Fractal interpolation in the context of prediction accuracy optimization</strong><br><button class=copy-to-clipboard title="Fractal interpolation in the context of prediction accuracy optimization" index=63>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-63 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Data Augmentation, LSTM<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00403v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00403v1.pdf filename=2403.00403v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper focuses on the hypothesis of optimizing time series predictions using fractal interpolation techniques. In general, the accuracy of machine learning model predictions is closely related to the quality and quantitative aspects of the <b>data</b> <b>used,</b> following the principle of \textit{garbage-in, garbage-out}. In order to quantitatively and qualitatively augment datasets, one of the most prevalent concerns of <b>data</b> <b>scientists</b> is to generate synthetic <b>data,</b> <b>which</b> should follow as closely as possible the actual pattern of the original <b>data.</b> <b>This</b> study proposes three different <b>data</b> <b>augmentation</b> strategies based on fractal interpolation, namely the \textit{Closest Hurst Strategy}, \textit{Closest Values Strategy} and \textit{Formula Strategy}. To validate the strategies, we used four public datasets from the literature, as well as a private dataset obtained from meteorological records in the city of Brasov, Romania. The prediction results obtained with the <b>LSTM</b> model using the presented interpolation strategies showed a significant accuracy improvement compared to the raw datasets, thus providing a possible answer to practical problems in the field of remote sensing and sensor sensitivity. Moreover, our methodologies answer some optimization-related open questions for the fractal interpolation step using \textit{Optuna} framework.</p></p class="citation"></blockquote><h3 id=2540--64243-fedrdma-communication-efficient-cross-silo-federated-llm-via-chunked-rdma-transmission-zeling-zhang-et-al-2024>(25/40 | 64/243) FedRDMA: Communication-Efficient Cross-Silo Federated LLM via Chunked RDMA Transmission (Zeling Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zeling Zhang, Dongqi Cai, Yiran Zhang, Mengwei Xu, Shangguang Wang, Ao Zhou. (2024)<br><strong>FedRDMA: Communication-Efficient Cross-Silo Federated LLM via Chunked RDMA Transmission</strong><br><button class=copy-to-clipboard title="FedRDMA: Communication-Efficient Cross-Silo Federated LLM via Chunked RDMA Transmission" index=64>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-64 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-DC, cs-LG, cs-NI, cs.LG<br>Keyword Score: 20<br>Keywords: Federated Learning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00881v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00881v1.pdf filename=2403.00881v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Communication overhead is a significant bottleneck in <b>federated</b> <b>learning</b> (FL), which has been exaggerated with the increasing size of AI models. In this paper, we propose FedRDMA, a communication-efficient cross-silo FL system that integrates RDMA into the FL communication protocol. To overcome the limitations of RDMA in wide-area networks (WANs), FedRDMA divides the updated model into chunks and designs a series of optimization techniques to improve the efficiency and robustness of RDMA-based communication. We implement FedRDMA atop the industrial <b>federated</b> <b>learning</b> framework and evaluate it on a real-world cross-silo FL scenario. The experimental results show that \sys can achieve up to 3.8$\times$ speedup in communication efficiency compared to traditional TCP/IP-based FL systems.</p></p class="citation"></blockquote><h3 id=2640--65243-advancing-additive-manufacturing-through-deep-learning-a-comprehensive-review-of-current-progress-and-future-challenges-amirul-islam-saimon-et-al-2024>(26/40 | 65/243) Advancing Additive Manufacturing through Deep Learning: A Comprehensive Review of Current Progress and Future Challenges (Amirul Islam Saimon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amirul Islam Saimon, Emmanuel Yangue, Xiaowei Yue, Zhenyu, Kong, Chenang Liu. (2024)<br><strong>Advancing Additive Manufacturing through Deep Learning: A Comprehensive Review of Current Progress and Future Challenges</strong><br><button class=copy-to-clipboard title="Advancing Additive Manufacturing through Deep Learning: A Comprehensive Review of Current Progress and Future Challenges" index=65>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-65 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 15<br>Keywords: Geometry, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00669v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00669v1.pdf filename=2403.00669v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Additive manufacturing (AM) has already proved itself to be the potential alternative to widely-used subtractive manufacturing due to its extraordinary capacity of manufacturing highly customized products with minimum material wastage. Nevertheless, it is still not being considered as the primary choice for the industry due to some of its major inherent challenges, including complex and dynamic process interactions, which are sometimes difficult to fully understand even with traditional machine learning because of the involvement of high-dimensional data such as images, point clouds, and voxels. However, the recent emergence of deep learning (DL) is showing great promise in overcoming many of these challenges as DL can automatically capture complex relationships from high-dimensional data without hand-crafted feature extraction. Therefore, the volume of research in the intersection of AM and DL is exponentially growing each year which makes it difficult for the researchers to keep track of the trend and future potential directions. Furthermore, to the best of our knowledge, there is no comprehensive review paper in this research track summarizing the recent studies. Therefore, this paper reviews the recent studies that apply DL for making the AM process better with a high-level summary of their contributions and limitations. Finally, it <b>summarizes</b> the current challenges and recommends some of the promising opportunities in this domain for further investigation with a special focus on generalizing DL models for wide-range of <b>geometry</b> types, managing uncertainties both in AM data and DL models, overcoming limited and noisy AM data issues by incorporating generative models, and unveiling the potential of interpretable DL for AM.</p></p class="citation"></blockquote><h3 id=2740--66243-adaptive-learning-rate-for-follow-the-regularized-leader-competitive-ratio-analysis-and-best-of-both-worlds-shinji-ito-et-al-2024>(27/40 | 66/243) Adaptive Learning Rate for Follow-the-Regularized-Leader: Competitive Ratio Analysis and Best-of-Both-Worlds (Shinji Ito et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shinji Ito, Taira Tsuchiya, Junya Honda. (2024)<br><strong>Adaptive Learning Rate for Follow-the-Regularized-Leader: Competitive Ratio Analysis and Best-of-Both-Worlds</strong><br><button class=copy-to-clipboard title="Adaptive Learning Rate for Follow-the-Regularized-Leader: Competitive Ratio Analysis and Best-of-Both-Worlds" index=66>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-66 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 13<br>Keywords: Graph, Bandit Algorithm<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00715v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00715v1.pdf filename=2403.00715v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Follow-The-Regularized-Leader (FTRL) is known as an effective and versatile approach in online learning, where appropriate choice of the learning rate is crucial for smaller regret. To this end, we formulate the problem of adjusting FTRL&rsquo;s learning rate as a sequential decision-making problem and introduce the framework of competitive analysis. We establish a lower bound for the competitive ratio and propose update rules for learning rate that achieves an upper bound within a constant factor of this lower bound. Specifically, we illustrate that the optimal competitive ratio is characterized by the (approximate) monotonicity of components of the penalty term, showing that a constant competitive ratio is achievable if the components of the penalty term form a monotonically non-increasing sequence, and derive a tight competitive ratio when penalty terms are $\xi$-approximately monotone non-increasing. Our proposed update rule, referred to as \textit{stability-penalty matching}, also facilitates constructing the Best-Of-Both-Worlds (BOBW) algorithms for stochastic and adversarial environments. In these environments our result contributes to achieve tighter regret bound and broaden the applicability of algorithms for various settings such as multi-armed <b>bandits,</b> <b>graph</b> <b>bandits,</b> linear <b>bandits,</b> and contextual <b>bandits.</b></p></p class="citation"></blockquote><h3 id=2840--67243-reusing-historical-trajectories-in-natural-policy-gradient-via-importance-sampling-convergence-and-convergence-rate-yifan-lin-et-al-2024>(28/40 | 67/243) Reusing Historical Trajectories in Natural Policy Gradient via Importance Sampling: Convergence and Convergence Rate (Yifan Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yifan Lin, Yuhao Wang, Enlu Zhou. (2024)<br><strong>Reusing Historical Trajectories in Natural Policy Gradient via Importance Sampling: Convergence and Convergence Rate</strong><br><button class=copy-to-clipboard title="Reusing Historical Trajectories in Natural Policy Gradient via Importance Sampling: Convergence and Convergence Rate" index=67>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-67 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-OC<br>Keyword Score: 13<br>Keywords: Benchmarking, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00675v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00675v1.pdf filename=2403.00675v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Reinforcement</b> <b>learning</b> provides a mathematical framework for learning-based control, whose success largely depends on the amount of data it can utilize. The efficient utilization of historical trajectories obtained from previous policies is essential for expediting policy optimization. Empirical evidence has shown that policy gradient methods based on importance sampling work well. However, existing literature often neglect the interdependence between trajectories from different iterations, and the good empirical performance lacks a rigorous theoretical justification. In this paper, we study a variant of the natural policy gradient method with reusing historical trajectories via importance sampling. We show that the bias of the proposed estimator of the gradient is asymptotically negligible, the resultant algorithm is convergent, and reusing past trajectories helps improve the convergence rate. We further apply the proposed estimator to popular policy optimization algorithms such as trust region policy optimization. Our theoretical results are verified on classical <b>benchmarks.</b></p></p class="citation"></blockquote><h3 id=2940--68243-snapshot-reinforcement-learning-leveraging-prior-trajectories-for-efficiency-yanxiao-zhao-et-al-2024>(29/40 | 68/243) Snapshot Reinforcement Learning: Leveraging Prior Trajectories for Efficiency (Yanxiao Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yanxiao Zhao, Yangge Qian, Tianyi Wang, Jingyang Shan, Xiaolin Qin. (2024)<br><strong>Snapshot Reinforcement Learning: Leveraging Prior Trajectories for Efficiency</strong><br><button class=copy-to-clipboard title="Snapshot Reinforcement Learning: Leveraging Prior Trajectories for Efficiency" index=68>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-68 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 13<br>Keywords: Benchmarking, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00673v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00673v1.pdf filename=2403.00673v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep <b>reinforcement</b> <b>learning</b> (DRL) algorithms require substantial samples and computational resources to achieve higher performance, which restricts their practical application and poses challenges for further development. Given the constraint of limited resources, it is essential to leverage existing computational work (e.g., learned policies, samples) to enhance sample efficiency and reduce the computational resource consumption of DRL algorithms. Previous works to leverage existing computational work require intrusive modifications to existing algorithms and models, designed specifically for specific algorithms, lacking flexibility and universality. In this paper, we present the Snapshot <b>Reinforcement</b> <b>Learning</b> (SnapshotRL) framework, which enhances sample efficiency by simply altering environments, without making any modifications to algorithms and models. By allowing student agents to choose states in teacher trajectories as the initial state to sample, SnapshotRL can effectively utilize teacher trajectories to assist student agents in training, allowing student agents to explore a larger state space at the early training phase. We propose a simple and effective SnapshotRL baseline algorithm, S3RL, which integrates well with existing DRL algorithms. Our experiments demonstrate that integrating S3RL with TD3, SAC, and PPO algorithms on the MuJoCo <b>benchmark</b> significantly improves sample efficiency and average return, without extra samples and additional computational resources.</p></p class="citation"></blockquote><h3 id=3040--69243-efficientzero-v2-mastering-discrete-and-continuous-control-with-limited-data-shengjie-wang-et-al-2024>(30/40 | 69/243) EfficientZero V2: Mastering Discrete and Continuous Control with Limited Data (Shengjie Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shengjie Wang, Shaohuai Liu, Weirui Ye, Jiacheng You, Yang Gao. (2024)<br><strong>EfficientZero V2: Mastering Discrete and Continuous Control with Limited Data</strong><br><button class=copy-to-clipboard title="EfficientZero V2: Mastering Discrete and Continuous Control with Limited Data" index=69>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-69 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-RO, cs.LG<br>Keyword Score: 13<br>Keywords: Benchmarking, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00564v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00564v1.pdf filename=2403.00564v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Sample efficiency remains a crucial challenge in applying <b>Reinforcement</b> <b>Learning</b> (RL) to real-world tasks. While recent algorithms have made significant strides in improving sample efficiency, none have achieved consistently superior performance across diverse domains. In this paper, we introduce EfficientZero V2, a general framework designed for sample-efficient RL algorithms. We have expanded the performance of EfficientZero to multiple domains, encompassing both continuous and discrete actions, as well as visual and low-dimensional inputs. With a series of improvements we propose, EfficientZero V2 outperforms the current state-of-the-art (SOTA) by a significant margin in diverse tasks under the limited data setting. EfficientZero V2 exhibits a notable advancement over the prevailing general algorithm, DreamerV3, achieving superior outcomes in 50 of 66 evaluated tasks across diverse <b>benchmarks,</b> such as Atari 100k, Proprio Control, and Vision Control.</p></p class="citation"></blockquote><h3 id=3140--70243-motif-distribution-and-function-of-sparse-deep-neural-networks-olivia-t-zahn-et-al-2024>(31/40 | 70/243) Motif distribution and function of sparse deep neural networks (Olivia T. Zahn et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Olivia T. Zahn, Thomas L. Daniel, J. Nathan Kutz. (2024)<br><strong>Motif distribution and function of sparse deep neural networks</strong><br><button class=copy-to-clipboard title="Motif distribution and function of sparse deep neural networks" index=70>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-70 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Pruning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00974v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00974v1.pdf filename=2403.00974v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We characterize the connectivity structure of feed-forward, deep neural networks (DNNs) using network motif theory. To address whether a particular motif distribution is characteristic of the training task, or function of the DNN, we compare the connectivity structure of 350 DNNs trained to simulate a bio-mechanical flight control system with different randomly initialized parameters. We develop and implement algorithms for counting second- and third-order motifs and calculate their significance using their Z-score. The DNNs are trained to solve the inverse problem of the flight dynamics model in Bustamante, et al. (2022) (i.e., predict the controls necessary for controlled flight from the initial and final state-space inputs) and are sparsified through an iterative <b>pruning</b> and retraining algorithm Zahn, et al. (2022). We show that, despite random initialization of network parameters, enforced sparsity causes DNNs to converge to similar connectivity patterns as characterized by their motif distributions. The results suggest how neural network function can be encoded in motif distributions, suggesting a variety of experiments for informing function and control.</p></p class="citation"></blockquote><h3 id=3240--71243-tree-regularized-tabular-embeddings-xuan-li-et-al-2024>(32/40 | 71/243) Tree-Regularized Tabular Embeddings (Xuan Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xuan Li, Yun Wang, Bo Li. (2024)<br><strong>Tree-Regularized Tabular Embeddings</strong><br><button class=copy-to-clipboard title="Tree-Regularized Tabular Embeddings" index=71>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-71 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00963v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00963v1.pdf filename=2403.00963v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Tabular neural network (NN) has attracted remarkable attentions and its recent advances have gradually narrowed the performance gap with respect to tree-based models on many public datasets. While the mainstreams focus on calibrating NN to fit tabular data, we emphasize the importance of homogeneous embeddings and alternately concentrate on regularizing tabular inputs through <b>supervised</b> pretraining. Specifically, we extend a recent work (DeepTLF) and utilize the structure of pretrained tree ensembles to transform raw variables into a single vector (T2V), or an array of tokens (T2T). Without loss of space efficiency, these binarized embeddings can be consumed by canonical tabular NN with fully-connected or attention-based building blocks. Through quantitative experiments on 88 OpenML datasets with binary classification task, we validated that the proposed tree-regularized representation not only tapers the difference with respect to tree-based models, but also achieves on-par and better performance when compared with advanced NN models. Most importantly, it possesses better robustness and can be easily scaled and generalized as standalone encoder for tabular modality. Codes: <a href=https://github.com/milanlx/tree-regularized-embedding>https://github.com/milanlx/tree-regularized-embedding</a>.</p></p class="citation"></blockquote><h3 id=3340--72243-scalable-learning-of-item-response-theory-models-susanne-frick-et-al-2024>(33/40 | 72/243) Scalable Learning of Item Response Theory Models (Susanne Frick et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Susanne Frick, Amer Krivošija, Alexander Munteanu. (2024)<br><strong>Scalable Learning of Item Response Theory Models</strong><br><button class=copy-to-clipboard title="Scalable Learning of Item Response Theory Models" index=72>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-72 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-DS, cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Logistic Regression<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00680v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00680v1.pdf filename=2403.00680v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Item Response Theory (IRT) models aim to assess latent abilities of $n$ examinees along with latent difficulty characteristics of $m$ test items from categorical data that indicates the quality of their corresponding answers. Classical psychometric assessments are based on a relatively small number of examinees and items, say a class of $200$ students solving an exam comprising $10$ problems. More recent global large scale assessments such as PISA, or internet studies, may lead to significantly increased numbers of participants. Additionally, in the context of Machine Learning where algorithms take the role of examinees and data analysis problems take the role of items, both $n$ and $m$ may become very large, challenging the efficiency and scalability of computations. To learn the latent variables in IRT models from large data, we leverage the similarity of these models to <b>logistic</b> <b>regression,</b> which can be approximated accurately using small weighted subsets called coresets. We develop coresets for their use in alternating IRT training algorithms, facilitating scalable learning from large data.</p></p class="citation"></blockquote><h3 id=3440--73243-indirectly-parameterized-concrete-autoencoders-alfred-nilsson-et-al-2024>(34/40 | 73/243) Indirectly Parameterized Concrete Autoencoders (Alfred Nilsson et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alfred Nilsson, Klas Wijk, Sai bharath chandra Gutha, Erik Englesson, Alexandra Hotti, Carlo Saccardi, Oskar Kviman, Jens Lagergren, Ricardo Vinuesa, Hossein Azizpour. (2024)<br><strong>Indirectly Parameterized Concrete Autoencoders</strong><br><button class=copy-to-clipboard title="Indirectly Parameterized Concrete Autoencoders" index=73>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-73 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00563v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00563v1.pdf filename=2403.00563v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Feature selection is a crucial task in settings where data is high-dimensional or acquiring the full set of features is costly. Recent developments in neural network-based embedded feature selection show promising results across a wide range of applications. Concrete <b>Autoencoders</b> (CAEs), considered state-of-the-art in embedded feature selection, may struggle to achieve stable joint optimization, hurting their training time and generalization. In this work, we identify that this instability is correlated with the CAE learning duplicate selections. To remedy this, we propose a simple and effective improvement: Indirectly Parameterized CAEs (IP-CAEs). IP-CAEs learn an embedding and a mapping from it to the Gumbel-Softmax distributions&rsquo; parameters. Despite being simple to implement, IP-CAE exhibits significant and consistent improvements over CAE in both generalization and training time across several datasets for reconstruction and classification. Unlike CAE, IP-CAE effectively leverages non-linear relationships and does not require retraining the jointly optimized decoder. Furthermore, our approach is, in principle, generalizable to Gumbel-Softmax distributions beyond feature selection.</p></p class="citation"></blockquote><h3 id=3540--74243-provably-robust-dpo-aligning-language-models-with-noisy-feedback-sayak-ray-chowdhury-et-al-2024>(35/40 | 74/243) Provably Robust DPO: Aligning Language Models with Noisy Feedback (Sayak Ray Chowdhury et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sayak Ray Chowdhury, Anush Kini, Nagarajan Natarajan. (2024)<br><strong>Provably Robust DPO: Aligning Language Models with Noisy Feedback</strong><br><button class=copy-to-clipboard title="Provably Robust DPO: Aligning Language Models with Noisy Feedback" index=74>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-74 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Direct Preference Optimization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00409v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00409v1.pdf filename=2403.00409v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Learning from preference-based feedback has recently gained traction as a promising approach to align language models with human interests. While these aligned generative models have demonstrated impressive capabilities across various tasks, their dependence on high-quality human preference data poses a bottleneck in practical applications. Specifically, noisy (incorrect and ambiguous) preference pairs in the dataset might restrict the language models from capturing human intent accurately. While practitioners have recently proposed heuristics to mitigate the effect of noisy preferences, a complete theoretical understanding of their workings remain elusive. In this work, we aim to bridge this gap by by introducing a general framework for policy optimization in the presence of random preference flips. We focus on the <b>direct</b> <b>preference</b> <b>optimization</b> (DPO) algorithm in particular since it assumes that preferences adhere to the Bradley-Terry-Luce (BTL) model, raising concerns about the impact of noisy data on the learned policy. We design a novel loss function, which de-bias the effect of noise on average, making a policy trained by minimizing that loss robust to the noise. Under log-linear parameterization of the policy class and assuming good feature coverage of the SFT policy, we prove that the sub-optimality gap of the proposed robust DPO (rDPO) policy compared to the optimal policy is of the order $O(\frac{1}{1-2\epsilon}\sqrt{\frac{d}{n}})$, where $\epsilon &lt; 1/2$ is flip rate of labels, $d$ is policy parameter dimension and $n$ is size of dataset. Our experiments on IMDb sentiment generation and Anthropic&rsquo;s helpful-harmless dataset show that rDPO is robust to noise in preference labels compared to vanilla DPO and other heuristics proposed by practitioners.</p></p class="citation"></blockquote><h3 id=3640--75243-disaggregated-multi-tower-topology-aware-modeling-technique-for-efficient-large-scale-recommendation-liang-luo-et-al-2024>(36/40 | 75/243) Disaggregated Multi-Tower: Topology-aware Modeling Technique for Efficient Large-Scale Recommendation (Liang Luo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Liang Luo, Buyun Zhang, Michael Tsang, Yinbin Ma, Ching-Hsiang Chu, Yuxin Chen, Shen Li, Yuchen Hao, Yanli Zhao, Guna Lakshminarayanan, Ellie Dingqiao Wen, Jongsoo Park, Dheevatsa Mudigere, Maxim Naumov. (2024)<br><strong>Disaggregated Multi-Tower: Topology-aware Modeling Technique for Efficient Large-Scale Recommendation</strong><br><button class=copy-to-clipboard title="Disaggregated Multi-Tower: Topology-aware Modeling Technique for Efficient Large-Scale Recommendation" index=75>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-75 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-DC, cs-IR, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00877v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00877v2.pdf filename=2403.00877v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study a mismatch between the deep learning <b>recommendation</b> models&rsquo; flat architecture, common distributed training paradigm and hierarchical data center topology. To address the associated inefficiencies, we propose Disaggregated Multi-Tower (DMT), a modeling technique that consists of (1) Semantic-preserving Tower Transform (SPTT), a novel training paradigm that decomposes the monolithic global embedding lookup process into disjoint towers to exploit data center locality; (2) Tower Module (TM), a synergistic dense component attached to each tower to reduce model complexity and communication volume through hierarchical feature interaction; and (3) Tower Partitioner (TP), a feature partitioner to systematically create towers with meaningful feature interactions and load balanced assignments to preserve model quality and training throughput via learned embeddings. We show that DMT can achieve up to 1.9x speedup compared to the state-of-the-art baselines without losing accuracy across multiple generations of hardware at large data center scales.</p></p class="citation"></blockquote><h3 id=3740--76243-scale-invariant-gradient-aggregation-for-constrained-multi-objective-reinforcement-learning-dohyeong-kim-et-al-2024>(37/40 | 76/243) Scale-Invariant Gradient Aggregation for Constrained Multi-Objective Reinforcement Learning (Dohyeong Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dohyeong Kim, Mineui Hong, Jeongho Park, Songhwai Oh. (2024)<br><strong>Scale-Invariant Gradient Aggregation for Constrained Multi-Objective Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Scale-Invariant Gradient Aggregation for Constrained Multi-Objective Reinforcement Learning" index=76>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-76 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00282v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00282v1.pdf filename=2403.00282v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multi-objective <b>reinforcement</b> <b>learning</b> (MORL) aims to find a set of Pareto optimal policies to cover various preferences. However, to apply MORL in real-world applications, it is important to find policies that are not only Pareto optimal but also satisfy pre-defined constraints for safety. To this end, we propose a constrained MORL (CMORL) algorithm called Constrained Multi-Objective Gradient Aggregator (CoMOGA). Recognizing the difficulty of handling multiple objectives and constraints concurrently, CoMOGA relaxes the original CMORL problem into a constrained optimization problem by transforming the objectives into additional constraints. This novel transformation process ensures that the converted constraints are invariant to the objective scales while having the same effect as the original objectives. We show that the proposed method converges to a local Pareto optimal policy while satisfying the predefined constraints. Empirical evaluations across various tasks show that the proposed method outperforms other baselines by consistently meeting constraints and demonstrating invariance to the objective scales.</p></p class="citation"></blockquote><h3 id=3840--77243-shifted-interpolation-for-differential-privacy-jinho-bok-et-al-2024>(38/40 | 77/243) Shifted Interpolation for Differential Privacy (Jinho Bok et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinho Bok, Weijie Su, Jason M. Altschuler. (2024)<br><strong>Shifted Interpolation for Differential Privacy</strong><br><button class=copy-to-clipboard title="Shifted Interpolation for Differential Privacy" index=77>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-77 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-LG, cs.LG, math-OC, math-ST, stat-ML, stat-TH<br>Keyword Score: 10<br>Keywords: Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00278v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00278v1.pdf filename=2403.00278v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Noisy gradient descent and its variants are the predominant algorithms for differentially private machine learning. It is a fundamental question to quantify their privacy leakage, yet tight characterizations remain open even in the foundational setting of convex losses. This paper improves over previous analyses by establishing (and refining) the &ldquo;privacy amplification by iteration&rdquo; phenomenon in the unifying framework of $f$-differential privacy&ndash;which tightly captures all aspects of the privacy loss and immediately implies tighter privacy accounting in other notions of <b>differential</b> <b>privacy,</b> e.g., $(\varepsilon,\delta)$-DP and Renyi DP. Our key technical insight is the construction of shifted interpolated processes that unravel the popular shifted-divergences argument, enabling generalizations beyond divergence-based relaxations of DP. Notably, this leads to the first exact privacy analysis in the foundational setting of strongly convex optimization. Our techniques extend to many settings: convex/strongly convex, constrained/unconstrained, full/cyclic/stochastic batches, and all combinations thereof. As an immediate corollary, we recover the $f$-DP characterization of the exponential mechanism for strongly convex optimization in Gopi et al. (2022), and moreover extend this result to more general settings.</p></p class="citation"></blockquote><h3 id=3940--78243-enhancing-multivariate-time-series-forecasting-with-mutual-information-driven-cross-variable-and-temporal-modeling-shiyi-qi-et-al-2024>(39/40 | 78/243) Enhancing Multivariate Time Series Forecasting with Mutual Information-driven Cross-Variable and Temporal Modeling (Shiyi Qi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shiyi Qi, Liangjian Wen, Yiduo Li, Yuanhang Yang, Zhe Li, Zhongwen Rao, Lujia Pan, Zenglin Xu. (2024)<br><strong>Enhancing Multivariate Time Series Forecasting with Mutual Information-driven Cross-Variable and Temporal Modeling</strong><br><button class=copy-to-clipboard title="Enhancing Multivariate Time Series Forecasting with Mutual Information-driven Cross-Variable and Temporal Modeling" index=78>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-78 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Mutual Information<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00869v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00869v1.pdf filename=2403.00869v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements have underscored the impact of deep learning techniques on multivariate time series forecasting (MTSF). Generally, these techniques are bifurcated into two categories: Channel-independence and Channel-mixing approaches. Although Channel-independence methods typically yield better results, Channel-mixing could theoretically offer improvements by leveraging inter-variable correlations. Nonetheless, we argue that the integration of uncorrelated information in channel-mixing methods could curtail the potential enhancement in MTSF model performance. To substantiate this claim, we introduce the Cross-variable Decorrelation Aware feature Modeling (CDAM) for Channel-mixing approaches, aiming to refine Channel-mixing by minimizing redundant information between channels while enhancing relevant <b>mutual</b> <b>information.</b> Furthermore, we introduce the Temporal correlation Aware Modeling (TAM) to exploit temporal correlations, a step beyond conventional single-step forecasting methods. This strategy maximizes the <b>mutual</b> <b>information</b> between adjacent sub-sequences of both the forecasted and target series. Combining CDAM and TAM, our novel framework significantly surpasses existing models, including those previously considered state-of-the-art, in comprehensive tests.</p></p class="citation"></blockquote><h3 id=4040--79243-imitation-learning-datasets-a-toolkit-for-creating-datasets-training-agents-and-benchmarking-nathan-gavenski-et-al-2024>(40/40 | 79/243) Imitation Learning Datasets: A Toolkit For Creating Datasets, Training Agents and Benchmarking (Nathan Gavenski et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nathan Gavenski, Michael Luck, Odinaldo Rodrigues. (2024)<br><strong>Imitation Learning Datasets: A Toolkit For Creating Datasets, Training Agents and Benchmarking</strong><br><button class=copy-to-clipboard title="Imitation Learning Datasets: A Toolkit For Creating Datasets, Training Agents and Benchmarking" index=79>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-79 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 6<br>Keywords: Benchmarking, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00550v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00550v1.pdf filename=2403.00550v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Imitation learning field requires expert data to train agents in a task. Most often, this learning approach suffers from the absence of available data, which results in techniques being tested on its dataset. Creating datasets is a cumbersome process requiring researchers to train expert agents from scratch, record their interactions and test each <b>benchmark</b> method with newly created data. Moreover, creating new datasets for each new technique results in a lack of consistency in the evaluation process since each dataset can drastically vary in state and action distribution. In response, this work aims to address these issues by creating Imitation Learning Datasets, a toolkit that allows for: (i) curated expert policies with multithreaded support for faster dataset creation; (ii) readily available datasets and techniques with precise measurements; and (iii) sharing implementations of common imitation learning techniques. Demonstration link: <a href=https://nathangavenski.github.io/#/il-datasets-video>https://nathangavenski.github.io/#/il-datasets-video</a></p></p class="citation"></blockquote><h2 id=cscr-9>cs.CR (9)</h2><h3 id=19--80243-crimson-empowering-strategic-reasoning-in-cybersecurity-through-large-language-models-jiandong-jin-et-al-2024>(1/9 | 80/243) Crimson: Empowering Strategic Reasoning in Cybersecurity through Large Language Models (Jiandong Jin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiandong Jin, Bowen Tang, Mingxuan Ma, Xiao Liu, Yunfei Wang, Qingnan Lai, Jia Yang, Changling Zhou. (2024)<br><strong>Crimson: Empowering Strategic Reasoning in Cybersecurity through Large Language Models</strong><br><button class=copy-to-clipboard title="Crimson: Empowering Strategic Reasoning in Cybersecurity through Large Language Models" index=80>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-80 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs.CR<br>Keyword Score: 80<br>Keywords: Fine-tuning, Fine-tuning, human-in-the-loop, GPT, GPT-4, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00878v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00878v1.pdf filename=2403.00878v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduces Crimson, a system that enhances the strategic <b>reasoning</b> capabilities of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> within the realm of cybersecurity. By correlating CVEs with MITRE ATT&amp;CK techniques, Crimson advances threat anticipation and strategic defense efforts. Our approach includes defining and evaluating cybersecurity strategic tasks, alongside implementing a comprehensive <b>human-in-the-loop</b> data-synthetic workflow to develop the CVE-to-ATT&amp;CK Mapping (CVEM) dataset. We further enhance <b>LLMs&rsquo;</b> <b>reasoning</b> abilities through a novel Retrieval-Aware Training (RAT) process and its refined iteration, RAT-R. Our findings demonstrate that an <b>LLM</b> <b>fine-tuned</b> with our techniques, possessing 7 billion parameters, approaches the performance level of <b>GPT-4,</b> showing markedly lower rates of hallucination and errors, and surpassing other models in strategic <b>reasoning</b> tasks. Moreover, domain-specific <b>fine-tuning</b> of embedding models significantly improves performance within cybersecurity contexts, underscoring the efficacy of our methodology. By leveraging Crimson to convert raw vulnerability data into structured and actionable insights, we bolster proactive cybersecurity defenses.</p></p class="citation"></blockquote><h3 id=29--81243-gradient-cuff-detecting-jailbreak-attacks-on-large-language-models-by-exploring-refusal-loss-landscapes-xiaomeng-hu-et-al-2024>(2/9 | 81/243) Gradient Cuff: Detecting Jailbreak Attacks on Large Language Models by Exploring Refusal Loss Landscapes (Xiaomeng Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaomeng Hu, Pin-Yu Chen, Tsung-Yi Ho. (2024)<br><strong>Gradient Cuff: Detecting Jailbreak Attacks on Large Language Models by Exploring Refusal Loss Landscapes</strong><br><button class=copy-to-clipboard title="Gradient Cuff: Detecting Jailbreak Attacks on Large Language Models by Exploring Refusal Loss Landscapes" index=81>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-81 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CL, cs-CR, cs-LG, cs.CR<br>Keyword Score: 70<br>Keywords: Generative AI, Reinforcement Learning, Reinforcement Learning from Human Feedback, Reinforcement Learning from Human Feedback, LLaMA, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00867v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00867v2.pdf filename=2403.00867v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> are becoming a prominent <b>generative</b> <b>AI</b> tool, where the user enters a query and the <b>LLM</b> generates an answer. To reduce harm and misuse, efforts have been made to align these <b>LLMs</b> to human values using advanced training techniques such as <b>Reinforcement</b> <b>Learning</b> <b>from</b> <b>Human</b> <b>Feedback</b> <b>(RLHF).</b> However, recent studies have highlighted the vulnerability of <b>LLMs</b> to adversarial jailbreak attempts aiming at subverting the embedded safety guardrails. To address this challenge, this paper defines and investigates the Refusal Loss of <b>LLMs</b> and then proposes a method called Gradient Cuff to detect jailbreak attempts. Gradient Cuff exploits the unique properties observed in the refusal loss landscape, including functional values and its smoothness, to design an effective two-step detection strategy. Experimental results on two aligned <b>LLMs</b> <b>(LLaMA-2-7B-Chat</b> and Vicuna-7B-V1.5) and six types of jailbreak attacks (GCG, AutoDAN, PAIR, TAP, Base64, and LRL) show that Gradient Cuff can significantly improve the <b>LLM&rsquo;s</b> rejection capability for malicious jailbreak queries, while maintaining the model&rsquo;s performance for benign user queries by adjusting the detection threshold.</p></p class="citation"></blockquote><h3 id=39--82243-basedai-a-decentralized-p2p-network-for-zero-knowledge-large-language-models-zk-llms-sean-wellington-2024>(3/9 | 82/243) BasedAI: A decentralized P2P network for Zero Knowledge Large Language Models (ZK-LLMs) (Sean Wellington, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sean Wellington. (2024)<br><strong>BasedAI: A decentralized P2P network for Zero Knowledge Large Language Models (ZK-LLMs)</strong><br><button class=copy-to-clipboard title="BasedAI: A decentralized P2P network for Zero Knowledge Large Language Models (ZK-LLMs)" index=82>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-82 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-IR, cs.CR<br>Keyword Score: 60<br>Keywords: Generative Adversarial Network, Quantization, Quantization, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01008v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01008v1.pdf filename=2403.01008v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>BasedAI is a distributed network of machines which introduces decentralized infrastructure capable of integrating Fully Homomorphic Encryption (FHE) with any <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> connected to its network. The proposed framework embeds a default mechanism, called &ldquo;Cerberus Squeezing&rdquo;, into the mining process which enables the transformation of a standard <b>LLMs</b> into encrypted zero-knowledge <b>LLMs,</b> or &ldquo;ZK-LLMs&rdquo;, leveraging insights from <b>generative</b> <b>adversarial</b> <b>networks</b> for data privacy. This novel <b>quantization</b> mechanism empowers BasedAI miners to process and respond to <b>prompts</b> derived from User interaction with <b>LLMs</b> without the need for decrypting either the queries or their corresponding responses. The introduction of Cerberus Squeezing significantly improves performance degradation caused by <b>quantized</b> functions in current FHE-compliant computing environments by proactively optimizing calls between users, miners, and validators.</p></p class="citation"></blockquote><h3 id=49--83243-improving-android-malware-detection-through-data-augmentation-using-wasserstein-generative-adversarial-networks-kawana-stalin-et-al-2024>(4/9 | 83/243) Improving Android Malware Detection Through Data Augmentation Using Wasserstein Generative Adversarial Networks (Kawana Stalin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kawana Stalin, Mikias Berhanu Mekoya. (2024)<br><strong>Improving Android Malware Detection Through Data Augmentation Using Wasserstein Generative Adversarial Networks</strong><br><button class=copy-to-clipboard title="Improving Android Malware Detection Through Data Augmentation Using Wasserstein Generative Adversarial Networks" index=83>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-83 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs.CR<br>Keyword Score: 60<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, Data Augmentation, Generative Adversarial Network, Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00890v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00890v2.pdf filename=2403.00890v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Generative</b> <b>Adversarial</b> <b>Networks</b> <b>(GANs)</b> have demonstrated their versatility across various applications, including <b>data</b> <b>augmentation</b> and malware detection. This research explores the effectiveness of utilizing <b>GAN-generated</b> <b>data</b> <b>to</b> train a model for the detection of Android malware. Given the considerable storage requirements of Android applications, the study proposes a method to synthetically represent <b>data</b> <b>using</b> <b>GANs,</b> thereby reducing storage demands. The proposed methodology involves creating image representations of features extracted from an existing dataset. A <b>GAN</b> model is then employed to generate a more extensive dataset consisting of realistic synthetic grayscale images. Subsequently, this synthetic dataset is utilized to train a <b>Convolutional</b> <b>Neural</b> <b>Network</b> <b>(CNN)</b> designed to identify previously unseen Android malware applications. The study includes a comparative analysis of the <b>CNN&rsquo;s</b> performance when trained on real images versus synthetic images generated by the <b>GAN.</b> Furthermore, the research explores variations in performance between the Wasserstein <b>Generative</b> <b>Adversarial</b> <b>Network</b> (WGAN) and the Deep <b>Convolutional</b> <b>Generative</b> <b>Adversarial</b> <b>Network</b> (DCGAN). The investigation extends to studying the impact of image size and malware obfuscation on the classification model&rsquo;s effectiveness. The <b>data</b> <b>augmentation</b> approach implemented in this study resulted in a notable performance enhancement of the classification model, ranging from 1.5% to 7%, depending on the dataset. The highest achieved F1 score reached 0.975. Keywords&ndash;Generative Adversarial Networks, Android Malware, <b>Data</b> <b>Augmentation,</b> Wasserstein <b>Generative</b> <b>Adversarial</b> <b>Network</b></p></p class="citation"></blockquote><h3 id=59--84243-opaf-optimized-secure-two-party-computation-protocols-for-nonlinear-activation-functions-in-recurrent-neural-network-qian-feng-et-al-2024>(5/9 | 84/243) OPAF: Optimized Secure Two-Party Computation Protocols for Nonlinear Activation Functions in Recurrent Neural Network (Qian Feng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qian Feng, Zhihua Xia, Zhifeng Xu, Jiasi Weng, Jian Weng. (2024)<br><strong>OPAF: Optimized Secure Two-Party Computation Protocols for Nonlinear Activation Functions in Recurrent Neural Network</strong><br><button class=copy-to-clipboard title="OPAF: Optimized Secure Two-Party Computation Protocols for Nonlinear Activation Functions in Recurrent Neural Network" index=84>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-84 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 30<br>Keywords: Convolution, Fine-tuning, Recurrent Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00239v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00239v1.pdf filename=2403.00239v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep neural network (DNN) typically involves <b>convolutions,</b> pooling, and activation function. Due to the growing concern about privacy, privacy-preserving DNN becomes a hot research topic. Generally, the <b>convolution</b> and pooling operations can be supported by additive homomorphic and secure comparison, but the secure implementation of activation functions is not so straightforward for the requirements of accuracy and efficiency, especially for the non-linear ones such as exponential, sigmoid, and tanh functions. This paper pays a special attention to the implementation of such non-linear functions in semi-honest model with two-party settings, for which SIRNN is the current state-of-the-art. Different from previous works, we proposed improved implementations for these functions by using their intrinsic features as well as worthy tiny tricks. At first, we propose a novel and efficient protocol for exponential function by using a divide-and-conquer strategy with most of the computations executed locally. Exponential protocol is widely used in machine learning tasks such as Poisson regression, and is also a key component of sigmoid and tanh functions. Next, we take advantage of the symmetry of sigmoid and Tanh, and <b>fine-tune</b> the inputs to reduce the 2PC building blocks, which helps to save overhead and improve performance. As a result, we implement these functions with fewer fundamental building blocks. The comprehensive evaluations show that our protocols achieve state-of-the-art precision while reducing run-time by approximately 57%, 44%, and 42% for exponential (with only negative inputs), sigmoid, and Tanh functions, respectively.</p></p class="citation"></blockquote><h3 id=69--85243-private-benchmarking-to-prevent-contamination-and-improve-comparative-evaluation-of-llms-nishanth-chandran-et-al-2024>(6/9 | 85/243) Private Benchmarking to Prevent Contamination and Improve Comparative Evaluation of LLMs (Nishanth Chandran et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nishanth Chandran, Sunayana Sitaram, Divya Gupta, Rahul Sharma, Kashish Mittal, Manohar Swaminathan. (2024)<br><strong>Private Benchmarking to Prevent Contamination and Improve Comparative Evaluation of LLMs</strong><br><button class=copy-to-clipboard title="Private Benchmarking to Prevent Contamination and Improve Comparative Evaluation of LLMs" index=85>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-85 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CL, cs-CR, cs.CR<br>Keyword Score: 26<br>Keywords: Benchmarking, Benchmarking, Fine-tuning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00393v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00393v1.pdf filename=2403.00393v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Benchmarking</b> is the de-facto standard for evaluating <b>LLMs,</b> due to its speed, replicability and low cost. However, recent work has pointed out that the majority of the open source <b>benchmarks</b> available today have been contaminated or leaked into <b>LLMs,</b> meaning that <b>LLMs</b> have access to test data during pretraining and/or <b>fine-tuning.</b> This raises serious concerns about the validity of <b>benchmarking</b> studies conducted so far and the future of evaluation using <b>benchmarks.</b> To solve this problem, we propose Private <b>Benchmarking,</b> a solution where test datasets are kept private and models are evaluated without revealing the test data to the model. We describe various scenarios (depending on the trust placed on model owners or dataset owners), and present solutions to avoid data contamination using private <b>benchmarking.</b> For scenarios where the model weights need to be kept private, we describe solutions from confidential computing and cryptography that can aid in private <b>benchmarking.</b> Finally, we present solutions the problem of <b>benchmark</b> dataset auditing, to ensure that private <b>benchmarks</b> are of sufficiently high quality.</p></p class="citation"></blockquote><h3 id=79--86243-blockchain-empowered-federated-learning-benefits-challenges-and-solutions-zeju-cai-et-al-2024>(7/9 | 86/243) Blockchain-empowered Federated Learning: Benefits, Challenges, and Solutions (Zeju Cai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zeju Cai, Jianguo Chen, Yuting Fan, Zibin Zheng, Keqin Li. (2024)<br><strong>Blockchain-empowered Federated Learning: Benefits, Challenges, and Solutions</strong><br><button class=copy-to-clipboard title="Blockchain-empowered Federated Learning: Benefits, Challenges, and Solutions" index=86>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-86 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-LG, cs.CR<br>Keyword Score: 20<br>Keywords: Fairness, Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00873v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00873v1.pdf filename=2403.00873v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Federated</b> <b>learning</b> (FL) is a distributed machine learning approach that protects user data privacy by training models locally on clients and aggregating them on a parameter server. While effective at preserving privacy, FL systems face limitations such as single points of failure, lack of incentives, and inadequate security. To address these challenges, blockchain technology is integrated into FL systems to provide stronger security, <b>fairness,</b> and scalability. However, blockchain-empowered FL (BC-FL) systems introduce additional demands on network, computing, and storage resources. This survey provides a comprehensive review of recent research on BC-FL systems, analyzing the benefits and challenges associated with blockchain integration. We explore why blockchain is applicable to FL, how it can be implemented, and the challenges and existing solutions for its integration. Additionally, we offer insights on future research directions for the BC-FL system.</p></p class="citation"></blockquote><h3 id=89--87243-teach-llms-to-phish-stealing-private-information-from-language-models-ashwinee-panda-et-al-2024>(8/9 | 87/243) Teach LLMs to Phish: Stealing Private Information from Language Models (Ashwinee Panda et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ashwinee Panda, Christopher A. Choquette-Choo, Zhengming Zhang, Yaoqing Yang, Prateek Mittal. (2024)<br><strong>Teach LLMs to Phish: Stealing Private Information from Language Models</strong><br><button class=copy-to-clipboard title="Teach LLMs to Phish: Stealing Private Information from Language Models" index=87>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-87 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CL, cs-CR, cs-LG, cs.CR<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00871v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00871v1.pdf filename=2403.00871v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>When <b>large</b> <b>language</b> <b>models</b> are trained on private data, it can be a significant privacy risk for them to memorize and regurgitate sensitive information. In this work, we propose a new practical data extraction attack that we call &ldquo;neural phishing&rdquo;. This attack enables an adversary to target and extract sensitive or personally identifiable information (PII), e.g., credit card numbers, from a model trained on user data with upwards of 10% attack success rates, at times, as high as 50%. Our attack assumes only that an adversary can insert as few as 10s of benign-appearing sentences into the training dataset using only vague priors on the structure of the user data.</p></p class="citation"></blockquote><h3 id=99--88243-transfer-learning-for-security-challenges-and-future-directions-adrian-shuai-li-et-al-2024>(9/9 | 88/243) Transfer Learning for Security: Challenges and Future Directions (Adrian Shuai Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Adrian Shuai Li, Arun Iyengar, Ashish Kundu, Elisa Bertino. (2024)<br><strong>Transfer Learning for Security: Challenges and Future Directions</strong><br><button class=copy-to-clipboard title="Transfer Learning for Security: Challenges and Future Directions" index=88>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-88 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-LG, cs.CR<br>Keyword Score: 10<br>Keywords: Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00935v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00935v1.pdf filename=2403.00935v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Many machine learning and data mining algorithms rely on the assumption that the training and testing data share the same feature space and distribution. However, this assumption may not always hold. For instance, there are situations where we need to classify data in one domain, but we only have sufficient training data available from a different domain. The latter data may follow a distinct distribution. In such cases, successfully transferring knowledge across domains can significantly improve learning performance and reduce the need for extensive data labeling efforts. <b>Transfer</b> <b>learning</b> (TL) has thus emerged as a promising framework to tackle this challenge, particularly in security-related tasks. This paper aims to review the current advancements in utilizing TL techniques for security. The paper includes a discussion of the existing research gaps in applying TL in the security domain, as well as exploring potential future research directions and issues that arise in the context of TL-assisted security solutions.</p></p class="citation"></blockquote><h2 id=cscv-46>cs.CV (46)</h2><h3 id=146--89243-multimodal-arxiv-a-dataset-for-improving-scientific-comprehension-of-large-vision-language-models-lei-li-et-al-2024>(1/46 | 89/243) Multimodal ArXiv: A Dataset for Improving Scientific Comprehension of Large Vision-Language Models (Lei Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lei Li, Yuqi Wang, Runxin Xu, Peiyi Wang, Xiachong Feng, Lingpeng Kong, Qi Liu. (2024)<br><strong>Multimodal ArXiv: A Dataset for Improving Scientific Comprehension of Large Vision-Language Models</strong><br><button class=copy-to-clipboard title="Multimodal ArXiv: A Dataset for Improving Scientific Comprehension of Large Vision-Language Models" index=89>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-89 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CL, cs-CV, cs.CV<br>Keyword Score: 77<br>Keywords: Benchmarking, Benchmarking, Geometry, Multi-modal, Multi-modal, GPT, Mathematical Reasoning, Question Answering, Reasoning, Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00231v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00231v2.pdf filename=2403.00231v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Large <b>vision-language</b> models (LVLMs), exemplified by <b>GPT-4V,</b> excel across diverse tasks involving concrete images from natural scenes. However, their ability to interpret abstract figures, such as <b>geometry</b> shapes and scientific plots, remains limited due to a scarcity of training datasets in scientific domains. To fill this gap, we introduce <b>Multimodal</b> ArXiv, consisting of ArXivCap and ArXivQA, for enhancing LVLMs scientific comprehension. ArXivCap is a figure-caption dataset comprising 6.4M images and 3.9M captions sourced from 572K ArXiv papers spanning various scientific domains. Drawing from ArXivCap, we introduce ArXivQA, a <b>question-answering</b> <b>dataset</b> generated by <b>prompting</b> <b>GPT-4V</b> based on scientific figures. ArXivQA greatly enhances LVLMs&rsquo; <b>mathematical</b> <b>reasoning</b> capabilities, achieving a 10.4% absolute accuracy gain on a <b>multimodal</b> <b>mathematical</b> <b>reasoning</b> <b>benchmark.</b> Furthermore, employing ArXivCap, we devise four vision-to-text tasks for <b>benchmarking</b> LVLMs. Evaluation results with state-of-the-art LVLMs underscore their struggle with the nuanced semantics of academic figures, with domain-specific training yielding substantial performance gains. Our error analysis uncovers misinterpretations of visual context, recognition errors, and the production of overly simplified captions by current LVLMs, shedding light on future improvements.</p></p class="citation"></blockquote><h3 id=246--90243-robust-deep-labeling-of-radiological-emphysema-subtypes-using-squeeze-and-excitation-convolutional-neural-networks-the-mesa-lung-and-spiromics-studies-artur-wysoczanski-et-al-2024>(2/46 | 90/243) Robust deep labeling of radiological emphysema subtypes using squeeze and excitation convolutional neural networks: The MESA Lung and SPIROMICS Studies (Artur Wysoczanski et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Artur Wysoczanski, Nabil Ettehadi, Soroush Arabshahi, Yifei Sun, Karen Hinkley Stukovsky, Karol E. Watson, MeiLan K. Han, Erin D Michos, Alejandro P. Comellas, Eric A. Hoffman, Andrew F. Laine, R. Graham Barr, Elsa D. Angelini. (2024)<br><strong>Robust deep labeling of radiological emphysema subtypes using squeeze and excitation convolutional neural networks: The MESA Lung and SPIROMICS Studies</strong><br><button class=copy-to-clipboard title="Robust deep labeling of radiological emphysema subtypes using squeeze and excitation convolutional neural networks: The MESA Lung and SPIROMICS Studies" index=90>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-90 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 60<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, Supervised Learning, Unsupervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00257v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00257v1.pdf filename=2403.00257v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Pulmonary emphysema, the progressive, irreversible loss of lung tissue, is conventionally categorized into three subtypes identifiable on pathology and on lung computed tomography (CT) images. Recent work has led to the <b>unsupervised</b> <b>learning</b> of ten spatially-informed lung texture patterns (sLTPs) on lung CT, representing distinct patterns of emphysematous lung parenchyma based on both textural appearance and spatial location within the lung, and which aggregate into 6 robust and reproducible CT Emphysema Subtypes (CTES). Existing methods for sLTP segmentation, however, are slow and highly sensitive to changes in CT acquisition protocol. In this work, we present a robust 3-D squeeze-and-excitation <b>CNN</b> for <b>supervised</b> classification of sLTPs and CTES on lung CT. Our results demonstrate that this model achieves accurate and reproducible sLTP segmentation on lung CTscans, across two independent cohorts and independently of scanner manufacturer and model.</p></p class="citation"></blockquote><h3 id=346--91243-hypersdfusion-bridging-hierarchical-structures-in-language-and-geometry-for-enhanced-3d-text2shape-generation-zhiying-leng-et-al-2024>(3/46 | 91/243) HyperSDFusion: Bridging Hierarchical Structures in Language and Geometry for Enhanced 3D Text2Shape Generation (Zhiying Leng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhiying Leng, Tolga Birdal, Xiaohui Liang, Federico Tombari. (2024)<br><strong>HyperSDFusion: Bridging Hierarchical Structures in Language and Geometry for Enhanced 3D Text2Shape Generation</strong><br><button class=copy-to-clipboard title="HyperSDFusion: Bridging Hierarchical Structures in Language and Geometry for Enhanced 3D Text2Shape Generation" index=91>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-91 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 53<br>Keywords: Diffusion Model, Convolution, Geometry, Multi-modal, Representation Learning, Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00372v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00372v1.pdf filename=2403.00372v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>3D shape generation from text is a fundamental task in 3D <b>representation</b> <b>learning.</b> The text-shape pairs exhibit a hierarchical structure, where a general text like &ldquo;chair&rdquo; covers all 3D shapes of the chair, while more detailed <b>prompts</b> refer to more specific shapes. Furthermore, both text and 3D shapes are inherently hierarchical structures. However, existing Text2Shape methods, such as SDFusion, do not exploit that. In this work, we propose HyperSDFusion, a dual-branch <b>diffusion</b> <b>model</b> that generates 3D shapes from a given text. Since hyperbolic space is suitable for handling hierarchical data, we propose to learn the hierarchical <b>representations</b> <b>of</b> text and 3D shapes in hyperbolic space. First, we introduce a hyperbolic <b>text-image</b> encoder to learn the sequential and <b>multi-modal</b> hierarchical features of text in hyperbolic space. In addition, we design a hyperbolic text-graph <b>convolution</b> module to learn the hierarchical features of text in hyperbolic space. In order to fully utilize these text features, we introduce a dual-branch structure to embed text features in 3D feature space. At last, to endow the generated 3D shapes with a hierarchical structure, we devise a hyperbolic hierarchical loss. Our method is the first to explore the hyperbolic hierarchical <b>representation</b> <b>for</b> text-to-shape generation. Experimental results on the existing text-to-shape paired dataset, Text2Shape, achieved state-of-the-art results.</p></p class="citation"></blockquote><h3 id=446--92243-flatten-long-range-loss-landscapes-for-cross-domain-few-shot-learning-yixiong-zou-et-al-2024>(4/46 | 92/243) Flatten Long-Range Loss Landscapes for Cross-Domain Few-Shot Learning (Yixiong Zou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yixiong Zou, Yicong Liu, Yiman Hu, Yuhua Li, Ruixuan Li. (2024)<br><strong>Flatten Long-Range Loss Landscapes for Cross-Domain Few-Shot Learning</strong><br><button class=copy-to-clipboard title="Flatten Long-Range Loss Landscapes for Cross-Domain Few-Shot Learning" index=92>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-92 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Convolutional Neural Network, Few-shot, Few-shot Learning, Fine-tuning, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00567v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00567v1.pdf filename=2403.00567v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Cross-domain <b>few-shot</b> <b>learning</b> (CDFSL) aims to acquire knowledge from limited training data in the target domain by leveraging prior knowledge transferred from source domains with abundant training samples. CDFSL faces challenges in transferring knowledge across dissimilar domains and <b>fine-tuning</b> models with limited training data. To address these challenges, we initially extend the analysis of loss landscapes from the parameter space to the representation space, which allows us to simultaneously interpret the transferring and <b>fine-tuning</b> difficulties of CDFSL models. We observe that sharp minima in the loss landscapes of the representation space result in representations that are hard to transfer and <b>fine-tune.</b> Moreover, existing flatness-based methods have limited generalization ability due to their short-range flatness. To enhance the transferability and facilitate <b>fine-tuning,</b> we introduce a simple yet effective approach to achieve long-range flattening of the minima in the loss landscape. This approach considers representations that are differently normalized as minima in the loss landscape and flattens the high-loss region in the middle by randomly sampling interpolated representations. We implement this method as a new normalization layer that replaces the original one in both <b>CNNs</b> and ViTs. This layer is simple and lightweight, introducing only a minimal number of additional parameters. Experimental results on 8 datasets demonstrate that our approach outperforms state-of-the-art methods in terms of average accuracy. Moreover, our method achieves performance improvements of up to 9% compared to the current best approaches on individual datasets. Our code will be released.</p></p class="citation"></blockquote><h3 id=546--93243-visionllama-a-unified-llama-interface-for-vision-tasks-xiangxiang-chu-et-al-2024>(5/46 | 93/243) VisionLLaMA: A Unified LLaMA Interface for Vision Tasks (Xiangxiang Chu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiangxiang Chu, Jianlin Su, Bo Zhang, Chunhua Shen. (2024)<br><strong>VisionLLaMA: A Unified LLaMA Interface for Vision Tasks</strong><br><button class=copy-to-clipboard title="VisionLLaMA: A Unified LLaMA Interface for Vision Tasks" index=93>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-93 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Vision Transformer, LLaMA, Transformer, Large Language Model, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00522v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00522v1.pdf filename=2403.00522v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> are built on top of a <b>transformer-based</b> architecture to process textual inputs. For example, the <b>LLaMA</b> stands out among many open-source implementations. Can the same <b>transformer</b> be used to process 2D images? In this paper, we answer this question by unveiling a <b>LLaMA-like</b> <b>vision</b> <b>transformer</b> in plain and pyramid forms, termed VisionLLaMA, which is tailored for this purpose. VisionLLaMA is a unified and generic modelling framework for solving most <b>vision</b> <b>tasks.</b> We extensively evaluate its effectiveness using typical pre-training paradigms in a good portion of downstream tasks of image perception and especially image generation. In many cases, VisionLLaMA have exhibited substantial gains over the previous state-of-the-art <b>vision</b> <b>transformers.</b> We believe that VisionLLaMA can serve as a strong new baseline model for <b>vision</b> <b>generation</b> and understanding. Our code will be released at <a href=https://github.com/Meituan-AutoML/VisionLLaMA>https://github.com/Meituan-AutoML/VisionLLaMA</a>.</p></p class="citation"></blockquote><h3 id=646--94243-deformable-one-shot-face-stylization-via-dino-semantic-guidance-yang-zhou-et-al-2024>(6/46 | 94/243) Deformable One-shot Face Stylization via DINO Semantic Guidance (Yang Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yang Zhou, Zichong Chen, Hui Huang. (2024)<br><strong>Deformable One-shot Face Stylization via DINO Semantic Guidance</strong><br><button class=copy-to-clipboard title="Deformable One-shot Face Stylization via DINO Semantic Guidance" index=94>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-94 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Vision Transformer, Fine-tuning, Self-supervised Learning, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00459v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00459v2.pdf filename=2403.00459v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper addresses the complex issue of one-shot face stylization, focusing on the simultaneous consideration of appearance and structure, where previous methods have fallen short. We explore deformation-aware face stylization that diverges from traditional single-image style reference, opting for a real-style image pair instead. The cornerstone of our method is the utilization of a <b>self-supervised</b> <b>vision</b> <b>transformer,</b> specifically DINO-ViT, to establish a robust and consistent facial structure representation across both real and style domains. Our stylization process begins by adapting the StyleGAN generator to be deformation-aware through the integration of spatial <b>transformers</b> (STN). We then introduce two innovative constraints for generator <b>fine-tuning</b> under the guidance of DINO semantics: i) a directional deformation loss that regulates directional vectors in DINO space, and ii) a relative structural consistency constraint based on DINO token self-similarities, ensuring diverse generation. Additionally, style-mixing is employed to align the color generation with the reference, minimizing inconsistent correspondences. This framework delivers enhanced deformability for general one-shot face stylization, achieving notable efficiency with a <b>fine-tuning</b> duration of approximately 10 minutes. Extensive qualitative and quantitative comparisons demonstrate our superiority over state-of-the-art one-shot face stylization methods. Code is available at <a href=https://github.com/zichongc/DoesFS>https://github.com/zichongc/DoesFS</a></p></p class="citation"></blockquote><h3 id=746--95243-masklrf-self-supervised-pretraining-via-masked-autoencoding-of-local-reference-frames-for-rotation-invariant-3d-point-set-analysis-takahiko-furuya-2024>(7/46 | 95/243) MaskLRF: Self-supervised Pretraining via Masked Autoencoding of Local Reference Frames for Rotation-invariant 3D Point Set Analysis (Takahiko Furuya, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Takahiko Furuya. (2024)<br><strong>MaskLRF: Self-supervised Pretraining via Masked Autoencoding of Local Reference Frames for Rotation-invariant 3D Point Set Analysis</strong><br><button class=copy-to-clipboard title="MaskLRF: Self-supervised Pretraining via Masked Autoencoding of Local Reference Frames for Rotation-invariant 3D Point Set Analysis" index=95>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-95 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 45<br>Keywords: Geometry, Self-supervised Learning, Self-supervised Pre-training, Domain Adaptation, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00206v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00206v1.pdf filename=2403.00206v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Following the successes in the fields of vision and language, <b>self-supervised</b> <b>pretraining</b> via masked autoencoding of 3D point set data, or Masked Point Modeling (MPM), has achieved state-of-the-art accuracy in various downstream tasks. However, current MPM methods lack a property essential for 3D point set analysis, namely, invariance against rotation of 3D objects/scenes. Existing MPM methods are thus not necessarily suitable for real-world applications where 3D point sets may have inconsistent orientations. This paper develops, for the first time, a rotation-invariant <b>self-supervised</b> <b>pretraining</b> framework for practical 3D point set analysis. The proposed algorithm, called MaskLRF, learns rotation-invariant and highly generalizable latent features via masked autoencoding of 3D points within Local Reference Frames (LRFs), which are not affected by rotation of 3D point sets. MaskLRF enhances the quality of latent features by integrating feature refinement using relative pose encoding and feature reconstruction using low-level but rich 3D <b>geometry.</b> The efficacy of MaskLRF is validated via extensive experiments on diverse downstream tasks including classification, segmentation, registration, and <b>domain</b> <b>adaptation.</b> I confirm that MaskLRF achieves new state-of-the-art accuracies in analyzing 3D point sets having inconsistent orientations. Code will be available at: <a href=https://github.com/takahikof/MaskLRF>https://github.com/takahikof/MaskLRF</a></p></p class="citation"></blockquote><h3 id=846--96243-improving-explicit-spatial-relationships-in-text-to-image-generation-through-an-automatically-derived-dataset-ander-salaberria-et-al-2024>(8/46 | 96/243) Improving Explicit Spatial Relationships in Text-to-Image Generation through an Automatically Derived Dataset (Ander Salaberria et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ander Salaberria, Gorka Azkune, Oier Lopez de Lacalle, Aitor Soroa, Eneko Agirre, Frank Keller. (2024)<br><strong>Improving Explicit Spatial Relationships in Text-to-Image Generation through an Automatically Derived Dataset</strong><br><button class=copy-to-clipboard title="Improving Explicit Spatial Relationships in Text-to-Image Generation through an Automatically Derived Dataset" index=96>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-96 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Diffusion Model, Fine-tuning, Fine-tuning, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00587v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00587v1.pdf filename=2403.00587v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing work has observed that current <b>text-to-image</b> systems do not accurately reflect explicit spatial relations between objects such as &rsquo;left of&rsquo; or &lsquo;below&rsquo;. We hypothesize that this is because explicit spatial relations rarely appear in the image captions used to train these models. We propose an automatic method that, given existing images, generates synthetic captions that contain 14 explicit spatial relations. We introduce the Spatial Relation for Generation (SR4G) dataset, which contains 9.9 millions image-caption pairs for training, and more than 60 thousand captions for evaluation. In order to test generalization we also provide an &lsquo;unseen&rsquo; split, where the set of objects in the train and test captions are disjoint. SR4G is the first dataset that can be used to spatially <b>fine-tune</b> <b>text-to-image</b> systems. We show that <b>fine-tuning</b> two different Stable <b>Diffusion</b> <b>models</b> (denoted as SD$<em>{SR4G}$) yields up to 9 points improvements in the VISOR metric. The improvement holds in the &lsquo;unseen&rsquo; split, showing that SD$</em>{SR4G}$ is able to generalize to unseen objects. SD$_{SR4G}$ improves the state-of-the-art with fewer parameters, and avoids complex architectures. Our analysis shows that improvement is consistent for all relations. The dataset and the code will be publicly available.</p></p class="citation"></blockquote><h3 id=946--97243-data-efficient-event-camera-pre-training-via-disentangled-masked-modeling-zhenpeng-huang-et-al-2024>(9/46 | 97/243) Data-efficient Event Camera Pre-training via Disentangled Masked Modeling (Zhenpeng Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhenpeng Huang, Chao Li, Hao Chen, Yongjian Deng, Yifeng Geng, Limin Wang. (2024)<br><strong>Data-efficient Event Camera Pre-training via Disentangled Masked Modeling</strong><br><button class=copy-to-clipboard title="Data-efficient Event Camera Pre-training via Disentangled Masked Modeling" index=97>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-97 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Knowledge Distillation, Knowledge Distillation, Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00416v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00416v1.pdf filename=2403.00416v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we present a new data-efficient voxel-based <b>self-supervised</b> <b>learning</b> method for event cameras. Our pre-training overcomes the limitations of previous methods, which either sacrifice temporal information by converting event sequences into 2D images for utilizing pre-trained image models or directly employ paired image data for <b>knowledge</b> <b>distillation</b> to enhance the learning of event streams. In order to make our pre-training data-efficient, we first design a semantic-uniform masking method to address the learning imbalance caused by the varying reconstruction difficulties of different regions in non-uniform data when using random masking. In addition, we ease the traditional hybrid masked modeling process by explicitly decomposing it into two branches, namely local spatio-temporal reconstruction and global semantic reconstruction to encourage the encoder to capture local correlations and global semantics, respectively. This decomposition allows our <b>selfsupervised</b> <b>learning</b> method to converge faster with minimal pre-training data. Compared to previous approaches, our <b>self-supervised</b> <b>learning</b> method does not rely on paired RGB images, yet enables simultaneous exploration of spatial and temporal cues in multiple scales. It exhibits excellent generalization performance and demonstrates significant improvements across various tasks with fewer parameters and lower computational costs.</p></p class="citation"></blockquote><h3 id=1046--98243-invariant-test-time-adaptation-for-vision-language-model-generalization-huan-ma-et-al-2024>(10/46 | 98/243) Invariant Test-Time Adaptation for Vision-Language Model Generalization (Huan Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Huan Ma, Yan Zhu, Changqing Zhang, Peilin Zhao, Baoyuan Wu, Long-Kai Huang, Qinghua Hu, Bingzhe Wu. (2024)<br><strong>Invariant Test-Time Adaptation for Vision-Language Model Generalization</strong><br><button class=copy-to-clipboard title="Invariant Test-Time Adaptation for Vision-Language Model Generalization" index=98>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-98 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 40<br>Keywords: Foundation Model, Image2text, Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00376v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00376v1.pdf filename=2403.00376v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Vision-language</b> <b>foundation</b> <b>models</b> have exhibited remarkable success across a multitude of downstream tasks due to their scalability on extensive <b>image-text</b> paired datasets. However, these models display significant limitations when applied to long-tail tasks, such as fine-grained image classification, as a result of &ldquo;decision shortcuts&rdquo; that hinders their generalization capabilities. In this work, we find that the CLIP model possesses a rich set of features, encompassing both \textit{desired invariant causal features} and \textit{undesired decision shortcuts}. Moreover, the underperformance of CLIP on downstream tasks originates from its inability to effectively utilize pre-trained features in accordance with specific task requirements. To address this challenge, this paper introduces a test-time <b>prompt</b> tuning paradigm that optimizes a learnable <b>prompt,</b> thereby compelling the model to exploit genuine causal invariant features while disregarding decision shortcuts during the inference phase. The proposed method effectively alleviates excessive dependence on potentially misleading, task-irrelevant contextual information, while concurrently emphasizing critical, task-related visual cues. We conduct comparative analysis of the proposed method against various approaches which validates its effectiveness.</p></p class="citation"></blockquote><h3 id=1146--99243-odm-a-text-image-further-alignment-pre-training-approach-for-scene-text-detection-and-spotting-chen-duan-et-al-2024>(11/46 | 99/243) ODM: A Text-Image Further Alignment Pre-training Approach for Scene Text Detection and Spotting (Chen Duan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chen Duan, Pei Fu, Shan Guo, Qianyi Jiang, Xiaoming Wei. (2024)<br><strong>ODM: A Text-Image Further Alignment Pre-training Approach for Scene Text Detection and Spotting</strong><br><button class=copy-to-clipboard title="ODM: A Text-Image Further Alignment Pre-training Approach for Scene Text Detection and Spotting" index=99>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-99 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Optical Character Recognition, Optical Character Recognition, Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00303v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00303v1.pdf filename=2403.00303v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, <b>text-image</b> joint pre-training techniques have shown promising results in various tasks. However, in <b>Optical</b> <b>Character</b> <b>Recognition</b> <b>(OCR)</b> tasks, aligning text instances with their corresponding text regions in images poses a challenge, as it requires effective alignment between text and <b>OCR-Text</b> (referring to the text in images as <b>OCR-Text</b> to distinguish from the text in natural language) rather than a holistic understanding of the overall image content. In this paper, we propose a new pre-training method called <b>OCR-Text</b> Destylization Modeling (ODM) that transfers diverse styles of text found in images to a uniform style based on the text <b>prompt.</b> With ODM, we achieve better alignment between text and <b>OCR-Text</b> and enable pre-trained models to adapt to the complex and diverse styles of scene text detection and spotting tasks. Additionally, we have designed a new labeling generation method specifically for ODM and combined it with our proposed Text-Controller module to address the challenge of annotation costs in <b>OCR</b> tasks, allowing a larger amount of unlabeled data to participate in pre-training. Extensive experiments on multiple public datasets demonstrate that our method significantly improves performance and outperforms current pre-training methods in scene text detection and spotting tasks. Code is available at {https://github.com/PriNing/ODM}.</p></p class="citation"></blockquote><h3 id=1246--100243-halc-object-hallucination-reduction-via-adaptive-focal-contrast-decoding-zhaorun-chen-et-al-2024>(12/46 | 100/243) HALC: Object Hallucination Reduction via Adaptive Focal-Contrast Decoding (Zhaorun Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhaorun Chen, Zhuokai Zhao, Hongyin Luo, Huaxiu Yao, Bo Li, Jiawei Zhou. (2024)<br><strong>HALC: Object Hallucination Reduction via Adaptive Focal-Contrast Decoding</strong><br><button class=copy-to-clipboard title="HALC: Object Hallucination Reduction via Adaptive Focal-Contrast Decoding" index=100>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-100 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 36<br>Keywords: Benchmarking, Multi-modal, Grounding, Text Generation, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00425v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00425v1.pdf filename=2403.00425v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While large <b>vision-language</b> models (LVLMs) have demonstrated impressive capabilities in interpreting <b>multi-modal</b> contexts, they invariably suffer from object hallucinations (OH). We introduce HALC, a novel decoding algorithm designed to mitigate OH in LVLMs. HALC leverages distinct fine-grained optimal visual information in <b>vision-language</b> tasks and operates on both local and global contexts simultaneously. Specifically, HALC integrates a robust auto-focal <b>grounding</b> mechanism (locally) to correct hallucinated tokens on the fly, and a specialized beam search algorithm (globally) to significantly reduce OH while preserving <b>text</b> <b>generation</b> quality. Additionally, HALC can be integrated into any LVLMs as a plug-and-play module without extra training. Extensive experimental studies demonstrate the effectiveness of HALC in reducing OH, outperforming state-of-the-arts across four <b>benchmarks.</b></p></p class="citation"></blockquote><h3 id=1346--101243-learning-and-leveraging-world-models-in-visual-representation-learning-quentin-garrido-et-al-2024>(13/46 | 101/243) Learning and Leveraging World Models in Visual Representation Learning (Quentin Garrido et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Quentin Garrido, Mahmoud Assran, Nicolas Ballas, Adrien Bardes, Laurent Najman, Yann LeCun. (2024)<br><strong>Learning and Leveraging World Models in Visual Representation Learning</strong><br><button class=copy-to-clipboard title="Learning and Leveraging World Models in Visual Representation Learning" index=101>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-101 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 35<br>Keywords: Fine-tuning, Fine-tuning, Representation Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00504v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00504v1.pdf filename=2403.00504v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Joint-Embedding Predictive Architecture (JEPA) has emerged as a promising <b>self-supervised</b> approach that learns by leveraging a world model. While previously limited to predicting missing parts of an input, we explore how to generalize the JEPA prediction task to a broader set of corruptions. We introduce Image World Models, an approach that goes beyond masked image modeling and learns to predict the effect of global photometric transformations in latent space. We study the recipe of learning performant IWMs and show that it relies on three key aspects: conditioning, prediction difficulty, and capacity. Additionally, we show that the predictive world model learned by IWM can be adapted through <b>finetuning</b> to solve diverse tasks; a <b>fine-tuned</b> IWM world model matches or surpasses the performance of previous <b>self-supervised</b> methods. Finally, we show that learning with an IWM allows one to control the abstraction level of the learned <b>representations,</b> <b>learning</b> invariant <b>representations</b> <b>such</b> as contrastive methods, or equivariant <b>representations</b> <b>such</b> as masked image modelling.</p></p class="citation"></blockquote><h3 id=1446--102243-tempcompass-do-video-llms-really-understand-videos-yuanxin-liu-et-al-2024>(14/46 | 102/243) TempCompass: Do Video LLMs Really Understand Videos? (Yuanxin Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuanxin Liu, Shicheng Li, Yi Liu, Yuxiang Wang, Shuhuai Ren, Lei Li, Sishuo Chen, Xu Sun, Lu Hou. (2024)<br><strong>TempCompass: Do Video LLMs Really Understand Videos?</strong><br><button class=copy-to-clipboard title="TempCompass: Do Video LLMs Really Understand Videos?" index=102>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-102 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Benchmarking, Question Answering, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00476v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00476v1.pdf filename=2403.00476v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, there is a surge in interest surrounding video <b>large</b> <b>language</b> <b>models</b> (Video <b>LLMs).</b> However, existing <b>benchmarks</b> fail to provide a comprehensive feedback on the temporal perception ability of Video <b>LLMs.</b> On the one hand, most of them are unable to distinguish between different temporal aspects (e.g., speed, direction) and thus cannot reflect the nuanced performance on these specific aspects. On the other hand, they are limited in the diversity of task formats (e.g., only multi-choice <b>QA),</b> which hinders the understanding of how temporal perception performance may vary across different types of tasks. Motivated by these two problems, we propose the \textbf{TempCompass} <b>benchmark,</b> which introduces a diversity of temporal aspects and task formats. To collect high-quality test data, we devise two novel strategies: (1) In video collection, we construct conflicting videos that share the same static content but differ in a specific temporal aspect, which prevents Video <b>LLMs</b> from leveraging single-frame bias or language priors. (2) To collect the task instructions, we propose a paradigm where humans first annotate meta-information for a video and then an <b>LLM</b> generates the instruction. We also design an <b>LLM-based</b> approach to automatically and accurately evaluate the responses from Video <b>LLMs.</b> Based on TempCompass, we comprehensively evaluate 8 state-of-the-art (SOTA) Video <b>LLMs</b> and 3 Image <b>LLMs,</b> and reveal the discerning fact that these models exhibit notably poor temporal perception ability. Our data will be available at \url{https://github.com/llyx97/TempCompass}.</p></p class="citation"></blockquote><h3 id=1546--103243-task-indicating-transformer-for-task-conditional-dense-predictions-yuxiang-lu-et-al-2024>(15/46 | 103/243) Task Indicating Transformer for Task-conditional Dense Predictions (Yuxiang Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuxiang Lu, Shalayiding Sirejiding, Bayram Bayramli, Suizhi Huang, Yue Ding, Hongtao Lu. (2024)<br><strong>Task Indicating Transformer for Task-conditional Dense Predictions</strong><br><button class=copy-to-clipboard title="Task Indicating Transformer for Task-conditional Dense Predictions" index=103>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-103 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Graph Attention Networks, Benchmarking, Convolutional Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00327v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00327v1.pdf filename=2403.00327v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The task-conditional model is a distinctive stream for efficient multi-task learning. Existing works encounter a critical limitation in learning task-agnostic and task-specific representations, primarily due to shortcomings in global context modeling arising from <b>CNN-based</b> architectures, as well as a deficiency in multi-scale feature interaction within the decoder. In this paper, we introduce a novel task-conditional framework called Task Indicating <b>Transformer</b> (TIT) to tackle this challenge. Our approach designs a Mix Task Adapter module within the <b>transformer</b> block, which incorporates a Task Indicating Matrix through matrix decomposition, thereby enhancing long-range dependency modeling and parameter-efficient feature adaptation by capturing intra- and inter-task features. Moreover, we propose a Task Gate Decoder module that harnesses a Task Indicating Vector and <b>gating</b> mechanism to facilitate adaptive multi-scale feature refinement guided by task embeddings. Experiments on two public multi-task dense prediction <b>benchmarks,</b> NYUD-v2 and PASCAL-Context, demonstrate that our approach surpasses state-of-the-art task-conditional methods.</p></p class="citation"></blockquote><h3 id=1646--104243-multi-modal-attribute-prompting-for-vision-language-models-xin-liu-et-al-2024>(16/46 | 104/243) Multi-modal Attribute Prompting for Vision-Language Models (Xin Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xin Liu, Jiamin Wu, Tianzhu Zhang. (2024)<br><strong>Multi-modal Attribute Prompting for Vision-Language Models</strong><br><button class=copy-to-clipboard title="Multi-modal Attribute Prompting for Vision-Language Models" index=104>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-104 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Few-shot, Multi-modal, Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00219v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00219v1.pdf filename=2403.00219v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Large pre-trained <b>Vision-Language</b> Models (VLMs), like CLIP, exhibit strong generalization ability to downstream tasks but struggle in <b>few-shot</b> scenarios. Existing <b>prompting</b> techniques primarily focus on global text and image representations, yet overlooking <b>multi-modal</b> attribute characteristics. This limitation hinders the model&rsquo;s ability to perceive fine-grained visual details and restricts its generalization ability to a broader range of unseen classes. To address this issue, we propose a <b>Multi-modal</b> Attribute <b>Prompting</b> method (MAP) by jointly exploring textual attribute <b>prompting,</b> visual attribute <b>prompting,</b> and attribute-level alignment. The proposed MAP enjoys several merits. First, we introduce learnable visual attribute <b>prompts</b> enhanced by textual attribute semantics to adaptively capture visual attributes for images from unknown categories, boosting fine-grained visual perception capabilities for CLIP. Second, the proposed attribute-level alignment complements the global alignment to enhance the robustness of cross-modal alignment for open-vocabulary objects. To our knowledge, this is the first work to establish cross-modal attribute-level alignment for CLIP-based <b>few-shot</b> adaptation. Extensive experimental results on 11 datasets demonstrate that our method performs favorably against state-of-the-art approaches.</p></p class="citation"></blockquote><h3 id=1746--105243-region-adaptive-transform-with-segmentation-prior-for-image-compression-yuxi-liu-et-al-2024>(17/46 | 105/243) Region-Adaptive Transform with Segmentation Prior for Image Compression (Yuxi Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuxi Liu, Wenhan Yang, Huihui Bai, Yunchao Wei, Yao Zhao. (2024)<br><strong>Region-Adaptive Transform with Segmentation Prior for Image Compression</strong><br><button class=copy-to-clipboard title="Region-Adaptive Transform with Segmentation Prior for Image Compression" index=105>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-105 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00628v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00628v1.pdf filename=2403.00628v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Learned Image Compression (LIC) has shown remarkable progress in recent years. Existing works commonly employ <b>CNN-based</b> or <b>self-attention-based</b> modules as transform methods for compression. However, there is no prior research on neural transform that focuses on specific regions. In response, we introduce the class-agnostic segmentation masks (i.e. semantic masks without category labels) for extracting region-adaptive contextual information. Our proposed module, Region-Adaptive Transform, applies adaptive <b>convolutions</b> on different regions guided by the masks. Additionally, we introduce a plug-and-play module named Scale Affine Layer to incorporate rich contexts from various regions. While there have been prior image compression efforts that involve segmentation masks as additional intermediate inputs, our approach differs significantly from them. Our advantages lie in that, to avoid extra bitrate overhead, we treat these masks as privilege information, which is accessible during the model training stage but not required during the inference phase. To the best of our knowledge, we are the first to employ class-agnostic masks as privilege information and achieve superior performance in pixel-fidelity metrics, such as Peak Signal to Noise Ratio (PSNR). The experimental results demonstrate our improvement compared to previously well-performing methods, with about 8.2% bitrate saving compared to VTM-17.0. The code will be released at <a href=https://github.com/GityuxiLiu/Region-Adaptive-Transform-with-Segmentation-Prior-for-Image-Compression>https://github.com/GityuxiLiu/Region-Adaptive-Transform-with-Segmentation-Prior-for-Image-Compression</a>.</p></p class="citation"></blockquote><h3 id=1846--106243-flattening-singular-values-of-factorized-convolution-for-medical-images-zexin-feng-et-al-2024>(18/46 | 106/243) Flattening Singular Values of Factorized Convolution for Medical Images (Zexin Feng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zexin Feng, Na Zeng, Jiansheng Fang, Xingyue Wang, Xiaoxi Lu, Heng Meng, Jiang Liu. (2024)<br><strong>Flattening Singular Values of Factorized Convolution for Medical Images</strong><br><button class=copy-to-clipboard title="Flattening Singular Values of Factorized Convolution for Medical Images" index=106>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-106 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00606v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00606v1.pdf filename=2403.00606v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Convolutional</b> <b>neural</b> <b>networks</b> <b>(CNNs)</b> have long been the paradigm of choice for robust medical image processing (MIP). Therefore, it is crucial to effectively and efficiently deploy <b>CNNs</b> on devices with different computing capabilities to support computer-aided diagnosis. Many methods employ factorized <b>convolutional</b> <b>layers</b> <b>to</b> alleviate the burden of limited computational resources at the expense of expressiveness. To this end, given weak medical image-driven <b>CNN</b> model optimization, a Singular value equalization generalizer-induced Factorized <b>Convolution</b> (SFConv) is proposed to improve the expressive power of factorized <b>convolutions</b> in MIP models. We first decompose the weight matrix of <b>convolutional</b> <b>filters</b> <b>into</b> two low-rank matrices to achieve model reduction. Then minimize the KL divergence between the two low-rank weight matrices and the uniform distribution, thereby reducing the number of singular value directions with significant variance. Extensive experiments on fundus and OCTA datasets demonstrate that our SFConv yields competitive expressiveness over vanilla <b>convolutions</b> while reducing complexity.</p></p class="citation"></blockquote><h3 id=1946--107243-lomoe-localized-multi-object-editing-via-multi-diffusion-goirik-chakrabarty-et-al-2024>(19/46 | 107/243) LoMOE: Localized Multi-Object Editing via Multi-Diffusion (Goirik Chakrabarty et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Goirik Chakrabarty, Aditya Chandrasekar, Ramya Hebbalaguppe, Prathosh AP. (2024)<br><strong>LoMOE: Localized Multi-Object Editing via Multi-Diffusion</strong><br><button class=copy-to-clipboard title="LoMOE: Localized Multi-Object Editing via Multi-Diffusion" index=107>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-107 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-GR, cs-LG, cs.CV<br>Keyword Score: 30<br>Keywords: Diffusion Model, Zero-shot, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00437v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00437v1.pdf filename=2403.00437v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent developments in the field of <b>diffusion</b> <b>models</b> have demonstrated an exceptional capacity to generate high-quality <b>prompt-conditioned</b> image edits. Nevertheless, previous approaches have primarily relied on textual <b>prompts</b> for image editing, which tend to be less effective when making precise edits to specific objects or fine-grained regions within a scene containing single/multiple objects. We introduce a novel framework for <b>zero-shot</b> localized multi-object editing through a multi-diffusion process to overcome this challenge. This framework empowers users to perform various operations on objects within an image, such as adding, replacing, or editing $\textbf{many}$ objects in a complex scene $\textbf{in one pass}$. Our approach leverages foreground masks and corresponding simple text <b>prompts</b> that exert localized influences on the target regions resulting in high-fidelity image editing. A combination of cross-attention and background preservation losses within the latent space ensures that the characteristics of the object being edited are preserved while simultaneously achieving a high-quality, seamless reconstruction of the background with fewer artifacts compared to the current methods. We also curate and release a dataset dedicated to multi-object editing, named $\texttt{LoMOE}$-Bench. Our experiments against existing state-of-the-art methods demonstrate the improved effectiveness of our approach in terms of both image editing quality and inference speed.</p></p class="citation"></blockquote><h3 id=2046--108243-parameter-efficient-tuning-of-large-convolutional-models-wei-chen-et-al-2024>(20/46 | 108/243) Parameter-Efficient Tuning of Large Convolutional Models (Wei Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wei Chen, Zichen Miao, Qiang Qiu. (2024)<br><strong>Parameter-Efficient Tuning of Large Convolutional Models</strong><br><button class=copy-to-clipboard title="Parameter-Efficient Tuning of Large Convolutional Models" index=108>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-108 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 30<br>Keywords: Convolution, Fine-tuning, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00269v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00269v1.pdf filename=2403.00269v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To address the high computational and parameter complexity associated with <b>fine-tuning</b> large pre-trained models, researchers have developed parameter-efficient methods, where only partial parameters are updated for downstream tasks. However, these works often overlook the distinct properties of <b>convolutional</b> kernels, which still remain essential elements in many large models, such as Stable Diffusion. In this study, we first introduce filter subspace by decomposing <b>convolutional</b> kernels within each network layer over a small set of filter subspace elements, referred to as filter atoms. We then <b>fine-tune</b> these models to extract task-specific representation by only adapting the filter atoms, a few hundred parameters typically. To potentially expand the parameter space for tuning, we further show a simple approach to generate an overcomplete filter subspace by recursively decomposing each filter atom over another set of filter atoms. The <b>fine-tuning</b> of filter atoms reshapes the filter subspace, enabling <b>convolutional</b> layers to adapt to diverse downstream tasks efficiently. Extensive experiments show that such a simple scheme surpasses previous tuning baselines for both discriminate and generative tasks. Our approach can potentially be complementary to many existing <b>fine-tuning</b> methods.</p></p class="citation"></blockquote><h3 id=2146--109243-abductive-ego-view-accident-video-understanding-for-safe-driving-perception-jianwu-fang-et-al-2024>(21/46 | 109/243) Abductive Ego-View Accident Video Understanding for Safe Driving Perception (Jianwu Fang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianwu Fang, Lei-lei Li, Junfei Zhou, Junbin Xiao, Hongkai Yu, Chen Lv, Jianru Xue, Tat-Seng Chua. (2024)<br><strong>Abductive Ego-View Accident Video Understanding for Safe Driving Perception</strong><br><button class=copy-to-clipboard title="Abductive Ego-View Accident Video Understanding for Safe Driving Perception" index=109>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-109 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 29<br>Keywords: Diffusion Model, Object Detection, Benchmarking, Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00436v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00436v1.pdf filename=2403.00436v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present MM-AU, a novel dataset for <b>Multi-Modal</b> Accident video Understanding. MM-AU contains 11,727 in-the-wild ego-view accident videos, each with temporally aligned text descriptions. We annotate over 2.23 million <b>object</b> <b>boxes</b> and 58,650 pairs of video-based accident reasons, covering 58 accident categories. MM-AU supports various accident understanding tasks, particularly <b>multimodal</b> video <b>diffusion</b> <b>to</b> understand accident cause-effect chains for safe driving. With MM-AU, we present an Abductive accident Video understanding framework for Safe Driving perception (AdVersa-SD). AdVersa-SD performs video <b>diffusion</b> <b>via</b> an <b>Object-Centric</b> <b>Video</b> <b>Diffusion</b> <b>(OAVD)</b> method which is driven by an abductive CLIP model. This model involves a contrastive interaction loss to learn the pair co-occurrence of normal, near-accident, accident frames with the corresponding text descriptions, such as accident reasons, prevention advice, and accident categories. OAVD enforces the causal region learning while fixing the content of the original frame background in video generation, to find the dominant cause-effect chain for certain accidents. Extensive experiments verify the abductive ability of AdVersa-SD and the superiority of OAVD against the state-of-the-art <b>diffusion</b> <b>models.</b> Additionally, we provide careful <b>benchmark</b> evaluations for <b>object</b> <b>detection</b> and accident reason answering since AdVersa-SD relies on precise <b>object</b> <b>and</b> accident reason information.</p></p class="citation"></blockquote><h3 id=2246--110243-semantics-enhanced-cross-modal-masked-image-modeling-for-vision-language-pre-training-haowei-liu-et-al-2024>(22/46 | 110/243) Semantics-enhanced Cross-modal Masked Image Modeling for Vision-Language Pre-training (Haowei Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haowei Liu, Yaya Shi, Haiyang Xu, Chunfeng Yuan, Qinghao Ye, Chenliang Li, Ming Yan, Ji Zhang, Fei Huang, Bing Li, Weiming Hu. (2024)<br><strong>Semantics-enhanced Cross-modal Masked Image Modeling for Vision-Language Pre-training</strong><br><button class=copy-to-clipboard title="Semantics-enhanced Cross-modal Masked Image Modeling for Vision-Language Pre-training" index=110>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-110 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 25<br>Keywords: Representation Learning, Self-supervised Learning, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00249v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00249v1.pdf filename=2403.00249v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In <b>vision-language</b> pre-training (VLP), masked image modeling (MIM) has recently been introduced for fine-grained cross-modal alignment. However, in most existing methods, the reconstruction targets for MIM lack high-level semantics, and text is not sufficiently involved in masked modeling. These two drawbacks limit the effect of MIM in facilitating cross-modal semantic alignment. In this work, we propose a semantics-enhanced cross-modal MIM framework (SemMIM) for <b>vision-language</b> <b>representation</b> <b>learning.</b> Specifically, to provide more semantically meaningful supervision for MIM, we propose a local semantics enhancing approach, which harvest high-level semantics from global image features via <b>self-supervised</b> agreement learning and transfer them to local patch encodings by sharing the encoding space. Moreover, to achieve deep involvement of text during the entire MIM process, we propose a text-guided masking strategy and devise an efficient way of injecting textual information in both masked modeling and reconstruction target acquisition. Experimental results validate that our method improves the effectiveness of the MIM task in facilitating cross-modal semantic alignment. Compared to previous VLP models with similar model size and data scale, our SemMIM model achieves state-of-the-art or competitive performance on multiple downstream <b>vision-language</b> tasks.</p></p class="citation"></blockquote><h3 id=2346--111243-glfnet-global-local-frequency-filter-networks-for-efficient-medical-image-segmentation-athanasios-tragakis-et-al-2024>(23/46 | 111/243) GLFNET: Global-Local (frequency) Filter Networks for efficient medical image segmentation (Athanasios Tragakis et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Athanasios Tragakis, Qianying Liu, Chaitanya Kaul, Swalpa Kumar Roy, Hang Dai, Fani Deligianni, Roderick Murray-Smith, Daniele Faccio. (2024)<br><strong>GLFNET: Global-Local (frequency) Filter Networks for efficient medical image segmentation</strong><br><button class=copy-to-clipboard title="GLFNET: Global-Local (frequency) Filter Networks for efficient medical image segmentation" index=111>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-111 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Benchmarking, Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00396v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00396v1.pdf filename=2403.00396v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose a novel <b>transformer-style</b> architecture called Global-Local Filter Network (GLFNet) for medical image segmentation and demonstrate its state-of-the-art performance. We replace the <b>self-attention</b> mechanism with a combination of global-local filter blocks to optimize model efficiency. The global filters extract features from the whole feature map whereas the local filters are being adaptively created as 4x4 patches of the same feature map and add restricted scale information. In particular, the feature extraction takes place in the frequency domain rather than the commonly used spatial (image) domain to facilitate faster computations. The fusion of information from both spatial and frequency spaces creates an efficient model with regards to complexity, required data and performance. We test GLFNet on three <b>benchmark</b> datasets achieving state-of-the-art performance on all of them while being almost twice as efficient in terms of GFLOP operations.</p></p class="citation"></blockquote><h3 id=2446--112243-learning-causal-features-for-incremental-object-detection-zhenwei-he-et-al-2024>(24/46 | 112/243) Learning Causal Features for Incremental Object Detection (Zhenwei He et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhenwei He, Lei Zhang. (2024)<br><strong>Learning Causal Features for Incremental Object Detection</strong><br><button class=copy-to-clipboard title="Learning Causal Features for Incremental Object Detection" index=112>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-112 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Object Detection, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00591v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00591v1.pdf filename=2403.00591v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Object</b> <b>detection</b> limits its recognizable categories during the training phase, in which it can not cover all <b>objects</b> <b>of</b> interest for users. To satisfy the practical necessity, the incremental learning ability of the detector becomes a critical factor for real-world applications. Unfortunately, neural networks unavoidably meet catastrophic forgetting problem when it is implemented on a new task. To this end, many incremental <b>object</b> <b>detection</b> models preserve the knowledge of previous tasks by replaying samples or <b>distillation</b> from previous models. However, they ignore an important factor that the performance of the model mostly depends on its feature. These models try to rouse the memory of the neural network with previous samples but not to prevent forgetting. To this end, in this paper, we propose an incremental causal <b>object</b> <b>detection</b> (ICOD) model by learning causal features, which can adapt to more tasks. Traditional <b>object</b> <b>detection</b> models, unavoidably depend on the data-bias or data-specific features to get the detection results, which can not adapt to the new task. When the model meets the requirements of incremental learning, the data-bias information is not beneficial to the new task, and the incremental learning may eliminate these features and lead to forgetting. To this end, our ICOD is introduced to learn the causal features, rather than the data-bias features when training the detector. Thus, when the model is implemented to a new task, the causal features of the old task can aid the incremental learning process to alleviate the catastrophic forgetting problem. We conduct our model on several experiments, which shows a causal feature without data-bias can make the model adapt to new tasks better. \keywords{Object detection, incremental learning, causal feature.</p></p class="citation"></blockquote><h3 id=2546--113243-damsdet-dynamic-adaptive-multispectral-detection-transformer-with-competitive-query-selection-and-adaptive-feature-fusion-junjie-guo-et-al-2024>(25/46 | 113/243) DAMSDet: Dynamic Adaptive Multispectral Detection Transformer with Competitive Query Selection and Adaptive Feature Fusion (Junjie Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junjie Guo, Chenqiang Gao, Fangcen Liu, Deyu Meng, Xinbo Gao. (2024)<br><strong>DAMSDet: Dynamic Adaptive Multispectral Detection Transformer with Competitive Query Selection and Adaptive Feature Fusion</strong><br><button class=copy-to-clipboard title="DAMSDet: Dynamic Adaptive Multispectral Detection Transformer with Competitive Query Selection and Adaptive Feature Fusion" index=113>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-113 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Object Detection, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00326v3 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00326v3.pdf filename=2403.00326v3.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Infrared-visible <b>object</b> <b>detection</b> aims to achieve robust even full-day <b>object</b> <b>detection</b> by fusing the complementary information of infrared and visible images. However, highly dynamically variable complementary characteristics and commonly existing modality misalignment make the fusion of complementary information difficult. In this paper, we propose a Dynamic Adaptive Multispectral Detection <b>Transformer</b> (DAMSDet) to simultaneously address these two challenges. Specifically, we propose a Modality Competitive Query Selection strategy to provide useful prior information. This strategy can dynamically select basic salient modality feature representation for each <b>object.</b> <b>To</b> effectively mine the complementary information and adapt to misalignment situations, we propose a Multispectral Deformable Cross-attention module to adaptively sample and aggregate multi-semantic level features of infrared and visible images for each <b>object.</b> <b>In</b> addition, we further adopt the cascade structure of DETR to better mine complementary information. Experiments on four public datasets of different scenes demonstrate significant improvements compared to other state-of-the-art methods. The code will be released at <a href=https://github.com/gjj45/DAMSDet>https://github.com/gjj45/DAMSDet</a>.</p></p class="citation"></blockquote><h3 id=2646--114243-yolo-med--multi-task-interaction-network-for-biomedical-images-suizhi-huang-et-al-2024>(26/46 | 114/243) YOLO-MED : Multi-Task Interaction Network for Biomedical Images (Suizhi Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Suizhi Huang, Shalayiding Sirejiding, Yuxiang Lu, Yue Ding, Leheng Liu, Hui Zhou, Hongtao Lu. (2024)<br><strong>YOLO-MED : Multi-Task Interaction Network for Biomedical Images</strong><br><button class=copy-to-clipboard title="YOLO-MED : Multi-Task Interaction Network for Biomedical Images" index=114>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-114 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Yolo, Object Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00245v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00245v1.pdf filename=2403.00245v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Object</b> <b>detection</b> and semantic segmentation are pivotal components in biomedical image analysis. Current single-task networks exhibit promising outcomes in both detection and segmentation tasks. Multi-task networks have gained prominence due to their capability to simultaneously tackle segmentation and detection tasks, while also accelerating the segmentation inference. Nevertheless, recent multi-task networks confront distinct limitations such as the difficulty in striking a balance between accuracy and inference speed. Additionally, they often overlook the integration of cross-scale features, which is especially important for biomedical image analysis. In this study, we propose an efficient end-to-end multi-task network capable of concurrently performing <b>object</b> <b>detection</b> and semantic segmentation called <b>YOLO-Med.</b> Our model employs a backbone and a neck for multi-scale feature extraction, complemented by the inclusion of two task-specific decoders. A cross-scale task-interaction module is employed in order to facilitate information fusion between various tasks. Our model exhibits promising results in balancing accuracy and speed when evaluated on the Kvasir-seg dataset and a private biomedical image dataset.</p></p class="citation"></blockquote><h3 id=2746--115243-revisiting-disentanglement-in-downstream-tasks-a-study-on-its-necessity-for-abstract-visual-reasoning-ruiqian-nai-et-al-2024>(27/46 | 115/243) Revisiting Disentanglement in Downstream Tasks: A Study on Its Necessity for Abstract Visual Reasoning (Ruiqian Nai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruiqian Nai, Zixin Wen, Ji Li, Yuanzhi Li, Yang Gao. (2024)<br><strong>Revisiting Disentanglement in Downstream Tasks: A Study on Its Necessity for Abstract Visual Reasoning</strong><br><button class=copy-to-clipboard title="Revisiting Disentanglement in Downstream Tasks: A Study on Its Necessity for Abstract Visual Reasoning" index=115>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-115 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 15<br>Keywords: Representation Learning, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00352v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00352v1.pdf filename=2403.00352v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In <b>representation</b> <b>learning,</b> a disentangled <b>representation</b> <b>is</b> highly desirable as it encodes generative factors of data in a separable and compact pattern. Researchers have advocated leveraging disentangled <b>representations</b> <b>to</b> complete downstream tasks with encouraging empirical evidence. This paper further investigates the necessity of disentangled <b>representation</b> <b>in</b> downstream applications. Specifically, we show that dimension-wise disentangled <b>representations</b> <b>are</b> unnecessary on a fundamental downstream task, abstract visual <b>reasoning.</b> We provide extensive empirical evidence against the necessity of disentanglement, covering multiple datasets, <b>representation</b> <b>learning</b> methods, and downstream network architectures. Furthermore, our findings suggest that the informativeness of <b>representations</b> <b>is</b> a better indicator of downstream performance than disentanglement. Finally, the positive correlation between informativeness and disentanglement explains the claimed usefulness of disentangled <b>representations</b> <b>in</b> previous works. The source code is available at <a href=https://github.com/Richard-coder-Nai/disentanglement-lib-necessity.git>https://github.com/Richard-coder-Nai/disentanglement-lib-necessity.git</a>.</p></p class="citation"></blockquote><h3 id=2846--116243-can-transformers-capture-spatial-relations-between-objects-chuan-wen-et-al-2024>(28/46 | 116/243) Can Transformers Capture Spatial Relations between Objects? (Chuan Wen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chuan Wen, Dinesh Jayaraman, Yang Gao. (2024)<br><strong>Can Transformers Capture Spatial Relations between Objects?</strong><br><button class=copy-to-clipboard title="Can Transformers Capture Spatial Relations between Objects?" index=116>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-116 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00729v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00729v1.pdf filename=2403.00729v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Spatial relationships between objects represent key scene information for humans to understand and interact with the world. To study the capability of current computer vision systems to recognize physically grounded spatial relations, we start by proposing precise relation definitions that permit consistently annotating a <b>benchmark</b> dataset. Despite the apparent simplicity of this task relative to others in the recognition literature, we observe that existing approaches perform poorly on this <b>benchmark.</b> We propose new approaches exploiting the long-range attention capabilities of <b>transformers</b> for this task, and evaluating key design principles. We identify a simple &ldquo;RelatiViT&rdquo; architecture and demonstrate that it outperforms all current approaches. To our knowledge, this is the first method to convincingly outperform naive baselines on spatial relation prediction in in-the-wild settings. The code and datasets are available in \url{https://sites.google.com/view/spatial-relation}.</p></p class="citation"></blockquote><h3 id=2946--117243-rethinking-few-shot-3d-point-cloud-semantic-segmentation-zhaochong-an-et-al-2024>(29/46 | 117/243) Rethinking Few-shot 3D Point Cloud Semantic Segmentation (Zhaochong An et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhaochong An, Guolei Sun, Yun Liu, Fayao Liu, Zongwei Wu, Dan Wang, Luc Van Gool, Serge Belongie. (2024)<br><strong>Rethinking Few-shot 3D Point Cloud Semantic Segmentation</strong><br><button class=copy-to-clipboard title="Rethinking Few-shot 3D Point Cloud Semantic Segmentation" index=117>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-117 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Few-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00592v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00592v1.pdf filename=2403.00592v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper revisits <b>few-shot</b> 3D point cloud semantic segmentation (FS-PCS), with a focus on two significant issues in the state-of-the-art: foreground leakage and sparse point distribution. The former arises from non-uniform point sampling, allowing models to distinguish the density disparities between foreground and background for easier segmentation. The latter results from sampling only 2,048 points, limiting semantic information and deviating from the real-world practice. To address these issues, we introduce a standardized FS-PCS setting, upon which a new <b>benchmark</b> is built. Moreover, we propose a novel FS-PCS model. While previous methods are based on feature optimization by mainly refining support features to enhance prototypes, our method is based on correlation optimization, referred to as Correlation Optimization Segmentation (COSeg). Specifically, we compute Class-specific Multi-prototypical Correlation (CMC) for each query point, representing its correlations to category prototypes. Then, we propose the Hyper Correlation Augmentation (HCA) module to enhance CMC. Furthermore, tackling the inherent property of <b>few-shot</b> training to incur base susceptibility for models, we propose to learn non-parametric prototypes for the base classes during training. The learned base prototypes are used to calibrate correlations for the background class through a Base Prototypes Calibration (BPC) module. Experiments on popular datasets demonstrate the superiority of COSeg over existing methods. The code is available at: <a href=https://github.com/ZhaochongAn/COSeg>https://github.com/ZhaochongAn/COSeg</a></p></p class="citation"></blockquote><h3 id=3046--118243-rethinking-cluster-conditioned-diffusion-models-nikolas-adaloglou-et-al-2024>(30/46 | 118/243) Rethinking cluster-conditioned diffusion models (Nikolas Adaloglou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nikolas Adaloglou, Tim Kaiser, Felix Michels, Markus Kollmann. (2024)<br><strong>Rethinking cluster-conditioned diffusion models</strong><br><button class=copy-to-clipboard title="Rethinking cluster-conditioned diffusion models" index=118>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-118 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 13<br>Keywords: Diffusion Model, Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00570v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00570v1.pdf filename=2403.00570v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a comprehensive experimental study on image-level conditioning for <b>diffusion</b> <b>models</b> using cluster assignments. We elucidate how individual components regarding image <b>clustering</b> impact image synthesis across three datasets. By combining recent advancements from image <b>clustering</b> and <b>diffusion</b> <b>models,</b> we show that, given the optimal cluster granularity with respect to image synthesis (visual groups), cluster-conditioning can achieve state-of-the-art FID (i.e. 1.67, 2.17 on CIFAR10 and CIFAR100 respectively), while attaining a strong training sample efficiency. Finally, we propose a novel method to derive an upper cluster bound that reduces the search space of the visual groups using solely feature-based <b>clustering.</b> Unlike existing approaches, we find no significant connection between <b>clustering</b> and cluster-conditional image generation. The code and cluster assignments will be released.</p></p class="citation"></blockquote><h3 id=3146--119243-multi-task-learning-using-uncertainty-to-weigh-losses-for-heterogeneous-face-attribute-estimation-huaqing-yuan-et-al-2024>(31/46 | 119/243) Multi-Task Learning Using Uncertainty to Weigh Losses for Heterogeneous Face Attribute Estimation (Huaqing Yuan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Huaqing Yuan, Yi He, Peng Du, Lu Song. (2024)<br><strong>Multi-Task Learning Using Uncertainty to Weigh Losses for Heterogeneous Face Attribute Estimation</strong><br><button class=copy-to-clipboard title="Multi-Task Learning Using Uncertainty to Weigh Losses for Heterogeneous Face Attribute Estimation" index=119>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-119 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Parameter Sharing<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00561v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00561v1.pdf filename=2403.00561v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Face images contain a wide variety of attribute information. In this paper, we propose a generalized framework for joint estimation of ordinal and nominal attributes based on information sharing. We tackle the correlation problem between heterogeneous attributes using hard <b>parameter</b> <b>sharing</b> of shallow features, and trade-off multiple loss functions by considering homoskedastic uncertainty for each attribute estimation task. This leads to optimal estimation of multiple attributes of the face and reduces the training cost of multitask learning. Experimental results on <b>benchmarks</b> with multiple face attributes show that the proposed approach has superior performance compared to state of the art. Finally, we discuss the bias issues arising from the proposed approach in face attribute estimation and validate its feasibility on edge systems.</p></p class="citation"></blockquote><h3 id=3246--120243-improving-acne-image-grading-with-label-distribution-smoothing-kirill-prokhorov-et-al-2024>(32/46 | 120/243) Improving Acne Image Grading with Label Distribution Smoothing (Kirill Prokhorov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kirill Prokhorov, Alexandr A. Kalinin. (2024)<br><strong>Improving Acne Image Grading with Label Distribution Smoothing</strong><br><button class=copy-to-clipboard title="Improving Acne Image Grading with Label Distribution Smoothing" index=120>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-120 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Label Smoothing<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00268v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00268v1.pdf filename=2403.00268v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Acne, a prevalent skin condition, necessitates precise severity assessment for effective treatment. Acne severity grading typically involves lesion counting and global assessment. However, manual grading suffers from variability and inefficiency, highlighting the need for automated tools. Recently, <b>label</b> <b>distribution</b> learning (LDL) was proposed as an effective framework for acne image grading, but its effectiveness is hindered by severity scales that assign varying numbers of lesions to different severity grades. Addressing these limitations, we proposed to incorporate severity scale information into lesion counting by combining LDL with <b>label</b> <b>smoothing,</b> and to decouple if from global assessment. A novel weighting scheme in our approach adjusts the degree of <b>label</b> <b>smoothing</b> based on the severity grading scale. This method helped to effectively manage <b>label</b> <b>uncertainty</b> without compromising class distinctiveness. Applied to the <b>benchmark</b> ACNE04 dataset, our model demonstrated improved performance in automated acne grading, showcasing its potential in enhancing acne diagnostics. The source code is publicly available at <a href=http://github.com/openface-io/acne-lds>http://github.com/openface-io/acne-lds</a>.</p></p class="citation"></blockquote><h3 id=3346--121243-spatial-cascaded-clustering-and-weighted-memory-for-unsupervised-person-re-identification-jiahao-hong-et-al-2024>(33/46 | 121/243) Spatial Cascaded Clustering and Weighted Memory for Unsupervised Person Re-identification (Jiahao Hong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiahao Hong, Jialong Zuo, Chuchu Han, Ruochen Zheng, Ming Tian, Changxin Gao, Nong Sang. (2024)<br><strong>Spatial Cascaded Clustering and Weighted Memory for Unsupervised Person Re-identification</strong><br><button class=copy-to-clipboard title="Spatial Cascaded Clustering and Weighted Memory for Unsupervised Person Re-identification" index=121>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-121 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Clustering, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00261v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00261v1.pdf filename=2403.00261v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent <b>unsupervised</b> person re-identification (re-ID) methods achieve high performance by leveraging fine-grained local context. These methods are referred to as part-based methods. However, most part-based methods obtain local contexts through horizontal division, which suffer from misalignment due to various human poses. Additionally, the misalignment of semantic information in part features restricts the use of metric learning, thus affecting the effectiveness of part-based methods. The two issues mentioned above result in the under-utilization of part features in part-based methods. We introduce the Spatial Cascaded <b>Clustering</b> and Weighted Memory (SCWM) method to address these challenges. SCWM aims to parse and align more accurate local contexts for different human body parts while allowing the memory module to balance hard example mining and noise suppression. Specifically, we first analyze the foreground omissions and spatial confusions issues in the previous method. Then, we propose foreground and space corrections to enhance the completeness and reasonableness of the human parsing results. Next, we introduce a weighted memory and utilize two weighting strategies. These strategies address hard sample mining for global features and enhance noise resistance for part features, which enables better utilization of both global and part features. Extensive experiments on Market-1501 and MSMT17 validate the proposed method&rsquo;s effectiveness over many state-of-the-art methods.</p></p class="citation"></blockquote><h3 id=3446--122243-point-could-mamba-point-cloud-learning-via-state-space-model-tao-zhang-et-al-2024>(34/46 | 122/243) Point Could Mamba: Point Cloud Learning via State Space Model (Tao Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tao Zhang, Xiangtai Li, Haobo Yuan, Shunping Ji, Shuicheng Yan. (2024)<br><strong>Point Could Mamba: Point Cloud Learning via State Space Model</strong><br><button class=copy-to-clipboard title="Point Could Mamba: Point Cloud Learning via State Space Model" index=122>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-122 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00762v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00762v1.pdf filename=2403.00762v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, for the first time, we demonstrate that Mamba-based point cloud methods can outperform point-based methods. Mamba exhibits strong global modeling capabilities and linear computational complexity, making it highly attractive for point cloud analysis. To enable more effective processing of 3-D point cloud data by Mamba, we propose a novel Consistent Traverse Serialization to convert point clouds into 1-D point sequences while ensuring that neighboring points in the sequence are also spatially adjacent. Consistent Traverse Serialization yields six variants by permuting the order of x, y, and z coordinates, and the synergistic use of these variants aids Mamba in comprehensively observing point cloud data. Furthermore, to assist Mamba in handling point sequences with different orders more effectively, we introduce point <b>prompts</b> to inform Mamba of the sequence&rsquo;s arrangement rules. Finally, we propose positional encoding based on spatial coordinate mapping to inject positional information into point cloud sequences better. Based on these improvements, we construct a point cloud network named Point Cloud Mamba, which combines local and global modeling. Point Cloud Mamba surpasses the SOTA point-based method PointNeXt and achieves new SOTA performance on the ScanObjectNN, ModelNet40, and ShapeNetPart datasets.</p></p class="citation"></blockquote><h3 id=3546--123243-tri-modal-motion-retrieval-by-learning-a-joint-embedding-space-kangning-yin-et-al-2024>(35/46 | 123/243) Tri-Modal Motion Retrieval by Learning a Joint Embedding Space (Kangning Yin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kangning Yin, Shihao Zou, Yuxuan Ge, Zheng Tian. (2024)<br><strong>Tri-Modal Motion Retrieval by Learning a Joint Embedding Space</strong><br><button class=copy-to-clipboard title="Tri-Modal Motion Retrieval by Learning a Joint Embedding Space" index=123>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-123 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Information Retrieval<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00691v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00691v1.pdf filename=2403.00691v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Information</b> <b>retrieval</b> is an ever-evolving and crucial research domain. The substantial demand for high-quality human motion data especially in online acquirement has led to a surge in human motion research works. Prior works have mainly concentrated on dual-modality learning, such as text and motion tasks, but three-modality learning has been rarely explored. Intuitively, an extra introduced modality can enrich a model&rsquo;s application scenario, and more importantly, an adequate choice of the extra modality can also act as an intermediary and enhance the alignment between the other two disparate modalities. In this work, we introduce LAVIMO (LAnguage-VIdeo-MOtion alignment), a novel framework for three-modality learning integrating human-centric videos as an additional modality, thereby effectively bridging the gap between text and motion. Moreover, our approach leverages a specially designed attention mechanism to foster enhanced alignment and synergistic effects among text, video, and motion modalities. Empirically, our results on the HumanML3D and KIT-ML datasets show that LAVIMO achieves state-of-the-art performance in various motion-related cross-modal retrieval tasks, including text-to-motion, motion-to-text, video-to-motion and motion-to-video.</p></p class="citation"></blockquote><h3 id=3646--124243-diff-plugin-revitalizing-details-for-diffusion-based-low-level-tasks-yuhao-liu-et-al-2024>(36/46 | 124/243) Diff-Plugin: Revitalizing Details for Diffusion-based Low-level Tasks (Yuhao Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuhao Liu, Fang Liu, Zhanghan Ke, Nanxuan Zhao, Rynson W. H. Lau. (2024)<br><strong>Diff-Plugin: Revitalizing Details for Diffusion-based Low-level Tasks</strong><br><button class=copy-to-clipboard title="Diff-Plugin: Revitalizing Details for Diffusion-based Low-level Tasks" index=124>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-124 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00644v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00644v1.pdf filename=2403.00644v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Diffusion</b> <b>models</b> trained on large-scale datasets have achieved remarkable progress in image synthesis. However, due to the randomness in the <b>diffusion</b> <b>process,</b> they often struggle with handling diverse low-level tasks that require details preservation. To overcome this limitation, we present a new Diff-Plugin framework to enable a single pre-trained <b>diffusion</b> <b>model</b> to generate high-fidelity results across a variety of low-level tasks. Specifically, we first propose a lightweight Task-Plugin module with a dual branch design to provide task-specific priors, guiding the <b>diffusion</b> <b>process</b> in preserving image content. We then propose a Plugin-Selector that can automatically select different Task-Plugins based on the text instruction, allowing users to edit images by indicating multiple low-level tasks with natural language. We conduct extensive experiments on 8 low-level vision tasks. The results demonstrate the superiority of Diff-Plugin over existing methods, particularly in real-world scenarios. Our ablations further validate that Diff-Plugin is stable, schedulable, and supports robust training across different dataset sizes.</p></p class="citation"></blockquote><h3 id=3746--125243-realcustom-narrowing-real-text-word-for-real-time-open-domain-text-to-image-customization-mengqi-huang-et-al-2024>(37/46 | 125/243) RealCustom: Narrowing Real Text Word for Real-Time Open-Domain Text-to-Image Customization (Mengqi Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mengqi Huang, Zhendong Mao, Mingcong Liu, Qian He, Yongdong Zhang. (2024)<br><strong>RealCustom: Narrowing Real Text Word for Real-Time Open-Domain Text-to-Image Customization</strong><br><button class=copy-to-clipboard title="RealCustom: Narrowing Real Text Word for Real-Time Open-Domain Text-to-Image Customization" index=125>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-125 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00483v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00483v1.pdf filename=2403.00483v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Text-to-image</b> customization, which aims to synthesize text-driven images for the given subjects, has recently revolutionized content creation. Existing works follow the pseudo-word paradigm, i.e., represent the given subjects as pseudo-words and then compose them with the given text. However, the inherent entangled influence scope of pseudo-words with the given text results in a dual-optimum paradox, i.e., the similarity of the given subjects and the controllability of the given text could not be optimal simultaneously. We present RealCustom that, for the first time, disentangles similarity from controllability by precisely limiting subject influence to relevant parts only, achieved by gradually narrowing real text word from its general connotation to the specific subject and using its cross-attention to distinguish relevance. Specifically, RealCustom introduces a novel &ldquo;train-inference&rdquo; decoupled framework: (1) during training, RealCustom learns general alignment between visual conditions to original textual conditions by a novel adaptive scoring module to adaptively modulate influence quantity; (2) during inference, a novel adaptive mask guidance strategy is proposed to iteratively update the influence scope and influence quantity of the given subjects to gradually narrow the generation of the real text word. Comprehensive experiments demonstrate the superior real-time customization ability of RealCustom in the open domain, achieving both unprecedented similarity of the given subjects and controllability of the given text for the first time. The project page is <a href=https://corleone-huang.github.io/realcustom/>https://corleone-huang.github.io/realcustom/</a>.</p></p class="citation"></blockquote><h3 id=3846--126243-when-controlnet-meets-inexplicit-masks-a-case-study-of-controlnet-on-its-contour-following-ability-wenjie-xuan-et-al-2024>(38/46 | 126/243) When ControlNet Meets Inexplicit Masks: A Case Study of ControlNet on its Contour-following Ability (Wenjie Xuan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenjie Xuan, Yufei Xu, Shanshan Zhao, Chaoyue Wang, Juhua Liu, Bo Du, Dacheng Tao. (2024)<br><strong>When ControlNet Meets Inexplicit Masks: A Case Study of ControlNet on its Contour-following Ability</strong><br><button class=copy-to-clipboard title="When ControlNet Meets Inexplicit Masks: A Case Study of ControlNet on its Contour-following Ability" index=126>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-126 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: ControlNet<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00467v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00467v1.pdf filename=2403.00467v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>ControlNet</b> excels at creating content that closely matches precise contours in user-provided masks. However, when these masks contain noise, as a frequent occurrence with non-expert users, the output would include unwanted artifacts. This paper first highlights the crucial role of controlling the impact of these inexplicit masks with diverse deterioration levels through in-depth analysis. Subsequently, to enhance controllability with inexplicit masks, an advanced Shape-aware <b>ControlNet</b> consisting of a deterioration estimator and a shape-prior modulation block is devised. The deterioration estimator assesses the deterioration factor of the provided masks. Then this factor is utilized in the modulation block to adaptively modulate the model&rsquo;s contour-following ability, which helps it dismiss the noise part in the inexplicit masks. Extensive experiments prove its effectiveness in encouraging <b>ControlNet</b> to interpret inaccurate spatial conditions robustly rather than blindly following the given contours. We showcase application scenarios like modifying shape priors and composable shape-controllable generation. Codes are soon available.</p></p class="citation"></blockquote><h3 id=3946--127243-an-ordinal-diffusion-model-for-generating-medical-images-with-different-severity-levels-shumpei-takezaki-et-al-2024>(39/46 | 127/243) An Ordinal Diffusion Model for Generating Medical Images with Different Severity Levels (Shumpei Takezaki et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shumpei Takezaki, Seiichi Uchida. (2024)<br><strong>An Ordinal Diffusion Model for Generating Medical Images with Different Severity Levels</strong><br><button class=copy-to-clipboard title="An Ordinal Diffusion Model for Generating Medical Images with Different Severity Levels" index=127>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-127 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00452v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00452v1.pdf filename=2403.00452v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Diffusion</b> <b>models</b> have recently been used for medical image generation because of their high image quality. In this study, we focus on generating medical images with ordinal classes, which have ordinal relationships, such as severity levels. We propose an Ordinal <b>Diffusion</b> <b>Model</b> (ODM) that controls the ordinal relationships of the estimated noise images among the classes. Our model was evaluated experimentally by generating retinal and endoscopic images of multiple severity classes. ODM achieved higher performance than conventional generative models by generating realistic images, especially in high-severity classes with fewer training samples.</p></p class="citation"></blockquote><h3 id=4046--128243-small-versatile-and-mighty-a-range-view-perception-framework-qiang-meng-et-al-2024>(40/46 | 128/243) Small, Versatile and Mighty: A Range-View Perception Framework (Qiang Meng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qiang Meng, Xiao Wang, JiaBao Wang, Liujiang Yan, Ke Wang. (2024)<br><strong>Small, Versatile and Mighty: A Range-View Perception Framework</strong><br><button class=copy-to-clipboard title="Small, Versatile and Mighty: A Range-View Perception Framework" index=128>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-128 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00325v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00325v1.pdf filename=2403.00325v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite its compactness and information integrity, the range view representation of LiDAR data rarely occurs as the first choice for 3D perception tasks. In this work, we further push the envelop of the range-view representation with a novel multi-task framework, achieving unprecedented 3D detection performances. Our proposed Small, Versatile, and Mighty (SVM) network utilizes a pure <b>convolutional</b> architecture to fully unleash the efficiency and multi-tasking potentials of the range view representation. To boost detection performances, we first propose a range-view specific Perspective Centric Label Assignment (PCLA) strategy, and a novel View Adaptive Regression (VAR) module to further refine hard-to-predict box properties. In addition, our framework seamlessly integrates semantic segmentation and panoptic segmentation tasks for the LiDAR point cloud, without extra modules. Among range-view-based methods, our model achieves new state-of-the-art detection performances on the Waymo Open Dataset. Especially, over 10 mAP improvement over <b>convolutional</b> counterparts can be obtained on the vehicle class. Our presented results for other tasks further reveal the multi-task capabilities of the proposed small but mighty framework.</p></p class="citation"></blockquote><h3 id=4146--129243-trustworthy-self-attention-enabling-the-network-to-focus-only-on-the-most-relevant-references-yu-jing-et-al-2024>(41/46 | 129/243) Trustworthy Self-Attention: Enabling the Network to Focus Only on the Most Relevant References (Yu Jing et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yu Jing, Tan Yujuan, Ren Ao, Liu Duo. (2024)<br><strong>Trustworthy Self-Attention: Enabling the Network to Focus Only on the Most Relevant References</strong><br><button class=copy-to-clipboard title="Trustworthy Self-Attention: Enabling the Network to Focus Only on the Most Relevant References" index=129>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-129 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00211v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00211v1.pdf filename=2403.00211v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The prediction of optical flow for occluded points is still a difficult problem that has not yet been solved. Recent methods use <b>self-attention</b> to find relevant non-occluded points as references for estimating the optical flow of occluded points based on the assumption of self-similarity. However, they rely on visual features of a single image and weak constraints, which are not sufficient to constrain the trained network to focus on erroneous and weakly relevant reference points. We make full use of online occlusion recognition information to construct occlusion extended visual features and two strong constraints, allowing the network to learn to focus only on the most relevant references without requiring occlusion ground truth to participate in the training of the network. Our method adds very few network parameters to the original framework, making it very lightweight. Extensive experiments show that our model has the greatest cross-dataset generalization. Our method achieves much greater error reduction, 18.6%, 16.2%, and 20.1% for all points, non-occluded points, and occluded points respectively from the state-of-the-art GMA-base method, MATCHFlow(GMA), on Sintel Albedo pass. Furthermore, our model achieves state-of-the-art performance on the Sintel bench-marks, ranking #1 among all published methods on Sintel clean pass. The code will be open-source.</p></p class="citation"></blockquote><h3 id=4246--130243-chartreformer-natural-language-driven-chart-image-editing-pengyu-yan-et-al-2024>(42/46 | 130/243) ChartReformer: Natural Language-Driven Chart Image Editing (Pengyu Yan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pengyu Yan, Mahesh Bhosale, Jay Lal, Bikhyat Adhikari, David Doermann. (2024)<br><strong>ChartReformer: Natural Language-Driven Chart Image Editing</strong><br><button class=copy-to-clipboard title="ChartReformer: Natural Language-Driven Chart Image Editing" index=130>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-130 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00209v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00209v1.pdf filename=2403.00209v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Chart visualizations are essential for data interpretation and communication; however, most charts are only accessible in image format and lack the corresponding data tables and supplementary information, making it difficult to alter their appearance for different application scenarios. To eliminate the need for original underlying data and information to perform chart editing, we propose ChartReformer, a natural language-driven chart image editing solution that directly edits the charts from the input images with the given instruction <b>prompts.</b> The key in this method is that we allow the model to comprehend the chart and reason over the <b>prompt</b> to generate the corresponding underlying data table and visual attributes for new charts, enabling precise edits. Additionally, to generalize ChartReformer, we define and standardize various types of chart editing, covering style, layout, format, and data-centric edits. The experiments show promising results for the natural language-driven chart image editing.</p></p class="citation"></blockquote><h3 id=4346--131243-g3dr-generative-3d-reconstruction-in-imagenet-pradyumna-reddy-et-al-2024>(43/46 | 131/243) G3DR: Generative 3D Reconstruction in ImageNet (Pradyumna Reddy et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pradyumna Reddy, Ismail Elezi, Jiankang Deng. (2024)<br><strong>G3DR: Generative 3D Reconstruction in ImageNet</strong><br><button class=copy-to-clipboard title="G3DR: Generative 3D Reconstruction in ImageNet" index=131>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-131 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-GR, cs.CV<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00939v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00939v1.pdf filename=2403.00939v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce a novel 3D generative method, Generative 3D Reconstruction (G3DR) in ImageNet, capable of generating diverse and high-quality 3D objects from single images, addressing the limitations of existing methods. At the heart of our framework is a novel depth regularization technique that enables the generation of scenes with high-geometric fidelity. G3DR also leverages a pretrained language-vision model, such as CLIP, to enable reconstruction in novel views and improve the visual realism of generations. Additionally, G3DR designs a simple but effective sampling procedure to further improve the quality of generations. G3DR offers diverse and efficient 3D asset generation based on class or text conditioning. Despite its simplicity, G3DR is able to beat state-of-theart methods, improving over them by up to 22% in perceptual metrics and 90% in <b>geometry</b> scores, while needing only half of the training time. Code is available at <a href=https://github.com/preddy5/G3DR>https://github.com/preddy5/G3DR</a></p></p class="citation"></blockquote><h3 id=4446--132243-deep-learning-computed-tomography-based-on-the-defrise-and-clack-algorithm-chengze-ye-et-al-2024>(44/46 | 132/243) Deep Learning Computed Tomography based on the Defrise and Clack Algorithm (Chengze Ye et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chengze Ye, Linda-Sophie Schneider, Yipeng Sun, Andreas Maier. (2024)<br><strong>Deep Learning Computed Tomography based on the Defrise and Clack Algorithm</strong><br><button class=copy-to-clipboard title="Deep Learning Computed Tomography based on the Defrise and Clack Algorithm" index=132>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-132 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00426v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00426v1.pdf filename=2403.00426v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study presents a novel approach for reconstructing cone beam computed tomography (CBCT) for specific orbits using known operator learning. Unlike traditional methods, this technique employs a filtered backprojection type (FBP-type) algorithm, which integrates a unique, adaptive filtering process. This process involves a series of operations, including weightings, differentiations, the 2D Radon transform, and backprojection. The filter is designed for a specific orbit <b>geometry</b> and is obtained using a data-driven approach based on deep learning. The approach efficiently learns and optimizes the orbit-related component of the filter. The method has demonstrated its ability through experimentation by successfully learning parameters from circular orbit projection data. Subsequently, the optimized parameters are used to reconstruct images, resulting in outcomes that closely resemble the analytical solution. This demonstrates the potential of the method to learn appropriate parameters from any specific orbit projection data and achieve reconstruction. The algorithm has demonstrated improvement, particularly in enhancing reconstruction speed and reducing memory usage for handling specific orbit reconstruction.</p></p class="citation"></blockquote><h3 id=4546--133243-rethinking-classifier-re-training-in-long-tailed-recognition-a-simple-logits-retargeting-approach-han-lu-et-al-2024>(45/46 | 133/243) Rethinking Classifier Re-Training in Long-Tailed Recognition: A Simple Logits Retargeting Approach (Han Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Han Lu, Siyu Sun, Yichen Xie, Liqing Zhang, Xiaokang Yang, Junchi Yan. (2024)<br><strong>Rethinking Classifier Re-Training in Long-Tailed Recognition: A Simple Logits Retargeting Approach</strong><br><button class=copy-to-clipboard title="Rethinking Classifier Re-Training in Long-Tailed Recognition: A Simple Logits Retargeting Approach" index=133>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-133 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 5<br>Keywords: Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00250v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00250v1.pdf filename=2403.00250v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the long-tailed recognition field, the Decoupled Training paradigm has demonstrated remarkable capabilities among various methods. This paradigm decouples the training process into separate <b>representation</b> <b>learning</b> and classifier re-training. Previous works have attempted to improve both stages simultaneously, making it difficult to isolate the effect of classifier re-training. Furthermore, recent empirical studies have demonstrated that simple regularization can yield strong feature <b>representations,</b> <b>emphasizing</b> the need to reassess existing classifier re-training methods. In this study, we revisit classifier re-training methods based on a unified feature <b>representation</b> <b>and</b> re-evaluate their performances. We propose a new metric called Logits Magnitude as a superior measure of model performance, replacing the commonly used Weight Norm. However, since it is hard to directly optimize the new metric during training, we introduce a suitable approximate invariant called Regularized Standard Deviation. Based on the two newly proposed metrics, we prove that reducing the absolute value of Logits Magnitude when it is nearly balanced can effectively decrease errors and disturbances during training, leading to better model performance. Motivated by these findings, we develop a simple logits retargeting approach (LORT) without the requirement of prior knowledge of the number of samples per class. LORT divides the original one-hot label into small true label probabilities and large negative label probabilities distributed across each class. Our method achieves state-of-the-art performance on various imbalanced datasets, including CIFAR100-LT, ImageNet-LT, and iNaturalist2018.</p></p class="citation"></blockquote><h3 id=4646--134243-sure-survey-recipes-for-building-reliable-and-robust-deep-networks-yuting-li-et-al-2024>(46/46 | 134/243) SURE: SUrvey REcipes for building reliable and robust deep networks (Yuting Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuting Li, Yingyi Chen, Xuanlong Yu, Dexiong Chen, Xi Shen. (2024)<br><strong>SURE: SUrvey REcipes for building reliable and robust deep networks</strong><br><button class=copy-to-clipboard title="SURE: SUrvey REcipes for building reliable and robust deep networks" index=134>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-134 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00543v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00543v1.pdf filename=2403.00543v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we revisit techniques for uncertainty estimation within deep neural networks and consolidate a suite of techniques to enhance their reliability. Our investigation reveals that an integrated application of diverse techniques&ndash;spanning model regularization, classifier and optimization&ndash;substantially improves the accuracy of uncertainty predictions in image classification tasks. The synergistic effect of these techniques culminates in our novel SURE approach. We rigorously evaluate SURE against the <b>benchmark</b> of failure prediction, a critical testbed for uncertainty estimation efficacy. Our results showcase a consistently better performance than models that individually deploy each technique, across various datasets and model architectures. When applied to real-world challenges, such as data corruption, label noise, and long-tailed class distribution, SURE exhibits remarkable robustness, delivering results that are superior or on par with current state-of-the-art specialized methods. Particularly on Animal-10N and Food-101N for learning with noisy labels, SURE achieves state-of-the-art performance without any task-specific adjustments. This work not only sets a new <b>benchmark</b> for robust uncertainty estimation but also paves the way for its application in diverse, real-world scenarios where reliability is paramount. Our code is available at \url{https://yutingli0606.github.io/SURE/}.</p></p class="citation"></blockquote><h2 id=csse-4>cs.SE (4)</h2><h3 id=14--135243-a-systematic-evaluation-of-large-language-models-for-generating-programming-code-wenpin-hou-et-al-2024>(1/4 | 135/243) A systematic evaluation of large language models for generating programming code (Wenpin Hou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenpin Hou, Zhicheng Ji. (2024)<br><strong>A systematic evaluation of large language models for generating programming code</strong><br><button class=copy-to-clipboard title="A systematic evaluation of large language models for generating programming code" index=135>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-135 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-CL, cs-PL, cs-SE, cs.SE<br>Keyword Score: 70<br>Keywords: Claude, GPT, GPT-4, Gemini, Code Generation, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00894v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00894v1.pdf filename=2403.00894v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We systematically evaluated the performance of seven <b>large</b> <b>language</b> <b>models</b> in generating programming <b>code</b> <b>using</b> various <b>prompt</b> strategies, programming languages, and task difficulties. <b>GPT-4</b> substantially outperforms other <b>large</b> <b>language</b> <b>models,</b> including <b>Gemini</b> Ultra and <b>Claude</b> 2. The coding performance of <b>GPT-4</b> varies considerably with different <b>prompt</b> strategies. In most LeetCode and GeeksforGeeks coding contests evaluated in this study, <b>GPT-4</b> employing the optimal <b>prompt</b> strategy outperforms 85 percent of human participants. Additionally, <b>GPT-4</b> demonstrates strong capabilities in translating <b>code</b> <b>between</b> different programming languages and in learning from past errors. The computational efficiency of the <b>code</b> <b>generated</b> by <b>GPT-4</b> is comparable to that of human programmers. These results suggest that <b>GPT-4</b> has the potential to serve as a reliable assistant in programming <b>code</b> <b>generation</b> and software development.</p></p class="citation"></blockquote><h3 id=24--136243-when-large-language-models-confront-repository-level-automatic-program-repair-how-well-they-done-yuxiao-chen-et-al-2024>(2/4 | 136/243) When Large Language Models Confront Repository-Level Automatic Program Repair: How Well They Done? (Yuxiao Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuxiao Chen, Jingzheng Wu, Xiang Ling, Changjiang Li, Zhiqing Rui, Tianyue Luo, Yanjun Wu. (2024)<br><strong>When Large Language Models Confront Repository-Level Automatic Program Repair: How Well They Done?</strong><br><button class=copy-to-clipboard title="When Large Language Models Confront Repository-Level Automatic Program Repair: How Well They Done?" index=136>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-136 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 53<br>Keywords: Benchmarking, GPT-3, GPT-3.5, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00448v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00448v1.pdf filename=2403.00448v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have demonstrated substantial potential in addressing automatic program repair (APR) tasks. However, the current evaluation of these models for APR tasks focuses solely on the limited context of the single function or file where the bug is located, overlooking the valuable information in the repository-level context. This paper investigates the performance of popular <b>LLMs</b> in handling repository-level repair tasks. We introduce RepoBugs, a new <b>benchmark</b> comprising 124 typical repository-level bugs from open-source repositories. Preliminary experiments using <b>GPT3.5</b> based on the function where the error is located, reveal that the repair rate on RepoBugs is only 22.58%, significantly diverging from the performance of <b>GPT3.5</b> on function-level bugs in related studies. This underscores the importance of providing repository-level context when addressing bugs at this level. However, the repository-level context offered by the preliminary method often proves redundant and imprecise and easily exceeds the <b>prompt</b> length limit of <b>LLMs.</b> To solve the problem, we propose a simple and universal repository-level context extraction method (RLCE) designed to provide more precise context for repository-level code repair tasks. Evaluations of three mainstream <b>LLMs</b> show that RLCE significantly enhances the ability to repair repository-level bugs. The improvement reaches a maximum of 160% compared to the preliminary method. Additionally, we conduct a comprehensive analysis of the effectiveness and limitations of RLCE, along with the capacity of <b>LLMs</b> to address repository-level bugs, offering valuable insights for future research.</p></p class="citation"></blockquote><h3 id=34--137243-a-survey-on-self-healing-software-system-zahra-yazdanparast-2024>(3/4 | 137/243) A Survey on Self-healing Software System (Zahra Yazdanparast, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zahra Yazdanparast. (2024)<br><strong>A Survey on Self-healing Software System</strong><br><button class=copy-to-clipboard title="A Survey on Self-healing Software System" index=137>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-137 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 10<br>Keywords: Human Intervention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00455v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00455v1.pdf filename=2403.00455v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the increasing complexity of software systems, it becomes very difficult to install, configure, adjust, and maintain them. As systems become more interconnected and diverse, system architects are less able to predict and design the interaction between components, deferring the handling of these issues to runtime. One of the important problems that occur during execution is system failures, which increase the need for self-healing systems. The main purpose of self-healing is to have an automatic system that can heal itself without <b>human</b> <b>intervention.</b> This system has predefined actions and procedures that are suitable for recovering the system from different failure modes. In this study, different self-healing methods are categorized and a summary of them is presented.</p></p class="citation"></blockquote><h3 id=44--138243-dypybench-a-benchmark-of-executable-python-software-islem-bouzenia-et-al-2024>(4/4 | 138/243) DyPyBench: A Benchmark of Executable Python Software (Islem Bouzenia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Islem Bouzenia, Bajaj Piyush Krishan, Michael Pradel. (2024)<br><strong>DyPyBench: A Benchmark of Executable Python Software</strong><br><button class=copy-to-clipboard title="DyPyBench: A Benchmark of Executable Python Software" index=138>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-138 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 6<br>Keywords: Graph, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00539v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00539v1.pdf filename=2403.00539v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Python has emerged as one of the most popular programming languages, extensively utilized in domains such as machine learning, data analysis, and web applications. Python&rsquo;s dynamic nature and extensive usage make it an attractive candidate for dynamic program analysis. However, unlike for other popular languages, there currently is no comprehensive <b>benchmark</b> suite of executable Python projects, which hinders the development of dynamic analyses. This work addresses this gap by presenting DyPyBench, the first <b>benchmark</b> of Python projects that is large scale, diverse, ready to run (i.e., with fully configured and prepared test suites), and ready to analyze (by integrating with the DynaPyt dynamic analysis framework). The <b>benchmark</b> encompasses 50 popular opensource projects from various application domains, with a total of 681k lines of Python code, and 30k test cases. DyPyBench enables various applications in testing and dynamic analysis, of which we explore three in this work: (i) Gathering dynamic call <b>graphs</b> and empirically comparing them to statically computed call <b>graphs,</b> which exposes and quantifies limitations of existing call <b>graph</b> construction techniques for Python. (ii) Using DyPyBench to build a training data set for LExecutor, a neural model that learns to predict values that otherwise would be missing at runtime. (iii) Using dynamically gathered execution traces to mine API usage specifications, which establishes a baseline for future work on specification mining for Python. We envision DyPyBench to provide a basis for other dynamic analyses and for studying the runtime behavior of Python code.</p></p class="citation"></blockquote><h2 id=eesssy-7>eess.SY (7)</h2><h3 id=17--139243-powerflowmultinet-multigraph-neural-networks-for-unbalanced-three-phase-distribution-systems-salah-ghamizi-et-al-2024>(1/7 | 139/243) PowerFlowMultiNet: Multigraph Neural Networks for Unbalanced Three-Phase Distribution Systems (Salah Ghamizi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Salah Ghamizi, Jun Cao, Aoxiang Ma, Pedro Rodriguez. (2024)<br><strong>PowerFlowMultiNet: Multigraph Neural Networks for Unbalanced Three-Phase Distribution Systems</strong><br><button class=copy-to-clipboard title="PowerFlowMultiNet: Multigraph Neural Networks for Unbalanced Three-Phase Distribution Systems" index=139>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-139 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-LG, cs-SY, eess-SY, eess.SY<br>Keyword Score: 63<br>Keywords: Message-Passing, Graph, Graph Embedding, Graph Neural Network, Graph Neural Network, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00892v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00892v1.pdf filename=2403.00892v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Efficiently solving unbalanced three-phase power flow in distribution grids is pivotal for grid analysis and <b>simulation.</b> There is a pressing need for scalable algorithms capable of handling large-scale unbalanced power grids that can provide accurate and fast solutions. To address this, deep learning techniques, especially <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs),</b> have emerged. However, existing literature primarily focuses on balanced networks, leaving a critical gap in supporting unbalanced three-phase power grids. This letter introduces PowerFlowMultiNet, a novel multigraph <b>GNN</b> framework explicitly designed for unbalanced three-phase power grids. The proposed approach models each phase separately in a multigraph representation, effectively capturing the inherent asymmetry in unbalanced grids. A <b>graph</b> <b>embedding</b> <b>mechanism</b> utilizing message passing is introduced to capture spatial dependencies within the power system network. PowerFlowMultiNet outperforms traditional methods and other deep learning approaches in terms of accuracy and computational speed. Rigorous testing reveals significantly lower error rates and a notable hundredfold increase in computational speed for large power networks compared to model-based methods.</p></p class="citation"></blockquote><h3 id=27--140243-a-holistic-power-optimization-approach-for-microgrid-control-based-on-deep-reinforcement-learning-fulong-yao-et-al-2024>(2/7 | 140/243) A Holistic Power Optimization Approach for Microgrid Control Based on Deep Reinforcement Learning (Fulong Yao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fulong Yao, Wanqing Zhao, Matthew Forshaw, Yang Song. (2024)<br><strong>A Holistic Power Optimization Approach for Microgrid Control Based on Deep Reinforcement Learning</strong><br><button class=copy-to-clipboard title="A Holistic Power Optimization Approach for Microgrid Control Based on Deep Reinforcement Learning" index=140>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-140 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 40<br>Keywords: Markov Decision Process, Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01013v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01013v1.pdf filename=2403.01013v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The global energy landscape is undergoing a transformation towards decarbonization, sustainability, and cost-efficiency. In this transition, microgrid systems integrated with renewable energy sources (RES) and energy storage systems (ESS) have emerged as a crucial component. However, optimizing the operational control of such an integrated energy system lacks a holistic view of multiple environmental, infrastructural and economic considerations, not to mention the need to factor in the uncertainties from both the supply and demand. This paper presents a holistic datadriven power optimization approach based on deep <b>reinforcement</b> <b>learning</b> (DRL) for microgrid control considering the multiple needs of decarbonization, sustainability and cost-efficiency. First, two data-driven control schemes, namely the prediction-based (PB) and prediction-free (PF) schemes, are devised to formulate the control problem within a <b>Markov</b> <b>decision</b> <b>process</b> (MDP). Second, a multivariate objective (reward) function is designed to account for the market profits, carbon emissions, peak load, and battery degradation of the microgrid system. Third, we develop a Double Dueling Deep Q Network (D3QN) architecture to optimize the power flows for real-time energy management and determine charging/discharging strategies of ESS. Finally, extensive <b>simulations</b> are conducted to demonstrate the effectiveness and superiority of the proposed approach through a comparative analysis. The results and analysis also suggest the respective circumstances for using the two control schemes in practical implementations with uncertainties.</p></p class="citation"></blockquote><h3 id=37--141243-data-based-control-of-continuous-time-linear-systems-with-performance-specifications-victor-g-lopez-et-al-2024>(3/7 | 141/243) Data-Based Control of Continuous-Time Linear Systems with Performance Specifications (Victor G. Lopez et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Victor G. Lopez, Matthias A. Müller. (2024)<br><strong>Data-Based Control of Continuous-Time Linear Systems with Performance Specifications</strong><br><button class=copy-to-clipboard title="Data-Based Control of Continuous-Time Linear Systems with Performance Specifications" index=141>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-141 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 40<br>Keywords: Continuous Time, Continuous Time, Discrete Time, Discrete Time, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00424v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00424v1.pdf filename=2403.00424v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The design of direct data-based controllers has become a fundamental part of control theory research in the last few years. In this paper, we consider three classes of data-based state feedback control problems for linear systems. These control problems are such that, besides stabilization, some additional performance requirements must be satisfied. First, we formulate and solve a trajectory-reference control problem, on which desired closed-loop trajectories are known and a controller that allows the system to closely follow those trajectories is computed. Then, in the area of data-based optimal control, we solve two different problems: the inverse problem of optimal control, and the solution of the LQR problem for <b>continuous-time</b> <b>systems.</b> Finally, we consider the case in which the precise position of the desired poles of the closed-loop system is known, and introduce a data-based variant of a robust pole-placement procedure. Although we focus on <b>continuous-time</b> <b>systems,</b> all of the presented methods can also be easily formulated for the <b>discrete-time</b> <b>case.</b> The applicability of the proposed methods is tested using numerical <b>simulations.</b></p></p class="citation"></blockquote><h3 id=47--142243-policy-optimization-for-pde-control-with-a-warm-start-xiangyuan-zhang-et-al-2024>(4/7 | 142/243) Policy Optimization for PDE Control with a Warm Start (Xiangyuan Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiangyuan Zhang, Saviz Mowlavi, Mouhacine Benosman, Tamer Başar. (2024)<br><strong>Policy Optimization for PDE Control with a Warm Start</strong><br><button class=copy-to-clipboard title="Policy Optimization for PDE Control with a Warm Start" index=142>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-142 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-AI, cs-SY, eess-SY, eess.SY, math-OC<br>Keyword Score: 20<br>Keywords: Fine-tuning, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01005v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01005v1.pdf filename=2403.01005v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Dimensionality reduction is crucial for controlling nonlinear partial differential equations (PDE) through a &ldquo;reduce-then-design&rdquo; strategy, which identifies a reduced-order model and then implements model-based control solutions. However, inaccuracies in the reduced-order modeling can substantially degrade controller performance, especially in PDEs with chaotic behavior. To address this issue, we augment the reduce-then-design procedure with a policy optimization (PO) step. The PO step <b>fine-tunes</b> the model-based controller to compensate for the modeling error from dimensionality reduction. This augmentation shifts the overall strategy into reduce-then-design-then-adapt, where the model-based controller serves as a warm start for PO. Specifically, we study the state-feedback tracking control of PDEs that aims to align the PDE state with a specific constant target subject to a linear-quadratic cost. Through extensive experiments, we show that a few iterations of PO can significantly improve the model-based controller performance. Our approach offers a cost-effective alternative to PDE control using end-to-end <b>reinforcement</b> <b>learning.</b></p></p class="citation"></blockquote><h3 id=57--143243-distributed-mpc-for-autonomous-ships-on-inland-waterways-with-collaborative-collision-avoidance-hoang-anh-tran-et-al-2024>(5/7 | 143/243) Distributed MPC for autonomous ships on inland waterways with collaborative collision avoidance (Hoang Anh Tran et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hoang Anh Tran, Tor Arne Johansen, Rudy R. Negenborn. (2024)<br><strong>Distributed MPC for autonomous ships on inland waterways with collaborative collision avoidance</strong><br><button class=copy-to-clipboard title="Distributed MPC for autonomous ships on inland waterways with collaborative collision avoidance" index=143>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-143 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00554v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00554v1.pdf filename=2403.00554v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a distributed solution for the problem of collaborative collision avoidance for autonomous inland waterway ships. A two-layer collision avoidance framework that considers inland waterway traffic regulations is proposed to increase navigational safety for autonomous ships. Our approach allows for modifying traffic rules without changing the collision avoidance algorithm, and is based on a novel formulation of model predictive control (MPC) for collision avoidance of ships. This MPC formulation is designed for inland waterway traffic and can handle complex scenarios. The alternating direction method of multipliers is used as a scheme for exchanging and negotiating intentions among ships. <b>Simulation</b> results show that the proposed algorithm can comply with traffic rules. Furthermore, the proposed algorithm can safely deviate from traffic rules when necessary to increase efficiency in complex scenarios.</p></p class="citation"></blockquote><h3 id=67--144243-optimization-of-the-energy-comfort-trade-off-of-hvac-systems-in-electric-city-buses-based-on-a-steady-state-model-fabio-widmer-et-al-2024>(6/7 | 144/243) Optimization of the Energy-Comfort Trade-Off of HVAC Systems in Electric City Buses Based on a Steady-State Model (Fabio Widmer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fabio Widmer, Stijn van Dooren, Christopher H. Onder. (2024)<br><strong>Optimization of the Energy-Comfort Trade-Off of HVAC Systems in Electric City Buses Based on a Steady-State Model</strong><br><button class=copy-to-clipboard title="Optimization of the Energy-Comfort Trade-Off of HVAC Systems in Electric City Buses Based on a Steady-State Model" index=144>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-144 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00517v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00517v1.pdf filename=2403.00517v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The electrification of public transport vehicles offers the potential to relieve city centers of pollutant and noise emissions. Furthermore, electric buses have lower life-cycle greenhouse gas (GHG) emissions than diesel buses, particularly when operated with sustainably produced electricity. However, the heating, ventilation, and air-conditioning (HVAC) system can consume a significant amount of energy, thus limiting the achievable driving range. In this paper, we address the HVAC system in an electric city bus by analyzing the trade-off between the energy consumption and the thermal comfort of the passengers. We do this by developing a dynamic thermal model for the bus cabin, which we simplify by considering it to be in steady state. We introduce a method that is able to quickly optimize the steady-state HVAC system inputs for a large number of samples representative of a year-round operation. A comparison between the results from the steady-state optimization approach and a dynamic <b>simulation</b> reveal small deviations in both the HVAC system power demand and achieved thermal comfort. Thus, the approximation of the system performance with a steady-state model is justified. We present two case studies to demonstrate the practical relevance of the approach. First, we show how the method can be used to compare different system designs based on a year-round performance evaluation. Second, we show how the method can be used to generate accurate setpoints for online controllers. In conclusion, this study shows that a steady-state analysis of the HVAC systems of an electric city bus is a valuable approach to evaluate and optimize its performance.</p></p class="citation"></blockquote><h3 id=77--145243-sindy-vs-hard-nonlinearities-and-hidden-dynamics-a-benchmarking-study-aurelio-raffa-ugolini-et-al-2024>(7/7 | 145/243) SINDy vs Hard Nonlinearities and Hidden Dynamics: a Benchmarking Study (Aurelio Raffa Ugolini et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aurelio Raffa Ugolini, Valentina Breschi, Andrea Manzoni, Mara Tanelli. (2024)<br><strong>SINDy vs Hard Nonlinearities and Hidden Dynamics: a Benchmarking Study</strong><br><button class=copy-to-clipboard title="SINDy vs Hard Nonlinearities and Hidden Dynamics: a Benchmarking Study" index=145>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-145 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-LG, cs-SY, eess-SY, eess.SY<br>Keyword Score: 6<br>Keywords: Benchmarking, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00578v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00578v1.pdf filename=2403.00578v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work we analyze the effectiveness of the Sparse Identification of Nonlinear Dynamics (SINDy) technique on three <b>benchmark</b> datasets for nonlinear identification, to provide a better understanding of its suitability when tackling real dynamical systems. While SINDy can be an appealing strategy for pursuing physics-based learning, our analysis highlights difficulties in dealing with unobserved states and non-smooth dynamics. Due to the ubiquity of these features in real systems in general, and control applications in particular, we complement our analysis with hands-on approaches to tackle these issues in order to exploit SINDy also in these challenging contexts.</p></p class="citation"></blockquote><h2 id=eessiv-3>eess.IV (3)</h2><h3 id=13--146243-visrec-a-semi-supervised-approach-to-radio-interferometric-data-reconstruction-ruoqi-wang-et-al-2024>(1/3 | 146/243) VisRec: A Semi-Supervised Approach to Radio Interferometric Data Reconstruction (Ruoqi Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruoqi Wang, Haitao Wang, Qiong Luo, Feng Wang, Hejun Wu. (2024)<br><strong>VisRec: A Semi-Supervised Approach to Radio Interferometric Data Reconstruction</strong><br><button class=copy-to-clipboard title="VisRec: A Semi-Supervised Approach to Radio Interferometric Data Reconstruction" index=146>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-146 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: astro-ph-GA, cs-AI, cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 60<br>Keywords: Data Augmentation, Semi-Supervised Learning, Supervised Learning, Supervised Learning, Unsupervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00897v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00897v1.pdf filename=2403.00897v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Radio telescopes produce visibility <b>data</b> <b>about</b> celestial objects, but these <b>data</b> <b>are</b> sparse and noisy. As a result, images created on raw visibility <b>data</b> <b>are</b> of low quality. Recent studies have used deep learning models to reconstruct visibility <b>data</b> <b>to</b> get cleaner images. However, these methods rely on a substantial amount of labeled training <b>data,</b> <b>which</b> requires significant labeling effort from radio astronomers. Addressing this challenge, we propose VisRec, a model-agnostic <b>semi-supervised</b> <b>learning</b> approach to the reconstruction of visibility <b>data.</b> <b>Specifically,</b> VisRec consists of both a <b>supervised</b> <b>learning</b> module and an <b>unsupervised</b> <b>learning</b> module. In the <b>supervised</b> <b>learning</b> module, we introduce a set of <b>data</b> <b>augmentation</b> functions to produce diverse training examples. In comparison, the <b>unsupervised</b> <b>learning</b> module in VisRec augments unlabeled <b>data</b> <b>and</b> uses reconstructions from non-augmented visibility <b>data</b> <b>as</b> pseudo-labels for training. This hybrid approach allows VisRec to effectively leverage both labeled and unlabeled <b>data.</b> <b>This</b> way, VisRec performs well even when labeled <b>data</b> <b>is</b> scarce. Our evaluation results show that VisRec outperforms all baseline methods in reconstruction quality, robustness against common observation perturbation, and generalizability to different telescope configurations.</p></p class="citation"></blockquote><h3 id=23--147243-graph-theory-and-gnns-to-unravel-the-topographical-organization-of-brain-lesions-in-variants-of-alzheimers-disease-progression-leopold-hebert-stevens-et-al-2024>(2/3 | 147/243) Graph Theory and GNNs to Unravel the Topographical Organization of Brain Lesions in Variants of Alzheimer&rsquo;s Disease Progression (Leopold Hebert-Stevens et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Leopold Hebert-Stevens, Gabriel Jimenez, Benoit Delatour, Lev Stimmer, Daniel Racoceanu. (2024)<br><strong>Graph Theory and GNNs to Unravel the Topographical Organization of Brain Lesions in Variants of Alzheimer&rsquo;s Disease Progression</strong><br><button class=copy-to-clipboard title="Graph Theory and GNNs to Unravel the Topographical Organization of Brain Lesions in Variants of Alzheimer's Disease Progression" index=147>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-147 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 43<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Convolutional Neural Network, Explainable AI<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00636v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00636v1.pdf filename=2403.00636v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study utilizes <b>graph</b> <b>theory</b> <b>and</b> deep learning to assess variations in Alzheimer&rsquo;s disease (AD) neuropathologies, focusing on classic (cAD) and rapid (rpAD) progression forms. It analyses the distribution of amyloid plaques and tau tangles in postmortem brain tissues. Histopathological images are converted into tau-pathology-based <b>graphs,</b> <b>and</b> <b>derived</b> metrics are used for statistical analysis and in machine learning classifiers. These classifiers incorporate SHAP value explainability to differentiate between cAD and rpAD. <b>Graph</b> <b>neural</b> <b>networks</b> <b>(GNNs)</b> demonstrate greater efficiency than traditional <b>CNN</b> methods in analyzing this data, preserving spatial pathology context. Additionally, <b>GNNs</b> provide significant insights through <b>explainable</b> <b>AI</b> techniques. The analysis shows denser networks in rpAD and a distinctive impact on brain cortical layers: rpAD predominantly affects middle layers, whereas cAD influences both superficial and deep layers of the same cortical regions. These results suggest a unique neuropathological network organization for each AD variant.</p></p class="citation"></blockquote><h3 id=33--148243-relaxometry-guided-quantitative-cardiac-magnetic-resonance-image-reconstruction-yidong-zhao-et-al-2024>(3/3 | 148/243) Relaxometry Guided Quantitative Cardiac Magnetic Resonance Image Reconstruction (Yidong Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yidong Zhao, Yi Zhang, Qian Tao. (2024)<br><strong>Relaxometry Guided Quantitative Cardiac Magnetic Resonance Image Reconstruction</strong><br><button class=copy-to-clipboard title="Relaxometry Guided Quantitative Cardiac Magnetic Resonance Image Reconstruction" index=148>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-148 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 20<br>Keywords: Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00549v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00549v1.pdf filename=2403.00549v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep learning-based methods have achieved prestigious performance for magnetic resonance imaging (MRI) reconstruction, enabling fast imaging for many clinical applications. Previous methods employ <b>convolutional</b> <b>networks</b> to learn the image prior as the regularization term. In quantitative MRI, the physical model of nuclear magnetic resonance relaxometry is known, providing additional prior knowledge for image reconstruction. However, traditional reconstruction networks are limited to learning the spatial domain prior knowledge, ignoring the relaxometry prior. Therefore, we propose a relaxometry-guided quantitative MRI reconstruction framework to learn the spatial prior from data and the relaxometry prior from MRI physics. Additionally, we also evaluated the performance of two popular reconstruction backbones, namely, recurrent variational networks (RVN) and variational networks (VN) with U- Net. Experiments demonstrate that the proposed method achieves highly promising results in quantitative MRI reconstruction.</p></p class="citation"></blockquote><h2 id=csro-16>cs.RO (16)</h2><h3 id=116--149243-predicting-uav-type-an-exploration-of-sampling-and-data-augmentation-for-time-series-classification-tarik-crnovrsanin-et-al-2024>(1/16 | 149/243) Predicting UAV Type: An Exploration of Sampling and Data Augmentation for Time Series Classification (Tarik Crnovrsanin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tarik Crnovrsanin, Calvin Yu, Dane Hankamer, Cody Dunne. (2024)<br><strong>Predicting UAV Type: An Exploration of Sampling and Data Augmentation for Time Series Classification</strong><br><button class=copy-to-clipboard title="Predicting UAV Type: An Exploration of Sampling and Data Augmentation for Time Series Classification" index=149>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-149 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs.RO<br>Keyword Score: 50<br>Keywords: Data Augmentation, LSTM, LSTM, LSTM, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00565v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00565v1.pdf filename=2403.00565v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Unmanned aerial vehicles are becoming common and have many productive uses. However, their increased prevalence raises safety concerns &ndash; how can we protect restricted airspace? Knowing the type of unmanned aerial vehicle can go a <b>long</b> <b>way</b> <b>in</b> <b>determining</b> any potential risks it carries. For instance, fixed-wing craft can carry more weight over longer distances, thus potentially posing a more significant threat. This paper presents a machine learning model for classifying unmanned aerial vehicles as quadrotor, hexarotor, or fixed-wing. Our approach effectively applies a <b>Long-Short</b> <b>Term</b> <b>Memory</b> <b>(LSTM)</b> neural network for the purpose of time series classification. We performed experiments to test the effects of changing the timestamp sampling method and addressing the imbalance in the class distribution. Through these experiments, we identified the top-performing sampling and class imbalance fixing methods. Averaging the macro f-scores across 10 folds of <b>data,</b> <b>we</b> found that the majority quadrotor class was predicted well (98.16%), and, despite an extreme class imbalance, the model could also predicted a majority of fixed-wing flights correctly (73.15%). Hexarotor instances were often misclassified as quadrotors due to the similarity of multirotors in general (42.15%). However, results remained relatively stable across certain methods, which <b>prompted</b> us to analyze and report on their tradeoffs. The supplemental material for this paper, including the code and <b>data</b> <b>for</b> running all the experiments and generating the results tables, is available at <a href=https://osf.io/mnsgk/>https://osf.io/mnsgk/</a>.</p></p class="citation"></blockquote><h3 id=216--150243-never-ending-embodied-robot-learning-wenqi-liang-et-al-2024>(2/16 | 150/243) Never-Ending Embodied Robot Learning (Wenqi Liang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenqi Liang, Gan Sun, Qian He, Yu Ren, Jiahua Dong, Yang Cong. (2024)<br><strong>Never-Ending Embodied Robot Learning</strong><br><button class=copy-to-clipboard title="Never-Ending Embodied Robot Learning" index=150>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-150 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs.RO<br>Keyword Score: 39<br>Keywords: Benchmarking, Knowledge Distillation, Multi-modal, Multi-modal, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00336v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00336v1.pdf filename=2403.00336v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Relying on <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> embodied robots could perform complex <b>multimodal</b> robot manipulation tasks from visual observations with powerful generalization ability. However, most visual behavior-cloning agents suffer from manipulation performance degradation and skill knowledge forgetting when adapting into a series of challenging unseen tasks. We here investigate the above challenge with NBCagent in embodied robots, a pioneering language-conditioned Never-ending Behavior-Cloning agent, which can continually learn observation knowledge of novel robot manipulation skills from skill-specific and skill-shared attributes. Specifically, we establish a skill-specific evolving planner to perform knowledge decoupling, which can continually embed novel skill-specific knowledge in our NBCagent agent from latent and low-rank space. Meanwhile, we propose a skill-shared semantics rendering module and a skill-shared representation <b>distillation</b> module to effectively transfer anti-forgetting skill-shared knowledge, further tackling catastrophic forgetting on old skills from semantics and representation aspects. Finally, we design a continual embodied robot manipulation <b>benchmark,</b> and several expensive experiments demonstrate the significant performance of our method. Visual results, code, and dataset are provided at: <a href=https://neragent.github.io>https://neragent.github.io</a>.</p></p class="citation"></blockquote><h3 id=316--151243-selfi-autonomous-self-improvement-with-reinforcement-learning-for-social-navigation-noriaki-hirose-et-al-2024>(3/16 | 151/243) SELFI: Autonomous Self-Improvement with Reinforcement Learning for Social Navigation (Noriaki Hirose et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Noriaki Hirose, Dhruv Shah, Kyle Stachowicz, Ajay Sridhar, Sergey Levine. (2024)<br><strong>SELFI: Autonomous Self-Improvement with Reinforcement Learning for Social Navigation</strong><br><button class=copy-to-clipboard title="SELFI: Autonomous Self-Improvement with Reinforcement Learning for Social Navigation" index=151>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-151 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-CV, cs-LG, cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Fine-tuning, Human Intervention, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00991v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00991v1.pdf filename=2403.00991v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Autonomous self-improving robots that interact and improve with experience are key to the real-world deployment of robotic systems. In this paper, we propose an online learning method, SELFI, that leverages online robot experience to rapidly <b>fine-tune</b> pre-trained control policies efficiently. SELFI applies online model-free <b>reinforcement</b> <b>learning</b> on top of offline model-based learning to bring out the best parts of both learning paradigms. Specifically, SELFI stabilizes the online learning process by incorporating the same model-based learning objective from offline pre-training into the Q-values learned with online model-free <b>reinforcement</b> <b>learning.</b> We evaluate SELFI in multiple real-world environments and report improvements in terms of collision avoidance, as well as more socially compliant behavior, measured by a <b>human</b> <b>user</b> study. SELFI enables us to quickly learn useful robotic behaviors with less <b>human</b> <b>interventions</b> such as pre-emptive behavior for the pedestrians, collision avoidance for small and transparent objects, and avoiding travel on uneven floor surfaces. We provide supplementary videos to demonstrate the performance of our <b>fine-tuned</b> policy on our project page.</p></p class="citation"></blockquote><h3 id=416--152243-safe-hybrid-action-reinforcement-learning-based-decision-and-control-for-discretionary-lane-change-ruichen-xu-et-al-2024>(4/16 | 152/243) Safe Hybrid-Action Reinforcement Learning-Based Decision and Control for Discretionary Lane Change (Ruichen Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruichen Xu, Xiao Liu, Jinming Xu, Yuan Lin. (2024)<br><strong>Safe Hybrid-Action Reinforcement Learning-Based Decision and Control for Discretionary Lane Change</strong><br><button class=copy-to-clipboard title="Safe Hybrid-Action Reinforcement Learning-Based Decision and Control for Discretionary Lane Change" index=152>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-152 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-LG, cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00446v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00446v1.pdf filename=2403.00446v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Autonomous lane-change, a key feature of advanced driver-assistance systems, can enhance traffic efficiency and reduce the incidence of accidents. However, safe driving of autonomous vehicles remains challenging in complex environments. How to perform safe and appropriate lane change is a popular topic of research in the field of autonomous driving. Currently, few papers consider the safety of <b>reinforcement</b> <b>learning</b> in autonomous lane-change scenarios. We introduce safe hybrid-action <b>reinforcement</b> <b>learning</b> into discretionary lane change for the first time and propose Parameterized Soft Actor-Critic with PID Lagrangian (PASAC-PIDLag) algorithm. Furthermore, we conduct a comparative analysis of the Parameterized Soft Actor-Critic (PASAC), which is an unsafe version of PASAC-PIDLag. Both algorithms are employed to train the lane-change strategy of autonomous vehicles to output discrete lane-change decision and longitudinal vehicle acceleration. Our <b>simulation</b> results indicate that at a traffic density of 15 vehicles per kilometer (15 veh/km), the PASAC-PIDLag algorithm exhibits superior safety with a collision rate of 0%, outperforming the PASAC algorithm, which has a collision rate of 1%. The outcomes of the generalization assessments reveal that at low traffic density levels, both the PASAC-PIDLag and PASAC algorithms are proficient in attaining a 0% collision rate. Under conditions of high traffic flow density, the PASAC-PIDLag algorithm surpasses PASAC in terms of both safety and optimality.</p></p class="citation"></blockquote><h3 id=516--153243-structured-deep-neural-networks-based-backstepping-trajectory-tracking-control-for-lagrangian-systems-jiajun-qian-et-al-2024>(5/16 | 153/243) Structured Deep Neural Networks-Based Backstepping Trajectory Tracking Control for Lagrangian Systems (Jiajun Qian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiajun Qian, Liang Xu, Xiaoqiang Ren, Xiaofan Wang. (2024)<br><strong>Structured Deep Neural Networks-Based Backstepping Trajectory Tracking Control for Lagrangian Systems</strong><br><button class=copy-to-clipboard title="Structured Deep Neural Networks-Based Backstepping Trajectory Tracking Control for Lagrangian Systems" index=153>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-153 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-LG, cs-RO, cs-SY, cs.RO, eess-SY<br>Keyword Score: 25<br>Keywords: Black Box, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00381v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00381v1.pdf filename=2403.00381v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep neural networks (DNN) are increasingly being used to learn controllers due to their excellent approximation capabilities. However, their <b>black-box</b> <b>nature</b> poses significant challenges to closed-loop stability guarantees and performance analysis. In this paper, we introduce a structured DNN-based controller for the trajectory tracking control of Lagrangian systems using backing techniques. By properly designing neural network structures, the proposed controller can ensure closed-loop stability for any compatible neural network parameters. In addition, improved control performance can be achieved by further optimizing neural network parameters. Besides, we provide explicit upper bounds on tracking errors in terms of controller parameters, which allows us to achieve the desired tracking performance by properly selecting the controller parameters. Furthermore, when system models are unknown, we propose an improved Lagrangian neural network (LNN) structure to learn the system dynamics and design the controller. We show that in the presence of model approximation errors and external disturbances, the closed-loop stability and tracking control performance can still be guaranteed. The effectiveness of the proposed approach is demonstrated through <b>simulations.</b></p></p class="citation"></blockquote><h3 id=616--154243-optimal-robot-formations-balancing-range-based-observability-and-user-defined-configurations-syed-shabbir-ahmed-et-al-2024>(6/16 | 154/243) Optimal Robot Formations: Balancing Range-Based Observability and User-Defined Configurations (Syed Shabbir Ahmed et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Syed Shabbir Ahmed, Mohammed Ayman Shalaby, Jerome Le Ny, James Richard Forbes. (2024)<br><strong>Optimal Robot Formations: Balancing Range-Based Observability and User-Defined Configurations</strong><br><button class=copy-to-clipboard title="Optimal Robot Formations: Balancing Range-Based Observability and User-Defined Configurations" index=154>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-154 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00988v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00988v1.pdf filename=2403.00988v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces a set of customizable and novel cost functions that enable the user to easily specify desirable robot formations, such as a ``high-coverage&rsquo;&rsquo; infrastructure-inspection formation, while maintaining high relative pose estimation accuracy. The overall cost function balances the need for the robots to be close together for good ranging-based relative localization accuracy and the need for the robots to achieve specific tasks, such as minimizing the time taken to inspect a given area. The formations found by minimizing the aggregated cost function are evaluated in a coverage path planning task in <b>simulation</b> and experiment, where the robots localize themselves and unknown landmarks using a simultaneous localization and mapping algorithm based on the extended Kalman filter. Compared to an optimal formation that maximizes ranging-based relative localization accuracy, these formations significantly reduce the time to cover a given area with minimal impact on relative pose estimation accuracy.</p></p class="citation"></blockquote><h3 id=716--155243-joint-spatial-temporal-calibration-for-camera-and-global-pose-sensor-junlin-song-et-al-2024>(7/16 | 155/243) Joint Spatial-Temporal Calibration for Camera and Global Pose Sensor (Junlin Song et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junlin Song, Antoine Richard, Miguel Olivares-Mendez. (2024)<br><strong>Joint Spatial-Temporal Calibration for Camera and Global Pose Sensor</strong><br><button class=copy-to-clipboard title="Joint Spatial-Temporal Calibration for Camera and Global Pose Sensor" index=155>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-155 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-CV, cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00976v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00976v1.pdf filename=2403.00976v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In robotics, motion capture systems have been widely used to measure the accuracy of localization algorithms. Moreover, this infrastructure can also be used for other computer vision tasks, such as the evaluation of Visual (-Inertial) SLAM dynamic initialization, multi-object tracking, or automatic annotation. Yet, to work optimally, these functionalities require having accurate and reliable spatial-temporal calibration parameters between the camera and the global pose sensor. In this study, we provide two novel solutions to estimate these calibration parameters. Firstly, we design an offline target-based method with high accuracy and consistency. Spatial-temporal parameters, camera intrinsic, and trajectory are optimized simultaneously. Then, we propose an online target-less method, eliminating the need for a calibration target and enabling the estimation of time-varying spatial-temporal parameters. Additionally, we perform detailed observability analysis for the target-less method. Our theoretical findings regarding observability are validated by <b>simulation</b> experiments and provide explainable guidelines for calibration. Finally, the accuracy and consistency of two proposed methods are evaluated with hand-held real-world datasets where traditional hand-eye calibration method do not work.</p></p class="citation"></blockquote><h3 id=816--156243-nussbaum-function-based-approach-for-tracking-control-of-robot-manipulators-hamed-rahimi-nohooji-et-al-2024>(8/16 | 156/243) Nussbaum Function Based Approach for Tracking Control of Robot Manipulators (Hamed Rahimi Nohooji et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hamed Rahimi Nohooji, Holger Voos. (2024)<br><strong>Nussbaum Function Based Approach for Tracking Control of Robot Manipulators</strong><br><button class=copy-to-clipboard title="Nussbaum Function Based Approach for Tracking Control of Robot Manipulators" index=156>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-156 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs-SY, cs.RO, eess-SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00970v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00970v1.pdf filename=2403.00970v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces a novel Nussbaum function-based PID control for robotic manipulators. The integration of the Nussbaum function into the PID framework provides a solution with a simple structure that effectively tackles the challenge of unknown control directions. Stability is achieved through a combination of neural network-based estimation and Lyapunov analysis, facilitating automatic gain adjustment without the need for system dynamics. Our approach offers a gain determination with minimum parameter requirements, significantly reducing the complexity and enhancing the efficiency of robotic manipulator control. The paper guarantees that all signals within the closed-loop system remain bounded. Lastly, numerical <b>simulations</b> validate the theoretical framework, confirming the effectiveness of the proposed control strategy in enhancing robotic manipulator control.</p></p class="citation"></blockquote><h3 id=916--157243-suturing-tasks-automation-based-on-skills-learned-from-demonstrations-a-simulation-study-haoying-zhou-et-al-2024>(9/16 | 157/243) Suturing Tasks Automation Based on Skills Learned From Demonstrations: A Simulation Study (Haoying Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haoying Zhou, Yiwei Jiang, Shang Gao, Shiyue Wang, Peter Kazanzides, Gregory S. Fischer. (2024)<br><strong>Suturing Tasks Automation Based on Skills Learned From Demonstrations: A Simulation Study</strong><br><button class=copy-to-clipboard title="Suturing Tasks Automation Based on Skills Learned From Demonstrations: A Simulation Study" index=157>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-157 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00956v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00956v1.pdf filename=2403.00956v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we develop an open-source surgical <b>simulation</b> environment that includes a realistic model obtained by MRI-scanning a physical phantom, for the purpose of training and evaluating a Learning from Demonstration (LfD) algorithm for autonomous suturing. The LfD algorithm utilizes Dynamic Movement Primitives (DMP) and Locally Weighted Regression (LWR), but focuses on the needle trajectory, rather than the instruments, to obtain better generality with respect to needle grasps. We conduct a user study to collect multiple suturing demonstrations and perform a comprehensive analysis of the ability of the LfD algorithm to generalize from a demonstration at one location in one phantom to different locations in the same phantom and to a different phantom. Our results indicate good generalization, on the order of 91.5%, when learning from more experienced subjects, indicating the need to integrate skill assessment in the future.</p></p class="citation"></blockquote><h3 id=1016--158243-optimizing-dynamic-balance-in-a-rat-robot-via-the-lateral-flexion-of-a-soft-actuated-spine-yuhong-huang-et-al-2024>(10/16 | 158/243) Optimizing Dynamic Balance in a Rat Robot via the Lateral Flexion of a Soft Actuated Spine (Yuhong Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuhong Huang, Zhenshan Bing, Zitao Zhang, Genghang Zhuang, Kai Huang, Alois Knoll. (2024)<br><strong>Optimizing Dynamic Balance in a Rat Robot via the Lateral Flexion of a Soft Actuated Spine</strong><br><button class=copy-to-clipboard title="Optimizing Dynamic Balance in a Rat Robot via the Lateral Flexion of a Soft Actuated Spine" index=158>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-158 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00944v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00944v1.pdf filename=2403.00944v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Balancing oneself using the spine is a physiological alignment of the body posture in the most efficient manner by the muscular forces for mammals. For this reason, we can see many disabled quadruped animals can still stand or walk even with three limbs. This paper investigates the optimization of dynamic balance during trot gait based on the spatial relationship between the center of mass (CoM) and support area influenced by spinal flexion. During trotting, the robot balance is significantly influenced by the distance of the CoM to the support area formed by diagonal footholds. In this context, lateral spinal flexion, which is able to modify the position of footholds, holds promise for optimizing balance during trotting. This paper explores this phenomenon using a rat robot equipped with a soft actuated spine. Based on the lateral flexion of the spine, we establish a kinematic model to quantify the impact of spinal flexion on robot balance during trot gait. Subsequently, we develop an optimized controller for spinal flexion, designed to enhance balance without altering the leg locomotion. The effectiveness of our proposed controller is evaluated through extensive <b>simulations</b> and physical experiments conducted on a rat robot. Compared to both a non-spine based trot gait controller and a trot gait controller with lateral spinal flexion, our proposed optimized controller effectively improves the dynamic balance of the robot and retains the desired locomotion during trotting.</p></p class="citation"></blockquote><h3 id=1116--159243-prime-scaffolding-manipulation-tasks-with-behavior-primitives-for-data-efficient-imitation-learning-tian-gao-et-al-2024>(11/16 | 159/243) PRIME: Scaffolding Manipulation Tasks with Behavior Primitives for Data-Efficient Imitation Learning (Tian Gao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tian Gao, Soroush Nasiriany, Huihan Liu, Quantao Yang, Yuke Zhu. (2024)<br><strong>PRIME: Scaffolding Manipulation Tasks with Behavior Primitives for Data-Efficient Imitation Learning</strong><br><button class=copy-to-clipboard title="PRIME: Scaffolding Manipulation Tasks with Behavior Primitives for Data-Efficient Imitation Learning" index=159>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-159 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-LG, cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00929v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00929v1.pdf filename=2403.00929v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Imitation learning has shown great potential for enabling robots to acquire complex manipulation behaviors. However, these algorithms suffer from high sample complexity in long-horizon tasks, where compounding errors accumulate over the task horizons. We present PRIME (PRimitive-based IMitation with data Efficiency), a behavior primitive-based framework designed for improving the data efficiency of imitation learning. PRIME scaffolds robot tasks by decomposing task demonstrations into primitive sequences, followed by learning a high-level control policy to sequence primitives through imitation learning. Our experiments demonstrate that PRIME achieves a significant performance improvement in multi-stage manipulation tasks, with 10-34% higher success rates in <b>simulation</b> over state-of-the-art baselines and 20-48% on physical hardware.</p></p class="citation"></blockquote><h3 id=1216--160243-robust-online-epistemic-replanning-of-multi-robot-missions-lauren-bramblett-et-al-2024>(12/16 | 160/243) Robust Online Epistemic Replanning of Multi-Robot Missions (Lauren Bramblett et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lauren Bramblett, Branko Miloradovic, Patrick Sherman, Alessandro V. Papadopoulos, Nicola Bezzo. (2024)<br><strong>Robust Online Epistemic Replanning of Multi-Robot Missions</strong><br><button class=copy-to-clipboard title="Robust Online Epistemic Replanning of Multi-Robot Missions" index=160>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-160 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00641v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00641v1.pdf filename=2403.00641v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As Multi-Robot Systems (MRS) become more affordable and computing capabilities grow, they provide significant advantages for complex applications such as environmental monitoring, underwater inspections, or space exploration. However, accounting for potential communication loss or the unavailability of communication infrastructures in these application domains remains an open problem. Much of the applicable MRS research assumes that the system can sustain communication through proximity regulations and formation control or by devising a framework for separating and adhering to a predetermined plan for extended periods of disconnection. The latter technique enables an MRS to be more efficient, but breakdowns and environmental uncertainties can have a domino effect throughout the system, particularly when the mission goal is intricate or time-sensitive. To deal with this problem, our proposed framework has two main phases: i) a centralized planner to allocate mission tasks by rewarding intermittent rendezvous between robots to mitigate the effects of the unforeseen events during mission execution, and ii) a decentralized replanning scheme leveraging epistemic planning to formalize belief propagation and a Monte Carlo tree search for policy optimization given distributed rational belief updates. The proposed framework outperforms a baseline heuristic and is validated using <b>simulations</b> and experiments with aerial vehicles.</p></p class="citation"></blockquote><h3 id=1316--161243-learning-quadrupedal-locomotion-with-impaired-joints-using-random-joint-masking-mincheol-kim-et-al-2024>(13/16 | 161/243) Learning Quadrupedal Locomotion with Impaired Joints Using Random Joint Masking (Mincheol Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mincheol Kim, Ukcheol Shin, Jung-Yup Kim. (2024)<br><strong>Learning Quadrupedal Locomotion with Impaired Joints Using Random Joint Masking</strong><br><button class=copy-to-clipboard title="Learning Quadrupedal Locomotion with Impaired Joints Using Random Joint Masking" index=161>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-161 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Curriculum Learning, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00398v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00398v1.pdf filename=2403.00398v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Quadrupedal robots have played a crucial role in various environments, from structured environments to complex harsh terrains, thanks to their agile locomotion ability. However, these robots can easily lose their locomotion functionality if damaged by external accidents or internal malfunctions. In this paper, we propose a novel deep <b>reinforcement</b> <b>learning</b> framework to enable a quadrupedal robot to walk with impaired joints. The proposed framework consists of three components: 1) a random joint masking strategy for simulating impaired joint scenarios, 2) a joint state estimator to predict an implicit status of current joint condition based on past observation history, and 3) progressive <b>curriculum</b> <b>learning</b> to allow a single network to conduct both normal gait and various joint-impaired gaits. We verify that our framework enables the Unitree&rsquo;s Go1 robot to walk under various impaired joint conditions in real-world indoor and outdoor environments.</p></p class="citation"></blockquote><h3 id=1416--162243-robustifying-a-policy-in-multi-agent-rl-with-diverse-cooperative-behavior-and-adversarial-style-sampling-for-assistive-tasks-tayuki-osa-et-al-2024>(14/16 | 162/243) Robustifying a Policy in Multi-Agent RL with Diverse Cooperative Behavior and Adversarial Style Sampling for Assistive Tasks (Tayuki Osa et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tayuki Osa, Tatsuya Harada. (2024)<br><strong>Robustifying a Policy in Multi-Agent RL with Diverse Cooperative Behavior and Adversarial Style Sampling for Assistive Tasks</strong><br><button class=copy-to-clipboard title="Robustifying a Policy in Multi-Agent RL with Diverse Cooperative Behavior and Adversarial Style Sampling for Assistive Tasks" index=162>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-162 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-LG, cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00344v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00344v1.pdf filename=2403.00344v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Autonomous assistance of people with motor impairments is one of the most promising applications of autonomous robotic systems. Recent studies have reported encouraging results using deep <b>reinforcement</b> <b>learning</b> (RL) in the healthcare domain. Previous studies showed that assistive tasks can be formulated as multi-agent RL, wherein there are two agents: a caregiver and a care-receiver. However, policies trained in multi-agent RL are often sensitive to the policies of other agents. In such a case, a trained caregiver&rsquo;s policy may not work for different care-receivers. To alleviate this issue, we propose a framework that learns a robust caregiver&rsquo;s policy by training it for diverse care-receiver responses. In our framework, diverse care-receiver responses are autonomously learned through trials and errors. In addition, to robustify the care-giver&rsquo;s policy, we propose a strategy for sampling a care-receiver&rsquo;s response in an adversarial manner during the training. We evaluated the proposed method using tasks in an Assistive Gym. We demonstrate that policies trained with a popular deep RL method are vulnerable to changes in policies of other agents and that the proposed framework improves the robustness against such changes.</p></p class="citation"></blockquote><h3 id=1516--163243-complete-and-near-optimal-robotic-crack-coverage-and-filling-in-civil-infrastructure-vishnu-veeraraghavan-et-al-2024>(15/16 | 163/243) Complete and Near-Optimal Robotic Crack Coverage and Filling in Civil Infrastructure (Vishnu Veeraraghavan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vishnu Veeraraghavan, Kyle Hunte, Jingang Yi, Kaiyan Yu. (2024)<br><strong>Complete and Near-Optimal Robotic Crack Coverage and Filling in Civil Infrastructure</strong><br><button class=copy-to-clipboard title="Complete and Near-Optimal Robotic Crack Coverage and Filling in Civil Infrastructure" index=163>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-163 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs-SY, cs.RO, eess-SY<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00613v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00613v1.pdf filename=2403.00613v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a simultaneous sensor-based inspection and footprint coverage (SIFC) planning and control design with applications to autonomous robotic crack mapping and filling. The main challenge of the SIFC problem lies in the coupling of complete sensing (for mapping) and robotic footprint (for filling) coverage tasks. Initially, we assume known target information (e.g., crack) and employ classic cell decomposition methods to achieve complete sensing coverage of the workspace and complete robotic footprint coverage using the least-cost route. Subsequently, we generalize the algorithm to handle unknown target information, allowing the robot to scan and incrementally construct the target <b>graph</b> online while conducting robotic footprint coverage. The online polynomial-time SIFC planning algorithm minimizes the total robot traveling distance, guarantees complete sensing coverage of the entire workspace, and achieves near-optimal robotic footprint coverage, as demonstrated through empirical experiments. For the demonstrated application, we design coordinated nozzle motion control with the planned robot trajectory to efficiently fill all cracks within the robot&rsquo;s footprint. Experimental results are presented to illustrate the algorithm&rsquo;s design, performance, and comparisons. The SIFC algorithm offers a high-efficiency motion planning solution for various robotic applications requiring simultaneous sensing and actuation coverage.</p></p class="citation"></blockquote><h3 id=1616--164243-model-based-planning-and-control-for-terrestrial-aerial-bimodal-vehicles-with-passive-wheels-ruibin-zhang-et-al-2024>(16/16 | 164/243) Model-Based Planning and Control for Terrestrial-Aerial Bimodal Vehicles with Passive Wheels (Ruibin Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruibin Zhang, Junxiao Lin, Yuze Wu, Yuman Gao, Chi Wang, Chao Xu, Yanjun Cao, Fei Gao. (2024)<br><strong>Model-Based Planning and Control for Terrestrial-Aerial Bimodal Vehicles with Passive Wheels</strong><br><button class=copy-to-clipboard title="Model-Based Planning and Control for Terrestrial-Aerial Bimodal Vehicles with Passive Wheels" index=164>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-164 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00322v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00322v1.pdf filename=2403.00322v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Terrestrial and aerial bimodal vehicles have gained widespread attention due to their cross-domain maneuverability. Nevertheless, their bimodal dynamics significantly increase the complexity of motion planning and control, thus hindering robust and efficient autonomous navigation in unknown environments. To resolve this issue, we develop a model-based planning and control framework for terrestrial aerial bi-modal vehicles. This work begins by deriving a unified dynamic model and the corresponding differential flatness. Leveraging differential flatness, an optimization-based trajectory planner is proposed, which takes into account both solution quality and computational efficiency. Moreover, we design a tracking controller using nonlinear model predictive control based on the proposed unified dynamic model to achieve accurate trajectory tracking and smooth mode transition. We validate our framework through extensive <b>benchmark</b> comparisons and experiments, demonstrating its effectiveness in terms of planning quality and control performance.</p></p class="citation"></blockquote><h2 id=cshc-8>cs.HC (8)</h2><h3 id=18--165243-leveraging-prompt-based-large-language-models-predicting-pandemic-health-decisions-and-outcomes-through-social-media-language-xiaohan-ding-et-al-2024>(1/8 | 165/243) Leveraging Prompt-Based Large Language Models: Predicting Pandemic Health Decisions and Outcomes Through Social Media Language (Xiaohan Ding et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaohan Ding, Buse Carik, Uma Sushmitha Gunturi, Valerie Reyna, Eugenia H. Rho. (2024)<br><strong>Leveraging Prompt-Based Large Language Models: Predicting Pandemic Health Decisions and Outcomes Through Social Media Language</strong><br><button class=copy-to-clipboard title="Leveraging Prompt-Based Large Language Models: Predicting Pandemic Health Decisions and Outcomes Through Social Media Language" index=165>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-165 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-AI, cs-CL, cs-HC, cs-SI, cs.HC<br>Keyword Score: 40<br>Keywords: Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00994v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00994v1.pdf filename=2403.00994v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce a multi-step <b>reasoning</b> framework using <b>prompt-based</b> <b>LLMs</b> to examine the relationship between social media language patterns and trends in national health outcomes. Grounded in fuzzy-trace theory, which emphasizes the importance of gists of causal coherence in effective health communication, we introduce Role-Based Incremental Coaching (RBIC), a <b>prompt-based</b> <b>LLM</b> framework, to identify gists at-scale. Using RBIC, we systematically extract gists from subreddit discussions opposing COVID-19 health measures (Study 1). We then track how these gists evolve across key events (Study 2) and assess their influence on online engagement (Study 3). Finally, we investigate how the volume of gists is associated with national health trends like vaccine uptake and hospitalizations (Study 4). Our work is the first to empirically link social media linguistic patterns to real-world public health trends, highlighting the potential of <b>prompt-based</b> <b>LLMs</b> in identifying critical online discussion patterns that can form the basis of public health communication strategies.</p></p class="citation"></blockquote><h3 id=28--166243-to-trust-or-distrust-trust-measures-validating-questionnaires-for-trust-in-ai-nicolas-scharowski-et-al-2024>(2/8 | 166/243) To Trust or Distrust Trust Measures: Validating Questionnaires for Trust in AI (Nicolas Scharowski et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nicolas Scharowski, Sebastian A. C. Perrig, Lena Fanya Aeschbach, Nick von Felten, Klaus Opwis, Philipp Wintersberger, Florian Brühlmann. (2024)<br><strong>To Trust or Distrust Trust Measures: Validating Questionnaires for Trust in AI</strong><br><button class=copy-to-clipboard title="To Trust or Distrust Trust Measures: Validating Questionnaires for Trust in AI" index=166>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-166 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 20<br>Keywords: Recommendation, Chatbot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00582v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00582v1.pdf filename=2403.00582v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite the importance of trust in human-AI interactions, researchers must adopt questionnaires from other disciplines that lack validation in the AI context. Motivated by the need for reliable and valid measures, we investigated the psychometric quality of two trust questionnaires, the Trust between People and Automation scale (TPA) by Jian et al. (2000) and the Trust Scale for the AI Context (TAI) by Hoffman et al. (2023). In a pre-registered online experiment (N = 1485), participants observed interactions with trustworthy and untrustworthy AI (autonomous vehicle and <b>chatbot).</b> Results support the psychometric quality of the TAI while revealing opportunities to improve the TPA, which we outline in our <b>recommendations</b> for using the two questionnaires. Furthermore, our findings provide additional empirical evidence of trust and distrust as two distinct constructs that may coexist independently. Building on our findings, we highlight the opportunities and added value of measuring both trust and distrust in human-AI research and advocate for further work on both constructs.</p></p class="citation"></blockquote><h3 id=38--167243-multiple-ways-of-working-with-users-to-develop-physically-assistive-robots-amal-nanavati-et-al-2024>(3/8 | 167/243) Multiple Ways of Working with Users to Develop Physically Assistive Robots (Amal Nanavati et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amal Nanavati, Max Pascher, Vinitha Ranganeni, Ethan K. Gordon, Taylor Kessler Faulkner, Siddhartha S. Srinivasa, Maya Cakmak, Patrícia Alves-Oliveira, Jens Gerken. (2024)<br><strong>Multiple Ways of Working with Users to Develop Physically Assistive Robots</strong><br><button class=copy-to-clipboard title="Multiple Ways of Working with Users to Develop Physically Assistive Robots" index=167>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-167 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs-RO, cs.HC<br>Keyword Score: 13<br>Keywords: Recommendation, Sample Size<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00489v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00489v2.pdf filename=2403.00489v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite the growth of physically assistive robotics (PAR) research over the last decade, nearly half of PAR user studies do not involve participants with the target disabilities. There are several reasons for this &ndash; recruitment challenges, small <b>sample</b> <b>sizes,</b> and transportation logistics &ndash; all influenced by systemic barriers that people with disabilities face. However, it is well-established that working with end-users results in technology that better addresses their needs and integrates with their lived circumstances. In this paper, we reflect on multiple approaches we have taken to working with people with motor impairments across the design, development, and evaluation of three PAR projects: (a) assistive feeding with a robot arm; (b) assistive teleoperation with a mobile manipulator; and (c) shared control with a robot arm. We discuss these approaches to working with users along three dimensions &ndash; individual vs. community-level insight, logistic burden on end-users vs. researchers, and benefit to researchers vs. community &ndash; and share <b>recommendations</b> for how other PAR researchers can incorporate users into their work.</p></p class="citation"></blockquote><h3 id=48--168243-metamorpheus-interactive-affective-and-creative-dream-narration-through-metaphorical-visual-storytelling-qian-wan-et-al-2024>(4/8 | 168/243) Metamorpheus: Interactive, Affective, and Creative Dream Narration Through Metaphorical Visual Storytelling (Qian Wan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qian Wan, Xin Feng, Yining Bei, Zhiqi Gao, Zhicong Lu. (2024)<br><strong>Metamorpheus: Interactive, Affective, and Creative Dream Narration Through Metaphorical Visual Storytelling</strong><br><button class=copy-to-clipboard title="Metamorpheus: Interactive, Affective, and Creative Dream Narration Through Metaphorical Visual Storytelling" index=168>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-168 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-AI, cs-CL, cs-CY, cs-HC, cs.HC<br>Keyword Score: 10<br>Keywords: Generative AI<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00632v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00632v1.pdf filename=2403.00632v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Human emotions are essentially molded by lived experiences, from which we construct personalised meaning. The engagement in such meaning-making process has been practiced as an intervention in various psychotherapies to promote wellness. Nevertheless, to support recollecting and recounting lived experiences in everyday life remains under explored in HCI. It also remains unknown how technologies such as <b>generative</b> <b>AI</b> models can facilitate the meaning making process, and ultimately support affective mindfulness. In this paper we present Metamorpheus, an affective interface that engages users in a creative visual storytelling of emotional experiences during dreams. Metamorpheus arranges the storyline based on a dream&rsquo;s emotional arc, and provokes self-reflection through the creation of metaphorical images and text depictions. The system provides metaphor suggestions, and generates visual metaphors and text depictions using <b>generative</b> <b>AI</b> models, while users can apply generations to recolour and re-arrange the interface to be visually affective. Our experience-centred evaluation manifests that, by interacting with Metamorpheus, users can recall their dreams in vivid detail, through which they relive and reflect upon their experiences in a meaningful way.</p></p class="citation"></blockquote><h3 id=58--169243-authors-values-and-attitudes-towards-ai-bridged-scalable-personalization-of-creative-language-arts-taewook-kim-et-al-2024>(5/8 | 169/243) Authors&rsquo; Values and Attitudes Towards AI-bridged Scalable Personalization of Creative Language Arts (Taewook Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Taewook Kim, Hyomin Han, Eytan Adar, Matthew Kay, John Joon Young Chung. (2024)<br><strong>Authors&rsquo; Values and Attitudes Towards AI-bridged Scalable Personalization of Creative Language Arts</strong><br><button class=copy-to-clipboard title="Authors' Values and Attitudes Towards AI-bridged Scalable Personalization of Creative Language Arts" index=169>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-169 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-AI, cs-HC, cs.HC<br>Keyword Score: 10<br>Keywords: Generative AI<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00439v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00439v1.pdf filename=2403.00439v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Generative</b> <b>AI</b> has the potential to create a new form of interactive media: AI-bridged creative language arts (CLA), which bridge the author and audience by personalizing the author&rsquo;s vision to the audience&rsquo;s context and taste at scale. However, it is unclear what the authors&rsquo; values and attitudes would be regarding AI-bridged CLA. To identify these values and attitudes, we conducted an interview study with 18 authors across eight genres (e.g., poetry, comics) by presenting speculative but realistic AI-bridged CLA scenarios. We identified three benefits derived from the dynamics between author, artifact, and audience: those that 1) authors get from the process, 2) audiences get from the artifact, and 3) authors get from the audience. We found how AI-bridged CLA would either promote or reduce these benefits, along with authors&rsquo; concerns. We hope our investigation hints at how AI can provide intriguing experiences to CLA audiences while promoting authors&rsquo; values.</p></p class="citation"></blockquote><h3 id=68--170243-can-a-funny-chatbot-make-a-difference-infusing-humor-into-conversational-agent-for-behavioral-intervention-xin-sun-et-al-2024>(6/8 | 170/243) Can a Funny Chatbot Make a Difference? Infusing Humor into Conversational Agent for Behavioral Intervention (Xin Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xin Sun, Isabelle Teljeur, Zhuying Li, Jos A. Bosch. (2024)<br><strong>Can a Funny Chatbot Make a Difference? Infusing Humor into Conversational Agent for Behavioral Intervention</strong><br><button class=copy-to-clipboard title="Can a Funny Chatbot Make a Difference? Infusing Humor into Conversational Agent for Behavioral Intervention" index=170>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-170 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 10<br>Keywords: Chatbot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00365v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00365v1.pdf filename=2403.00365v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Regular physical activity is crucial for reducing the risk of non-communicable disease (NCD). With NCDs on the rise globally, there is an urgent need for effective health interventions, with <b>chatbots</b> emerging as a viable and cost-effective option because of limited healthcare accessibility. Although health professionals often utilize behavior change techniques (BCTs) to boost physical activity levels and enhance client engagement and motivation by affiliative humor, the efficacy of humor in <b>chatbot-delivered</b> interventions is not well-understood. This study conducted a randomized controlled trial to examine the impact of the generative humorous communication style in a 10-day <b>chatbot-delivered</b> intervention for physical activity. It further investigated if user engagement and motivation act as mediators between the communication style and changes in physical activity levels. 66 participants engaged with the <b>chatbots</b> across three groups (humorous, non-humorous, and no-intervention) and responded to daily ecological momentary assessment questionnaires assessing engagement, motivation, and physical activity levels. Multilevel time series analyses revealed that an affiliative humorous communication style positively impacted physical activity levels over time, with user engagement acting as a mediator in this relationship, whereas motivation did not. These findings clarify the role of humorous communication style in <b>chatbot-delivered</b> physical activity interventions, offering valuable insights for future development of intelligent conversational agents incorporating humor.</p></p class="citation"></blockquote><h3 id=78--171243-nova-a-visual-interface-for-assessing-polarizing-media-coverage-keshav-dasu-et-al-2024>(7/8 | 171/243) NOVA: A visual interface for assessing polarizing media coverage (Keshav Dasu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Keshav Dasu, Sam Yu-Te Lee, Ying-Cheng Chen, Kwan-Liu Ma. (2024)<br><strong>NOVA: A visual interface for assessing polarizing media coverage</strong><br><button class=copy-to-clipboard title="NOVA: A visual interface for assessing polarizing media coverage" index=171>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-171 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00334v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00334v1.pdf filename=2403.00334v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Within the United States, the majority of the populace receives their news online. U.S mainstream media outlets both generate and influence the news consumed by U.S citizens. Many of these citizens have their personal beliefs about these outlets and question the <b>fairness</b> of their reporting. We offer an interactive visualization system for the public to assess their perception of the mainstream media&rsquo;s coverage of a topic against the data. Our system combines belief elicitation techniques and narrative structure designs, emphasizing transparency and user-friendliness to facilitate users&rsquo; self-assessment on personal beliefs. We gathered $\sim${25k} articles from the span of 2020-2022 from six mainstream media outlets as a testbed. To evaluate our system, we present usage scenarios alongside a user study with a qualitative analysis of user exploration strategies for personal belief assessment. We report our observations from this study and discuss future work and challenges of developing tools for the public to assess media outlet coverage and belief updating on provocative topics.</p></p class="citation"></blockquote><h3 id=88--172243-maidr-making-statistical-visualizations-accessible-with-multimodal-data-representation-jooyoung-seo-et-al-2024>(8/8 | 172/243) MAIDR: Making Statistical Visualizations Accessible with Multimodal Data Representation (JooYoung Seo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>JooYoung Seo, Yilin Xia, Bongshin Lee, Sean McCurry, Yu Jun Yam. (2024)<br><strong>MAIDR: Making Statistical Visualizations Accessible with Multimodal Data Representation</strong><br><button class=copy-to-clipboard title="MAIDR: Making Statistical Visualizations Accessible with Multimodal Data Representation" index=172>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-172 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-GR, cs-HC, cs.HC<br>Keyword Score: 6<br>Keywords: Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00717v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00717v1.pdf filename=2403.00717v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper investigates new data exploration experiences that enable blind users to interact with statistical data visualizations$-$bar plots, heat maps, box plots, and scatter plots$-$leveraging <b>multimodal</b> data representations. In addition to sonification and textual descriptions that are commonly employed by existing accessible visualizations, our MAIDR <b>(multimodal</b> access and interactive data representation) system incorporates two additional modalities (braille and review) that offer complementary benefits. It also provides blind users with the autonomy and control to interactively access and understand data visualizations. In a user study involving 11 blind participants, we found the MAIDR system facilitated the accurate interpretation of statistical visualizations. Participants exhibited a range of strategies in combining multiple modalities, influenced by their past interactions and experiences with data visualizations. This work accentuates the overlooked potential of combining refreshable tactile representation with other modalities and elevates the discussion on the importance of user autonomy when designing accessible data visualizations.</p></p class="citation"></blockquote><h2 id=csai-7>cs.AI (7)</h2><h3 id=17--173243-playing-nethack-with-llms-potential--limitations-as-zero-shot-agents-dominik-jeurissen-et-al-2024>(1/7 | 173/243) Playing NetHack with LLMs: Potential & Limitations as Zero-Shot Agents (Dominik Jeurissen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dominik Jeurissen, Diego Perez-Liebana, Jeremy Gow, Duygu Cakmak, James Kwan. (2024)<br><strong>Playing NetHack with LLMs: Potential & Limitations as Zero-Shot Agents</strong><br><button class=copy-to-clipboard title="Playing NetHack with LLMs: Potential & Limitations as Zero-Shot Agents" index=173>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-173 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 40<br>Keywords: Zero-shot, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00690v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00690v1.pdf filename=2403.00690v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have shown great success as high-level planners for <b>zero-shot</b> game-playing agents. However, these agents are primarily evaluated on Minecraft, where long-term planning is relatively straightforward. In contrast, agents tested in dynamic robot environments face limitations due to simplistic environments with only a few objects and interactions. To fill this gap in the literature, we present NetPlay, the first <b>LLM-powered</b> <b>zero-shot</b> agent for the challenging roguelike NetHack. NetHack is a particularly challenging environment due to its diverse set of items and monsters, complex interactions, and many ways to die. NetPlay uses an architecture designed for dynamic robot environments, modified for NetHack. Like previous approaches, it <b>prompts</b> the <b>LLM</b> to choose from predefined skills and tracks past interactions to enhance decision-making. Given NetHack&rsquo;s unpredictable nature, NetPlay detects important game events to interrupt running skills, enabling it to react to unforeseen circumstances. While NetPlay demonstrates considerable flexibility and proficiency in interacting with NetHack&rsquo;s mechanics, it struggles with ambiguous task descriptions and a lack of explicit feedback. Our findings demonstrate that NetPlay performs best with detailed context information, indicating the necessity for dynamic methods in supplying context information for complex games such as NetHack.</p></p class="citation"></blockquote><h3 id=27--174243-deep-reinforcement-learning-for-solving-management-problems-towards-a-large-management-mode-jinyang-jiang-et-al-2024>(2/7 | 174/243) Deep Reinforcement Learning for Solving Management Problems: Towards A Large Management Mode (Jinyang Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinyang Jiang, Xiaotian Liu, Tao Ren, Qinghao Wang, Yi Zheng, Yufu Du, Yijie Peng, Cheng Zhang. (2024)<br><strong>Deep Reinforcement Learning for Solving Management Problems: Towards A Large Management Mode</strong><br><button class=copy-to-clipboard title="Deep Reinforcement Learning for Solving Management Problems: Towards A Large Management Mode" index=174>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-174 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LG, cs.AI<br>Keyword Score: 30<br>Keywords: Recommendation, Reinforcement Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00318v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00318v1.pdf filename=2403.00318v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce a deep <b>reinforcement</b> <b>learning</b> (DRL) approach for solving management problems including inventory management, dynamic pricing, and <b>recommendation.</b> This DRL approach has the potential to lead to a large management model based on certain <b>transformer</b> neural network structures, resulting in an artificial general intelligence paradigm for various management tasks. Traditional methods have limitations for solving complex real-world problems, and we demonstrate how DRL can surpass existing heuristic approaches for solving management tasks. We aim to solve the problems in a unified framework, considering the interconnections between different tasks. Central to our methodology is the development of a foundational decision model coordinating decisions across the different domains through generative decision-making. Our experimental results affirm the effectiveness of our DRL-based framework in complex and dynamic business environments. This work opens new pathways for the application of DRL in management problems, highlighting its potential to revolutionize traditional business management.</p></p class="citation"></blockquote><h3 id=37--175243-even-ifs-from-if-onlys-are-the-best-semi-factual-explanations-found-using-counterfactuals-as-guides-saugat-aryal-et-al-2024>(3/7 | 175/243) Even-Ifs From If-Onlys: Are the Best Semi-Factual Explanations Found Using Counterfactuals As Guides? (Saugat Aryal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Saugat Aryal, Mark T. Keane. (2024)<br><strong>Even-Ifs From If-Onlys: Are the Best Semi-Factual Explanations Found Using Counterfactuals As Guides?</strong><br><button class=copy-to-clipboard title="Even-Ifs From If-Onlys: Are the Best Semi-Factual Explanations Found Using Counterfactuals As Guides?" index=175>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-175 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 25<br>Keywords: Black Box, Counter-factual, Explainable AI<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00980v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00980v1.pdf filename=2403.00980v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, <b>counterfactuals</b> using &ldquo;if-only&rdquo; explanations have become very popular in <b>eXplainable</b> <b>AI</b> (XAI), as they describe which changes to feature-inputs of a <b>black-box</b> <b>AI</b> system result in changes to a (usually negative) decision-outcome. Even more recently, semi-factuals using &ldquo;even-if&rdquo; explanations have gained more attention. They elucidate the feature-input changes that do \textit{not} change the decision-outcome of the AI system, with a potential to suggest more beneficial recourses. Some semi-factual methods use <b>counterfactuals</b> to the query-instance to guide semi-factual production (so-called <b>counterfactual-guided</b> methods), whereas others do not (so-called <b>counterfactual-free</b> methods). In this work, we perform comprehensive tests of 8 semi-factual methods on 7 datasets using 5 key metrics, to determine whether <b>counterfactual</b> guidance is necessary to find the best semi-factuals. The results of these tests suggests not, but rather that computing other aspects of the decision space lead to better semi-factual XAI.</p></p class="citation"></blockquote><h3 id=47--176243-softened-symbol-grounding-for-neuro-symbolic-systems-zenan-li-et-al-2024>(4/7 | 176/243) Softened Symbol Grounding for Neuro-symbolic Systems (Zenan Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zenan Li, Yuan Yao, Taolue Chen, Jingwei Xu, Chun Cao, Xiaoxing Ma, Jian Lü. (2024)<br><strong>Softened Symbol Grounding for Neuro-symbolic Systems</strong><br><button class=copy-to-clipboard title="Softened Symbol Grounding for Neuro-symbolic Systems" index=176>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-176 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LG, cs.AI<br>Keyword Score: 20<br>Keywords: Grounding, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00323v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00323v1.pdf filename=2403.00323v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neuro-symbolic learning generally consists of two separated worlds, i.e., neural network training and symbolic constraint solving, whose success hinges on symbol <b>grounding,</b> a fundamental problem in AI. This paper presents a novel, softened symbol <b>grounding</b> process, bridging the gap between the two worlds, and resulting in an effective and efficient neuro-symbolic learning framework. Technically, the framework features (1) modeling of symbol solution states as a Boltzmann distribution, which avoids expensive state searching and facilitates mutually beneficial interactions between network training and symbolic <b>reasoning;(2)</b> a new MCMC technique leveraging projection and SMT solvers, which efficiently samples from disconnected symbol solution spaces; (3) an annealing mechanism that can escape from %being trapped into sub-optimal symbol <b>groundings.</b> Experiments with three representative neuro symbolic learning tasks demonstrate that, owining to its superior symbol <b>grounding</b> capability, our framework successfully solves problems well beyond the frontier of the existing proposals.</p></p class="citation"></blockquote><h3 id=57--177243-a-survey-of-route-recommendations-methods-applications-and-opportunities-shiming-zhang-et-al-2024>(5/7 | 177/243) A Survey of Route Recommendations: Methods, Applications, and Opportunities (Shiming Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shiming Zhang, Zhipeng Luo, Li Yang, Fei Teng, Tianrui Li. (2024)<br><strong>A Survey of Route Recommendations: Methods, Applications, and Opportunities</strong><br><button class=copy-to-clipboard title="A Survey of Route Recommendations: Methods, Applications, and Opportunities" index=177>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-177 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LG, cs.AI<br>Keyword Score: 13<br>Keywords: Multi-modal, Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00284v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00284v1.pdf filename=2403.00284v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Nowadays, with advanced information technologies deployed citywide, large data volumes and powerful computational resources are intelligentizing modern city development. As an important part of intelligent transportation, route <b>recommendation</b> and its applications are widely used, directly influencing citizens` travel habits. Developing smart and efficient travel routes based on big data (possibly <b>multi-modal)</b> has become a central challenge in route <b>recommendation</b> research. Our survey offers a comprehensive review of route <b>recommendation</b> work based on urban computing. It is organized by the following three parts: 1) Methodology-wise. We categorize a large volume of traditional machine learning and modern deep learning methods. Also, we discuss their historical relations and reveal the edge-cutting progress. 2) Application-wise. We present numerous novel applications related to route commendation within urban computing scenarios. 3) We discuss current problems and challenges and envision several promising research directions. We believe that this survey can help relevant researchers quickly familiarize themselves with the current state of route <b>recommendation</b> research and then direct them to future research trends.</p></p class="citation"></blockquote><h3 id=67--178243-know-your-exceptions-towards-an-ontology-of-exceptions-in-knowledge-representation-gabriele-sacco-et-al-2024>(6/7 | 178/243) Know your exceptions: Towards an Ontology of Exceptions in Knowledge Representation (Gabriele Sacco et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gabriele Sacco, Loris Bozzato, Oliver Kutz. (2024)<br><strong>Know your exceptions: Towards an Ontology of Exceptions in Knowledge Representation</strong><br><button class=copy-to-clipboard title="Know your exceptions: Towards an Ontology of Exceptions in Knowledge Representation" index=178>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-178 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: I-2-4, cs-AI, cs.AI<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00685v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00685v2.pdf filename=2403.00685v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Defeasible <b>reasoning</b> is a kind of <b>reasoning</b> where some generalisations may not be valid in all circumstances, that is general conclusions may fail in some cases. Various formalisms have been developed to model this kind of <b>reasoning,</b> which is characteristic of common-sense contexts. However, it is not easy for a modeller to choose among these systems the one that better fits its domain from an ontological point of view. In this paper we first propose a framework based on the notions of exceptionality and defeasibility in order to be able to compare formalisms and reveal their ontological commitments. Then, we apply this framework to compare four systems, showing the differences that may occur from an ontological perspective.</p></p class="citation"></blockquote><h3 id=77--179243-axe-the-x-in-xai-a-plea-for-understandable-ai-andrés-páez-2024>(7/7 | 179/243) Axe the X in XAI: A Plea for Understandable AI (Andrés Páez, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andrés Páez. (2024)<br><strong>Axe the X in XAI: A Plea for Understandable AI</strong><br><button class=copy-to-clipboard title="Axe the X in XAI: A Plea for Understandable AI" index=179>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-179 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LG, cs.AI<br>Keyword Score: 10<br>Keywords: Explainable AI<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00315v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00315v1.pdf filename=2403.00315v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In a recent paper, Erasmus et al. (2021) defend the idea that the ambiguity of the term &ldquo;explanation&rdquo; in <b>explainable</b> <b>AI</b> (XAI) can be solved by adopting any of four different extant accounts of explanation in the philosophy of science: the Deductive Nomological, Inductive Statistical, Causal Mechanical, and New Mechanist models. In this chapter, I show that the authors&rsquo; claim that these accounts can be applied to deep neural networks as they would to any natural phenomenon is mistaken. I also provide a more general argument as to why the notion of explainability as it is currently used in the XAI literature bears little resemblance to the traditional concept of scientific explanation. It would be more fruitful to use the label &ldquo;understandable AI&rdquo; to avoid the confusion that surrounds the goal and purposes of XAI. In the second half of the chapter, I argue for a pragmatic conception of understanding that is better suited to play the central role attributed to explanation in XAI. Following Kuorikoski & Ylikoski (2015), the conditions of satisfaction for understanding an ML system are fleshed out in terms of an agent&rsquo;s success in using the system, in drawing correct inferences from it.</p></p class="citation"></blockquote><h2 id=eessas-2>eess.AS (2)</h2><h3 id=12--180243-efficient-adapter-tuning-of-pre-trained-speech-models-for-automatic-speaker-verification-mufan-sang-et-al-2024>(1/2 | 180/243) Efficient Adapter Tuning of Pre-trained Speech Models for Automatic Speaker Verification (Mufan Sang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mufan Sang, John H. L. Hansen. (2024)<br><strong>Efficient Adapter Tuning of Pre-trained Speech Models for Automatic Speaker Verification</strong><br><button class=copy-to-clipboard title="Efficient Adapter Tuning of Pre-trained Speech Models for Automatic Speaker Verification" index=180>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-180 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-LG, cs-SD, eess-AS, eess.AS<br>Keyword Score: 40<br>Keywords: Fine-tuning, Self-supervised Learning, Transfer Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00293v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00293v1.pdf filename=2403.00293v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With excellent generalization ability, <b>self-supervised</b> speech models have shown impressive performance on various downstream speech tasks in the pre-training and <b>fine-tuning</b> paradigm. However, as the growing size of pre-trained models, <b>fine-tuning</b> becomes practically unfeasible due to heavy computation and storage overhead, as well as the risk of overfitting. Adapters are lightweight modules inserted into pre-trained models to facilitate parameter-efficient adaptation. In this paper, we propose an effective adapter framework designed for adapting <b>self-supervised</b> speech models to the speaker verification task. With a parallel adapter design, our proposed framework inserts two types of adapters into the pre-trained model, allowing the adaptation of latent features within intermediate <b>Transformer</b> layers and output embeddings from all <b>Transformer</b> layers. We conduct comprehensive experiments to validate the efficiency and effectiveness of the proposed framework. Experimental results on the VoxCeleb1 dataset demonstrate that the proposed adapters surpass <b>fine-tuning</b> and other parameter-efficient <b>transfer</b> <b>learning</b> methods, achieving superior performance while updating only 5% of the parameters.</p></p class="citation"></blockquote><h3 id=22--181243-the-impact-of-frequency-bands-on-acoustic-anomaly-detection-of-machines-using-deep-learning-based-model-tin-nguyen-et-al-2024>(2/2 | 181/243) The Impact of Frequency Bands on Acoustic Anomaly Detection of Machines using Deep Learning Based Model (Tin Nguyen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tin Nguyen, Lam Pham, Phat Lam, Dat Ngo, Hieu Tang, Alexander Schindler. (2024)<br><strong>The Impact of Frequency Bands on Acoustic Anomaly Detection of Machines using Deep Learning Based Model</strong><br><button class=copy-to-clipboard title="The Impact of Frequency Bands on Acoustic Anomaly Detection of Machines using Deep Learning Based Model" index=181>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-181 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-SD, eess-AS, eess.AS<br>Keyword Score: 23<br>Keywords: Anomaly Detection, Benchmarking, Data Augmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00379v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00379v1.pdf filename=2403.00379v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we propose a deep learning based model for Acoustic <b>Anomaly</b> <b>Detection</b> of Machines, the task for detecting abnormal machines by analysing the machine sound. By conducting extensive experiments, we indicate that multiple techniques of pseudo audios, audio segment, <b>data</b> <b>augmentation,</b> Mahalanobis distance, and narrow frequency bands, which mainly focus on feature engineering, are effective to enhance the system performance. Among the evaluating techniques, the narrow frequency bands presents a significant impact. Indeed, our proposed model, which focuses on the narrow frequency bands, outperforms the DCASE baseline on the <b>benchmark</b> dataset of DCASE 2022 Task 2 Development set. The important role of the narrow frequency bands indicated in this paper inspires the research community on the task of Acoustic <b>Anomaly</b> <b>Detection</b> of Machines to further investigate and propose novel network architectures focusing on the frequency bands.</p></p class="citation"></blockquote><h2 id=csir-7>cs.IR (7)</h2><h3 id=17--182243-generalized-user-representations-for-transfer-learning-ghazal-fazelnia-et-al-2024>(1/7 | 182/243) Generalized User Representations for Transfer Learning (Ghazal Fazelnia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ghazal Fazelnia, Sanket Gupta, Claire Keum, Mark Koh, Ian Anderson, Mounia Lalmas. (2024)<br><strong>Generalized User Representations for Transfer Learning</strong><br><button class=copy-to-clipboard title="Generalized User Representations for Transfer Learning" index=182>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-182 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs-LG, cs.IR<br>Keyword Score: 35<br>Keywords: Autoencoder, Recommender System, Representation Learning, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00584v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00584v1.pdf filename=2403.00584v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a novel framework for user <b>representation</b> <b>in</b> large-scale <b>recommender</b> <b>systems,</b> aiming at effectively representing diverse user taste in a generalized manner. Our approach employs a two-stage methodology combining <b>representation</b> <b>learning</b> and <b>transfer</b> <b>learning.</b> The <b>representation</b> <b>learning</b> model uses an <b>autoencoder</b> that compresses various user features into a <b>representation</b> <b>space.</b> In the second stage, downstream task-specific models leverage user <b>representations</b> <b>via</b> <b>transfer</b> <b>learning</b> instead of curating user features individually. We further augment this methodology on the <b>representation&rsquo;s</b> <b>input</b> features to increase flexibility and enable reaction to user events, including new user experiences, in Near-Real Time. Additionally, we propose a novel solution to manage deployment of this framework in production models, allowing downstream models to work independently. We validate the performance of our framework through rigorous offline and online experiments within a large-scale system, showcasing its remarkable efficacy across multiple evaluation tasks. Finally, we show how the proposed framework can significantly reduce infrastructure costs compared to alternative approaches.</p></p class="citation"></blockquote><h3 id=27--183243-end-to-end-graph-sequential-representation-learning-for-accurate-recommendations-vladimir-baikalov-et-al-2024>(2/7 | 183/243) End-to-end Graph-Sequential Representation Learning for Accurate Recommendations (Vladimir Baikalov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vladimir Baikalov, Evgeny Frolov. (2024)<br><strong>End-to-end Graph-Sequential Representation Learning for Accurate Recommendations</strong><br><button class=copy-to-clipboard title="End-to-end Graph-Sequential Representation Learning for Accurate Recommendations" index=183>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-183 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-AI, cs-IR, cs-LG, cs.IR<br>Keyword Score: 28<br>Keywords: Graph, Recommendation, Recommender System, Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00895v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00895v1.pdf filename=2403.00895v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Many recent advancements in <b>recommender</b> <b>systems</b> have focused on developing sequence-based and <b>graph-based</b> approaches. Both approaches proved useful in modeling intricate relationships within behavioral data, leading to promising outcomes in personalized ranking and next-item <b>recommendation</b> tasks while maintaining good scalability. However, they capture very different signals from data. While the former approach represents users directly through ordered interactions with recent items, the latter one aims to capture indirect dependencies across the interactions <b>graph.</b> This paper presents a novel multi-representational learning framework that exploits the synergies between these two paradigms. Our empirical evaluation on several datasets demonstrates that mutual training of sequential and <b>graph</b> components with the proposed framework significantly improves <b>recommendations</b> performance.</p></p class="citation"></blockquote><h3 id=37--184243-an-interpretable-ensemble-of-graph-and-language-models-for-improving-search-relevance-in-e-commerce-nurendra-choudhary-et-al-2024>(3/7 | 184/243) An Interpretable Ensemble of Graph and Language Models for Improving Search Relevance in E-Commerce (Nurendra Choudhary et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nurendra Choudhary, Edward W Huang, Karthik Subbian, Chandan K. Reddy. (2024)<br><strong>An Interpretable Ensemble of Graph and Language Models for Improving Search Relevance in E-Commerce</strong><br><button class=copy-to-clipboard title="An Interpretable Ensemble of Graph and Language Models for Improving Search Relevance in E-Commerce" index=184>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-184 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: H-3-3; I-2-7; J-7, cs-CL, cs-IR, cs.IR<br>Keyword Score: 23<br>Keywords: Graph, Graph Neural Network, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00923v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00923v1.pdf filename=2403.00923v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The problem of search relevance in the E-commerce domain is a challenging one since it involves understanding the intent of a user&rsquo;s short nuanced query and matching it with the appropriate products in the catalog. This problem has traditionally been addressed using language models (LMs) and <b>graph</b> <b>neural</b> <b>networks</b> <b>(GNNs)</b> to capture semantic and inter-product behavior signals, respectively. However, the rapid development of new architectures has created a gap between research and the practical adoption of these techniques. Evaluating the generalizability of these models for deployment requires extensive experimentation on complex, real-world datasets, which can be non-trivial and expensive. Furthermore, such models often operate on latent space representations that are incomprehensible to humans, making it difficult to evaluate and compare the effectiveness of different models. This lack of interpretability hinders the development and adoption of new techniques in the field. To bridge this gap, we propose Plug and Play <b>Graph</b> <b>LAnguage</b> <b>Model</b> (PP-GLAM), an explainable ensemble of plug and play models. Our approach uses a modular framework with uniform data processing pipelines. It employs additive explanation metrics to independently decide whether to include (i) language model candidates, (ii) <b>GNN</b> model candidates, and (iii) inter-product behavioral signals. For the task of search relevance, we show that PP-GLAM outperforms several state-of-the-art baselines as well as a proprietary model on real-world multilingual, multi-regional e-commerce datasets. To promote better model comprehensibility and adoption, we also provide an analysis of the explainability and computational complexity of our model. We also provide the public codebase and provide a deployment strategy for practical implementation.</p></p class="citation"></blockquote><h3 id=47--185243-iai-moviebot-20-an-enhanced-research-platform-with-trainable-neural-components-and-transparent-user-modeling-nolwenn-bernard-et-al-2024>(4/7 | 185/243) IAI MovieBot 2.0: An Enhanced Research Platform with Trainable Neural Components and Transparent User Modeling (Nolwenn Bernard et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nolwenn Bernard, Ivica Kostric, Krisztian Balog. (2024)<br><strong>IAI MovieBot 2.0: An Enhanced Research Platform with Trainable Neural Components and Transparent User Modeling</strong><br><button class=copy-to-clipboard title="IAI MovieBot 2.0: An Enhanced Research Platform with Trainable Neural Components and Transparent User Modeling" index=185>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-185 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 20<br>Keywords: Recommender System, Natural Language Understanding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00520v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00520v1.pdf filename=2403.00520v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While interest in conversational <b>recommender</b> <b>systems</b> has been on the rise, operational systems suitable for serving as research platforms for comprehensive studies are currently lacking. This paper introduces an enhanced version of the IAI MovieBot conversational movie <b>recommender</b> <b>system,</b> aiming to evolve it into a robust and adaptable platform for conducting user-facing experiments. The key highlights of this enhancement include the addition of trainable neural components for <b>natural</b> <b>language</b> <b>understanding</b> and dialogue policy, transparent and explainable modeling of user preferences, along with improvements in the user interface and research infrastructure.</p></p class="citation"></blockquote><h3 id=57--186243-recommending-target-actions-outside-sessions-in-the-data-poor-insurance-domain-simone-borg-bruun-et-al-2024>(5/7 | 186/243) Recommending Target Actions Outside Sessions in the Data-poor Insurance Domain (Simone Borg Bruun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Simone Borg Bruun, Christina Lioma, Maria Maistro. (2024)<br><strong>Recommending Target Actions Outside Sessions in the Data-poor Insurance Domain</strong><br><button class=copy-to-clipboard title="Recommending Target Actions Outside Sessions in the Data-poor Insurance Domain" index=186>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-186 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 20<br>Keywords: Recommendation, Recurrent Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00368v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00368v1.pdf filename=2403.00368v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Providing personalized <b>recommendations</b> for insurance products is particularly challenging due to the intrinsic and distinctive features of the insurance domain. First, unlike more traditional domains like retail, movie etc., a large amount of user feedback is not available and the item catalog is smaller. Second, due to the higher complexity of products, the majority of users still prefer to complete their purchases over the phone instead of online. We present different recommender models to address such data scarcity in the insurance domain. We use <b>recurrent</b> <b>neural</b> <b>networks</b> with 3 different types of loss functions and architectures (cross-entropy, censored Weibull, attention). Our models cope with data scarcity by learning from multiple sessions and different types of user actions. Moreover, differently from previous session-based models, our models learn to predict a target action that does not happen within the session. Our models outperform state-of-the-art baselines on a real-world insurance dataset, with ca. 44K users, 16 items, 54K purchases and 117K sessions. Moreover, combining our models with demographic data boosts the performance. Analysis shows that considering multiple sessions and several types of actions are both beneficial for the models, and that our models are not unfair with respect to age, gender and income.</p></p class="citation"></blockquote><h3 id=67--187243-open-assistant-toolkit----version-2-sophie-fischer-et-al-2024>(6/7 | 187/243) Open Assistant Toolkit &ndash; version 2 (Sophie Fischer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sophie Fischer, Federico Rossetto, Carlos Gemmell, Andrew Ramsay, Iain Mackie, Philip Zubel, Niklas Tecklenburg, Jeffrey Dalton. (2024)<br><strong>Open Assistant Toolkit &ndash; version 2</strong><br><button class=copy-to-clipboard title="Open Assistant Toolkit -- version 2" index=187>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-187 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 16<br>Keywords: Multi-modal, Multi-modal, Code Generation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00586v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00586v1.pdf filename=2403.00586v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present the second version of the Open Assistant Toolkit (OAT-v2), an open-source task-oriented conversational system for composing generative neural models. OAT-v2 is a scalable and flexible assistant platform supporting multiple domains and modalities of user interaction. It splits processing a user utterance into modular system components, including submodules such as action <b>code</b> <b>generation,</b> <b>multimodal</b> content retrieval, and knowledge-augmented response generation. Developed over multiple years of the Alexa TaskBot challenge, OAT-v2 is a proven system that enables scalable and robust experimentation in experimental and real-world deployment. OAT-v2 provides open models and software for research and commercial applications to enable the future of <b>multimodal</b> virtual assistants across diverse applications and types of rich interaction.</p></p class="citation"></blockquote><h3 id=77--188243-dual-granularity-medication-recommendation-based-on-causal-inference-shunpan-liang-et-al-2024>(7/7 | 188/243) Dual-Granularity Medication Recommendation Based on Causal Inference (Shunpan Liang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shunpan Liang, Xiang Li, Xiang Li, Chen Li, Yu Lei, Yulei Hou, Tengfei Ma. (2024)<br><strong>Dual-Granularity Medication Recommendation Based on Causal Inference</strong><br><button class=copy-to-clipboard title="Dual-Granularity Medication Recommendation Based on Causal Inference" index=188>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-188 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-AI, cs-IR, cs.IR<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00880v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00880v1.pdf filename=2403.00880v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As medical demands grow and machine learning technology advances, AI-based diagnostic and treatment systems are garnering increasing attention. Medication <b>recommendation</b> aims to integrate patients&rsquo; long-term health records with medical knowledge, recommending accuracy and safe medication combinations for specific conditions. However, most existing researches treat medication <b>recommendation</b> systems merely as variants of traditional <b>recommendation</b> systems, overlooking the heterogeneity between medications and diseases. To address this challenge, we propose DGMed, a framework for medication <b>recommendation.</b> DGMed utilizes causal inference to uncover the connections among medical entities and presents an innovative feature alignment method to tackle heterogeneity issues. Specifically, this study first applies causal inference to analyze the quantified therapeutic effects of medications on specific diseases from historical records, uncovering potential links between medical entities. Subsequently, we integrate molecular-level knowledge, aligning the embeddings of medications and diseases within the molecular space to effectively tackle their heterogeneity. Ultimately, based on relationships at the entity level, we adaptively adjust the <b>recommendation</b> probabilities of medication and recommend medication combinations according to the patient&rsquo;s current health condition. Experimental results on a real-world dataset show that our method surpasses existing state-of-the-art baselines in four evaluation metrics, demonstrating superior performance in both accuracy and safety aspects. Compared to the sub-optimal model, our approach improved accuracy by 4.40%, reduced the risk of side effects by 6.14%, and increased time efficiency by 47.15%.</p></p class="citation"></blockquote><h2 id=eesssp-2>eess.SP (2)</h2><h3 id=12--189243-toward-autonomous-cooperation-in-heterogeneous-nanosatellite-constellations-using-dynamic-graph-neural-networks-guillem-casadesus-vila-et-al-2024>(1/2 | 189/243) Toward Autonomous Cooperation in Heterogeneous Nanosatellite Constellations Using Dynamic Graph Neural Networks (Guillem Casadesus-Vila et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guillem Casadesus-Vila, Joan-Adria Ruiz-de-Azua, Eduard Alarcon. (2024)<br><strong>Toward Autonomous Cooperation in Heterogeneous Nanosatellite Constellations Using Dynamic Graph Neural Networks</strong><br><button class=copy-to-clipboard title="Toward Autonomous Cooperation in Heterogeneous Nanosatellite Constellations Using Dynamic Graph Neural Networks" index=189>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-189 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SP<br>Categories: cs-AI, cs-NI, eess-SP, eess.SP<br>Keyword Score: 33<br>Keywords: Graph, Graph Neural Network, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00692v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00692v2.pdf filename=2403.00692v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The upcoming landscape of Earth Observation missions will defined by networked heterogeneous nanosatellite constellations required to meet strict mission requirements, such as revisit times and spatial resolution. However, scheduling satellite communications in these satellite networks through efficiently creating a global satellite Contact Plan (CP) is a complex task, with current solutions requiring ground-based coordination or being limited by onboard computational resources. The paper proposes a novel approach to overcome these challenges by modeling the constellations and CP as dynamic networks and employing <b>graph-based</b> <b>techniques.</b> <b>The</b> proposed method utilizes a state-of-the-art dynamic <b>graph</b> <b>neural</b> <b>network</b> to evaluate the performance of a given CP and update it using a heuristic algorithm based on simulated annealing. The trained neural network can predict the network delay with a mean absolute error of 3.6 minutes. <b>Simulation</b> results show that the proposed method can successfully design a contact plan for large satellite networks, improving the delay by 29.1%, similar to a traditional approach, while performing the objective evaluations 20x faster.</p></p class="citation"></blockquote><h3 id=22--190243-diffraction-and-scattering-aware-radio-map-and-environment-reconstruction-using-geometry-model-assisted-deep-learning-wangqian-chen-et-al-2024>(2/2 | 190/243) Diffraction and Scattering Aware Radio Map and Environment Reconstruction using Geometry Model-Assisted Deep Learning (Wangqian Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wangqian Chen, Junting Chen. (2024)<br><strong>Diffraction and Scattering Aware Radio Map and Environment Reconstruction using Geometry Model-Assisted Deep Learning</strong><br><button class=copy-to-clipboard title="Diffraction and Scattering Aware Radio Map and Environment Reconstruction using Geometry Model-Assisted Deep Learning" index=190>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-190 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SP<br>Categories: cs-LG, eess-SP, eess.SP<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00229v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00229v1.pdf filename=2403.00229v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Machine learning (ML) facilitates rapid channel modeling for 5G and beyond wireless communication systems. Many existing ML techniques utilize a city map to construct the radio map; however, an updated city map may not always be available. This paper proposes to employ the received signal strength (RSS) data to jointly construct the radio map and the virtual environment by exploiting the <b>geometry</b> structure of the environment. In contrast to many existing ML approaches that lack of an environment model, we develop a virtual obstacle model and characterize the <b>geometry</b> relation between the propagation paths and the virtual obstacles. A multi-screen knife-edge model is adopted to extract the key diffraction features, and these features are fed into a neural network (NN) for diffraction representation. To describe the scattering, as oppose to most existing methods that directly input an entire city map, our model focuses on the <b>geometry</b> structure from the local area surrounding the TX-RX pair and the spatial invariance of such local <b>geometry</b> structure is exploited. Numerical experiments demonstrate that, in addition to reconstructing a 3D virtual environment, the proposed model outperforms the state-of-the-art methods in radio map construction with 10%-18% accuracy improvements. It can also reduce 20% data and 50% training epochs when transferred to a new environment.</p></p class="citation"></blockquote><h2 id=csni-2>cs.NI (2)</h2><h3 id=12--191243-exploring-upper-6ghz-and-mmwave-in-real-world-5g-networks-a-direct-on-field-comparison-marcello-morini-et-al-2024>(1/2 | 191/243) Exploring Upper-6GHz and mmWave in Real-World 5G Networks: A Direct on-Field Comparison (Marcello Morini et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marcello Morini, Eugenio Moro, Ilario Filippini, Antonio Capone, Danilo De Donno. (2024)<br><strong>Exploring Upper-6GHz and mmWave in Real-World 5G Networks: A Direct on-Field Comparison</strong><br><button class=copy-to-clipboard title="Exploring Upper-6GHz and mmWave in Real-World 5G Networks: A Direct on-Field Comparison" index=191>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-191 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI, eess-SP<br>Keyword Score: 33<br>Keywords: Benchmarking, Simulation, Simulator, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00668v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00668v1.pdf filename=2403.00668v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The spectrum crunch challenge poses a vital threat to the progress of cellular networks and recently <b>prompted</b> the inclusion of millimeter wave (mmWave) and Upper 6GHz (U6G) in the 3GPP standards. These two bands promise to unlock a large portion of untapped spectrum, but the harsh propagation due to the increased carrier frequency might negatively impact the performance of urban Radio Access Network (RAN) deployments. Within the span of a year, two co-located 5G networks operating in these frequency bands were deployed at Politecnico di Milano, Milan, Italy, entirely dedicated to the dense urban performance assessment of the two systems. This paper presents an in-depth analysis of the measurement campaigns conducted on them, with the U6G campaign representing the first of its kind. A <b>benchmark</b> is provided by ray-tracing <b>simulations.</b> The results suggest that networks operating in these frequency bands provide good indoor and outdoor coverage and throughput in urban scenarios, even when deployed in the macro base station setup common to lower frequencies. In addition, a comparative performance analysis of these two key technologies is provided, offering insights on their relative strengths, weaknesses and improvement margins and informing on which bands is better suited for urban macro coverage.</p></p class="citation"></blockquote><h3 id=22--192243-comparative-study-of-simulators-for-vehicular-networks-rida-saghir-et-al-2024>(2/2 | 192/243) Comparative Study of Simulators for Vehicular Networks (Rida Saghir et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rida Saghir, Thenuka Karunathilake, Anna Förster. (2024)<br><strong>Comparative Study of Simulators for Vehicular Networks</strong><br><button class=copy-to-clipboard title="Comparative Study of Simulators for Vehicular Networks" index=192>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-192 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00546v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00546v1.pdf filename=2403.00546v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Vehicular Adhoc networks (VANETs) are composed of vehicles connected with wireless links to exchange data. VANETs have become the backbone of the Intelligent Transportation Systems (ITS) in smart cities and enable many essential services like roadside safety, traffic management, platooning, etc with vehicle-to-vehicle (V2V) and vehicle-to-infrastructure (V2I) communications. In any form of research testing and evaluation plays a crucial role. However, in VANETs, real-world experiments require high investment, and heavy resources and can cause many practical difficulties. Therefore, <b>simulations</b> have become critical and the primary way of evaluating VANETs&rsquo; applications. Furthermore, the upfront challenge is the realistic capture of the networking mechanism of VANETs, which varies from situation to situation. Several factors may contribute to the successful achievement of a random realistic networking behavior. However, the biggest dependency is a powerful tool for the implementation, which could probably take into account all the configuration parameters, loss factors, mobility schemes, and other key features of a VANET, yet give out practical performance metrics with a good trade-off between investment of resources and the results. Hence, the aim of this research is to evaluate some simulators in the scope of VANETs with respect to resource utilization, packet delivery, and computational time.</p></p class="citation"></blockquote><h2 id=csit-9>cs.IT (9)</h2><h3 id=19--193243-complex-valued-neural-network-based-federated-learning-for-multi-user-indoor-positioning-performance-optimization-hanzhi-yu-et-al-2024>(1/9 | 193/243) Complex-Valued Neural Network based Federated Learning for Multi-user Indoor Positioning Performance Optimization (Hanzhi Yu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hanzhi Yu, Mingzhe Chen, Yuchen Liu. (2024)<br><strong>Complex-Valued Neural Network based Federated Learning for Multi-user Indoor Positioning Performance Optimization</strong><br><button class=copy-to-clipboard title="Complex-Valued Neural Network based Federated Learning for Multi-user Indoor Positioning Performance Optimization" index=193>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-193 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 30<br>Keywords: Federated Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00665v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00665v1.pdf filename=2403.00665v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this article, the use of channel state information (CSI) for indoor positioning is studied. In the considered model, a server equipped with several antennas sends pilot signals to users, while each user uses the received pilot signals to estimate channel states for user positioning. To this end, we formulate the positioning problem as an optimization problem aiming to minimize the gap between the estimated positions and the ground truth positions of users. To solve this problem, we design a complex-valued neural network (CVNN) model based <b>federated</b> <b>learning</b> (FL) algorithm. Compared to standard real-valued centralized machine learning (ML) methods, our proposed algorithm has two main advantages. First, our proposed algorithm can directly process complex-valued CSI data without data transformation. Second, our proposed algorithm is a distributed ML method that does not require users to send their CSI data to the server. Since the output of our proposed algorithm is complex-valued which consists of the real and imaginary parts, we study the use of the CVNN to implement two learning tasks. First, the proposed algorithm directly outputs the estimated positions of a user. Here, the real and imaginary parts of an output neuron represent the 2D coordinates of the user. Second, the proposed method can output two CSI features (i.e., line-of-sight/non-line-of-sight transmission link classification and time of arrival (TOA) prediction) which can be used in traditional positioning algorithms. <b>Simulation</b> results demonstrate that our designed CVNN based FL can reduce the mean positioning error between the estimated position and the actual position by up to 36%, compared to a RVNN based FL which requires to transform CSI data into real-valued data.</p></p class="citation"></blockquote><h3 id=29--194243-federated-learning-via-lattice-joint-source-channel-coding-seyed-mohammad-azimi-abarghouyi-et-al-2024>(2/9 | 194/243) Federated Learning via Lattice Joint Source-Channel Coding (Seyed Mohammad Azimi-Abarghouyi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Seyed Mohammad Azimi-Abarghouyi, Lav R. Varshney. (2024)<br><strong>Federated Learning via Lattice Joint Source-Channel Coding</strong><br><button class=copy-to-clipboard title="Federated Learning via Lattice Joint Source-Channel Coding" index=194>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-194 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs-LG, cs.IT, math-IT<br>Keyword Score: 20<br>Keywords: Federated Learning, Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.01023v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.01023v1.pdf filename=2403.01023v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces a universal <b>federated</b> <b>learning</b> framework that enables over-the-air computation via digital communications, using a new joint source-channel coding scheme. Without relying on channel state information at devices, this scheme employs lattice codes to both <b>quantize</b> model parameters and exploit interference from the devices. A novel two-layer receiver structure at the server is designed to reliably decode an integer combination of the <b>quantized</b> model parameters as a lattice point for the purpose of aggregation. Numerical experiments validate the effectiveness of the proposed scheme. Even with the challenges posed by channel conditions and device heterogeneity, the proposed scheme markedly surpasses other over-the-air FL strategies.</p></p class="citation"></blockquote><h3 id=39--195243-on-non-interactive-simulation-of-distributed-sources-with-finite-alphabets-hojat-allah-salehi-et-al-2024>(3/9 | 195/243) On Non-Interactive Simulation of Distributed Sources with Finite Alphabets (Hojat Allah Salehi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hojat Allah Salehi, Farhad Shirani. (2024)<br><strong>On Non-Interactive Simulation of Distributed Sources with Finite Alphabets</strong><br><button class=copy-to-clipboard title="On Non-Interactive Simulation of Distributed Sources with Finite Alphabets" index=195>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-195 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-CR, cs-IT, cs.IT, eess-SP, math-IT, math-PR<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00989v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00989v1.pdf filename=2403.00989v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work presents a Fourier analysis framework for the non-interactive source <b>simulation</b> (NISS) problem. Two distributed agents observe a pair of sequences $X^d$ and $Y^d$ drawn according to a joint distribution $P_{X^dY^d}$. The agents aim to generate outputs $U=f_d(X^d)$ and $V=g_d(Y^d)$ with a joint distribution sufficiently close in total variation to a target distribution $Q_{UV}$. Existing works have shown that the NISS problem with finite-alphabet outputs is decidable. For the binary-output NISS, an upper-bound to the input complexity was derived which is $O(\exp\operatorname{poly}(\frac{1}{\epsilon}))$. In this work, the input complexity and algorithm design are addressed in several classes of NISS scenarios. For binary-output NISS scenarios with doubly-symmetric binary inputs, it is shown that the input complexity is $\Theta(\log{\frac{1}{\epsilon}})$, thus providing a super-exponential improvement in input complexity. An explicit characterization of the simulating pair of functions is provided. For general finite-input scenarios, a constructive algorithm is introduced that explicitly finds the simulating functions $(f_d(X^d),g_d(Y^d))$. The approach relies on a novel Fourier analysis framework. Various numerical <b>simulations</b> of NISS scenarios with IID inputs are provided. Furthermore, to illustrate the general applicability of the Fourier framework, several examples with non-IID inputs, including entanglement-assisted NISS and NISS with Markovian inputs are provided.</p></p class="citation"></blockquote><h3 id=49--196243-shortened-polar-codes-under-automorphism-ensemble-decoding-charles-pillet-et-al-2024>(4/9 | 196/243) Shortened Polar Codes under Automorphism Ensemble Decoding (Charles Pillet et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Charles Pillet, Ilshat Sagitov, Valerio Bioglio, Pascal Giard. (2024)<br><strong>Shortened Polar Codes under Automorphism Ensemble Decoding</strong><br><button class=copy-to-clipboard title="Shortened Polar Codes under Automorphism Ensemble Decoding" index=196>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-196 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00622v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00622v1.pdf filename=2403.00622v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we propose a low-latency decoding solution of shortened polar codes based on their automorphism groups. The automorphism group of shortened polar codes, designed according to two existing shortening patterns, are shown to be limited but non-empty, making the Automorphism Ensemble (AE) decoding of shortened polar codes possible. Extensive <b>simulation</b> results for shortened polar codes under AE are provided and are compared to the SC-List (SCL) algorithm. The block-error rate of shortened polar codes under AE matches or beats SCL while lowering the decoding latency.</p></p class="citation"></blockquote><h3 id=59--197243-impact-of-inter-operator-interference-via-reconfigurable-intelligent-surfaces-nikolaos-i-miridakis-et-al-2024>(5/9 | 197/243) Impact of Inter-Operator Interference via Reconfigurable Intelligent Surfaces (Nikolaos I. Miridakis et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nikolaos I. Miridakis, Theodoros A. Tsiftsis, Panagiotis A. Karkazis, Helen C. Leligou, Petar Popovski. (2024)<br><strong>Impact of Inter-Operator Interference via Reconfigurable Intelligent Surfaces</strong><br><button class=copy-to-clipboard title="Impact of Inter-Operator Interference via Reconfigurable Intelligent Surfaces" index=197>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-197 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00349v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00349v1.pdf filename=2403.00349v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A wireless communication system is studied that operates in the presence of multiple reconfigurable intelligent surfaces (RISs). In particular, a multi-operator environment is considered where each operator utilizes an RIS to enhance its communication quality. Although out-of-band interference does not exist (since each operator uses isolated spectrum resources), RISs controlled by different operators do affect the system performance of one another due to the inherently rapid phase shift adjustments that occur on an independent basis. The system performance of such a communication scenario is analytically studied for the practical case where discrete-only phase shifts occur at RIS. The proposed framework is quite general since it is valid under arbitrary channel fading conditions as well as the presence (or not) of the transceiver&rsquo;s direct link. Finally, the derived analytical results are verified via numerical and <b>simulation</b> trial as well as some novel and useful engineering outcomes are manifested.</p></p class="citation"></blockquote><h3 id=69--198243-deep-iot-downlink-enhanced-efficient-power-internet-of-things-yulin-shao-2024>(6/9 | 198/243) DEEP-IoT: Downlink-Enhanced Efficient-Power Internet of Things (Yulin Shao, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yulin Shao. (2024)<br><strong>DEEP-IoT: Downlink-Enhanced Efficient-Power Internet of Things</strong><br><button class=copy-to-clipboard title="DEEP-IoT: Downlink-Enhanced Efficient-Power Internet of Things" index=198>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-198 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs-LG, cs-SY, cs.IT, eess-SP, eess-SY, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00321v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00321v1.pdf filename=2403.00321v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>At the heart of the Internet of Things (IoT) &ndash; a domain witnessing explosive growth &ndash; the imperative for energy efficiency and the extension of device lifespans has never been more pressing. This paper presents DEEP-IoT, a revolutionary communication paradigm poised to redefine how IoT devices communicate. Through a pioneering &ldquo;listen more, transmit less&rdquo; strategy, DEEP-IoT challenges and transforms the traditional transmitter (IoT devices)-centric communication model to one where the receiver (the access point) play a pivotal role, thereby cutting down energy use and boosting device longevity. We not only conceptualize DEEP-IoT but also actualize it by integrating deep learning-enhanced feedback channel codes within a narrow-band system. <b>Simulation</b> results show a significant enhancement in the operational lifespan of IoT cells &ndash; surpassing traditional systems using Turbo and Polar codes by up to 52.71%. This leap signifies a paradigm shift in IoT communications, setting the stage for a future where IoT devices boast unprecedented efficiency and durability.</p></p class="citation"></blockquote><h3 id=79--199243-probabilistic-semantic-communication-over-wireless-networks-with-rate-splitting-zhouxiang-zhao-et-al-2024>(7/9 | 199/243) Probabilistic Semantic Communication over Wireless Networks with Rate Splitting (Zhouxiang Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhouxiang Zhao, Zhaohui Yang, Ye Hu, Qianqian Yang, Wei Xu, Zhaoyang Zhang. (2024)<br><strong>Probabilistic Semantic Communication over Wireless Networks with Rate Splitting</strong><br><button class=copy-to-clipboard title="Probabilistic Semantic Communication over Wireless Networks with Rate Splitting" index=199>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-199 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 13<br>Keywords: Graph, Information Compression<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00434v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00434v1.pdf filename=2403.00434v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, the problem of joint transmission and computation resource allocation for probabilistic semantic communication (PSC) system with rate splitting multiple access (RSMA) is investigated. In the considered model, the base station (BS) needs to transmit a large amount of data to multiple users with RSMA. Due to limited communication resources, the BS is required to utilize semantic communication techniques to compress the large-sized data. The semantic communication is enabled by shared probability <b>graphs</b> between the BS and the users. The probability <b>graph</b> can be used to further compress the transmission data at the BS, while the received compressed semantic <b>information</b> <b>can</b> be recovered through using the same shared probability <b>graph</b> at each user side. The semantic <b>information</b> <b>compression</b> progress consumes additional computation power at the BS, which inevitably decreases the transmission power due to limited total power budget. Considering both the effect of semantic compression ratio and computation power, the semantic rate expression for RSMA is first obtained. Then, based on the obtained rate expression, an optimization problem is formulated with the aim of maximizing the sum of semantic rates of all users under total power, semantic compression ratio, and rate allocation constraints. To tackle this problem, an iterative algorithm is proposed, where the rate allocation and transmit beamforming design subproblem is solved using a successive convex approximation method, and the semantic compression ratio subproblem is addressed using a greedy algorithm. Numerical results validate the effectiveness of the proposed scheme.</p></p class="citation"></blockquote><h3 id=89--200243-decentralized-uncoded-storage-elastic-computing-with-heterogeneous-computation-speeds-wenbo-huang-et-al-2024>(8/9 | 200/243) Decentralized Uncoded Storage Elastic Computing with Heterogeneous Computation Speeds (Wenbo Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenbo Huang, Xudong You, Kai Wan, Robert Caiming Qiu, Mingyue Ji. (2024)<br><strong>Decentralized Uncoded Storage Elastic Computing with Heterogeneous Computation Speeds</strong><br><button class=copy-to-clipboard title="Decentralized Uncoded Storage Elastic Computing with Heterogeneous Computation Speeds" index=200>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-200 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, math-IT<br>Keyword Score: 10<br>Keywords: MNIST<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00585v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00585v1.pdf filename=2403.00585v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Elasticity plays an important role in modern cloud computing systems. Elastic computing allows virtual machines (i.e., computing nodes) to be preempted when high-priority jobs arise, and also allows new virtual machines to participate in the computation. In 2018, Yang et al. introduced Coded Storage Elastic Computing (CSEC) to address the elasticity using coding technology, with lower storage and computation load requirements. However, CSEC is limited to certain types of computations (e.g., linear) due to the coded data storage based on linear coding. Then Centralized Uncoded Storage Elastic Computing (CUSEC) with heterogeneous computation speeds was proposed, which directly copies parts of data into the virtual machines. In all existing works in elastic computing, the storage assignment is centralized, meaning that the number and identity of all virtual machines possible used in the whole computation process are known during the storage assignment. In this paper, we consider Decentralized Uncoded Storage Elastic Computing (DUSEC) with heterogeneous computation speeds, where any available virtual machine can join the computation which is not predicted and thus coordination among different virtual machines&rsquo; storage assignments is not allowed. Under a decentralized storage assignment originally proposed in coded caching by Maddah-Ali and Niesen, we propose a computing scheme with closed-form optimal computation time. We also run experiments over <b>MNIST</b> dataset with Softmax regression model through the Tencent cloud platform, and the experiment results demonstrate that the proposed DUSEC system approaches the state-of-art best storage assignment in the CUSEC system in computation time.</p></p class="citation"></blockquote><h3 id=99--201243-nearest-neighbours-estimators-for-conditional-mutual-information-jake-witter-et-al-2024>(9/9 | 201/243) Nearest-Neighbours Estimators for Conditional Mutual Information (Jake Witter et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jake Witter, Conor Houghton. (2024)<br><strong>Nearest-Neighbours Estimators for Conditional Mutual Information</strong><br><button class=copy-to-clipboard title="Nearest-Neighbours Estimators for Conditional Mutual Information" index=201>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-201 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, math-IT<br>Keyword Score: 10<br>Keywords: Mutual Information<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00556v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00556v1.pdf filename=2403.00556v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The conditional <b>mutual</b> <b>information</b> quantifies the conditional dependence of two random variables. It has numerous applications; it forms, for example, part of the definition of transfer entropy, a common measure of the causal relationship between time series. It does, however, require a lot of data to estimate accurately and suffers the curse of dimensionality, limiting its application in machine learning and data science. However, the Kozachenko-Leonenko approach can address this problem: it is possible, in this approach to define a nearest-neighbour estimator which depends only on the distance between data points and not on the dimension of the data. Furthermore, the bias can be calculated analytically for this estimator. Here this estimator is described and is tested on simulated data.</p></p class="citation"></blockquote><h2 id=csar-1>cs.AR (1)</h2><h3 id=11--202243-neupims-npu-pim-heterogeneous-acceleration-for-batched-llm-inferencing-guseul-heo-et-al-2024>(1/1 | 202/243) NeuPIMs: NPU-PIM Heterogeneous Acceleration for Batched LLM Inferencing (Guseul Heo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guseul Heo, Sangyeop Lee, Jaehong Cho, Hyunmin Choi, Sanghyeon Lee, Hyungkyu Ham, Gwangsun Kim, Divya Mahajan, Jongse Park. (2024)<br><strong>NeuPIMs: NPU-PIM Heterogeneous Acceleration for Batched LLM Inferencing</strong><br><button class=copy-to-clipboard title="NeuPIMs: NPU-PIM Heterogeneous Acceleration for Batched LLM Inferencing" index=202>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-202 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AR<br>Categories: cs-AR, cs.AR<br>Keyword Score: 30<br>Keywords: Transformer, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00579v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00579v2.pdf filename=2403.00579v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Modern <b>transformer-based</b> <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> are constructed with a series of decoder blocks. Each block comprises three key components: (1) QKV generation, (2) multi-head attention, and (3) feed-forward networks. In batched processing, QKV generation and feed-forward networks involve compute-intensive matrix-matrix multiplications (GEMM), while multi-head attention requires bandwidth-heavy matrix-vector multiplications (GEMV). Machine learning accelerators like TPUs or NPUs are proficient in handling GEMM but are less efficient for GEMV computations. Conversely, Processing-in-Memory (PIM) technology is tailored for efficient GEMV computation, while it lacks the computational power to effectively handle GEMM. Inspired by this insight, we propose NeuPIMs, a heterogeneous accelerator-based system that jointly exploits a conventional GEMM-focused NPU and GEMV-optimized PIM devices. The main challenge in efficiently integrating NPU and PIM lies in enabling concurrent operations on both platforms, each addressing a specific kernel type. First, existing PIMs typically operate in a &ldquo;blocked&rdquo; mode, allowing only either NPU or PIM to be active at any given time. Second, the inherent dependencies between GEMM and GEMV in <b>LLMs</b> restrict their parallel processing. To tackle these challenges, NeuPIMs is equipped with dual row buffers in each bank, facilitating the simultaneous management of memory read/write operations and PIM commands. Further, NeuPIMs employs a runtime sub-batch interleaving technique to maximize concurrent execution, leveraging batch parallelism to allow two independent sub-batches to be pipelined within a single NeuPIMs node. Our evaluation demonstrates that compared to an NPU-only approach and a na"ive NPU-PIM integrated system, NeuPIMs achieves 2.3$\times$ and 1.6$\times$ throughput improvement, respectively.</p></p class="citation"></blockquote><h2 id=csds-4>cs.DS (4)</h2><h3 id=14--203243-analysis-of-phylogeny-tracking-algorithms-for-serial-and-multiprocess-applications-matthew-andres-moreno-et-al-2024>(1/4 | 203/243) Analysis of Phylogeny Tracking Algorithms for Serial and Multiprocess Applications (Matthew Andres Moreno et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Matthew Andres Moreno, Santiago Rodriguez Papa, Emily Dolson. (2024)<br><strong>Analysis of Phylogeny Tracking Algorithms for Serial and Multiprocess Applications</strong><br><button class=copy-to-clipboard title="Analysis of Phylogeny Tracking Algorithms for Serial and Multiprocess Applications" index=203>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-203 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs.DS, q-bio-PE<br>Keyword Score: 30<br>Keywords: Pruning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00246v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00246v2.pdf filename=2403.00246v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Since the advent of modern bioinformatics, the challenging, multifaceted problem of reconstructing phylogenetic history from biological sequences has hatched perennial statistical and algorithmic innovation. Studies of the phylogenetic dynamics of digital, agent-based evolutionary models motivate a peculiar converse question: how to best engineer tracking to facilitate fast, accurate, and memory-efficient lineage reconstructions? Here, we formally describe procedures for phylogenetic analysis in both serial and distributed computing scenarios. With respect to the former, we demonstrate reference-counting-based <b>pruning</b> of extinct lineages. For the latter, we introduce a trie-based phylogenetic reconstruction approach for &ldquo;hereditary stratigraphy&rdquo; genome annotations. This process allows phylogenetic relationships between genomes to be inferred by comparing their similarities, akin to reconstruction of natural history from biological DNA sequences. Phylogenetic analysis capabilities significantly advance distributed agent-based <b>simulations</b> as a tool for evolutionary research, and also benefit application-oriented evolutionary computing. Such tracing could extend also to other digital artifacts that proliferate through replication, like digital media and computer viruses.</p></p class="citation"></blockquote><h3 id=24--204243-algorithms-for-efficient-compact-online-data-stream-curation-matthew-andres-moreno-et-al-2024>(2/4 | 204/243) Algorithms for Efficient, Compact Online Data Stream Curation (Matthew Andres Moreno et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Matthew Andres Moreno, Santiago Rodriguez Papa, Emily Dolson. (2024)<br><strong>Algorithms for Efficient, Compact Online Data Stream Curation</strong><br><button class=copy-to-clipboard title="Algorithms for Efficient, Compact Online Data Stream Curation" index=204>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-204 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs.DS<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00266v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00266v1.pdf filename=2403.00266v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Data stream algorithms tackle operations on high-volume sequences of read-once data items. Data stream scenarios include inherently real-time systems like sensor networks and financial markets. They also arise in purely-computational scenarios like ordered traversal of big data or long-running iterative <b>simulations.</b> In this work, we develop methods to maintain running archives of stream data that are temporally representative, a task we call &ldquo;stream curation.&rdquo; Our approach contributes to rich existing literature on data stream binning, which we extend by providing stateless (i.e., non-iterative) curation schemes that enable key optimizations to trim archive storage overhead and streamline processing of incoming observations. We also broaden support to cover new trade-offs between curated archive size and temporal coverage. We present a suite of five stream curation algorithms that span $\mathcal{O}(n)$, $\mathcal{O}(\log n)$, and $\mathcal{O}(1)$ orders of growth for retained data items. Within each order of growth, algorithms are provided to maintain even coverage across history or bias coverage toward more recent time points. More broadly, memory-efficient stream curation can boost the data stream mining capabilities of low-grade hardware in roles such as sensor nodes and data logging devices.</p></p class="citation"></blockquote><h3 id=34--205243-undercomplete-decomposition-of-symmetric-tensors-in-linear-time-and-smoothed-analysis-of-the-condition-number-pascal-koiran-et-al-2024>(3/4 | 205/243) Undercomplete Decomposition of Symmetric Tensors in Linear Time, and Smoothed Analysis of the Condition Number (Pascal Koiran et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pascal Koiran, Subhayan Saha. (2024)<br><strong>Undercomplete Decomposition of Symmetric Tensors in Linear Time, and Smoothed Analysis of the Condition Number</strong><br><button class=copy-to-clipboard title="Undercomplete Decomposition of Symmetric Tensors in Linear Time, and Smoothed Analysis of the Condition Number" index=205>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-205 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: 68W20, 68W40, 65F35, 15A69, F-2-1; G-1-3, cs-CC, cs-DS, cs-NA, cs.DS, math-NA<br>Keyword Score: 15<br>Keywords: Black Box, Tensor Decomposition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00643v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00643v1.pdf filename=2403.00643v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study symmetric <b>tensor</b> <b>decompositions,</b> i.e., decompositions of the form $T = \sum_{i=1}^r u_i^{\otimes 3}$ where $T$ is a symmetric <b>tensor</b> <b>of</b> order 3 and $u_i \in \mathbb{C}^n$.In order to obtain efficient decomposition algorithms, it is necessary to require additional properties from $u_i$. In this paper we assume that the $u_i$ are linearly independent. This implies $r \leq n$,that is, the decomposition of T is undercomplete. We give a randomized algorithm for the following problem in the exact arithmetic model of computation: Let $T$ be an order-3 symmetric <b>tensor</b> <b>that</b> has an undercomplete decomposition.Then given some $T&rsquo;$ close to $T$, an accuracy parameter $\varepsilon$, and an upper bound B on the condition number of the <b>tensor,</b> <b>output</b> vectors $u&rsquo;_i$ such that $||u_i - u&rsquo;_i|| \leq \varepsilon$ (up to permutation and multiplication by cube roots of unity) with high probability. The main novel features of our algorithm are: 1) We provide the first algorithm for this problem that runs in linear time in the size of the input <b>tensor.</b> <b>More</b> specifically, it requires $O(n^3)$ arithmetic operations for all accuracy parameters $\varepsilon =$ 1/poly(n) and B = poly(n). 2) Our algorithm is robust, that is, it can handle inverse-quasi-polynomial noise (in $n$,B,$\frac{1}{\varepsilon}$) in the input <b>tensor.</b> <b>3)</b> We present a smoothed analysis of the condition number of the <b>tensor</b> <b>decomposition</b> problem. This guarantees that the condition number is low with high probability and further shows that our algorithm runs in linear time, except for some rare badly conditioned inputs. Our main algorithm is a reduction to the complete case ($r=n$) treated in our previous work [Koiran,Saha,CIAC 2023]. For efficiency reasons we cannot use this algorithm as a blackbox. Instead, we show that it can be run on an implicitly represented <b>tensor</b> <b>obtained</b> from the input <b>tensor</b> <b>by</b> a change of basis.</p></p class="citation"></blockquote><h3 id=44--206243-polyamorous-scheduling-leszek-gąsieniec-et-al-2024>(4/4 | 206/243) Polyamorous Scheduling (Leszek Gąsieniec et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Leszek Gąsieniec, Benjamin Smith, Sebastian Wild. (2024)<br><strong>Polyamorous Scheduling</strong><br><button class=copy-to-clipboard title="Polyamorous Scheduling" index=206>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-206 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs-SI, cs.DS, math-OC<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00465v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00465v1.pdf filename=2403.00465v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Finding schedules for pairwise meetings between the members of a complex social group without creating interpersonal conflict is challenging, especially when different relationships have different needs. We formally define and study the underlying optimisation problem: Polyamorous Scheduling. In Polyamorous Scheduling, we are given an edge-weighted <b>graph</b> and try to find a periodic schedule of matchings in this <b>graph</b> such that the maximal weighted waiting time between consecutive occurrences of the same edge is minimised. We show that the problem is NP-hard and that there is no efficient approximation algorithm with a better ratio than 13/12 unless P = NP. On the positive side, we obtain an $O(\log n)$-approximation algorithm. We also define a generalisation of density from the Pinwheel Scheduling Problem, &ldquo;poly density&rdquo;, and ask whether there exists a poly density threshold similar to the 5/6-density threshold for Pinwheel Scheduling [Kawamura, STOC 2024]. Polyamorous Scheduling is a natural generalisation of Pinwheel Scheduling with respect to its optimisation variant, Bamboo Garden Trimming. Our work contributes the first nontrivial hardness-of-approximation reduction for any periodic scheduling problem, and opens up numerous avenues for further study of Polyamorous Scheduling.</p></p class="citation"></blockquote><h2 id=csne-1>cs.NE (1)</h2><h3 id=11--207243-fast-and-efficient-local-search-for-genetic-programming-based-loss-function-learning-christian-raymond-et-al-2024>(1/1 | 207/243) Fast and Efficient Local Search for Genetic Programming Based Loss Function Learning (Christian Raymond et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Christian Raymond, Qi Chen, Bing Xue, Mengjie Zhang. (2024)<br><strong>Fast and Efficient Local Search for Genetic Programming Based Loss Function Learning</strong><br><button class=copy-to-clipboard title="Fast and Efficient Local Search for Genetic Programming Based Loss Function Learning" index=207>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-207 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-AI, cs-CV, cs-LG, cs-NE, cs.NE<br>Keyword Score: 30<br>Keywords: Meta Learning, Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00865v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00865v1.pdf filename=2403.00865v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we develop upon the topic of loss function learning, an emergent <b>meta-learning</b> <b>paradigm</b> that aims to learn loss functions that significantly improve the performance of the models trained under them. Specifically, we propose a new <b>meta-learning</b> <b>framework</b> for task and model-agnostic loss function learning via a hybrid search approach. The framework first uses genetic programming to find a set of symbolic loss functions. Second, the set of learned loss functions is subsequently parameterized and optimized via unrolled differentiation. The versatility and performance of the proposed framework are empirically validated on a diverse set of <b>supervised</b> <b>learning</b> tasks. Results show that the learned loss functions bring improved convergence, sample efficiency, and inference performance on tabulated, computer vision, and natural language processing problems, using a variety of task-specific neural network architectures.</p></p class="citation"></blockquote><h2 id=cssi-2>cs.SI (2)</h2><h3 id=12--208243-cost-effective-activity-control-of-asymptomatic-carriers-in-layered-temporal-social-networks-masoumeh-moradian-et-al-2024>(1/2 | 208/243) Cost-Effective Activity Control of Asymptomatic Carriers in Layered Temporal Social Networks (Masoumeh Moradian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Masoumeh Moradian, Aresh Dadlani, Rasul Kairgeldin, Ahmad Khonsari. (2024)<br><strong>Cost-Effective Activity Control of Asymptomatic Carriers in Layered Temporal Social Networks</strong><br><button class=copy-to-clipboard title="Cost-Effective Activity Control of Asymptomatic Carriers in Layered Temporal Social Networks" index=208>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-208 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-MA, cs-SI, cs.SI<br>Keyword Score: 23<br>Keywords: Graph, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00725v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00725v1.pdf filename=2403.00725v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The robustness of human social networks against epidemic propagation relies on the propensity for physical contact adaptation. During the early phase of infection, asymptomatic carriers exhibit the same activity level as susceptible individuals, which presents challenges for incorporating control measures in epidemic projection models. This paper focuses on modeling and cost-efficient activity control of susceptible and carrier individuals in the context of the susceptible-carrier-infected-removed (SCIR) epidemic model over a two-layer contact network. In this model, individuals switch from a static contact layer to create new links in a temporal layer based on state-dependent activation rates. We derive conditions for the infection to die out or persist in a homogeneous network. Considering the significant costs associated with reducing the activity of susceptible and carrier individuals, we formulate an optimization problem to minimize the disease decay rate while constrained by a limited budget. We propose the use of successive geometric programming (SGP) approximation for this optimization task. Through <b>simulation</b> experiments on Poisson random <b>graphs,</b> we assess the impact of different parameters on disease prevalence. The results demonstrate that our SGP framework achieves a cost reduction of nearly 33% compared to conventional methods based on degree and closeness centrality.</p></p class="citation"></blockquote><h3 id=22--209243-identify-critical-nodes-in-complex-network-with-large-language-models-jinzhu-mao-et-al-2024>(2/2 | 209/243) Identify Critical Nodes in Complex Network with Large Language Models (Jinzhu Mao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinzhu Mao, Dongyun Zou, Li Sheng, Siyi Liu, Chen Gao, Yue Wang, Yong Li. (2024)<br><strong>Identify Critical Nodes in Complex Network with Large Language Models</strong><br><button class=copy-to-clipboard title="Identify Critical Nodes in Complex Network with Large Language Models" index=209>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-209 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-AI, cs-NE, cs-SI, cs.SI<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03962v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03962v1.pdf filename=2403.03962v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Identifying critical nodes in networks is a classical decision-making task, and many methods struggle to strike a balance between adaptability and utility. Therefore, we propose an approach that empowers Evolutionary Algorithm (EA) with <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> to generate a function called &ldquo;score_nodes&rdquo; which can further be used to identify crucial nodes based on their assigned scores. Our model consists of three main components: Manual Initialization, Population Management, and <b>LLMs-based</b> Evolution. It evolves from initial populations with a set of designed node scoring functions created manually. <b>LLMs</b> leverage their strong contextual understanding and rich programming skills to perform crossover and mutation operations on the individuals, generating excellent new functions. These functions are then categorized, ranked, and eliminated to ensure the stable development of the populations while preserving diversity. Extensive experiments demonstrate the excellent performance of our method, showcasing its strong generalization ability compared to other state-of-the-art algorithms. It can consistently and orderly generate diverse and efficient node scoring functions. All source codes and models that can reproduce all results in this work are publicly available at this link: \url{https://anonymous.4open.science/r/LLM4CN-6520}</p></p class="citation"></blockquote><h2 id=csma-1>cs.MA (1)</h2><h3 id=11--210243-composite-distributed-learning-and-synchronization-of-nonlinear-multi-agent-systems-with-complete-uncertain-dynamics-emadodin-jandaghi-et-al-2024>(1/1 | 210/243) Composite Distributed Learning and Synchronization of Nonlinear Multi-Agent Systems with Complete Uncertain Dynamics (Emadodin Jandaghi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Emadodin Jandaghi, Dalton L. Stein, Adam Hoburg, Mingxi Zhou, Chengzhi Yuan. (2024)<br><strong>Composite Distributed Learning and Synchronization of Nonlinear Multi-Agent Systems with Complete Uncertain Dynamics</strong><br><button class=copy-to-clipboard title="Composite Distributed Learning and Synchronization of Nonlinear Multi-Agent Systems with Complete Uncertain Dynamics" index=210>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-210 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MA<br>Categories: cs-MA, cs-RO, cs-SY, cs.MA, eess-SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00987v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00987v1.pdf filename=2403.00987v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper addresses the challenging problem of composite synchronization and learning control in a network of multi-agent robotic manipulator systems operating under heterogeneous nonlinear uncertainties within a leader-follower framework. A novel two-layer distributed adaptive learning control strategy is introduced, comprising a first-layer distributed cooperative estimator and a second-layer decentralized deterministic learning controller. The primary objective of the first layer is to facilitate each robotic agent&rsquo;s estimation of the leader&rsquo;s information. The second layer is responsible for both enabling individual robot agents to track desired reference trajectories and accurately identifying and learning their nonlinear uncertain dynamics. The proposed distributed learning control scheme represents an advancement in the existing literature due to its ability to manage robotic agents with completely uncertain dynamics including uncertain mass matrices. This framework allows the robotic control to be environment-independent which can be used in various settings, from underwater to space where identifying system dynamics parameters is challenging. The stability and parameter convergence of the closed-loop system are rigorously analyzed using the Lyapunov method. Numerical <b>simulations</b> conducted on multi-agent robot manipulators validate the effectiveness of the proposed scheme. The identified nonlinear dynamics can be saved and reused whenever the system restarts.</p></p class="citation"></blockquote><h2 id=cssd-2>cs.SD (2)</h2><h3 id=12--211243-scaling-up-adaptive-filter-optimizers-jonah-casebeer-et-al-2024>(1/2 | 211/243) Scaling Up Adaptive Filter Optimizers (Jonah Casebeer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jonah Casebeer, Nicholas J. Bryan, Paris Smaragdis. (2024)<br><strong>Scaling Up Adaptive Filter Optimizers</strong><br><button class=copy-to-clipboard title="Scaling Up Adaptive Filter Optimizers" index=211>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-211 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-SD, cs.SD, eess-AS<br>Keyword Score: 20<br>Keywords: Pruning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00977v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00977v1.pdf filename=2403.00977v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce a new online adaptive filtering method called <b>supervised</b> multi-step adaptive filters (SMS-AF). Our method uses neural networks to control or optimize linear multi-delay or multi-channel frequency-domain filters and can flexibly scale-up performance at the cost of increased compute &ndash; a property rarely addressed in the AF literature, but critical for many applications. To do so, we extend recent work with a set of improvements including feature <b>pruning,</b> a <b>supervised</b> loss, and multiple optimization steps per time-frame. These improvements work in a cohesive manner to unlock scaling. Furthermore, we show how our method relates to Kalman filtering and meta-adaptive filtering, making it seamlessly applicable to a diverse set of AF tasks. We evaluate our method on acoustic echo cancellation (AEC) and multi-channel speech enhancement tasks and compare against several baselines on standard synthetic and real-world datasets. Results show our method performance scales with inference cost and model capacity, yields multi-dB performance gains for both tasks, and is real-time capable on a single CPU core.</p></p class="citation"></blockquote><h3 id=22--212243-voxgenesis-unsupervised-discovery-of-latent-speaker-manifold-for-speech-synthesis-weiwei-lin-et-al-2024>(2/2 | 212/243) VoxGenesis: Unsupervised Discovery of Latent Speaker Manifold for Speech Synthesis (Weiwei Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Weiwei Lin, Chenhang He, Man-Wai Mak, Jiachen Lian, Kong Aik Lee. (2024)<br><strong>VoxGenesis: Unsupervised Discovery of Latent Speaker Manifold for Speech Synthesis</strong><br><button class=copy-to-clipboard title="VoxGenesis: Unsupervised Discovery of Latent Speaker Manifold for Speech Synthesis" index=212>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-212 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-LG, cs-SD, cs.SD, eess-AS<br>Keyword Score: 20<br>Keywords: Supervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00529v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00529v1.pdf filename=2403.00529v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Achieving nuanced and accurate emulation of human voice has been a longstanding goal in artificial intelligence. Although significant progress has been made in recent years, the mainstream of speech synthesis models still relies on <b>supervised</b> speaker modeling and explicit reference utterances. However, there are many aspects of human voice, such as emotion, intonation, and speaking style, for which it is hard to obtain accurate labels. In this paper, we propose VoxGenesis, a novel <b>unsupervised</b> speech synthesis framework that can discover a latent speaker manifold and meaningful voice editing directions without supervision. VoxGenesis is conceptually simple. Instead of mapping speech features to waveforms deterministically, VoxGenesis transforms a Gaussian distribution into speech distributions conditioned and aligned by semantic tokens. This forces the model to learn a speaker distribution disentangled from the semantic content. During the inference, sampling from the Gaussian distribution enables the creation of novel speakers with distinct characteristics. More importantly, the exploration of latent space uncovers human-interpretable directions associated with specific speaker characteristics such as gender attributes, pitch, tone, and emotion, allowing for voice editing by manipulating the latent codes along these identified directions. We conduct extensive experiments to evaluate the proposed VoxGenesis using both subjective and objective metrics, finding that it produces significantly more diverse and realistic speakers with distinct characteristics than the previous approaches. We also show that latent space manipulation produces consistent and human-identifiable effects that are not detrimental to the speech quality, which was not possible with previous approaches. Audio samples of VoxGenesis can be found at: \url{https://bit.ly/VoxGenesis}.</p></p class="citation"></blockquote><h2 id=mathna-4>math.NA (4)</h2><h3 id=14--213243-computational-homogenization-for-aerogel-like-polydisperse-open-porous-materials-using-neural-network--based-surrogate-models-on-the-microscale-axel-klawonn-et-al-2024>(1/4 | 213/243) Computational homogenization for aerogel-like polydisperse open-porous materials using neural network&ndash;based surrogate models on the microscale (Axel Klawonn et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Axel Klawonn, Martin Lanser, Lucas Mager, Ameya Rege. (2024)<br><strong>Computational homogenization for aerogel-like polydisperse open-porous materials using neural network&ndash;based surrogate models on the microscale</strong><br><button class=copy-to-clipboard title="Computational homogenization for aerogel-like polydisperse open-porous materials using neural network--based surrogate models on the microscale" index=213>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-213 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: 68T07, 65N30, 74Q05, cs-NA, math-NA, math.NA<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00571v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00571v1.pdf filename=2403.00571v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The morphology of nanostructured materials exhibiting a polydisperse porous space, such as aerogels, is very open porous and fine grained. Therefore, a <b>simulation</b> of the deformation of a large aerogel structure resolving the nanostructure would be extremely expensive. Thus, multi-scale or homogenization approaches have to be considered. Here, a computational scale bridging approach based on the FE$^2$ method is suggested, where the macroscopic scale is discretized using finite elements while the microstructure of the open-porous material is resolved as a network of Euler-Bernoulli beams. Here, the beam frame based RVEs (representative volume elements) have pores whose size distribution follows the measured values for a specific material. This is a well-known approach to model aerogel structures. For the computational homogenization, an approach to average the first Piola-Kirchhoff stresses in a beam frame by neglecting rotational moments is suggested. To further overcome the computationally most expensive part in the homogenization method, that is, solving the RVEs and averaging their stress fields, a surrogate model is introduced based on neural networks. The networks input is the localized deformation gradient on the macroscopic scale and its output is the averaged stress for the specific material. It is trained on data generated by the beam frame based approach. The effiency and robustness of both homogenization approaches is shown numerically, the approximation properties of the surrogate model is verified for different macroscopic problems and discretizations. Different (Quasi-)Newton solvers are considered on the macroscopic scale and compared with respect to their convergence properties.</p></p class="citation"></blockquote><h3 id=24--214243-implicit-high-order-gas-kinetic-schemes-for-compressible-flows-on-three-dimensional-unstructured-meshes-ii-unsteady-flows-yaqing-yang-et-al-2024>(2/4 | 214/243) Implicit high-order gas-kinetic schemes for compressible flows on three-dimensional unstructured meshes II: unsteady flows (Yaqing Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yaqing Yang, Liang Pan, Kun Xu. (2024)<br><strong>Implicit high-order gas-kinetic schemes for compressible flows on three-dimensional unstructured meshes II: unsteady flows</strong><br><button class=copy-to-clipboard title="Implicit high-order gas-kinetic schemes for compressible flows on three-dimensional unstructured meshes II: unsteady flows" index=214>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-214 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA, physics-flu-dyn<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00482v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00482v1.pdf filename=2403.00482v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>For the <b>simulations</b> of unsteady flow, the global time step becomes really small with a large variation of local cell size. In this paper, an implicit high-order gas-kinetic scheme (HGKS) is developed to remove the restrictions on the time step for unsteady <b>simulations.</b> In order to improve the efficiency and keep the high-order accuracy, a two-stage third-order implicit time-accurate discretization is proposed. In each stage, an artificial steady solution is obtained for the implicit system with the pseudo-time iteration. In the iteration, the classical implicit methods are adopted to solve the nonlinear system, including the lower-upper symmetric Gauss-Seidel (LUSGS) and generalized minimum residual (GMRES) methods. To achieve the spatial accuracy, the HGKSs with both non-compact and compact reconstructions are constructed. For the non-compact scheme, the weighted essentially non-oscillatory (WENO) reconstruction is used. For the compact one, the Hermite WENO (HWENO) reconstruction is adopted due to the updates of both cell-averaged flow variables and their derivatives. The expected third-order temporal accuracy is achieved with the two-stage temporal discretization. For the smooth flow, only a single artificial iteration is needed. For uniform meshes, the efficiency of the current implicit method improves significantly in comparison with the explicit one. For the flow with discontinuities, compared with the well-known Crank-Nicholson method, the spurious oscillations in the current schemes are well suppressed. The increase of the artificial iteration steps introduces extra reconstructions associating with a reduction of the computational efficiency. Overall, the current implicit method leads to an improvement in efficiency over the explicit one in the cases with a large variation of mesh size.</p></p class="citation"></blockquote><h3 id=34--215243-enhancing-biomechanical-simulations-based-on-a-posteriori-error-estimates-the-potential-of-dual-weighted-residual-driven-adaptive-mesh-refinement-huu-phuoc-bui-et-al-2024>(3/4 | 215/243) Enhancing Biomechanical Simulations Based on A Posteriori Error Estimates: The Potential of Dual Weighted Residual-Driven Adaptive Mesh Refinement (Huu Phuoc Bui et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Huu Phuoc Bui, Michel Duprez, Pierre-Yves Rohan, Arnaud Lejeune, Stephane P. A. Bordas, Marek Bucki, Franz Chouly. (2024)<br><strong>Enhancing Biomechanical Simulations Based on A Posteriori Error Estimates: The Potential of Dual Weighted Residual-Driven Adaptive Mesh Refinement</strong><br><button class=copy-to-clipboard title="Enhancing Biomechanical Simulations Based on A Posteriori Error Estimates: The Potential of Dual Weighted Residual-Driven Adaptive Mesh Refinement" index=215>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-215 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: 65N30, 92C10, 74B20, cs-NA, math-NA, math.NA<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00401v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00401v1.pdf filename=2403.00401v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Finite Element Method (FEM) is a well-established procedure for computing approximate solutions to deterministic engineering problems described by partial differential equations. FEM produces discrete approximations of the solution with a discretisation error that can be an be quantified with \emph{a posteriori} error estimates. The practical relevance of error estimates for biomechanics problems, especially for soft tissue where the response is governed by large strains, is rarely addressed. In this contribution, we propose an implementation of \emph{a posteriori} error estimates targeting a user-defined quantity of interest, using the Dual Weighted Residual (DWR) technique tailored to biomechanics. The proposed method considers a general setting that encompasses three-dimensional geometries and model non-linearities, which appear in hyperelastic soft tissues. We take advantage of the automatic differentiation capabilities embedded in modern finite element software, which allows the error estimates to be computed generically for a large class of models and constitutive laws. First we validate our methodology using experimental measurements from silicone samples, and then illustrate its applicability for patient-specific computations of pressure ulcers on a human heel.</p></p class="citation"></blockquote><h3 id=44--216243-a-gradually-reinforced-sample-average-approximation-differentiable-homotopy-method-for-a-system-of-stochastic-equations-peixuan-li-et-al-2024>(4/4 | 216/243) A Gradually Reinforced Sample-Average-Approximation Differentiable Homotopy Method for a System of Stochastic Equations (Peixuan Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Peixuan Li, Chuangyin Dang, Yang Zhan. (2024)<br><strong>A Gradually Reinforced Sample-Average-Approximation Differentiable Homotopy Method for a System of Stochastic Equations</strong><br><button class=copy-to-clipboard title="A Gradually Reinforced Sample-Average-Approximation Differentiable Homotopy Method for a System of Stochastic Equations" index=216>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-216 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA<br>Keyword Score: 3<br>Keywords: Sample Size<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00294v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00294v1.pdf filename=2403.00294v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper intends to apply the <b>sample-average-approximation</b> <b>(SAA)</b> scheme to solve a system of stochastic equations (SSE), which has many applications in a variety of fields. The SAA is an effective paradigm to address risks and uncertainty in stochastic models from the perspective of Monte Carlo principle. Nonetheless, a numerical conflict arises from the <b>sample</b> <b>size</b> of SAA when one has to make a tradeoff between the accuracy of solutions and the computational cost. To alleviate this issue, we incorporate a gradually reinforced SAA scheme into a differentiable homotopy method and develop a gradually reinforced <b>sample-average-approximation</b> <b>(GRSAA)</b> differentiable homotopy method in this paper. By introducing a series of continuously differentiable functions of the homotopy parameter $t$ ranging between zero and one, we establish a differentiable homotopy system, which is able to gradually increase the <b>sample</b> <b>size</b> of SAA as $t$ descends from one to zero. The set of solutions to the homotopy system contains an everywhere smooth path, which starts from an arbitrary point and ends at a solution to the SAA with any desired accuracy. The GRSAA differentiable homotopy method serves as a bridge to link the gradually reinforced SAA scheme and a differentiable homotopy method and retains the nice property of global convergence the homotopy method possesses while greatly reducing the computational cost for attaining a desired solution to the original SSE. Several numerical experiments further confirm the effectiveness and efficiency of the proposed method.</p></p class="citation"></blockquote><h2 id=statml-3>stat.ML (3)</h2><h3 id=13--217243-validation-of-ml-uq-calibration-statistics-using-simulated-reference-values-a-sensitivity-analysis-pascal-pernot-2024>(1/3 | 217/243) Validation of ML-UQ calibration statistics using simulated reference values: a sensitivity analysis (Pascal Pernot, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pascal Pernot. (2024)<br><strong>Validation of ML-UQ calibration statistics using simulated reference values: a sensitivity analysis</strong><br><button class=copy-to-clipboard title="Validation of ML-UQ calibration statistics using simulated reference values: a sensitivity analysis" index=217>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-217 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, physics-chem-ph, stat-ML, stat.ML<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00423v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00423v1.pdf filename=2403.00423v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Some popular Machine Learning Uncertainty Quantification (ML-UQ) calibration statistics do not have predefined reference values and are mostly used in comparative studies. In consequence, calibration is almost never validated and the diagnostic is left to the appreciation of the reader. Simulated reference values, based on synthetic calibrated datasets derived from actual uncertainties, have been proposed to palliate this problem. As the generative probability distribution for the <b>simulation</b> of synthetic errors is often not constrained, the sensitivity of simulated reference values to the choice of generative distribution might be problematic, shedding a doubt on the calibration diagnostic. This study explores various facets of this problem, and shows that some statistics are excessively sensitive to the choice of generative distribution to be used for validation when the generative distribution is unknown. This is the case, for instance, of the correlation coefficient between absolute errors and uncertainties (CC) and of the expected normalized calibration error (ENCE). A robust validation workflow to deal with simulated reference values is proposed.</p></p class="citation"></blockquote><h3 id=23--218243-causal-bandits-with-general-causal-models-and-interventions-zirui-yan-et-al-2024>(2/3 | 218/243) Causal Bandits with General Causal Models and Interventions (Zirui Yan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zirui Yan, Dennis Wei, Dmitriy Katz-Rogozhnikov, Prasanna Sattigeri, Ali Tajer. (2024)<br><strong>Causal Bandits with General Causal Models and Interventions</strong><br><button class=copy-to-clipboard title="Causal Bandits with General Causal Models and Interventions" index=218>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-218 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 13<br>Keywords: Graph, Bandit Algorithm<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00233v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00233v1.pdf filename=2403.00233v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper considers causal <b>bandits</b> (CBs) for the sequential design of interventions in a causal system. The objective is to optimize a reward function via minimizing a measure of cumulative regret with respect to the best sequence of interventions in hindsight. The paper advances the results on CBs in three directions. First, the structural causal models (SCMs) are assumed to be unknown and drawn arbitrarily from a general class $\mathcal{F}$ of Lipschitz-continuous functions. Existing results are often focused on (generalized) linear SCMs. Second, the interventions are assumed to be generalized soft with any desired level of granularity, resulting in an infinite number of possible interventions. The existing literature, in contrast, generally adopts atomic and hard interventions. Third, we provide general upper and lower bounds on regret. The upper bounds subsume (and improve) known bounds for special cases. The lower bounds are generally hitherto unknown. These bounds are characterized as functions of the (i) <b>graph</b> parameters, (ii) eluder dimension of the space of SCMs, denoted by $\operatorname{dim}(\mathcal{F})$, and (iii) the covering number of the function space, denoted by ${\rm cn}(\mathcal{F})$. Specifically, the cumulative achievable regret over horizon $T$ is $\mathcal{O}(K d^{L-1}\sqrt{T\operatorname{dim}(\mathcal{F}) \log({\rm cn}(\mathcal{F}))})$, where $K$ is related to the Lipschitz constants, $d$ is the <b>graph&rsquo;s</b> maximum in-degree, and $L$ is the length of the longest causal path. The upper bound is further refined for special classes of SCMs (neural network, polynomial, and linear), and their corresponding lower bounds are provided.</p></p class="citation"></blockquote><h3 id=33--219243-lossless-compression-of-deep-neural-networks-a-high-dimensional-neural-tangent-kernel-approach-lingyu-gu-et-al-2024>(3/3 | 219/243) &lsquo;Lossless&rsquo; Compression of Deep Neural Networks: A High-dimensional Neural Tangent Kernel Approach (Lingyu Gu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lingyu Gu, Yongqi Du, Yuan Zhang, Di Xie, Shiliang Pu, Robert C. Qiu, Zhenyu Liao. (2024)<br><strong>&lsquo;Lossless&rsquo; Compression of Deep Neural Networks: A High-dimensional Neural Tangent Kernel Approach</strong><br><button class=copy-to-clipboard title="'Lossless' Compression of Deep Neural Networks: A High-dimensional Neural Tangent Kernel Approach" index=219>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-219 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 10<br>Keywords: Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00258v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00258v1.pdf filename=2403.00258v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Modern deep neural networks (DNNs) are extremely powerful; however, this comes at the price of increased depth and having more parameters per layer, making their training and inference more computationally challenging. In an attempt to address this key limitation, efforts have been devoted to the compression (e.g., sparsification and/or <b>quantization)</b> of these large-scale machine learning models, so that they can be deployed on low-power IoT devices. In this paper, building upon recent advances in neural tangent kernel (NTK) and random matrix theory (RMT), we provide a novel compression approach to wide and fully-connected \emph{deep} neural nets. Specifically, we demonstrate that in the high-dimensional regime where the number of data points $n$ and their dimension $p$ are both large, and under a Gaussian mixture model for the data, there exists \emph{asymptotic spectral equivalence} between the NTK matrices for a large family of DNN models. This theoretical result enables &ldquo;lossless&rdquo; compression of a given DNN to be performed, in the sense that the compressed network yields asymptotically the same NTK as the original (dense and unquantized) network, with its weights and activations taking values \emph{only} in ${ 0, \pm 1 }$ up to a scaling. Experiments on both synthetic and real-world data are conducted to support the advantages of the proposed compression scheme, with code available at \url{https://github.com/Model-Compression/Lossless_Compression}.</p></p class="citation"></blockquote><h2 id=physicsmed-ph-1>physics.med-ph (1)</h2><h3 id=11--220243-list-mode-pet-image-reconstruction-using-dykstra-like-splitting-kibo-ote-et-al-2024>(1/1 | 220/243) List-Mode PET Image Reconstruction Using Dykstra-Like Splitting (Kibo Ote et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kibo Ote, Fumio Hashimoto, Yuya Onishi, Yasuomi Ouchi. (2024)<br><strong>List-Mode PET Image Reconstruction Using Dykstra-Like Splitting</strong><br><button class=copy-to-clipboard title="List-Mode PET Image Reconstruction Using Dykstra-Like Splitting" index=220>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-220 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.med-ph<br>Categories: cs-CV, eess-IV, physics-med-ph, physics.med-ph<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00394v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00394v1.pdf filename=2403.00394v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To converge the block iterative method in image reconstruction for positron emission tomography (PET), careful control of relaxation parameters is required, which is a challenging task. The automatic determination of relaxation parameters for list-mode reconstructions also remains challenging. Therefore, a different approach than controlling relaxation parameters would be desired by list-mode PET reconstruction. In this study, we propose a list-mode maximum likelihood Dykstra-like splitting PET reconstruction (LM-MLDS). LM-MLDS converges the list-mode block iterative method by adding the distance from an initial image as a penalty term into an objective function. LM-MLDS takes a two-step approach because its performance depends on the quality of the initial image. The first step uses a uniform image as the initial image, and then the second step uses a reconstructed image after one main iteration as the initial image. We evaluated LM-MLDS using <b>simulation</b> and clinical data. LM-MLDS provided a higher peak signal-to-noise ratio and suppressed an oscillation of tradeoff curves between noise and contrast than the other block iterative methods. In a clinical study, LM-MLDS removed the false hotspots at the edge of the axial field of view and improved the image quality of slices covering the top of the head to the cerebellum. LM-MLDS showed different noise properties than the other methods due to Gaussian denoising induced by the proximity operator. The list-mode proximal splitting PET reconstruction is useful not only for optimizing nondifferentiable functions such as total variation but also for converging block iterative methods without controlling relaxation parameters.</p></p class="citation"></blockquote><h2 id=statme-1>stat.ME (1)</h2><h3 id=11--221243-stable-reduced-rank-var-identification-xinhui-rong-et-al-2024>(1/1 | 221/243) Stable Reduced-Rank VAR Identification (Xinhui Rong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinhui Rong, Victor Solo. (2024)<br><strong>Stable Reduced-Rank VAR Identification</strong><br><button class=copy-to-clipboard title="Stable Reduced-Rank VAR Identification" index=221>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-221 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ME<br>Categories: cs-SY, eess-SY, stat-ME, stat.ME<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00237v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00237v1.pdf filename=2403.00237v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The vector autoregression (VAR) has been widely used in system identification, econometrics, natural science, and many other areas. However, when the state dimension becomes large the parameter dimension explodes. So rank reduced modelling is attractive and is well developed. But a fundamental requirement in almost all applications is stability of the fitted model. And this has not been addressed in the rank reduced case. Here, we develop, for the first time, a closed-form formula for an estimator of a rank reduced transition matrix which is guaranteed to be stable. We show that our estimator is consistent and asymptotically statistically efficient and illustrate it in comparative <b>simulations.</b></p></p class="citation"></blockquote><h2 id=cscy-1>cs.CY (1)</h2><h3 id=11--222243-exploring-the-dynamic-interplay-of-cognitive-load-and-emotional-arousal-by-using-multimodal-measurements-correlation-of-pupil-diameter-and-emotional-arousal-in-emotionally-engaging-tasks-c-kosel-et-al-2024>(1/1 | 222/243) Exploring the dynamic interplay of cognitive load and emotional arousal by using multimodal measurements: Correlation of pupil diameter and emotional arousal in emotionally engaging tasks (C. Kosel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>C. Kosel, S. Michel, T. Seidel, M. Foerster. (2024)<br><strong>Exploring the dynamic interplay of cognitive load and emotional arousal by using multimodal measurements: Correlation of pupil diameter and emotional arousal in emotionally engaging tasks</strong><br><button class=copy-to-clipboard title="Exploring the dynamic interplay of cognitive load and emotional arousal by using multimodal measurements: Correlation of pupil diameter and emotional arousal in emotionally engaging tasks" index=222>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-222 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CV, cs-CY, cs.CY<br>Keyword Score: 16<br>Keywords: Multi-modal, Multi-modal, Emotion Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00366v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00366v1.pdf filename=2403.00366v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multimodal</b> data analysis and validation based on streams from state-of-the-art sensor technology such as eye-tracking or <b>emotion</b> <b>recognition</b> using the Facial Action Coding System (FACTs) with deep learning allows educational researchers to study multifaceted learning and problem-solving processes and to improve educational experiences. This study aims to investigate the correlation between two continuous sensor streams, pupil diameter as an indicator of cognitive workload and FACTs with deep learning as an indicator of <b>emotional</b> <b>arousal</b> (RQ 1a), specifically for epochs of high, medium, and low arousal (RQ 1b). Furthermore, the time lag between <b>emotional</b> <b>arousal</b> and pupil diameter data will be analyzed (RQ 2). 28 participants worked on three cognitively demanding and emotionally engaging everyday moral dilemmas while eye-tracking and <b>emotion</b> <b>recognition</b> data were collected. The data were pre-processed in Phyton (synchronization, blink control, downsampling) and analyzed using correlation analysis and Granger causality tests. The results show negative and statistically significant correlations between the data streams for <b>emotional</b> <b>arousal</b> and pupil diameter. However, the correlation is negative and significant only for epochs of high arousal, while positive but non-significant relationships were found for epochs of medium or low arousal. The average time lag for the relationship between arousal and pupil diameter was 2.8 ms. In contrast to previous findings without a <b>multimodal</b> approach suggesting a positive correlation between the constructs, the results contribute to the state of research by highlighting the importance of <b>multimodal</b> data validation and research on convergent vagility. Future research should consider <b>emotional</b> <b>regulation</b> strategies and <b>emotional</b> <b>valence.</b></p></p class="citation"></blockquote><h2 id=csdc-3>cs.DC (3)</h2><h3 id=13--223243-a-spark-optimizer-for-adaptive-fine-grained-parameter-tuning-chenghao-lyu-et-al-2024>(1/3 | 223/243) A Spark Optimizer for Adaptive, Fine-Grained Parameter Tuning (Chenghao Lyu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chenghao Lyu, Qi Fan, Philippe Guyard, Yanlei Diao. (2024)<br><strong>A Spark Optimizer for Adaptive, Fine-Grained Parameter Tuning</strong><br><button class=copy-to-clipboard title="A Spark Optimizer for Adaptive, Fine-Grained Parameter Tuning" index=223>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-223 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs.DC<br>Keyword Score: 13<br>Keywords: Benchmarking, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00995v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00995v1.pdf filename=2403.00995v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As Spark becomes a common big data analytics platform, its growing complexity makes automatic tuning of numerous parameters critical for performance. Our work on Spark parameter tuning is particularly motivated by two recent trends: Spark&rsquo;s Adaptive Query Execution (AQE) based on runtime statistics, and the increasingly popular Spark cloud deployments that make cost-performance <b>reasoning</b> crucial for the end user. This paper presents our design of a Spark optimizer that controls all tunable parameters (collectively called a &ldquo;configuration&rdquo;) of each query in the new AQE architecture to explore its performance benefits and, at the same time, casts the tuning problem in the theoretically sound multi-objective optimization setting to better adapt to user cost-performance preferences. To this end, we propose a novel hybrid compile-time/runtime approach to multi-granularity tuning of diverse, correlated Spark parameters, as well as a suite of modeling and optimization techniques to solve the tuning problem in the MOO setting while meeting the stringent time constraint of 1-2 seconds for cloud use. Our evaluation results using the TPC-H and TPC-DS <b>benchmarks</b> demonstrate the superior performance of our approach: (i) When prioritizing latency, it achieves an average of 61% and 64% reduction for TPC-H and TPC-DS, respectively, under the solving time of 0.62-0.83 sec, outperforming the most competitive MOO method that reduces only 18-25% latency with high solving time of 2.4-15 sec. (ii) When shifting preferences between latency and cost, our approach dominates the solutions from alternative methods by a wide margin, exhibiting superior adaptability to varying preferences.</p></p class="citation"></blockquote><h3 id=23--224243-neural-acceleration-of-incomplete-cholesky-preconditioners-joshua-dennis-booth-et-al-2024>(2/3 | 224/243) Neural Acceleration of Incomplete Cholesky Preconditioners (Joshua Dennis Booth et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Joshua Dennis Booth, Hongyang Sun, Trevor Garnett. (2024)<br><strong>Neural Acceleration of Incomplete Cholesky Preconditioners</strong><br><button class=copy-to-clipboard title="Neural Acceleration of Incomplete Cholesky Preconditioners" index=224>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-224 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs-NA, cs.DC, math-NA<br>Keyword Score: 5<br>Keywords: Black Box<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00743v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00743v1.pdf filename=2403.00743v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The solution of a sparse system of linear equations is ubiquitous in scientific applications. Iterative methods, such as the Preconditioned Conjugate Gradient method (PCG), are normally chosen over direct methods due to memory and computational complexity constraints. However, the efficiency of these methods depends on the preconditioner utilized. The development of the preconditioner normally requires some insight into the sparse linear system and the desired trade-off of generating the preconditioner and the reduction in the number of iterations. Incomplete factorization methods tend to be <b>black</b> <b>box</b> methods to generate these preconditioners but may fail for a number of reasons. These reasons include numerical issues that require searching for adequate scaling, shifting, and fill-in while utilizing a difficult to parallelize algorithm. With a move towards heterogeneous computing, many sparse applications find GPUs that are optimized for dense tensor applications like training neural networks being underutilized. In this work, we demonstrate that a simple artificial neural network trained either at compile time or in parallel to the running application on a GPU can provide an incomplete sparse Cholesky factorization that can be used as a preconditioner. This generated preconditioner is as good or better in terms of reduction of iterations than the one found using multiple preconditioning techniques such as scaling and shifting. Moreover, the generated method also works and never fails to produce a preconditioner that does not reduce the iteration count.</p></p class="citation"></blockquote><h3 id=33--225243-windgp-efficient-graph-partitioning-on-heterogenous-machines-li-zeng-et-al-2024>(3/3 | 225/243) WindGP: Efficient Graph Partitioning on Heterogenous Machines (Li Zeng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Li Zeng, Haohan Huang, Binfan Zheng, Kang Yang, Shengcheng Shao, Jinhua Zhou, Jun Xie, Rongqian Zhao, Xin Chen. (2024)<br><strong>WindGP: Efficient Graph Partitioning on Heterogenous Machines</strong><br><button class=copy-to-clipboard title="WindGP: Efficient Graph Partitioning on Heterogenous Machines" index=225>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-225 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs.DC<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00331v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00331v2.pdf filename=2403.00331v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> Partitioning is widely used in many real-world applications such as fraud detection and social network analysis, in order to enable the distributed <b>graph</b> computing on large <b>graphs.</b> However, existing works fail to balance the computation cost and communication cost on machines with different power (including computing capability, network bandwidth and memory size), as they only consider replication factor and neglect the difference of machines in realistic data centers. In this paper, we propose a general <b>graph</b> partitioning algorithm WindGP, which can support fast and high-quality edge partitioning on heterogeneous machines. WindGP designs novel preprocessing techniques to simplify the metric and balance the computation cost according to the characteristics of <b>graphs</b> and machines. Also, best-first search is proposed instead of BFS and DFS, in order to generate clusters with high cohesion. Furthermore, WindGP adaptively tunes the partition results by sophisticated local search methods. Extensive experiments show that WindGP outperforms all state-of-the-art partition methods by 1.35 - 27 times on both dense and sparse distributed <b>graph</b> algorithms, and has good scalability with <b>graph</b> size and machine number.</p></p class="citation"></blockquote><h2 id=csgt-4>cs.GT (4)</h2><h3 id=14--226243-the-price-of-fairness-in-bipartite-matching-rémi-castera-et-al-2024>(1/4 | 226/243) The Price of Fairness in Bipartite Matching (Rémi Castera et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rémi Castera, Felipe Garrido-Lucero, Mathieu Molina, Simon Mauras, Patrick Loiseau, Vianney Perchet. (2024)<br><strong>The Price of Fairness in Bipartite Matching</strong><br><button class=copy-to-clipboard title="The Price of Fairness in Bipartite Matching" index=226>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-226 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-GT, cs.GT<br>Keyword Score: 13<br>Keywords: Graph, Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00397v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00397v1.pdf filename=2403.00397v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We investigate notions of group <b>fairness</b> in bipartite matching markets involving agents and jobs, where agents are grouped based on sensitive attributes. Employing a geometric approach, we characterize how many agents can be matched in each group, showing that the set of feasible matchings forms a (discrete) polymatroid. We show how we can define weakly-fair matchings geometrically, for which poly-matroid properties imply that they are maximal. Next, we focus on strong <b>fairness</b> notions (inspired by group-fairness metrics in machine learning), where each group gets their exact same fraction of their entitlement, and we explore the Price of <b>Fairness</b> (PoF), i.e., the loss in optimality when imposing such <b>fairness</b> constraints. Importantly, we advocate for the notion of opportunity <b>fairness,</b> where a group entitlement is the maximum number of agents that can be matched without the presence of other competing groups. We show that the opportunity PoF is bounded independently of the number of agents and jobs, but may be linear in the number of groups. Finally, we provide improved bounds with additional structural properties, or with stochastic <b>graphs.</b></p></p class="citation"></blockquote><h3 id=24--227243-understanding-police-force-resource-allocation-using-adversarial-optimal-transport-with-incomplete-information-yinan-hu-et-al-2024>(2/4 | 227/243) Understanding Police Force Resource Allocation using Adversarial Optimal Transport with Incomplete Information (Yinan Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yinan Hu, Juntao Chen, Quanyan Zhu. (2024)<br><strong>Understanding Police Force Resource Allocation using Adversarial Optimal Transport with Incomplete Information</strong><br><button class=copy-to-clipboard title="Understanding Police Force Resource Allocation using Adversarial Optimal Transport with Incomplete Information" index=227>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-227 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-GT, cs-SY, cs.GT, eess-SY<br>Keyword Score: 10<br>Keywords: Markov Game<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00972v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00972v1.pdf filename=2403.00972v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Adversarial optimal transport has been proven useful as a mathematical formulation to model resource allocation problems to maximize the efficiency of transportation with an adversary, who modifies the data. It is often the case, however, that only the adversary knows which nodes are malicious and which are not. In this paper we formulate the problem of seeking adversarial optimal transport into Bayesian games. We construct the concept of Bayesian equilibrium and design a distributed algorithm that achieve those equilibria, making our model applicable to large-scale networks. Keywords: game theory, crime control, <b>Markov</b> <b>games</b></p></p class="citation"></blockquote><h3 id=34--228243-on-the-hardness-of-fair-allocation-under-ternary-valuations-zack-fitzsimmons-et-al-2024>(3/4 | 228/243) On the Hardness of Fair Allocation under Ternary Valuations (Zack Fitzsimmons et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zack Fitzsimmons, Vignesh Viswanathan, Yair Zick. (2024)<br><strong>On the Hardness of Fair Allocation under Ternary Valuations</strong><br><button class=copy-to-clipboard title="On the Hardness of Fair Allocation under Ternary Valuations" index=228>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-228 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-GT, cs.GT<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00943v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00943v1.pdf filename=2403.00943v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the problem of fair allocation of indivisible items when agents have ternary additive valuations &ndash; each agent values each item at some fixed integer values $a$, $b$, or $c$ that are common to all agents. The notions of <b>fairness</b> we consider are max Nash welfare (MNW), when $a$, $b$, and $c$ are non-negative, and max egalitarian welfare (MEW). We show that for any distinct non-negative $a$, $b$, and $c$, maximizing Nash welfare is APX-hard &ndash; i.e., the problem does not admit a PTAS unless P = NP. We also show that for any distinct $a$, $b$, and $c$, maximizing egalitarian welfare is APX-hard except for a few cases when $b = 0$ that admit efficient algorithms. These results make significant progress towards completely characterizing the complexity of computing exact MNW allocations and MEW allocations. En route, we resolve open questions left by prior work regarding the complexity of computing MNW allocations under bivalued valuations, and MEW allocations under ternary mixed manna.</p></p class="citation"></blockquote><h3 id=44--229243-as-soon-as-possible-but-rationally-véronique-bruyère-et-al-2024>(4/4 | 229/243) As Soon as Possible but Rationally (Véronique Bruyère et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Véronique Bruyère, Christophe Grandmont, Jean-François Raskin. (2024)<br><strong>As Soon as Possible but Rationally</strong><br><button class=copy-to-clipboard title="As Soon as Possible but Rationally" index=229>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-229 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-GT, cs.GT<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00399v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00399v1.pdf filename=2403.00399v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper addresses complexity problems in rational verification and synthesis for multi-player games played on weighted <b>graphs,</b> where the objective of each player is to minimize the cost of reaching a specific set of target vertices. In these games, one player, referred to as the system, declares his strategy upfront. The other players, composing the environment, then rationally make their moves according to their objectives. The rational behavior of these responding players is captured through two models: they opt for strategies that either represent a Nash equilibrium or lead to a play with a Pareto-optimal cost tuple.</p></p class="citation"></blockquote><h2 id=q-bioqm-1>q-bio.QM (1)</h2><h3 id=11--230243-enhancing-protein-predictive-models-via-proteins-data-augmentation-a-benchmark-and-new-directions-rui-sun-et-al-2024>(1/1 | 230/243) Enhancing Protein Predictive Models via Proteins Data Augmentation: A Benchmark and New Directions (Rui Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rui Sun, Lirong Wu, Haitao Lin, Yufei Huang, Stan Z. Li. (2024)<br><strong>Enhancing Protein Predictive Models via Proteins Data Augmentation: A Benchmark and New Directions</strong><br><button class=copy-to-clipboard title="Enhancing Protein Predictive Models via Proteins Data Augmentation: A Benchmark and New Directions" index=230>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-230 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.QM<br>Categories: cs-AI, cs-LG, q-bio-BM, q-bio-QM, q-bio.QM<br>Keyword Score: 13<br>Keywords: Benchmarking, Data Augmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00875v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00875v1.pdf filename=2403.00875v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Augmentation is an effective alternative to utilize the small amount of labeled protein <b>data.</b> <b>However,</b> most of the existing work focuses on design-ing new architectures or pre-training tasks, and relatively little work has studied <b>data</b> <b>augmentation</b> for proteins. This paper extends <b>data</b> <b>augmentation</b> techniques previously used for images and texts to proteins and then <b>benchmarks</b> these techniques on a variety of protein-related tasks, providing the first comprehensive evaluation of protein augmentation. Furthermore, we propose two novel semantic-level protein augmentation methods, namely Integrated Gradients Substitution and Back Translation Substitution, which enable protein semantic-aware augmentation through saliency detection and biological knowledge. Finally, we integrate extended and proposed augmentations into an augmentation pool and propose a simple but effective framework, namely Automated Protein Augmentation (APA), which can adaptively select the most suitable augmentation combinations for different tasks. Extensive experiments have shown that APA enhances the performance of five protein related tasks by an average of 10.55% across three architectures compared to vanilla implementations without augmentation, highlighting its potential to make a great impact on the field.</p></p class="citation"></blockquote><h2 id=statap-1>stat.AP (1)</h2><h3 id=11--231243-binary-gaussian-copula-synthesis-a-novel-data-augmentation-technique-to-advance-ml-based-clinical-decision-support-systems-for-early-prediction-of-dialysis-among-ckd-patients-hamed-khosravi-et-al-2024>(1/1 | 231/243) Binary Gaussian Copula Synthesis: A Novel Data Augmentation Technique to Advance ML-based Clinical Decision Support Systems for Early Prediction of Dialysis Among CKD Patients (Hamed Khosravi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hamed Khosravi, Srinjoy Das, Abdullah Al-Mamun, Imtiaz Ahmed. (2024)<br><strong>Binary Gaussian Copula Synthesis: A Novel Data Augmentation Technique to Advance ML-based Clinical Decision Support Systems for Early Prediction of Dialysis Among CKD Patients</strong><br><button class=copy-to-clipboard title="Binary Gaussian Copula Synthesis: A Novel Data Augmentation Technique to Advance ML-based Clinical Decision Support Systems for Early Prediction of Dialysis Among CKD Patients" index=231>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-231 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.AP<br>Categories: cs-AI, cs-LG, stat-AP, stat.AP<br>Keyword Score: 10<br>Keywords: Data Augmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00965v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00965v1.pdf filename=2403.00965v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Center for Disease Control estimates that over 37 million US adults suffer from chronic kidney disease (CKD), yet 9 out of 10 of these individuals are unaware of their condition due to the absence of symptoms in the early stages. It has a significant impact on patients&rsquo; quality of life, particularly when it progresses to the need for dialysis. Early prediction of dialysis is crucial as it can significantly improve patient outcomes and assist healthcare providers in making timely and informed decisions. However, developing an effective machine learning (ML)-based Clinical Decision Support System (CDSS) for early dialysis prediction poses a key challenge due to the imbalanced nature of <b>data.</b> <b>To</b> address this challenge, this study evaluates various <b>data</b> <b>augmentation</b> techniques to understand their effectiveness on real-world datasets. We propose a new approach named Binary Gaussian Copula Synthesis (BGCS). BGCS is tailored for binary medical datasets and excels in generating synthetic minority <b>data</b> <b>that</b> mirrors the distribution of the original <b>data.</b> <b>BGCS</b> enhances early dialysis prediction by outperforming traditional methods in detecting dialysis patients. For the best ML model, Random Forest, BCGS achieved a 72% improvement, surpassing the state-of-the-art augmentation approaches. Also, we present a ML-based CDSS, designed to aid clinicians in making informed decisions. CDSS, which utilizes decision tree models, is developed to improve patient outcomes, identify critical variables, and thereby enable clinicians to make proactive decisions, and strategize treatment plans effectively for CKD patients who are more likely to require dialysis in the near future. Through comprehensive feature analysis and meticulous <b>data</b> <b>preparation,</b> we ensure that the CDSS&rsquo;s dialysis predictions are not only accurate but also actionable, providing a valuable tool in the management and treatment of CKD.</p></p class="citation"></blockquote><h2 id=cslo-2>cs.LO (2)</h2><h3 id=12--232243-semi-automated-modular-formal-verification-of-critical-software-liveness-and-completeness-thresholds-tobias-reinhard-2024>(1/2 | 232/243) Semi-Automated Modular Formal Verification of Critical Software: Liveness and Completeness Thresholds (Tobias Reinhard, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tobias Reinhard. (2024)<br><strong>Semi-Automated Modular Formal Verification of Critical Software: Liveness and Completeness Thresholds</strong><br><button class=copy-to-clipboard title="Semi-Automated Modular Formal Verification of Critical Software: Liveness and Completeness Thresholds" index=232>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-232 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LO<br>Categories: F-3-1; D-2-4, cs-LO, cs.LO<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00934v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00934v1.pdf filename=2403.00934v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this dissertation we describe two contributions to the state of the art in <b>reasoning</b> about liveness and safety, respectively. Programs for multiprocessor machines commonly perform busy waiting for synchronization. We propose the first separation logic for modularly verifying termination of such programs under fair scheduling. Our logic requires the proof author to associate a ghost signal with each busy-waiting loop and allows such loops to iterate while their corresponding signal $s$ is not set. The proof author further has to define a well-founded order on signals and to prove that if the looping thread holds an obligation to set a signal $s&rsquo;$, then $s&rsquo;$ is ordered above $s$. By using conventional shared state invariants to associate the state of ghost signals with the state of data structures, programs busy-waiting for arbitrary conditions over arbitrary data structures can be verified. Moreover, we present the first study of completeness thresholds for bounded memory safety proofs. Specifically, we consider heap-manipulating programs that iterate over arrays without allocating or freeing memory. In this setting, we present the first notion of completeness thresholds for program verification which reduce unbounded memory safety proofs to bounded ones. Furthermore, we demonstrate that we can characterise completeness thresholds for simple classes of array traversing programs. Finally, we suggest avenues of research to scale this technique theoretically, i.e., to larger classes of programs (heap manipulation, tree-like data structures), and practically by highlighting automation opportunities.</p></p class="citation"></blockquote><h3 id=22--233243-analyzing-divergence-for-nondeterministic-probabilistic-models-hao-wu-et-al-2024>(2/2 | 233/243) Analyzing Divergence for Nondeterministic Probabilistic Models (Hao Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hao Wu, Yuxi Fu, Huan Long, Xian Xu, Wenbo Zhang. (2024)<br><strong>Analyzing Divergence for Nondeterministic Probabilistic Models</strong><br><button class=copy-to-clipboard title="Analyzing Divergence for Nondeterministic Probabilistic Models" index=233>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-233 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LO<br>Categories: cs-LO, cs.LO<br>Keyword Score: 10<br>Keywords: Probabilistic Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00491v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00491v1.pdf filename=2403.00491v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Branching and weak <b>probabilistic</b> <b>bisimilarities</b> are two well-known notions capturing behavioral equivalence between nondeterministic <b>probabilistic</b> <b>systems.</b> For <b>probabilistic</b> <b>systems,</b> divergence is of major concern. Recently several divergence-sensitive refinements of branching and weak <b>probabilistic</b> <b>bisimilarities</b> have been proposed in the literature. Both the definitions of these equivalences and the techniques to investigate them differ significantly. This paper presents a comprehensive comparative study on divergence-sensitive behavioral equivalence relations that refine the branching and weak <b>probabilistic</b> <b>bisimilarities.</b> Additionally, these equivalence relations are shown to have efficient checking algorithms. The techniques of this paper might be of independent interest in a more general setting.</p></p class="citation"></blockquote><h2 id=mathoc-1>math.OC (1)</h2><h3 id=11--234243-primal-dual-ilqr-joão-sousa-pinto-et-al-2024>(1/1 | 234/243) Primal-Dual iLQR (João Sousa-Pinto et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>João Sousa-Pinto, Dominique Orban. (2024)<br><strong>Primal-Dual iLQR</strong><br><button class=copy-to-clipboard title="Primal-Dual iLQR" index=234>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-234 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: 49M15, G-1-6, cs-RO, math-OC, math.OC<br>Keyword Score: 10<br>Keywords: Discrete Time, Discrete Time<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00748v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00748v2.pdf filename=2403.00748v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce a new algorithm for solving unconstrained <b>discrete-time</b> <b>optimal</b> control problems. Our method follows a direct multiple shooting approach, and consists of applying the SQP method together with an $\ell_2$ augmented Lagrangian primal-dual merit function. We use the LQR algorithm to efficiently solve the primal-dual SQP problem. As our algorithm is a specialization of NPSQP (Gill et al. 1992), it inherits its generic properties, including global convergence, fast local convergence, and the lack of need for second order corrections, improving on existing direct multiple shooting approaches such as GNMS (Giftthaler et al. 2018) and FDDP (Mastalli et al. 2020).</p></p class="citation"></blockquote><h2 id=q-fincp-1>q-fin.CP (1)</h2><h3 id=11--235243-a-time-stepping-deep-gradient-flow-method-for-option-pricing-in-rough-diffusion-models-antonis-papapantoleon-et-al-2024>(1/1 | 235/243) A time-stepping deep gradient flow method for option pricing in (rough) diffusion models (Antonis Papapantoleon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Antonis Papapantoleon, Jasper Rou. (2024)<br><strong>A time-stepping deep gradient flow method for option pricing in (rough) diffusion models</strong><br><button class=copy-to-clipboard title="A time-stepping deep gradient flow method for option pricing in (rough) diffusion models" index=235>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-235 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-fin.CP<br>Categories: 91G20, 91G60, 68T07, cs-LG, math-PR, q-fin-CP, q-fin-MF, q-fin.CP<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00746v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00746v1.pdf filename=2403.00746v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We develop a novel deep learning approach for pricing European options in <b>diffusion</b> <b>models,</b> that can efficiently handle high-dimensional problems resulting from Markovian approximations of rough volatility models. The option pricing partial differential equation is reformulated as an energy minimization problem, which is approximated in a time-stepping fashion by deep artificial neural networks. The proposed scheme respects the asymptotic behavior of option prices for large levels of moneyness, and adheres to a priori known bounds for option prices. The accuracy and efficiency of the proposed method is assessed in a series of numerical examples, with particular focus in the lifted Heston model.</p></p class="citation"></blockquote><h2 id=astro-phim-1>astro-ph.IM (1)</h2><h3 id=11--236243-autonomous-robotic-arm-manipulation-for-planetary-missions-using-causal-machine-learning-c-mcdonnell-et-al-2024>(1/1 | 236/243) Autonomous Robotic Arm Manipulation for Planetary Missions using Causal Machine Learning (C. McDonnell et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>C. McDonnell, M. Arana-Catania, S. Upadhyay. (2024)<br><strong>Autonomous Robotic Arm Manipulation for Planetary Missions using Causal Machine Learning</strong><br><button class=copy-to-clipboard title="Autonomous Robotic Arm Manipulation for Planetary Missions using Causal Machine Learning" index=236>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-236 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: astro-ph.IM<br>Categories: astro-ph-EP, astro-ph-IM, astro-ph.IM, cs-LG, cs-RO<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00470v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00470v1.pdf filename=2403.00470v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Autonomous robotic arm manipulators have the potential to make planetary exploration and in-situ resource utilization missions more time efficient and productive, as the manipulator can handle the objects itself and perform goal-specific actions. We train a manipulator to autonomously study objects of which it has no prior knowledge, such as planetary rocks. This is achieved using causal machine learning in a simulated planetary environment. Here, the manipulator interacts with objects, and classifies them based on differing causal factors. These are parameters, such as mass or friction coefficient, that causally determine the outcomes of its interactions. Through <b>reinforcement</b> <b>learning,</b> the manipulator learns to interact in ways that reveal the underlying causal factors. We show that this method works even without any prior knowledge of the objects, or any previously-collected training data. We carry out the training in planetary exploration conditions, with realistic manipulator models.</p></p class="citation"></blockquote><h2 id=cond-matmtrl-sci-1>cond-mat.mtrl-sci (1)</h2><h3 id=11--237243-deciphering-diffuse-scattering-with-machine-learning-and-the-equivariant-foundation-model-the-case-of-molten-feo-ganesh-sivaraman-et-al-2024>(1/1 | 237/243) Deciphering diffuse scattering with machine learning and the equivariant foundation model: The case of molten FeO (Ganesh Sivaraman et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ganesh Sivaraman, Chris J. Benmore. (2024)<br><strong>Deciphering diffuse scattering with machine learning and the equivariant foundation model: The case of molten FeO</strong><br><button class=copy-to-clipboard title="Deciphering diffuse scattering with machine learning and the equivariant foundation model: The case of molten FeO" index=237>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-237 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cond-mat.mtrl-sci<br>Categories: cond-mat-mtrl-sci, cond-mat.mtrl-sci, cs-LG<br>Keyword Score: 10<br>Keywords: Foundation Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00259v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00259v1.pdf filename=2403.00259v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Bridging the gap between diffuse x-ray or neutron scattering measurements and predicted structures derived from atom-atom pair potentials in disordered materials, has been a longstanding challenge in condensed matter physics. This perspective gives a brief overview of the traditional approaches employed over the past several decades. Namely, the use of approximate interatomic pair potentials that relate 3-dimensional structural models to the measured structure factor and its associated pair distribution function. The use of machine learned interatomic potentials has grown in the past few years, and has been particularly successful in the cases of ionic and oxide systems. Recent advances in large scale sampling, along with a direct integration of scattering measurements into the model development, has provided improved agreement between experiments and large-scale models calculated with quantum mechanical accuracy. However, details of local polyhedral bonding and connectivity in meta-stable disordered systems still require improvement. Here we leverage MACE-MP-0; a newly introduced equivariant <b>foundation</b> <b>model</b> and validate the results against high-quality experimental scattering data for the case of molten iron(II) oxide (FeO). These preliminary results suggest that the emerging <b>foundation</b> <b>model</b> has the potential to surpass the traditional limitations of classical interatomic potentials.</p></p class="citation"></blockquote><h2 id=cscg-1>cs.CG (1)</h2><h3 id=11--238243-happy-ending-an-empty-hexagon-in-every-set-of-30-points-marijn-j-h-heule-et-al-2024>(1/1 | 238/243) Happy Ending: An Empty Hexagon in Every Set of 30 Points (Marijn J. H. Heule et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marijn J. H. Heule, Manfred Scheucher. (2024)<br><strong>Happy Ending: An Empty Hexagon in Every Set of 30 Points</strong><br><button class=copy-to-clipboard title="Happy Ending: An Empty Hexagon in Every Set of 30 Points" index=238>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-238 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CG<br>Categories: cs-CG, cs-LO, cs.CG, math-CO<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00737v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00737v1.pdf filename=2403.00737v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Satisfiability solving has been used to tackle a range of long-standing open math problems in recent years. We add another success by solving a <b>geometry</b> problem that originated a century ago. In the 1930s, Esther Klein&rsquo;s exploration of unavoidable shapes in planar point sets in general position showed that every set of five points includes four points in convex position. For a long time, it was open if an empty hexagon, i.e., six points in convex position without a point inside, can be avoided. In 2006, Gerken and Nicol'as independently proved that the answer is no. We establish the exact bound: Every 30-point set in the plane in general position contains an empty hexagon. Our key contributions include an effective, compact encoding and a search-space partitioning strategy enabling linear-time speedups even when using thousands of cores.</p></p class="citation"></blockquote><h2 id=mathmg-1>math.MG (1)</h2><h3 id=11--239243-path-tracking-using-echoes-in-an-unknown-environment-the-issue-of-symmetries-and-how-to-break-them-mireille-boutin-et-al-2024>(1/1 | 239/243) Path Tracking using Echoes in an Unknown Environment: the Issue of Symmetries and How to Break Them (Mireille Boutin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mireille Boutin, Gregor Kemper. (2024)<br><strong>Path Tracking using Echoes in an Unknown Environment: the Issue of Symmetries and How to Break Them</strong><br><button class=copy-to-clipboard title="Path Tracking using Echoes in an Unknown Environment: the Issue of Symmetries and How to Break Them" index=239>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-239 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.MG<br>Categories: 51K99, 51-08, 70E60, cs-RO, math-MG, math.MG<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00698v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00698v1.pdf filename=2403.00698v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper deals with the problem of reconstructing the path of a vehicle in an unknown environment consisting of planar structures using sound. Many systems in the literature do this by using a loudspeaker and microphones mounted on a vehicle. Symmetries in the environment lead to solution ambiguities for such systems. We propose to resolve this issue by placing the loudspeaker at a fixed location in the environment rather than on the vehicle. The question of whether this will remove ambiguities regardless of the environment <b>geometry</b> leads to a question about breaking symmetries that can be phrased in purely mathematical terms. We solve this question in the affirmative if the <b>geometry</b> is in dimension three or bigger, and give counterexamples in dimension two. Excluding the rare situations where the counterexamples arise, we also give an affirmative answer in dimension two. Our results lead to a simple path reconstruction algorithm for a vehicle carrying four microphones navigating within an environment in which a loudspeaker at a fixed position emits short bursts of sounds. This algorithm could be combined with other methods from the literature to construct a path tracking system for vehicles navigating within a potentially symmetric environment.</p></p class="citation"></blockquote><h2 id=mathco-1>math.CO (1)</h2><h3 id=11--240243-edge-open-packing-complexity-algorithmic-aspects-and-bounds-boštjan-brešar-et-al-2024>(1/1 | 240/243) Edge open packing: complexity, algorithmic aspects, and bounds (Boštjan Brešar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Boštjan Brešar, Babak Samadi. (2024)<br><strong>Edge open packing: complexity, algorithmic aspects, and bounds</strong><br><button class=copy-to-clipboard title="Edge open packing: complexity, algorithmic aspects, and bounds" index=240>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-240 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.CO<br>Categories: 05C70, 05C85, 68Q17, cs-DM, math-CO, math.CO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00750v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00750v1.pdf filename=2403.00750v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Given a <b>graph</b> $G$, two edges $e_{1},e_{2}\in E(G)$ are said to have a common edge $e$ if $e$ joins an endvertex of $e_{1}$ to an endvertex of $e_{2}$. A subset $B\subseteq E(G)$ is an edge open packing set in $G$ if no two edges of $B$ have a common edge in $G$, and the maximum cardinality of such a set in $G$ is called the edge open packing number, $\rho_{e}^{o}(G)$, of $G$. In this paper, we prove that the decision version of the edge open packing number is NP-complete even when restricted to <b>graphs</b> with universal vertices, Eulerian bipartite <b>graphs,</b> and planar <b>graphs</b> with maximum degree $4$, respectively. In contrast, we present a linear-time algorithm that computes the edge open packing number of a tree. We also resolve two problems posed in the seminal paper [Edge open packing sets in <b>graphs,</b> RAIRO-Oper.\ Res.\ 56 (2022) 3765&ndash;3776]. Notably, we characterize the <b>graphs</b> $G$ that attain the upper bound $\rho_e^o(G)\le |E(G)|/\delta(G)$, and provide lower and upper bounds for the edge-deleted subgraph of a <b>graph</b> and establish the corresponding realization result.</p></p class="citation"></blockquote><h2 id=q-fingn-1>q-fin.GN (1)</h2><h3 id=11--241243-assessing-the-efficacy-of-heuristic-based-address-clustering-for-bitcoin-hugo-schnoering-et-al-2024>(1/1 | 241/243) Assessing the Efficacy of Heuristic-Based Address Clustering for Bitcoin (Hugo Schnoering et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hugo Schnoering, Pierre Porthaux, Michalis Vazirgiannis. (2024)<br><strong>Assessing the Efficacy of Heuristic-Based Address Clustering for Bitcoin</strong><br><button class=copy-to-clipboard title="Assessing the Efficacy of Heuristic-Based Address Clustering for Bitcoin" index=241>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-241 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-fin.GN<br>Categories: cs-CR, cs-SI, q-fin-GN, q-fin.GN<br>Keyword Score: 3<br>Keywords: Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00523v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00523v1.pdf filename=2403.00523v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Exploring transactions within the Bitcoin blockchain entails examining the transfer of bitcoins among several hundred million entities. However, it is often impractical and resource-consuming to study such a vast number of entities. Consequently, entity <b>clustering</b> serves as an initial step in most analytical studies. This process often employs heuristics grounded in the practices and behaviors of these entities. In this research, we delve into the examination of two widely used heuristics, alongside the introduction of four novel ones. Our contribution includes the introduction of the \textit{clustering ratio}, a metric designed to quantify the reduction in the number of entities achieved by a given heuristic. The assessment of this reduction ratio plays an important role in justifying the selection of a specific heuristic for analytical purposes. Given the dynamic nature of the Bitcoin system, characterized by a continuous increase in the number of entities on the blockchain, and the evolving behaviors of these entities, we extend our study to explore the temporal evolution of the <b>clustering</b> ratio for each heuristic. This temporal analysis enhances our understanding of the effectiveness of these heuristics over time.</p></p class="citation"></blockquote><h2 id=cscc-1>cs.CC (1)</h2><h3 id=11--242243-graph-homomorphism-monotone-classes-and-bounded-pathwidth-tala-eagling-vose-et-al-2024>(1/1 | 242/243) Graph Homomorphism, Monotone Classes and Bounded Pathwidth (Tala Eagling-Vose et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tala Eagling-Vose, Barnaby Martin, Daniel Paulusma, Mark Siggers, Siani Smith. (2024)<br><strong>Graph Homomorphism, Monotone Classes and Bounded Pathwidth</strong><br><button class=copy-to-clipboard title="Graph Homomorphism, Monotone Classes and Bounded Pathwidth" index=242>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-242 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CC<br>Categories: cs-CC, cs-LO, cs.CC<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00497v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00497v1.pdf filename=2403.00497v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A recent paper describes a framework for studying the computational complexity of <b>graph</b> problems on monotone classes, that is those omitting a set of <b>graphs</b> as a subgraph. If the problems lie in the framework, and many do, then the computational complexity can be described for all monotone classes defined by a finite set of omitted subgraphs. It is known that certain homomorphism problems, e.g. $C_5$-Colouring, do not sit in the framework. By contrast, we show that the more general problem of <b>Graph</b> Homomorphism does sit in the framework. The original framework had examples where hard versus easy were NP-complete versus P, or at least quadratic versus almost linear. We give the first example of a problem in the framework such that hardness is in the polynomial hierarchy above NP. Considering a variant of the colouring game as studied by Bodlaender, we show that with the restriction of bounded alternation, the list version of this problem is contained in the framework. The hard cases are $\Pi_{2k}^\mathrm{P}$-complete and the easy cases are in P. The cases in P comprise those classes for which the pathwidth is bounded. Bodlaender explains that Sequential $3$-Colouring Construction Game is in P on classes with bounded vertex separation number, which coincides with bounded pathwidth on unordered <b>graphs.</b> However, these <b>graphs</b> are ordered with a playing order for the two players, which corresponds to a prefix pattern in a quantified formula. We prove that Sequential $3$-Colouring Construction Game is Pspace-complete on some class of bounded pathwidth, using a celebrated result of Atserias and Oliva. We consider several locally constrained variants of the homomorphism problem. Like $C_5$-Colouring, none of these is in the framework. However, when we consider the bounded-degree restrictions, we prove that each of these problems is in our framework.</p></p class="citation"></blockquote><h2 id=csgr-1>cs.GR (1)</h2><h3 id=11--243243-hybrid-base-complex-extract-and-visualize-structure-of-hex-dominant-meshes-lei-si-et-al-2024>(1/1 | 243/243) Hybrid Base Complex: Extract and Visualize Structure of Hex-dominant Meshes (Lei Si et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lei Si, Haowei Cao, Guoning Chen. (2024)<br><strong>Hybrid Base Complex: Extract and Visualize Structure of Hex-dominant Meshes</strong><br><button class=copy-to-clipboard title="Hybrid Base Complex: Extract and Visualize Structure of Hex-dominant Meshes" index=243>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-243 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GR<br>Categories: cs-GR, cs.GR<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00300v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00300v1.pdf filename=2403.00300v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Hex-dominant mesh generation has received significant attention in recent research due to its superior robustness compared to pure hex-mesh generation techniques. In this work, we introduce the first structure for analyzing hex-dominant meshes. This structure builds on the base complex of pure hex-meshes but incorporates the non-hex elements for a more comprehensive and complete representation. We provide its definition and describe its construction steps. Based on this structure, we present an extraction and categorization of sheets using advanced <b>graph</b> matching techniques to handle the non-hex elements. This enables us to develop an enhanced visual analysis of the structure for any hex-dominant meshes.We apply this structure-based visual analysis to compare hex-dominant meshes generated by different methods to study their advantages and disadvantages. This complements the standard quality metric based on the non-hex element percentage for hex-dominant meshes. Moreover, we propose a strategy to extract a cleaned (optimized) valence-based singularity <b>graph</b> wireframe to analyze the structure for both mesh and sheets. Our results demonstrate that the proposed hybrid base complex provides a coarse representation for mesh element, and the proposed valence singularity <b>graph</b> wireframe provides a better internal visualization of hex-dominant meshes.</p></p class="citation"></blockquote></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/ title="arXiv @ 2024.03.02" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Prev</div><div class=next-prev-text>arXiv @ 2024.03.02</div></a></div><div class="col-md-6 next-article"><a href=/akitenkrad-blog/posts/arxiv/202403/20240304000000/ title="arXiv @ 2024.03.04" class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>arXiv @ 2024.03.04</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#primary-categories>Primary Categories</a></li><li><a href=#keywords>Keywords</a></li><li><a href=#cscl-37>cs.CL (37)</a><ul><li><a href=#137--1243-softtiger-a-clinical-foundation-model-for-healthcare-workflows-ye-chen-et-al-2024>(1/37 | 1/243) SoftTiger: A Clinical Foundation Model for Healthcare Workflows (Ye Chen et al., 2024)</a></li><li><a href=#237--2243-malto-at-semeval-2024-task-6-leveraging-synthetic-data-for-llm-hallucination-detection-federico-borra-et-al-2024>(2/37 | 2/243) MALTO at SemEval-2024 Task 6: Leveraging Synthetic Data for LLM Hallucination Detection (Federico Borra et al., 2024)</a></li><li><a href=#337--3243-surveying-the-dead-minds-historical-psychological-text-analysis-with-contextualized-construct-representation-ccr-for-classical-chinese-yuqi-chen-et-al-2024>(3/37 | 3/243) Surveying the Dead Minds: Historical-Psychological Text Analysis with Contextualized Construct Representation (CCR) for Classical Chinese (Yuqi Chen et al., 2024)</a></li><li><a href=#437--4243-cross-lingual-learning-vs-low-resource-fine-tuning-a-case-study-with-fact-checking-in-turkish-recep-firat-cekinel-et-al-2024>(4/37 | 4/243) Cross-Lingual Learning vs. Low-Resource Fine-Tuning: A Case Study with Fact-Checking in Turkish (Recep Firat Cekinel et al., 2024)</a></li><li><a href=#537--5243-llms-for-targeted-sentiment-in-news-headlines-exploring-different-levels-of-prompt-prescriptiveness-jana-juroš-et-al-2024>(5/37 | 5/243) LLMs for Targeted Sentiment in News Headlines: Exploring Different Levels of Prompt Prescriptiveness (Jana Juroš et al., 2024)</a></li><li><a href=#637--6243-improving-socratic-question-generation-using-data-augmentation-and-preference-optimization-nischal-ashok-kumar-et-al-2024>(6/37 | 6/243) Improving Socratic Question Generation using Data Augmentation and Preference Optimization (Nischal Ashok Kumar et al., 2024)</a></li><li><a href=#737--7243-benchmarking-zero-shot-stance-detection-with-flant5-xxl-insights-from-training-data-prompting-and-decoding-strategies-into-its-near-sota-performance-rachith-aiyappa-et-al-2024>(7/37 | 7/243) Benchmarking zero-shot stance detection with FlanT5-XXL: Insights from training data, prompting, and decoding strategies into its near-SoTA performance (Rachith Aiyappa et al., 2024)</a></li><li><a href=#837--8243-attribute-structuring-improves-llm-based-evaluation-of-clinical-text-summaries-zelalem-gero-et-al-2024>(8/37 | 8/243) Attribute Structuring Improves LLM-Based Evaluation of Clinical Text Summaries (Zelalem Gero et al., 2024)</a></li><li><a href=#937--9243-localrqa-from-generating-data-to-locally-training-testing-and-deploying-retrieval-augmented-qa-systems-xiao-yu-et-al-2024>(9/37 | 9/243) LocalRQA: From Generating Data to Locally Training, Testing, and Deploying Retrieval-Augmented QA Systems (Xiao Yu et al., 2024)</a></li><li><a href=#1037--10243-large-language-models-for-simultaneous-named-entity-extraction-and-spelling-correction-edward-whittaker-et-al-2024>(10/37 | 10/243) Large Language Models for Simultaneous Named Entity Extraction and Spelling Correction (Edward Whittaker et al., 2024)</a></li><li><a href=#1137--11243-formulation-comparison-for-timeline-construction-using-llms-kimihiro-hasegawa-et-al-2024>(11/37 | 11/243) Formulation Comparison for Timeline Construction using LLMs (Kimihiro Hasegawa et al., 2024)</a></li><li><a href=#1237--12243-extracting-polymer-nanocomposite-samples-from-full-length-documents-ghazal-khalighinejad-et-al-2024>(12/37 | 12/243) Extracting Polymer Nanocomposite Samples from Full-Length Documents (Ghazal Khalighinejad et al., 2024)</a></li><li><a href=#1337--13243-hierarchical-indexing-for-retrieval-augmented-opinion-summarization-tom-hosking-et-al-2024>(13/37 | 13/243) Hierarchical Indexing for Retrieval-Augmented Opinion Summarization (Tom Hosking et al., 2024)</a></li><li><a href=#1437--14243-axolotl-fairness-through-assisted-self-debiasing-of-large-language-model-outputs-sana-ebrahimi-et-al-2024>(14/37 | 14/243) AXOLOTL: Fairness through Assisted Self-Debiasing of Large Language Model Outputs (Sana Ebrahimi et al., 2024)</a></li><li><a href=#1537--15243-mediswift-efficient-sparse-pre-trained-biomedical-language-models-vithursan-thangarasa-et-al-2024>(15/37 | 15/243) MediSwift: Efficient Sparse Pre-trained Biomedical Language Models (Vithursan Thangarasa et al., 2024)</a></li><li><a href=#1637--16243-standardizing-the-measurement-of-text-diversity-a-tool-and-a-comparative-analysis-of-scores-chantal-shaib-et-al-2024>(16/37 | 16/243) Standardizing the Measurement of Text Diversity: A Tool and a Comparative Analysis of Scores (Chantal Shaib et al., 2024)</a></li><li><a href=#1737--17243-lucid-llm-generated-utterances-for-complex-and-interesting-dialogues-joe-stacey-et-al-2024>(17/37 | 17/243) LUCID: LLM-Generated Utterances for Complex and Interesting Dialogues (Joe Stacey et al., 2024)</a></li><li><a href=#1837--18243-peacock-a-family-of-arabic-multimodal-large-language-models-and-benchmarks-fakhraddin-alwajih-et-al-2024>(18/37 | 18/243) Peacock: A Family of Arabic Multimodal Large Language Models and Benchmarks (Fakhraddin Alwajih et al., 2024)</a></li><li><a href=#1937--19243-autord-an-automatic-and-end-to-end-system-for-rare-disease-knowledge-graph-construction-based-on-ontologies-enhanced-large-language-models-lang-cao-et-al-2024>(19/37 | 19/243) AutoRD: An Automatic and End-to-End System for Rare Disease Knowledge Graph Construction Based on Ontologies-enhanced Large Language Models (Lang Cao et al., 2024)</a></li><li><a href=#2037--20243-merging-text-transformer-models-from-different-initializations-neha-verma-et-al-2024>(20/37 | 20/243) Merging Text Transformer Models from Different Initializations (Neha Verma et al., 2024)</a></li><li><a href=#2137--21243-self-consistent-decoding-for-more-factual-open-responses-christopher-malon-et-al-2024>(21/37 | 21/243) Self-Consistent Decoding for More Factual Open Responses (Christopher Malon et al., 2024)</a></li><li><a href=#2237--22243-diahalu-a-dialogue-level-hallucination-evaluation-benchmark-for-large-language-models-kedi-chen-et-al-2024>(22/37 | 22/243) DiaHalu: A Dialogue-level Hallucination Evaluation Benchmark for Large Language Models (Kedi Chen et al., 2024)</a></li><li><a href=#2337--23243-margin-discrepancy-based-adversarial-training-for-multi-domain-text-classification-yuan-wu-2024>(23/37 | 23/243) Margin Discrepancy-based Adversarial Training for Multi-Domain Text Classification (Yuan Wu, 2024)</a></li><li><a href=#2437--24243-mitigating-reversal-curse-via-semantic-aware-permutation-training-qingyan-guo-et-al-2024>(24/37 | 24/243) Mitigating Reversal Curse via Semantic-aware Permutation Training (Qingyan Guo et al., 2024)</a></li><li><a href=#2537--25243-rethinking-tokenization-crafting-better-tokenizers-for-large-language-models-jinbiao-yang-2024>(25/37 | 25/243) Rethinking Tokenization: Crafting Better Tokenizers for Large Language Models (Jinbiao Yang, 2024)</a></li><li><a href=#2637--26243-post-decoder-biasing-for-end-to-end-speech-recognition-of-multi-turn-medical-interview-heyang-liu-et-al-2024>(26/37 | 26/243) Post-decoder Biasing for End-to-End Speech Recognition of Multi-turn Medical Interview (Heyang Liu et al., 2024)</a></li><li><a href=#2737--27243-semi-instruct-bridging-natural-instruct-and-self-instruct-for-code-large-language-models-xianzhen-luo-et-al-2024>(27/37 | 27/243) Semi-Instruct: Bridging Natural-Instruct and Self-Instruct for Code Large Language Models (Xianzhen Luo et al., 2024)</a></li><li><a href=#2837--28243-dpp-based-adversarial-prompt-searching-for-lanugage-models-xu-zhang-et-al-2024>(28/37 | 28/243) DPP-Based Adversarial Prompt Searching for Lanugage Models (Xu Zhang et al., 2024)</a></li><li><a href=#2937--29243-gender-bias-in-large-language-models-across-multiple-languages-jinman-zhao-et-al-2024>(29/37 | 29/243) Gender Bias in Large Language Models across Multiple Languages (Jinman Zhao et al., 2024)</a></li><li><a href=#3037--30243-transcription-and-translation-of-videos-using-fine-tuned-xlsr-wav2vec2-on-custom-dataset-and-mbart-aniket-tathe-et-al-2024>(30/37 | 30/243) Transcription and translation of videos using fine-tuned XLSR Wav2Vec2 on custom dataset and mBART (Aniket Tathe et al., 2024)</a></li><li><a href=#3137--31243-few-shot-relation-extraction-with-hybrid-visual-evidence-jiaying-gong-et-al-2024>(31/37 | 31/243) Few-Shot Relation Extraction with Hybrid Visual Evidence (Jiaying Gong et al., 2024)</a></li><li><a href=#3237--32243-do-zombies-understand-a-choose-your-own-adventure-exploration-of-machine-cognition-ariel-goldstein-et-al-2024>(32/37 | 32/243) Do Zombies Understand? A Choose-Your-Own-Adventure Exploration of Machine Cognition (Ariel Goldstein et al., 2024)</a></li><li><a href=#3337--33243-rome-memorization-insights-from-text-probability-and-hidden-state-in-large-language-models-bo-li-et-al-2024>(33/37 | 33/243) ROME: Memorization Insights from Text, Probability and Hidden State in Large Language Models (Bo Li et al., 2024)</a></li><li><a href=#3437--34243-a-semantic-distance-metric-learning-approach-for-lexical-semantic-change-detection-taichi-aida-et-al-2024>(34/37 | 34/243) A Semantic Distance Metric Learning approach for Lexical Semantic Change Detection (Taichi Aida et al., 2024)</a></li><li><a href=#3537--35243-predictions-from-language-models-for-multiple-choice-tasks-are-not-robust-under-variation-of-scoring-methods-polina-tsvilodub-et-al-2024>(35/37 | 35/243) Predictions from language models for multiple-choice tasks are not robust under variation of scoring methods (Polina Tsvilodub et al., 2024)</a></li><li><a href=#3637--36243-self-consistent-reasoning-based-aspect-sentiment-quad-prediction-with-extract-then-assign-strategy-jieyong-kim-et-al-2024>(36/37 | 36/243) Self-Consistent Reasoning-based Aspect-Sentiment Quad Prediction with Extract-Then-Assign Strategy (Jieyong Kim et al., 2024)</a></li><li><a href=#3737--37243-word-order-and-world-knowledge-qinghua-zhao-et-al-2024>(37/37 | 37/243) Word Order and World Knowledge (Qinghua Zhao et al., 2024)</a></li></ul></li><li><a href=#csdb-2>cs.DB (2)</a><ul><li><a href=#12--38243-dfin-sql-integrating-focused-schema-with-din-sql-for-superior-accuracy-in-large-scale-databases-shai-volvovsky-et-al-2024>(1/2 | 38/243) DFIN-SQL: Integrating Focused Schema with DIN-SQL for Superior Accuracy in Large-Scale Databases (Shai Volvovsky et al., 2024)</a></li><li><a href=#22--39243-text-classification-of-column-headers-with-a-controlled-vocabulary-leveraging-llms-for-metadata-enrichment-margherita-martorana-et-al-2024>(2/2 | 39/243) Text classification of column headers with a controlled vocabulary: leveraging LLMs for metadata enrichment (Margherita Martorana et al., 2024)</a></li></ul></li><li><a href=#cslg-40>cs.LG (40)</a><ul><li><a href=#140--40243-differentially-private-knowledge-distillation-via-synthetic-text-generation-james-flemings-et-al-2024>(1/40 | 40/243) Differentially Private Knowledge Distillation via Synthetic Text Generation (James Flemings et al., 2024)</a></li><li><a href=#240--41243-a-survey-of-geometric-graph-neural-networks-data-structures-models-and-applications-jiaqi-han-et-al-2024>(2/40 | 41/243) A Survey of Geometric Graph Neural Networks: Data Structures, Models and Applications (Jiaqi Han et al., 2024)</a></li><li><a href=#340--42243-cloud-based-federated-learning-framework-for-mri-segmentation-rukesh-prajapati-et-al-2024>(3/40 | 42/243) Cloud-based Federated Learning Framework for MRI Segmentation (Rukesh Prajapati et al., 2024)</a></li><li><a href=#440--43243-overestimation-overfitting-and-plasticity-in-actor-critic-the-bitter-lesson-of-reinforcement-learning-michal-nauman-et-al-2024>(4/40 | 43/243) Overestimation, Overfitting, and Plasticity in Actor-Critic: the Bitter Lesson of Reinforcement Learning (Michal Nauman et al., 2024)</a></li><li><a href=#540--44243-a-regularization-based-transfer-learning-method-for-information-extraction-via-instructed-graph-decoder-kedi-chen-et-al-2024>(5/40 | 44/243) A Regularization-based Transfer Learning Method for Information Extraction via Instructed Graph Decoder (Kedi Chen et al., 2024)</a></li><li><a href=#640--45243-equipment-health-assessment-time-series-analysis-for-wind-turbine-performance-jana-backhus-et-al-2024>(6/40 | 45/243) Equipment Health Assessment: Time Series Analysis for Wind Turbine Performance (Jana Backhus et al., 2024)</a></li><li><a href=#740--46243-atp-enabling-fast-llm-serving-via-attention-on-top-principal-keys-yue-niu-et-al-2024>(7/40 | 46/243) ATP: Enabling Fast LLM Serving via Attention on Top Principal Keys (Yue Niu et al., 2024)</a></li><li><a href=#840--47243-scale-free-adversarial-reinforcement-learning-mingyu-chen-et-al-2024>(8/40 | 47/243) Scale-free Adversarial Reinforcement Learning (Mingyu Chen et al., 2024)</a></li><li><a href=#940--48243-bias-mitigation-in-fine-tuning-pre-trained-models-for-enhanced-fairness-and-efficiency-yixuan-zhang-et-al-2024>(9/40 | 48/243) Bias Mitigation in Fine-tuning Pre-trained Models for Enhanced Fairness and Efficiency (Yixuan Zhang et al., 2024)</a></li><li><a href=#1040--49243-robust-deep-reinforcement-learning-through-adversarial-attacks-and-training--a-survey-lucas-schott-et-al-2024>(10/40 | 49/243) Robust Deep Reinforcement Learning Through Adversarial Attacks and Training : A Survey (Lucas Schott et al., 2024)</a></li><li><a href=#1140--50243-robust-policy-learning-via-offline-skill-diffusion-woo-kyung-kim-et-al-2024>(11/40 | 50/243) Robust Policy Learning via Offline Skill Diffusion (Woo Kyung Kim et al., 2024)</a></li><li><a href=#1240--51243-efficient-reinforcement-learning-for-global-decision-making-in-the-presence-of-local-agents-at-scale-emile-anand-et-al-2024>(12/40 | 51/243) Efficient Reinforcement Learning for Global Decision Making in the Presence of Local Agents at Scale (Emile Anand et al., 2024)</a></li><li><a href=#1340--52243-beyond-single-model-views-for-deep-learning-optimization-versus-generalizability-of-stochastic-optimization-algorithms-toki-tahmid-inan-et-al-2024>(13/40 | 52/243) Beyond Single-Model Views for Deep Learning: Optimization versus Generalizability of Stochastic Optimization Algorithms (Toki Tahmid Inan et al., 2024)</a></li><li><a href=#1440--53243-graph-construction-with-flexible-nodes-for-traffic-demand-prediction-jinyan-hou-et-al-2024>(14/40 | 53/243) Graph Construction with Flexible Nodes for Traffic Demand Prediction (Jinyan Hou et al., 2024)</a></li><li><a href=#1540--54243-on-the-role-of-information-structure-in-reinforcement-learning-for-partially-observable-sequential-teams-and-games-awni-altabaa-et-al-2024>(15/40 | 54/243) On the Role of Information Structure in Reinforcement Learning for Partially-Observable Sequential Teams and Games (Awni Altabaa et al., 2024)</a></li><li><a href=#1640--55243-subhomogeneous-deep-equilibrium-models-pietro-sittoni-et-al-2024>(16/40 | 55/243) Subhomogeneous Deep Equilibrium Models (Pietro Sittoni et al., 2024)</a></li><li><a href=#1740--56243-nonlinear-sheaf-diffusion-in-graph-neural-networks-olga-zaghen-2024>(17/40 | 56/243) Nonlinear Sheaf Diffusion in Graph Neural Networks (Olga Zaghen, 2024)</a></li><li><a href=#1840--57243-distributional-dataset-distillation-with-subtask-decomposition-tian-qin-et-al-2024>(18/40 | 57/243) Distributional Dataset Distillation with Subtask Decomposition (Tian Qin et al., 2024)</a></li><li><a href=#1940--58243-fine-tuning-with-very-large-dropout-jianyu-zhang-et-al-2024>(19/40 | 58/243) Fine-tuning with Very Large Dropout (Jianyu Zhang et al., 2024)</a></li><li><a href=#2040--59243-resilience-of-entropy-model-in-distributed-neural-networks-milin-zhang-et-al-2024>(20/40 | 59/243) Resilience of Entropy Model in Distributed Neural Networks (Milin Zhang et al., 2024)</a></li><li><a href=#2140--60243-atp-an-efficient-and-scalable-method-for-localizing-llm-behaviour-to-components-jános-kramár-et-al-2024>(21/40 | 60/243) AtP*: An efficient and scalable method for localizing LLM behaviour to components (János Kramár et al., 2024)</a></li><li><a href=#2240--61243-rethinking-the-uniformity-metric-in-self-supervised-learning-xianghong-fang-et-al-2024>(22/40 | 61/243) Rethinking The Uniformity Metric in Self-Supervised Learning (Xianghong Fang et al., 2024)</a></li><li><a href=#2340--62243-epsilon-greedy-thompson-sampling-to-bayesian-optimization-bach-do-et-al-2024>(23/40 | 62/243) Epsilon-Greedy Thompson Sampling to Bayesian Optimization (Bach Do et al., 2024)</a></li><li><a href=#2440--63243-fractal-interpolation-in-the-context-of-prediction-accuracy-optimization-alexandra-baicoianu-et-al-2024>(24/40 | 63/243) Fractal interpolation in the context of prediction accuracy optimization (Alexandra Baicoianu et al., 2024)</a></li><li><a href=#2540--64243-fedrdma-communication-efficient-cross-silo-federated-llm-via-chunked-rdma-transmission-zeling-zhang-et-al-2024>(25/40 | 64/243) FedRDMA: Communication-Efficient Cross-Silo Federated LLM via Chunked RDMA Transmission (Zeling Zhang et al., 2024)</a></li><li><a href=#2640--65243-advancing-additive-manufacturing-through-deep-learning-a-comprehensive-review-of-current-progress-and-future-challenges-amirul-islam-saimon-et-al-2024>(26/40 | 65/243) Advancing Additive Manufacturing through Deep Learning: A Comprehensive Review of Current Progress and Future Challenges (Amirul Islam Saimon et al., 2024)</a></li><li><a href=#2740--66243-adaptive-learning-rate-for-follow-the-regularized-leader-competitive-ratio-analysis-and-best-of-both-worlds-shinji-ito-et-al-2024>(27/40 | 66/243) Adaptive Learning Rate for Follow-the-Regularized-Leader: Competitive Ratio Analysis and Best-of-Both-Worlds (Shinji Ito et al., 2024)</a></li><li><a href=#2840--67243-reusing-historical-trajectories-in-natural-policy-gradient-via-importance-sampling-convergence-and-convergence-rate-yifan-lin-et-al-2024>(28/40 | 67/243) Reusing Historical Trajectories in Natural Policy Gradient via Importance Sampling: Convergence and Convergence Rate (Yifan Lin et al., 2024)</a></li><li><a href=#2940--68243-snapshot-reinforcement-learning-leveraging-prior-trajectories-for-efficiency-yanxiao-zhao-et-al-2024>(29/40 | 68/243) Snapshot Reinforcement Learning: Leveraging Prior Trajectories for Efficiency (Yanxiao Zhao et al., 2024)</a></li><li><a href=#3040--69243-efficientzero-v2-mastering-discrete-and-continuous-control-with-limited-data-shengjie-wang-et-al-2024>(30/40 | 69/243) EfficientZero V2: Mastering Discrete and Continuous Control with Limited Data (Shengjie Wang et al., 2024)</a></li><li><a href=#3140--70243-motif-distribution-and-function-of-sparse-deep-neural-networks-olivia-t-zahn-et-al-2024>(31/40 | 70/243) Motif distribution and function of sparse deep neural networks (Olivia T. Zahn et al., 2024)</a></li><li><a href=#3240--71243-tree-regularized-tabular-embeddings-xuan-li-et-al-2024>(32/40 | 71/243) Tree-Regularized Tabular Embeddings (Xuan Li et al., 2024)</a></li><li><a href=#3340--72243-scalable-learning-of-item-response-theory-models-susanne-frick-et-al-2024>(33/40 | 72/243) Scalable Learning of Item Response Theory Models (Susanne Frick et al., 2024)</a></li><li><a href=#3440--73243-indirectly-parameterized-concrete-autoencoders-alfred-nilsson-et-al-2024>(34/40 | 73/243) Indirectly Parameterized Concrete Autoencoders (Alfred Nilsson et al., 2024)</a></li><li><a href=#3540--74243-provably-robust-dpo-aligning-language-models-with-noisy-feedback-sayak-ray-chowdhury-et-al-2024>(35/40 | 74/243) Provably Robust DPO: Aligning Language Models with Noisy Feedback (Sayak Ray Chowdhury et al., 2024)</a></li><li><a href=#3640--75243-disaggregated-multi-tower-topology-aware-modeling-technique-for-efficient-large-scale-recommendation-liang-luo-et-al-2024>(36/40 | 75/243) Disaggregated Multi-Tower: Topology-aware Modeling Technique for Efficient Large-Scale Recommendation (Liang Luo et al., 2024)</a></li><li><a href=#3740--76243-scale-invariant-gradient-aggregation-for-constrained-multi-objective-reinforcement-learning-dohyeong-kim-et-al-2024>(37/40 | 76/243) Scale-Invariant Gradient Aggregation for Constrained Multi-Objective Reinforcement Learning (Dohyeong Kim et al., 2024)</a></li><li><a href=#3840--77243-shifted-interpolation-for-differential-privacy-jinho-bok-et-al-2024>(38/40 | 77/243) Shifted Interpolation for Differential Privacy (Jinho Bok et al., 2024)</a></li><li><a href=#3940--78243-enhancing-multivariate-time-series-forecasting-with-mutual-information-driven-cross-variable-and-temporal-modeling-shiyi-qi-et-al-2024>(39/40 | 78/243) Enhancing Multivariate Time Series Forecasting with Mutual Information-driven Cross-Variable and Temporal Modeling (Shiyi Qi et al., 2024)</a></li><li><a href=#4040--79243-imitation-learning-datasets-a-toolkit-for-creating-datasets-training-agents-and-benchmarking-nathan-gavenski-et-al-2024>(40/40 | 79/243) Imitation Learning Datasets: A Toolkit For Creating Datasets, Training Agents and Benchmarking (Nathan Gavenski et al., 2024)</a></li></ul></li><li><a href=#cscr-9>cs.CR (9)</a><ul><li><a href=#19--80243-crimson-empowering-strategic-reasoning-in-cybersecurity-through-large-language-models-jiandong-jin-et-al-2024>(1/9 | 80/243) Crimson: Empowering Strategic Reasoning in Cybersecurity through Large Language Models (Jiandong Jin et al., 2024)</a></li><li><a href=#29--81243-gradient-cuff-detecting-jailbreak-attacks-on-large-language-models-by-exploring-refusal-loss-landscapes-xiaomeng-hu-et-al-2024>(2/9 | 81/243) Gradient Cuff: Detecting Jailbreak Attacks on Large Language Models by Exploring Refusal Loss Landscapes (Xiaomeng Hu et al., 2024)</a></li><li><a href=#39--82243-basedai-a-decentralized-p2p-network-for-zero-knowledge-large-language-models-zk-llms-sean-wellington-2024>(3/9 | 82/243) BasedAI: A decentralized P2P network for Zero Knowledge Large Language Models (ZK-LLMs) (Sean Wellington, 2024)</a></li><li><a href=#49--83243-improving-android-malware-detection-through-data-augmentation-using-wasserstein-generative-adversarial-networks-kawana-stalin-et-al-2024>(4/9 | 83/243) Improving Android Malware Detection Through Data Augmentation Using Wasserstein Generative Adversarial Networks (Kawana Stalin et al., 2024)</a></li><li><a href=#59--84243-opaf-optimized-secure-two-party-computation-protocols-for-nonlinear-activation-functions-in-recurrent-neural-network-qian-feng-et-al-2024>(5/9 | 84/243) OPAF: Optimized Secure Two-Party Computation Protocols for Nonlinear Activation Functions in Recurrent Neural Network (Qian Feng et al., 2024)</a></li><li><a href=#69--85243-private-benchmarking-to-prevent-contamination-and-improve-comparative-evaluation-of-llms-nishanth-chandran-et-al-2024>(6/9 | 85/243) Private Benchmarking to Prevent Contamination and Improve Comparative Evaluation of LLMs (Nishanth Chandran et al., 2024)</a></li><li><a href=#79--86243-blockchain-empowered-federated-learning-benefits-challenges-and-solutions-zeju-cai-et-al-2024>(7/9 | 86/243) Blockchain-empowered Federated Learning: Benefits, Challenges, and Solutions (Zeju Cai et al., 2024)</a></li><li><a href=#89--87243-teach-llms-to-phish-stealing-private-information-from-language-models-ashwinee-panda-et-al-2024>(8/9 | 87/243) Teach LLMs to Phish: Stealing Private Information from Language Models (Ashwinee Panda et al., 2024)</a></li><li><a href=#99--88243-transfer-learning-for-security-challenges-and-future-directions-adrian-shuai-li-et-al-2024>(9/9 | 88/243) Transfer Learning for Security: Challenges and Future Directions (Adrian Shuai Li et al., 2024)</a></li></ul></li><li><a href=#cscv-46>cs.CV (46)</a><ul><li><a href=#146--89243-multimodal-arxiv-a-dataset-for-improving-scientific-comprehension-of-large-vision-language-models-lei-li-et-al-2024>(1/46 | 89/243) Multimodal ArXiv: A Dataset for Improving Scientific Comprehension of Large Vision-Language Models (Lei Li et al., 2024)</a></li><li><a href=#246--90243-robust-deep-labeling-of-radiological-emphysema-subtypes-using-squeeze-and-excitation-convolutional-neural-networks-the-mesa-lung-and-spiromics-studies-artur-wysoczanski-et-al-2024>(2/46 | 90/243) Robust deep labeling of radiological emphysema subtypes using squeeze and excitation convolutional neural networks: The MESA Lung and SPIROMICS Studies (Artur Wysoczanski et al., 2024)</a></li><li><a href=#346--91243-hypersdfusion-bridging-hierarchical-structures-in-language-and-geometry-for-enhanced-3d-text2shape-generation-zhiying-leng-et-al-2024>(3/46 | 91/243) HyperSDFusion: Bridging Hierarchical Structures in Language and Geometry for Enhanced 3D Text2Shape Generation (Zhiying Leng et al., 2024)</a></li><li><a href=#446--92243-flatten-long-range-loss-landscapes-for-cross-domain-few-shot-learning-yixiong-zou-et-al-2024>(4/46 | 92/243) Flatten Long-Range Loss Landscapes for Cross-Domain Few-Shot Learning (Yixiong Zou et al., 2024)</a></li><li><a href=#546--93243-visionllama-a-unified-llama-interface-for-vision-tasks-xiangxiang-chu-et-al-2024>(5/46 | 93/243) VisionLLaMA: A Unified LLaMA Interface for Vision Tasks (Xiangxiang Chu et al., 2024)</a></li><li><a href=#646--94243-deformable-one-shot-face-stylization-via-dino-semantic-guidance-yang-zhou-et-al-2024>(6/46 | 94/243) Deformable One-shot Face Stylization via DINO Semantic Guidance (Yang Zhou et al., 2024)</a></li><li><a href=#746--95243-masklrf-self-supervised-pretraining-via-masked-autoencoding-of-local-reference-frames-for-rotation-invariant-3d-point-set-analysis-takahiko-furuya-2024>(7/46 | 95/243) MaskLRF: Self-supervised Pretraining via Masked Autoencoding of Local Reference Frames for Rotation-invariant 3D Point Set Analysis (Takahiko Furuya, 2024)</a></li><li><a href=#846--96243-improving-explicit-spatial-relationships-in-text-to-image-generation-through-an-automatically-derived-dataset-ander-salaberria-et-al-2024>(8/46 | 96/243) Improving Explicit Spatial Relationships in Text-to-Image Generation through an Automatically Derived Dataset (Ander Salaberria et al., 2024)</a></li><li><a href=#946--97243-data-efficient-event-camera-pre-training-via-disentangled-masked-modeling-zhenpeng-huang-et-al-2024>(9/46 | 97/243) Data-efficient Event Camera Pre-training via Disentangled Masked Modeling (Zhenpeng Huang et al., 2024)</a></li><li><a href=#1046--98243-invariant-test-time-adaptation-for-vision-language-model-generalization-huan-ma-et-al-2024>(10/46 | 98/243) Invariant Test-Time Adaptation for Vision-Language Model Generalization (Huan Ma et al., 2024)</a></li><li><a href=#1146--99243-odm-a-text-image-further-alignment-pre-training-approach-for-scene-text-detection-and-spotting-chen-duan-et-al-2024>(11/46 | 99/243) ODM: A Text-Image Further Alignment Pre-training Approach for Scene Text Detection and Spotting (Chen Duan et al., 2024)</a></li><li><a href=#1246--100243-halc-object-hallucination-reduction-via-adaptive-focal-contrast-decoding-zhaorun-chen-et-al-2024>(12/46 | 100/243) HALC: Object Hallucination Reduction via Adaptive Focal-Contrast Decoding (Zhaorun Chen et al., 2024)</a></li><li><a href=#1346--101243-learning-and-leveraging-world-models-in-visual-representation-learning-quentin-garrido-et-al-2024>(13/46 | 101/243) Learning and Leveraging World Models in Visual Representation Learning (Quentin Garrido et al., 2024)</a></li><li><a href=#1446--102243-tempcompass-do-video-llms-really-understand-videos-yuanxin-liu-et-al-2024>(14/46 | 102/243) TempCompass: Do Video LLMs Really Understand Videos? (Yuanxin Liu et al., 2024)</a></li><li><a href=#1546--103243-task-indicating-transformer-for-task-conditional-dense-predictions-yuxiang-lu-et-al-2024>(15/46 | 103/243) Task Indicating Transformer for Task-conditional Dense Predictions (Yuxiang Lu et al., 2024)</a></li><li><a href=#1646--104243-multi-modal-attribute-prompting-for-vision-language-models-xin-liu-et-al-2024>(16/46 | 104/243) Multi-modal Attribute Prompting for Vision-Language Models (Xin Liu et al., 2024)</a></li><li><a href=#1746--105243-region-adaptive-transform-with-segmentation-prior-for-image-compression-yuxi-liu-et-al-2024>(17/46 | 105/243) Region-Adaptive Transform with Segmentation Prior for Image Compression (Yuxi Liu et al., 2024)</a></li><li><a href=#1846--106243-flattening-singular-values-of-factorized-convolution-for-medical-images-zexin-feng-et-al-2024>(18/46 | 106/243) Flattening Singular Values of Factorized Convolution for Medical Images (Zexin Feng et al., 2024)</a></li><li><a href=#1946--107243-lomoe-localized-multi-object-editing-via-multi-diffusion-goirik-chakrabarty-et-al-2024>(19/46 | 107/243) LoMOE: Localized Multi-Object Editing via Multi-Diffusion (Goirik Chakrabarty et al., 2024)</a></li><li><a href=#2046--108243-parameter-efficient-tuning-of-large-convolutional-models-wei-chen-et-al-2024>(20/46 | 108/243) Parameter-Efficient Tuning of Large Convolutional Models (Wei Chen et al., 2024)</a></li><li><a href=#2146--109243-abductive-ego-view-accident-video-understanding-for-safe-driving-perception-jianwu-fang-et-al-2024>(21/46 | 109/243) Abductive Ego-View Accident Video Understanding for Safe Driving Perception (Jianwu Fang et al., 2024)</a></li><li><a href=#2246--110243-semantics-enhanced-cross-modal-masked-image-modeling-for-vision-language-pre-training-haowei-liu-et-al-2024>(22/46 | 110/243) Semantics-enhanced Cross-modal Masked Image Modeling for Vision-Language Pre-training (Haowei Liu et al., 2024)</a></li><li><a href=#2346--111243-glfnet-global-local-frequency-filter-networks-for-efficient-medical-image-segmentation-athanasios-tragakis-et-al-2024>(23/46 | 111/243) GLFNET: Global-Local (frequency) Filter Networks for efficient medical image segmentation (Athanasios Tragakis et al., 2024)</a></li><li><a href=#2446--112243-learning-causal-features-for-incremental-object-detection-zhenwei-he-et-al-2024>(24/46 | 112/243) Learning Causal Features for Incremental Object Detection (Zhenwei He et al., 2024)</a></li><li><a href=#2546--113243-damsdet-dynamic-adaptive-multispectral-detection-transformer-with-competitive-query-selection-and-adaptive-feature-fusion-junjie-guo-et-al-2024>(25/46 | 113/243) DAMSDet: Dynamic Adaptive Multispectral Detection Transformer with Competitive Query Selection and Adaptive Feature Fusion (Junjie Guo et al., 2024)</a></li><li><a href=#2646--114243-yolo-med--multi-task-interaction-network-for-biomedical-images-suizhi-huang-et-al-2024>(26/46 | 114/243) YOLO-MED : Multi-Task Interaction Network for Biomedical Images (Suizhi Huang et al., 2024)</a></li><li><a href=#2746--115243-revisiting-disentanglement-in-downstream-tasks-a-study-on-its-necessity-for-abstract-visual-reasoning-ruiqian-nai-et-al-2024>(27/46 | 115/243) Revisiting Disentanglement in Downstream Tasks: A Study on Its Necessity for Abstract Visual Reasoning (Ruiqian Nai et al., 2024)</a></li><li><a href=#2846--116243-can-transformers-capture-spatial-relations-between-objects-chuan-wen-et-al-2024>(28/46 | 116/243) Can Transformers Capture Spatial Relations between Objects? (Chuan Wen et al., 2024)</a></li><li><a href=#2946--117243-rethinking-few-shot-3d-point-cloud-semantic-segmentation-zhaochong-an-et-al-2024>(29/46 | 117/243) Rethinking Few-shot 3D Point Cloud Semantic Segmentation (Zhaochong An et al., 2024)</a></li><li><a href=#3046--118243-rethinking-cluster-conditioned-diffusion-models-nikolas-adaloglou-et-al-2024>(30/46 | 118/243) Rethinking cluster-conditioned diffusion models (Nikolas Adaloglou et al., 2024)</a></li><li><a href=#3146--119243-multi-task-learning-using-uncertainty-to-weigh-losses-for-heterogeneous-face-attribute-estimation-huaqing-yuan-et-al-2024>(31/46 | 119/243) Multi-Task Learning Using Uncertainty to Weigh Losses for Heterogeneous Face Attribute Estimation (Huaqing Yuan et al., 2024)</a></li><li><a href=#3246--120243-improving-acne-image-grading-with-label-distribution-smoothing-kirill-prokhorov-et-al-2024>(32/46 | 120/243) Improving Acne Image Grading with Label Distribution Smoothing (Kirill Prokhorov et al., 2024)</a></li><li><a href=#3346--121243-spatial-cascaded-clustering-and-weighted-memory-for-unsupervised-person-re-identification-jiahao-hong-et-al-2024>(33/46 | 121/243) Spatial Cascaded Clustering and Weighted Memory for Unsupervised Person Re-identification (Jiahao Hong et al., 2024)</a></li><li><a href=#3446--122243-point-could-mamba-point-cloud-learning-via-state-space-model-tao-zhang-et-al-2024>(34/46 | 122/243) Point Could Mamba: Point Cloud Learning via State Space Model (Tao Zhang et al., 2024)</a></li><li><a href=#3546--123243-tri-modal-motion-retrieval-by-learning-a-joint-embedding-space-kangning-yin-et-al-2024>(35/46 | 123/243) Tri-Modal Motion Retrieval by Learning a Joint Embedding Space (Kangning Yin et al., 2024)</a></li><li><a href=#3646--124243-diff-plugin-revitalizing-details-for-diffusion-based-low-level-tasks-yuhao-liu-et-al-2024>(36/46 | 124/243) Diff-Plugin: Revitalizing Details for Diffusion-based Low-level Tasks (Yuhao Liu et al., 2024)</a></li><li><a href=#3746--125243-realcustom-narrowing-real-text-word-for-real-time-open-domain-text-to-image-customization-mengqi-huang-et-al-2024>(37/46 | 125/243) RealCustom: Narrowing Real Text Word for Real-Time Open-Domain Text-to-Image Customization (Mengqi Huang et al., 2024)</a></li><li><a href=#3846--126243-when-controlnet-meets-inexplicit-masks-a-case-study-of-controlnet-on-its-contour-following-ability-wenjie-xuan-et-al-2024>(38/46 | 126/243) When ControlNet Meets Inexplicit Masks: A Case Study of ControlNet on its Contour-following Ability (Wenjie Xuan et al., 2024)</a></li><li><a href=#3946--127243-an-ordinal-diffusion-model-for-generating-medical-images-with-different-severity-levels-shumpei-takezaki-et-al-2024>(39/46 | 127/243) An Ordinal Diffusion Model for Generating Medical Images with Different Severity Levels (Shumpei Takezaki et al., 2024)</a></li><li><a href=#4046--128243-small-versatile-and-mighty-a-range-view-perception-framework-qiang-meng-et-al-2024>(40/46 | 128/243) Small, Versatile and Mighty: A Range-View Perception Framework (Qiang Meng et al., 2024)</a></li><li><a href=#4146--129243-trustworthy-self-attention-enabling-the-network-to-focus-only-on-the-most-relevant-references-yu-jing-et-al-2024>(41/46 | 129/243) Trustworthy Self-Attention: Enabling the Network to Focus Only on the Most Relevant References (Yu Jing et al., 2024)</a></li><li><a href=#4246--130243-chartreformer-natural-language-driven-chart-image-editing-pengyu-yan-et-al-2024>(42/46 | 130/243) ChartReformer: Natural Language-Driven Chart Image Editing (Pengyu Yan et al., 2024)</a></li><li><a href=#4346--131243-g3dr-generative-3d-reconstruction-in-imagenet-pradyumna-reddy-et-al-2024>(43/46 | 131/243) G3DR: Generative 3D Reconstruction in ImageNet (Pradyumna Reddy et al., 2024)</a></li><li><a href=#4446--132243-deep-learning-computed-tomography-based-on-the-defrise-and-clack-algorithm-chengze-ye-et-al-2024>(44/46 | 132/243) Deep Learning Computed Tomography based on the Defrise and Clack Algorithm (Chengze Ye et al., 2024)</a></li><li><a href=#4546--133243-rethinking-classifier-re-training-in-long-tailed-recognition-a-simple-logits-retargeting-approach-han-lu-et-al-2024>(45/46 | 133/243) Rethinking Classifier Re-Training in Long-Tailed Recognition: A Simple Logits Retargeting Approach (Han Lu et al., 2024)</a></li><li><a href=#4646--134243-sure-survey-recipes-for-building-reliable-and-robust-deep-networks-yuting-li-et-al-2024>(46/46 | 134/243) SURE: SUrvey REcipes for building reliable and robust deep networks (Yuting Li et al., 2024)</a></li></ul></li><li><a href=#csse-4>cs.SE (4)</a><ul><li><a href=#14--135243-a-systematic-evaluation-of-large-language-models-for-generating-programming-code-wenpin-hou-et-al-2024>(1/4 | 135/243) A systematic evaluation of large language models for generating programming code (Wenpin Hou et al., 2024)</a></li><li><a href=#24--136243-when-large-language-models-confront-repository-level-automatic-program-repair-how-well-they-done-yuxiao-chen-et-al-2024>(2/4 | 136/243) When Large Language Models Confront Repository-Level Automatic Program Repair: How Well They Done? (Yuxiao Chen et al., 2024)</a></li><li><a href=#34--137243-a-survey-on-self-healing-software-system-zahra-yazdanparast-2024>(3/4 | 137/243) A Survey on Self-healing Software System (Zahra Yazdanparast, 2024)</a></li><li><a href=#44--138243-dypybench-a-benchmark-of-executable-python-software-islem-bouzenia-et-al-2024>(4/4 | 138/243) DyPyBench: A Benchmark of Executable Python Software (Islem Bouzenia et al., 2024)</a></li></ul></li><li><a href=#eesssy-7>eess.SY (7)</a><ul><li><a href=#17--139243-powerflowmultinet-multigraph-neural-networks-for-unbalanced-three-phase-distribution-systems-salah-ghamizi-et-al-2024>(1/7 | 139/243) PowerFlowMultiNet: Multigraph Neural Networks for Unbalanced Three-Phase Distribution Systems (Salah Ghamizi et al., 2024)</a></li><li><a href=#27--140243-a-holistic-power-optimization-approach-for-microgrid-control-based-on-deep-reinforcement-learning-fulong-yao-et-al-2024>(2/7 | 140/243) A Holistic Power Optimization Approach for Microgrid Control Based on Deep Reinforcement Learning (Fulong Yao et al., 2024)</a></li><li><a href=#37--141243-data-based-control-of-continuous-time-linear-systems-with-performance-specifications-victor-g-lopez-et-al-2024>(3/7 | 141/243) Data-Based Control of Continuous-Time Linear Systems with Performance Specifications (Victor G. Lopez et al., 2024)</a></li><li><a href=#47--142243-policy-optimization-for-pde-control-with-a-warm-start-xiangyuan-zhang-et-al-2024>(4/7 | 142/243) Policy Optimization for PDE Control with a Warm Start (Xiangyuan Zhang et al., 2024)</a></li><li><a href=#57--143243-distributed-mpc-for-autonomous-ships-on-inland-waterways-with-collaborative-collision-avoidance-hoang-anh-tran-et-al-2024>(5/7 | 143/243) Distributed MPC for autonomous ships on inland waterways with collaborative collision avoidance (Hoang Anh Tran et al., 2024)</a></li><li><a href=#67--144243-optimization-of-the-energy-comfort-trade-off-of-hvac-systems-in-electric-city-buses-based-on-a-steady-state-model-fabio-widmer-et-al-2024>(6/7 | 144/243) Optimization of the Energy-Comfort Trade-Off of HVAC Systems in Electric City Buses Based on a Steady-State Model (Fabio Widmer et al., 2024)</a></li><li><a href=#77--145243-sindy-vs-hard-nonlinearities-and-hidden-dynamics-a-benchmarking-study-aurelio-raffa-ugolini-et-al-2024>(7/7 | 145/243) SINDy vs Hard Nonlinearities and Hidden Dynamics: a Benchmarking Study (Aurelio Raffa Ugolini et al., 2024)</a></li></ul></li><li><a href=#eessiv-3>eess.IV (3)</a><ul><li><a href=#13--146243-visrec-a-semi-supervised-approach-to-radio-interferometric-data-reconstruction-ruoqi-wang-et-al-2024>(1/3 | 146/243) VisRec: A Semi-Supervised Approach to Radio Interferometric Data Reconstruction (Ruoqi Wang et al., 2024)</a></li><li><a href=#23--147243-graph-theory-and-gnns-to-unravel-the-topographical-organization-of-brain-lesions-in-variants-of-alzheimers-disease-progression-leopold-hebert-stevens-et-al-2024>(2/3 | 147/243) Graph Theory and GNNs to Unravel the Topographical Organization of Brain Lesions in Variants of Alzheimer&rsquo;s Disease Progression (Leopold Hebert-Stevens et al., 2024)</a></li><li><a href=#33--148243-relaxometry-guided-quantitative-cardiac-magnetic-resonance-image-reconstruction-yidong-zhao-et-al-2024>(3/3 | 148/243) Relaxometry Guided Quantitative Cardiac Magnetic Resonance Image Reconstruction (Yidong Zhao et al., 2024)</a></li></ul></li><li><a href=#csro-16>cs.RO (16)</a><ul><li><a href=#116--149243-predicting-uav-type-an-exploration-of-sampling-and-data-augmentation-for-time-series-classification-tarik-crnovrsanin-et-al-2024>(1/16 | 149/243) Predicting UAV Type: An Exploration of Sampling and Data Augmentation for Time Series Classification (Tarik Crnovrsanin et al., 2024)</a></li><li><a href=#216--150243-never-ending-embodied-robot-learning-wenqi-liang-et-al-2024>(2/16 | 150/243) Never-Ending Embodied Robot Learning (Wenqi Liang et al., 2024)</a></li><li><a href=#316--151243-selfi-autonomous-self-improvement-with-reinforcement-learning-for-social-navigation-noriaki-hirose-et-al-2024>(3/16 | 151/243) SELFI: Autonomous Self-Improvement with Reinforcement Learning for Social Navigation (Noriaki Hirose et al., 2024)</a></li><li><a href=#416--152243-safe-hybrid-action-reinforcement-learning-based-decision-and-control-for-discretionary-lane-change-ruichen-xu-et-al-2024>(4/16 | 152/243) Safe Hybrid-Action Reinforcement Learning-Based Decision and Control for Discretionary Lane Change (Ruichen Xu et al., 2024)</a></li><li><a href=#516--153243-structured-deep-neural-networks-based-backstepping-trajectory-tracking-control-for-lagrangian-systems-jiajun-qian-et-al-2024>(5/16 | 153/243) Structured Deep Neural Networks-Based Backstepping Trajectory Tracking Control for Lagrangian Systems (Jiajun Qian et al., 2024)</a></li><li><a href=#616--154243-optimal-robot-formations-balancing-range-based-observability-and-user-defined-configurations-syed-shabbir-ahmed-et-al-2024>(6/16 | 154/243) Optimal Robot Formations: Balancing Range-Based Observability and User-Defined Configurations (Syed Shabbir Ahmed et al., 2024)</a></li><li><a href=#716--155243-joint-spatial-temporal-calibration-for-camera-and-global-pose-sensor-junlin-song-et-al-2024>(7/16 | 155/243) Joint Spatial-Temporal Calibration for Camera and Global Pose Sensor (Junlin Song et al., 2024)</a></li><li><a href=#816--156243-nussbaum-function-based-approach-for-tracking-control-of-robot-manipulators-hamed-rahimi-nohooji-et-al-2024>(8/16 | 156/243) Nussbaum Function Based Approach for Tracking Control of Robot Manipulators (Hamed Rahimi Nohooji et al., 2024)</a></li><li><a href=#916--157243-suturing-tasks-automation-based-on-skills-learned-from-demonstrations-a-simulation-study-haoying-zhou-et-al-2024>(9/16 | 157/243) Suturing Tasks Automation Based on Skills Learned From Demonstrations: A Simulation Study (Haoying Zhou et al., 2024)</a></li><li><a href=#1016--158243-optimizing-dynamic-balance-in-a-rat-robot-via-the-lateral-flexion-of-a-soft-actuated-spine-yuhong-huang-et-al-2024>(10/16 | 158/243) Optimizing Dynamic Balance in a Rat Robot via the Lateral Flexion of a Soft Actuated Spine (Yuhong Huang et al., 2024)</a></li><li><a href=#1116--159243-prime-scaffolding-manipulation-tasks-with-behavior-primitives-for-data-efficient-imitation-learning-tian-gao-et-al-2024>(11/16 | 159/243) PRIME: Scaffolding Manipulation Tasks with Behavior Primitives for Data-Efficient Imitation Learning (Tian Gao et al., 2024)</a></li><li><a href=#1216--160243-robust-online-epistemic-replanning-of-multi-robot-missions-lauren-bramblett-et-al-2024>(12/16 | 160/243) Robust Online Epistemic Replanning of Multi-Robot Missions (Lauren Bramblett et al., 2024)</a></li><li><a href=#1316--161243-learning-quadrupedal-locomotion-with-impaired-joints-using-random-joint-masking-mincheol-kim-et-al-2024>(13/16 | 161/243) Learning Quadrupedal Locomotion with Impaired Joints Using Random Joint Masking (Mincheol Kim et al., 2024)</a></li><li><a href=#1416--162243-robustifying-a-policy-in-multi-agent-rl-with-diverse-cooperative-behavior-and-adversarial-style-sampling-for-assistive-tasks-tayuki-osa-et-al-2024>(14/16 | 162/243) Robustifying a Policy in Multi-Agent RL with Diverse Cooperative Behavior and Adversarial Style Sampling for Assistive Tasks (Tayuki Osa et al., 2024)</a></li><li><a href=#1516--163243-complete-and-near-optimal-robotic-crack-coverage-and-filling-in-civil-infrastructure-vishnu-veeraraghavan-et-al-2024>(15/16 | 163/243) Complete and Near-Optimal Robotic Crack Coverage and Filling in Civil Infrastructure (Vishnu Veeraraghavan et al., 2024)</a></li><li><a href=#1616--164243-model-based-planning-and-control-for-terrestrial-aerial-bimodal-vehicles-with-passive-wheels-ruibin-zhang-et-al-2024>(16/16 | 164/243) Model-Based Planning and Control for Terrestrial-Aerial Bimodal Vehicles with Passive Wheels (Ruibin Zhang et al., 2024)</a></li></ul></li><li><a href=#cshc-8>cs.HC (8)</a><ul><li><a href=#18--165243-leveraging-prompt-based-large-language-models-predicting-pandemic-health-decisions-and-outcomes-through-social-media-language-xiaohan-ding-et-al-2024>(1/8 | 165/243) Leveraging Prompt-Based Large Language Models: Predicting Pandemic Health Decisions and Outcomes Through Social Media Language (Xiaohan Ding et al., 2024)</a></li><li><a href=#28--166243-to-trust-or-distrust-trust-measures-validating-questionnaires-for-trust-in-ai-nicolas-scharowski-et-al-2024>(2/8 | 166/243) To Trust or Distrust Trust Measures: Validating Questionnaires for Trust in AI (Nicolas Scharowski et al., 2024)</a></li><li><a href=#38--167243-multiple-ways-of-working-with-users-to-develop-physically-assistive-robots-amal-nanavati-et-al-2024>(3/8 | 167/243) Multiple Ways of Working with Users to Develop Physically Assistive Robots (Amal Nanavati et al., 2024)</a></li><li><a href=#48--168243-metamorpheus-interactive-affective-and-creative-dream-narration-through-metaphorical-visual-storytelling-qian-wan-et-al-2024>(4/8 | 168/243) Metamorpheus: Interactive, Affective, and Creative Dream Narration Through Metaphorical Visual Storytelling (Qian Wan et al., 2024)</a></li><li><a href=#58--169243-authors-values-and-attitudes-towards-ai-bridged-scalable-personalization-of-creative-language-arts-taewook-kim-et-al-2024>(5/8 | 169/243) Authors&rsquo; Values and Attitudes Towards AI-bridged Scalable Personalization of Creative Language Arts (Taewook Kim et al., 2024)</a></li><li><a href=#68--170243-can-a-funny-chatbot-make-a-difference-infusing-humor-into-conversational-agent-for-behavioral-intervention-xin-sun-et-al-2024>(6/8 | 170/243) Can a Funny Chatbot Make a Difference? Infusing Humor into Conversational Agent for Behavioral Intervention (Xin Sun et al., 2024)</a></li><li><a href=#78--171243-nova-a-visual-interface-for-assessing-polarizing-media-coverage-keshav-dasu-et-al-2024>(7/8 | 171/243) NOVA: A visual interface for assessing polarizing media coverage (Keshav Dasu et al., 2024)</a></li><li><a href=#88--172243-maidr-making-statistical-visualizations-accessible-with-multimodal-data-representation-jooyoung-seo-et-al-2024>(8/8 | 172/243) MAIDR: Making Statistical Visualizations Accessible with Multimodal Data Representation (JooYoung Seo et al., 2024)</a></li></ul></li><li><a href=#csai-7>cs.AI (7)</a><ul><li><a href=#17--173243-playing-nethack-with-llms-potential--limitations-as-zero-shot-agents-dominik-jeurissen-et-al-2024>(1/7 | 173/243) Playing NetHack with LLMs: Potential & Limitations as Zero-Shot Agents (Dominik Jeurissen et al., 2024)</a></li><li><a href=#27--174243-deep-reinforcement-learning-for-solving-management-problems-towards-a-large-management-mode-jinyang-jiang-et-al-2024>(2/7 | 174/243) Deep Reinforcement Learning for Solving Management Problems: Towards A Large Management Mode (Jinyang Jiang et al., 2024)</a></li><li><a href=#37--175243-even-ifs-from-if-onlys-are-the-best-semi-factual-explanations-found-using-counterfactuals-as-guides-saugat-aryal-et-al-2024>(3/7 | 175/243) Even-Ifs From If-Onlys: Are the Best Semi-Factual Explanations Found Using Counterfactuals As Guides? (Saugat Aryal et al., 2024)</a></li><li><a href=#47--176243-softened-symbol-grounding-for-neuro-symbolic-systems-zenan-li-et-al-2024>(4/7 | 176/243) Softened Symbol Grounding for Neuro-symbolic Systems (Zenan Li et al., 2024)</a></li><li><a href=#57--177243-a-survey-of-route-recommendations-methods-applications-and-opportunities-shiming-zhang-et-al-2024>(5/7 | 177/243) A Survey of Route Recommendations: Methods, Applications, and Opportunities (Shiming Zhang et al., 2024)</a></li><li><a href=#67--178243-know-your-exceptions-towards-an-ontology-of-exceptions-in-knowledge-representation-gabriele-sacco-et-al-2024>(6/7 | 178/243) Know your exceptions: Towards an Ontology of Exceptions in Knowledge Representation (Gabriele Sacco et al., 2024)</a></li><li><a href=#77--179243-axe-the-x-in-xai-a-plea-for-understandable-ai-andrés-páez-2024>(7/7 | 179/243) Axe the X in XAI: A Plea for Understandable AI (Andrés Páez, 2024)</a></li></ul></li><li><a href=#eessas-2>eess.AS (2)</a><ul><li><a href=#12--180243-efficient-adapter-tuning-of-pre-trained-speech-models-for-automatic-speaker-verification-mufan-sang-et-al-2024>(1/2 | 180/243) Efficient Adapter Tuning of Pre-trained Speech Models for Automatic Speaker Verification (Mufan Sang et al., 2024)</a></li><li><a href=#22--181243-the-impact-of-frequency-bands-on-acoustic-anomaly-detection-of-machines-using-deep-learning-based-model-tin-nguyen-et-al-2024>(2/2 | 181/243) The Impact of Frequency Bands on Acoustic Anomaly Detection of Machines using Deep Learning Based Model (Tin Nguyen et al., 2024)</a></li></ul></li><li><a href=#csir-7>cs.IR (7)</a><ul><li><a href=#17--182243-generalized-user-representations-for-transfer-learning-ghazal-fazelnia-et-al-2024>(1/7 | 182/243) Generalized User Representations for Transfer Learning (Ghazal Fazelnia et al., 2024)</a></li><li><a href=#27--183243-end-to-end-graph-sequential-representation-learning-for-accurate-recommendations-vladimir-baikalov-et-al-2024>(2/7 | 183/243) End-to-end Graph-Sequential Representation Learning for Accurate Recommendations (Vladimir Baikalov et al., 2024)</a></li><li><a href=#37--184243-an-interpretable-ensemble-of-graph-and-language-models-for-improving-search-relevance-in-e-commerce-nurendra-choudhary-et-al-2024>(3/7 | 184/243) An Interpretable Ensemble of Graph and Language Models for Improving Search Relevance in E-Commerce (Nurendra Choudhary et al., 2024)</a></li><li><a href=#47--185243-iai-moviebot-20-an-enhanced-research-platform-with-trainable-neural-components-and-transparent-user-modeling-nolwenn-bernard-et-al-2024>(4/7 | 185/243) IAI MovieBot 2.0: An Enhanced Research Platform with Trainable Neural Components and Transparent User Modeling (Nolwenn Bernard et al., 2024)</a></li><li><a href=#57--186243-recommending-target-actions-outside-sessions-in-the-data-poor-insurance-domain-simone-borg-bruun-et-al-2024>(5/7 | 186/243) Recommending Target Actions Outside Sessions in the Data-poor Insurance Domain (Simone Borg Bruun et al., 2024)</a></li><li><a href=#67--187243-open-assistant-toolkit----version-2-sophie-fischer-et-al-2024>(6/7 | 187/243) Open Assistant Toolkit &ndash; version 2 (Sophie Fischer et al., 2024)</a></li><li><a href=#77--188243-dual-granularity-medication-recommendation-based-on-causal-inference-shunpan-liang-et-al-2024>(7/7 | 188/243) Dual-Granularity Medication Recommendation Based on Causal Inference (Shunpan Liang et al., 2024)</a></li></ul></li><li><a href=#eesssp-2>eess.SP (2)</a><ul><li><a href=#12--189243-toward-autonomous-cooperation-in-heterogeneous-nanosatellite-constellations-using-dynamic-graph-neural-networks-guillem-casadesus-vila-et-al-2024>(1/2 | 189/243) Toward Autonomous Cooperation in Heterogeneous Nanosatellite Constellations Using Dynamic Graph Neural Networks (Guillem Casadesus-Vila et al., 2024)</a></li><li><a href=#22--190243-diffraction-and-scattering-aware-radio-map-and-environment-reconstruction-using-geometry-model-assisted-deep-learning-wangqian-chen-et-al-2024>(2/2 | 190/243) Diffraction and Scattering Aware Radio Map and Environment Reconstruction using Geometry Model-Assisted Deep Learning (Wangqian Chen et al., 2024)</a></li></ul></li><li><a href=#csni-2>cs.NI (2)</a><ul><li><a href=#12--191243-exploring-upper-6ghz-and-mmwave-in-real-world-5g-networks-a-direct-on-field-comparison-marcello-morini-et-al-2024>(1/2 | 191/243) Exploring Upper-6GHz and mmWave in Real-World 5G Networks: A Direct on-Field Comparison (Marcello Morini et al., 2024)</a></li><li><a href=#22--192243-comparative-study-of-simulators-for-vehicular-networks-rida-saghir-et-al-2024>(2/2 | 192/243) Comparative Study of Simulators for Vehicular Networks (Rida Saghir et al., 2024)</a></li></ul></li><li><a href=#csit-9>cs.IT (9)</a><ul><li><a href=#19--193243-complex-valued-neural-network-based-federated-learning-for-multi-user-indoor-positioning-performance-optimization-hanzhi-yu-et-al-2024>(1/9 | 193/243) Complex-Valued Neural Network based Federated Learning for Multi-user Indoor Positioning Performance Optimization (Hanzhi Yu et al., 2024)</a></li><li><a href=#29--194243-federated-learning-via-lattice-joint-source-channel-coding-seyed-mohammad-azimi-abarghouyi-et-al-2024>(2/9 | 194/243) Federated Learning via Lattice Joint Source-Channel Coding (Seyed Mohammad Azimi-Abarghouyi et al., 2024)</a></li><li><a href=#39--195243-on-non-interactive-simulation-of-distributed-sources-with-finite-alphabets-hojat-allah-salehi-et-al-2024>(3/9 | 195/243) On Non-Interactive Simulation of Distributed Sources with Finite Alphabets (Hojat Allah Salehi et al., 2024)</a></li><li><a href=#49--196243-shortened-polar-codes-under-automorphism-ensemble-decoding-charles-pillet-et-al-2024>(4/9 | 196/243) Shortened Polar Codes under Automorphism Ensemble Decoding (Charles Pillet et al., 2024)</a></li><li><a href=#59--197243-impact-of-inter-operator-interference-via-reconfigurable-intelligent-surfaces-nikolaos-i-miridakis-et-al-2024>(5/9 | 197/243) Impact of Inter-Operator Interference via Reconfigurable Intelligent Surfaces (Nikolaos I. Miridakis et al., 2024)</a></li><li><a href=#69--198243-deep-iot-downlink-enhanced-efficient-power-internet-of-things-yulin-shao-2024>(6/9 | 198/243) DEEP-IoT: Downlink-Enhanced Efficient-Power Internet of Things (Yulin Shao, 2024)</a></li><li><a href=#79--199243-probabilistic-semantic-communication-over-wireless-networks-with-rate-splitting-zhouxiang-zhao-et-al-2024>(7/9 | 199/243) Probabilistic Semantic Communication over Wireless Networks with Rate Splitting (Zhouxiang Zhao et al., 2024)</a></li><li><a href=#89--200243-decentralized-uncoded-storage-elastic-computing-with-heterogeneous-computation-speeds-wenbo-huang-et-al-2024>(8/9 | 200/243) Decentralized Uncoded Storage Elastic Computing with Heterogeneous Computation Speeds (Wenbo Huang et al., 2024)</a></li><li><a href=#99--201243-nearest-neighbours-estimators-for-conditional-mutual-information-jake-witter-et-al-2024>(9/9 | 201/243) Nearest-Neighbours Estimators for Conditional Mutual Information (Jake Witter et al., 2024)</a></li></ul></li><li><a href=#csar-1>cs.AR (1)</a><ul><li><a href=#11--202243-neupims-npu-pim-heterogeneous-acceleration-for-batched-llm-inferencing-guseul-heo-et-al-2024>(1/1 | 202/243) NeuPIMs: NPU-PIM Heterogeneous Acceleration for Batched LLM Inferencing (Guseul Heo et al., 2024)</a></li></ul></li><li><a href=#csds-4>cs.DS (4)</a><ul><li><a href=#14--203243-analysis-of-phylogeny-tracking-algorithms-for-serial-and-multiprocess-applications-matthew-andres-moreno-et-al-2024>(1/4 | 203/243) Analysis of Phylogeny Tracking Algorithms for Serial and Multiprocess Applications (Matthew Andres Moreno et al., 2024)</a></li><li><a href=#24--204243-algorithms-for-efficient-compact-online-data-stream-curation-matthew-andres-moreno-et-al-2024>(2/4 | 204/243) Algorithms for Efficient, Compact Online Data Stream Curation (Matthew Andres Moreno et al., 2024)</a></li><li><a href=#34--205243-undercomplete-decomposition-of-symmetric-tensors-in-linear-time-and-smoothed-analysis-of-the-condition-number-pascal-koiran-et-al-2024>(3/4 | 205/243) Undercomplete Decomposition of Symmetric Tensors in Linear Time, and Smoothed Analysis of the Condition Number (Pascal Koiran et al., 2024)</a></li><li><a href=#44--206243-polyamorous-scheduling-leszek-gąsieniec-et-al-2024>(4/4 | 206/243) Polyamorous Scheduling (Leszek Gąsieniec et al., 2024)</a></li></ul></li><li><a href=#csne-1>cs.NE (1)</a><ul><li><a href=#11--207243-fast-and-efficient-local-search-for-genetic-programming-based-loss-function-learning-christian-raymond-et-al-2024>(1/1 | 207/243) Fast and Efficient Local Search for Genetic Programming Based Loss Function Learning (Christian Raymond et al., 2024)</a></li></ul></li><li><a href=#cssi-2>cs.SI (2)</a><ul><li><a href=#12--208243-cost-effective-activity-control-of-asymptomatic-carriers-in-layered-temporal-social-networks-masoumeh-moradian-et-al-2024>(1/2 | 208/243) Cost-Effective Activity Control of Asymptomatic Carriers in Layered Temporal Social Networks (Masoumeh Moradian et al., 2024)</a></li><li><a href=#22--209243-identify-critical-nodes-in-complex-network-with-large-language-models-jinzhu-mao-et-al-2024>(2/2 | 209/243) Identify Critical Nodes in Complex Network with Large Language Models (Jinzhu Mao et al., 2024)</a></li></ul></li><li><a href=#csma-1>cs.MA (1)</a><ul><li><a href=#11--210243-composite-distributed-learning-and-synchronization-of-nonlinear-multi-agent-systems-with-complete-uncertain-dynamics-emadodin-jandaghi-et-al-2024>(1/1 | 210/243) Composite Distributed Learning and Synchronization of Nonlinear Multi-Agent Systems with Complete Uncertain Dynamics (Emadodin Jandaghi et al., 2024)</a></li></ul></li><li><a href=#cssd-2>cs.SD (2)</a><ul><li><a href=#12--211243-scaling-up-adaptive-filter-optimizers-jonah-casebeer-et-al-2024>(1/2 | 211/243) Scaling Up Adaptive Filter Optimizers (Jonah Casebeer et al., 2024)</a></li><li><a href=#22--212243-voxgenesis-unsupervised-discovery-of-latent-speaker-manifold-for-speech-synthesis-weiwei-lin-et-al-2024>(2/2 | 212/243) VoxGenesis: Unsupervised Discovery of Latent Speaker Manifold for Speech Synthesis (Weiwei Lin et al., 2024)</a></li></ul></li><li><a href=#mathna-4>math.NA (4)</a><ul><li><a href=#14--213243-computational-homogenization-for-aerogel-like-polydisperse-open-porous-materials-using-neural-network--based-surrogate-models-on-the-microscale-axel-klawonn-et-al-2024>(1/4 | 213/243) Computational homogenization for aerogel-like polydisperse open-porous materials using neural network&ndash;based surrogate models on the microscale (Axel Klawonn et al., 2024)</a></li><li><a href=#24--214243-implicit-high-order-gas-kinetic-schemes-for-compressible-flows-on-three-dimensional-unstructured-meshes-ii-unsteady-flows-yaqing-yang-et-al-2024>(2/4 | 214/243) Implicit high-order gas-kinetic schemes for compressible flows on three-dimensional unstructured meshes II: unsteady flows (Yaqing Yang et al., 2024)</a></li><li><a href=#34--215243-enhancing-biomechanical-simulations-based-on-a-posteriori-error-estimates-the-potential-of-dual-weighted-residual-driven-adaptive-mesh-refinement-huu-phuoc-bui-et-al-2024>(3/4 | 215/243) Enhancing Biomechanical Simulations Based on A Posteriori Error Estimates: The Potential of Dual Weighted Residual-Driven Adaptive Mesh Refinement (Huu Phuoc Bui et al., 2024)</a></li><li><a href=#44--216243-a-gradually-reinforced-sample-average-approximation-differentiable-homotopy-method-for-a-system-of-stochastic-equations-peixuan-li-et-al-2024>(4/4 | 216/243) A Gradually Reinforced Sample-Average-Approximation Differentiable Homotopy Method for a System of Stochastic Equations (Peixuan Li et al., 2024)</a></li></ul></li><li><a href=#statml-3>stat.ML (3)</a><ul><li><a href=#13--217243-validation-of-ml-uq-calibration-statistics-using-simulated-reference-values-a-sensitivity-analysis-pascal-pernot-2024>(1/3 | 217/243) Validation of ML-UQ calibration statistics using simulated reference values: a sensitivity analysis (Pascal Pernot, 2024)</a></li><li><a href=#23--218243-causal-bandits-with-general-causal-models-and-interventions-zirui-yan-et-al-2024>(2/3 | 218/243) Causal Bandits with General Causal Models and Interventions (Zirui Yan et al., 2024)</a></li><li><a href=#33--219243-lossless-compression-of-deep-neural-networks-a-high-dimensional-neural-tangent-kernel-approach-lingyu-gu-et-al-2024>(3/3 | 219/243) &lsquo;Lossless&rsquo; Compression of Deep Neural Networks: A High-dimensional Neural Tangent Kernel Approach (Lingyu Gu et al., 2024)</a></li></ul></li><li><a href=#physicsmed-ph-1>physics.med-ph (1)</a><ul><li><a href=#11--220243-list-mode-pet-image-reconstruction-using-dykstra-like-splitting-kibo-ote-et-al-2024>(1/1 | 220/243) List-Mode PET Image Reconstruction Using Dykstra-Like Splitting (Kibo Ote et al., 2024)</a></li></ul></li><li><a href=#statme-1>stat.ME (1)</a><ul><li><a href=#11--221243-stable-reduced-rank-var-identification-xinhui-rong-et-al-2024>(1/1 | 221/243) Stable Reduced-Rank VAR Identification (Xinhui Rong et al., 2024)</a></li></ul></li><li><a href=#cscy-1>cs.CY (1)</a><ul><li><a href=#11--222243-exploring-the-dynamic-interplay-of-cognitive-load-and-emotional-arousal-by-using-multimodal-measurements-correlation-of-pupil-diameter-and-emotional-arousal-in-emotionally-engaging-tasks-c-kosel-et-al-2024>(1/1 | 222/243) Exploring the dynamic interplay of cognitive load and emotional arousal by using multimodal measurements: Correlation of pupil diameter and emotional arousal in emotionally engaging tasks (C. Kosel et al., 2024)</a></li></ul></li><li><a href=#csdc-3>cs.DC (3)</a><ul><li><a href=#13--223243-a-spark-optimizer-for-adaptive-fine-grained-parameter-tuning-chenghao-lyu-et-al-2024>(1/3 | 223/243) A Spark Optimizer for Adaptive, Fine-Grained Parameter Tuning (Chenghao Lyu et al., 2024)</a></li><li><a href=#23--224243-neural-acceleration-of-incomplete-cholesky-preconditioners-joshua-dennis-booth-et-al-2024>(2/3 | 224/243) Neural Acceleration of Incomplete Cholesky Preconditioners (Joshua Dennis Booth et al., 2024)</a></li><li><a href=#33--225243-windgp-efficient-graph-partitioning-on-heterogenous-machines-li-zeng-et-al-2024>(3/3 | 225/243) WindGP: Efficient Graph Partitioning on Heterogenous Machines (Li Zeng et al., 2024)</a></li></ul></li><li><a href=#csgt-4>cs.GT (4)</a><ul><li><a href=#14--226243-the-price-of-fairness-in-bipartite-matching-rémi-castera-et-al-2024>(1/4 | 226/243) The Price of Fairness in Bipartite Matching (Rémi Castera et al., 2024)</a></li><li><a href=#24--227243-understanding-police-force-resource-allocation-using-adversarial-optimal-transport-with-incomplete-information-yinan-hu-et-al-2024>(2/4 | 227/243) Understanding Police Force Resource Allocation using Adversarial Optimal Transport with Incomplete Information (Yinan Hu et al., 2024)</a></li><li><a href=#34--228243-on-the-hardness-of-fair-allocation-under-ternary-valuations-zack-fitzsimmons-et-al-2024>(3/4 | 228/243) On the Hardness of Fair Allocation under Ternary Valuations (Zack Fitzsimmons et al., 2024)</a></li><li><a href=#44--229243-as-soon-as-possible-but-rationally-véronique-bruyère-et-al-2024>(4/4 | 229/243) As Soon as Possible but Rationally (Véronique Bruyère et al., 2024)</a></li></ul></li><li><a href=#q-bioqm-1>q-bio.QM (1)</a><ul><li><a href=#11--230243-enhancing-protein-predictive-models-via-proteins-data-augmentation-a-benchmark-and-new-directions-rui-sun-et-al-2024>(1/1 | 230/243) Enhancing Protein Predictive Models via Proteins Data Augmentation: A Benchmark and New Directions (Rui Sun et al., 2024)</a></li></ul></li><li><a href=#statap-1>stat.AP (1)</a><ul><li><a href=#11--231243-binary-gaussian-copula-synthesis-a-novel-data-augmentation-technique-to-advance-ml-based-clinical-decision-support-systems-for-early-prediction-of-dialysis-among-ckd-patients-hamed-khosravi-et-al-2024>(1/1 | 231/243) Binary Gaussian Copula Synthesis: A Novel Data Augmentation Technique to Advance ML-based Clinical Decision Support Systems for Early Prediction of Dialysis Among CKD Patients (Hamed Khosravi et al., 2024)</a></li></ul></li><li><a href=#cslo-2>cs.LO (2)</a><ul><li><a href=#12--232243-semi-automated-modular-formal-verification-of-critical-software-liveness-and-completeness-thresholds-tobias-reinhard-2024>(1/2 | 232/243) Semi-Automated Modular Formal Verification of Critical Software: Liveness and Completeness Thresholds (Tobias Reinhard, 2024)</a></li><li><a href=#22--233243-analyzing-divergence-for-nondeterministic-probabilistic-models-hao-wu-et-al-2024>(2/2 | 233/243) Analyzing Divergence for Nondeterministic Probabilistic Models (Hao Wu et al., 2024)</a></li></ul></li><li><a href=#mathoc-1>math.OC (1)</a><ul><li><a href=#11--234243-primal-dual-ilqr-joão-sousa-pinto-et-al-2024>(1/1 | 234/243) Primal-Dual iLQR (João Sousa-Pinto et al., 2024)</a></li></ul></li><li><a href=#q-fincp-1>q-fin.CP (1)</a><ul><li><a href=#11--235243-a-time-stepping-deep-gradient-flow-method-for-option-pricing-in-rough-diffusion-models-antonis-papapantoleon-et-al-2024>(1/1 | 235/243) A time-stepping deep gradient flow method for option pricing in (rough) diffusion models (Antonis Papapantoleon et al., 2024)</a></li></ul></li><li><a href=#astro-phim-1>astro-ph.IM (1)</a><ul><li><a href=#11--236243-autonomous-robotic-arm-manipulation-for-planetary-missions-using-causal-machine-learning-c-mcdonnell-et-al-2024>(1/1 | 236/243) Autonomous Robotic Arm Manipulation for Planetary Missions using Causal Machine Learning (C. McDonnell et al., 2024)</a></li></ul></li><li><a href=#cond-matmtrl-sci-1>cond-mat.mtrl-sci (1)</a><ul><li><a href=#11--237243-deciphering-diffuse-scattering-with-machine-learning-and-the-equivariant-foundation-model-the-case-of-molten-feo-ganesh-sivaraman-et-al-2024>(1/1 | 237/243) Deciphering diffuse scattering with machine learning and the equivariant foundation model: The case of molten FeO (Ganesh Sivaraman et al., 2024)</a></li></ul></li><li><a href=#cscg-1>cs.CG (1)</a><ul><li><a href=#11--238243-happy-ending-an-empty-hexagon-in-every-set-of-30-points-marijn-j-h-heule-et-al-2024>(1/1 | 238/243) Happy Ending: An Empty Hexagon in Every Set of 30 Points (Marijn J. H. Heule et al., 2024)</a></li></ul></li><li><a href=#mathmg-1>math.MG (1)</a><ul><li><a href=#11--239243-path-tracking-using-echoes-in-an-unknown-environment-the-issue-of-symmetries-and-how-to-break-them-mireille-boutin-et-al-2024>(1/1 | 239/243) Path Tracking using Echoes in an Unknown Environment: the Issue of Symmetries and How to Break Them (Mireille Boutin et al., 2024)</a></li></ul></li><li><a href=#mathco-1>math.CO (1)</a><ul><li><a href=#11--240243-edge-open-packing-complexity-algorithmic-aspects-and-bounds-boštjan-brešar-et-al-2024>(1/1 | 240/243) Edge open packing: complexity, algorithmic aspects, and bounds (Boštjan Brešar et al., 2024)</a></li></ul></li><li><a href=#q-fingn-1>q-fin.GN (1)</a><ul><li><a href=#11--241243-assessing-the-efficacy-of-heuristic-based-address-clustering-for-bitcoin-hugo-schnoering-et-al-2024>(1/1 | 241/243) Assessing the Efficacy of Heuristic-Based Address Clustering for Bitcoin (Hugo Schnoering et al., 2024)</a></li></ul></li><li><a href=#cscc-1>cs.CC (1)</a><ul><li><a href=#11--242243-graph-homomorphism-monotone-classes-and-bounded-pathwidth-tala-eagling-vose-et-al-2024>(1/1 | 242/243) Graph Homomorphism, Monotone Classes and Bounded Pathwidth (Tala Eagling-Vose et al., 2024)</a></li></ul></li><li><a href=#csgr-1>cs.GR (1)</a><ul><li><a href=#11--243243-hybrid-base-complex-extract-and-visualize-structure-of-hex-dominant-meshes-lei-si-et-al-2024>(1/1 | 243/243) Hybrid Base Complex: Extract and Visualize Structure of Hex-dominant Meshes (Lei Si et al., 2024)</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:contact.to.akitenkrad@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>contact.to.akitenkrad@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/akitenkrad-blog/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2020 Akitenkrad.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/akitenkrad-blog/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/jquery-3.4.1.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/popper.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/bootstrap.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/navbar.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/plyr.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=https://akitenkrad.github.io/akitenkrad-blog/js/single.js></script><script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.css><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.js></script><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!0})})</script></body></html>