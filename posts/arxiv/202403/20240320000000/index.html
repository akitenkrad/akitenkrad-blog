<!doctype html><html><head><title>arXiv @ 2024.03.20</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/bootstrap.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/main.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/navbar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/plyr.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/flag-icon.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/custom/style.css><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM crossorigin=anonymous><script async src=https://cdn.jsdelivr.net/npm/es-module-shims@1/dist/es-module-shims.min.js crossorigin=anonymous></script><script type=importmap>
{
  "imports": {
    "@popperjs/core": "https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.8/dist/esm/popper.min.js",
    "bootstrap": "https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.esm.min.js"
  }
}
</script><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js integrity=sha384-geWF76RCwLtnZ8qwWowPQNguL3RmwHVBC9FhGdlKrxdiJJigb/j/68SIy3Te4Bkz crossorigin=anonymous></script><script type=module src=https://akitenkrad.github.io/akitenkrad-blog/js/custom/custom.js></script><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/fontawesome/css/all.min.css><link rel=icon type=image/png href=https://akitenkrad.github.io/akitenkrad-blog/images/favicons/favicon-96x96_huf1ee13f0caf27d1547f91fb46207d708_13005_42x0_resize_box_3.png><meta property="og:title" content="arXiv @ 2024.03.20"><meta property="og:description" content="Primary Categories cond-mat.str-el (1) cs.AI (9) cs.AR (1) cs.CE (1) cs.CL (43) cs.CR (8) cs.CV (122) cs.CY (5) cs.DB (1) cs.DC (4) cs.DS (1) cs.GR (2) cs.GT (1) cs.HC (5) cs.IR (1) cs.IT (6) cs.LG (50) cs.MM (1) cs.NE (2) cs.NI (1) cs.OS (1) cs.PL (1) cs.RO (32) cs.SD (6) cs.SE (3) eess.IV (13) eess.SP (1) eess.SY (9) hep-ph (1) hep-th (1) math.AT (1) math.CO (2) math.NA (4) math.OC (2) physics."><meta property="og:type" content="article"><meta property="og:url" content="https://akitenkrad.github.io/akitenkrad-blog/posts/arxiv/202403/20240320000000/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-03-20T00:00:00+00:00"><meta property="article:modified_time" content="2024-03-20T00:00:00+00:00"><meta name=description content="arXiv @ 2024.03.20"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/single.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/sidebar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/style.css><script type=text/javascript src=//ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js></script><script language=JavaScript>$(document).ready(function(){$("a[href^='http']:not([href*='"+location.hostname+"'])").attr("target","_blank")})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-1MYYZQG0WE"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-1MYYZQG0WE",{anonymize_ip:!1})}</script></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow" style=position:fixed><div class=container><button class="navbar-toggler navbar-light navbar-for-sidebar" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span>
</button>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav mr-auto"><li class=nav-item><a class=navbar-brand href=/akitenkrad-blog><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png alt=Logo>
Akitenkrad's Blog</a></li></ul><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=/akitenkrad-blog#home>Home</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#about>About</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#recent-posts>Recent Posts</a></li><li class=nav-item><a class=nav-link id=blog-link href=https://akitenkrad.github.io/akitenkrad-blog/posts>Posts</a></li><li class=nav-item><a class=nav-link id=tag-link href=https://akitenkrad.github.io/akitenkrad-blog/tags>Tags</a></li></ul></div></div><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/akitenkrad-blog/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/>Papers</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202205/>2022.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202205/20220518224923/ title="A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks">A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220508162318/ title="A Deep Cascade Model for Multi-Document Reading Comprehension">A Deep Cascade Model for Multi-Document Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220506021208/ title="A Primer in BERTology: What We Know About How BERT Works">A Primer in BERTology: What We Know About How BERT Works</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220529131339/ title="Attention Is All You Need">Attention Is All You Need</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220503010000/ title="DeBERTa: Decoding-Enhanced BERT with Disentangled Attention">DeBERTa: Decoding-Enhanced BERT with Disentangled Attention</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220505222900/ title="Dense Passage Retrieval for Open-Domain Question Answering">Dense Passage Retrieval for Open-Domain Question Answering</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220511010217/ title="Multi-Style Generative Reading Comprehension">Multi-Style Generative Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220530102936/ title="Neural Machine Translation of Rare Words with Subword Units">Neural Machine Translation of Rare Words with Subword Units</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220523223206/ title="RoBERTa: A Robustly Optimized BERT Pretraining Approach">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220520124748/ title="Semi-Supervised Classification with Graph Convolutional Networks">Semi-Supervised Classification with Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220509110738/ title="Survey on graph embeddings and their applications to machine learning problems on graphs">Survey on graph embeddings and their applications to machine learning problems on graphs</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220514151839/ title="UnitedQA: A Hybrid Approach for Open Domain Question Answering">UnitedQA: A Hybrid Approach for Open Domain Question Answering</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202206/>2022.06</a><ul><li><a href=/akitenkrad-blog/posts/papers/202206/20220612105422/ title="Attributed Network Embedding for Learning in a Dynamic Environment">Attributed Network Embedding for Learning in a Dynamic Environment</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220608085622/ title="CodeBERT: A Pre-Trained Model for Programming and Natural Languages">CodeBERT: A Pre-Trained Model for Programming and Natural Languages</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220618223844/ title="High-order Proximity Preserved Embedding for Dynamic Networks">High-order Proximity Preserved Embedding for Dynamic Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220602171700/ title="S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension">S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202207/>2022.07</a><ul><li><a href=/akitenkrad-blog/posts/papers/202207/20220727145036/ title="Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions">Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions</a></li><li><a href=/akitenkrad-blog/posts/papers/202207/20220726163444/ title="Dynamic Network Embedding Survey">Dynamic Network Embedding Survey</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/papers/202208/20220802103319/ title="Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN">Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/papers/202209/20220909180827/ title="Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting">Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202305/>2023.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202305/20230513094804/ title="Improving Language Understanding by Generative Pre-Training">Improving Language Understanding by Generative Pre-Training</a></li><li><a href=/akitenkrad-blog/posts/papers/202305/20230514200241/ title="Language Models are Unsupervised Multitask Learners">Language Models are Unsupervised Multitask Learners</a></li></ul></li><li><a href=/akitenkrad-blog/posts/papers/202306/ title=2023.06>2023.06</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/>arXiv</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202307/>2023.07</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230701000000/ title="arXiv @ 2023.07.01">arXiv @ 2023.07.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230702000000/ title="arXiv @ 2023.07.02">arXiv @ 2023.07.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230703000000/ title="arXiv @ 2023.07.03">arXiv @ 2023.07.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230704000000/ title="arXiv @ 2023.07.04">arXiv @ 2023.07.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230705000000/ title="arXiv @ 2023.07.05">arXiv @ 2023.07.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230706000000/ title="arXiv @ 2023.07.06">arXiv @ 2023.07.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230707000000/ title="arXiv @ 2023.07.07">arXiv @ 2023.07.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230708000000/ title="arXiv @ 2023.07.08">arXiv @ 2023.07.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230709000000/ title="arXiv @ 2023.07.09">arXiv @ 2023.07.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230710000000/ title="arXiv @ 2023.07.10">arXiv @ 2023.07.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230711000000/ title="arXiv @ 2023.07.11">arXiv @ 2023.07.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230712000000/ title="arXiv @ 2023.07.12">arXiv @ 2023.07.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230713000000/ title="arXiv @ 2023.07.13">arXiv @ 2023.07.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230714000000/ title="arXiv @ 2023.07.14">arXiv @ 2023.07.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230715000000/ title="arXiv @ 2023.07.15">arXiv @ 2023.07.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230716000000/ title="arXiv @ 2023.07.16">arXiv @ 2023.07.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230717000000/ title="arXiv @ 2023.07.17">arXiv @ 2023.07.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230718000000/ title="arXiv @ 2023.07.18">arXiv @ 2023.07.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230719000000/ title="arXiv @ 2023.07.19">arXiv @ 2023.07.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230720000000/ title="arXiv @ 2023.07.20">arXiv @ 2023.07.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230721000000/ title="arXiv @ 2023.07.21">arXiv @ 2023.07.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230722000000/ title="arXiv @ 2023.07.22">arXiv @ 2023.07.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230723000000/ title="arXiv @ 2023.07.23">arXiv @ 2023.07.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230724000000/ title="arXiv @ 2023.07.24">arXiv @ 2023.07.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230725000000/ title="arXiv @ 2023.07.25">arXiv @ 2023.07.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230726000000/ title="arXiv @ 2023.07.26">arXiv @ 2023.07.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230727000000/ title="arXiv @ 2023.07.27">arXiv @ 2023.07.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230728000000/ title="arXiv @ 2023.07.28">arXiv @ 2023.07.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230729000000/ title="arXiv @ 2023.07.29">arXiv @ 2023.07.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230730000000/ title="arXiv @ 2023.07.30">arXiv @ 2023.07.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230731000000/ title="arXiv @ 2023.07.31">arXiv @ 2023.07.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202308/>2023.08</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230801000000/ title="arXiv @ 2023.08.01">arXiv @ 2023.08.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230802000000/ title="arXiv @ 2023.08.02">arXiv @ 2023.08.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230803000000/ title="arXiv @ 2023.08.03">arXiv @ 2023.08.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230804000000/ title="arXiv @ 2023.08.04">arXiv @ 2023.08.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230805000000/ title="arXiv @ 2023.08.05">arXiv @ 2023.08.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230806000000/ title="arXiv @ 2023.08.06">arXiv @ 2023.08.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230807000000/ title="arXiv @ 2023.08.07">arXiv @ 2023.08.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230808000000/ title="arXiv @ 2023.08.08">arXiv @ 2023.08.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230809000000/ title="arXiv @ 2023.08.09">arXiv @ 2023.08.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230810000000/ title="arXiv @ 2023.08.10">arXiv @ 2023.08.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230811000000/ title="arXiv @ 2023.08.11">arXiv @ 2023.08.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230812000000/ title="arXiv @ 2023.08.12">arXiv @ 2023.08.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230813000000/ title="arXiv @ 2023.08.13">arXiv @ 2023.08.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230814000000/ title="arXiv @ 2023.08.14">arXiv @ 2023.08.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230815000000/ title="arXiv @ 2023.08.15">arXiv @ 2023.08.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230816000000/ title="arXiv @ 2023.08.16">arXiv @ 2023.08.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230817000000/ title="arXiv @ 2023.08.17">arXiv @ 2023.08.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230818000000/ title="arXiv @ 2023.08.18">arXiv @ 2023.08.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230819000000/ title="arXiv @ 2023.08.19">arXiv @ 2023.08.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230820000000/ title="arXiv @ 2023.08.20">arXiv @ 2023.08.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230821000000/ title="arXiv @ 2023.08.21">arXiv @ 2023.08.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230822000000/ title="arXiv @ 2023.08.22">arXiv @ 2023.08.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230823000000/ title="arXiv @ 2023.08.23">arXiv @ 2023.08.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230824000000/ title="arXiv @ 2023.08.24">arXiv @ 2023.08.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230825000000/ title="arXiv @ 2023.08.25">arXiv @ 2023.08.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230826000000/ title="arXiv @ 2023.08.26">arXiv @ 2023.08.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230827000000/ title="arXiv @ 2023.08.27">arXiv @ 2023.08.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230828000000/ title="arXiv @ 2023.08.28">arXiv @ 2023.08.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230829000000/ title="arXiv @ 2023.08.29">arXiv @ 2023.08.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230830000000/ title="arXiv @ 2023.08.30">arXiv @ 2023.08.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230831000000/ title="arXiv @ 2023.08.31">arXiv @ 2023.08.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202309/>2023.09</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230901000000/ title="arXiv @ 2023.09.01">arXiv @ 2023.09.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230902000000/ title="arXiv @ 2023.09.02">arXiv @ 2023.09.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230903000000/ title="arXiv @ 2023.09.03">arXiv @ 2023.09.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230904000000/ title="arXiv @ 2023.09.04">arXiv @ 2023.09.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230905000000/ title="arXiv @ 2023.09.05">arXiv @ 2023.09.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230906000000/ title="arXiv @ 2023.09.06">arXiv @ 2023.09.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230907000000/ title="arXiv @ 2023.09.07">arXiv @ 2023.09.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230908000000/ title="arXiv @ 2023.09.08">arXiv @ 2023.09.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230909000000/ title="arXiv @ 2023.09.09">arXiv @ 2023.09.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230910000000/ title="arXiv @ 2023.09.10">arXiv @ 2023.09.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230911000000/ title="arXiv @ 2023.09.11">arXiv @ 2023.09.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230912000000/ title="arXiv @ 2023.09.12">arXiv @ 2023.09.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230913000000/ title="arXiv @ 2023.09.13">arXiv @ 2023.09.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230914000000/ title="arXiv @ 2023.09.14">arXiv @ 2023.09.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230915000000/ title="arXiv @ 2023.09.15">arXiv @ 2023.09.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230916000000/ title="arXiv @ 2023.09.16">arXiv @ 2023.09.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230917000000/ title="arXiv @ 2023.09.17">arXiv @ 2023.09.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230918000000/ title="arXiv @ 2023.09.18">arXiv @ 2023.09.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230919000000/ title="arXiv @ 2023.09.19">arXiv @ 2023.09.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230920000000/ title="arXiv @ 2023.09.20">arXiv @ 2023.09.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230921000000/ title="arXiv @ 2023.09.21">arXiv @ 2023.09.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230922000000/ title="arXiv @ 2023.09.22">arXiv @ 2023.09.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230923000000/ title="arXiv @ 2023.09.23">arXiv @ 2023.09.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230924000000/ title="arXiv @ 2023.09.24">arXiv @ 2023.09.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230925000000/ title="arXiv @ 2023.09.25">arXiv @ 2023.09.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230926000000/ title="arXiv @ 2023.09.26">arXiv @ 2023.09.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230927000000/ title="arXiv @ 2023.09.27">arXiv @ 2023.09.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230928000000/ title="arXiv @ 2023.09.28">arXiv @ 2023.09.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230929000000/ title="arXiv @ 2023.09.29">arXiv @ 2023.09.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230930000000/ title="arXiv @ 2023.09.30">arXiv @ 2023.09.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202310/>2023.1</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231001000000/ title="arXiv @ 2023.10.01">arXiv @ 2023.10.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231002000000/ title="arXiv @ 2023.10.02">arXiv @ 2023.10.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231003000000/ title="arXiv @ 2023.10.03">arXiv @ 2023.10.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231004000000/ title="arXiv @ 2023.10.04">arXiv @ 2023.10.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231005000000/ title="arXiv @ 2023.10.05">arXiv @ 2023.10.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231006000000/ title="arXiv @ 2023.10.06">arXiv @ 2023.10.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231007000000/ title="arXiv @ 2023.10.07">arXiv @ 2023.10.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231008000000/ title="arXiv @ 2023.10.08">arXiv @ 2023.10.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231009000000/ title="arXiv @ 2023.10.09">arXiv @ 2023.10.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231010000000/ title="arXiv @ 2023.10.10">arXiv @ 2023.10.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231011000000/ title="arXiv @ 2023.10.11">arXiv @ 2023.10.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231012000000/ title="arXiv @ 2023.10.12">arXiv @ 2023.10.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231013000000/ title="arXiv @ 2023.10.13">arXiv @ 2023.10.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231014000000/ title="arXiv @ 2023.10.14">arXiv @ 2023.10.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231015000000/ title="arXiv @ 2023.10.15">arXiv @ 2023.10.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231016000000/ title="arXiv @ 2023.10.16">arXiv @ 2023.10.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231017000000/ title="arXiv @ 2023.10.17">arXiv @ 2023.10.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231018000000/ title="arXiv @ 2023.10.18">arXiv @ 2023.10.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231019000000/ title="arXiv @ 2023.10.19">arXiv @ 2023.10.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231020000000/ title="arXiv @ 2023.10.20">arXiv @ 2023.10.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231021000000/ title="arXiv @ 2023.10.21">arXiv @ 2023.10.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231022000000/ title="arXiv @ 2023.10.22">arXiv @ 2023.10.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231023000000/ title="arXiv @ 2023.10.23">arXiv @ 2023.10.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231024000000/ title="arXiv @ 2023.10.24">arXiv @ 2023.10.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231025000000/ title="arXiv @ 2023.10.25">arXiv @ 2023.10.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231026000000/ title="arXiv @ 2023.10.26">arXiv @ 2023.10.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231027000000/ title="arXiv @ 2023.10.27">arXiv @ 2023.10.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231028000000/ title="arXiv @ 2023.10.28">arXiv @ 2023.10.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231029000000/ title="arXiv @ 2023.10.29">arXiv @ 2023.10.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231030000000/ title="arXiv @ 2023.10.30">arXiv @ 2023.10.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231031000000/ title="arXiv @ 2023.10.31">arXiv @ 2023.10.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202311/>2023.11</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231101000000/ title="arXiv @ 2023.11.01">arXiv @ 2023.11.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231102000000/ title="arXiv @ 2023.11.02">arXiv @ 2023.11.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231103000000/ title="arXiv @ 2023.11.03">arXiv @ 2023.11.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231104000000/ title="arXiv @ 2023.11.04">arXiv @ 2023.11.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231105000000/ title="arXiv @ 2023.11.05">arXiv @ 2023.11.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231106000000/ title="arXiv @ 2023.11.06">arXiv @ 2023.11.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231107000000/ title="arXiv @ 2023.11.07">arXiv @ 2023.11.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231108000000/ title="arXiv @ 2023.11.08">arXiv @ 2023.11.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231109000000/ title="arXiv @ 2023.11.09">arXiv @ 2023.11.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231110000000/ title="arXiv @ 2023.11.10">arXiv @ 2023.11.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231111000000/ title="arXiv @ 2023.11.11">arXiv @ 2023.11.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231112000000/ title="arXiv @ 2023.11.12">arXiv @ 2023.11.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231113000000/ title="arXiv @ 2023.11.13">arXiv @ 2023.11.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231114000000/ title="arXiv @ 2023.11.14">arXiv @ 2023.11.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231115000000/ title="arXiv @ 2023.11.15">arXiv @ 2023.11.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231116000000/ title="arXiv @ 2023.11.16">arXiv @ 2023.11.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231117000000/ title="arXiv @ 2023.11.17">arXiv @ 2023.11.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231118000000/ title="arXiv @ 2023.11.18">arXiv @ 2023.11.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231119000000/ title="arXiv @ 2023.11.19">arXiv @ 2023.11.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231120000000/ title="arXiv @ 2023.11.20">arXiv @ 2023.11.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231121000000/ title="arXiv @ 2023.11.21">arXiv @ 2023.11.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231122000000/ title="arXiv @ 2023.11.22">arXiv @ 2023.11.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231123000000/ title="arXiv @ 2023.11.23">arXiv @ 2023.11.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231124000000/ title="arXiv @ 2023.11.24">arXiv @ 2023.11.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231125000000/ title="arXiv @ 2023.11.25">arXiv @ 2023.11.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231126000000/ title="arXiv @ 2023.11.26">arXiv @ 2023.11.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231127000000/ title="arXiv @ 2023.11.27">arXiv @ 2023.11.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231128000000/ title="arXiv @ 2023.11.28">arXiv @ 2023.11.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231129000000/ title="arXiv @ 2023.11.29">arXiv @ 2023.11.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231130000000/ title="arXiv @ 2023.11.30">arXiv @ 2023.11.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202312/>2023.12</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231201000000/ title="arXiv @ 2023.12.01">arXiv @ 2023.12.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231202000000/ title="arXiv @ 2023.12.02">arXiv @ 2023.12.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231203000000/ title="arXiv @ 2023.12.03">arXiv @ 2023.12.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231204000000/ title="arXiv @ 2023.12.04">arXiv @ 2023.12.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231205000000/ title="arXiv @ 2023.12.05">arXiv @ 2023.12.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231206000000/ title="arXiv @ 2023.12.06">arXiv @ 2023.12.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231207000000/ title="arXiv @ 2023.12.07">arXiv @ 2023.12.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231208000000/ title="arXiv @ 2023.12.08">arXiv @ 2023.12.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231209000000/ title="arXiv @ 2023.12.09">arXiv @ 2023.12.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231210000000/ title="arXiv @ 2023.12.10">arXiv @ 2023.12.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231211000000/ title="arXiv @ 2023.12.11">arXiv @ 2023.12.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231212000000/ title="arXiv @ 2023.12.12">arXiv @ 2023.12.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231213000000/ title="arXiv @ 2023.12.13">arXiv @ 2023.12.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231214000000/ title="arXiv @ 2023.12.14">arXiv @ 2023.12.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231215000000/ title="arXiv @ 2023.12.15">arXiv @ 2023.12.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231216000000/ title="arXiv @ 2023.12.16">arXiv @ 2023.12.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231217000000/ title="arXiv @ 2023.12.17">arXiv @ 2023.12.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231218000000/ title="arXiv @ 2023.12.18">arXiv @ 2023.12.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231219000000/ title="arXiv @ 2023.12.19">arXiv @ 2023.12.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231220000000/ title="arXiv @ 2023.12.20">arXiv @ 2023.12.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231221000000/ title="arXiv @ 2023.12.21">arXiv @ 2023.12.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231222000000/ title="arXiv @ 2023.12.22">arXiv @ 2023.12.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231223000000/ title="arXiv @ 2023.12.23">arXiv @ 2023.12.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231224000000/ title="arXiv @ 2023.12.24">arXiv @ 2023.12.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231225000000/ title="arXiv @ 2023.12.25">arXiv @ 2023.12.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231226000000/ title="arXiv @ 2023.12.26">arXiv @ 2023.12.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231227000000/ title="arXiv @ 2023.12.27">arXiv @ 2023.12.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231228000000/ title="arXiv @ 2023.12.28">arXiv @ 2023.12.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231229000000/ title="arXiv @ 2023.12.29">arXiv @ 2023.12.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231230000000/ title="arXiv @ 2023.12.30">arXiv @ 2023.12.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231231000000/ title="arXiv @ 2023.12.31">arXiv @ 2023.12.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202401/>2024.01</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240101000000/ title="arXiv @ 2024.01.01">arXiv @ 2024.01.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240102000000/ title="arXiv @ 2024.01.02">arXiv @ 2024.01.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240103000000/ title="arXiv @ 2024.01.03">arXiv @ 2024.01.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240104000000/ title="arXiv @ 2024.01.04">arXiv @ 2024.01.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240105000000/ title="arXiv @ 2024.01.05">arXiv @ 2024.01.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240106000000/ title="arXiv @ 2024.01.06">arXiv @ 2024.01.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240107000000/ title="arXiv @ 2024.01.07">arXiv @ 2024.01.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240108000000/ title="arXiv @ 2024.01.08">arXiv @ 2024.01.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240109000000/ title="arXiv @ 2024.01.09">arXiv @ 2024.01.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240110000000/ title="arXiv @ 2024.01.10">arXiv @ 2024.01.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240111000000/ title="arXiv @ 2024.01.11">arXiv @ 2024.01.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240112000000/ title="arXiv @ 2024.01.12">arXiv @ 2024.01.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240113000000/ title="arXiv @ 2024.01.13">arXiv @ 2024.01.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240114000000/ title="arXiv @ 2024.01.14">arXiv @ 2024.01.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240115000000/ title="arXiv @ 2024.01.15">arXiv @ 2024.01.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240116000000/ title="arXiv @ 2024.01.16">arXiv @ 2024.01.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240117000000/ title="arXiv @ 2024.01.17">arXiv @ 2024.01.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240118000000/ title="arXiv @ 2024.01.18">arXiv @ 2024.01.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240119000000/ title="arXiv @ 2024.01.19">arXiv @ 2024.01.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240120000000/ title="arXiv @ 2024.01.20">arXiv @ 2024.01.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240121000000/ title="arXiv @ 2024.01.21">arXiv @ 2024.01.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240122000000/ title="arXiv @ 2024.01.22">arXiv @ 2024.01.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240123000000/ title="arXiv @ 2024.01.23">arXiv @ 2024.01.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240124000000/ title="arXiv @ 2024.01.24">arXiv @ 2024.01.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240125000000/ title="arXiv @ 2024.01.25">arXiv @ 2024.01.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240126000000/ title="arXiv @ 2024.01.26">arXiv @ 2024.01.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240127000000/ title="arXiv @ 2024.01.27">arXiv @ 2024.01.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240128000000/ title="arXiv @ 2024.01.28">arXiv @ 2024.01.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240129000000/ title="arXiv @ 2024.01.29">arXiv @ 2024.01.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240130000000/ title="arXiv @ 2024.01.30">arXiv @ 2024.01.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240131000000/ title="arXiv @ 2024.01.31">arXiv @ 2024.01.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202402/>2024.02</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240201000000/ title="arXiv @ 2024.02.01">arXiv @ 2024.02.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240202000000/ title="arXiv @ 2024.02.02">arXiv @ 2024.02.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240203000000/ title="arXiv @ 2024.02.03">arXiv @ 2024.02.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240204000000/ title="arXiv @ 2024.02.04">arXiv @ 2024.02.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240205000000/ title="arXiv @ 2024.02.05">arXiv @ 2024.02.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240206000000/ title="arXiv @ 2024.02.06">arXiv @ 2024.02.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/ title="arXiv @ 2024.02.07">arXiv @ 2024.02.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/ title="arXiv @ 2024.02.08">arXiv @ 2024.02.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/ title="arXiv @ 2024.02.09">arXiv @ 2024.02.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/ title="arXiv @ 2024.02.10">arXiv @ 2024.02.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240211000000/ title="arXiv @ 2024.02.11">arXiv @ 2024.02.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/ title="arXiv @ 2024.02.12">arXiv @ 2024.02.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/ title="arXiv @ 2024.02.13">arXiv @ 2024.02.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/ title="arXiv @ 2024.02.14">arXiv @ 2024.02.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/ title="arXiv @ 2024.02.15">arXiv @ 2024.02.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240216000000/ title="arXiv @ 2024.02.16">arXiv @ 2024.02.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/ title="arXiv @ 2024.02.17">arXiv @ 2024.02.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240218000000/ title="arXiv @ 2024.02.18">arXiv @ 2024.02.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/ title="arXiv @ 2024.02.19">arXiv @ 2024.02.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/ title="arXiv @ 2024.02.20">arXiv @ 2024.02.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/ title="arXiv @ 2024.02.21">arXiv @ 2024.02.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/ title="arXiv @ 2024.02.22">arXiv @ 2024.02.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/ title="arXiv @ 2024.02.23">arXiv @ 2024.02.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/ title="arXiv @ 2024.02.24">arXiv @ 2024.02.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/ title="arXiv @ 2024.02.25">arXiv @ 2024.02.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/ title="arXiv @ 2024.02.26">arXiv @ 2024.02.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/ title="arXiv @ 2024.02.27">arXiv @ 2024.02.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/ title="arXiv @ 2024.02.28">arXiv @ 2024.02.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/ title="arXiv @ 2024.02.29">arXiv @ 2024.02.29</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/202403/>2024.03</a><ul class=active><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/ title="arXiv @ 2024.03.01">arXiv @ 2024.03.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/ title="arXiv @ 2024.03.02">arXiv @ 2024.03.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/ title="arXiv @ 2024.03.03">arXiv @ 2024.03.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240304000000/ title="arXiv @ 2024.03.04">arXiv @ 2024.03.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/ title="arXiv @ 2024.03.05">arXiv @ 2024.03.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/ title="arXiv @ 2024.03.06">arXiv @ 2024.03.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/ title="arXiv @ 2024.03.07">arXiv @ 2024.03.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/ title="arXiv @ 2024.03.08">arXiv @ 2024.03.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/ title="arXiv @ 2024.03.09">arXiv @ 2024.03.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/ title="arXiv @ 2024.03.10">arXiv @ 2024.03.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/ title="arXiv @ 2024.03.11">arXiv @ 2024.03.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/ title="arXiv @ 2024.03.12">arXiv @ 2024.03.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/ title="arXiv @ 2024.03.13">arXiv @ 2024.03.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/ title="arXiv @ 2024.03.14">arXiv @ 2024.03.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/ title="arXiv @ 2024.03.15">arXiv @ 2024.03.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/ title="arXiv @ 2024.03.16">arXiv @ 2024.03.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/ title="arXiv @ 2024.03.17">arXiv @ 2024.03.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/ title="arXiv @ 2024.03.18">arXiv @ 2024.03.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240319000000/ title="arXiv @ 2024.03.19">arXiv @ 2024.03.19</a></li><li><a class=active href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/ title="arXiv @ 2024.03.20">arXiv @ 2024.03.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/ title="arXiv @ 2024.03.21">arXiv @ 2024.03.21</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/algorithms/>Algorithms</a><ul><li><a href=/akitenkrad-blog/posts/algorithms/bandit-algorithm-basic/ title="Bandit Algorithm Basic">Bandit Algorithm Basic</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/conference/>Conference</a><ul><li><a href=/akitenkrad-blog/posts/conference/acl/ title="Annual Meeting of the Association for Computational Linguistics">Annual Meeting of the Association for Computational Linguistics</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/>Figures</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/figures/202208/20220822092748/ title=1ヶ月当たりの実労働時間の推移>1ヶ月当たりの実労働時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220813115200/ title=コンビニエンスストアの店舗数の推移>コンビニエンスストアの店舗数の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220821113322/ title=地域別1世帯当たり1ヶ月間の支出額の推移>地域別1世帯当たり1ヶ月間の支出額の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220828111836/ title="夫婦別子供の有無別生活時間 (2016)">夫婦別子供の有無別生活時間 (2016)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220827103615/ title=家族類型の変遷>家族類型の変遷</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220815131016/ title="年齢別大学院入学者数 (2021)">年齢別大学院入学者数 (2021)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220818122628/ title=情報サービス業における企業特殊的人的資本（名目）の推移>情報サービス業における企業特殊的人的資本（名目）の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220824095127/ title=有業者の社会生活における活動の変化>有業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220814175022/ title=東京23区の乗用車保有台数>東京23区の乗用車保有台数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220817121117/ title="業種別企業特殊的人的資本 (2018)">業種別企業特殊的人的資本 (2018)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220825094111/ title=無業者の社会生活における活動の変化>無業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220823104100/ title=睡眠時間の推移>睡眠時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220826102652/ title=総人口の推移>総人口の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220820225528/ title=都道府県別国公立別学校数・学生数>都道府県別国公立別学校数・学生数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220816120845/ title="都道府県別消費者物価指数/教育（全国平均=100）">都道府県別消費者物価指数/教育（全国平均=100）</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220812173814/ title=金沢市の夏の月平均気温の遷移>金沢市の夏の月平均気温の遷移</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/figures/202209/20220907122906/ title="ACL-2022 WordCloud">ACL-2022 WordCloud</a></li></ul></li></ul></li><li><a href=/akitenkrad-blog/posts/latex/ title="Latex Mathematics Syntax Guide">Latex Mathematics Syntax Guide</a></li><li><a href=/akitenkrad-blog/posts/markdown/ title="Markdown Sample">Markdown Sample</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/akitenkrad-blog/posts/arxiv/202403/20240320000000/hero.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_120x120_fit_box_3.png alt="Author Image"><h5 class=author-name></h5><p>Wednesday, Mar 20, 2024</p></div><div class=title><h1>arXiv @ 2024.03.20</h1></div><div class=taxonomy-terms><ul><li class=rounded><a href=/akitenkrad-blog/tags/arxiv class="btn, btn-sm">arXiv</a></li><li class=rounded><a href=/akitenkrad-blog/tags/published2024 class="btn, btn-sm">2024</a></li></ul></div><div class=post-content id=post-content><figure style=border:none;width:100%;display:flex;justify-content:center><iframe src=pie.html width=900 height=620 style=border:none></iframe></figure><h2 id=primary-categories>Primary Categories</h2><ul><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/#cond-matstr-el-1>cond-mat.str-el (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/#csai-9>cs.AI (9)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/#csar-1>cs.AR (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/#csce-1>cs.CE (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/#cscl-43>cs.CL (43)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/#cscr-8>cs.CR (8)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/#cscv-122>cs.CV (122)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/#cscy-5>cs.CY (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/#csdb-1>cs.DB (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/#csdc-4>cs.DC (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/#csds-1>cs.DS (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/#csgr-2>cs.GR (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/#csgt-1>cs.GT (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/#cshc-5>cs.HC (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/#csir-1>cs.IR (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/#csit-6>cs.IT (6)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/#cslg-50>cs.LG (50)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/#csmm-1>cs.MM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/#csne-2>cs.NE (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/#csni-1>cs.NI (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/#csos-1>cs.OS (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/#cspl-1>cs.PL (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/#csro-32>cs.RO (32)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/#cssd-6>cs.SD (6)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/#csse-3>cs.SE (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/#eessiv-13>eess.IV (13)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/#eesssp-1>eess.SP (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/#eesssy-9>eess.SY (9)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/#hep-ph-1>hep-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/#hep-th-1>hep-th (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/#mathat-1>math.AT (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/#mathco-2>math.CO (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/#mathna-4>math.NA (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/#mathoc-2>math.OC (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/#physicsdata-an-1>physics.data-an (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/#q-biocb-1>q-bio.CB (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/#q-bionc-1>q-bio.NC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/#statml-5>stat.ML (5)</a></li></ul><h2 id=keywords>Keywords</h2><table border=1 class=dataframe><thead><tr style=text-align:right><th>keyword</th><th>cs.CL</th><th>cs.CV</th><th>cs.LG</th><th>cs.RO</th><th>eess.IV</th></tr></thead><tbody><tr><td>Adversarial Attack</td><td>1</td><td>4</td><td></td><td></td><td></td></tr><tr><td>Adversarial Learning</td><td></td><td>1</td><td></td><td></td><td>1</td></tr><tr><td>Adversarial Purification</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Anomaly Detection</td><td></td><td>3</td><td></td><td></td><td></td></tr><tr><td>Autoencoder</td><td></td><td>4</td><td>1</td><td></td><td>1</td></tr><tr><td>Automatic Evaluation</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Automatic Speech Recognition</td><td>1</td><td></td><td></td><td>1</td><td></td></tr><tr><td>BERT</td><td>1</td><td></td><td>1</td><td></td><td></td></tr><tr><td>BERTScore</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>BLEU</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Bandit Algorithm</td><td></td><td></td><td>2</td><td></td><td></td></tr><tr><td>Benchmarking</td><td>11</td><td>33</td><td>8</td><td>5</td><td>4</td></tr><tr><td>Black Box</td><td>1</td><td>1</td><td></td><td>1</td><td></td></tr><tr><td>ChatGPT</td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>Clustering</td><td></td><td></td><td></td><td></td><td>1</td></tr><tr><td>Code Generation</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Common-sense Reasoning</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Constrained Learning</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Continual Learning</td><td>1</td><td>2</td><td>1</td><td>1</td><td></td></tr><tr><td>Continuous Time</td><td></td><td></td><td></td><td>4</td><td></td></tr><tr><td>Contrastive Learning</td><td>1</td><td>2</td><td>3</td><td>1</td><td></td></tr><tr><td>ControlNet</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Convolution</td><td></td><td>12</td><td>2</td><td>3</td><td>2</td></tr><tr><td>Convolutional Neural Network</td><td></td><td>16</td><td>3</td><td>3</td><td>4</td></tr><tr><td>Counter-factual</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Counterfactual Reasoning</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Data Augmentation</td><td>1</td><td>4</td><td>2</td><td></td><td>1</td></tr><tr><td>Differential Privacy</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Diffusion Model</td><td></td><td>27</td><td>3</td><td>1</td><td>1</td></tr><tr><td>Distribution Shift</td><td></td><td>6</td><td>2</td><td></td><td></td></tr><tr><td>Domain Adaptation</td><td></td><td>1</td><td></td><td>2</td><td>2</td></tr><tr><td>Emotion Recognition</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Explainable AI</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Face Recognition</td><td></td><td>3</td><td></td><td></td><td></td></tr><tr><td>Fact Verification</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Fairness</td><td></td><td></td><td>1</td><td></td><td>1</td></tr><tr><td>Federated Learning</td><td></td><td>2</td><td>3</td><td></td><td></td></tr><tr><td>Few-shot</td><td>5</td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>Few-shot Learning</td><td>2</td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>Fine-tuning</td><td>12</td><td>16</td><td>7</td><td>1</td><td>1</td></tr><tr><td>Foundation Model</td><td>1</td><td>8</td><td></td><td>1</td><td></td></tr><tr><td>GPT</td><td>10</td><td>3</td><td></td><td></td><td></td></tr><tr><td>GPT-2</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>GPT-3</td><td>7</td><td></td><td></td><td></td><td></td></tr><tr><td>GPT-3.5</td><td>6</td><td></td><td></td><td></td><td></td></tr><tr><td>GPT-4</td><td>7</td><td></td><td></td><td></td><td></td></tr><tr><td>GPT-4 turbo</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Gaussian Process</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Gemini</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Generative AI</td><td></td><td>1</td><td>2</td><td></td><td></td></tr><tr><td>Generative Adversarial Network</td><td></td><td>7</td><td></td><td></td><td></td></tr><tr><td>Geometry</td><td></td><td>10</td><td>1</td><td>1</td><td></td></tr><tr><td>Graph</td><td>1</td><td>6</td><td>8</td><td>2</td><td></td></tr><tr><td>Graph Attention Networks</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Graph Classification</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Graph Neural Network</td><td></td><td>2</td><td>8</td><td></td><td></td></tr><tr><td>Grounding</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Hallucination Detection</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>High-Resource</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Image2text</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>In-context Learning</td><td>4</td><td></td><td>2</td><td>2</td><td></td></tr><tr><td>Information Retrieval</td><td>1</td><td>1</td><td></td><td>1</td><td></td></tr><tr><td>Instruction Following</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Instruction Tuning</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Knowledge Distillation</td><td>1</td><td>17</td><td>4</td><td>1</td><td></td></tr><tr><td>Knowledge Graph</td><td>1</td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>Knowledge Transfer</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>LLaMA</td><td>4</td><td></td><td></td><td></td><td></td></tr><tr><td>LSTM</td><td></td><td></td><td>2</td><td></td><td></td></tr><tr><td>Large Language Model</td><td>56</td><td>13</td><td>11</td><td>5</td><td></td></tr><tr><td>Logistic Regression</td><td>1</td><td></td><td></td><td></td><td>1</td></tr><tr><td>Low-Resource</td><td>3</td><td></td><td></td><td></td><td></td></tr><tr><td>MNIST</td><td></td><td>1</td><td>3</td><td></td><td></td></tr><tr><td>Markov Decision Process</td><td></td><td></td><td>4</td><td></td><td></td></tr><tr><td>Markov Game</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Masked Language Model</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Mathematical Reasoning</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Message-Passing</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Meta Learning</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Model Pruning</td><td>1</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Multi-modal</td><td>3</td><td>22</td><td>2</td><td>4</td><td></td></tr><tr><td>Multiple Instance Learning</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Mutual Information</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Named Entity Recognition</td><td>4</td><td></td><td></td><td></td><td></td></tr><tr><td>Neural Machine Translation</td><td>4</td><td></td><td></td><td></td><td></td></tr><tr><td>Node Classification</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Object Detection</td><td></td><td>10</td><td></td><td>1</td><td></td></tr><tr><td>Optical Character Recognition</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Out-of-distribution</td><td>1</td><td>3</td><td>3</td><td>1</td><td></td></tr><tr><td>Pre-trained Language Model</td><td>1</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Prompt</td><td>9</td><td>11</td><td>3</td><td>2</td><td></td></tr><tr><td>Pruning</td><td>1</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Quantization</td><td></td><td></td><td></td><td></td><td>2</td></tr><tr><td>Question Answering</td><td>3</td><td>2</td><td></td><td>1</td><td></td></tr><tr><td>Reasoning</td><td>8</td><td>2</td><td>3</td><td>3</td><td></td></tr><tr><td>Recurrent Neural Network</td><td></td><td></td><td>3</td><td></td><td></td></tr><tr><td>Reinforcement Learning</td><td>3</td><td>1</td><td>10</td><td>7</td><td></td></tr><tr><td>Representation Learning</td><td></td><td>2</td><td>6</td><td></td><td>2</td></tr><tr><td>Retrieval-Augmented Generation</td><td>5</td><td></td><td></td><td></td><td></td></tr><tr><td>RoBERTa</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Sample Size</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Self-Attention</td><td></td><td>2</td><td></td><td>1</td><td></td></tr><tr><td>Self-supervised Learning</td><td></td><td>4</td><td>5</td><td>2</td><td>4</td></tr><tr><td>Semantic Parsing</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Semi-Supervised Learning</td><td></td><td>3</td><td>1</td><td></td><td></td></tr><tr><td>Sentence Embedding</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Sentiment Analysis</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Simulation</td><td>2</td><td>2</td><td>5</td><td>17</td><td></td></tr><tr><td>Simulator</td><td>2</td><td>2</td><td>5</td><td>17</td><td></td></tr><tr><td>Style Transfer</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Summarization</td><td>3</td><td>3</td><td></td><td></td><td></td></tr><tr><td>Supervised Learning</td><td>6</td><td>4</td><td>10</td><td></td><td>1</td></tr><tr><td>T5</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>TF-IDF</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Text Embedding</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Text Generation</td><td>5</td><td></td><td></td><td></td><td></td></tr><tr><td>Text Mining</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Text2image</td><td></td><td>10</td><td>1</td><td></td><td></td></tr><tr><td>Tokenization</td><td></td><td></td><td></td><td></td><td>1</td></tr><tr><td>Transfer Learning</td><td></td><td>3</td><td>2</td><td></td><td></td></tr><tr><td>Transformer</td><td>4</td><td>16</td><td>7</td><td>1</td><td></td></tr><tr><td>Unsupervised Learning</td><td></td><td>6</td><td>1</td><td></td><td>1</td></tr><tr><td>Variational Autoencoder</td><td></td><td>1</td><td></td><td></td><td>1</td></tr><tr><td>Vision Transformer</td><td></td><td>6</td><td></td><td></td><td></td></tr><tr><td>Vision-and-Language</td><td>1</td><td>11</td><td></td><td></td><td></td></tr><tr><td>Visual Question Answering</td><td></td><td>2</td><td></td><td>1</td><td></td></tr><tr><td>Weakly Supervised Learning</td><td>1</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Weakly-supervised Learning</td><td></td><td>1</td><td>2</td><td></td><td>1</td></tr><tr><td>Zero-shot</td><td>5</td><td>16</td><td></td><td>2</td><td></td></tr><tr><td>Zero-shot Learning</td><td>1</td><td></td><td></td><td>1</td><td></td></tr><tr><td>human-in-the-loop</td><td></td><td>1</td><td></td><td></td><td></td></tr></tbody></table><script>$(function(){$("table").addClass("keyword-table table-bordered border-success"),$("table thead").addClass("sticky-top"),$("table tbody td").css("text-align","")})</script><h2 id=cscl-43>cs.CL (43)</h2><h3 id=143--1350-embedded-named-entity-recognition-using-probing-classifiers-nicholas-popovič-et-al-2024>(1/43 | 1/350) Embedded Named Entity Recognition using Probing Classifiers (Nicholas Popovič et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nicholas Popovič, Michael Färber. (2024)<br><strong>Embedded Named Entity Recognition using Probing Classifiers</strong><br><button class=copy-to-clipboard title="Embedded Named Entity Recognition using Probing Classifiers" index=1>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-1 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 110<br>Keywords: Fine-tuning, Retrieval-Augmented Generation, Retrieval-Augmented Generation, GPT, GPT-2, Fact Verification, Information Retrieval, Named Entity Recognition, Named Entity Recognition, Text Generation, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11747v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11747v1.pdf filename=2403.11747v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Extracting semantic <b>information</b> <b>from</b> generated <b>text</b> <b>is</b> a useful tool for applications such as automated <b>fact</b> <b>checking</b> or <b>retrieval</b> <b>augmented</b> <b>generation.</b> Currently, this requires either separate models during inference, which increases computational cost, or destructive <b>fine-tuning</b> of the language model. Instead, we propose directly embedding <b>information</b> <b>extraction</b> capabilities into <b>pre-trained</b> <b>language</b> <b>models</b> using probing classifiers, enabling efficient simultaneous <b>text</b> <b>generation</b> and <b>information</b> <b>extraction.</b> For this, we introduce an approach called EMBER and show that it enables <b>named</b> <b>entity</b> <b>recognition</b> in decoder-only language models without <b>fine-tuning</b> them and while incurring minimal additional computational cost at inference time. Specifically, our experiments using <b>GPT-2</b> show that EMBER maintains high token generation rates during streaming <b>text</b> <b>generation,</b> with only a negligible decrease in speed of around 1% compared to a 43.64% slowdown measured for a baseline using a separate <b>NER</b> model. Code and data are available at <a href=https://github.com/nicpopovic/EMBER>https://github.com/nicpopovic/EMBER</a>.</p></p class="citation"></blockquote><h3 id=243--2350-cicle-conformal-in-context-learning-for-largescale-multi-class-food-risk-classification-korbinian-randl-et-al-2024>(2/43 | 2/350) CICLe: Conformal In-Context Learning for Largescale Multi-Class Food Risk Classification (Korbinian Randl et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Korbinian Randl, John Pavlopoulos, Aron Henriksson, Tony Lindgren. (2024)<br><strong>CICLe: Conformal In-Context Learning for Largescale Multi-Class Food Risk Classification</strong><br><button class=copy-to-clipboard title="CICLe: Conformal In-Context Learning for Largescale Multi-Class Food Risk Classification" index=2>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-2 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 83<br>Keywords: Benchmarking, Logistic Regression, RoBERTa, Transformer, In-context Learning, In-context Learning, Large Language Model, Prompt, TF-IDF<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11904v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11904v1.pdf filename=2403.11904v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Contaminated or adulterated food poses a substantial risk to human health. Given sets of labeled web texts for training, Machine Learning and Natural Language Processing can be applied to automatically detect such risks. We publish a dataset of 7,546 short texts describing public food recall announcements. Each text is manually labeled, on two granularity levels (coarse and fine), for food products and hazards that the recall corresponds to. We describe the dataset and <b>benchmark</b> naive, traditional, and <b>Transformer</b> models. Based on our analysis, <b>Logistic</b> <b>Regression</b> based on a <b>tf-idf</b> representation outperforms <b>RoBERTa</b> and XLM-R on classes with low support. Finally, we discuss different <b>prompting</b> strategies and present an <b>LLM-in-the-loop</b> framework, based on Conformal Prediction, which boosts the performance of the base classifier while reducing energy consumption compared to normal <b>prompting.</b></p></p class="citation"></blockquote><h3 id=343--3350-ensuring-safe-and-high-quality-outputs-a-guideline-library-approach-for-language-models-yi-luo-et-al-2024>(3/43 | 3/350) Ensuring Safe and High-Quality Outputs: A Guideline Library Approach for Language Models (Yi Luo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yi Luo, Zhenghao Lin, Yuhao Zhang, Jiashuo Sun, Chen Lin, Chengjin Xu, Xiangdong Su, Yelong Shen, Jian Guo, Yeyun Gong. (2024)<br><strong>Ensuring Safe and High-Quality Outputs: A Guideline Library Approach for Language Models</strong><br><button class=copy-to-clipboard title="Ensuring Safe and High-Quality Outputs: A Guideline Library Approach for Language Models" index=3>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-3 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 83<br>Keywords: Benchmarking, Fine-tuning, Fine-tuning, GPT, GPT-3, GPT-3.5, GPT-4, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11838v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11838v1.pdf filename=2403.11838v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> exhibit impressive capabilities but also present risks such as biased content generation and privacy issues. One of the current alignment techniques includes principle-driven integration, but it faces challenges arising from the imprecision of manually crafted rules and inadequate risk perception in models without safety training. To address these, we introduce Guide-Align, a two-stage approach. Initially, a safety-trained model identifies potential risks and formulates specific guidelines for various inputs, thereby establishing a comprehensive library of guidelines and models for input-guidelines retrieval. Subsequently, the retrieval model correlates new inputs with pertinent guidelines, guiding <b>LLMs</b> in response generation to ensure safe and high-quality outputs, thus aligning with human values. An additional optional stage involves <b>fine-tuning</b> a model with new well-aligned datasets generated through the process implemented in the second stage. Our method customizes guidelines to accommodate diverse inputs, thereby enhancing the fine-grainedness and comprehensiveness of the guideline library. Furthermore, it incorporates safety expertise from a safety-trained <b>LLM</b> through a lightweight retrieval model. We evaluated our approach on three <b>benchmarks,</b> demonstrating significant improvements in <b>LLM</b> security and quality. Notably, our <b>fine-tuned</b> model, Labrador, even at 13 billion parameters, outperforms <b>GPT-3.5-turbo</b> and surpasses <b>GPT-4</b> in alignment capabilities.</p></p class="citation"></blockquote><h3 id=443--4350-finllama-financial-sentiment-classification-for-algorithmic-trading-applications-thanos-konstantinidis-et-al-2024>(4/43 | 4/350) FinLlama: Financial Sentiment Classification for Algorithmic Trading Applications (Thanos Konstantinidis et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Thanos Konstantinidis, Giorgos Iacovides, Mingxue Xu, Tony G. Constantinides, Danilo Mandic. (2024)<br><strong>FinLlama: Financial Sentiment Classification for Algorithmic Trading Applications</strong><br><button class=copy-to-clipboard title="FinLlama: Financial Sentiment Classification for Algorithmic Trading Applications" index=4>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-4 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL, q-fin-ST, q-fin-TR<br>Keyword Score: 80<br>Keywords: Fine-tuning, Simulation, Simulator, Supervised Learning, LLaMA, Sentiment Analysis, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12285v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12285v1.pdf filename=2403.12285v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>There are multiple sources of financial news online which influence market movements and trader&rsquo;s decisions. This highlights the need for accurate <b>sentiment</b> <b>analysis,</b> in addition to having appropriate algorithmic trading techniques, to arrive at better informed trading decisions. Standard lexicon based <b>sentiment</b> <b>approaches</b> have demonstrated their power in aiding financial decisions. However, they are known to suffer from issues related to context sensitivity and word ordering. <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> can also be used in this context, but they are not finance-specific and tend to require significant computational resources. To facilitate a finance specific <b>LLM</b> framework, we introduce a novel approach based on the <b>Llama</b> 2 7B foundational model, in order to benefit from its generative nature and comprehensive language manipulation. This is achieved by <b>fine-tuning</b> the Llama2 7B model on a small portion of <b>supervised</b> financial <b>sentiment</b> <b>analysis</b> data, so as to jointly handle the complexities of financial lexicon and context, and further equipping it with a neural network based decision mechanism. Such a generator-classifier scheme, referred to as FinLlama, is trained not only to classify the <b>sentiment</b> <b>valence</b> but also quantify its strength, thus offering traders a nuanced insight into financial news articles. Complementing this, the implementation of parameter-efficient <b>fine-tuning</b> through LoRA optimises trainable parameters, thus minimising computational and memory requirements, without sacrificing accuracy. <b>Simulation</b> results demonstrate the ability of the proposed FinLlama to provide a framework for enhanced portfolio management decisions and increased market returns. These results underpin the ability of FinLlama to construct high-return portfolios which exhibit enhanced resilience, even during volatile periods and unpredictable market events.</p></p class="citation"></blockquote><h3 id=543--5350-evaluating-named-entity-recognition-comparative-analysis-of-mono--and-multilingual-transformer-models-on-brazilian-corporate-earnings-call-transcriptions-ramon-abilio-et-al-2024>(5/43 | 5/350) Evaluating Named Entity Recognition: Comparative Analysis of Mono- and Multilingual Transformer Models on Brazilian Corporate Earnings Call Transcriptions (Ramon Abilio et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ramon Abilio, Guilherme Palermo Coelho, Ana Estela Antunes da Silva. (2024)<br><strong>Evaluating Named Entity Recognition: Comparative Analysis of Mono- and Multilingual Transformer Models on Brazilian Corporate Earnings Call Transcriptions</strong><br><button class=copy-to-clipboard title="Evaluating Named Entity Recognition: Comparative Analysis of Mono- and Multilingual Transformer Models on Brazilian Corporate Earnings Call Transcriptions" index=5>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-5 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: 68T50, cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 80<br>Keywords: Fine-tuning, BERT, T5, Transformer, Named Entity Recognition, Named Entity Recognition, Text Generation, Weakly Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12212v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12212v1.pdf filename=2403.12212v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Named</b> <b>Entity</b> <b>Recognition</b> <b>(NER)</b> is a Natural Language Processing technique for extracting information from textual documents. However, much of the existing research on <b>NER</b> has been centered around English-language documents, leaving a gap in the availability of datasets tailored to the financial domain in Portuguese. This study addresses the need for <b>NER</b> within the financial domain, focusing on Portuguese-language <b>texts</b> <b>extracted</b> from earnings call transcriptions of Brazilian banks. By curating a comprehensive dataset comprising 384 transcriptions and leveraging <b>weak</b> <b>supervision</b> techniques for annotation, we evaluate the performance of monolingual models trained on Portuguese (BERTimbau and PTT5) and multilingual models (mBERT and mT5). Notably, we introduce a novel approach that reframes the token classification task as a <b>text</b> <b>generation</b> problem, enabling <b>fine-tuning</b> and evaluation of <b>T5</b> models. Following the <b>fine-tuning</b> of the models, we conduct an evaluation on the test dataset, employing performance and error metrics. Our findings reveal that <b>BERT-based</b> models consistently outperform <b>T5-based</b> models. Furthermore, while the multilingual models exhibit comparable macro F1-scores, BERTimbau demonstrates superior performance over PTT5. A manual analysis of sentences generated by PTT5 and mT5 unveils a degree of similarity ranging from 0.89 to 1.0, between the original and generated sentences. However, critical errors emerge as both models exhibit discrepancies, such as alterations to monetary and percentage values, underscoring the importance of accuracy and consistency in the financial domain. Despite these challenges, PTT5 and mT5 achieve impressive macro F1-scores of 98.52% and 98.85%, respectively, with our proposed approach. Furthermore, our study sheds light on notable disparities in memory and time consumption for inference across the models.</p></p class="citation"></blockquote><h3 id=643--6350-tnt-llm-text-mining-at-scale-with-large-language-models-mengting-wan-et-al-2024>(6/43 | 6/350) TnT-LLM: Text Mining at Scale with Large Language Models (Mengting Wan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mengting Wan, Tara Safavi, Sujay Kumar Jauhar, Yujin Kim, Scott Counts, Jennifer Neville, Siddharth Suri, Chirag Shah, Ryen W White, Longqi Yang, Reid Andersen, Georg Buscher, Dhruv Joshi, Nagu Rangan. (2024)<br><strong>TnT-LLM: Text Mining at Scale with Large Language Models</strong><br><button class=copy-to-clipboard title="TnT-LLM: Text Mining at Scale with Large Language Models" index=6>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-6 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-IR, cs.CL<br>Keyword Score: 80<br>Keywords: Automatic Evaluation, Supervised Learning, Zero-shot, Reasoning, Text Mining, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12173v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12173v1.pdf filename=2403.12173v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Transforming unstructured <b>text</b> <b>into</b> structured and meaningful forms, organized by useful category labels, is a fundamental step in <b>text</b> <b>mining</b> for downstream analysis and application. However, most existing methods for producing label taxonomies and building <b>text-based</b> <b>label</b> classifiers still rely heavily on domain expertise and manual curation, making the process expensive and time-consuming. This is particularly challenging when the label space is under-specified and <b>large-scale</b> <b>data</b> <b>annotations</b> are unavailable. In this paper, we address these challenges with <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> whose <b>prompt-based</b> interface facilitates the induction and use of <b>large-scale</b> <b>pseudo</b> <b>labels.</b> We propose TnT-LLM, a two-phase framework that employs <b>LLMs</b> to automate the process of end-to-end label generation and assignment with minimal human effort for any given use-case. In the first phase, we introduce a <b>zero-shot,</b> multi-stage <b>reasoning</b> approach which enables <b>LLMs</b> to produce and refine a label taxonomy iteratively. In the second phase, <b>LLMs</b> are used as data labelers that yield training samples so that lightweight <b>supervised</b> classifiers can be reliably built, deployed, and served at scale. We apply TnT-LLM to the analysis of user intent and conversational domain for Bing Copilot (formerly Bing Chat), an open-domain chat-based search engine. Extensive experiments using both human and <b>automatic</b> <b>evaluation</b> metrics demonstrate that TnT-LLM generates more accurate and relevant label taxonomies when compared against state-of-the-art baselines, and achieves a favorable balance between accuracy and efficiency for classification at scale. We also share our practical experiences and insights on the challenges and opportunities of using <b>LLMs</b> for <b>large-scale</b> <b>text</b> <b>mining</b> in real-world applications.</p></p class="citation"></blockquote><h3 id=743--7350-a-novel-paradigm-boosting-translation-capabilities-of-large-language-models-jiaxin-guo-et-al-2024>(7/43 | 7/350) A Novel Paradigm Boosting Translation Capabilities of Large Language Models (Jiaxin Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiaxin Guo, Hao Yang, Zongyao Li, Daimeng Wei, Hengchao Shang, Xiaoyu Chen. (2024)<br><strong>A Novel Paradigm Boosting Translation Capabilities of Large Language Models</strong><br><button class=copy-to-clipboard title="A Novel Paradigm Boosting Translation Capabilities of Large Language Models" index=7>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-7 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 80<br>Keywords: Fine-tuning, Supervised Learning, GPT-3, GPT-3.5, Neural Machine Translation, Neural Machine Translation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11430v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11430v1.pdf filename=2403.11430v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a study on strategies to enhance the translation capabilities of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> in the context of <b>machine</b> <b>translation</b> <b>(MT)</b> tasks. The paper proposes a novel paradigm consisting of three stages: Secondary Pre-training using Extensive Monolingual Data, Continual Pre-training with Interlinear Text Format Documents, and Leveraging Source-Language Consistent Instruction for <b>Supervised</b> <b>Fine-Tuning.</b> Previous research on <b>LLMs</b> focused on various strategies for <b>supervised</b> <b>fine-tuning</b> (SFT), but their effectiveness has been limited. While traditional <b>machine</b> <b>translation</b> approaches rely on vast amounts of parallel bilingual data, our paradigm highlights the importance of using smaller sets of high-quality bilingual data. We argue that the focus should be on augmenting <b>LLMs&rsquo;</b> cross-lingual alignment abilities during pre-training rather than solely relying on extensive bilingual data during SFT. Experimental results conducted using the Llama2 model, particularly on Chinese-Llama2 after monolingual augmentation, demonstrate the improved translation capabilities of <b>LLMs.</b> A significant contribution of our approach lies in Stage2: Continual Pre-training with Interlinear Text Format Documents, which requires less than 1B training data, making our method highly efficient. Additionally, in Stage3, we observed that setting instructions consistent with the source language benefits the <b>supervised</b> <b>fine-tuning</b> process. Experimental results demonstrate that our approach surpasses previous work and achieves superior performance compared to models such as NLLB-54B and <b>GPT3.5-text-davinci-003,</b> despite having a significantly smaller parameter count of only 7B or 13B. This achievement establishes our method as a pioneering strategy in the field of <b>machine</b> <b>translation.</b></p></p class="citation"></blockquote><h3 id=843--8350-construction-of-hyper-relational-knowledge-graphs-using-pre-trained-large-language-models-preetha-datta-et-al-2024>(8/43 | 8/350) Construction of Hyper-Relational Knowledge Graphs Using Pre-Trained Large Language Models (Preetha Datta et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Preetha Datta, Fedor Vitiugin, Anastasiia Chizhikova, Nitin Sawhney. (2024)<br><strong>Construction of Hyper-Relational Knowledge Graphs Using Pre-Trained Large Language Models</strong><br><button class=copy-to-clipboard title="Construction of Hyper-Relational Knowledge Graphs Using Pre-Trained Large Language Models" index=8>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-8 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 78<br>Keywords: Graph, Knowledge Graph, Supervised Learning, Zero-shot, GPT, GPT-3, GPT-3.5, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11786v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11786v1.pdf filename=2403.11786v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Extracting hyper-relations is crucial for constructing comprehensive <b>knowledge</b> <b>graphs,</b> but there are limited <b>supervised</b> methods available for this task. To address this gap, we introduce a <b>zero-shot</b> <b>prompt-based</b> method using OpenAI&rsquo;s <b>GPT-3.5</b> model for extracting hyper-relational <b>knowledge</b> <b>from</b> text. Comparing our model with a baseline, we achieved promising results, with a recall of 0.77. Although our precision is currently lower, a detailed analysis of the model outputs has uncovered potential pathways for future research in this area.</p></p class="citation"></blockquote><h3 id=943--9350-hatecot-an-explanation-enhanced-dataset-for-generalizable-offensive-speech-detection-via-large-language-models-huy-nghiem-et-al-2024>(9/43 | 9/350) HateCOT: An Explanation-Enhanced Dataset for Generalizable Offensive Speech Detection via Large Language Models (Huy Nghiem et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Huy Nghiem, Hal Daumé III. (2024)<br><strong>HateCOT: An Explanation-Enhanced Dataset for Generalizable Offensive Speech Detection via Large Language Models</strong><br><button class=copy-to-clipboard title="HateCOT: An Explanation-Enhanced Dataset for Generalizable Offensive Speech Detection via Large Language Models" index=9>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-9 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-SI, cs.CL<br>Keyword Score: 73<br>Keywords: Benchmarking, Few-shot, Fine-tuning, Low-Resource, GPT, GPT-3, GPT-3.5, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11456v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11456v1.pdf filename=2403.11456v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The ubiquitousness of social media has led to the need for reliable and efficient detection of offensive content to limit harmful effects. This has led to a proliferation of datasets and models related to detecting offensive content. While sophisticated models have attained strong performance on individual datasets, these models often do not generalize due to differences between how &ldquo;offensive content&rdquo; is conceptualized, and the resulting differences in how these datasets are labeled. In this paper, we introduce HateCOT, a dataset of 52,000 samples drawn from diverse existing sources with explanations generated by <b>GPT-3.5-Turbo</b> and human-curated. We show that pre-training models for the detection of offensive content on HateCOT significantly boots open-sourced Language Models on three <b>benchmark</b> datasets in both zero and <b>few-shot</b> settings, despite differences in domain and task.} We further find that HateCOT enables effective K-shot <b>fine-tuning</b> in the <b>low-resource</b> settings.</p></p class="citation"></blockquote><h3 id=1043--10350-easyjailbreak-a-unified-framework-for-jailbreaking-large-language-models-weikang-zhou-et-al-2024>(10/43 | 10/350) EasyJailbreak: A Unified Framework for Jailbreaking Large Language Models (Weikang Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Weikang Zhou, Xiao Wang, Limao Xiong, Han Xia, Yingshuang Gu, Mingxu Chai, Fukang Zhu, Caishuang Huang, Shihan Dou, Zhiheng Xi, Rui Zheng, Songyang Gao, Yicheng Zou, Hang Yan, Yifan Le, Ruohui Wang, Lijun Li, Jing Shao, Tao Gui, Qi Zhang, Xuanjing Huang. (2024)<br><strong>EasyJailbreak: A Unified Framework for Jailbreaking Large Language Models</strong><br><button class=copy-to-clipboard title="EasyJailbreak: A Unified Framework for Jailbreaking Large Language Models" index=10>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-10 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: GPT, GPT-3, GPT-3.5, GPT-4, Automatic Speech Recognition, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12171v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12171v1.pdf filename=2403.12171v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Jailbreak attacks are crucial for identifying and mitigating the security vulnerabilities of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> They are designed to bypass safeguards and elicit prohibited outputs. However, due to significant differences among various jailbreak methods, there is no standard implementation framework available for the community, which limits comprehensive security evaluations. This paper introduces EasyJailbreak, a unified framework simplifying the construction and evaluation of jailbreak attacks against <b>LLMs.</b> It builds jailbreak attacks using four components: Selector, Mutator, Constraint, and Evaluator. This modular framework enables researchers to easily construct attacks from combinations of novel and existing components. So far, EasyJailbreak supports 11 distinct jailbreak methods and facilitates the security validation of a broad spectrum of <b>LLMs.</b> Our validation across 10 distinct <b>LLMs</b> reveals a significant vulnerability, with an average breach probability of 60% under various jailbreaking attacks. Notably, even advanced models like <b>GPT-3.5-Turbo</b> and <b>GPT-4</b> exhibit average Attack Success Rates <b>(ASR)</b> of 57% and 33%, respectively. We have released a wealth of resources for researchers, including a web platform, PyPI published package, screencast video, and experimental outputs.</p></p class="citation"></blockquote><h3 id=1143--11350-enhancing-hokkien-dual-translation-by-exploring-and-standardizing-of-four-writing-systems-bo-han-lu-et-al-2024>(11/43 | 11/350) Enhancing Hokkien Dual Translation by Exploring and Standardizing of Four Writing Systems (Bo-Han Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bo-Han Lu, Yi-Hsuan Lin, En-Shiun Annie Lee, Richard Tzong-Han Tsai. (2024)<br><strong>Enhancing Hokkien Dual Translation by Exploring and Standardizing of Four Writing Systems</strong><br><button class=copy-to-clipboard title="Enhancing Hokkien Dual Translation by Exploring and Standardizing of Four Writing Systems" index=11>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-11 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: Fine-tuning, High-Resource, Low-Resource, GPT, GPT-4, LLaMA, Neural Machine Translation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12024v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12024v1.pdf filename=2403.12024v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Machine</b> <b>translation</b> focuses mainly on <b>high-resource</b> languages (HRLs), while <b>low-resource</b> languages (LRLs) like Taiwanese Hokkien are relatively under-explored. This study aims to address this gap by developing a dual translation model between Taiwanese Hokkien and both Traditional Mandarin Chinese and English. We employ a pre-trained LLaMA2-7B model specialized in Traditional Mandarin Chinese to leverage the orthographic similarities between Taiwanese Hokkien Han and Traditional Mandarin Chinese. Our comprehensive experiments involve translation tasks across various writing systems of Taiwanese Hokkien and between Taiwanese Hokkien and other HRLs. We find that the use of a limited monolingual corpus also further improve the model&rsquo;s Taiwanese Hokkien capabilities. We then utilize our translation model to standardize all Taiwanese Hokkien writing systems into Hokkien Han, resulting in further performance improvements. Additionally, we introduce an evaluation method incorporating back-translation and <b>GPT-4</b> to ensure reliable translation quality assessment even for LRLs. The study contributes to narrowing the resource gap for Taiwanese Hokkien and empirically investigates the advantages and limitations of pre-training and <b>fine-tuning</b> based on <b>LLaMA</b> 2.</p></p class="citation"></blockquote><h3 id=1243--12350-envgen-generating-and-adapting-environments-via-llms-for-training-embodied-agents-abhay-zala-et-al-2024>(12/43 | 12/350) EnvGen: Generating and Adapting Environments via LLMs for Training Embodied Agents (Abhay Zala et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Abhay Zala, Jaemin Cho, Han Lin, Jaehong Yoon, Mohit Bansal. (2024)<br><strong>EnvGen: Generating and Adapting Environments via LLMs for Training Embodied Agents</strong><br><button class=copy-to-clipboard title="EnvGen: Generating and Adapting Environments via LLMs for Training Embodied Agents" index=12>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-12 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 70<br>Keywords: Reinforcement Learning, GPT, GPT-4, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12014v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12014v1.pdf filename=2403.12014v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent SOTA approaches for embodied learning via interaction directly employ <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> as agents to determine the next steps in an environment. Due to their world knowledge and <b>reasoning</b> capabilities, <b>LLM</b> agents achieve stronger performance than previous smaller agents based on <b>reinforcement</b> <b>learning</b> (RL); however, frequently calling <b>LLMs</b> is slow and expensive. Instead of directly employing <b>LLMs</b> as agents, can we use <b>LLMs&rsquo;</b> <b>reasoning</b> capabilities to adaptively create training environments to help smaller embodied RL agents learn useful skills that they are weak at? We propose EnvGen, a novel framework to address this question. First, we <b>prompt</b> an <b>LLM</b> to generate training environments that allow agents to quickly learn different tasks in parallel. Concretely, the <b>LLM</b> is given the task description and simulator objectives that the agents should learn and is then asked to generate a set of environment configurations (e.g., different terrains, items given to agents, etc.). Next, we train a small RL agent in a mixture of the original and <b>LLM-generated</b> environments. Then, we enable the <b>LLM</b> to continuously adapt the generated environments to progressively improve the skills that the agent is weak at, by providing feedback to the <b>LLM</b> in the form of the agent&rsquo;s performance. We demonstrate the usefulness of EnvGen with comprehensive experiments in Crafter and Heist environments. We find that a small RL agent trained with EnvGen can outperform SOTA methods, including a <b>GPT-4</b> agent, and learns long-horizon tasks significantly faster. We show qualitatively how the <b>LLM</b> adapts training environments to help improve RL agents&rsquo; weaker skills over time. Additionally, EnvGen is substantially more efficient as it only uses a small number of <b>LLM</b> calls (e.g., 4 in total), whereas <b>LLM</b> agents require thousands of <b>LLM</b> calls. Lastly, we present detailed ablation studies for our design choices.</p></p class="citation"></blockquote><h3 id=1343--13350-gpt-4-as-evaluator-evaluating-large-language-models-on-pest-management-in-agriculture-shanglong-yang-et-al-2024>(13/43 | 13/350) GPT-4 as Evaluator: Evaluating Large Language Models on Pest Management in Agriculture (Shanglong Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shanglong Yang, Zhipeng Yuan, Shunbao Li, Ruoling Peng, Kang Liu, Po Yang. (2024)<br><strong>GPT-4 as Evaluator: Evaluating Large Language Models on Pest Management in Agriculture</strong><br><button class=copy-to-clipboard title="GPT-4 as Evaluator: Evaluating Large Language Models on Pest Management in Agriculture" index=13>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-13 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: GPT, GPT-3, GPT-4, Transformer, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11858v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11858v1.pdf filename=2403.11858v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the rapidly evolving field of artificial intelligence (AI), the application of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> in agriculture, particularly in pest management, remains nascent. We aimed to prove the feasibility by evaluating the content of the pest management advice generated by <b>LLMs,</b> including the Generative Pre-trained <b>Transformer</b> <b>(GPT)</b> series from OpenAI and the FLAN series from Google. Considering the context-specific properties of agricultural advice, automatically measuring or quantifying the quality of text generated by <b>LLMs</b> becomes a significant challenge. We proposed an innovative approach, using <b>GPT-4</b> as an evaluator, to score the generated content on Coherence, Logical Consistency, Fluency, Relevance, Comprehensibility, and Exhaustiveness. Additionally, we integrated an expert system based on crop threshold data as a baseline to obtain scores for Factual Accuracy on whether pests found in crop fields should take management action. Each model&rsquo;s score was weighted by percentage to obtain a final score. The results showed that <b>GPT-3.4</b> and <b>GPT-4</b> outperform the FLAN models in most evaluation categories. Furthermore, the use of instruction-based <b>prompting</b> containing domain-specific knowledge proved the feasibility of <b>LLMs</b> as an effective tool in agriculture, with an accuracy rate of 72%, demonstrating <b>LLMs&rsquo;</b> effectiveness in providing pest management suggestions.</p></p class="citation"></blockquote><h3 id=1443--14350-metaphor-understanding-challenge-dataset-for-llms-xiaoyu-tong-et-al-2024>(14/43 | 14/350) Metaphor Understanding Challenge Dataset for LLMs (Xiaoyu Tong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaoyu Tong, Rochelle Choenni, Martha Lewis, Ekaterina Shutova. (2024)<br><strong>Metaphor Understanding Challenge Dataset for LLMs</strong><br><button class=copy-to-clipboard title="Metaphor Understanding Challenge Dataset for LLMs" index=14>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-14 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: GPT, GPT-3, GPT-3.5, LLaMA, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11810v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11810v1.pdf filename=2403.11810v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Metaphors in natural language are a reflection of fundamental cognitive processes such as analogical <b>reasoning</b> and categorisation, and are deeply rooted in everyday communication. Metaphor understanding is therefore an essential task for <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> We release the Metaphor Understanding Challenge Dataset (MUNCH), designed to evaluate the metaphor understanding capabilities of <b>LLMs.</b> The dataset provides over 10k paraphrases for sentences containing metaphor use, as well as 1.5k instances containing inapt paraphrases. The inapt paraphrases were carefully selected to serve as control to determine whether the model indeed performs full metaphor interpretation or rather resorts to lexical similarity. All apt and inapt paraphrases were manually annotated. The metaphorical sentences cover natural metaphor uses across 4 genres (academic, news, fiction, and conversation), and they exhibit different levels of novelty. Experiments with <b>LLaMA</b> and <b>GPT-3.5</b> demonstrate that MUNCH presents a challenging task for <b>LLMs.</b> The dataset is freely accessible at <a href=https://github.com/xiaoyuisrain/metaphor-understanding-challenge>https://github.com/xiaoyuisrain/metaphor-understanding-challenge</a>.</p></p class="citation"></blockquote><h3 id=1543--15350-counting-stars-a-simple-efficient-and-reasonable-strategy-for-evaluating-long-context-large-language-models-mingyang-song-et-al-2024>(15/43 | 15/350) Counting-Stars: A Simple, Efficient, and Reasonable Strategy for Evaluating Long-Context Large Language Models (Mingyang Song et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mingyang Song, Mao Zheng, Xuan Luo. (2024)<br><strong>Counting-Stars: A Simple, Efficient, and Reasonable Strategy for Evaluating Long-Context Large Language Models</strong><br><button class=copy-to-clipboard title="Counting-Stars: A Simple, Efficient, and Reasonable Strategy for Evaluating Long-Context Large Language Models" index=15>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-15 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 63<br>Keywords: Benchmarking, ChatGPT, GPT, GPT-4, GPT-4 turbo, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11802v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11802v1.pdf filename=2403.11802v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While recent research endeavors have concentrated on developing <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> with robust long-context capabilities, due to the lack of appropriate evaluation strategies, relatively little is known about how well the long-context processing abilities and performance of leading <b>LLMs</b> (e.g., <b>ChatGPT</b> and KimiChat). To address this gap, we propose a simple, efficient, and reasonable strategy for evaluating long-context <b>LLMs</b> as a new <b>benchmark,</b> named Counting-Stars. The Counting-Stars is designed to require <b>LLMs</b> to fully understand and capture long dependencies in long contexts and be able to collect inter-dependency across multiple pieces of evidence spanning the entire context to finish the task. Based on the Counting-Stars, we conduct experiments to evaluate the two leading long-context <b>LLMs,</b> i.e., <b>GPT-4</b> <b>Turbo</b> and Kimi Chat. The experimental results indicate that <b>GPT-4</b> <b>Turbo</b> and Kimi Chat achieve significant performance in the long context from 4K to 128K. We further present two intriguing analyses regarding the behavior of <b>LLMs</b> processing long context.</p></p class="citation"></blockquote><h3 id=1643--16350-co3-low-resource-contrastive-co-training-for-generative-conversational-query-rewrite-yifei-yuan-et-al-2024>(16/43 | 16/350) CO3: Low-resource Contrastive Co-training for Generative Conversational Query Rewrite (Yifei Yuan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yifei Yuan, Chen Shi, Runze Wang, Liyi Chen, Renjun Hu, Zengming Zhang, Feijun Jiang, Wai Lam. (2024)<br><strong>CO3: Low-resource Contrastive Co-training for Generative Conversational Query Rewrite</strong><br><button class=copy-to-clipboard title="CO3: Low-resource Contrastive Co-training for Generative Conversational Query Rewrite" index=16>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-16 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Contrastive Learning, Data Augmentation, Few-shot, Few-shot Learning, Low-Resource, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11873v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11873v1.pdf filename=2403.11873v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Generative query rewrite generates reconstructed query rewrites using the conversation history while rely heavily on gold rewrite pairs that are expensive to obtain. Recently, <b>few-shot</b> <b>learning</b> is gaining increasing popularity for this task, whereas these methods are sensitive to the inherent noise due to limited <b>data</b> <b>size.</b> Besides, both attempts face performance degradation when there exists language style shift between training and testing cases. To this end, we study <b>low-resource</b> generative conversational query rewrite that is robust to both noise and language style shift. The core idea is to utilize massive unlabeled <b>data</b> <b>to</b> make further improvements via a <b>contrastive</b> <b>co-training</b> paradigm. Specifically, we co-train two dual models (namely Rewriter and Simplifier) such that each of them provides extra guidance through pseudo-labeling for enhancing the other in an iterative manner. We also leverage <b>contrastive</b> <b>learning</b> with <b>data</b> <b>augmentation,</b> which enables our model pay more attention on the truly valuable information than the noise. Extensive experiments demonstrate the superiority of our model under both <b>few-shot</b> <b>and</b> <b>zero-shot</b> scenarios. We also verify the better generalization ability of our model when encountering language style shift.</p></p class="citation"></blockquote><h3 id=1743--17350-towards-understanding-the-relationship-between-in-context-learning-and-compositional-generalization-sungjun-han-et-al-2024>(17/43 | 17/350) Towards Understanding the Relationship between In-context Learning and Compositional Generalization (Sungjun Han et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sungjun Han, Sebastian Padó. (2024)<br><strong>Towards Understanding the Relationship between In-context Learning and Compositional Generalization</strong><br><button class=copy-to-clipboard title="Towards Understanding the Relationship between In-context Learning and Compositional Generalization" index=17>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-17 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 60<br>Keywords: Few-shot, Few-shot Learning, Out-of-distribution, Transformer, In-context Learning, In-context Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11834v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11834v1.pdf filename=2403.11834v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>According to the principle of compositional generalization, the meaning of a complex expression can be understood as a function of the meaning of its parts and of how they are combined. This principle is crucial for human language processing and also, arguably, for NLP models in the face of <b>out-of-distribution</b> data. However, many neural network models, including <b>Transformers,</b> have been shown to struggle with compositional generalization. In this paper, we hypothesize that forcing models to <b>in-context</b> <b>learn</b> can provide an inductive bias to promote compositional generalization. To test this hypothesis, we train a causal <b>Transformer</b> in a setting that renders ordinary learning very difficult: we present it with different orderings of the training instance and shuffle instance labels. This corresponds to training the model on all possible <b>few-shot</b> <b>learning</b> problems attainable from the dataset. The model can solve the task, however, by utilizing earlier examples to generalize to later ones (i.e. <b>in-context</b> <b>learning).</b> In evaluations on the datasets, SCAN, COGS, and GeoQuery, models trained in this manner indeed show improved compositional generalization. This indicates the usefulness of <b>in-context</b> <b>learning</b> problems as an inductive bias for generalization.</p></p class="citation"></blockquote><h3 id=1843--18350-lets-focus-on-neuron-neuron-level-supervised-fine-tuning-for-large-language-model-haoyun-xu-et-al-2024>(18/43 | 18/350) Let&rsquo;s Focus on Neuron: Neuron-Level Supervised Fine-tuning for Large Language Model (Haoyun Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haoyun Xu, Runzhe Zhan, Derek F. Wong, Lidia S. Chao. (2024)<br><strong>Let&rsquo;s Focus on Neuron: Neuron-Level Supervised Fine-tuning for Large Language Model</strong><br><button class=copy-to-clipboard title="Let's Focus on Neuron: Neuron-Level Supervised Fine-tuning for Large Language Model" index=18>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-18 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Fine-tuning, Model Pruning, Pruning, Supervised Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11621v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11621v1.pdf filename=2403.11621v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> are composed of neurons that exhibit various behaviors and roles, which become increasingly diversified as <b>models</b> <b>scale.</b> Recent studies have revealed that not all neurons are active across different datasets, and this sparsity correlates positively with the task-specific ability, leading to advancements in <b>model</b> <b>pruning</b> and training efficiency. Traditional <b>fine-tuning</b> methods engage all parameters of <b>LLMs,</b> which is computationally expensive and may not be necessary. In contrast, Parameter-Efficient <b>Fine-Tuning</b> (PEFT) approaches aim to minimize the number of trainable parameters, yet they still operate at a relatively macro scale (e.g., layer-level). We introduce Neuron-Level <b>Fine-Tuning</b> (NeFT), a novel approach that refines the granularity of parameter training down to the individual neuron, enabling more precise and computationally efficient <b>model</b> <b>updates.</b> The experimental results show that NeFT not only exceeded the performance of full-parameter <b>fine-tuning</b> and PEFT but also provided insights into the analysis of neurons.</p></p class="citation"></blockquote><h3 id=1943--19350-openeval-benchmarking-chinese-llms-across-capability-alignment-and-safety-chuang-liu-et-al-2024>(19/43 | 19/350) OpenEval: Benchmarking Chinese LLMs across Capability, Alignment and Safety (Chuang Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chuang Liu, Linhao Yu, Jiaxuan Li, Renren Jin, Yufei Huang, Ling Shi, Junhui Zhang, Xinmeng Ji, Tingting Cui, Tao Liu, Jinwang Song, Hongying Zan, Sun Li, Deyi Xiong. (2024)<br><strong>OpenEval: Benchmarking Chinese LLMs across Capability, Alignment and Safety</strong><br><button class=copy-to-clipboard title="OpenEval: Benchmarking Chinese LLMs across Capability, Alignment and Safety" index=19>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-19 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 56<br>Keywords: Benchmarking, Benchmarking, Common-sense Reasoning, Mathematical Reasoning, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12316v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12316v1.pdf filename=2403.12316v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rapid development of Chinese <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> poses big challenges for efficient <b>LLM</b> evaluation. While current initiatives have introduced new <b>benchmarks</b> or evaluation platforms for assessing Chinese <b>LLMs,</b> many of these focus primarily on capabilities, usually overlooking potential alignment and safety issues. To address this gap, we introduce OpenEval, an evaluation testbed that <b>benchmarks</b> Chinese <b>LLMs</b> across capability, alignment and safety. For capability assessment, we include 12 <b>benchmark</b> datasets to evaluate Chinese <b>LLMs</b> from 4 sub-dimensions: NLP tasks, disciplinary knowledge, <b>commonsense</b> <b>reasoning</b> and <b>mathematical</b> <b>reasoning.</b> For alignment assessment, OpenEval contains 7 datasets that examines the bias, offensiveness and illegalness in the outputs yielded by Chinese <b>LLMs.</b> To evaluate safety, especially anticipated risks (e.g., power-seeking, self-awareness) of advanced <b>LLMs,</b> we include 6 datasets. In addition to these <b>benchmarks,</b> we have implemented a phased public evaluation and <b>benchmark</b> update strategy to ensure that OpenEval is in line with the development of Chinese <b>LLMs</b> or even able to provide cutting-edge <b>benchmark</b> datasets to guide the development of Chinese <b>LLMs.</b> In our first public evaluation, we have tested a range of Chinese <b>LLMs,</b> spanning from 7B to 72B parameters, including both open-source and proprietary models. Evaluation results indicate that while Chinese <b>LLMs</b> have shown impressive performance in certain tasks, more attention should be directed towards broader aspects such as <b>commonsense</b> <b>reasoning,</b> alignment, and safety.</p></p class="citation"></blockquote><h3 id=2043--20350-leveraging-large-language-models-to-extract-information-on-substance-use-disorder-severity-from-clinical-notes-a-zero-shot-learning-approach-maria-mahbub-et-al-2024>(20/43 | 20/350) Leveraging Large Language Models to Extract Information on Substance Use Disorder Severity from Clinical Notes: A Zero-shot Learning Approach (Maria Mahbub et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Maria Mahbub, Gregory M. Dams, Sudarshan Srinivasan, Caitlin Rizy, Ioana Danciu, Jodie Trafton, Kathryn Knight. (2024)<br><strong>Leveraging Large Language Models to Extract Information on Substance Use Disorder Severity from Clinical Notes: A Zero-shot Learning Approach</strong><br><button class=copy-to-clipboard title="Leveraging Large Language Models to Extract Information on Substance Use Disorder Severity from Clinical Notes: A Zero-shot Learning Approach" index=20>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-20 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Zero-shot, Large Language Model, Large Language Model, Prompt, Zero-shot Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12297v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12297v1.pdf filename=2403.12297v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Substance use disorder (SUD) poses a major concern due to its detrimental effects on health and society. SUD identification and treatment depend on a variety of factors such as severity, co-determinants (e.g., withdrawal symptoms), and social determinants of health. Existing diagnostic coding systems used by American insurance providers, like the International Classification of Diseases (ICD-10), lack granularity for certain diagnoses, but clinicians will add this granularity (as that found within the Diagnostic and Statistical Manual of Mental Disorders classification or DSM-5) as supplemental unstructured text in clinical notes. Traditional natural language processing (NLP) methods face limitations in accurately parsing such diverse clinical language. <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> offer promise in overcoming these challenges by adapting to diverse language patterns. This study investigates the application of <b>LLMs</b> for extracting severity-related information for various SUD diagnoses from clinical notes. We propose a workflow employing <b>zero-shot</b> <b>learning</b> of <b>LLMs</b> with carefully crafted <b>prompts</b> and post-processing techniques. Through experimentation with Flan-T5, an open-source <b>LLM,</b> we demonstrate its superior recall compared to the rule-based approach. Focusing on 11 categories of SUD diagnoses, we show the effectiveness of <b>LLMs</b> in extracting severity information, contributing to improved risk assessment and treatment planning for SUD patients.</p></p class="citation"></blockquote><h3 id=2143--21350-queryagent-a-reliable-and-efficient-reasoning-framework-with-environmental-feedback-based-self-correction-xiang-huang-et-al-2024>(21/43 | 21/350) QueryAgent: A Reliable and Efficient Reasoning Framework with Environmental Feedback based Self-Correction (Xiang Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiang Huang, Sitao Cheng, Shanshan Huang, Jiayu Shen, Yong Xu, Chaoyun Zhang, Yuzhong Qu. (2024)<br><strong>QueryAgent: A Reliable and Efficient Reasoning Framework with Environmental Feedback based Self-Correction</strong><br><button class=copy-to-clipboard title="QueryAgent: A Reliable and Efficient Reasoning Framework with Environmental Feedback based Self-Correction" index=21>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-21 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Few-shot, Reasoning, Semantic Parsing, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11886v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11886v1.pdf filename=2403.11886v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Employing <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> for <b>semantic</b> <b>parsing</b> has achieved remarkable success. However, we find existing methods fall short in terms of reliability and efficiency when hallucinations are encountered. In this paper, we address these challenges with a framework called QueryAgent, which solves a question step-by-step and performs step-wise self-correction. We introduce an environmental feedback-based self-correction method called ERASER. Unlike traditional approaches, ERASER leverages rich environmental feedback in the intermediate steps to perform selective and differentiated self-correction only when necessary. Experimental results demonstrate that QueryAgent notably outperforms all previous <b>few-shot</b> methods using only one example on GrailQA and GraphQ by 7.0 and 15.0 F1. Moreover, our approach exhibits superiority in terms of efficiency, including runtime, query overhead, and API invocation costs. By leveraging ERASER, we further improve another baseline (i.e., AgentBench) by approximately 10 points, revealing the strong transferability of our approach.</p></p class="citation"></blockquote><h3 id=2243--22350-reinforcement-learning-with-token-level-feedback-for-controllable-text-generation-wendi-li-et-al-2024>(22/43 | 22/350) Reinforcement Learning with Token-level Feedback for Controllable Text Generation (Wendi Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wendi Li, Wei Wei, Kaihe Xu, Wenfeng Xie, Dangyang Chen, Yu Cheng. (2024)<br><strong>Reinforcement Learning with Token-level Feedback for Controllable Text Generation</strong><br><button class=copy-to-clipboard title="Reinforcement Learning with Token-level Feedback for Controllable Text Generation" index=22>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-22 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Fine-tuning, Reinforcement Learning, Text Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11558v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11558v1.pdf filename=2403.11558v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To meet the requirements of real-world applications, it is essential to control generations of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> Prior research has tried to introduce <b>reinforcement</b> <b>learning</b> (RL) into controllable <b>text</b> <b>generation</b> while most existing methods suffer from overfitting issues <b>(finetuning-based</b> methods) or semantic collapse (post-processing methods). However, current RL methods are generally guided by coarse-grained (sentence/paragraph-level) feedback, which may lead to suboptimal performance owing to semantic twists or progressions within sentences. To tackle that, we propose a novel <b>reinforcement</b> <b>learning</b> algorithm named TOLE which formulates TOken-LEvel rewards for controllable <b>text</b> <b>generation,</b> and employs a &ldquo;first-quantize-then-noise&rdquo; paradigm to enhance the robustness of the RL algorithm.Furthermore, TOLE can be flexibly extended to multiple constraints with little computational expense. Experimental results show that our algorithm can achieve superior performance on both single-attribute and multi-attribute control tasks. We have released our codes at <a href=https://github.com/WindyLee0822/CTG>https://github.com/WindyLee0822/CTG</a></p></p class="citation"></blockquote><h3 id=2343--23350-dee-dual-stage-explainable-evaluation-method-for-text-generation-shenyu-zhang-et-al-2024>(23/43 | 23/350) DEE: Dual-stage Explainable Evaluation Method for Text Generation (Shenyu Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shenyu Zhang, Yu Li, Rui Wu, Xiutian Huang, Yongrui Chen, Wenhao Xu, Guilin Qi. (2024)<br><strong>DEE: Dual-stage Explainable Evaluation Method for Text Generation</strong><br><button class=copy-to-clipboard title="DEE: Dual-stage Explainable Evaluation Method for Text Generation" index=23>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-23 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Fine-tuning, LLaMA, Text Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11509v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11509v1.pdf filename=2403.11509v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Automatic methods for evaluating machine-generated <b>texts</b> <b>hold</b> significant importance due to the expanding applications of generative systems. Conventional methods tend to grapple with a lack of explainability, issuing a solitary numerical score to signify the assessment outcome. Recent advancements have sought to mitigate this limitation by incorporating <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to offer more detailed error analyses, yet their applicability remains constrained, particularly in industrial contexts where comprehensive error coverage and swift detection are paramount. To alleviate these challenges, we introduce DEE, a Dual-stage Explainable Evaluation method for estimating the quality of <b>text</b> <b>generation.</b> Built upon <b>Llama</b> 2, DEE follows a dual-stage principle guided by stage-specific instructions to perform efficient identification of errors in generated <b>texts</b> <b>in</b> the initial stage and subsequently delves into providing comprehensive diagnostic reports in the second stage. DEE is <b>fine-tuned</b> on our elaborately assembled dataset AntEval, which encompasses 15K examples from 4 real-world applications of Alipay that employ generative systems. The dataset concerns newly emerged issues like hallucination and toxicity, thereby broadening the scope of DEE&rsquo;s evaluation criteria. Experimental results affirm that DEE&rsquo;s superiority over existing evaluation methods, achieving significant improvements in both human correlation as well as efficiency.</p></p class="citation"></blockquote><h3 id=2443--24350-inscl-a-data-efficient-continual-learning-paradigm-for-fine-tuning-large-language-models-with-instructions-yifan-wang-et-al-2024>(24/43 | 24/350) InsCL: A Data-efficient Continual Learning Paradigm for Fine-tuning Large Language Models with Instructions (Yifan Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yifan Wang, Yafei Liu, Chufan Shi, Haoling Li, Chen Chen, Haonan Lu, Yujiu Yang. (2024)<br><strong>InsCL: A Data-efficient Continual Learning Paradigm for Fine-tuning Large Language Models with Instructions</strong><br><button class=copy-to-clipboard title="InsCL: A Data-efficient Continual Learning Paradigm for Fine-tuning Large Language Models with Instructions" index=24>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-24 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Continual Learning, Fine-tuning, Instruction Tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11435v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11435v1.pdf filename=2403.11435v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Instruction</b> <b>tuning</b> effectively optimizes <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> for downstream tasks. Due to the changing environment in real-life applications, <b>LLMs</b> necessitate <b>continual</b> <b>task-specific</b> adaptation without catastrophic forgetting. Considering the heavy computational cost, replay-based <b>Continual</b> <b>Learning</b> (CL) methods are the simplest and most widely used for <b>LLMs</b> to address the forgetting issue. However, traditional replay-based methods do not fully utilize <b>instructions</b> <b>to</b> customize the replay strategy. In this work, we propose a novel paradigm called <b>Instruction-based</b> <b>Continual</b> <b>Learning</b> (InsCL). InsCL dynamically replays previous data based on task similarity, calculated by Wasserstein Distance with <b>instructions.</b> <b>Moreover,</b> we further introduce an <b>Instruction</b> <b>Information</b> Metric (InsInfo) to quantify the complexity and diversity of <b>instructions.</b> <b>According</b> to InsInfo, InsCL guides the replay process more inclined to high-quality data. We conduct extensive experiments over 16 tasks with different training orders, observing consistent performance improvements of InsCL. When all tasks have been trained, InsCL achieves performance gains of 3.0 Relative Gain compared with Random Replay, and 27.96 Relative Gain compared with No Replay.</p></p class="citation"></blockquote><h3 id=2543--25350-dynamic-contexts-for-generating-suggestion-questions-in-rag-based-conversational-systems-anuja-tayal-et-al-2024>(25/43 | 25/350) Dynamic Contexts for Generating Suggestion Questions in RAG Based Conversational Systems (Anuja Tayal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anuja Tayal, Aman Tyagi. (2024)<br><strong>Dynamic Contexts for Generating Suggestion Questions in RAG Based Conversational Systems</strong><br><button class=copy-to-clipboard title="Dynamic Contexts for Generating Suggestion Questions in RAG Based Conversational Systems" index=25>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-25 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Few-shot, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11413v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11413v1.pdf filename=2403.11413v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>When interacting with <b>Retrieval-Augmented</b> <b>Generation</b> <b>(RAG)-based</b> conversational agents, the users must carefully craft their queries to be understood correctly. Yet, understanding the system&rsquo;s capabilities can be challenging for the users, leading to ambiguous questions that necessitate further clarification. This work aims to bridge the gap by developing a suggestion question generator. To generate suggestion questions, our approach involves utilizing dynamic context, which includes both dynamic <b>few-shot</b> examples and dynamically retrieved contexts. Through experiments, we show that the dynamic contexts approach can generate better suggestion questions as compared to other <b>prompting</b> approaches.</p></p class="citation"></blockquote><h3 id=2643--26350-x-llava-optimizing-bilingual-large-vision-language-alignment-dongjae-shin-et-al-2024>(26/43 | 26/350) X-LLaVA: Optimizing Bilingual Large Vision-Language Alignment (Dongjae Shin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dongjae Shin, Hyunseok Lim, Inho Won, Changsu Choi, Minjun Kim, Seungwoo Song, Hangyeol Yoo, Sangmin Kim, Kyungtae Lim. (2024)<br><strong>X-LLaVA: Optimizing Bilingual Large Vision-Language Alignment</strong><br><button class=copy-to-clipboard title="X-LLaVA: Optimizing Bilingual Large Vision-Language Alignment" index=26>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-26 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 46<br>Keywords: Multi-modal, Multi-modal, GPT-4, Large Language Model, Large Language Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11399v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11399v1.pdf filename=2403.11399v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The impressive development of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> is expanding into the realm of <b>large</b> <b>multimodal</b> <b>models</b> (LMMs), which incorporate multiple types of data beyond text. However, the nature of <b>multimodal</b> models leads to significant expenses in the creation of training data. Furthermore, constructing multilingual data for LMMs presents its own set of challenges due to language diversity and complexity. Therefore, in this study, we propose two cost-effective methods to solve this problem: (1) vocabulary expansion and pretraining of multilingual <b>LLM</b> for specific languages, and (2) automatic and elaborate construction of <b>multimodal</b> datasets using <b>GPT4-V.</b> Based on015 these methods, we constructed a 91K English-Korean-Chinese multilingual, <b>multimodal</b> training dataset. Additionally, we developed a bilingual <b>multimodal</b> model that exhibits excellent performance in both Korean and English, surpassing existing approaches.</p></p class="citation"></blockquote><h3 id=2743--27350-novelqa-a-benchmark-for-long-range-novel-question-answering-cunxiang-wang-et-al-2024>(27/43 | 27/350) NovelQA: A Benchmark for Long-Range Novel Question Answering (Cunxiang Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Cunxiang Wang, Ruoxi Ning, Boqi Pan, Tonghui Wu, Qipeng Guo, Cheng Deng, Guangsheng Bao, Qian Wang, Yue Zhang. (2024)<br><strong>NovelQA: A Benchmark for Long-Range Novel Question Answering</strong><br><button class=copy-to-clipboard title="NovelQA: A Benchmark for Long-Range Novel Question Answering" index=27>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-27 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Benchmarking, Question Answering, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12766v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12766v1.pdf filename=2403.12766v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rapid advancement of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> has introduced a new frontier in natural language processing, particularly in understanding and processing long-context information. However, the evaluation of these models&rsquo; long-context abilities remains a challenge due to the limitations of current <b>benchmarks.</b> To address this gap, we introduce NovelQA, a <b>benchmark</b> specifically designed to test the capabilities of <b>LLMs</b> with extended texts. Constructed from English novels, NovelQA offers a unique blend of complexity, length, and narrative coherence, making it an ideal tool for assessing deep textual understanding in <b>LLMs.</b> This paper presents the design and construction of NovelQA, highlighting its manual annotation, and diverse <b>question</b> <b>types.</b> Our evaluation of Long-context <b>LLMs</b> on NovelQA reveals significant insights into the models&rsquo; performance, particularly emphasizing the challenges they face with multi-hop <b>reasoning,</b> detail-oriented <b>questions,</b> <b>and</b> extremely long input with more than 100,000 tokens. The results underscore the necessity for further advancements in <b>LLMs</b> to improve their long-context comprehension and computational literary studies.</p></p class="citation"></blockquote><h3 id=2843--28350-stylechat-learning-recitation-augmented-memory-in-llms-for-stylized-dialogue-generation-jinpeng-li-et-al-2024>(28/43 | 28/350) StyleChat: Learning Recitation-Augmented Memory in LLMs for Stylized Dialogue Generation (Jinpeng Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinpeng Li, Zekai Zhang, Quan Tu, Xin Cheng, Dongyan Zhao, Rui Yan. (2024)<br><strong>StyleChat: Learning Recitation-Augmented Memory in LLMs for Stylized Dialogue Generation</strong><br><button class=copy-to-clipboard title="StyleChat: Learning Recitation-Augmented Memory in LLMs for Stylized Dialogue Generation" index=28>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-28 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Benchmarking, Supervised Learning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11439v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11439v1.pdf filename=2403.11439v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> demonstrate superior performance in generative scenarios and have attracted widespread attention. Among them, stylized dialogue generation is essential in the context of <b>LLMs</b> for building intelligent and engaging dialogue agent. However the ability of <b>LLMs</b> is data-driven and limited by data bias, leading to poor performance on specific tasks. In particular, stylized dialogue generation suffers from a severe lack of <b>supervised</b> data. Furthermore, although many <b>prompt-based</b> methods have been proposed to accomplish specific tasks, their performance in complex real-world scenarios involving a wide variety of dialog styles further enhancement. In this work, we first introduce a stylized dialogue dataset StyleEval with 38 styles by leveraging the generative power of <b>LLMs</b> comprehensively, which has been carefully constructed with rigorous human-led quality control. Based on this, we propose the stylized dialogue framework StyleChat via recitation-augmented memory strategy and multi-task style learning strategy to promote generalization ability. To evaluate the effectiveness of our approach, we created a test <b>benchmark</b> that included both a generation task and a choice task to comprehensively evaluate trained models and assess whether styles and preferences are remembered and understood. Experimental results show that our proposed framework StyleChat outperforms all the baselines and helps to break the style boundary of <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=2943--29350-zero-shot-multi-task-hallucination-detection-patanjali-bhamidipati-et-al-2024>(29/43 | 29/350) Zero-Shot Multi-task Hallucination Detection (Patanjali Bhamidipati et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Patanjali Bhamidipati, Advaith Malladi, Manish Shrivastava, Radhika Mamidi. (2024)<br><strong>Zero-Shot Multi-task Hallucination Detection</strong><br><button class=copy-to-clipboard title="Zero-Shot Multi-task Hallucination Detection" index=29>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-29 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Zero-shot, Hallucination Detection, Text Generation, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12244v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12244v1.pdf filename=2403.12244v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent studies, the extensive utilization of <b>large</b> <b>language</b> <b>models</b> has underscored the importance of robust evaluation methodologies for assessing <b>text</b> <b>generation</b> quality and relevance to specific tasks. This has revealed a prevalent issue known as <b>hallucination,</b> <b>an</b> emergent condition in the model where generated <b>text</b> <b>lacks</b> faithfulness to the source and deviates from the evaluation criteria. In this study, we formally define <b>hallucination</b> <b>and</b> propose a framework for its quantitative detection in a <b>zero-shot</b> setting, leveraging our definition and the assumption that model outputs entail task and sample specific inputs. In detecting <b>hallucinations,</b> <b>our</b> solution achieves an accuracy of 0.78 in a model-aware setting and 0.61 in a model-agnostic setting. Notably, our solution maintains computational efficiency, requiring far less computational resources than other SOTA approaches, aligning with the trend towards lightweight and compressed models.</p></p class="citation"></blockquote><h3 id=3043--30350-using-generative-text-models-to-create-qualitative-codebooks-for-student-evaluations-of-teaching-andrew-katz-et-al-2024>(30/43 | 30/350) Using Generative Text Models to Create Qualitative Codebooks for Student Evaluations of Teaching (Andrew Katz et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andrew Katz, Mitchell Gerhardt, Michelle Soledad. (2024)<br><strong>Using Generative Text Models to Create Qualitative Codebooks for Student Evaluations of Teaching</strong><br><button class=copy-to-clipboard title="Using Generative Text Models to Create Qualitative Codebooks for Student Evaluations of Teaching" index=30>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-30 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-HC, cs.CL<br>Keyword Score: 40<br>Keywords: Knowledge Distillation, Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11984v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11984v1.pdf filename=2403.11984v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Feedback is a critical aspect of improvement. Unfortunately, when there is a lot of feedback from multiple sources, it can be difficult to <b>distill</b> the information into actionable insights. Consider student evaluations of teaching (SETs), which are important sources of feedback for educators. They can give instructors insights into what worked during a semester. A collection of SETs can also be useful to administrators as signals for courses or entire programs. However, on a <b>large</b> <b>scale</b> <b>as</b> in high-enrollment courses or administrative records over several years, the volume of SETs can render them difficult to analyze. In this paper, we discuss a novel method for analyzing SETs using natural language processing (NLP) and <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> We demonstrate the method by applying it to a corpus of 5,000 SETs from a <b>large</b> <b>public</b> <b>university.</b> We show that the method can be used to extract, embed, cluster, and <b>summarize</b> the SETs to identify the themes they express. More generally, this work illustrates how to use the combination of NLP techniques and <b>LLMs</b> to generate a codebook for SETs. We conclude by discussing the implications of this method for analyzing SETs and other types of student writing in teaching and research settings.</p></p class="citation"></blockquote><h3 id=3143--31350-language-evolution-with-deep-learning-mathieu-rita-et-al-2024>(31/43 | 31/350) Language Evolution with Deep Learning (Mathieu Rita et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mathieu Rita, Paul Michel, Rahma Chaabouni, Olivier Pietquin, Emmanuel Dupoux, Florian Strub. (2024)<br><strong>Language Evolution with Deep Learning</strong><br><button class=copy-to-clipboard title="Language Evolution with Deep Learning" index=31>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-31 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-MA, cs.CL<br>Keyword Score: 40<br>Keywords: Reinforcement Learning, Simulation, Simulator, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11958v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11958v1.pdf filename=2403.11958v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Computational modeling plays an essential role in the study of language emergence. It aims to simulate the conditions and learning processes that could trigger the emergence of a structured language within a simulated controlled environment. Several methods have been used to investigate the origin of our language, including agent-based systems, Bayesian agents, genetic algorithms, and rule-based systems. This chapter explores another class of computational models that have recently revolutionized the field of machine learning: deep learning models. The chapter introduces the basic concepts of deep and <b>reinforcement</b> <b>learning</b> methods and <b>summarizes</b> their helpfulness for simulating language emergence. It also discusses the key findings, limitations, and recent attempts to build realistic <b>simulations.</b> This chapter targets linguists and cognitive scientists seeking an introduction to deep learning as a tool to investigate language evolution.</p></p class="citation"></blockquote><h3 id=3243--32350-reference-based-metrics-disprove-themselves-in-question-generation-bang-nguyen-et-al-2024>(32/43 | 32/350) Reference-based Metrics Disprove Themselves in Question Generation (Bang Nguyen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bang Nguyen, Mengxia Yu, Yun Huang, Meng Jiang. (2024)<br><strong>Reference-based Metrics Disprove Themselves in Question Generation</strong><br><button class=copy-to-clipboard title="Reference-based Metrics Disprove Themselves in Question Generation" index=32>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-32 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, BERTScore, BLEU, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12242v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12242v1.pdf filename=2403.12242v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Reference-based metrics such as <b>BLEU</b> and <b>BERTScore</b> are widely used to evaluate question generation (QG). In this study, on QG <b>benchmarks</b> such as SQuAD and HotpotQA, we find that using human-written references cannot guarantee the effectiveness of the reference-based metrics. Most QG <b>benchmarks</b> have only one reference; we replicated the annotation process and collect another reference. A good metric was expected to grade a human-validated question no worse than generated questions. However, the results of reference-based metrics on our newly collected reference disproved the metrics themselves. We propose a reference-free metric consisted of multi-dimensional criteria such as naturalness, answerability, and complexity, utilizing <b>large</b> <b>language</b> <b>models.</b> These criteria are not constrained to the syntactic or semantic of a single reference question, and the metric does not require a diverse set of references. Experiments reveal that our metric accurately distinguishes between high-quality questions and flawed ones, and achieves state-of-the-art alignment with human judgment.</p></p class="citation"></blockquote><h3 id=3343--33350-reasoning-abilities-of-large-language-models-in-depth-analysis-on-the-abstraction-and-reasoning-corpus-seungpil-lee-et-al-2024>(33/43 | 33/350) Reasoning Abilities of Large Language Models: In-Depth Analysis on the Abstraction and Reasoning Corpus (Seungpil Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Seungpil Lee, Woochang Sim, Donghyeon Shin, Sanha Hwang, Wongyu Seo, Jiwon Park, Seokki Lee, Sejin Kim, Sundong Kim. (2024)<br><strong>Reasoning Abilities of Large Language Models: In-Depth Analysis on the Abstraction and Reasoning Corpus</strong><br><button class=copy-to-clipboard title="Reasoning Abilities of Large Language Models: In-Depth Analysis on the Abstraction and Reasoning Corpus" index=33>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-33 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-ET, cs-SC, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11793v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11793v1.pdf filename=2403.11793v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The existing methods for evaluating the inference abilities of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have been results-centric, making it difficult to assess the inference process. We introduce a new approach using the Abstract and <b>Reasoning</b> Corpus (ARC) dataset to evaluate the inference and contextual understanding abilities of <b>large</b> <b>language</b> <b>models</b> in a process-centric manner. ARC demands rigorous logical structures for problem-solving, making it a <b>benchmark</b> that facilitates the comparison of model inference abilities with humans. Experimental results confirm that while <b>large</b> <b>language</b> <b>models</b> possess weak inference abilities, they still lag in terms of logical coherence, compositionality, and productivity. Our experiments highlight the <b>reasoning</b> capabilities of <b>LLMs,</b> proposing development paths for achieving human-level <b>reasoning.</b></p></p class="citation"></blockquote><h3 id=3443--34350-syn-qa2-evaluating-false-assumptions-in-long-tail-questions-with-synthetic-qa-datasets-ashwin-daswani-et-al-2024>(34/43 | 34/350) Syn-QA2: Evaluating False Assumptions in Long-tail Questions with Synthetic QA Datasets (Ashwin Daswani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ashwin Daswani, Rohan Sawant, Najoung Kim. (2024)<br><strong>Syn-QA2: Evaluating False Assumptions in Long-tail Questions with Synthetic QA Datasets</strong><br><button class=copy-to-clipboard title="Syn-QA2: Evaluating False Assumptions in Long-tail Questions with Synthetic QA Datasets" index=34>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-34 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Question Answering, Question Answering, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12145v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12145v1.pdf filename=2403.12145v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Sensitivity to false assumptions (or false premises) in information-seeking <b>questions</b> <b>is</b> critical for robust <b>question-answering</b> <b>(QA)</b> systems. Recent work has shown that false assumptions in naturally occurring <b>questions</b> <b>pose</b> challenges to current models, with low performance on both generative <b>QA</b> and simple detection tasks (Kim et al. 2023). However, the focus of existing work on naturally occurring <b>questions</b> <b>leads</b> to a gap in the analysis of model behavior on the long tail of the distribution of possible <b>questions.</b> <b>To</b> this end, we introduce Syn-(QA)$^2$, a set of two synthetically generated <b>QA</b> datasets: one generated using perturbed relations from Wikidata, and the other by perturbing HotpotQA (Yang et al. 2018). Our findings from evaluating a range of <b>large</b> <b>language</b> <b>models</b> are threefold: (1) false assumptions in <b>QA</b> are challenging, echoing the findings of prior work, (2) the binary detection task is challenging even compared to the difficulty of generative <b>QA</b> itself, possibly due to the linguistic structure of the problem, and (3) the detection task is more challenging with long-tail <b>questions</b> <b>compared</b> to naturally occurring <b>questions,</b> <b>highlighting</b> the utility of our synthetic datasets and generation method.</p></p class="citation"></blockquote><h3 id=3543--35350-from-pixels-to-insights-a-survey-on-automatic-chart-understanding-in-the-era-of-large-foundation-models-kung-hsiang-huang-et-al-2024>(35/43 | 35/350) From Pixels to Insights: A Survey on Automatic Chart Understanding in the Era of Large Foundation Models (Kung-Hsiang Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kung-Hsiang Huang, Hou Pong Chan, Yi R. Fung, Haoyi Qiu, Mingyang Zhou, Shafiq Joty, Shih-Fu Chang, Heng Ji. (2024)<br><strong>From Pixels to Insights: A Survey on Automatic Chart Understanding in the Era of Large Foundation Models</strong><br><button class=copy-to-clipboard title="From Pixels to Insights: A Survey on Automatic Chart Understanding in the Era of Large Foundation Models" index=35>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-35 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-CV, cs.CL<br>Keyword Score: 30<br>Keywords: Foundation Model, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12027v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12027v1.pdf filename=2403.12027v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Data visualization in the form of charts plays a pivotal role in data analysis, offering critical insights and aiding in informed decision-making. Automatic chart understanding has witnessed significant advancements with the rise of <b>large</b> <b>foundation</b> <b>models</b> in recent years. <b>Foundation</b> <b>models,</b> such as <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> have revolutionized various natural language processing (NLP) tasks and are increasingly being applied to chart understanding tasks. This survey paper provides a comprehensive overview of the recent developments, challenges, and future directions in chart understanding within the context of these <b>foundation</b> <b>models.</b> The paper begins by defining chart understanding, outlining problem formulations, and discussing fundamental building blocks crucial for studying chart understanding tasks. In the section on tasks and datasets, we explore various tasks within chart understanding and discuss their evaluation metrics and sources of both charts and textual inputs. Modeling strategies are then examined, encompassing both classification-based and generation-based approaches, along with tool augmentation techniques that enhance chart understanding performance. Furthermore, we discuss the state-of-the-art performance of each task and discuss how we can improve the performance. Challenges and future directions are addressed in a dedicated section, highlighting issues such as domain-specific charts, lack of efforts in evaluation, and agent-oriented settings. This survey paper serves to provide valuable insights and directions for future research in chart understanding leveraging <b>large</b> <b>foundation</b> <b>models.</b> The studies mentioned in this paper, along with emerging new research, will be continually updated at: <a href=https://github.com/khuangaf/Awesome-Chart-Understanding>https://github.com/khuangaf/Awesome-Chart-Understanding</a>.</p></p class="citation"></blockquote><h3 id=3643--36350-investigating-markers-and-drivers-of-gender-bias-in-machine-translations-peter-j-barclay-et-al-2024>(36/43 | 36/350) Investigating Markers and Drivers of Gender Bias in Machine Translations (Peter J Barclay et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Peter J Barclay, Ashkan Sami. (2024)<br><strong>Investigating Markers and Drivers of Gender Bias in Machine Translations</strong><br><button class=copy-to-clipboard title="Investigating Markers and Drivers of Gender Bias in Machine Translations" index=36>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-36 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CY, cs-SE, cs.CL<br>Keyword Score: 30<br>Keywords: Neural Machine Translation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11896v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11896v1.pdf filename=2403.11896v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Implicit gender bias in <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> is a well-documented problem, and implications of gender introduced into automatic translations can perpetuate real-world biases. However, some <b>LLMs</b> use heuristics or post-processing to mask such bias, making investigation difficult. Here, we examine bias in <b>LLMss</b> via back-translation, using the DeepL translation API to investigate the bias evinced when repeatedly translating a set of 56 Software Engineering tasks used in a previous study. Each statement starts with &lsquo;she&rsquo;, and is translated first into a &lsquo;genderless&rsquo; intermediate language then back into English; we then examine pronoun-choice in the back-translated texts. We expand prior research in the following ways: (1) by comparing results across five intermediate languages, namely Finnish, Indonesian, Estonian, Turkish and Hungarian; (2) by proposing a novel metric for assessing the variation in gender implied in the repeated translations, avoiding the over-interpretation of individual pronouns, apparent in earlier work; (3) by investigating sentence features that drive bias; (4) and by comparing results from three time-lapsed datasets to establish the reproducibility of the approach. We found that some languages display similar patterns of pronoun use, falling into three loose groups, but that patterns vary between groups; this underlines the need to work with multiple languages. We also identify the main verb appearing in a sentence as a likely significant driver of implied gender in the translations. Moreover, we see a good level of replicability in the results, and establish that our variation metric proves robust despite an obvious change in the behaviour of the DeepL translation API during the course of the study. These results show that the back-translation method can provide further insights into bias in language models.</p></p class="citation"></blockquote><h3 id=3743--37350-revisiting-the-classics-a-study-on-identifying-and-rectifying-gender-stereotypes-in-rhymes-and-poems-aditya-narayan-sankaran-et-al-2024>(37/43 | 37/350) Revisiting The Classics: A Study on Identifying and Rectifying Gender Stereotypes in Rhymes and Poems (Aditya Narayan Sankaran et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aditya Narayan Sankaran, Vigneshwaran Shankaran, Sampath Lonka, Rajesh Sharma. (2024)<br><strong>Revisiting The Classics: A Study on Identifying and Rectifying Gender Stereotypes in Rhymes and Poems</strong><br><button class=copy-to-clipboard title="Revisiting The Classics: A Study on Identifying and Rectifying Gender Stereotypes in Rhymes and Poems" index=37>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-37 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11752v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11752v1.pdf filename=2403.11752v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Rhymes and poems are a powerful medium for transmitting cultural norms and societal roles. However, the pervasive existence of gender stereotypes in these works perpetuates biased perceptions and limits the scope of individuals&rsquo; identities. Past works have shown that stereotyping and prejudice emerge in early childhood, and developmental research on causal mechanisms is critical for understanding and controlling stereotyping and prejudice. This work contributes by gathering a dataset of rhymes and poems to identify gender stereotypes and propose a model with 97% accuracy to identify gender bias. Gender stereotypes were rectified using a <b>Large</b> <b>Language</b> <b>Model</b> <b>(LLM)</b> and its effectiveness was evaluated in a comparative survey against human educator rectifications. To <b>summarize,</b> this work highlights the pervasive nature of gender stereotypes in literary works and reveals the potential of <b>LLMs</b> to rectify gender stereotypes. This study raises awareness and promotes inclusivity within artistic expressions, making a significant contribution to the discourse on gender equality.</p></p class="citation"></blockquote><h3 id=3843--38350-from-explainable-to-interpretable-deep-learning-for-natural-language-processing-in-healthcare-how-far-from-reality-guangming-huang-et-al-2024>(38/43 | 38/350) From explainable to interpretable deep learning for natural language processing in healthcare: how far from reality? (Guangming Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guangming Huang, Yunfei Long, Yingya Li, Giorgos Papanastasiou. (2024)<br><strong>From explainable to interpretable deep learning for natural language processing in healthcare: how far from reality?</strong><br><button class=copy-to-clipboard title="From explainable to interpretable deep learning for natural language processing in healthcare: how far from reality?" index=38>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-38 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 26<br>Keywords: Benchmarking, Multi-modal, Reasoning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11894v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11894v1.pdf filename=2403.11894v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep learning (DL) has substantially enhanced healthcare research by addressing various natural language processing (NLP) tasks. Yet, the increasing complexity of DL-based NLP methods necessitates transparent model interpretability, or at least explainability, for reliable decision-making. This work presents a thorough scoping review on explainable and interpretable DL in healthcare NLP. The term &ldquo;XIAI&rdquo; (eXplainable and Interpretable Artificial Intelligence) was introduced to distinguish XAI from IAI. Methods were further categorized based on their functionality (model-, input-, output-based) and scope (local, global). Our analysis shows that attention mechanisms were the most dominant emerging IAI. Moreover, IAI is increasingly used against XAI. The major challenges identified are that most XIAI do not explore &ldquo;global&rdquo; modeling processes, the lack of best practices, and the unmet need for systematic evaluation and <b>benchmarks.</b> Important opportunities were raised such as using &ldquo;attention&rdquo; to enhance <b>multi-modal</b> XIAI for personalized medicine and combine DL with causal <b>reasoning.</b> Our discussion encourages the integration of XIAI in <b>LLMs</b> and domain-specific smaller models. Our review can stimulate further research and <b>benchmarks</b> toward improving inherent IAI and engaging complex NLP in healthcare.</p></p class="citation"></blockquote><h3 id=3943--39350-sscae----semantic-syntactic-and-context-aware-natural-language-adversarial-examples-generator-javad-rafiei-asl-et-al-2024>(39/43 | 39/350) SSCAE &ndash; Semantic, Syntactic, and Context-aware natural language Adversarial Examples generator (Javad Rafiei Asl et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Javad Rafiei Asl, Mohammad H. Rafiei, Manar Alohaly, Daniel Takabi. (2024)<br><strong>SSCAE &ndash; Semantic, Syntactic, and Context-aware natural language Adversarial Examples generator</strong><br><button class=copy-to-clipboard title="SSCAE -- Semantic, Syntactic, and Context-aware natural language Adversarial Examples generator" index=39>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-39 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CR, cs-LG, cs.CL<br>Keyword Score: 25<br>Keywords: Black Box, Masked Language Model, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11833v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11833v1.pdf filename=2403.11833v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Machine learning models are vulnerable to maliciously crafted <b>Adversarial</b> <b>Examples</b> (AEs). Training a machine learning model with AEs improves its robustness and stability against <b>adversarial</b> <b>attacks.</b> It is essential to develop models that produce high-quality AEs. Developing such models has been much slower in natural language processing (NLP) than in areas such as computer vision. This paper introduces a practical and efficient <b>adversarial</b> <b>attack</b> model called SSCAE for \textbf{S}emantic, \textbf{S}yntactic, and \textbf{C}ontext-aware natural language \textbf{AE}s generator. SSCAE identifies important words and uses a <b>masked</b> <b>language</b> <b>model</b> to generate an early set of substitutions. Next, two well-known language models are employed to evaluate the initial set in terms of semantic and syntactic characteristics. We introduce (1) a dynamic threshold to capture more efficient perturbations and (2) a local greedy search to generate high-quality AEs. As a <b>black-box</b> <b>method,</b> SSCAE generates humanly imperceptible and context-aware AEs that preserve semantic consistency and the source language&rsquo;s syntactical and grammatical requirements. The effectiveness and superiority of the proposed SSCAE model are illustrated with fifteen comparative experiments and extensive sensitivity analysis for parameter optimization. SSCAE outperforms the existing models in all experiments while maintaining a higher semantic consistency with a lower query number and a comparable perturbation rate.</p></p class="citation"></blockquote><h3 id=4043--40350-a-comparative-investigation-of-compositional-syntax-and-semantics-in-dall-e-2-elliot-murphy-et-al-2024>(40/43 | 40/350) A Comparative Investigation of Compositional Syntax and Semantics in DALL-E 2 (Elliot Murphy et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Elliot Murphy, Jill de Villiers, Sofia Lucero Morales. (2024)<br><strong>A Comparative Investigation of Compositional Syntax and Semantics in DALL-E 2</strong><br><button class=copy-to-clipboard title="A Comparative Investigation of Compositional Syntax and Semantics in DALL-E 2" index=40>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-40 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12294v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12294v1.pdf filename=2403.12294v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this study we compared how well DALL-E 2 visually represented the meaning of linguistic <b>prompts</b> also given to young children in comprehension tests. Sentences representing fundamental components of grammatical knowledge were selected from assessment tests used with several hundred English-speaking children aged 2-7 years for whom we had collected original item-level data. DALL-E 2 was given these <b>prompts</b> five times to generate 20 cartoons per item, for 9 adult judges to score. Results revealed no conditions in which DALL-E 2-generated images that matched the semantic accuracy of children, even at the youngest age (2 years). DALL-E 2 failed to assign the appropriate roles in reversible forms; it failed on negation despite an easier contrastive <b>prompt</b> than the children received; it often assigned the adjective to the wrong noun; it ignored implicit agents in passives. This work points to a clear absence of compositional sentence representations for DALL-E 2.</p></p class="citation"></blockquote><h3 id=4143--41350-adaptative-bilingual-aligning-using-multilingual-sentence-embedding-olivier-kraif-2024>(41/43 | 41/350) Adaptative Bilingual Aligning Using Multilingual Sentence Embedding (Olivier Kraif, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Olivier Kraif. (2024)<br><strong>Adaptative Bilingual Aligning Using Multilingual Sentence Embedding</strong><br><button class=copy-to-clipboard title="Adaptative Bilingual Aligning Using Multilingual Sentence Embedding" index=41>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-41 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Sentence Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11921v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11921v1.pdf filename=2403.11921v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we present an adaptive bitextual alignment system called AIlign. This aligner relies on <b>sentence</b> <b>embeddings</b> to extract reliable anchor points that can guide the alignment path, even for texts whose parallelism is fragmentary and not strictly monotonic. In an experiment on several datasets, we show that AIlign achieves results equivalent to the state of the art, with quasi-linear complexity. In addition, AIlign is able to handle texts whose parallelism and monotonicity properties are only satisfied locally, unlike recent systems such as Vecalign or Bertalign.</p></p class="citation"></blockquote><h3 id=4243--42350-a-closer-look-at-claim-decomposition-miriam-wanner-et-al-2024>(42/43 | 42/350) A Closer Look at Claim Decomposition (Miriam Wanner et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Miriam Wanner, Seth Ebner, Zhengping Jiang, Mark Dredze, Benjamin Van Durme. (2024)<br><strong>A Closer Look at Claim Decomposition</strong><br><button class=copy-to-clipboard title="A Closer Look at Claim Decomposition" index=42>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-42 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11903v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11903v1.pdf filename=2403.11903v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As generated text becomes more commonplace, it is increasingly important to evaluate how well-supported such text is by external knowledge sources. Many approaches for evaluating textual support rely on some method for decomposing text into its individual subclaims which are scored against a trusted reference. We investigate how various methods of claim decomposition &ndash; especially <b>LLM-based</b> methods &ndash; affect the result of an evaluation approach such as the recently proposed FActScore, finding that it is sensitive to the decomposition method used. This sensitivity arises because such metrics attribute overall textual support to the model that generated the text even though error can also come from the metric&rsquo;s decomposition step. To measure decomposition quality, we introduce an adaptation of FActScore, which we call DecompScore. We then propose an <b>LLM-based</b> approach to generating decompositions inspired by Bertrand Russell&rsquo;s theory of logical atomism and neo-Davidsonian semantics and demonstrate its improved decomposition quality over previous methods.</p></p class="citation"></blockquote><h3 id=4343--43350-word-orders-impacts-insights-from-reordering-and-generation-analysis-qinghua-zhao-et-al-2024>(43/43 | 43/350) Word Order&rsquo;s Impacts: Insights from Reordering and Generation Analysis (Qinghua Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qinghua Zhao, Jiaang Li, Lei Li, Zenghui Zhou, Junfeng Liu. (2024)<br><strong>Word Order&rsquo;s Impacts: Insights from Reordering and Generation Analysis</strong><br><button class=copy-to-clipboard title="Word Order's Impacts: Insights from Reordering and Generation Analysis" index=43>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-43 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: ChatGPT<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11473v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11473v1.pdf filename=2403.11473v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing works have studied the impacts of the order of words within natural text. They usually analyze it by destroying the original order of words to create a scrambled sequence, and then comparing the models&rsquo; performance between the original and scrambled sequences. The experimental results demonstrate marginal drops. Considering this findings, different hypothesis about word order is proposed, including <code>the order of words is redundant with lexical semantics'', and </code>models do not rely on word order&rsquo;&rsquo;. In this paper, we revisit the aforementioned hypotheses by adding a order reconstruction perspective, and selecting datasets of different spectrum. Specifically, we first select four different datasets, and then design order reconstruction and continuing generation tasks. Empirical findings support that <b>ChatGPT</b> relies on word order to infer, but cannot support or negate the redundancy relations between word order lexical semantics.</p></p class="citation"></blockquote><h2 id=cscv-122>cs.CV (122)</h2><h3 id=1122--44350-leveraging-spatial-and-semantic-feature-extraction-for-skin-cancer-diagnosis-with-capsule-networks-and-graph-neural-networks-k-p-santoso-et-al-2024>(1/122 | 44/350) Leveraging Spatial and Semantic Feature Extraction for Skin Cancer Diagnosis with Capsule Networks and Graph Neural Networks (K. P. Santoso et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>K. P. Santoso, R. V. H. Ginardi, R. A. Sastrowardoyo, F. A. Madany. (2024)<br><strong>Leveraging Spatial and Semantic Feature Extraction for Skin Cancer Diagnosis with Capsule Networks and Graph Neural Networks</strong><br><button class=copy-to-clipboard title="Leveraging Spatial and Semantic Feature Extraction for Skin Cancer Diagnosis with Capsule Networks and Graph Neural Networks" index=44>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-44 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 89<br>Keywords: MNIST, Graph, Graph Neural Network, Graph Neural Network, Benchmarking, Benchmarking, Convolution, Convolutional Neural Network, Convolutional Neural Network, Generative Adversarial Network, Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12009v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12009v2.pdf filename=2403.12009v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the realm of skin lesion image classification, the intricate spatial and semantic features pose significant challenges for conventional <b>Convolutional</b> <b>Neural</b> <b>Network</b> <b>(CNN)-based</b> methodologies. These challenges are compounded by the imbalanced nature of skin lesion datasets, which hampers the ability of models to learn minority class features effectively. Despite augmentation strategies, such as those using <b>Generative</b> <b>Adversarial</b> <b>Networks</b> <b>(GANs),</b> previous attempts have not fully addressed these complexities. This study introduces an innovative approach by integrating <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> with Capsule Networks to enhance classification performance. <b>GNNs,</b> known for their proficiency in handling <b>graph-structured</b> <b>data,</b> <b>offer</b> an advanced mechanism for capturing complex patterns and relationships beyond the capabilities of traditional <b>CNNs.</b> Capsule Networks further contribute by providing superior recognition of spatial hierarchies within images. Our research focuses on evaluating and enhancing the Tiny Pyramid Vision <b>GNN</b> (Tiny Pyramid ViG) architecture by incorporating it with a Capsule Network. This hybrid model was applied to the <b>MNIST:HAM10000</b> dataset, a comprehensive skin lesion dataset designed for <b>benchmarking</b> classification models. After 75 epochs of training, our model achieved a significant accuracy improvement, reaching 89.23% and 95.52%, surpassing established <b>benchmarks</b> such as GoogLeNet (83.94%), InceptionV3 (86.82%), MobileNet V3 (89.87%), EfficientNet-B7 (92.07%), ResNet18 (92.22%), ResNet34 (91.90%), ViT-Base (73.70%), and IRv2-SA (93.47%) on the same dataset. This outcome underscores the potential of our approach in overcoming the inherent challenges of skin lesion classification, contributing to the advancement of image-based diagnosis in dermatology.</p></p class="citation"></blockquote><h3 id=2122--45350-seta-semantic-aware-token-augmentation-for-domain-generalization-jintao-guo-et-al-2024>(2/122 | 45/350) SETA: Semantic-Aware Token Augmentation for Domain Generalization (Jintao Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jintao Guo, Lei Qi, Yinghuan Shi, Yang Gao. (2024)<br><strong>SETA: Semantic-Aware Token Augmentation for Domain Generalization</strong><br><button class=copy-to-clipboard title="SETA: Semantic-Aware Token Augmentation for Domain Generalization" index=45>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-45 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 73<br>Keywords: Vision Transformer, Benchmarking, Convolution, Convolutional Neural Network, Convolutional Neural Network, Data Augmentation, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11792v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11792v1.pdf filename=2403.11792v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Domain generalization (DG) aims to enhance the model robustness against domain shifts without accessing target domains. A prevalent category of methods for DG is <b>data</b> <b>augmentation,</b> which focuses on generating virtual samples to simulate domain shifts. However, existing augmentation techniques in DG are mainly tailored for <b>convolutional</b> <b>neural</b> <b>networks</b> <b>(CNNs),</b> with limited exploration in token-based architectures, i.e., <b>vision</b> <b>transformer</b> (ViT) and multi-layer perceptrons (MLP) models. In this paper, we study the impact of prior <b>CNN-based</b> augmentation methods on token-based models, revealing their performance is suboptimal due to the lack of incentivizing the model to learn holistic shape information. To tackle the issue, we propose the SEmantic-aware Token Augmentation (SETA) method. SETA transforms token features by perturbing local edge cues while preserving global shape features, thereby enhancing the model learning of shape information. To further enhance the generalization ability of the model, we introduce two stylized variants of our method combined with two state-of-the-art style augmentation methods in DG. We provide a theoretical insight into our method, demonstrating its effectiveness in reducing the generalization risk bound. Comprehensive experiments on five <b>benchmarks</b> prove that our method achieves SOTA performances across various ViT and MLP architectures. Our code is available at <a href=https://github.com/lingeringlight/SETA>https://github.com/lingeringlight/SETA</a>.</p></p class="citation"></blockquote><h3 id=3122--46350-ttt-kd-test-time-training-for-3d-semantic-segmentation-through-knowledge-distillation-from-foundation-models-lisa-weijler-et-al-2024>(3/122 | 46/350) TTT-KD: Test-Time Training for 3D Semantic Segmentation through Knowledge Distillation from Foundation Models (Lisa Weijler et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lisa Weijler, Muhammad Jehanzeb Mirza, Leon Sick, Can Ekkazan, Pedro Hermosilla. (2024)<br><strong>TTT-KD: Test-Time Training for 3D Semantic Segmentation through Knowledge Distillation from Foundation Models</strong><br><button class=copy-to-clipboard title="TTT-KD: Test-Time Training for 3D Semantic Segmentation through Knowledge Distillation from Foundation Models" index=46>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-46 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 73<br>Keywords: Benchmarking, Distribution Shift, Distribution Shift, Foundation Model, Knowledge Distillation, Knowledge Distillation, Knowledge Distillation, Out-of-distribution, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11691v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11691v1.pdf filename=2403.11691v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Test-Time Training (TTT) proposes to adapt a pre-trained network to changing data <b>distributions</b> <b>on-the-fly.</b> In this work, we propose the first TTT method for 3D semantic segmentation, TTT-KD, which models <b>Knowledge</b> <b>Distillation</b> <b>(KD)</b> from <b>foundation</b> <b>models</b> (e.g. DINOv2) as a <b>self-supervised</b> objective for adaptation to <b>distribution</b> <b>shifts</b> at test-time. Given access to paired image-pointcloud (2D-3D) data, we first optimize a 3D segmentation backbone for the main task of semantic segmentation using the pointclouds and the task of 2D $\to$ 3D <b>KD</b> by using an off-the-shelf 2D pre-trained <b>foundation</b> <b>model.</b> At test-time, our TTT-KD updates the 3D segmentation backbone for each test sample, by using the <b>self-supervised</b> task of <b>knowledge</b> <b>distillation,</b> before performing the final prediction. Extensive evaluations on multiple indoor and outdoor 3D segmentation <b>benchmarks</b> show the utility of TTT-KD, as it improves performance for both in-distribution (ID) and <b>out-of-distribution</b> (ODO) test datasets. We achieve a gain of up to 13% mIoU (7% on average) when the train and test <b>distributions</b> <b>are</b> similar and up to 45% (20% on average) when adapting to OOD test samples.</p></p class="citation"></blockquote><h3 id=4122--47350-flexcap-generating-rich-localized-and-flexible-captions-in-images-debidatta-dwibedi-et-al-2024>(4/122 | 47/350) FlexCap: Generating Rich, Localized, and Flexible Captions in Images (Debidatta Dwibedi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Debidatta Dwibedi, Vidhi Jain, Jonathan Tompson, Andrew Zisserman, Yusuf Aytar. (2024)<br><strong>FlexCap: Generating Rich, Localized, and Flexible Captions in Images</strong><br><button class=copy-to-clipboard title="FlexCap: Generating Rich, Localized, and Flexible Captions in Images" index=47>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-47 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CL, cs-CV, cs-LG, cs.CV<br>Keyword Score: 70<br>Keywords: Object Detection, Zero-shot, Question Answering, Visual Question Answering, Visual Question Answering, Large Language Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12026v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12026v1.pdf filename=2403.12026v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce a versatile $\textit{flexible-captioning}$ <b>vision-language</b> model (VLM) capable of generating region-specific descriptions of varying lengths. The model, FlexCap, is trained to produce length-conditioned captions for input bounding boxes, and this allows control over the information density of its output, with descriptions ranging from concise <b>object</b> <b>labels</b> to detailed captions. To achieve this we create <b>large-scale</b> <b>training</b> <b>datasets</b> of image region descriptions of varying length, starting from captioned images. This flexible-captioning capability has several valuable applications. First, FlexCap demonstrates superior performance in dense captioning tasks on the <b>Visual</b> <b>Genome</b> <b>dataset.</b> Second, a <b>visual</b> <b>question</b> <b>answering</b> <b>(VQA)</b> system can be built by employing FlexCap to generate localized descriptions as inputs to a <b>large</b> <b>language</b> <b>model.</b> The resulting system achieves state-of-the-art <b>zero-shot</b> performance on a number of <b>VQA</b> datasets. We also demonstrate a $\textit{localize-then-describe}$ approach with FlexCap can be better at open-ended <b>object</b> <b>detection</b> than a $\textit{describe-then-localize}$ approach with other VLMs. We highlight a novel characteristic of FlexCap, which is its ability to extract diverse <b>visual</b> <b>information</b> <b>through</b> prefix conditioning. Finally, we qualitatively demonstrate FlexCap&rsquo;s broad applicability in tasks such as image labeling, <b>object</b> <b>attribute</b> recognition, and <b>visual</b> <b>dialog.</b> <b>Project</b> webpage: <a href=https://flex-cap.github.io>https://flex-cap.github.io</a> .</p></p class="citation"></blockquote><h3 id=5122--48350-arc2face-a-foundation-model-of-human-faces-foivos-paraperas-papantoniou-et-al-2024>(5/122 | 48/350) Arc2Face: A Foundation Model of Human Faces (Foivos Paraperas Papantoniou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Foivos Paraperas Papantoniou, Alexandros Lattas, Stylianos Moschoglou, Jiankang Deng, Bernhard Kainz, Stefanos Zafeiriou. (2024)<br><strong>Arc2Face: A Foundation Model of Human Faces</strong><br><button class=copy-to-clipboard title="Arc2Face: A Foundation Model of Human Faces" index=48>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-48 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 70<br>Keywords: Diffusion Model, Face Recognition, Foundation Model, Zero-shot, Text2image, Prompt, Text Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11641v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11641v1.pdf filename=2403.11641v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents Arc2Face, an identity-conditioned <b>face</b> <b>foundation</b> <b>model,</b> which, given the ArcFace embedding of a person, can generate diverse photo-realistic images with an unparalleled degree of <b>face</b> <b>similarity</b> than existing models. Despite previous attempts to decode <b>face</b> <b>recognition</b> features into detailed images, we find that common high-resolution datasets (e.g. FFHQ) lack sufficient identities to reconstruct any subject. To that end, we meticulously upsample a significant portion of the WebFace42M database, the largest public dataset for <b>face</b> <b>recognition</b> (FR). Arc2Face builds upon a pretrained Stable <b>Diffusion</b> <b>model,</b> yet adapts it to the task of ID-to-face generation, conditioned solely on ID vectors. Deviating from recent works that combine ID with <b>text</b> <b>embeddings</b> for <b>zero-shot</b> personalization of <b>text-to-image</b> <b>models,</b> we emphasize on the compactness of FR features, which can fully capture the essence of the human <b>face,</b> <b>as</b> opposed to hand-crafted <b>prompts.</b> Crucially, <b>text-augmented</b> <b>models</b> struggle to decouple identity and <b>text,</b> <b>usually</b> necessitating some description of the given <b>face</b> <b>to</b> achieve satisfactory similarity. Arc2Face, however, only needs the discriminative features of ArcFace to guide the generation, offering a robust prior for a plethora of tasks where ID consistency is of paramount importance. As an example, we train a FR model on synthetic images from our model and achieve superior performance to existing synthetic datasets.</p></p class="citation"></blockquote><h3 id=6122--49350-vmambair-visual-state-space-model-for-image-restoration-yuan-shi-et-al-2024>(6/122 | 49/350) VmambaIR: Visual State Space Model for Image Restoration (Yuan Shi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuan Shi, Bin Xia, Xiaoyu Jin, Xing Wang, Tianyu Zhao, Xin Xia, Xuefeng Xiao, Wenming Yang. (2024)<br><strong>VmambaIR: Visual State Space Model for Image Restoration</strong><br><button class=copy-to-clipboard title="VmambaIR: Visual State Space Model for Image Restoration" index=49>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-49 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 70<br>Keywords: Diffusion Model, Convolution, Convolutional Neural Network, Convolutional Neural Network, Generative Adversarial Network, Generative Adversarial Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11423v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11423v1.pdf filename=2403.11423v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Image restoration is a critical task in low-level computer vision, aiming to restore high-quality images from degraded inputs. Various models, such as <b>convolutional</b> <b>neural</b> <b>networks</b> <b>(CNNs),</b> <b>generative</b> <b>adversarial</b> <b>networks</b> <b>(GANs),</b> <b>transformers,</b> and <b>diffusion</b> <b>models</b> (DMs), have been employed to address this problem with significant impact. However, <b>CNNs</b> have limitations in capturing long-range dependencies. DMs require large prior models and computationally intensive denoising steps. <b>Transformers</b> have powerful modeling capabilities but face challenges due to quadratic complexity with input image size. To address these challenges, we propose VmambaIR, which introduces State Space Models (SSMs) with linear complexity into comprehensive image restoration tasks. We utilize a Unet architecture to stack our proposed Omni Selective Scan (OSS) blocks, consisting of an OSS module and an Efficient Feed-Forward Network (EFFN). Our proposed omni selective scan mechanism overcomes the unidirectional modeling limitation of SSMs by efficiently modeling image information flows in all six directions. Furthermore, we conducted a comprehensive evaluation of our VmambaIR across multiple image restoration tasks, including image deraining, single image super-resolution, and real-world image super-resolution. Extensive experimental results demonstrate that our proposed VmambaIR achieves state-of-the-art (SOTA) performance with much fewer computational resources and parameters. Our research highlights the potential of state space models as promising alternatives to the <b>transformer</b> and <b>CNN</b> architectures in serving as foundational frameworks for next-generation low-level visual tasks.</p></p class="citation"></blockquote><h3 id=7122--50350-videoagent-a-memory-augmented-multimodal-agent-for-video-understanding-yue-fan-et-al-2024>(7/122 | 50/350) VideoAgent: A Memory-augmented Multimodal Agent for Video Understanding (Yue Fan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yue Fan, Xiaojian Ma, Rujie Wu, Yuntao Du, Jiaqi Li, Zhi Gao, Qing Li. (2024)<br><strong>VideoAgent: A Memory-augmented Multimodal Agent for Video Understanding</strong><br><button class=copy-to-clipboard title="VideoAgent: A Memory-augmented Multimodal Agent for Video Understanding" index=50>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-50 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 69<br>Keywords: Benchmarking, Foundation Model, Multi-modal, Multi-modal, Zero-shot, Gemini, Large Language Model, Large Language Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11481v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11481v1.pdf filename=2403.11481v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We explore how reconciling several <b>foundation</b> <b>models</b> <b>(large</b> <b>language</b> <b>models</b> and <b>vision-language</b> models) with a novel unified memory mechanism could tackle the challenging video understanding problem, especially capturing the long-term temporal relations in lengthy videos. In particular, the proposed <b>multimodal</b> agent VideoAgent: 1) constructs a structured memory to store both the generic temporal event descriptions and object-centric tracking states of the video; 2) given an input task query, it employs tools including video segment localization and object memory querying along with other visual <b>foundation</b> <b>models</b> to interactively solve the task, utilizing the <b>zero-shot</b> tool-use ability of <b>LLMs.</b> VideoAgent demonstrates impressive performances on several long-horizon video understanding <b>benchmarks,</b> an average increase of 6.6% on NExT-QA and 26.0% on EgoSchema over baselines, closing the gap between open-sourced models and private counterparts including <b>Gemini</b> 1.5 Pro.</p></p class="citation"></blockquote><h3 id=8122--51350-geowizard-unleashing-the-diffusion-priors-for-3d-geometry-estimation-from-a-single-image-xiao-fu-et-al-2024>(8/122 | 51/350) GeoWizard: Unleashing the Diffusion Priors for 3D Geometry Estimation from a Single Image (Xiao Fu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiao Fu, Wei Yin, Mu Hu, Kaixuan Wang, Yuexin Ma, Ping Tan, Shaojie Shen, Dahua Lin, Xiaoxiao Long. (2024)<br><strong>GeoWizard: Unleashing the Diffusion Priors for 3D Geometry Estimation from a Single Image</strong><br><button class=copy-to-clipboard title="GeoWizard: Unleashing the Diffusion Priors for 3D Geometry Estimation from a Single Image" index=51>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-51 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 68<br>Keywords: Diffusion Model, Benchmarking, Convolutional Neural Network, Foundation Model, Geometry, Mutual Information, Zero-shot, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12013v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12013v1.pdf filename=2403.12013v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce GeoWizard, a new generative <b>foundation</b> <b>model</b> designed for estimating geometric attributes, e.g., depth and normals, from single images. While significant research has already been conducted in this area, the progress has been substantially limited by the low diversity and poor quality of publicly available datasets. As a result, the prior works either are constrained to limited scenarios or suffer from the inability to capture geometric details. In this paper, we demonstrate that generative models, as opposed to traditional discriminative models (e.g., <b>CNNs</b> and <b>Transformers),</b> can effectively address the inherently ill-posed problem. We further show that leveraging <b>diffusion</b> <b>priors</b> can markedly improve generalization, detail preservation, and efficiency in resource usage. Specifically, we extend the original stable <b>diffusion</b> <b>model</b> to jointly predict depth and normal, allowing <b>mutual</b> <b>information</b> exchange and high consistency between the two representations. More importantly, we propose a simple yet effective strategy to segregate the complex data distribution of various scenes into distinct sub-distributions. This strategy enables our model to recognize different scene layouts, capturing 3D <b>geometry</b> with remarkable fidelity. GeoWizard sets new <b>benchmarks</b> for <b>zero-shot</b> depth and normal prediction, significantly enhancing many downstream applications such as 3D reconstruction, 2D content creation, and novel viewpoint synthesis.</p></p class="citation"></blockquote><h3 id=9122--52350-meta-prompting-for-automating-zero-shot-visual-recognition-with-llms-m-jehanzeb-mirza-et-al-2024>(9/122 | 52/350) Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs (M. Jehanzeb Mirza et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>M. Jehanzeb Mirza, Leonid Karlinsky, Wei Lin, Sivan Doveh, Jakub Micorek, Mateusz Kozinski, Hilde Kuhene, Horst Possegger. (2024)<br><strong>Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs</strong><br><button class=copy-to-clipboard title="Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs" index=52>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-52 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 63<br>Keywords: Benchmarking, Zero-shot, GPT, Large Language Model, Large Language Model, Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11755v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11755v2.pdf filename=2403.11755v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Prompt</b> ensembling of <b>Large</b> <b>Language</b> <b>Model</b> <b>(LLM)</b> generated category-specific <b>prompts</b> has emerged as an effective method to enhance <b>zero-shot</b> recognition ability of <b>Vision-Language</b> Models (VLMs). To obtain these category-specific <b>prompts,</b> the present methods rely on hand-crafting the <b>prompts</b> to the <b>LLMs</b> for generating VLM <b>prompts</b> for the downstream tasks. However, this requires manually composing these task-specific <b>prompts</b> and still, they might not cover the diverse set of visual concepts and task-specific styles associated with the categories of interest. To effectively take humans out of the loop and completely automate the <b>prompt</b> generation process for <b>zero-shot</b> recognition, we propose Meta-Prompting for Visual Recognition (MPVR). Taking as input only minimal information about the target task, in the form of its short natural language description, and a list of associated class labels, MPVR automatically produces a diverse set of category-specific <b>prompts</b> resulting in a strong <b>zero-shot</b> classifier. MPVR generalizes effectively across various popular <b>zero-shot</b> image recognition <b>benchmarks</b> belonging to widely different domains when tested with multiple <b>LLMs</b> and VLMs. For example, MPVR obtains a <b>zero-shot</b> recognition improvement over CLIP by up to 19.8% and 18.2% (5.0% and 4.5% on average over 20 datasets) leveraging <b>GPT</b> and Mixtral <b>LLMs,</b> respectively</p></p class="citation"></blockquote><h3 id=10122--53350-one-step-image-translation-with-text-to-image-models-gaurav-parmar-et-al-2024>(10/122 | 53/350) One-Step Image Translation with Text-to-Image Models (Gaurav Parmar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gaurav Parmar, Taesung Park, Srinivasa Narasimhan, Jun-Yan Zhu. (2024)<br><strong>One-Step Image Translation with Text-to-Image Models</strong><br><button class=copy-to-clipboard title="One-Step Image Translation with Text-to-Image Models" index=53>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-53 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-GR, cs-LG, cs.CV<br>Keyword Score: 60<br>Keywords: Diffusion Model, Adversarial Learning, Fine-tuning, Generative Adversarial Network, Image2text, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12036v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12036v1.pdf filename=2403.12036v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we address two limitations of existing conditional <b>diffusion</b> <b>models:</b> their slow inference speed due to the iterative denoising process and their reliance on paired data for model <b>fine-tuning.</b> To tackle these issues, we introduce a general method for adapting a single-step <b>diffusion</b> <b>model</b> to new tasks and domains through <b>adversarial</b> <b>learning</b> objectives. Specifically, we consolidate various modules of the vanilla latent <b>diffusion</b> <b>model</b> into a single end-to-end generator network with small trainable weights, enhancing its ability to preserve the input <b>image</b> <b>structure</b> while reducing overfitting. We demonstrate that, for unpaired settings, our model CycleGAN-Turbo outperforms existing <b>GAN-based</b> and <b>diffusion-based</b> <b>methods</b> for various scene translation tasks, such as day-to-night conversion and adding/removing weather effects like fog, snow, and rain. We extend our method to paired settings, where our model pix2pix-Turbo is on par with recent works like Control-Net for Sketch2Photo and Edge2Image, but with a single-step inference. This work suggests that single-step <b>diffusion</b> <b>models</b> can serve as strong backbones for a range of <b>GAN</b> learning objectives. Our code and models are available at <a href=https://github.com/GaParmar/img2img-turbo>https://github.com/GaParmar/img2img-turbo</a>.</p></p class="citation"></blockquote><h3 id=11122--54350-idf-cr-iterative-diffusion-process-for-divide-and-conquer-cloud-removal-in-remote-sensing-images-meilin-wang-et-al-2024>(11/122 | 54/350) IDF-CR: Iterative Diffusion Process for Divide-and-Conquer Cloud Removal in Remote-sensing Images (Meilin Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Meilin Wang, Yexing Song, Pengxu Wei, Xiaoyu Xian, Yukai Shi, Liang Lin. (2024)<br><strong>IDF-CR: Iterative Diffusion Process for Divide-and-Conquer Cloud Removal in Remote-sensing Images</strong><br><button class=copy-to-clipboard title="IDF-CR: Iterative Diffusion Process for Divide-and-Conquer Cloud Removal in Remote-sensing Images" index=54>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-54 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV<br>Keyword Score: 60<br>Keywords: ControlNet, Diffusion Model, Convolution, Convolutional Neural Network, Convolutional Neural Network, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11870v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11870v1.pdf filename=2403.11870v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep learning technologies have demonstrated their effectiveness in removing cloud cover from optical remote-sensing images. <b>Convolutional</b> <b>Neural</b> <b>Networks</b> <b>(CNNs)</b> exert dominance in the cloud removal tasks. However, constrained by the inherent limitations of <b>convolutional</b> <b>operations,</b> <b>CNNs</b> can address only a modest fraction of cloud occlusion. In recent years, <b>diffusion</b> <b>models</b> have achieved state-of-the-art (SOTA) proficiency in image generation and reconstruction due to their formidable generative capabilities. Inspired by the rapid development of <b>diffusion</b> <b>models,</b> we first present an iterative <b>diffusion</b> <b>process</b> for cloud removal (IDF-CR), which exhibits a strong generative capabilities to achieve component divide-and-conquer cloud removal. IDF-CR consists of a pixel space cloud removal module (Pixel-CR) and a latent space iterative noise <b>diffusion</b> <b>network</b> (IND). Specifically, IDF-CR is divided into two-stage models that address pixel space and latent space. The two-stage model facilitates a strategic transition from preliminary cloud reduction to meticulous detail refinement. In the pixel space stage, Pixel-CR initiates the processing of cloudy images, yielding a suboptimal cloud removal prior to providing the <b>diffusion</b> <b>model</b> with prior cloud removal knowledge. In the latent space stage, the <b>diffusion</b> <b>model</b> transforms low-quality cloud removal into high-quality clean output. We refine the Stable <b>Diffusion</b> <b>by</b> implementing <b>ControlNet.</b> In addition, an <b>unsupervised</b> iterative noise refinement (INR) module is introduced for <b>diffusion</b> <b>model</b> to optimize the distribution of the predicted noise, thereby enhancing advanced detail recovery. Our model performs best with other SOTA methods, including image reconstruction and optical remote-sensing cloud removal on the optical remote-sensing datasets.</p></p class="citation"></blockquote><h3 id=12122--55350-agent3d-zero-an-agent-for-zero-shot-3d-understanding-sha-zhang-et-al-2024>(12/122 | 55/350) Agent3D-Zero: An Agent for Zero-shot 3D Understanding (Sha Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sha Zhang, Di Huang, Jiajun Deng, Shixiang Tang, Wanli Ouyang, Tong He, Yanyong Zhang. (2024)<br><strong>Agent3D-Zero: An Agent for Zero-shot 3D Understanding</strong><br><button class=copy-to-clipboard title="Agent3D-Zero: An Agent for Zero-shot 3D Understanding" index=55>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-55 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 60<br>Keywords: Fine-tuning, Zero-shot, Large Language Model, Large Language Model, Prompt, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11835v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11835v1.pdf filename=2403.11835v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The ability to understand and reason the 3D real world is a crucial milestone towards artificial general intelligence. The current common practice is to <b>finetune</b> <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> with 3D data and texts to enable 3D understanding. Despite their effectiveness, these approaches are inherently limited by the scale and diversity of the available 3D data. Alternatively, in this work, we introduce Agent3D-Zero, an innovative 3D-aware agent framework addressing the 3D scene understanding in a <b>zero-shot</b> manner. The essence of our approach centers on reconceptualizing the challenge of 3D scene perception as a process of understanding and synthesizing insights from multiple images, inspired by how our human beings attempt to understand 3D scenes. By consolidating this idea, we propose a novel way to make use of a <b>Large</b> <b>Visual</b> <b>Language</b> Model (VLM) via actively selecting and analyzing a series of viewpoints for 3D understanding. Specifically, given an input 3D scene, Agent3D-Zero first processes a bird&rsquo;s-eye view image with custom-designed visual <b>prompts,</b> then iteratively chooses the next viewpoints to observe and <b>summarize</b> the underlying knowledge. A distinctive advantage of Agent3D-Zero is the introduction of novel visual <b>prompts,</b> which significantly unleash the VLMs&rsquo; ability to identify the most informative viewpoints and thus facilitate observing 3D scenes. Extensive experiments demonstrate the effectiveness of the proposed framework in understanding diverse and previously unseen 3D environments.</p></p class="citation"></blockquote><h3 id=13122--56350-boosting-continuous-emotion-recognition-with-self-pretraining-using-masked-autoencoders-temporal-convolutional-networks-and-transformers-weiwei-zhou-et-al-2024>(13/122 | 56/350) Boosting Continuous Emotion Recognition with Self-Pretraining using Masked Autoencoders, Temporal Convolutional Networks, and Transformers (Weiwei Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Weiwei Zhou, Jiada Lu, Chenkun Ling, Weifeng Wang, Shaowei Liu. (2024)<br><strong>Boosting Continuous Emotion Recognition with Self-Pretraining using Masked Autoencoders, Temporal Convolutional Networks, and Transformers</strong><br><button class=copy-to-clipboard title="Boosting Continuous Emotion Recognition with Self-Pretraining using Masked Autoencoders, Temporal Convolutional Networks, and Transformers" index=56>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-56 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 60<br>Keywords: Autoencoder, Convolution, Convolutional Neural Network, Fine-tuning, Transformer, Emotion Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11440v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11440v1.pdf filename=2403.11440v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Human <b>emotion</b> <b>recognition</b> holds a pivotal role in facilitating seamless human-computer interaction. This paper delineates our methodology in tackling the Valence-Arousal (VA) Estimation Challenge, Expression (Expr) Classification Challenge, and Action Unit (AU) Detection Challenge within the ambit of the 6th Workshop and Competition on Affective Behavior Analysis in-the-wild (ABAW). Our study advocates a novel approach aimed at refining continuous <b>emotion</b> <b>recognition.</b> We achieve this by initially harnessing pre-training with Masked <b>Autoencoders</b> (MAE) on facial datasets, followed by <b>fine-tuning</b> on the aff-wild2 dataset annotated with expression (Expr) labels. The pre-trained model serves as an adept visual feature extractor, thereby enhancing the model&rsquo;s robustness. Furthermore, we bolster the performance of continuous <b>emotion</b> <b>recognition</b> by integrating Temporal <b>Convolutional</b> <b>Network</b> (TCN) modules and <b>Transformer</b> Encoder modules into our framework.</p></p class="citation"></blockquote><h3 id=14122--57350-ocr-is-all-you-need-importing-multi-modality-into-image-based-defect-detection-system-chih-chung-hsu-et-al-2024>(14/122 | 57/350) OCR is All you need: Importing Multi-Modality into Image-based Defect Detection System (Chih-Chung Hsu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chih-Chung Hsu, Chia-Ming Lee, Chun-Hung Sun, Kuang-Ming Wu. (2024)<br><strong>OCR is All you need: Importing Multi-Modality into Image-based Defect Detection System</strong><br><button class=copy-to-clipboard title="OCR is All you need: Importing Multi-Modality into Image-based Defect Detection System" index=57>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-57 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 53<br>Keywords: Optical Character Recognition, Optical Character Recognition, Graph Attention Networks, Convolution, Convolutional Neural Network, Sample Size<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11536v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11536v1.pdf filename=2403.11536v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Automatic <b>optical</b> <b>inspection</b> <b>(AOI)</b> plays a pivotal role in the manufacturing process, predominantly leveraging high-resolution imaging instruments for scanning purposes. It detects anomalies by analyzing image textures or patterns, making it an essential tool in industrial manufacturing and quality control. Despite its importance, the deployment of models for AOI often faces challenges. These include limited <b>sample</b> <b>sizes,</b> which hinder effective feature learning, variations among source domains, and sensitivities to changes in lighting and camera positions during imaging. These factors collectively compromise the accuracy of model predictions. Traditional AOI often fails to capitalize on the rich mechanism-parameter information from machines or inside images, including statistical parameters, which typically benefit AOI classification. To address this, we introduce an external modality-guided data mining framework, primarily rooted in <b>optical</b> <b>character</b> <b>recognition</b> <b>(OCR),</b> to extract statistical features from images as a second modality to enhance performance, termed OANet <b>(Ocr-Aoi-Net).</b> A key aspect of our approach is the alignment of external modality features, extracted using a single modality-aware model, with image features encoded by a <b>convolutional</b> <b>neural</b> <b>network.</b> This synergy enables a more refined fusion of semantic representations from different modalities. We further introduce feature refinement and a <b>gating</b> function in our OANet to optimize the combination of these features, enhancing inference and decision-making capabilities. Experimental outcomes show that our methodology considerably boosts the recall rate of the defect detection model and maintains high robustness even in challenging scenarios.</p></p class="citation"></blockquote><h3 id=15122--58350-hiri-vit-scaling-vision-transformer-with-high-resolution-inputs-ting-yao-et-al-2024>(15/122 | 58/350) HIRI-ViT: Scaling Vision Transformer with High Resolution Inputs (Ting Yao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ting Yao, Yehao Li, Yingwei Pan, Tao Mei. (2024)<br><strong>HIRI-ViT: Scaling Vision Transformer with High Resolution Inputs</strong><br><button class=copy-to-clipboard title="HIRI-ViT: Scaling Vision Transformer with High Resolution Inputs" index=58>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-58 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-MM, cs.CV<br>Keyword Score: 50<br>Keywords: Vision Transformer, Convolution, Convolutional Neural Network, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11999v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11999v1.pdf filename=2403.11999v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The hybrid deep models of <b>Vision</b> <b>Transformer</b> (ViT) and <b>Convolution</b> Neural Network <b>(CNN)</b> have emerged as a powerful class of backbones for <b>vision</b> <b>tasks.</b> Scaling up the input resolution of such hybrid backbones naturally strengthes model capacity, but inevitably suffers from heavy computational cost that scales quadratically. Instead, we present a new hybrid backbone with HIgh-Resolution Inputs (namely HIRI-ViT), that upgrades prevalent four-stage ViT to five-stage ViT tailored for high-resolution inputs. HIRI-ViT is built upon the seminal idea of decomposing the typical <b>CNN</b> operations into two parallel <b>CNN</b> branches in a cost-efficient manner. One high-resolution branch directly takes primary high-resolution features as inputs, but uses less <b>convolution</b> operations. The other low-resolution branch first performs down-sampling and then utilizes more <b>convolution</b> operations over such low-resolution features. Experiments on both recognition task (ImageNet-1K dataset) and dense prediction tasks (COCO and ADE20K datasets) demonstrate the superiority of HIRI-ViT. More remarkably, under comparable computational cost ($\sim$5.0 GFLOPs), HIRI-ViT achieves to-date the best published Top-1 accuracy of 84.3% on ImageNet with 448$\times$448 inputs, which absolutely improves 83.4% of iFormer-S by 0.9% with 224$\times$224 inputs.</p></p class="citation"></blockquote><h3 id=16122--59350-better-pseudo-labels-for-semi-supervised-instance-segmentation-françois-porcher-et-al-2024>(16/122 | 59/350) Better (pseudo-)labels for semi-supervised instance segmentation (François Porcher et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>François Porcher, Camille Couprie, Marc Szafraniec, Jakob Verbeek. (2024)<br><strong>Better (pseudo-)labels for semi-supervised instance segmentation</strong><br><button class=copy-to-clipboard title="Better (pseudo-)labels for semi-supervised instance segmentation" index=59>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-59 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Few-shot, Few-shot Learning, Knowledge Distillation, Supervised Learning, Image2text<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11675v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11675v1.pdf filename=2403.11675v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite the availability of large datasets for tasks like image classification and <b>image-text</b> alignment, labeled data for more complex recognition tasks, such as detection and segmentation, is less abundant. In particular, for instance segmentation annotations are time-consuming to produce, and the distribution of instances is often highly skewed across classes. While semi-supervised teacher-student <b>distillation</b> methods show promise in leveraging vast amounts of unlabeled data, they suffer from miscalibration, resulting in overconfidence in frequently represented classes and underconfidence in rarer ones. Additionally, these methods encounter difficulties in efficiently learning from a limited set of examples. We introduce a dual-strategy to enhance the teacher model&rsquo;s training process, substantially improving the performance on <b>few-shot</b> <b>learning.</b> Secondly, we propose a calibration correction mechanism that that enables the student model to correct the teacher&rsquo;s calibration errors. Using our approach, we observed marked improvements over a state-of-the-art <b>supervised</b> baseline performance on the LVIS dataset, with an increase of 2.8% in average precision (AP) and 10.3% gain in AP for rare classes.</p></p class="citation"></blockquote><h3 id=17122--60350-binary-noise-for-binary-tasks-masked-bernoulli-diffusion-for-unsupervised-anomaly-detection-julia-wolleb-et-al-2024>(17/122 | 60/350) Binary Noise for Binary Tasks: Masked Bernoulli Diffusion for Unsupervised Anomaly Detection (Julia Wolleb et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Julia Wolleb, Florentin Bieder, Paul Friedrich, Peter Zhang, Alicia Durrer, Philippe C. Cattin. (2024)<br><strong>Binary Noise for Binary Tasks: Masked Bernoulli Diffusion for Unsupervised Anomaly Detection</strong><br><button class=copy-to-clipboard title="Binary Noise for Binary Tasks: Masked Bernoulli Diffusion for Unsupervised Anomaly Detection" index=60>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-60 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV<br>Keyword Score: 50<br>Keywords: Diffusion Model, Anomaly Detection, Autoencoder, Out-of-distribution, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11667v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11667v1.pdf filename=2403.11667v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The high performance of denoising <b>diffusion</b> <b>models</b> for image generation has paved the way for their application in <b>unsupervised</b> medical <b>anomaly</b> <b>detection.</b> As <b>diffusion-based</b> <b>methods</b> require a lot of GPU memory and have long sampling times, we present a novel and fast <b>unsupervised</b> <b>anomaly</b> <b>detection</b> approach based on latent Bernoulli <b>diffusion</b> <b>models.</b> We first apply an <b>autoencoder</b> to compress the input images into a binary latent representation. Next, a <b>diffusion</b> <b>model</b> that follows a Bernoulli noise schedule is employed to this latent space and trained to restore binary latent representations from perturbed ones. The binary nature of this <b>diffusion</b> <b>model</b> allows us to identify entries in the latent space that have a high probability of flipping their binary code during the denoising process, which indicates <b>out-of-distribution</b> data. We propose a masking algorithm based on these probabilities, which improves the <b>anomaly</b> <b>detection</b> scores. We achieve state-of-the-art performance compared to other <b>diffusion-based</b> <b>unsupervised</b> <b>anomaly</b> <b>detection</b> algorithms while significantly reducing sampling time and memory consumption. The code is available at <a href=https://github.com/JuliaWolleb/Anomaly_berdiff>https://github.com/JuliaWolleb/Anomaly_berdiff</a>.</p></p class="citation"></blockquote><h3 id=18122--61350-ssap-a-shape-sensitive-adversarial-patch-for-comprehensive-disruption-of-monocular-depth-estimation-in-autonomous-navigation-applications-amira-guesmi-et-al-2024>(18/122 | 61/350) SSAP: A Shape-Sensitive Adversarial Patch for Comprehensive Disruption of Monocular Depth Estimation in Autonomous Navigation Applications (Amira Guesmi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amira Guesmi, Muhammad Abdullah Hanif, Ihsen Alouani, Bassem Ouni, Muhammad Shafique. (2024)<br><strong>SSAP: A Shape-Sensitive Adversarial Patch for Comprehensive Disruption of Monocular Depth Estimation in Autonomous Navigation Applications</strong><br><button class=copy-to-clipboard title="SSAP: A Shape-Sensitive Adversarial Patch for Comprehensive Disruption of Monocular Depth Estimation in Autonomous Navigation Applications" index=61>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-61 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV<br>Keyword Score: 50<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, Transformer, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11515v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11515v1.pdf filename=2403.11515v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Monocular depth estimation (MDE) has advanced significantly, primarily through the integration of <b>convolutional</b> <b>neural</b> <b>networks</b> <b>(CNNs)</b> and more recently, <b>Transformers.</b> However, concerns about their susceptibility to <b>adversarial</b> <b>attacks</b> have emerged, especially in safety-critical domains like autonomous driving and robotic navigation. Existing approaches for assessing <b>CNN-based</b> depth prediction methods have fallen short in inducing comprehensive disruptions to the vision system, often limited to specific local areas. In this paper, we introduce SSAP (Shape-Sensitive <b>Adversarial</b> <b>Patch),</b> a novel approach designed to comprehensively disrupt monocular depth estimation (MDE) in autonomous navigation applications. Our patch is crafted to selectively undermine MDE in two distinct ways: by distorting estimated distances or by creating the illusion of an object disappearing from the system&rsquo;s perspective. Notably, our patch is shape-sensitive, meaning it considers the specific shape and scale of the target object, thereby extending its influence beyond immediate proximity. Furthermore, our patch is trained to effectively address different scales and distances from the camera. Experimental results demonstrate that our approach induces a mean depth estimation error surpassing 0.5, impacting up to 99% of the targeted region for <b>CNN-based</b> MDE models. Additionally, we investigate the vulnerability of <b>Transformer-based</b> MDE models to patch-based attacks, revealing that SSAP yields a significant error of 0.59 and exerts substantial influence over 99% of the target region on these models.</p></p class="citation"></blockquote><h3 id=19122--62350-siamese-learning-with-joint-alignment-and-regression-for-weakly-supervised-video-paragraph-grounding-chaolei-tan-et-al-2024>(19/122 | 62/350) Siamese Learning with Joint Alignment and Regression for Weakly-Supervised Video Paragraph Grounding (Chaolei Tan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chaolei Tan, Jianhuang Lai, Wei-Shi Zheng, Jian-Fang Hu. (2024)<br><strong>Siamese Learning with Joint Alignment and Regression for Weakly-Supervised Video Paragraph Grounding</strong><br><button class=copy-to-clipboard title="Siamese Learning with Joint Alignment and Regression for Weakly-Supervised Video Paragraph Grounding" index=62>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-62 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Multiple Instance Learning, Semi-Supervised Learning, Weakly-supervised Learning, Transformer, Grounding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11463v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11463v1.pdf filename=2403.11463v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Video Paragraph <b>Grounding</b> (VPG) is an emerging task in video-language understanding, which aims at localizing <b>multiple</b> <b>sentences</b> <b>with</b> semantic relations and temporal order from an untrimmed video. However, existing VPG approaches are heavily reliant on a considerable number of temporal labels that are laborious and time-consuming to acquire. In this work, we introduce and explore <b>Weakly-Supervised</b> Video Paragraph <b>Grounding</b> (WSVPG) to eliminate the need of temporal annotations. Different from previous <b>weakly-supervised</b> <b>grounding</b> frameworks based on <b>multiple</b> <b>instance</b> <b>learning</b> or reconstruction learning for two-stage candidate ranking, we propose a novel siamese learning framework that jointly learns the cross-modal feature alignment and temporal coordinate regression without timestamp labels to achieve concise one-stage localization for WSVPG. Specifically, we devise a Siamese <b>Grounding</b> <b>TRansformer</b> (SiamGTR) consisting of two weight-sharing branches for learning complementary supervision. An Augmentation Branch is utilized for directly regressing the temporal boundaries of a complete paragraph within a pseudo video, and an Inference Branch is designed to capture the order-guided feature correspondence for localizing <b>multiple</b> <b>sentences</b> <b>in</b> a normal video. We demonstrate by extensive experiments that our paradigm has superior practicability and flexibility to achieve efficient <b>weakly-supervised</b> or <b>semi-supervised</b> <b>learning,</b> outperforming state-of-the-art methods trained with the same or stronger supervision.</p></p class="citation"></blockquote><h3 id=20122--63350-minedreamer-learning-to-follow-instructions-via-chain-of-imagination-for-simulated-world-control-enshen-zhou-et-al-2024>(20/122 | 63/350) MineDreamer: Learning to Follow Instructions via Chain-of-Imagination for Simulated-World Control (Enshen Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Enshen Zhou, Yiran Qin, Zhenfei Yin, Yuzhou Huang, Ruimao Zhang, Lu Sheng, Yu Qiao, Jing Shao. (2024)<br><strong>MineDreamer: Learning to Follow Instructions via Chain-of-Imagination for Simulated-World Control</strong><br><button class=copy-to-clipboard title="MineDreamer: Learning to Follow Instructions via Chain-of-Imagination for Simulated-World Control" index=63>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-63 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 46<br>Keywords: Diffusion Model, Multi-modal, Multi-modal, Instruction Following, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12037v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12037v2.pdf filename=2403.12037v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>It is a long-lasting goal to design a generalist-embodied agent that can follow diverse <b>instructions</b> <b>in</b> human-like ways. However, existing approaches often fail to steadily follow <b>instructions</b> <b>due</b> to difficulties in understanding abstract and sequential natural language <b>instructions.</b> <b>To</b> this end, we introduce MineDreamer, an open-ended embodied agent built upon the challenging Minecraft simulator with an innovative paradigm that enhances <b>instruction-following</b> <b>ability</b> in low-level control signal generation. Specifically, MineDreamer is developed on top of recent advances in <b>Multimodal</b> <b>Large</b> <b>Language</b> <b>Models</b> (MLLMs) and <b>diffusion</b> <b>models,</b> and we employ a Chain-of-Imagination (CoI) mechanism to envision the step-by-step process of executing <b>instructions</b> <b>and</b> translating imaginations into more precise visual <b>prompts</b> tailored to the current state; subsequently, the agent generates keyboard-and-mouse actions to efficiently achieve these imaginations, steadily following the <b>instructions</b> <b>at</b> each step. Extensive experiments demonstrate that MineDreamer follows single and multi-step <b>instructions</b> <b>steadily,</b> significantly outperforming the best generalist agent baseline and nearly doubling its performance. Moreover, qualitative analysis of the agent&rsquo;s imaginative ability reveals its generalization and comprehension of the open world.</p></p class="citation"></blockquote><h3 id=21122--64350-hvdistill-transferring-knowledge-from-images-to-point-clouds-via-unsupervised-hybrid-view-distillation-sha-zhang-et-al-2024>(21/122 | 64/350) HVDistill: Transferring Knowledge from Images to Point Clouds via Unsupervised Hybrid-View Distillation (Sha Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sha Zhang, Jiajun Deng, Lei Bai, Houqiang Li, Wanli Ouyang, Yanyong Zhang. (2024)<br><strong>HVDistill: Transferring Knowledge from Images to Point Clouds via Unsupervised Hybrid-View Distillation</strong><br><button class=copy-to-clipboard title="HVDistill: Transferring Knowledge from Images to Point Clouds via Unsupervised Hybrid-View Distillation" index=64>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-64 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 45<br>Keywords: Knowledge Distillation, Knowledge Distillation, Representation Learning, Self-supervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11817v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11817v1.pdf filename=2403.11817v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a hybrid-view-based <b>knowledge</b> <b>distillation</b> framework, termed HVDistill, to guide the feature learning of a point cloud neural network with a pre-trained image network in an <b>unsupervised</b> manner. By exploiting the geometric relationship between RGB cameras and LiDAR sensors, the correspondence between the two modalities based on both image-plane view and bird-eye view can be established, which facilitates <b>representation</b> <b>learning.</b> Specifically, the image-plane correspondences can be simply obtained by projecting the point clouds, while the bird-eye-view correspondences can be achieved by lifting pixels to the 3D space with the predicted depths under the supervision of projected point clouds. The image teacher networks provide rich semantics from the image-plane view and meanwhile acquire geometric information from the bird-eye view. Indeed, image features from the two views naturally complement each other and together can ameliorate the learned feature <b>representation</b> <b>of</b> the point cloud student networks. Moreover, with a <b>self-supervised</b> pre-trained 2D network, HVDistill requires neither 2D nor 3D annotations. We pre-train our model on nuScenes dataset and transfer it to several downstream tasks on nuScenes, SemanticKITTI, and KITTI datasets for evaluation. Extensive experimental results show that our method achieves consistent improvements over the baseline trained from scratch and significantly outperforms the existing schemes. Codes are available at <a href=mailto:git@github.com>git@github.com</a>:zhangsha1024/HVDistill.git.</p></p class="citation"></blockquote><h3 id=22122--65350-evaluating-text-to-image-synthesis-survey-and-taxonomy-of-image-quality-metrics-sebastian-hartwig-et-al-2024>(22/122 | 65/350) Evaluating Text to Image Synthesis: Survey and Taxonomy of Image Quality Metrics (Sebastian Hartwig et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sebastian Hartwig, Dominik Engel, Leon Sick, Hannah Kniesel, Tristan Payer, Poonam, Timo Ropinski. (2024)<br><strong>Evaluating Text to Image Synthesis: Survey and Taxonomy of Image Quality Metrics</strong><br><button class=copy-to-clipboard title="Evaluating Text to Image Synthesis: Survey and Taxonomy of Image Quality Metrics" index=65>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-65 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-GR, cs.CV<br>Keyword Score: 43<br>Keywords: Benchmarking, Foundation Model, Text2image, Text2image, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11821v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11821v1.pdf filename=2403.11821v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advances in <b>text-to-image</b> synthesis have been enabled by exploiting a combination of language and vision through <b>foundation</b> <b>models.</b> These models are pre-trained on tremendous amounts of <b>text-image</b> pairs sourced from the World Wide Web or other large-scale databases. As the demand for high-quality image generation shifts towards ensuring content alignment between text and image, novel evaluation metrics have been developed with the aim of mimicking human judgments. Thus, researchers have started to collect datasets with increasingly complex annotations to study the compositionality of <b>vision-language</b> models and their incorporation as a quality measure of compositional alignment between text and image contents. In this work, we provide a comprehensive overview of existing <b>text-to-image</b> evaluation metrics and propose a new taxonomy for categorizing these metrics. We also review frequently adopted <b>text-image</b> <b>benchmark</b> datasets before discussing techniques to optimize <b>text-to-image</b> synthesis models towards quality and human preferences. Ultimately, we derive guidelines for improving <b>text-to-image</b> evaluation and discuss the open challenges and current limitations.</p></p class="citation"></blockquote><h3 id=23122--66350-dynamic-tuning-towards-parameter-and-inference-efficiency-for-vit-adaptation-wangbo-zhao-et-al-2024>(23/122 | 66/350) Dynamic Tuning Towards Parameter and Inference Efficiency for ViT Adaptation (Wangbo Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wangbo Zhao, Jiasheng Tang, Yizeng Han, Yibing Song, Kai Wang, Gao Huang, Fan Wang, Yang You. (2024)<br><strong>Dynamic Tuning Towards Parameter and Inference Efficiency for ViT Adaptation</strong><br><button class=copy-to-clipboard title="Dynamic Tuning Towards Parameter and Inference Efficiency for ViT Adaptation" index=66>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-66 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 43<br>Keywords: Vision Transformer, Benchmarking, Fine-tuning, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11808v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11808v1.pdf filename=2403.11808v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing parameter-efficient <b>fine-tuning</b> (PEFT) methods have achieved significant success on <b>vision</b> <b>transformers</b> (ViTs) adaptation by improving parameter efficiency. However, the exploration of enhancing inference efficiency during adaptation remains underexplored. This limits the broader application of pre-trained ViT models, especially when the model is computationally extensive. In this paper, we propose Dynamic Tuning (DyT), a novel approach to improve both parameter and inference efficiency for ViT adaptation. Specifically, besides using the lightweight adapter modules, we propose a token dispatcher to distinguish informative tokens from less important ones, allowing the latter to dynamically skip the original block, thereby reducing the redundant computation during inference. Additionally, we explore multiple design variants to find the best practice of DyT. Finally, inspired by the mixture-of-experts (MoE) mechanism, we introduce an enhanced adapter to further boost the adaptation performance. We validate DyT across various tasks, including image/video recognition and semantic segmentation. For instance, DyT achieves comparable or even superior performance compared to existing PEFT methods while evoking only 71%-85% of their FLOPs on the VTAB-1K <b>benchmark.</b></p></p class="citation"></blockquote><h3 id=24122--67350-ln3diff-scalable-latent-neural-fields-diffusion-for-speedy-3d-generation-yushi-lan-et-al-2024>(24/122 | 67/350) LN3Diff: Scalable Latent Neural Fields Diffusion for Speedy 3D Generation (Yushi Lan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yushi Lan, Fangzhou Hong, Shuai Yang, Shangchen Zhou, Xuyi Meng, Bo Dai, Xingang Pan, Chen Change Loy. (2024)<br><strong>LN3Diff: Scalable Latent Neural Fields Diffusion for Speedy 3D Generation</strong><br><button class=copy-to-clipboard title="LN3Diff: Scalable Latent Neural Fields Diffusion for Speedy 3D Generation" index=67>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-67 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Diffusion Model, Autoencoder, Variational Autoencoder, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12019v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12019v1.pdf filename=2403.12019v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The field of neural rendering has witnessed significant progress with advancements in generative models and differentiable rendering techniques. Though 2D <b>diffusion</b> <b>has</b> achieved success, a unified 3D <b>diffusion</b> <b>pipeline</b> remains unsettled. This paper introduces a novel framework called LN3Diff to address this gap and enable fast, high-quality, and generic conditional 3D generation. Our approach harnesses a 3D-aware architecture and <b>variational</b> <b>autoencoder</b> (VAE) to encode the input image into a structured, compact, and 3D latent space. The latent is decoded by a <b>transformer-based</b> decoder into a high-capacity 3D neural field. Through training a <b>diffusion</b> <b>model</b> on this 3D-aware latent space, our method achieves state-of-the-art performance on ShapeNet for 3D generation and demonstrates superior performance in monocular 3D reconstruction and conditional 3D generation across various datasets. Moreover, it surpasses existing 3D <b>diffusion</b> <b>methods</b> in terms of inference speed, requiring no per-instance optimization. Our proposed LN3Diff presents a significant advancement in 3D generative modeling and holds promise for various applications in 3D vision and graphics tasks.</p></p class="citation"></blockquote><h3 id=25122--68350-videomv-consistent-multi-view-generation-based-on-large-video-generative-model-qi-zuo-et-al-2024>(25/122 | 68/350) VideoMV: Consistent Multi-View Generation Based on Large Video Generative Model (Qi Zuo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qi Zuo, Xiaodong Gu, Lingteng Qiu, Yuan Dong, Zhengyi Zhao, Weihao Yuan, Rui Peng, Siyu Zhu, Zilong Dong, Liefeng Bo, Qixing Huang. (2024)<br><strong>VideoMV: Consistent Multi-View Generation Based on Large Video Generative Model</strong><br><button class=copy-to-clipboard title="VideoMV: Consistent Multi-View Generation Based on Large Video Generative Model" index=68>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-68 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-GR, cs.CV<br>Keyword Score: 40<br>Keywords: Diffusion Model, Fine-tuning, Fine-tuning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12010v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12010v1.pdf filename=2403.12010v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Generating multi-view images based on text or single-image <b>prompts</b> is a critical capability for the creation of 3D content. Two fundamental questions on this topic are what data we use for training and how to ensure multi-view consistency. This paper introduces a novel framework that makes fundamental contributions to both questions. Unlike leveraging images from 2D <b>diffusion</b> <b>models</b> for training, we propose a dense consistent multi-view generation model that is <b>fine-tuned</b> from off-the-shelf video generative models. Images from video generative models are more suitable for multi-view generation because the underlying network architecture that generates them employs a temporal module to enforce frame consistency. Moreover, the video data sets used to train these models are abundant and diverse, leading to a reduced train-finetuning domain gap. To enhance multi-view consistency, we introduce a 3D-Aware Denoising Sampling, which first employs a feed-forward reconstruction module to get an explicit global 3D model, and then adopts a sampling strategy that effectively involves images rendered from the global 3D model into the denoising sampling loop to improve the multi-view consistency of the final images. As a by-product, this module also provides a fast way to create 3D assets represented by 3D Gaussians within a few seconds. Our approach can generate 24 dense views and converges much faster in training than state-of-the-art approaches (4 GPU hours versus many thousand GPU hours) with comparable visual quality and consistency. By further <b>fine-tuning,</b> our approach outperforms existing state-of-the-art methods in both quantitative metrics and visual effects. Our project page is aigc3d.github.io/VideoMV.</p></p class="citation"></blockquote><h3 id=26122--69350-genview-enhancing-view-quality-with-pretrained-generative-model-for-self-supervised-learning-xiaojie-li-et-al-2024>(26/122 | 69/350) GenView: Enhancing View Quality with Pretrained Generative Model for Self-Supervised Learning (Xiaojie Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaojie Li, Yibo Yang, Xiangtai Li, Jianlong Wu, Yue Yu, Bernard Ghanem, Min Zhang. (2024)<br><strong>GenView: Enhancing View Quality with Pretrained Generative Model for Self-Supervised Learning</strong><br><button class=copy-to-clipboard title="GenView: Enhancing View Quality with Pretrained Generative Model for Self-Supervised Learning" index=69>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-69 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Contrastive Learning, Data Augmentation, Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12003v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12003v1.pdf filename=2403.12003v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Self-supervised</b> <b>learning</b> has achieved remarkable success in acquiring high-quality representations from unlabeled <b>data.</b> <b>The</b> widely adopted <b>contrastive</b> <b>learning</b> framework aims to learn invariant representations by minimizing the distance between positive views originating from the same image. However, existing techniques to construct positive views highly rely on manual transformations, resulting in limited diversity and potentially false positive pairs. To tackle these challenges, we present GenView, a controllable framework that augments the diversity of positive views leveraging the power of pretrained generative models while preserving semantics. We develop an adaptive view generation method that dynamically adjusts the noise level in sampling to ensure the preservation of essential semantic meaning while introducing variability. Additionally, we introduce a quality-driven <b>contrastive</b> <b>loss,</b> which assesses the quality of positive pairs by considering both foreground similarity and background diversity. This loss prioritizes the high-quality positive pairs we construct while reducing the influence of low-quality pairs, thereby mitigating potential semantic inconsistencies introduced by generative models and aggressive <b>data</b> <b>augmentation.</b> Thanks to the improved positive view quality and the quality-driven <b>contrastive</b> <b>loss,</b> GenView significantly improves <b>self-supervised</b> <b>learning</b> across various tasks. For instance, GenView improves MoCov2 performance by 2.5%/2.2% on ImageNet linear/semi-supervised classification. Moreover, GenView even performs much better than naively augmenting the ImageNet dataset with Laion400M or ImageNet21K. Code is available at <a href=https://github.com/xiaojieli0903/genview>https://github.com/xiaojieli0903/genview</a>.</p></p class="citation"></blockquote><h3 id=27122--70350-superlora-parameter-efficient-unified-adaptation-of-multi-layer-attention-modules-xiangyu-chen-et-al-2024>(27/122 | 70/350) SuperLoRA: Parameter-Efficient Unified Adaptation of Multi-Layer Attention Modules (Xiangyu Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiangyu Chen, Jing Liu, Ye Wang, Pu Perry Wang, Matthew Brand, Guanghui Wang, Toshiaki Koike-Akino. (2024)<br><strong>SuperLoRA: Parameter-Efficient Unified Adaptation of Multi-Layer Attention Modules</strong><br><button class=copy-to-clipboard title="SuperLoRA: Parameter-Efficient Unified Adaptation of Multi-Layer Attention Modules" index=70>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-70 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 40<br>Keywords: Diffusion Model, Fine-tuning, Transfer Learning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11887v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11887v1.pdf filename=2403.11887v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Low-rank adaptation (LoRA) and its variants are widely employed in <b>fine-tuning</b> <b>large</b> <b>models,</b> <b>including</b> <b>large</b> <b>language</b> <b>models</b> for natural language processing and <b>diffusion</b> <b>models</b> for computer vision. This paper proposes a generalized framework called SuperLoRA that unifies and extends different LoRA variants, which can be realized under different hyper-parameter settings. Introducing grouping, folding, shuffling, projecting, and tensor factoring, SuperLoRA offers high flexibility compared with other LoRA variants and demonstrates superior performance for <b>transfer</b> <b>learning</b> tasks especially in the extremely few-parameter regimes.</p></p class="citation"></blockquote><h3 id=28122--71350-boosting-continual-learning-of-vision-language-models-via-mixture-of-experts-adapters-jiazuo-yu-et-al-2024>(28/122 | 71/350) Boosting Continual Learning of Vision-Language Models via Mixture-of-Experts Adapters (Jiazuo Yu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiazuo Yu, Yunzhi Zhuge, Lu Zhang, Dong Wang, Huchuan Lu, You He. (2024)<br><strong>Boosting Continual Learning of Vision-Language Models via Mixture-of-Experts Adapters</strong><br><button class=copy-to-clipboard title="Boosting Continual Learning of Vision-Language Models via Mixture-of-Experts Adapters" index=71>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-71 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Continual Learning, Out-of-distribution, Zero-shot, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11549v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11549v1.pdf filename=2403.11549v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Continual</b> <b>learning</b> can empower <b>vision-language</b> models to continuously acquire new knowledge, without the need for access to the entire historical dataset. However, mitigating the performance degradation in large-scale models is non-trivial due to (i) parameter shifts throughout lifelong learning and (ii) significant computational burdens associated with full-model tuning. In this work, we present a parameter-efficient <b>continual</b> <b>learning</b> framework to alleviate long-term forgetting in incremental learning with <b>vision-language</b> models. Our approach involves the dynamic expansion of a pre-trained CLIP model, through the integration of Mixture-of-Experts (MoE) adapters in response to new tasks. To preserve the <b>zero-shot</b> recognition capability of <b>vision-language</b> models, we further introduce a Distribution Discriminative Auto-Selector (DDAS) that automatically routes in-distribution and <b>out-of-distribution</b> inputs to the MoE Adapter and the original CLIP, respectively. Through extensive experiments across various settings, our proposed method consistently outperforms previous state-of-the-art approaches while concurrently reducing parameter training burdens by 60%. Our code locates at <a href=https://github.com/JiazuoYu/MoE-Adapters4CL>https://github.com/JiazuoYu/MoE-Adapters4CL</a></p></p class="citation"></blockquote><h3 id=29122--72350-continual-forgetting-for-pre-trained-vision-models-hongbo-zhao-et-al-2024>(29/122 | 72/350) Continual Forgetting for Pre-trained Vision Models (Hongbo Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hongbo Zhao, Bolin Ni, Haochen Wang, Junsong Fan, Fei Zhu, Yuxi Wang, Yuntao Chen, Gaofeng Meng, Zhaoxiang Zhang. (2024)<br><strong>Continual Forgetting for Pre-trained Vision Models</strong><br><button class=copy-to-clipboard title="Continual Forgetting for Pre-trained Vision Models" index=72>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-72 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Face Recognition, Object Detection, Fine-tuning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11530v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11530v1.pdf filename=2403.11530v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>For privacy and security concerns, the need to erase unwanted information from pre-trained vision models is becoming evident nowadays. In real-world scenarios, erasure requests originate at any time from both users and model owners. These requests usually form a sequence. Therefore, under such a setting, selective information is expected to be continuously removed from a pre-trained model while maintaining the rest. We define this problem as continual forgetting and identify two key challenges. (i) For unwanted knowledge, efficient and effective deleting is crucial. (ii) For remaining knowledge, the impact brought by the forgetting procedure should be minimal. To address them, we propose Group Sparse LoRA (GS-LoRA). Specifically, towards (i), we use LoRA modules to <b>fine-tune</b> the FFN layers in <b>Transformer</b> blocks for each forgetting task independently, and towards (ii), a simple group sparse regularization is adopted, enabling automatic selection of specific LoRA groups and zeroing out the others. GS-LoRA is effective, parameter-efficient, data-efficient, and easy to implement. We conduct extensive experiments on <b>face</b> <b>recognition,</b> <b>object</b> <b>detection</b> and image classification and demonstrate that GS-LoRA manages to forget specific classes with minimal impact on other classes. Codes will be released on \url{https://github.com/bjzhb666/GS-LoRA}.</p></p class="citation"></blockquote><h3 id=30122--73350-scene-llm-extending-language-model-for-3d-visual-understanding-and-reasoning-rao-fu-et-al-2024>(30/122 | 73/350) Scene-LLM: Extending Language Model for 3D Visual Understanding and Reasoning (Rao Fu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rao Fu, Jingyu Liu, Xilun Chen, Yixin Nie, Wenhan Xiong. (2024)<br><strong>Scene-LLM: Extending Language Model for 3D Visual Understanding and Reasoning</strong><br><button class=copy-to-clipboard title="Scene-LLM: Extending Language Model for 3D Visual Understanding and Reasoning" index=73>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-73 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Question Answering, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11401v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11401v1.pdf filename=2403.11401v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces Scene-LLM, a 3D-visual-language model that enhances embodied agents&rsquo; abilities in interactive 3D indoor environments by integrating the <b>reasoning</b> strengths of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> Scene-LLM adopts a hybrid 3D visual feature representation, that incorporates dense spatial information and supports scene state updates. The model employs a projection layer to efficiently project these features in the pre-trained textual embedding space, enabling effective interpretation of 3D visual information. Unique to our approach is the integration of both scene-level and ego-centric 3D information. This combination is pivotal for interactive planning, where scene-level data supports global planning and ego-centric data is important for localization. Notably, we use ego-centric 3D frame features for feature alignment, an efficient technique that enhances the model&rsquo;s ability to align features of small objects within the scene. Our experiments with Scene-LLM demonstrate its strong capabilities in dense captioning, <b>question</b> <b>answering,</b> and interactive planning. We believe Scene-LLM advances the field of 3D visual understanding and <b>reasoning,</b> offering new possibilities for sophisticated agent interactions in indoor settings.</p></p class="citation"></blockquote><h3 id=31122--74350-urban-scene-diffusion-through-semantic-occupancy-map-junge-zhang-et-al-2024>(31/122 | 74/350) Urban Scene Diffusion through Semantic Occupancy Map (Junge Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junge Zhang, Qihang Zhang, Li Zhang, Ramana Rao Kompella, Gaowen Liu, Bolei Zhou. (2024)<br><strong>Urban Scene Diffusion through Semantic Occupancy Map</strong><br><button class=copy-to-clipboard title="Urban Scene Diffusion through Semantic Occupancy Map" index=74>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-74 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 35<br>Keywords: Diffusion Model, Geometry, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11697v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11697v2.pdf filename=2403.11697v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Generating unbounded 3D scenes is crucial for large-scale scene understanding and <b>simulation.</b> Urban scenes, unlike natural landscapes, consist of various complex man-made objects and structures such as roads, traffic signs, vehicles, and buildings. To create a realistic and detailed urban scene, it is crucial to accurately represent the <b>geometry</b> and semantics of the underlying objects, going beyond their visual appearance. In this work, we propose UrbanDiffusion, a 3D <b>diffusion</b> <b>model</b> that is conditioned on a Bird&rsquo;s-Eye View (BEV) map and generates an urban scene with <b>geometry</b> and semantics in the form of semantic occupancy map. Our model introduces a novel paradigm that learns the data distribution of scene-level structures within a latent space and further enables the expansion of the synthesized scene into an arbitrary scale. After training on real-world driving datasets, our model can generate a wide range of diverse urban scenes given the BEV maps from the held-out set and also generalize to the synthesized maps from a driving simulator. We further demonstrate its application to scene image synthesis with a pretrained image generator as a prior.</p></p class="citation"></blockquote><h3 id=32122--75350-decotr-enhancing-depth-completion-with-2d-and-3d-attentions-yunxiao-shi-et-al-2024>(32/122 | 75/350) DeCoTR: Enhancing Depth Completion with 2D and 3D Attentions (Yunxiao Shi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yunxiao Shi, Manish Kumar Singh, Hong Cai, Fatih Porikli. (2024)<br><strong>DeCoTR: Enhancing Depth Completion with 2D and 3D Attentions</strong><br><button class=copy-to-clipboard title="DeCoTR: Enhancing Depth Completion with 2D and 3D Attentions" index=75>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-75 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Benchmarking, Convolution, Zero-shot, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12202v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12202v1.pdf filename=2403.12202v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we introduce a novel approach that harnesses both 2D and 3D attentions to enable highly accurate depth completion without requiring iterative spatial propagations. Specifically, we first enhance a baseline <b>convolutional</b> depth completion model by applying attention to 2D features in the bottleneck and skip connections. This effectively improves the performance of this simple network and sets it on par with the latest, complex <b>transformer-based</b> models. Leveraging the initial depths and features from this network, we uplift the 2D features to form a 3D point cloud and construct a 3D point <b>transformer</b> to process it, allowing the model to explicitly learn and exploit 3D geometric features. In addition, we propose normalization techniques to process the point cloud, which improves learning and leads to better accuracy than directly using point <b>transformers</b> off the shelf. Furthermore, we incorporate global attention on downsampled point cloud features, which enables long-range context while still being computationally feasible. We evaluate our method, DeCoTR, on established depth completion <b>benchmarks,</b> including NYU Depth V2 and KITTI, showcasing that it sets new state-of-the-art performance. We further conduct <b>zero-shot</b> evaluations on ScanNet and DDAD <b>benchmarks</b> and demonstrate that DeCoTR has superior generalizability compared to existing approaches.</p></p class="citation"></blockquote><h3 id=33122--76350-end-to-end-multi-modal-product-matching-in-fashion-e-commerce-sándor-tóth-et-al-2024>(33/122 | 76/350) End-to-end multi-modal product matching in fashion e-commerce (Sándor Tóth et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sándor Tóth, Stephen Wilson, Alexia Tsoukara, Enric Moreu, Anton Masalovich, Lars Roemheld. (2024)<br><strong>End-to-end multi-modal product matching in fashion e-commerce</strong><br><button class=copy-to-clipboard title="End-to-end multi-modal product matching in fashion e-commerce" index=76>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-76 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Contrastive Learning, Distribution Shift, Distribution Shift, Multi-modal, human-in-the-loop<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11593v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11593v1.pdf filename=2403.11593v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Product matching, the task of identifying different representations of the same product for better discoverability, curation, and pricing, is a key capability for online marketplace and e-commerce companies. We present a robust <b>multi-modal</b> product matching system in an industry setting, where large datasets, data <b>distribution</b> <b>shifts</b> and unseen domains pose challenges. We compare different approaches and conclude that a relatively straightforward projection of pretrained image and text encoders, trained through <b>contrastive</b> <b>learning,</b> yields state-of-the-art results, while balancing cost and performance. Our solution outperforms single modality matching systems and large pretrained models, such as CLIP. Furthermore we show how a <b>human-in-the-loop</b> process can be combined with model-based predictions to achieve near perfect precision in a production system.</p></p class="citation"></blockquote><h3 id=34122--77350-do-clips-always-generalize-better-than-imagenet-models-qizhou-wang-et-al-2024>(34/122 | 77/350) Do CLIPs Always Generalize Better than ImageNet Models? (Qizhou Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qizhou Wang, Yong Lin, Yongqiang Chen, Ludwig Schmidt, Bo Han, Tong Zhang. (2024)<br><strong>Do CLIPs Always Generalize Better than ImageNet Models?</strong><br><button class=copy-to-clipboard title="Do CLIPs Always Generalize Better than ImageNet Models?" index=77>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-77 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV, stat-ML<br>Keyword Score: 33<br>Keywords: Benchmarking, Distribution Shift, Distribution Shift, Foundation Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11497v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11497v1.pdf filename=2403.11497v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Large vision language models, such as CLIPs, have revolutionized modern machine learning. CLIPs have demonstrated great generalizability under <b>distribution</b> <b>shifts,</b> supported by an increasing body of literature. However, the evaluation datasets for CLIPs are variations primarily designed for ImageNet <b>benchmarks,</b> which may not fully reflect the extent to which CLIPs, e.g., pre-trained on LAION, robust to spurious correlations. To bridge the gap, we collect a real-world dataset called CounterAnimal that contains realistic spurious features found in animal photos. CounterAnimal consists of a) the common group: comprising animals on common backgrounds, and b) the counter group: including animals on unusual backgrounds. The performance drops from the common to counter groups quantify the reliance of models on spurious features (i.e., backgrounds) to predict the animals. We find that CLIPs trained on either LAION or the OpenAI data exhibit notable performance drops on the counter group. Surprisingly, we observe that single-modal models trained on ImageNet are more robust than CLIPs. We provide both theoretical and empirical explanations for why CLIPs still learn spurious features. Our findings suggest that <b>distribution</b> <b>shifts</b> remain an open problem for CLIPs, and one needs to be cautious about test setups when evaluating <b>foundation</b> <b>models</b> pre-trained on a significantly different scale and distribution.</p></p class="citation"></blockquote><h3 id=35122--78350-collage-prompting-budget-friendly-visual-recognition-with-gpt-4v-siyu-xu-et-al-2024>(35/122 | 78/350) Collage Prompting: Budget-Friendly Visual Recognition with GPT-4V (Siyu Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Siyu Xu, Yunke Wang, Daochang Liu, Chang Xu. (2024)<br><strong>Collage Prompting: Budget-Friendly Visual Recognition with GPT-4V</strong><br><button class=copy-to-clipboard title="Collage Prompting: Budget-Friendly Visual Recognition with GPT-4V" index=78>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-78 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Graph, Generative AI, GPT, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11468v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11468v1.pdf filename=2403.11468v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in <b>generative</b> <b>AI</b> have suggested that by taking visual <b>prompt,</b> <b>GPT-4V</b> can demonstrate significant proficiency in image recognition task. Despite its impressive capabilities, the financial cost associated with <b>GPT-4V&rsquo;s</b> inference presents a substantial barrier for its wide use. To address this challenge, our work introduces Collage <b>Prompting,</b> a budget-friendly <b>prompting</b> approach that concatenates multiple images into a single visual input. With collage <b>prompt,</b> <b>GPT-4V</b> is able to perform image recognition on several images simultaneously. Based on the observation that the accuracy of <b>GPT-4V&rsquo;s</b> image recognition varies significantly with the order of images within the collage <b>prompt,</b> our method further learns to optimize the arrangement of images for maximum recognition accuracy. A <b>graph</b> predictor is trained to indicate the accuracy of each collage <b>prompt,</b> then we propose an optimization method to navigate the search space of possible image arrangements. Experiment results across various datasets demonstrate the cost-efficiency score of collage <b>prompt</b> is much larger than standard <b>prompt.</b> Additionally, collage <b>prompt</b> with learned arrangement achieves clearly better accuracy than collage <b>prompt</b> with random arrangement in <b>GPT-4V&rsquo;s</b> visual recognition.</p></p class="citation"></blockquote><h3 id=36122--79350-effiperception-an-efficient-framework-for-various-perception-tasks-xinhao-xiang-et-al-2024>(36/122 | 79/350) EffiPerception: an Efficient Framework for Various Perception Tasks (Xinhao Xiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinhao Xiang, Simon Dräger, Jiawei Zhang. (2024)<br><strong>EffiPerception: an Efficient Framework for Various Perception Tasks</strong><br><button class=copy-to-clipboard title="EffiPerception: an Efficient Framework for Various Perception Tasks" index=79>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-79 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Object Detection, Data Augmentation, Pruning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12317v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12317v1.pdf filename=2403.12317v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The accuracy-speed-memory trade-off is always the priority to consider for several computer vision perception tasks. Previous methods mainly focus on a single or small couple of these tasks, such as creating effective <b>data</b> <b>augmentation,</b> feature extractor, learning strategies, etc. These approaches, however, could be inherently task-specific: their proposed model&rsquo;s performance may depend on a specific perception task or a dataset. Targeting to explore common learning patterns and increasing the module robustness, we propose the EffiPerception framework. It could achieve great accuracy-speed performance with relatively low memory cost under several perception tasks: 2D <b>Object</b> <b>Detection,</b> 3D <b>Object</b> <b>Detection,</b> 2D Instance Segmentation, and 3D Point Cloud Segmentation. Overall, the framework consists of three parts: (1) Efficient Feature Extractors, which extract the input features for each modality. (2) Efficient Layers, plug-in plug-out layers that further process the feature representation, aggregating core learned information while <b>pruning</b> noisy proposals. (3) The EffiOptim, an 8-bit optimizer to further cut down the computational cost and facilitate performance stability. Extensive experiments on the KITTI, semantic-KITTI, and COCO datasets revealed that EffiPerception could show great accuracy-speed-memory overall performance increase within the four detection and segmentation tasks, in comparison to earlier, well-respected methods.</p></p class="citation"></blockquote><h3 id=37122--80350-e2f-net-eyes-to-face-inpainting-via-stylegan-latent-space-ahmad-hassanpour-et-al-2024>(37/122 | 80/350) E2F-Net: Eyes-to-Face Inpainting via StyleGAN Latent Space (Ahmad Hassanpour et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ahmad Hassanpour, Fatemeh Jamalbafrani, Bian Yang, Kiran Raja, Raymond Veldhuis, Julian Fierrez. (2024)<br><strong>E2F-Net: Eyes-to-Face Inpainting via StyleGAN Latent Space</strong><br><button class=copy-to-clipboard title="E2F-Net: Eyes-to-Face Inpainting via StyleGAN Latent Space" index=80>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-80 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Face Recognition, Generative Adversarial Network, Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12197v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12197v1.pdf filename=2403.12197v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Face</b> <b>inpainting,</b> the technique of restoring missing or damaged regions in facial images, is pivotal for applications like <b>face</b> <b>recognition</b> in occluded scenarios and image analysis with poor-quality captures. This process not only needs to produce realistic visuals but also preserve individual identity characteristics. The aim of this paper is to inpaint a <b>face</b> <b>given</b> periocular region (eyes-to-face) through a proposed new <b>Generative</b> <b>Adversarial</b> <b>Network</b> <b>(GAN)-based</b> model called Eyes-to-Face Network (E2F-Net). The proposed approach extracts identity and non-identity features from the periocular region using two dedicated encoders have been used. The extracted features are then mapped to the latent space of a pre-trained StyleGAN generator to benefit from its state-of-the-art performance and its rich, diverse and expressive latent space without any additional training. We further improve the StyleGAN output to find the optimal code in the latent space using a new optimization for <b>GAN</b> inversion technique. Our E2F-Net requires a minimum training process reducing the computational complexity as a secondary benefit. Through extensive experiments, we show that our method successfully reconstructs the whole <b>face</b> <b>with</b> high quality, surpassing current techniques, despite significantly less training and supervision efforts. We have generated seven eyes-to-face datasets based on well-known public <b>face</b> <b>datasets</b> for training and verifying our proposed methods. The code and datasets are publicly available.</p></p class="citation"></blockquote><h3 id=38122--81350-generic-3d-diffusion-adapter-using-controlled-multi-view-editing-hansheng-chen-et-al-2024>(38/122 | 81/350) Generic 3D Diffusion Adapter Using Controlled Multi-View Editing (Hansheng Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hansheng Chen, Ruoxi Shi, Yulin Liu, Bokui Shen, Jiayuan Gu, Gordon Wetzstein, Hao Su, Leonidas Guibas. (2024)<br><strong>Generic 3D Diffusion Adapter Using Controlled Multi-View Editing</strong><br><button class=copy-to-clipboard title="Generic 3D Diffusion Adapter Using Controlled Multi-View Editing" index=81>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-81 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-GR, cs.CV<br>Keyword Score: 30<br>Keywords: Diffusion Model, Fine-tuning, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12032v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12032v2.pdf filename=2403.12032v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Open-domain 3D object synthesis has been lagging behind image synthesis due to limited data and higher computational complexity. To bridge this gap, recent works have investigated multi-view <b>diffusion</b> <b>but</b> often fall short in either 3D consistency, visual quality, or efficiency. This paper proposes MVEdit, which functions as a 3D counterpart of SDEdit, employing ancestral sampling to jointly denoise multi-view images and output high-quality textured meshes. Built on off-the-shelf 2D <b>diffusion</b> <b>models,</b> MVEdit achieves 3D consistency through a training-free 3D Adapter, which lifts the 2D views of the last timestep into a coherent 3D representation, then conditions the 2D views of the next timestep using rendered views, without uncompromising visual quality. With an inference time of only 2-5 minutes, this framework achieves better trade-off between quality and speed than score <b>distillation.</b> MVEdit is highly versatile and extendable, with a wide range of applications including text/image-to-3D generation, 3D-to-3D editing, and high-quality texture synthesis. In particular, evaluations demonstrate state-of-the-art performance in both image-to-3D and text-guided texture generation tasks. Additionally, we introduce a method for <b>fine-tuning</b> 2D latent <b>diffusion</b> <b>models</b> on small 3D datasets with limited resources, enabling fast low-resolution text-to-3D initialization.</p></p class="citation"></blockquote><h3 id=39122--82350-fast-high-resolution-image-synthesis-with-latent-adversarial-diffusion-distillation-axel-sauer-et-al-2024>(39/122 | 82/350) Fast High-Resolution Image Synthesis with Latent Adversarial Diffusion Distillation (Axel Sauer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Axel Sauer, Frederic Boesel, Tim Dockhorn, Andreas Blattmann, Patrick Esser, Robin Rombach. (2024)<br><strong>Fast High-Resolution Image Synthesis with Latent Adversarial Diffusion Distillation</strong><br><button class=copy-to-clipboard title="Fast High-Resolution Image Synthesis with Latent Adversarial Diffusion Distillation" index=82>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-82 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Diffusion Model, Knowledge Distillation, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12015v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12015v1.pdf filename=2403.12015v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Diffusion</b> <b>models</b> are the main driver of progress in image and video synthesis, but suffer from slow inference speed. <b>Distillation</b> methods, like the recently introduced adversarial <b>diffusion</b> <b>distillation</b> (ADD) aim to shift the model from many-shot to single-step inference, albeit at the cost of expensive and difficult optimization due to its reliance on a fixed pretrained DINOv2 discriminator. We introduce Latent Adversarial <b>Diffusion</b> <b>Distillation</b> (LADD), a novel <b>distillation</b> approach overcoming the limitations of ADD. In contrast to pixel-based ADD, LADD utilizes generative features from pretrained latent <b>diffusion</b> <b>models.</b> This approach simplifies training and enhances performance, enabling high-resolution multi-aspect ratio image synthesis. We apply LADD to Stable <b>Diffusion</b> <b>3</b> (8B) to obtain SD3-Turbo, a fast model that matches the performance of state-of-the-art <b>text-to-image</b> generators using only four unguided sampling steps. Moreover, we systematically investigate its scaling behavior and demonstrate LADD&rsquo;s effectiveness in various applications such as image editing and inpainting.</p></p class="citation"></blockquote><h3 id=40122--83350-layerdiff-exploring-text-guided-multi-layered-composable-image-synthesis-via-layer-collaborative-diffusion-model-runhui-huang-et-al-2024>(40/122 | 83/350) LayerDiff: Exploring Text-guided Multi-layered Composable Image Synthesis via Layer-Collaborative Diffusion Model (Runhui Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Runhui Huang, Kaixin Cai, Jianhua Han, Xiaodan Liang, Renjing Pei, Guansong Lu, Songcen Xu, Wei Zhang, Hang Xu. (2024)<br><strong>LayerDiff: Exploring Text-guided Multi-layered Composable Image Synthesis via Layer-Collaborative Diffusion Model</strong><br><button class=copy-to-clipboard title="LayerDiff: Exploring Text-guided Multi-layered Composable Image Synthesis via Layer-Collaborative Diffusion Model" index=83>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-83 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Diffusion Model, Style Transfer, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11929v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11929v1.pdf filename=2403.11929v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite the success of generating high-quality images given any text <b>prompts</b> by <b>diffusion-based</b> <b>generative</b> models, prior works directly generate the entire images, but cannot provide object-wise manipulation capability. To support wider real applications like professional graphic design and digital artistry, images are frequently created and manipulated in multiple layers to offer greater flexibility and control. Therefore in this paper, we propose a layer-collaborative <b>diffusion</b> <b>model,</b> named LayerDiff, specifically designed for text-guided, multi-layered, composable image synthesis. The composable image consists of a background layer, a set of foreground layers, and associated mask layers for each foreground element. To enable this, LayerDiff introduces a layer-based generation paradigm incorporating multiple layer-collaborative attention modules to capture inter-layer patterns. Specifically, an inter-layer attention module is designed to encourage information exchange and learning between layers, while a text-guided intra-layer attention module incorporates layer-specific <b>prompts</b> to direct the specific-content generation for each layer. A layer-specific <b>prompt-enhanced</b> module better captures detailed textual cues from the global <b>prompt.</b> Additionally, a self-mask guidance sampling strategy further unleashes the model&rsquo;s ability to generate multi-layered images. We also present a pipeline that integrates existing perceptual and generative models to produce a large dataset of high-quality, text-prompted, multi-layered images. Extensive experiments demonstrate that our LayerDiff model can generate high-quality multi-layered images with performance comparable to conventional whole-image generation methods. Moreover, LayerDiff enables a broader range of controllable generative applications, including layer-specific image editing and <b>style</b> <b>transfer.</b></p></p class="citation"></blockquote><h3 id=41122--84350-deep-medial-voxels-learned-medial-axis-approximations-for-anatomical-shape-modeling-antonio-pepe-et-al-2024>(41/122 | 84/350) Deep Medial Voxels: Learned Medial Axis Approximations for Anatomical Shape Modeling (Antonio Pepe et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Antonio Pepe, Richard Schussnig, Jianning Li, Christina Gsaxner, Dieter Schmalstieg, Jan Egger. (2024)<br><strong>Deep Medial Voxels: Learned Medial Axis Approximations for Anatomical Shape Modeling</strong><br><button class=copy-to-clipboard title="Deep Medial Voxels: Learned Medial Axis Approximations for Anatomical Shape Modeling" index=84>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-84 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Convolution, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11790v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11790v1.pdf filename=2403.11790v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Shape reconstruction from imaging volumes is a recurring need in medical image analysis. Common workflows start with a segmentation step, followed by careful post-processing and,finally, ad hoc meshing algorithms. As this sequence can be timeconsuming, neural networks are trained to reconstruct shapes through template deformation. These networks deliver state-ofthe-art results without manual intervention, but, so far, they have primarily been evaluated on anatomical shapes with little topological variety between individuals. In contrast, other works favor learning implicit shape models, which have multiple benefits for meshing and visualization. Our work follows this direction by introducing deep medial voxels, a semi-implicit representation that faithfully approximates the topological skeleton from imaging volumes and eventually leads to shape reconstruction via <b>convolution</b> surfaces. Our reconstruction technique shows potential for both visualization and computer <b>simulations.</b></p></p class="citation"></blockquote><h3 id=42122--85350-infinite-id-identity-preserved-personalization-via-id-semantics-decoupling-paradigm-yi-wu-et-al-2024>(42/122 | 85/350) Infinite-ID: Identity-preserved Personalization via ID-semantics Decoupling Paradigm (Yi Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yi Wu, Ziqiang Li, Heliang Zheng, Chaoyue Wang, Bin Li. (2024)<br><strong>Infinite-ID: Identity-preserved Personalization via ID-semantics Decoupling Paradigm</strong><br><button class=copy-to-clipboard title="Infinite-ID: Identity-preserved Personalization via ID-semantics Decoupling Paradigm" index=85>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-85 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Diffusion Model, Text2image, Text Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11781v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11781v1.pdf filename=2403.11781v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Drawing on recent advancements in <b>diffusion</b> <b>models</b> for <b>text-to-image</b> <b>generation,</b> identity-preserved personalization has made significant progress in accurately capturing specific identities with just a single reference image. However, existing methods primarily integrate reference images within the <b>text</b> <b>embedding</b> space, leading to a complex entanglement of image and <b>text</b> <b>information,</b> which poses challenges for preserving both identity fidelity and semantic consistency. To tackle this challenge, we propose Infinite-ID, an ID-semantics decoupling paradigm for identity-preserved personalization. Specifically, we introduce identity-enhanced training, incorporating an additional image cross-attention module to capture sufficient ID information while deactivating the original <b>text</b> <b>cross-attention</b> module of the <b>diffusion</b> <b>model.</b> This ensures that the image stream faithfully represents the identity provided by the reference image while mitigating interference from textual input. Additionally, we introduce a feature interaction mechanism that combines a mixed attention module with an AdaIN-mean operation to seamlessly merge the two streams. This mechanism not only enhances the fidelity of identity and semantic consistency but also enables convenient control over the styles of the generated images. Extensive experimental results on both raw photo generation and style image generation demonstrate the superior performance of our proposed method.</p></p class="citation"></blockquote><h3 id=43122--86350-effivedefficient-video-editing-via-text-instruction-diffusion-models-zhenghao-zhang-et-al-2024>(43/122 | 86/350) EffiVED:Efficient Video Editing via Text-instruction Diffusion Models (Zhenghao Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhenghao Zhang, Zuozhuo Dai, Long Qin, Weizhi Wang. (2024)<br><strong>EffiVED:Efficient Video Editing via Text-instruction Diffusion Models</strong><br><button class=copy-to-clipboard title="EffiVED:Efficient Video Editing via Text-instruction Diffusion Models" index=86>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-86 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Diffusion Model, Fine-tuning, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11568v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11568v1.pdf filename=2403.11568v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Large-scale text-to-video models have shown remarkable abilities, but their direct application in video editing remains challenging due to limited available datasets. Current video editing methods commonly require per-video <b>fine-tuning</b> of <b>diffusion</b> <b>models</b> or specific inversion optimization to ensure high-fidelity edits. In this paper, we introduce EffiVED, an efficient <b>diffusion-based</b> <b>model</b> that directly supports instruction-guided video editing. To achieve this, we present two efficient workflows to gather video editing pairs, utilizing augmentation and fundamental <b>vision-language</b> techniques. These workflows transform vast image editing datasets and open-world videos into a high-quality dataset for training EffiVED. Experimental results reveal that EffiVED not only generates high-quality editing videos but also executes rapidly. Finally, we demonstrate that our data collection method significantly improves editing performance and can potentially tackle the scarcity of video editing data. The datasets will be made publicly available upon publication.</p></p class="citation"></blockquote><h3 id=44122--87350-echoreel-enhancing-action-generation-of-existing-video-diffusion-models-jianzhi-liu-et-al-2024>(44/122 | 87/350) EchoReel: Enhancing Action Generation of Existing Video Diffusion Models (Jianzhi liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianzhi liu, Junchen Zhu, Lianli Gao, Jingkuan Song. (2024)<br><strong>EchoReel: Enhancing Action Generation of Existing Video Diffusion Models</strong><br><button class=copy-to-clipboard title="EchoReel: Enhancing Action Generation of Existing Video Diffusion Models" index=87>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-87 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Diffusion Model, Fine-tuning, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11535v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11535v1.pdf filename=2403.11535v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent large-scale video datasets have facilitated the generation of diverse open-domain videos of Video <b>Diffusion</b> <b>Models</b> (VDMs). Nonetheless, the efficacy of VDMs in assimilating complex knowledge from these datasets remains constrained by their inherent scale, leading to suboptimal comprehension and synthesis of numerous actions. In this paper, we introduce EchoReel, a novel approach to augment the capability of VDMs in generating intricate actions by emulating motions from pre-existing videos, which are readily accessible from databases or online repositories. EchoReel seamlessly integrates with existing VDMs, enhancing their ability to produce realistic motions without compromising their fundamental capabilities. Specifically, the Action Prism (AP), is introduced to <b>distill</b> motion information from reference videos, which requires training on only a small dataset. Leveraging the knowledge from pre-trained VDMs, EchoReel incorporates new action features into VDMs through the additional layers, eliminating the need for any further <b>fine-tuning</b> of untrained actions. Extensive experiments demonstrate that EchoReel is not merely replicating the whole content from references, and it significantly improves the generation of realistic actions, even in situations where existing VDMs might directly fail.</p></p class="citation"></blockquote><h3 id=45122--88350-cassr-activating-image-power-for-real-world-image-super-resolution-haolan-chen-et-al-2024>(45/122 | 88/350) CasSR: Activating Image Power for Real-World Image Super-Resolution (Haolan Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haolan Chen, Jinhua Hao, Kai Zhao, Kun Yuan, Ming Sun, Chao Zhou, Wei Hu. (2024)<br><strong>CasSR: Activating Image Power for Real-World Image Super-Resolution</strong><br><button class=copy-to-clipboard title="CasSR: Activating Image Power for Real-World Image Super-Resolution" index=88>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-88 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Diffusion Model, Information Retrieval, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11451v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11451v1.pdf filename=2403.11451v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The objective of image super-resolution is to generate clean and high-resolution images from degraded versions. Recent advancements in <b>diffusion</b> <b>modeling</b> have led to the emergence of various image super-resolution techniques that leverage pretrained <b>text-to-image</b> (T2I) models. Nevertheless, due to the prevalent severe degradation in low-resolution images and the inherent characteristics of <b>diffusion</b> <b>models,</b> achieving high-fidelity image restoration remains challenging. Existing methods often exhibit issues including semantic loss, artifacts, and the introduction of spurious content not present in the original image. To tackle this challenge, we propose Cascaded <b>diffusion</b> <b>for</b> Super-Resolution, CasSR , a novel method designed to produce highly detailed and realistic images. In particular, we develop a cascaded controllable <b>diffusion</b> <b>model</b> that aims to optimize the extraction of <b>information</b> <b>from</b> low-resolution images. This model generates a preliminary reference image to facilitate initial <b>information</b> <b>extraction</b> and degradation mitigation. Furthermore, we propose a multi-attention mechanism to enhance the T2I model&rsquo;s capability in maximizing the restoration of the original image content. Through a comprehensive blend of qualitative and quantitative analyses, we substantiate the efficacy and superiority of our approach.</p></p class="citation"></blockquote><h3 id=46122--89350-robust-overfitting-does-matter-test-time-adversarial-purification-with-fgsm-linyu-tang-et-al-2024>(46/122 | 89/350) Robust Overfitting Does Matter: Test-Time Adversarial Purification With FGSM (Linyu Tang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Linyu Tang, Lei Zhang. (2024)<br><strong>Robust Overfitting Does Matter: Test-Time Adversarial Purification With FGSM</strong><br><button class=copy-to-clipboard title="Robust Overfitting Does Matter: Test-Time Adversarial Purification With FGSM" index=89>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-89 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Prompt, Adversarial Attack, Adversarial Purification<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11448v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11448v1.pdf filename=2403.11448v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Numerous studies have demonstrated the susceptibility of deep neural networks (DNNs) to subtle <b>adversarial</b> <b>perturbations,</b> <b>prompting</b> the development of many advanced <b>adversarial</b> <b>defense</b> methods aimed at mitigating <b>adversarial</b> <b>attacks.</b> Current defense strategies usually train DNNs for a specific <b>adversarial</b> <b>attack</b> method and can achieve good robustness in defense against this type of <b>adversarial</b> <b>attack.</b> Nevertheless, when subjected to evaluations involving unfamiliar attack modalities, empirical evidence reveals a pronounced deterioration in the robustness of DNNs. Meanwhile, there is a trade-off between the classification accuracy of clean examples and <b>adversarial</b> <b>examples.</b> Most defense methods often sacrifice the accuracy of clean examples in order to improve the <b>adversarial</b> <b>robustness</b> of DNNs. To alleviate these problems and enhance the overall robust generalization of DNNs, we propose the Test-Time Pixel-Level <b>Adversarial</b> <b>Purification</b> (TPAP) method. This approach is based on the robust overfitting characteristic of DNNs to the fast gradient sign method (FGSM) on training and test datasets. It utilizes FGSM for <b>adversarial</b> <b>purification,</b> to process images for purifying unknown <b>adversarial</b> <b>perturbations</b> from pixels at testing time in a &ldquo;counter changes with changelessness&rdquo; manner, thereby enhancing the defense capability of DNNs against various unknown <b>adversarial</b> <b>attacks.</b> Extensive experimental results show that our method can effectively improve both overall robust generalization of DNNs, notably over previous methods.</p></p class="citation"></blockquote><h3 id=47122--90350-llava-uhd-an-lmm-perceiving-any-aspect-ratio-and-high-resolution-images-ruyi-xu-et-al-2024>(47/122 | 90/350) LLaVA-UHD: an LMM Perceiving Any Aspect Ratio and High-Resolution Images (Ruyi Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruyi Xu, Yuan Yao, Zonghao Guo, Junbo Cui, Zanlin Ni, Chunjiang Ge, Tat-Seng Chua, Zhiyuan Liu, Maosong Sun, Gao Huang. (2024)<br><strong>LLaVA-UHD: an LMM Perceiving Any Aspect Ratio and High-Resolution Images</strong><br><button class=copy-to-clipboard title="LLaVA-UHD: an LMM Perceiving Any Aspect Ratio and High-Resolution Images" index=90>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-90 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 29<br>Keywords: Benchmarking, Multi-modal, Multi-modal, GPT, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11703v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11703v1.pdf filename=2403.11703v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Visual encoding constitutes the basis of large <b>multimodal</b> models (LMMs) in understanding the visual world. Conventional LMMs process images in fixed sizes and limited resolutions, while recent explorations in this direction are limited in adaptivity, efficiency, and even correctness. In this work, we first take <b>GPT-4V</b> and LLaVA-1.5 as representative examples and expose systematic flaws rooted in their visual encoding strategy. To address the challenges, we present LLaVA-UHD, a large <b>multimodal</b> model that can efficiently perceive images in any aspect ratio and high resolution. LLaVA-UHD includes three key components: (1) An image modularization strategy that divides native-resolution images into smaller variable-sized slices for efficient and extensible encoding, (2) a compression module that further condenses image tokens from visual encoders, and (3) a spatial schema to organize slice tokens for <b>LLMs.</b> Comprehensive experiments show that LLaVA-UHD outperforms established LMMs trained with 2-3 orders of magnitude more data on 9 <b>benchmarks.</b> Notably, our model built on LLaVA-1.5 336x336 supports 6 times larger (i.e., 672x1088) resolution images using only 94% inference computation, and achieves 6.4 accuracy improvement on TextVQA. Moreover, the model can be efficiently trained in academic settings, within 23 hours on 8 A100 GPUs (vs. 26 hours of LLaVA-1.5). We make the data and code publicly available at <a href=https://github.com/thunlp/LLaVA-UHD>https://github.com/thunlp/LLaVA-UHD</a>.</p></p class="citation"></blockquote><h3 id=48122--91350-align-and-distill-unifying-and-improving-domain-adaptive-object-detection-justin-kay-et-al-2024>(48/122 | 91/350) Align and Distill: Unifying and Improving Domain Adaptive Object Detection (Justin Kay et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Justin Kay, Timm Haucke, Suzanne Stathatos, Siqi Deng, Erik Young, Pietro Perona, Sara Beery, Grant Van Horn. (2024)<br><strong>Align and Distill: Unifying and Improving Domain Adaptive Object Detection</strong><br><button class=copy-to-clipboard title="Align and Distill: Unifying and Improving Domain Adaptive Object Detection" index=91>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-91 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 26<br>Keywords: Object Detection, Benchmarking, Benchmarking, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12029v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12029v1.pdf filename=2403.12029v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Object</b> <b>detectors</b> often perform poorly on data that differs from their training set. Domain adaptive <b>object</b> <b>detection</b> (DAOD) methods have recently demonstrated strong results on addressing this challenge. Unfortunately, we identify systemic <b>benchmarking</b> pitfalls that call past results into question and hamper further progress: (a) Overestimation of performance due to underpowered baselines, (b) Inconsistent implementation practices preventing transparent comparisons of methods, and (c) Lack of generality due to outdated backbones and lack of diversity in <b>benchmarks.</b> We address these problems by introducing: (1) A unified <b>benchmarking</b> and implementation framework, Align and <b>Distill</b> (ALDI), enabling comparison of DAOD methods and supporting future development, (2) A fair and modern training and evaluation protocol for DAOD that addresses <b>benchmarking</b> pitfalls, (3) A new DAOD <b>benchmark</b> dataset, CFC-DAOD, enabling evaluation on diverse real-world data, and (4) A new method, ALDI++, that achieves state-of-the-art results by a large margin. ALDI++ outperforms the previous state-of-the-art by +3.5 AP50 on Cityscapes to Foggy Cityscapes, +5.7 AP50 on Sim10k to Cityscapes (where ours is the only method to outperform a fair baseline), and +2.0 AP50 on CFC Kenai to Channel. Our framework, dataset, and state-of-the-art method offer a critical reset for DAOD and provide a strong foundation for future research. Code and data are available: <a href=https://github.com/justinkay/aldi>https://github.com/justinkay/aldi</a> and <a href=https://github.com/visipedia/caltech-fish-counting>https://github.com/visipedia/caltech-fish-counting</a>.</p></p class="citation"></blockquote><h3 id=49122--92350-openocc-open-vocabulary-3d-scene-reconstruction-via-occupancy-representation-haochen-jiang-et-al-2024>(49/122 | 92/350) OpenOcc: Open Vocabulary 3D Scene Reconstruction via Occupancy Representation (Haochen Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haochen Jiang, Yueming Xu, Yihan Zeng, Hang Xu, Wei Zhang, Jianfeng Feng, Li Zhang. (2024)<br><strong>OpenOcc: Open Vocabulary 3D Scene Reconstruction via Occupancy Representation</strong><br><button class=copy-to-clipboard title="OpenOcc: Open Vocabulary 3D Scene Reconstruction via Occupancy Representation" index=92>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-92 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV<br>Keyword Score: 25<br>Keywords: Geometry, Knowledge Distillation, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11796v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11796v1.pdf filename=2403.11796v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>3D reconstruction has been widely used in autonomous navigation fields of mobile robotics. However, the former research can only provide the basic <b>geometry</b> structure without the capability of open-world scene understanding, limiting advanced tasks like human interaction and visual navigation. Moreover, traditional 3D scene understanding approaches rely on expensive labeled 3D datasets to train a model for a single task with supervision. Thus, geometric reconstruction with <b>zero-shot</b> scene understanding i.e. Open vocabulary 3D Understanding and Reconstruction, is crucial for the future development of mobile robots. In this paper, we propose OpenOcc, a novel framework unifying the 3D scene reconstruction and open vocabulary understanding with neural radiance fields. We model the geometric structure of the scene with occupancy representation and <b>distill</b> the pre-trained open vocabulary model into a 3D language field via volume rendering for <b>zero-shot</b> inference. Furthermore, a novel semantic-aware confidence propagation (SCP) method has been proposed to relieve the issue of language field representation degeneracy caused by inconsistent measurements in <b>distilled</b> features. Experimental results show that our approach achieves competitive performance in 3D scene understanding tasks, especially for small and long-tail objects.</p></p class="citation"></blockquote><h3 id=50122--93350-localstylefool-regional-video-style-transfer-attack-using-segment-anything-model-yuxin-cao-et-al-2024>(50/122 | 93/350) LocalStyleFool: Regional Video Style Transfer Attack Using Segment Anything Model (Yuxin Cao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuxin Cao, Jinghao Li, Xi Xiao, Derui Wang, Minhui Xue, Hao Ge, Wei Liu, Guangwu Hu. (2024)<br><strong>LocalStyleFool: Regional Video Style Transfer Attack Using Segment Anything Model</strong><br><button class=copy-to-clipboard title="LocalStyleFool: Regional Video Style Transfer Attack Using Segment Anything Model" index=93>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-93 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 25<br>Keywords: Black Box, Style Transfer, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11656v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11656v1.pdf filename=2403.11656v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Previous work has shown that well-crafted <b>adversarial</b> <b>perturbations</b> can threaten the security of video recognition systems. Attackers can invade such models with a low query budget when the perturbations are semantic-invariant, such as StyleFool. Despite the query efficiency, the naturalness of the minutia areas still requires amelioration, since StyleFool leverages <b>style</b> <b>transfer</b> to all pixels in each frame. To close the gap, we propose LocalStyleFool, an improved <b>black-box</b> <b>video</b> <b>adversarial</b> <b>attack</b> that superimposes regional <b>style-transfer-based</b> <b>perturbations</b> on videos. Benefiting from the popularity and scalably usability of Segment Anything Model (SAM), we first extract different regions according to semantic information and then track them through the video stream to maintain the temporal consistency. Then, we add <b>style-transfer-based</b> <b>perturbations</b> to several regions selected based on the associative criterion of transfer-based gradient information and regional area. Perturbation fine adjustment is followed to make stylized videos <b>adversarial.</b> <b>We</b> demonstrate that LocalStyleFool can improve both intra-frame and inter-frame naturalness through a human-assessed survey, while maintaining competitive fooling rate and query efficiency. Successful experiments on the high-resolution dataset also showcase that scrupulous segmentation of SAM helps to improve the scalability of <b>adversarial</b> <b>attacks</b> under high-resolution data.</p></p class="citation"></blockquote><h3 id=51122--94350-diffusion-models-are-geometry-critics-single-image-3d-editing-using-pre-trained-diffusion-priors-ruicheng-wang-et-al-2024>(51/122 | 94/350) Diffusion Models are Geometry Critics: Single Image 3D Editing Using Pre-Trained Diffusion Priors (Ruicheng Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruicheng Wang, Jianfeng Xiang, Jiaolong Yang, Xin Tong. (2024)<br><strong>Diffusion Models are Geometry Critics: Single Image 3D Editing Using Pre-Trained Diffusion Priors</strong><br><button class=copy-to-clipboard title="Diffusion Models are Geometry Critics: Single Image 3D Editing Using Pre-Trained Diffusion Priors" index=94>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-94 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 25<br>Keywords: Diffusion Model, Geometry, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11503v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11503v1.pdf filename=2403.11503v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose a novel image editing technique that enables 3D manipulations on single images, such as object rotation and translation. Existing 3D-aware image editing approaches typically rely on synthetic multi-view datasets for training specialized models, thus constraining their effectiveness on open-domain images featuring significantly more varied layouts and styles. In contrast, our method directly leverages powerful image <b>diffusion</b> <b>models</b> trained on a broad spectrum of <b>text-image</b> pairs and thus retain their exceptional generalization abilities. This objective is realized through the development of an iterative novel view synthesis and <b>geometry</b> alignment algorithm. The algorithm harnesses <b>diffusion</b> <b>models</b> for dual purposes: they provide appearance prior by predicting novel views of the selected object using estimated depth maps, and they act as a <b>geometry</b> critic by correcting misalignments in 3D shapes across the sampled views. Our method can generate high-quality 3D-aware image edits with large viewpoint transformations and high appearance and shape consistency with the input image, pushing the boundaries of what is possible with single-image 3D-aware editing.</p></p class="citation"></blockquote><h3 id=52122--95350-graph-jigsaw-conditioned-diffusion-model-for-skeleton-based-video-anomaly-detection-ali-karami-et-al-2024>(52/122 | 95/350) Graph-Jigsaw Conditioned Diffusion Model for Skeleton-based Video Anomaly Detection (Ali Karami et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ali Karami, Thi Kieu Khanh Ho, Narges Armanfard. (2024)<br><strong>Graph-Jigsaw Conditioned Diffusion Model for Skeleton-based Video Anomaly Detection</strong><br><button class=copy-to-clipboard title="Graph-Jigsaw Conditioned Diffusion Model for Skeleton-based Video Anomaly Detection" index=95>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-95 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Diffusion Model, Graph, Anomaly Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12172v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12172v1.pdf filename=2403.12172v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Skeleton-based video <b>anomaly</b> <b>detection</b> (SVAD) is a crucial task in computer vision. Accurately identifying abnormal patterns or events enables operators to promptly detect suspicious activities, thereby enhancing safety. Achieving this demands a comprehensive understanding of human motions, both at body and region levels, while also accounting for the wide variations of performing a single action. However, existing studies fail to simultaneously address these crucial properties. This paper introduces a novel, practical and lightweight framework, namely <b>Graph-Jigsaw</b> Conditioned <b>Diffusion</b> <b>Model</b> for Skeleton-based Video <b>Anomaly</b> <b>Detection</b> (GiCiSAD) to overcome the challenges associated with SVAD. GiCiSAD consists of three novel modules: the <b>Graph</b> Attention-based Forecasting module to capture the spatio-temporal dependencies inherent in the data, the <b>Graph-level</b> Jigsaw Puzzle Maker module to distinguish subtle region-level discrepancies between normal and abnormal motions, and the <b>Graph-based</b> Conditional <b>Diffusion</b> <b>model</b> to generate a wide spectrum of human motions. Extensive experiments on four widely used skeleton-based video datasets show that GiCiSAD outperforms existing methods with significantly fewer training parameters, establishing it as the new state-of-the-art.</p></p class="citation"></blockquote><h3 id=53122--96350-exploring-pre-trained-text-to-video-diffusion-models-for-referring-video-object-segmentation-zixin-zhu-et-al-2024>(53/122 | 96/350) Exploring Pre-trained Text-to-Video Diffusion Models for Referring Video Object Segmentation (Zixin Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zixin Zhu, Xuelu Feng, Dongdong Chen, Junsong Yuan, Chunming Qiao, Gang Hua. (2024)<br><strong>Exploring Pre-trained Text-to-Video Diffusion Models for Referring Video Object Segmentation</strong><br><button class=copy-to-clipboard title="Exploring Pre-trained Text-to-Video Diffusion Models for Referring Video Object Segmentation" index=96>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-96 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Diffusion Model, Benchmarking, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12042v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12042v1.pdf filename=2403.12042v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we explore the visual representations produced from a pre-trained text-to-video (T2V) <b>diffusion</b> <b>model</b> for video understanding tasks. We hypothesize that the latent representation learned from a pretrained generative T2V model encapsulates rich semantics and coherent temporal correspondences, thereby naturally facilitating video understanding. Our hypothesis is validated through the classic referring video object segmentation (R-VOS) task. We introduce a novel framework, termed ``VD-IT&rsquo;&rsquo;, tailored with dedicatedly designed components built upon a fixed pretrained T2V model. Specifically, VD-IT uses textual information as a conditional input, ensuring semantic consistency across time for precise temporal instance matching. It further incorporates image tokens as supplementary textual inputs, enriching the feature set to generate detailed and nuanced masks.Besides, instead of using the standard Gaussian noise, we propose to predict the video-specific noise with an extra noise prediction module, which can help preserve the feature fidelity and elevates segmentation quality. Through extensive experiments, we surprisingly observe that fixed generative T2V <b>diffusion</b> <b>models,</b> unlike commonly used video backbones (e.g., Video Swin <b>Transformer)</b> pretrained with discriminative image/video pre-tasks, exhibit better potential to maintain semantic alignment and temporal consistency. On existing standard <b>benchmarks,</b> our VD-IT achieves highly competitive results, surpassing many existing state-of-the-art methods. The code will be available at \url{https://github.com/buxiangzhiren/VD-IT}</p></p class="citation"></blockquote><h3 id=54122--97350-just-add-100-more-augmenting-nerf-based-pseudo-lidar-point-cloud-for-resolving-class-imbalance-problem-mincheol-chang-et-al-2024>(54/122 | 97/350) Just Add $100 More: Augmenting NeRF-based Pseudo-LiDAR Point Cloud for Resolving Class-imbalance Problem (Mincheol Chang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mincheol Chang, Siyeong Lee, Jinkyu Kim, Namil Kim. (2024)<br><strong>Just Add $100 More: Augmenting NeRF-based Pseudo-LiDAR Point Cloud for Resolving Class-imbalance Problem</strong><br><button class=copy-to-clipboard title="Just Add $100 More: Augmenting NeRF-based Pseudo-LiDAR Point Cloud for Resolving Class-imbalance Problem" index=97>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-97 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Object Detection, Benchmarking, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11573v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11573v2.pdf filename=2403.11573v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Typical LiDAR-based 3D <b>object</b> <b>detection</b> models are trained in a <b>supervised</b> manner with real-world data collection, which is often imbalanced over classes (or long-tailed). To deal with it, augmenting minority-class examples by sampling ground truth (GT) LiDAR points from a database and pasting them into a scene of interest is often used, but challenges still remain: inflexibility in locating GT samples and limited sample diversity. In this work, we propose to leverage pseudo-LiDAR point clouds generated (at a low cost) from videos capturing a surround view of miniatures or real-world <b>objects</b> <b>of</b> minor classes. Our method, called Pseudo Ground Truth Augmentation (PGT-Aug), consists of three main steps: (i) volumetric 3D instance reconstruction using a 2D-to-3D view synthesis model, (ii) <b>object-level</b> <b>domain</b> alignment with LiDAR intensity estimation and (iii) a hybrid context-aware placement method from ground and map information. We demonstrate the superiority and generality of our method through performance improvements in extensive experiments conducted on three popular <b>benchmarks,</b> i.e., nuScenes, KITTI, and Lyft, especially for the datasets with large domain gaps captured by different LiDAR configurations. Our code and data will be publicly available upon publication.</p></p class="citation"></blockquote><h3 id=55122--98350-semantic-prompting-with-image-token-for-continual-learning-jisu-han-et-al-2024>(55/122 | 98/350) Semantic Prompting with Image-Token for Continual Learning (Jisu Han et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jisu Han, Jaemin Na, Wonjun Hwang. (2024)<br><strong>Semantic Prompting with Image-Token for Continual Learning</strong><br><button class=copy-to-clipboard title="Semantic Prompting with Image-Token for Continual Learning" index=98>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-98 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 23<br>Keywords: Benchmarking, Continual Learning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11537v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11537v1.pdf filename=2403.11537v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Continual</b> <b>learning</b> aims to refine model parameters for new tasks while retaining knowledge from previous tasks. Recently, <b>prompt-based</b> learning has emerged to leverage pre-trained models to be <b>prompted</b> to learn subsequent tasks without the reliance on the rehearsal buffer. Although this approach has demonstrated outstanding results, existing methods depend on preceding task-selection process to choose appropriate <b>prompts.</b> However, imperfectness in task-selection may lead to negative impacts on the performance particularly in the scenarios where the number of tasks is large or task distributions are imbalanced. To address this issue, we introduce I-Prompt, a task-agnostic approach focuses on the visual semantic information of image tokens to eliminate task prediction. Our method consists of semantic <b>prompt</b> matching, which determines <b>prompts</b> based on similarities between tokens, and image token-level <b>prompting,</b> which applies <b>prompts</b> directly to image tokens in the intermediate layers. Consequently, our method achieves competitive performance on four <b>benchmarks</b> while significantly reducing training time compared to state-of-the-art methods. Moreover, we demonstrate the superiority of our method across various scenarios through extensive experiments.</p></p class="citation"></blockquote><h3 id=56122--99350-fed3dgs-scalable-3d-gaussian-splatting-with-federated-learning-teppei-suzuki-2024>(56/122 | 99/350) Fed3DGS: Scalable 3D Gaussian Splatting with Federated Learning (Teppei Suzuki, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Teppei Suzuki. (2024)<br><strong>Fed3DGS: Scalable 3D Gaussian Splatting with Federated Learning</strong><br><button class=copy-to-clipboard title="Fed3DGS: Scalable 3D Gaussian Splatting with Federated Learning" index=99>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-99 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Benchmarking, Federated Learning, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11460v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11460v1.pdf filename=2403.11460v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we present Fed3DGS, a scalable 3D reconstruction framework based on 3D Gaussian splatting (3DGS) with <b>federated</b> <b>learning.</b> Existing city-scale reconstruction methods typically adopt a centralized approach, which gathers all data in a central server and reconstructs scenes. The approach hampers scalability because it places a heavy load on the server and demands extensive data storage when reconstructing scenes on a scale beyond city-scale. In pursuit of a more scalable 3D reconstruction, we propose a <b>federated</b> <b>learning</b> framework with 3DGS, which is a decentralized framework and can potentially use distributed computational resources across millions of clients. We tailor a <b>distillation-based</b> model update scheme for 3DGS and introduce appearance modeling for handling non-IID data in the scenario of 3D reconstruction with <b>federated</b> <b>learning.</b> We simulate our method on several large-scale <b>benchmarks,</b> and our method demonstrates rendered image quality comparable to centralized approaches. In addition, we also simulate our method with data collected in different seasons, demonstrating that our framework can reflect changes in the scenes and our appearance modeling captures changes due to seasonal variations.</p></p class="citation"></blockquote><h3 id=57122--100350-hiker-sgg-hierarchical-knowledge-enhanced-robust-scene-graph-generation-ce-zhang-et-al-2024>(57/122 | 100/350) HiKER-SGG: Hierarchical Knowledge Enhanced Robust Scene Graph Generation (Ce Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ce Zhang, Simon Stepputtis, Joseph Campbell, Katia Sycara, Yaqi Xie. (2024)<br><strong>HiKER-SGG: Hierarchical Knowledge Enhanced Robust Scene Graph Generation</strong><br><button class=copy-to-clipboard title="HiKER-SGG: Hierarchical Knowledge Enhanced Robust Scene Graph Generation" index=100>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-100 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 21<br>Keywords: Graph, Benchmarking, Knowledge Graph, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12033v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12033v1.pdf filename=2403.12033v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Being able to understand visual scenes is a precursor for many downstream tasks, including autonomous driving, robotics, and other vision-based approaches. A common approach enabling the ability to reason over visual data is Scene <b>Graph</b> Generation (SGG); however, many existing approaches assume undisturbed vision, i.e., the absence of real-world corruptions such as fog, snow, smoke, as well as non-uniform perturbations like sun glare or water drops. In this work, we propose a novel SGG <b>benchmark</b> containing procedurally generated weather corruptions and other transformations over the Visual Genome dataset. Further, we introduce a corresponding approach, Hierarchical <b>Knowledge</b> <b>Enhanced</b> Robust Scene <b>Graph</b> Generation (HiKER-SGG), providing a strong baseline for scene <b>graph</b> generation under such challenging setting. At its core, HiKER-SGG utilizes a hierarchical <b>knowledge</b> <b>graph</b> in order to refine its predictions from coarse initial estimates to detailed predictions. In our extensive experiments, we show that HiKER-SGG does not only demonstrate superior performance on corrupted images in a <b>zero-shot</b> manner, but also outperforms current state-of-the-art methods on uncorrupted SGG tasks. Code is available at <a href=https://github.com/zhangce01/HiKER-SGG>https://github.com/zhangce01/HiKER-SGG</a>.</p></p class="citation"></blockquote><h3 id=58122--101350-development-of-automated-neural-network-prediction-for-echocardiographic-left-ventricular-ejection-fraction-yuting-zhang-et-al-2024>(58/122 | 101/350) Development of Automated Neural Network Prediction for Echocardiographic Left ventricular Ejection Fraction (Yuting Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuting Zhang, Boyang Liu, Karina V. Bunting, David Brind, Alexander Thorley, Andreas Karwath, Wenqi Lu, Diwei Zhou, Xiaoxia Wang, Alastair R. Mobley, Otilia Tica, Georgios Gkoutos, Dipak Kotecha, Jinming Duan. (2024)<br><strong>Development of Automated Neural Network Prediction for Echocardiographic Left ventricular Ejection Fraction</strong><br><button class=copy-to-clipboard title="Development of Automated Neural Network Prediction for Echocardiographic Left ventricular Ejection Fraction" index=101>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-101 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12152v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12152v1.pdf filename=2403.12152v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The echocardiographic measurement of left ventricular ejection fraction (LVEF) is fundamental to the diagnosis and classification of patients with heart failure (HF). In order to quantify LVEF automatically and accurately, this paper proposes a new pipeline method based on deep neural networks and ensemble learning. Within the pipeline, an Atrous <b>Convolutional</b> <b>Neural</b> <b>Network</b> (ACNN) was first trained to segment the left ventricle (LV), before employing the area-length formulation based on the ellipsoid single-plane model to calculate LVEF values. This formulation required inputs of LV area, derived from segmentation using an improved Jeffrey&rsquo;s method, as well as LV length, derived from a novel ensemble learning model. To further improve the pipeline&rsquo;s accuracy, an automated peak detection algorithm was used to identify end-diastolic and end-systolic frames, avoiding issues with human error. Subsequently, single-beat LVEF values were averaged across all cardiac cycles to obtain the final LVEF. This method was developed and internally validated in an open-source dataset containing 10,030 echocardiograms. The Pearson&rsquo;s correlation coefficient was 0.83 for LVEF prediction compared to expert human analysis (p&lt;0.001), with a subsequent area under the receiver operator curve (AUROC) of 0.98 (95% confidence interval 0.97 to 0.99) for categorisation of HF with reduced ejection (HFrEF; LVEF&lt;40%). In an external dataset with 200 echocardiograms, this method achieved an AUC of 0.90 (95% confidence interval 0.88 to 0.91) for HFrEF assessment. This study demonstrates that an automated neural network-based calculation of LVEF is comparable to expert clinicians performing time-consuming, frame-by-frame manual evaluation of cardiac systolic function.</p></p class="citation"></blockquote><h3 id=59122--102350-distilling-datasets-into-less-than-one-image-asaf-shul-et-al-2024>(59/122 | 102/350) Distilling Datasets Into Less Than One Image (Asaf Shul et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Asaf Shul, Eliahu Horwitz, Yedid Hoshen. (2024)<br><strong>Distilling Datasets Into Less Than One Image</strong><br><button class=copy-to-clipboard title="Distilling Datasets Into Less Than One Image" index=102>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-102 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Knowledge Distillation, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12040v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12040v1.pdf filename=2403.12040v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Dataset <b>distillation</b> aims to compress a dataset into a much smaller one so that a model trained on the <b>distilled</b> dataset achieves high accuracy. Current methods frame this as maximizing the <b>distilled</b> classification accuracy for a budget of K <b>distilled</b> images-per-class, where K is a positive integer. In this paper, we push the boundaries of dataset <b>distillation,</b> compressing the dataset into less than an image-per-class. It is important to realize that the meaningful quantity is not the number of <b>distilled</b> images-per-class but the number of <b>distilled</b> pixels-per-dataset. We therefore, propose Poster Dataset <b>Distillation</b> (PoDD), a new approach that <b>distills</b> the entire original dataset into a single poster. The poster approach motivates new technical solutions for creating training images and learnable labels. Our method can achieve comparable or better performance with less than an image-per-class compared to existing methods that use one image-per-class. Specifically, our method establishes a new state-of-the-art performance on CIFAR-10, CIFAR-100, and CUB200 using as little as 0.3 images-per-class.</p></p class="citation"></blockquote><h3 id=60122--103350-vfusion3d-learning-scalable-3d-generative-models-from-video-diffusion-models-junlin-han-et-al-2024>(60/122 | 103/350) VFusion3D: Learning Scalable 3D Generative Models from Video Diffusion Models (Junlin Han et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junlin Han, Filippos Kokkinos, Philip Torr. (2024)<br><strong>VFusion3D: Learning Scalable 3D Generative Models from Video Diffusion Models</strong><br><button class=copy-to-clipboard title="VFusion3D: Learning Scalable 3D Generative Models from Video Diffusion Models" index=103>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-103 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-GR, cs-LG, cs.CV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12034v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12034v1.pdf filename=2403.12034v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a novel paradigm for building scalable 3D generative models utilizing pre-trained video <b>diffusion</b> <b>models.</b> The primary obstacle in developing foundation 3D generative models is the limited availability of 3D data. Unlike images, texts, or videos, 3D data are not readily accessible and are difficult to acquire. This results in a significant disparity in scale compared to the vast quantities of other types of data. To address this issue, we propose using a video <b>diffusion</b> <b>model,</b> trained with extensive volumes of text, images, and videos, as a knowledge source for 3D data. By unlocking its multi-view generative capabilities through <b>fine-tuning,</b> we generate a large-scale synthetic multi-view dataset to train a feed-forward 3D generative model. The proposed model, VFusion3D, trained on nearly 3M synthetic multi-view data, can generate a 3D asset from a single image in seconds and achieves superior performance when compared to current SOTA feed-forward 3D generative models, with users preferring our results over 70% of the time.</p></p class="citation"></blockquote><h3 id=61122--104350-dreammotion-space-time-self-similarity-score-distillation-for-zero-shot-video-editing-hyeonho-jeong-et-al-2024>(61/122 | 104/350) DreamMotion: Space-Time Self-Similarity Score Distillation for Zero-Shot Video Editing (Hyeonho Jeong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hyeonho Jeong, Jinho Chang, Geon Yeong Park, Jong Chul Ye. (2024)<br><strong>DreamMotion: Space-Time Self-Similarity Score Distillation for Zero-Shot Video Editing</strong><br><button class=copy-to-clipboard title="DreamMotion: Space-Time Self-Similarity Score Distillation for Zero-Shot Video Editing" index=104>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-104 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Knowledge Distillation, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12002v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12002v1.pdf filename=2403.12002v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Text-driven diffusion-based video editing presents a unique challenge not encountered in image editing literature: establishing real-world motion. Unlike existing video editing approaches, here we focus on score <b>distillation</b> sampling to circumvent the standard reverse diffusion process and initiate optimization from videos that already exhibit natural motion. Our analysis reveals that while video score <b>distillation</b> can effectively introduce new content indicated by target text, it can also cause significant structure and motion deviation. To counteract this, we propose to match space-time self-similarities of the original video and the edited video during the score <b>distillation.</b> Thanks to the use of score <b>distillation,</b> our approach is model-agnostic, which can be applied for both cascaded and non-cascaded video diffusion frameworks. Through extensive comparisons with leading methods, our approach demonstrates its superiority in altering appearances while accurately preserving the original structure and motion.</p></p class="citation"></blockquote><h3 id=62122--105350-subjective-aligned-dateset-and-metric-for-text-to-video-quality-assessment-tengchuan-kou-et-al-2024>(62/122 | 105/350) Subjective-Aligned Dateset and Metric for Text-to-Video Quality Assessment (Tengchuan Kou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tengchuan Kou, Xiaohong Liu, Zicheng Zhang, Chunyi Li, Haoning Wu, Xiongkuo Min, Guangtao Zhai, Ning Liu. (2024)<br><strong>Subjective-Aligned Dateset and Metric for Text-to-Video Quality Assessment</strong><br><button class=copy-to-clipboard title="Subjective-Aligned Dateset and Metric for Text-to-Video Quality Assessment" index=105>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-105 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Transformer, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11956v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11956v2.pdf filename=2403.11956v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the rapid development of generative models, Artificial Intelligence-Generated Contents (AIGC) have exponentially increased in daily lives. Among them, Text-to-Video (T2V) generation has received widespread attention. Though many T2V models have been released for generating high perceptual quality videos, there is still lack of a method to evaluate the quality of these videos quantitatively. To solve this issue, we establish the largest-scale Text-to-Video Quality Assessment DataBase (T2VQA-DB) to date. The dataset is composed of 10,000 videos generated by 9 different T2V models. We also conduct a subjective study to obtain each video&rsquo;s corresponding mean opinion score. Based on T2VQA-DB, we propose a novel <b>transformer-based</b> model for subjective-aligned Text-to-Video Quality Assessment (T2VQA). The model extracts features from text-video alignment and video fidelity perspectives, then it leverages the ability of a <b>large</b> <b>language</b> <b>model</b> to give the prediction score. Experimental results show that T2VQA outperforms existing T2V metrics and SOTA video quality assessment models. Quantitative analysis indicates that T2VQA is capable of giving subjective-align predictions, validating its effectiveness. The dataset and code will be released at <a href=https://github.com/QMME/T2VQA>https://github.com/QMME/T2VQA</a>.</p></p class="citation"></blockquote><h3 id=63122--106350-prioritized-semantic-learning-for-zero-shot-instance-navigation-xander-sun-et-al-2024>(63/122 | 106/350) Prioritized Semantic Learning for Zero-shot Instance Navigation (Xander Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xander Sun, Louis Lau, Hoyard Zhi, Ronghe Qiu, Junwei Liang. (2024)<br><strong>Prioritized Semantic Learning for Zero-shot Instance Navigation</strong><br><button class=copy-to-clipboard title="Prioritized Semantic Learning for Zero-shot Instance Navigation" index=106>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-106 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Zero-shot, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11650v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11650v1.pdf filename=2403.11650v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study <b>zero-shot</b> instance navigation, in which the agent navigates to a specific object without using object annotations for training. Previous object navigation approaches apply the image-goal navigation (ImageNav) task (go to the location of an image) for pretraining, and transfer the agent to achieve object goals using a <b>vision-language</b> model. However, these approaches lead to issues of semantic neglect, where the model fails to learn meaningful semantic alignments. In this paper, we propose a Prioritized Semantic Learning (PSL) method to improve the semantic understanding ability of navigation agents. Specifically, a semantic-enhanced PSL agent is proposed and a prioritized semantic training strategy is introduced to select goal images that exhibit clear semantic supervision and relax the reward function from strict exact view matching. At inference time, a semantic expansion inference scheme is designed to preserve the same granularity level of the goal-semantic as training. Furthermore, for the popular HM3D environment, we present an Instance Navigation (InstanceNav) task that requires going to a specific object instance with detailed descriptions, as opposed to the Object Navigation (ObjectNav) task where the goal is defined merely by the object category. Our PSL agent outperforms the previous state-of-the-art by 66% on <b>zero-shot</b> ObjectNav in terms of success rate and is also superior on the new InstanceNav task. Code will be released at https://anonymous.4open. science/r/PSL/.</p></p class="citation"></blockquote><h3 id=64122--107350-compositional-kronecker-context-optimization-for-vision-language-models-kun-ding-et-al-2024>(64/122 | 107/350) Compositional Kronecker Context Optimization for Vision-Language Models (Kun Ding et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kun Ding, Xiaohui Li, Qiang Yu, Ying Wang, Haojian Zhang, Shiming Xiang. (2024)<br><strong>Compositional Kronecker Context Optimization for Vision-Language Models</strong><br><button class=copy-to-clipboard title="Compositional Kronecker Context Optimization for Vision-Language Models" index=107>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-107 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11631v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11631v1.pdf filename=2403.11631v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Context Optimization (CoOp) has emerged as a simple yet effective technique for adapting CLIP-like <b>vision-language</b> models to downstream image recognition tasks. Nevertheless, learning compact context with satisfactory base-to-new, domain and cross-task generalization ability while adapting to new tasks is still a challenge. To tackle such a challenge, we propose a lightweight yet generalizable approach termed Compositional Kronecker Context Optimization (CK-CoOp). Technically, the <b>prompt&rsquo;s</b> context words in CK-CoOp are learnable vectors, which are crafted by linearly combining base vectors sourced from a dictionary. These base vectors consist of a non-learnable component obtained by quantizing the weights in the token embedding layer, and a learnable component constructed by applying Kronecker product on several learnable tiny matrices. Intuitively, the compositional structure mitigates the risk of overfitting on training data by remembering more pre-trained knowledge. Meantime, the Kronecker product breaks the non-learnable restrictions of the dictionary, thereby enhancing representation ability with minimal additional parameters. Extensive experiments confirm that CK-CoOp achieves state-of-the-art performance under base-to-new, domain and cross-task generalization evaluation, but also has the metrics of fewer learnable parameters and efficient training and inference speed.</p></p class="citation"></blockquote><h3 id=65122--108350-lora-composer-leveraging-low-rank-adaptation-for-multi-concept-customization-in-training-free-diffusion-models-yang-yang-et-al-2024>(65/122 | 108/350) LoRA-Composer: Leveraging Low-Rank Adaptation for Multi-Concept Customization in Training-Free Diffusion Models (Yang Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yang Yang, Wen Wang, Liang Peng, Chaotian Song, Yao Chen, Hengjia Li, Xiaolong Yang, Qinglin Lu, Deng Cai, Boxi Wu, Wei Liu. (2024)<br><strong>LoRA-Composer: Leveraging Low-Rank Adaptation for Multi-Concept Customization in Training-Free Diffusion Models</strong><br><button class=copy-to-clipboard title="LoRA-Composer: Leveraging Low-Rank Adaptation for Multi-Concept Customization in Training-Free Diffusion Models" index=108>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-108 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11627v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11627v1.pdf filename=2403.11627v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Customization generation techniques have significantly advanced the synthesis of specific concepts across varied contexts. Multi-concept customization emerges as the challenging task within this domain. Existing approaches often rely on training a Low-Rank Adaptations (LoRA) fusion matrix of multiple LoRA to merge various concepts into a single image. However, we identify this straightforward method faces two major challenges: 1) concept confusion, which occurs when the model cannot preserve distinct individual characteristics, and 2) concept vanishing, where the model fails to generate the intended subjects. To address these issues, we introduce LoRA-Composer, a training-free framework designed for seamlessly integrating multiple LoRAs, thereby enhancing the harmony among different concepts within generated images. LoRA-Composer addresses concept vanishing through Concept Injection Constraints, enhancing concept visibility via an expanded cross-attention mechanism. To combat concept confusion, Concept Isolation Constraints are introduced, refining the <b>self-attention</b> computation. Furthermore, Latent Re-initialization is proposed to effectively stimulate concept-specific latent within designated regions. Our extensive testing showcases a notable enhancement in LoRA-Composer&rsquo;s performance compared to standard baselines, especially when eliminating the image-based conditions like canny edge or pose estimations. Code is released at <a href=https://github.com/Young98CN/LoRA>https://github.com/Young98CN/LoRA</a>_Composer.</p></p class="citation"></blockquote><h3 id=66122--109350-crs-diff-controllable-generative-remote-sensing-foundation-model-datao-tang-et-al-2024>(66/122 | 109/350) CRS-Diff: Controllable Generative Remote Sensing Foundation Model (Datao Tang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Datao Tang, Xiangyong Cao, Xingsong Hou, Zhongyuan Jiang, Deyu Meng. (2024)<br><strong>CRS-Diff: Controllable Generative Remote Sensing Foundation Model</strong><br><button class=copy-to-clipboard title="CRS-Diff: Controllable Generative Remote Sensing Foundation Model" index=109>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-109 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Foundation Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11614v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11614v1.pdf filename=2403.11614v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The emergence of <b>diffusion</b> <b>models</b> has revolutionized the field of image generation, providing new methods for creating high-quality, high-resolution images across various applications. However, the potential of these models for generating domain-specific images, particularly remote sensing (RS) images, remains largely untapped. RS images that are notable for their high resolution, extensive coverage, and rich information content, bring new challenges that general <b>diffusion</b> <b>models</b> may not adequately address. This paper proposes CRS-Diff, a pioneering <b>diffusion</b> <b>modeling</b> framework specifically tailored for generating remote sensing imagery, leveraging the inherent advantages of <b>diffusion</b> <b>models</b> while integrating advanced control mechanisms to ensure that the imagery is not only visually clear but also enriched with geographic and temporal information. The model integrates global and local control inputs, enabling precise combinations of generation conditions to refine the generation process. A comprehensive evaluation of CRS-Diff has demonstrated its superior capability to generate RS imagery both in a single condition and multiple conditions compared with previous methods in terms of image quality and diversity.</p></p class="citation"></blockquote><h3 id=67122--110350-dynosurf-neural-deformation-based-temporally-consistent-dynamic-surface-reconstruction-yuxin-yao-et-al-2024>(67/122 | 110/350) DynoSurf: Neural Deformation-based Temporally Consistent Dynamic Surface Reconstruction (Yuxin Yao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuxin Yao, Siyu Ren, Junhui Hou, Zhi Deng, Juyong Zhang, Wenping Wang. (2024)<br><strong>DynoSurf: Neural Deformation-based Temporally Consistent Dynamic Surface Reconstruction</strong><br><button class=copy-to-clipboard title="DynoSurf: Neural Deformation-based Temporally Consistent Dynamic Surface Reconstruction" index=110>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-110 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Unsupervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11586v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11586v1.pdf filename=2403.11586v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper explores the problem of reconstructing temporally consistent surfaces from a 3D point cloud sequence without correspondence. To address this challenging task, we propose DynoSurf, an <b>unsupervised</b> <b>learning</b> framework integrating a template surface representation with a learnable deformation field. Specifically, we design a coarse-to-fine strategy for learning the template surface based on the deformable tetrahedron representation. Furthermore, we propose a learnable deformation representation based on the learnable control points and blending weights, which can deform the template surface non-rigidly while maintaining the consistency of the local shape. Experimental results demonstrate the significant superiority of DynoSurf over current state-of-the-art approaches, showcasing its potential as a powerful tool for dynamic mesh reconstruction. The code is publicly available at <a href=https://github.com/yaoyx689/DynoSurf>https://github.com/yaoyx689/DynoSurf</a>.</p></p class="citation"></blockquote><h3 id=68122--111350-augment-before-copy-paste-data-and-memory-efficiency-oriented-instance-segmentation-framework-for-sport-scenes-chih-chung-hsu-et-al-2024>(68/122 | 111/350) Augment Before Copy-Paste: Data and Memory Efficiency-Oriented Instance Segmentation Framework for Sport-scenes (Chih-Chung Hsu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chih-Chung Hsu, Chia-Ming Lee, Ming-Shyen Wu. (2024)<br><strong>Augment Before Copy-Paste: Data and Memory Efficiency-Oriented Instance Segmentation Framework for Sport-scenes</strong><br><button class=copy-to-clipboard title="Augment Before Copy-Paste: Data and Memory Efficiency-Oriented Instance Segmentation Framework for Sport-scenes" index=111>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-111 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-MM, cs.CV<br>Keyword Score: 20<br>Keywords: Data Augmentation, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11572v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11572v1.pdf filename=2403.11572v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Instance segmentation is a fundamental task in computer vision with broad applications across various industries. In recent years, with the proliferation of deep learning and artificial intelligence applications, how to train effective models with limited <b>data</b> <b>has</b> become a pressing issue for both academia and industry. In the Visual Inductive Priors challenge (VIPriors2023), participants must train a model capable of precisely locating individuals on a basketball court, all while working with limited <b>data</b> <b>and</b> without the use of <b>transfer</b> <b>learning</b> or pre-trained models. We propose Memory effIciency inStance Segmentation framework based on visual inductive prior flow propagation that effectively incorporates inherent prior information from the dataset into both the <b>data</b> <b>preprocessing</b> and <b>data</b> <b>augmentation</b> stages, as well as the inference phase. Our team (ACVLAB) experiments demonstrate that our model achieves promising performance (0.509 <a href=mailto:AP@0.50>AP@0.50</a>:0.95) even under limited <b>data</b> <b>and</b> memory constraints.</p></p class="citation"></blockquote><h3 id=69122--112350-logicaldefender-discovering-extracting-and-utilizing-common-sense-knowledge-yuhe-liu-et-al-2024>(69/122 | 112/350) LogicalDefender: Discovering, Extracting, and Utilizing Common-Sense Knowledge (Yuhe Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuhe Liu, Mengxue Kang, Zengchang Qin, Xiangxiang Chu. (2024)<br><strong>LogicalDefender: Discovering, Extracting, and Utilizing Common-Sense Knowledge</strong><br><button class=copy-to-clipboard title="LogicalDefender: Discovering, Extracting, and Utilizing Common-Sense Knowledge" index=112>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-112 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Text2image, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11570v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11570v1.pdf filename=2403.11570v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Large <b>text-to-image</b> models have achieved astonishing performance in synthesizing diverse and high-quality images guided by texts. With detail-oriented conditioning control, even finer-grained spatial control can be achieved. However, some generated images still appear unreasonable, even with plentiful object features and a harmonious style. In this paper, we delve into the underlying causes and find that deep-level logical information, serving as common-sense knowledge, plays a significant role in understanding and processing images. Nonetheless, almost all models have neglected the importance of logical relations in images, resulting in poor performance in this aspect. Following this observation, we propose LogicalDefender, which combines images with the logical knowledge already <b>summarized</b> by humans in text. This encourages models to learn logical knowledge faster and better, and concurrently, extracts the widely applicable logical knowledge from both images and human knowledge. Experiments show that our model has achieved better logical performance, and the extracted logical knowledge can be effectively applied to other scenarios.</p></p class="citation"></blockquote><h3 id=70122--113350-learning-unified-reference-representation-for-unsupervised-multi-class-anomaly-detection-liren-he-et-al-2024>(70/122 | 113/350) Learning Unified Reference Representation for Unsupervised Multi-class Anomaly Detection (Liren He et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Liren He, Zhengkai Jiang, Jinlong Peng, Liang Liu, Qiangang Du, Xiaobin Hu, Wenbing Zhu, Mingmin Chi, Yabiao Wang, Chengjie Wang. (2024)<br><strong>Learning Unified Reference Representation for Unsupervised Multi-class Anomaly Detection</strong><br><button class=copy-to-clipboard title="Learning Unified Reference Representation for Unsupervised Multi-class Anomaly Detection" index=113>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-113 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Anomaly Detection, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11561v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11561v1.pdf filename=2403.11561v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the field of multi-class <b>anomaly</b> <b>detection,</b> reconstruction-based methods derived from single-class <b>anomaly</b> <b>detection</b> face the well-known challenge of <code>learning shortcuts'', wherein the model fails to learn the patterns of normal samples as it should, opting instead for shortcuts such as identity mapping or artificial noise elimination. Consequently, the model becomes unable to reconstruct genuine anomalies as normal instances, resulting in a failure of &lt;b>anomaly&lt;/b> &lt;b>detection.&lt;/b> To counter this issue, we present a novel unified feature reconstruction-based &lt;b>anomaly&lt;/b> &lt;b>detection&lt;/b> framework termed RLR (Reconstruct features from a Learnable Reference representation). Unlike previous methods, RLR utilizes learnable reference representations to compel the model to learn normal feature patterns explicitly, thereby prevents the model from succumbing to the </code>learning shortcuts&rsquo;&rsquo; issue. Additionally, RLR incorporates locality constraints into the learnable reference to facilitate more effective normal pattern capture and utilizes a masked learnable key attention mechanism to enhance robustness. Evaluation of RLR on the 15-category MVTec-AD dataset and the 12-category VisA dataset shows superior performance compared to state-of-the-art methods under the unified setting. The code of RLR will be publicly available.</p></p class="citation"></blockquote><h3 id=71122--114350-hierarchical-spatial-proximity-reasoning-for-vision-and-language-navigation-ming-xu-et-al-2024>(71/122 | 114/350) Hierarchical Spatial Proximity Reasoning for Vision-and-Language Navigation (Ming Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ming Xu, Zilong Xie. (2024)<br><strong>Hierarchical Spatial Proximity Reasoning for Vision-and-Language Navigation</strong><br><button class=copy-to-clipboard title="Hierarchical Spatial Proximity Reasoning for Vision-and-Language Navigation" index=114>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-114 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Reasoning, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11541v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11541v1.pdf filename=2403.11541v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Most <b>Vision-and-Language</b> Navigation (VLN) algorithms tend to make decision errors, primarily due to a lack of visual common sense and insufficient <b>reasoning</b> capabilities. To address this issue, this paper proposes a Hierarchical Spatial Proximity <b>Reasoning</b> (HSPR) model. Firstly, we design a Scene Understanding Auxiliary Task (SUAT) to assist the agent in constructing a knowledge base of hierarchical spatial proximity for <b>reasoning</b> navigation. Specifically, this task utilizes panoramic views and object features to identify regions in the navigation environment and uncover the adjacency relationships between regions, objects, and region-object pairs. Secondly, we dynamically construct a semantic topological map through agent-environment interactions and propose a Multi-step <b>Reasoning</b> Navigation Algorithm (MRNA) based on the map. This algorithm continuously plans various feasible paths from one region to another, utilizing the constructed proximity knowledge base, enabling more efficient exploration. Additionally, we introduce a Proximity Adaptive Attention Module (PAAM) and Residual Fusion Method (RFM) to enable the model to obtain more accurate navigation decision confidence. Finally, we conduct experiments on publicly available datasets including REVERIE, SOON, R2R, and R4R to validate the effectiveness of the proposed approach.</p></p class="citation"></blockquote><h3 id=72122--115350-zero-shot-compound-expression-recognition-with-visual-language-model-at-the-6th-abaw-challenge-jiahe-wang-et-al-2024>(72/122 | 115/350) Zero-shot Compound Expression Recognition with Visual Language Model at the 6th ABAW Challenge (Jiahe Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiahe Wang, Jiale Huang, Bingzhao Cai, Yifan Cao, Xin Yun, Shangfei Wang. (2024)<br><strong>Zero-shot Compound Expression Recognition with Visual Language Model at the 6th ABAW Challenge</strong><br><button class=copy-to-clipboard title="Zero-shot Compound Expression Recognition with Visual Language Model at the 6th ABAW Challenge" index=115>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-115 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Convolutional Neural Network, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11450v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11450v1.pdf filename=2403.11450v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Conventional approaches to facial expression recognition primarily focus on the classification of six basic facial expressions. Nevertheless, real-world situations present a wider range of complex compound expressions that consist of combinations of these basics ones due to limited availability of comprehensive training datasets. The 6th Workshop and Competition on Affective Behavior Analysis in-the-wild (ABAW) offered unlabeled datasets containing compound expressions. In this study, we propose a <b>zero-shot</b> approach for recognizing compound expressions by leveraging a pretrained visual language model integrated with some traditional <b>CNN</b> networks.</p></p class="citation"></blockquote><h3 id=73122--116350-dreamsampler-unifying-diffusion-sampling-and-score-distillation-for-image-manipulation-jeongsol-kim-et-al-2024>(73/122 | 116/350) DreamSampler: Unifying Diffusion Sampling and Score Distillation for Image Manipulation (Jeongsol Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jeongsol Kim, Geon Yeong Park, Jong Chul Ye. (2024)<br><strong>DreamSampler: Unifying Diffusion Sampling and Score Distillation for Image Manipulation</strong><br><button class=copy-to-clipboard title="DreamSampler: Unifying Diffusion Sampling and Score Distillation for Image Manipulation" index=116>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-116 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11415v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11415v1.pdf filename=2403.11415v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Reverse sampling and score-distillation have emerged as main workhorses in recent years for image manipulation using latent <b>diffusion</b> <b>models</b> (LDMs). While reverse <b>diffusion</b> <b>sampling</b> often requires adjustments of LDM architecture or feature engineering, score <b>distillation</b> offers a simple yet powerful model-agnostic approach, but it is often prone to mode-collapsing. To address these limitations and leverage the strengths of both approaches, here we introduce a novel framework called {\em DreamSampler}, which seamlessly integrates these two distinct approaches through the lens of regularized latent optimization. Similar to score-distillation, DreamSampler is a model-agnostic approach applicable to any LDM architecture, but it allows both <b>distillation</b> and reverse sampling with additional guidance for image editing and reconstruction. Through experiments involving image editing, SVG reconstruction and etc, we demonstrate the competitive performance of DreamSampler compared to existing approaches, while providing new applications.</p></p class="citation"></blockquote><h3 id=74122--117350-federated-modality-specific-encoders-and-multimodal-anchors-for-personalized-brain-tumor-segmentation-qian-dai-et-al-2024>(74/122 | 117/350) Federated Modality-specific Encoders and Multimodal Anchors for Personalized Brain Tumor Segmentation (Qian Dai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qian Dai, Dong Wei, Hong Liu, Jinghan Sun, Liansheng Wang, Yefeng Zheng. (2024)<br><strong>Federated Modality-specific Encoders and Multimodal Anchors for Personalized Brain Tumor Segmentation</strong><br><button class=copy-to-clipboard title="Federated Modality-specific Encoders and Multimodal Anchors for Personalized Brain Tumor Segmentation" index=117>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-117 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 19<br>Keywords: Benchmarking, Federated Learning, Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11803v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11803v1.pdf filename=2403.11803v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Most existing <b>federated</b> <b>learning</b> (FL) methods for medical image analysis only considered intramodal heterogeneity, limiting their applicability to <b>multimodal</b> imaging applications. In practice, it is not uncommon that some FL participants only possess a subset of the complete imaging modalities, posing inter-modal heterogeneity as a challenge to effectively training a global model on all participants&rsquo; data. In addition, each participant would expect to obtain a personalized model tailored for its local data characteristics from the FL in such a scenario. In this work, we propose a new FL framework with <b>federated</b> <b>modality-specific</b> encoders and <b>multimodal</b> anchors (FedMEMA) to simultaneously address the two concurrent issues. Above all, FedMEMA employs an exclusive encoder for each modality to account for the inter-modal heterogeneity in the first place. In the meantime, while the encoders are shared by the participants, the decoders are personalized to meet individual needs. Specifically, a server with full-modal data employs a fusion decoder to aggregate and fuse representations from all modality-specific encoders, thus bridging the modalities to optimize the encoders via backpropagation reversely. Meanwhile, multiple anchors are extracted from the fused <b>multimodal</b> representations and distributed to the clients in addition to the encoder parameters. On the other end, the clients with incomplete modalities calibrate their missing-modal representations toward the global full-modal anchors via scaled dot-product cross-attention, making up the information loss due to absent modalities while adapting the representations of present ones. FedMEMA is validated on the BraTS 2020 <b>benchmark</b> for <b>multimodal</b> brain tumor segmentation. Results show that it outperforms various up-to-date methods for <b>multimodal</b> and personalized FL and that its novel designs are effective. Our code is available.</p></p class="citation"></blockquote><h3 id=75122--118350-graphbev-towards-robust-bev-feature-alignment-for-multi-modal-3d-object-detection-ziying-song-et-al-2024>(75/122 | 118/350) GraphBEV: Towards Robust BEV Feature Alignment for Multi-Modal 3D Object Detection (Ziying Song et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ziying Song, Lei Yang, Shaoqing Xu, Lin Liu, Dongyang Xu, Caiyan Jia, Feiyang Jia, Li Wang. (2024)<br><strong>GraphBEV: Towards Robust BEV Feature Alignment for Multi-Modal 3D Object Detection</strong><br><button class=copy-to-clipboard title="GraphBEV: Towards Robust BEV Feature Alignment for Multi-Modal 3D Object Detection" index=118>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-118 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 16<br>Keywords: Object Detection, Graph, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11848v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11848v1.pdf filename=2403.11848v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Integrating LiDAR and camera information into Bird&rsquo;s-Eye-View (BEV) representation has emerged as a crucial aspect of 3D <b>object</b> <b>detection</b> in autonomous driving. However, existing methods are susceptible to the inaccurate calibration relationship between LiDAR and the camera sensor. Such inaccuracies result in errors in depth estimation for the camera branch, ultimately causing misalignment between LiDAR and camera BEV features. In this work, we propose a robust fusion framework called <b>Graph</b> BEV. Addressing errors caused by inaccurate point cloud projection, we introduce a Local Align module that employs neighbor-aware depth features via <b>Graph</b> matching. Additionally, we propose a Global Align module to rectify the misalignment between LiDAR and camera BEV features. Our <b>Graph</b> BEV framework achieves state-of-the-art performance, with an mAP of 70.1%, surpassing BEV Fusion by 1.6% on the nuscenes validation set. Importantly, our <b>Graph</b> BEV outperforms BEV Fusion by 8.3% under conditions with misalignment noise.</p></p class="citation"></blockquote><h3 id=76122--119350-modality-agnostic-fmri-decoding-of-vision-and-language-mitja-nikolaus-et-al-2024>(76/122 | 119/350) Modality-Agnostic fMRI Decoding of Vision and Language (Mitja Nikolaus et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mitja Nikolaus, Milad Mozafari, Nicholas Asher, Leila Reddy, Rufin VanRullen. (2024)<br><strong>Modality-Agnostic fMRI Decoding of Vision and Language</strong><br><button class=copy-to-clipboard title="Modality-Agnostic fMRI Decoding of Vision and Language" index=119>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-119 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CL, cs-CV, cs.CV<br>Keyword Score: 16<br>Keywords: Multi-modal, Multi-modal, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11771v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11771v1.pdf filename=2403.11771v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Previous studies have shown that it is possible to map brain activation data of subjects viewing images onto the feature representation space of not only vision models (modality-specific decoding) but also language models (cross-modal decoding). In this work, we introduce and use a new large-scale fMRI dataset (~8,500 trials per subject) of people watching both images and text descriptions of such images. This novel dataset enables the development of modality-agnostic decoders: a single decoder that can predict which stimulus a subject is seeing, irrespective of the modality (image or text) in which the stimulus is presented. We train and evaluate such decoders to map brain signals onto stimulus representations from a large range of publicly available vision, language and <b>multimodal</b> (vision+language) models. Our findings reveal that (1) modality-agnostic decoders perform as well as (and sometimes even better than) modality-specific decoders (2) modality-agnostic decoders mapping brain data onto representations from unimodal models perform as well as decoders relying on <b>multimodal</b> representations (3) while language and low-level visual (occipital) brain regions are best at decoding text and image stimuli, respectively, high-level visual (temporal) regions perform well on both stimulus types.</p></p class="citation"></blockquote><h3 id=77122--120350-circle-representation-for-medical-instance-object-segmentation-juming-xiong-et-al-2024>(77/122 | 120/350) Circle Representation for Medical Instance Object Segmentation (Juming Xiong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Juming Xiong, Ethan H. Nguyen, Yilin Liu, Ruining Deng, Regina N Tyree, Hernan Correa, Girish Hiremath, Yaohong Wang, Haichun Yang, Agnes B. Fogo, Yuankai Huo. (2024)<br><strong>Circle Representation for Medical Instance Object Segmentation</strong><br><button class=copy-to-clipboard title="Circle Representation for Medical Instance Object Segmentation" index=120>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-120 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 16<br>Keywords: Graph, Benchmarking, Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11507v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11507v1.pdf filename=2403.11507v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, circle representation has been introduced for medical imaging, designed specifically to enhance the detection of instance objects that are spherically shaped (e.g., cells, glomeruli, and nuclei). Given its outstanding effectiveness in instance detection, it is compelling to consider the application of circle representation for segmenting instance medical objects. In this study, we introduce CircleSnake, a simple end-to-end segmentation approach that utilizes circle contour deformation for segmenting ball-shaped medical objects at the instance level. The innovation of CircleSnake lies in these three areas: (1) It substitutes the complex bounding box-to-octagon contour transformation with a more consistent and rotation-invariant bounding circle-to-circle contour adaptation. This adaptation specifically targets ball-shaped medical objects. (2) The circle representation employed in CircleSnake significantly reduces the degrees of freedom to two, compared to eight in the octagon representation. This reduction enhances both the robustness of the segmentation performance and the rotational consistency of the method. (3) CircleSnake is the first end-to-end deep instance segmentation pipeline to incorporate circle representation, encompassing consistent circle detection, circle contour proposal, and circular <b>convolution</b> in a unified framework. This integration is achieved through the novel application of circular <b>graph</b> <b>convolution</b> within the context of circle detection and instance segmentation. In practical applications, such as the detection of glomeruli, nuclei, and eosinophils in pathological images, CircleSnake has demonstrated superior performance and greater rotation invariance when compared to <b>benchmarks.</b> The code has been made publicly available: <a href=https://github.com/hrlblab/CircleSnake>https://github.com/hrlblab/CircleSnake</a>.</p></p class="citation"></blockquote><h3 id=78122--121350-gnerp-gaussian-guided-neural-reconstruction-of-reflective-objects-with-noisy-polarization-priors-li-yang-et-al-2024>(78/122 | 121/350) GNeRP: Gaussian-guided Neural Reconstruction of Reflective Objects with Noisy Polarization Priors (LI Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>LI Yang, WU Ruizheng, LI Jiyong, CHEN Ying-cong. (2024)<br><strong>GNeRP: Gaussian-guided Neural Reconstruction of Reflective Objects with Noisy Polarization Priors</strong><br><button class=copy-to-clipboard title="GNeRP: Gaussian-guided Neural Reconstruction of Reflective Objects with Noisy Polarization Priors" index=121>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-121 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 15<br>Keywords: Geometry, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11899v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11899v1.pdf filename=2403.11899v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Learning surfaces from neural radiance field (NeRF) became a rising topic in Multi-View Stereo (MVS). Recent Signed Distance Function (SDF)-based methods demonstrated their ability to reconstruct accurate 3D shapes of Lambertian scenes. However, their results on reflective scenes are unsatisfactory due to the entanglement of specular radiance and complicated <b>geometry.</b> To address the challenges, we propose a Gaussian-based representation of normals in SDF fields. <b>Supervised</b> by polarization priors, this representation guides the learning of <b>geometry</b> behind the specular reflection and captures more details than existing methods. Moreover, we propose a reweighting strategy in the optimization process to alleviate the noise issue of polarization priors. To validate the effectiveness of our design, we capture polarimetric information, and ground truth meshes in additional reflective scenes with various <b>geometry.</b> We also evaluated our framework on the PANDORA dataset. Comparisons prove our method outperforms existing neural 3D reconstruction methods in reflective scenes by a large margin.</p></p class="citation"></blockquote><h3 id=79122--122350-relational-representation-learning-network-for-cross-spectral-image-patch-matching-chuang-yu-et-al-2024>(79/122 | 122/350) Relational Representation Learning Network for Cross-Spectral Image Patch Matching (Chuang Yu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chuang Yu, Yunpeng Liu, Jinmiao Zhao, Dou Quan, Zelin Shi. (2024)<br><strong>Relational Representation Learning Network for Cross-Spectral Image Patch Matching</strong><br><button class=copy-to-clipboard title="Relational Representation Learning Network for Cross-Spectral Image Patch Matching" index=122>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-122 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 15<br>Keywords: Autoencoder, Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11751v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11751v1.pdf filename=2403.11751v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, feature relation learning has drawn widespread attention in cross-spectral image patch matching. However, existing related research focuses on extracting diverse relations between image patch features and ignores sufficient intrinsic feature <b>representations</b> <b>of</b> individual image patches. Therefore, an innovative relational <b>representation</b> <b>learning</b> idea is proposed for the first time, which simultaneously focuses on sufficiently mining the intrinsic features of individual image patches and the relations between image patch features. Based on this, we construct a lightweight Relational <b>Representation</b> <b>Learning</b> Network (RRL-Net). Specifically, we innovatively construct an <b>autoencoder</b> to fully characterize the individual intrinsic features, and introduce a Feature Interaction Learning (FIL) module to extract deep-level feature relations. To further fully mine individual intrinsic features, a lightweight Multi-dimensional Global-to-Local Attention (MGLA) module is constructed to enhance the global feature extraction of individual image patches and capture local dependencies within global features. By combining the MGLA module, we further explore the feature extraction network and construct an Attention-based Lightweight Feature Extraction (ALFE) network. In addition, we propose a Multi-Loss Post-Pruning (MLPP) optimization strategy, which greatly promotes network optimization while avoiding increases in parameters and inference time. Extensive experiments demonstrate that our RRL-Net achieves state-of-the-art (SOTA) performance on multiple public datasets. Our code will be made public later.</p></p class="citation"></blockquote><h3 id=80122--123350-zero-shot-image-feature-consensus-with-deep-functional-maps-xinle-cheng-et-al-2024>(80/122 | 123/350) Zero-Shot Image Feature Consensus with Deep Functional Maps (Xinle Cheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinle Cheng, Congyue Deng, Adam Harley, Yixin Zhu, Leonidas Guibas. (2024)<br><strong>Zero-Shot Image Feature Consensus with Deep Functional Maps</strong><br><button class=copy-to-clipboard title="Zero-Shot Image Feature Consensus with Deep Functional Maps" index=123>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-123 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12038v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12038v1.pdf filename=2403.12038v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Correspondences emerge from large-scale vision models trained for generative and discriminative tasks. This has been revealed and <b>benchmarked</b> by computing correspondence maps between pairs of images, using nearest neighbors on the feature grids. Existing work has attempted to improve the quality of these correspondence maps by carefully mixing features from different sources, such as by combining the features of different layers or networks. We point out that a better correspondence strategy is available, which directly imposes structure on the correspondence field: the functional map. Wielding this simple mathematical tool, we lift the correspondence problem from the pixel space to the function space and directly optimize for mappings that are globally coherent. We demonstrate that our technique yields correspondences that are not only smoother but also more accurate, with the possibility of better reflecting the knowledge embedded in the large-scale vision models that we are studying. Our approach sets a new state-of-the-art on various dense correspondence tasks. We also demonstrate our effectiveness in keypoint correspondence and affordance map transfer.</p></p class="citation"></blockquote><h3 id=81122--124350-regennet-towards-human-action-reaction-synthesis-liang-xu-et-al-2024>(81/122 | 124/350) ReGenNet: Towards Human Action-Reaction Synthesis (Liang Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Liang Xu, Yizhou Zhou, Yichao Yan, Xin Jin, Wenhan Zhu, Fengyun Rao, Xiaokang Yang, Wenjun Zeng. (2024)<br><strong>ReGenNet: Towards Human Action-Reaction Synthesis</strong><br><button class=copy-to-clipboard title="ReGenNet: Towards Human Action-Reaction Synthesis" index=124>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-124 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11882v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11882v1.pdf filename=2403.11882v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Humans constantly interact with their surrounding environments. Current human-centric generative models mainly focus on synthesizing humans plausibly interacting with static scenes and objects, while the dynamic human action-reaction synthesis for ubiquitous causal human-human interactions is less explored. Human-human interactions can be regarded as asymmetric with actors and reactors in atomic interaction periods. In this paper, we comprehensively analyze the asymmetric, dynamic, synchronous, and detailed nature of human-human interactions and propose the first multi-setting human action-reaction synthesis <b>benchmark</b> to generate human reactions conditioned on given human actions. To begin with, we propose to annotate the actor-reactor order of the interaction sequences for the NTU120, InterHuman, and Chi3D datasets. Based on them, a diffusion-based generative model with a <b>Transformer</b> decoder architecture called ReGenNet together with an explicit distance-based interaction loss is proposed to predict human reactions in an online manner, where the future states of actors are unavailable to reactors. Quantitative and qualitative results show that our method can generate instant and plausible human reactions compared to the baselines, and can generalize to unseen actor motions and viewpoint changes.</p></p class="citation"></blockquote><h3 id=82122--125350-exploring-multi-modal-neural-scene-representations-with-applications-on-thermal-imaging-mert-özer-et-al-2024>(82/122 | 125/350) Exploring Multi-modal Neural Scene Representations With Applications on Thermal Imaging (Mert Özer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mert Özer, Maximilian Weiherer, Martin Hundhausen, Bernhard Egger. (2024)<br><strong>Exploring Multi-modal Neural Scene Representations With Applications on Thermal Imaging</strong><br><button class=copy-to-clipboard title="Exploring Multi-modal Neural Scene Representations With Applications on Thermal Imaging" index=125>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-125 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-GR, cs.CV<br>Keyword Score: 13<br>Keywords: Fine-tuning, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11865v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11865v1.pdf filename=2403.11865v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neural Radiance Fields (NeRFs) quickly evolved as the new de-facto standard for the task of novel view synthesis when trained on a set of RGB images. In this paper, we conduct a comprehensive evaluation of neural scene representations, such as NeRFs, in the context of <b>multi-modal</b> learning. Specifically, we present four different strategies of how to incorporate a second modality, other than RGB, into NeRFs: (1) training from scratch independently on both modalities; (2) pre-training on RGB and <b>fine-tuning</b> on the second modality; (3) adding a second branch; and (4) adding a separate component to predict (color) values of the additional modality. We chose thermal imaging as second modality since it strongly differs from RGB in terms of radiosity, making it challenging to integrate into neural scene representations. For the evaluation of the proposed strategies, we captured a new publicly available multi-view dataset, ThermalMix, consisting of six common objects and about 360 RGB and thermal images in total. We employ cross-modality calibration prior to data capturing, leading to high-quality alignments between RGB and thermal images. Our findings reveal that adding a second branch to NeRF performs best for novel view synthesis on thermal images while also yielding compelling results on RGB. Finally, we also show that our analysis generalizes to other modalities, including near-infrared images and depth maps. Project page: <a href=https://mert-o.github.io/ThermalNeRF/>https://mert-o.github.io/ThermalNeRF/</a>.</p></p class="citation"></blockquote><h3 id=83122--126350-lsknet-a-foundation-lightweight-backbone-for-remote-sensing-yuxuan-li-et-al-2024>(83/122 | 126/350) LSKNet: A Foundation Lightweight Backbone for Remote Sensing (Yuxuan Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuxuan Li, Xiang Li, Yimain Dai, Qibin Hou, Li Liu, Yongxiang Liu, Ming-Ming Cheng, Jian Yang. (2024)<br><strong>LSKNet: A Foundation Lightweight Backbone for Remote Sensing</strong><br><button class=copy-to-clipboard title="LSKNet: A Foundation Lightweight Backbone for Remote Sensing" index=126>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-126 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 13<br>Keywords: Object Detection, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11735v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11735v1.pdf filename=2403.11735v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Remote sensing images pose distinct challenges for downstream tasks due to their inherent complexity. While a considerable amount of research has been dedicated to remote sensing classification, <b>object</b> <b>detection</b> and semantic segmentation, most of these studies have overlooked the valuable prior knowledge embedded within remote sensing scenarios. Such prior knowledge can be useful because remote sensing <b>objects</b> <b>may</b> be mistakenly recognized without referencing a sufficiently long-range context, which can vary for different <b>objects.</b> <b>This</b> paper considers these priors and proposes a lightweight Large Selective Kernel Network (LSKNet) backbone. LSKNet can dynamically adjust its large spatial receptive field to better model the ranging context of various <b>objects</b> <b>in</b> remote sensing scenarios. To our knowledge, large and selective kernel mechanisms have not been previously explored in remote sensing images. Without bells and whistles, our lightweight LSKNet sets new state-of-the-art scores on standard remote sensing classification, <b>object</b> <b>detection</b> and semantic segmentation <b>benchmarks.</b> Our comprehensive analysis further validated the significance of the identified priors and the effectiveness of LSKNet. The code is available at <a href=https://github.com/zcablii/LSKNet>https://github.com/zcablii/LSKNet</a>.</p></p class="citation"></blockquote><h3 id=84122--127350-towards-generalizing-to-unseen-domains-with-few-labels-chamuditha-jayanga-galappaththige-et-al-2024>(84/122 | 127/350) Towards Generalizing to Unseen Domains with Few Labels (Chamuditha Jayanga Galappaththige et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chamuditha Jayanga Galappaththige, Sanoojan Baliah, Malitha Gunawardhana, Muhammad Haris Khan. (2024)<br><strong>Towards Generalizing to Unseen Domains with Few Labels</strong><br><button class=copy-to-clipboard title="Towards Generalizing to Unseen Domains with Few Labels" index=127>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-127 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Semi-Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11674v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11674v2.pdf filename=2403.11674v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We approach the challenge of addressing <b>semi-supervised</b> <b>domain</b> generalization (SSDG). Specifically, our aim is to obtain a model that learns domain-generalizable features by leveraging a limited subset of labelled data alongside a substantially larger pool of unlabeled data. Existing domain generalization (DG) methods which are unable to exploit unlabeled data perform poorly compared to <b>semi-supervised</b> <b>learning</b> (SSL) methods under SSDG setting. Nevertheless, SSL methods have considerable room for performance improvement when compared to fully-supervised DG training. To tackle this underexplored, yet highly practical problem of SSDG, we make the following core contributions. First, we propose a feature-based conformity technique that matches the posterior distributions from the feature space with the pseudo-label from the model&rsquo;s output space. Second, we develop a semantics alignment loss to learn semantically-compatible representations by regularizing the semantic structure in the feature space. Our method is plug-and-play and can be readily integrated with different SSL-based SSDG baselines without introducing any additional parameters. Extensive experimental results across five challenging DG <b>benchmarks</b> with four strong SSL baselines suggest that our method provides consistent and notable gains in two different SSDG settings.</p></p class="citation"></blockquote><h3 id=85122--128350-video-object-segmentation-with-dynamic-query-modulation-hantao-zhou-et-al-2024>(85/122 | 128/350) Video Object Segmentation with Dynamic Query Modulation (Hantao Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hantao Zhou, Runze Hu, Xiu Li. (2024)<br><strong>Video Object Segmentation with Dynamic Query Modulation</strong><br><button class=copy-to-clipboard title="Video Object Segmentation with Dynamic Query Modulation" index=128>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-128 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11529v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11529v1.pdf filename=2403.11529v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Storing intermediate frame segmentations as memory for long-range context modeling, spatial-temporal memory-based methods have recently showcased impressive results in semi-supervised video object segmentation (SVOS). However, these methods face two key limitations: 1) relying on non-local pixel-level matching to read memory, resulting in noisy retrieved features for segmentation; 2) segmenting each object independently without interaction. These shortcomings make the memory-based methods struggle in similar object and multi-object segmentation. To address these issues, we propose a query modulation method, termed QMVOS. This method <b>summarizes</b> object features into dynamic queries and then treats them as dynamic filters for mask prediction, thereby providing high-level descriptions and object-level perception for the model. Efficient and effective multi-object interactions are realized through inter-query attention. Extensive experiments demonstrate that our method can bring significant improvements to the memory-based SVOS method and achieve competitive performance on standard SVOS <b>benchmarks.</b> The code is available at <a href=https://github.com/zht8506/QMVOS>https://github.com/zht8506/QMVOS</a>.</p></p class="citation"></blockquote><h3 id=86122--129350-genflow-generalizable-recurrent-flow-for-6d-pose-refinement-of-novel-objects-sungphill-moon-et-al-2024>(86/122 | 129/350) GenFlow: Generalizable Recurrent Flow for 6D Pose Refinement of Novel Objects (Sungphill Moon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sungphill Moon, Hyeontae Son, Dongcheol Hur, Sangwook Kim. (2024)<br><strong>GenFlow: Generalizable Recurrent Flow for 6D Pose Refinement of Novel Objects</strong><br><button class=copy-to-clipboard title="GenFlow: Generalizable Recurrent Flow for 6D Pose Refinement of Novel Objects" index=129>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-129 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11510v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11510v1.pdf filename=2403.11510v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite the progress of learning-based methods for 6D object pose estimation, the trade-off between accuracy and scalability for novel objects still exists. Specifically, previous methods for novel objects do not make good use of the target object&rsquo;s 3D shape information since they focus on generalization by processing the shape indirectly, making them less effective. We present GenFlow, an approach that enables both accuracy and generalization to novel objects with the guidance of the target object&rsquo;s shape. Our method predicts optical flow between the rendered image and the observed image and refines the 6D pose iteratively. It boosts the performance by a constraint of the 3D shape and the generalizable geometric knowledge learned from an end-to-end differentiable system. We further improve our model by designing a cascade network architecture to exploit the multi-scale correlations and coarse-to-fine refinement. GenFlow ranked first on the unseen object pose estimation <b>benchmarks</b> in both the RGB and RGB-D cases. It also achieves performance competitive with existing state-of-the-art methods for the seen object pose estimation without any <b>fine-tuning.</b></p></p class="citation"></blockquote><h3 id=87122--130350-shapeformer-shape-prior-visible-to-amodal-transformer-based-amodal-instance-segmentation-minh-tran-et-al-2024>(87/122 | 130/350) ShapeFormer: Shape Prior Visible-to-Amodal Transformer-based Amodal Instance Segmentation (Minh Tran et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Minh Tran, Winston Bounsavy, Khoa Vo, Anh Nguyen, Tri Nguyen, Ngan Le. (2024)<br><strong>ShapeFormer: Shape Prior Visible-to-Amodal Transformer-based Amodal Instance Segmentation</strong><br><button class=copy-to-clipboard title="ShapeFormer: Shape Prior Visible-to-Amodal Transformer-based Amodal Instance Segmentation" index=130>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-130 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11376v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11376v1.pdf filename=2403.11376v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Amodal Instance Segmentation (AIS) presents a challenging task as it involves predicting both visible and occluded parts of objects within images. Existing AIS methods rely on a bidirectional approach, encompassing both the transition from amodal features to visible features (amodal-to-visible) and from visible features to amodal features (visible-to-amodal). Our observation shows that the utilization of amodal features through the amodal-to-visible can confuse the visible features due to the extra information of occluded/hidden segments not presented in visible display. Consequently, this compromised quality of visible features during the subsequent visible-to-amodal transition. To tackle this issue, we introduce ShapeFormer, a decoupled <b>Transformer-based</b> model with a visible-to-amodal transition. It facilitates the explicit relationship between output segmentations and avoids the need for amodal-to-visible transitions. ShapeFormer comprises three key modules: (i) Visible-Occluding Mask Head for predicting visible segmentation with occlusion awareness, (ii) Shape-Prior Amodal Mask Head for predicting amodal and occluded masks, and (iii) Category-Specific Shape Prior Retriever aims to provide shape prior knowledge. Comprehensive experiments and extensive ablation studies across various AIS <b>benchmarks</b> demonstrate the effectiveness of our ShapeFormer. The code is available at: <a href=https://github.com/UARK-AICV/ShapeFormer>https://github.com/UARK-AICV/ShapeFormer</a></p></p class="citation"></blockquote><h3 id=88122--131350-path-gptomic-a-balanced-multi-modal-learning-framework-for-survival-outcome-prediction-hongxiao-wang-et-al-2024>(88/122 | 131/350) Path-GPTOmic: A Balanced Multi-modal Learning Framework for Survival Outcome Prediction (Hongxiao Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hongxiao Wang, Yang Yang, Zhuo Zhao, Pengfei Gu, Nishchal Sapkota, Danny Z. Chen. (2024)<br><strong>Path-GPTOmic: A Balanced Multi-modal Learning Framework for Survival Outcome Prediction</strong><br><button class=copy-to-clipboard title="Path-GPTOmic: A Balanced Multi-modal Learning Framework for Survival Outcome Prediction" index=131>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-131 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV, q-bio-GN<br>Keyword Score: 13<br>Keywords: Foundation Model, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11375v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11375v1.pdf filename=2403.11375v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>For predicting cancer survival outcomes, standard approaches in clinical research are often based on two main modalities: pathology images for observing cell morphology features, and genomic (e.g., bulk RNA-seq) for quantifying gene expressions. However, existing pathology-genomic <b>multi-modal</b> algorithms face significant challenges: (1) Valuable biological insights regarding genes and gene-gene interactions are frequently overlooked; (2) one modality often dominates the optimization process, causing inadequate training for the other modality. In this paper, we introduce a new <b>multi-modal</b> ``Path-GPTOmic" framework for cancer survival outcome prediction. First, to extract valuable biological insights, we regulate the embedding space of a <b>foundation</b> <b>model,</b> scGPT, initially trained on single-cell RNA-seq data, making it adaptable for bulk RNA-seq data. Second, to address the imbalance-between-modalities problem, we propose a gradient modulation mechanism tailored to the Cox partial likelihood loss for survival prediction. The contributions of the modalities are dynamically monitored and adjusted during the training process, encouraging that both modalities are sufficiently trained. Evaluated on two TCGA(The Cancer Genome Atlas) datasets, our model achieves substantially improved survival prediction accuracy.</p></p class="citation"></blockquote><h3 id=89122--132350-thermonerf-multimodal-neural-radiance-fields-for-thermal-novel-view-synthesis-mariam-hassan-et-al-2024>(89/122 | 132/350) ThermoNeRF: Multimodal Neural Radiance Fields for Thermal Novel View Synthesis (Mariam Hassan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mariam Hassan, Florent Forest, Olga Fink, Malcolm Mielle. (2024)<br><strong>ThermoNeRF: Multimodal Neural Radiance Fields for Thermal Novel View Synthesis</strong><br><button class=copy-to-clipboard title="ThermoNeRF: Multimodal Neural Radiance Fields for Thermal Novel View Synthesis" index=132>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-132 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 11<br>Keywords: Geometry, Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12154v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12154v1.pdf filename=2403.12154v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Thermal scene reconstruction exhibit great potential for applications across a broad spectrum of fields, including building energy consumption analysis and non-destructive testing. However, existing methods typically require dense scene measurements and often rely on RGB images for 3D <b>geometry</b> reconstruction, with thermal information being projected post-reconstruction. This two-step strategy, adopted due to the lack of texture in thermal images, can lead to disparities between the <b>geometry</b> and temperatures of the reconstructed objects and those of the actual scene. To address this challenge, we propose ThermoNeRF, a novel <b>multimodal</b> approach based on Neural Radiance Fields, capable of rendering new RGB and thermal views of a scene jointly. To overcome the lack of texture in thermal images, we use paired RGB and thermal images to learn scene density, while distinct networks estimate color and temperature information. Furthermore, we introduce ThermoScenes, a new dataset to palliate the lack of available RGB+thermal datasets for scene reconstruction. Experimental results validate that ThermoNeRF achieves accurate thermal image synthesis, with an average mean absolute error of 1.5$^\circ$C, an improvement of over 50% compared to using concatenated RGB+thermal data with Nerfacto, a state-of-the-art NeRF method.</p></p class="citation"></blockquote><h3 id=90122--133350-prototipo-de-un-contador-bidireccional-automático-de-personas-basado-en-sensores-de-visión-3d-benjamín-ojeda-magaña-et-al-2024>(90/122 | 133/350) Prototipo de un Contador Bidireccional Automático de Personas basado en sensores de visión 3D (Benjamín Ojeda-Magaña et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Benjamín Ojeda-Magaña, Rubén Ruelas, José Guadalupe Robledo-Hernández, Víctor Manuel Rangel-Cobián, Fernando López Aguilar-Hernández. (2024)<br><strong>Prototipo de un Contador Bidireccional Automático de Personas basado en sensores de visión 3D</strong><br><button class=copy-to-clipboard title="Prototipo de un Contador Bidireccional Automático de Personas basado en sensores de visión 3D" index=133>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-133 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: I-4-9, cs-CV, cs.CV, eess-IV<br>Keyword Score: 10<br>Keywords: Object Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12310v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12310v1.pdf filename=2403.12310v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>3D sensors, also known as RGB-D sensors, utilize depth images where each pixel measures the distance from the camera to <b>objects,</b> <b>using</b> principles like structured light or time-of-flight. Advances in artificial vision have led to affordable 3D cameras capable of real-time <b>object</b> <b>detection</b> without <b>object</b> <b>movement,</b> surpassing 2D cameras in information depth. These cameras can identify <b>objects</b> <b>of</b> varying colors and reflectivities and are less affected by lighting changes. The described prototype uses RGB-D sensors for bidirectional people counting in venues, aiding security and surveillance in spaces like stadiums or airports. It determines real-time occupancy and checks against maximum capacity, crucial during emergencies. The system includes a RealSense D415 depth camera and a mini-computer running <b>object</b> <b>detection</b> algorithms to count people and a 2D camera for identity verification. The system supports statistical analysis and uses C++, Python, and PHP with OpenCV for image processing, demonstrating a comprehensive approach to monitoring venue occupancy.</p></p class="citation"></blockquote><h3 id=91122--134350-data-efficient-contrastive-language-image-pretraining-prioritizing-data-quality-over-quantity-siddharth-joshi-et-al-2024>(91/122 | 134/350) Data-Efficient Contrastive Language-Image Pretraining: Prioritizing Data Quality over Quantity (Siddharth Joshi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Siddharth Joshi, Arnav Jain, Ali Payani, Baharan Mirzasoleiman. (2024)<br><strong>Data-Efficient Contrastive Language-Image Pretraining: Prioritizing Data Quality over Quantity</strong><br><button class=copy-to-clipboard title="Data-Efficient Contrastive Language-Image Pretraining: Prioritizing Data Quality over Quantity" index=134>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-134 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 10<br>Keywords: Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12267v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12267v2.pdf filename=2403.12267v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Contrastive Language-Image Pre-training (CLIP) on large-scale image-caption datasets learns representations that can achieve remarkable <b>zero-shot</b> generalization. However, such models require a massive amount of pre-training data. Improving the quality of the pre-training data has been shown to be much more effective in improving CLIP&rsquo;s performance than increasing its volume. Nevertheless, finding small subsets of training data that provably generalize the best has remained an open question. In this work, we propose the first theoretically rigorous data selection method for CLIP. We show that subsets that closely preserve the cross-covariance of the images and captions of the full data provably achieve a superior generalization performance. Our extensive experiments on ConceptualCaptions3M and ConceptualCaptions12M demonstrate that subsets found by \method\ achieve over 2.7x and 1.4x the accuracy of the next best baseline on ImageNet and its shifted versions. Moreover, we show that our subsets obtain 1.5x the average accuracy across 11 downstream datasets, of the next best baseline. The code is available at: <a href=https://github.com/BigML-CS-UCLA/clipcov-data-efficient-clip>https://github.com/BigML-CS-UCLA/clipcov-data-efficient-clip</a>.</p></p class="citation"></blockquote><h3 id=92122--135350-fusion-transformer-with-object-mask-guidance-for-image-forgery-analysis-dimitrios-karageorgiou-et-al-2024>(92/122 | 135/350) Fusion Transformer with Object Mask Guidance for Image Forgery Analysis (Dimitrios Karageorgiou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dimitrios Karageorgiou, Giorgos Kordopatis-Zilos, Symeon Papadopoulos. (2024)<br><strong>Fusion Transformer with Object Mask Guidance for Image Forgery Analysis</strong><br><button class=copy-to-clipboard title="Fusion Transformer with Object Mask Guidance for Image Forgery Analysis" index=135>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-135 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12229v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12229v1.pdf filename=2403.12229v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we introduce OMG-Fuser, a fusion <b>transformer-based</b> network designed to extract information from various forensic signals to enable robust image forgery detection and localization. Our approach can operate with an arbitrary number of forensic signals and leverages object information for their analysis &ndash; unlike previous methods that rely on fusion schemes with few signals and often disregard image semantics. To this end, we design a forensic signal stream composed of a <b>transformer</b> guided by an object attention mechanism, associating patches that depict the same objects. In that way, we incorporate object-level information from the image. Each forensic signal is processed by a different stream that adapts to its peculiarities. Subsequently, a token fusion <b>transformer</b> efficiently aggregates the outputs of an arbitrary number of network streams and generates a fused representation for each image patch. These representations are finally processed by a long-range dependencies <b>transformer</b> that captures the intrinsic relations between the image patches. We assess two fusion variants on top of the proposed approach: (i) score-level fusion that fuses the outputs of multiple image forensics algorithms and (ii) feature-level fusion that fuses low-level forensic traces directly. Both variants exceed state-of-the-art performance on seven datasets for image forgery detection and localization, with a relative average improvement of 12.1% and 20.4% in terms of F1. Our network demonstrates robustness against traditional and novel forgery attacks and can be expanded with new signals without training from scratch.</p></p class="citation"></blockquote><h3 id=93122--136350-hoidiffusion-generating-realistic-3d-hand-object-interaction-data-mengqi-zhang-et-al-2024>(93/122 | 136/350) HOIDiffusion: Generating Realistic 3D Hand-Object Interaction Data (Mengqi Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mengqi Zhang, Yang Fu, Zheng Ding, Sifei Liu, Zhuowen Tu, Xiaolong Wang. (2024)<br><strong>HOIDiffusion: Generating Realistic 3D Hand-Object Interaction Data</strong><br><button class=copy-to-clipboard title="HOIDiffusion: Generating Realistic 3D Hand-Object Interaction Data" index=136>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-136 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12011v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12011v1.pdf filename=2403.12011v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>3D hand-object interaction data is scarce due to the hardware constraints in scaling up the data collection process. In this paper, we propose HOIDiffusion for generating realistic and diverse 3D hand-object interaction data. Our model is a conditional <b>diffusion</b> <b>model</b> that takes both the 3D hand-object geometric structure and text description as inputs for image synthesis. This offers a more controllable and realistic synthesis as we can specify the structure and style inputs in a disentangled manner. HOIDiffusion is trained by leveraging a <b>diffusion</b> <b>model</b> pre-trained on large-scale natural images and a few 3D human demonstrations. Beyond controllable image synthesis, we adopt the generated 3D data for learning 6D object pose estimation and show its effectiveness in improving perception systems. Project page: <a href=https://mq-zhang1.github.io/HOIDiffusion>https://mq-zhang1.github.io/HOIDiffusion</a></p></p class="citation"></blockquote><h3 id=94122--137350-sv3d-novel-multi-view-synthesis-and-3d-generation-from-a-single-image-using-latent-video-diffusion-vikram-voleti-et-al-2024>(94/122 | 137/350) SV3D: Novel Multi-view Synthesis and 3D Generation from a Single Image using Latent Video Diffusion (Vikram Voleti et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vikram Voleti, Chun-Han Yao, Mark Boss, Adam Letts, David Pankratz, Dmitry Tochilkin, Christian Laforte, Robin Rombach, Varun Jampani. (2024)<br><strong>SV3D: Novel Multi-view Synthesis and 3D Generation from a Single Image using Latent Video Diffusion</strong><br><button class=copy-to-clipboard title="SV3D: Novel Multi-view Synthesis and 3D Generation from a Single Image using Latent Video Diffusion" index=137>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-137 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12008v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12008v1.pdf filename=2403.12008v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present Stable Video 3D (SV3D) &ndash; a latent video <b>diffusion</b> <b>model</b> for high-resolution, image-to-multi-view generation of orbital videos around a 3D object. Recent work on 3D generation propose techniques to adapt 2D generative models for novel view synthesis (NVS) and 3D optimization. However, these methods have several disadvantages due to either limited views or inconsistent NVS, thereby affecting the performance of 3D object generation. In this work, we propose SV3D that adapts image-to-video <b>diffusion</b> <b>model</b> for novel multi-view synthesis and 3D generation, thereby leveraging the generalization and multi-view consistency of the video models, while further adding explicit camera control for NVS. We also propose improved 3D optimization techniques to use SV3D and its NVS outputs for image-to-3D generation. Extensive experimental results on multiple datasets with 2D and 3D metrics as well as user study demonstrate SV3D&rsquo;s state-of-the-art performance on NVS as well as 3D reconstruction compared to prior works.</p></p class="citation"></blockquote><h3 id=95122--138350-see-imagine-plan-discovering-and-hallucinating-tasks-from-a-single-image-chenyang-ma-et-al-2024>(95/122 | 138/350) See, Imagine, Plan: Discovering and Hallucinating Tasks from a Single Image (Chenyang Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chenyang Ma, Kai Lu, Ta-Ying Cheng, Niki Trigoni, Andrew Markham. (2024)<br><strong>See, Imagine, Plan: Discovering and Hallucinating Tasks from a Single Image</strong><br><button class=copy-to-clipboard title="See, Imagine, Plan: Discovering and Hallucinating Tasks from a Single Image" index=138>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-138 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13438v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13438v1.pdf filename=2403.13438v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Humans can not only recognize and understand the world in its current state but also envision future scenarios that extend beyond immediate perception. To resemble this profound human capacity, we introduce <b>zero-shot</b> task hallucination &ndash; given a single RGB image of any scene comprising unknown environments and objects, our model can identify potential tasks and imagine their execution in a vivid narrative, realized as a video. We develop a modular pipeline that progressively enhances scene decomposition, comprehension, and reconstruction, incorporating VLM for dynamic interaction and 3D motion planning for object trajectories. Our model can discover diverse tasks, with the generated task videos demonstrating realistic and compelling visual outcomes that are understandable by both machines and humans. Project Page: <a href=https://dannymcy.github.io/zeroshot_task_hallucination/>https://dannymcy.github.io/zeroshot_task_hallucination/</a></p></p class="citation"></blockquote><h3 id=96122--139350-exploring-facial-expression-recognition-through-semi-supervised-pretraining-and-temporal-modeling-jun-yu-et-al-2024>(96/122 | 139/350) Exploring Facial Expression Recognition through Semi-Supervised Pretraining and Temporal Modeling (Jun Yu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jun Yu, Zhihong Wei, Zhongpeng Cai, Gongpeng Zhao, Zerui Zhang, Yongqi Wang, Guochen Xie, Jichao Zhu, Wangyuan Zhu. (2024)<br><strong>Exploring Facial Expression Recognition through Semi-Supervised Pretraining and Temporal Modeling</strong><br><button class=copy-to-clipboard title="Exploring Facial Expression Recognition through Semi-Supervised Pretraining and Temporal Modeling" index=139>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-139 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Semi-Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11942v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11942v2.pdf filename=2403.11942v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Facial Expression Recognition (FER) plays a crucial role in computer vision and finds extensive applications across various fields. This paper aims to present our approach for the upcoming 6th Affective Behavior Analysis in-the-Wild (ABAW) competition, scheduled to be held at CVPR2024. In the facial expression recognition task, The limited size of the FER dataset poses a challenge to the expression recognition model&rsquo;s generalization ability, resulting in subpar recognition performance. To address this problem, we employ a <b>semi-supervised</b> <b>learning</b> technique to generate expression category pseudo-labels for unlabeled face data. At the same time, we uniformly sampled the labeled facial expression samples and implemented a debiased feedback learning strategy to address the problem of category imbalance in the dataset and the possible data bias in <b>semi-supervised</b> <b>learning.</b> Moreover, to further compensate for the limitation and bias of features obtained only from static images, we introduced a Temporal Encoder to learn and capture temporal relationships between neighbouring expression image features. In the 6th ABAW competition, our method achieved outstanding results on the official validation set, a result that fully confirms the effectiveness and competitiveness of our proposed method.</p></p class="citation"></blockquote><h3 id=97122--140350-intex-interactive-text-to-texture-synthesis-via-unified-depth-aware-inpainting-jiaxiang-tang-et-al-2024>(97/122 | 140/350) InTeX: Interactive Text-to-texture Synthesis via Unified Depth-aware Inpainting (Jiaxiang Tang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiaxiang Tang, Ruijie Lu, Xiaokang Chen, Xiang Wen, Gang Zeng, Ziwei Liu. (2024)<br><strong>InTeX: Interactive Text-to-texture Synthesis via Unified Depth-aware Inpainting</strong><br><button class=copy-to-clipboard title="InTeX: Interactive Text-to-texture Synthesis via Unified Depth-aware Inpainting" index=140>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-140 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11878v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11878v1.pdf filename=2403.11878v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Text-to-texture synthesis has become a new frontier in 3D content creation thanks to the recent advances in <b>text-to-image</b> models. Existing methods primarily adopt a combination of pretrained depth-aware diffusion and inpainting models, yet they exhibit shortcomings such as 3D inconsistency and limited controllability. To address these challenges, we introduce InteX, a novel framework for interactive text-to-texture synthesis. 1) InteX includes a user-friendly interface that facilitates interaction and control throughout the synthesis process, enabling region-specific repainting and precise texture editing. 2) Additionally, we develop a unified depth-aware inpainting model that integrates depth information with inpainting cues, effectively mitigating 3D inconsistencies and improving generation speed. Through extensive experiments, our framework has proven to be both practical and effective in text-to-texture synthesis, paving the way for high-quality 3D content creation.</p></p class="citation"></blockquote><h3 id=98122--141350-towards-real-time-fast-unmanned-aerial-vehicle-detection-using-dynamic-vision-sensors-jakub-mandula-et-al-2024>(98/122 | 141/350) Towards Real-Time Fast Unmanned Aerial Vehicle Detection Using Dynamic Vision Sensors (Jakub Mandula et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jakub Mandula, Jonas Kühne, Luca Pascarella, Michele Magno. (2024)<br><strong>Towards Real-Time Fast Unmanned Aerial Vehicle Detection Using Dynamic Vision Sensors</strong><br><button class=copy-to-clipboard title="Towards Real-Time Fast Unmanned Aerial Vehicle Detection Using Dynamic Vision Sensors" index=141>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-141 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV<br>Keyword Score: 10<br>Keywords: Object Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11875v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11875v1.pdf filename=2403.11875v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Unmanned Aerial Vehicles (UAVs) are gaining popularity in civil and military applications. However, uncontrolled access to restricted areas threatens privacy and security. Thus, prevention and detection of UAVs are pivotal to guarantee confidentiality and safety. Although active scanning, mainly based on radars, is one of the most accurate technologies, it can be expensive and less versatile than passive inspections, e.g., <b>object</b> <b>recognition.</b> Dynamic vision sensors (DVS) are bio-inspired event-based vision models that leverage timestamped pixel-level brightness changes in fast-moving scenes that adapt well to low-latency <b>object</b> <b>detection.</b> This paper presents F-UAV-D (Fast Unmanned Aerial Vehicle Detector), an embedded system that enables fast-moving drone detection. In particular, we propose a setup to exploit DVS as an alternative to RGB cameras in a real-time and low-power configuration. Our approach leverages the high-dynamic range (HDR) and background suppression of DVS and, when trained with various fast-moving drones, outperforms RGB input in suboptimal ambient conditions such as low illumination and fast-moving scenes. Our results show that F-UAV-D can (i) detect drones by using less than &lt;15 W on average and (ii) perform real-time inference (i.e., &lt;50 ms) by leveraging the CPU and GPU nodes of our edge computer.</p></p class="citation"></blockquote><h3 id=99122--142350-tcnet-continuous-sign-language-recognition-from-trajectories-and-correlated-regions-hui-lu-et-al-2024>(99/122 | 142/350) TCNet: Continuous Sign Language Recognition from Trajectories and Correlated Regions (Hui Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hui Lu, Albert Ali Salah, Ronald Poppe. (2024)<br><strong>TCNet: Continuous Sign Language Recognition from Trajectories and Correlated Regions</strong><br><button class=copy-to-clipboard title="TCNet: Continuous Sign Language Recognition from Trajectories and Correlated Regions" index=142>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-142 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11818v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11818v1.pdf filename=2403.11818v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A key challenge in continuous sign language recognition (CSLR) is to efficiently capture long-range spatial interactions over time from the video input. To address this challenge, we propose TCNet, a hybrid network that effectively models spatio-temporal information from Trajectories and Correlated regions. TCNet&rsquo;s trajectory module transforms frames into aligned trajectories composed of continuous visual tokens. In addition, for a query token, <b>self-attention</b> is learned along the trajectory. As such, our network can also focus on fine-grained spatio-temporal patterns, such as finger movements, of a specific region in motion. TCNet&rsquo;s correlation module uses a novel dynamic attention mechanism that filters out irrelevant frame regions. Additionally, it assigns dynamic key-value tokens from correlated regions to each query. Both innovations significantly reduce the computation cost and memory. We perform experiments on four large-scale datasets: PHOENIX14, PHOENIX14-T, CSL, and CSL-Daily, respectively. Our results demonstrate that TCNet consistently achieves state-of-the-art performance. For example, we improve over the previous state-of-the-art by 1.5% and 1.0% word error rate on PHOENIX14 and PHOENIX14-T, respectively.</p></p class="citation"></blockquote><h3 id=100122--143350-implicit-discriminative-knowledge-learning-for-visible-infrared-person-re-identification-kaijie-ren-et-al-2024>(100/122 | 143/350) Implicit Discriminative Knowledge Learning for Visible-Infrared Person Re-Identification (Kaijie Ren et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kaijie Ren, Lei Zhang. (2024)<br><strong>Implicit Discriminative Knowledge Learning for Visible-Infrared Person Re-Identification</strong><br><button class=copy-to-clipboard title="Implicit Discriminative Knowledge Learning for Visible-Infrared Person Re-Identification" index=143>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-143 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11708v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11708v1.pdf filename=2403.11708v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Visible-Infrared Person Re-identification (VI-ReID) is a challenging cross-modal pedestrian retrieval task, due to significant intra-class variations and cross-modal discrepancies among different cameras. Existing works mainly focus on embedding images of different modalities into a unified space to mine modality-shared features. They only seek distinctive information within these shared features, while ignoring the identity-aware useful information that is implicit in the modality-specific features. To address this issue, we propose a novel Implicit Discriminative Knowledge Learning (IDKL) network to uncover and leverage the implicit discriminative information contained within the modality-specific. First, we extract modality-specific and modality-shared features using a novel dual-stream network. Then, the modality-specific features undergo purification to reduce their modality style discrepancies while preserving identity-aware discriminative knowledge. Subsequently, this kind of implicit knowledge is <b>distilled</b> into the modality-shared feature to enhance its distinctiveness. Finally, an alignment loss is proposed to minimize modality discrepancy on enhanced modality-shared features. Extensive experiments on multiple public datasets demonstrate the superiority of IDKL network over the state-of-the-art methods. Code is available at <a href=https://github.com/1KK077/IDKL>https://github.com/1KK077/IDKL</a>.</p></p class="citation"></blockquote><h3 id=101122--144350-trajectorynas-a-neural-architecture-search-for-trajectory-prediction-ali-asghar-sharifi-et-al-2024>(101/122 | 144/350) TrajectoryNAS: A Neural Architecture Search for Trajectory Prediction (Ali Asghar Sharifi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ali Asghar Sharifi, Ali Zoljodi, Masoud Daneshtalab. (2024)<br><strong>TrajectoryNAS: A Neural Architecture Search for Trajectory Prediction</strong><br><button class=copy-to-clipboard title="TrajectoryNAS: A Neural Architecture Search for Trajectory Prediction" index=144>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-144 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Object Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11695v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11695v1.pdf filename=2403.11695v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Autonomous driving systems are a rapidly evolving technology that enables driverless car production. Trajectory prediction is a critical component of autonomous driving systems, enabling cars to anticipate the movements of surrounding <b>objects</b> <b>for</b> safe navigation. Trajectory prediction using Lidar point-cloud data performs better than 2D images due to providing 3D information. However, processing point-cloud data is more complicated and time-consuming than 2D images. Hence, state-of-the-art 3D trajectory predictions using point-cloud data suffer from slow and erroneous predictions. This paper introduces TrajectoryNAS, a pioneering method that focuses on utilizing point cloud data for trajectory prediction. By leveraging Neural Architecture Search (NAS), TrajectoryNAS automates the design of trajectory prediction models, encompassing <b>object</b> <b>detection,</b> tracking, and forecasting in a cohesive manner. This approach not only addresses the complex interdependencies among these tasks but also emphasizes the importance of accuracy and efficiency in trajectory modeling. Through empirical studies, TrajectoryNAS demonstrates its effectiveness in enhancing the performance of autonomous driving systems, marking a significant advancement in the field.Experimental results reveal that TrajcetoryNAS yield a minimum of 4.8 higger accuracy and 1.1* lower latency over competing methods on the NuScenes dataset.</p></p class="citation"></blockquote><h3 id=102122--145350-neds-slam-a-novel-neural-explicit-dense-semantic-slam-framework-using-3d-gaussian-splatting-yiming-ji-et-al-2024>(102/122 | 145/350) NEDS-SLAM: A Novel Neural Explicit Dense Semantic SLAM Framework using 3D Gaussian Splatting (Yiming Ji et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yiming Ji, Yang Liu, Guanghu Xie, Boyu Ma, Zongwu Xie. (2024)<br><strong>NEDS-SLAM: A Novel Neural Explicit Dense Semantic SLAM Framework using 3D Gaussian Splatting</strong><br><button class=copy-to-clipboard title="NEDS-SLAM: A Novel Neural Explicit Dense Semantic SLAM Framework using 3D Gaussian Splatting" index=145>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-145 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV<br>Keyword Score: 10<br>Keywords: Pruning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11679v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11679v1.pdf filename=2403.11679v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose NEDS-SLAM, an Explicit Dense semantic SLAM system based on 3D Gaussian representation, that enables robust 3D semantic mapping, accurate camera tracking, and high-quality rendering in real-time. In the system, we propose a Spatially Consistent Feature Fusion model to reduce the effect of erroneous estimates from pre-trained segmentation head on semantic reconstruction, achieving robust 3D semantic Gaussian mapping. Additionally, we employ a lightweight encoder-decoder to compress the high-dimensional semantic features into a compact 3D Gaussian representation, mitigating the burden of excessive memory consumption. Furthermore, we leverage the advantage of 3D Gaussian splatting, which enables efficient and differentiable novel view rendering, and propose a Virtual Camera View <b>Pruning</b> method to eliminate outlier GS points, thereby effectively enhancing the quality of scene representations. Our NEDS-SLAM method demonstrates competitive performance over existing dense semantic SLAM methods in terms of mapping and tracking accuracy on Replica and ScanNet datasets, while also showing excellent capabilities in 3D dense semantic mapping.</p></p class="citation"></blockquote><h3 id=103122--146350-medmerge-merging-models-for-effective-transfer-learning-to-medical-imaging-tasks-ibrahim-almakky-et-al-2024>(103/122 | 146/350) MedMerge: Merging Models for Effective Transfer Learning to Medical Imaging Tasks (Ibrahim Almakky et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ibrahim Almakky, Santosh Sanjeev, Anees Ur Rehman Hashmi, Mohammad Areeb Qazi, Mohammad Yaqub. (2024)<br><strong>MedMerge: Merging Models for Effective Transfer Learning to Medical Imaging Tasks</strong><br><button class=copy-to-clipboard title="MedMerge: Merging Models for Effective Transfer Learning to Medical Imaging Tasks" index=146>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-146 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11646v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11646v1.pdf filename=2403.11646v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Transfer</b> <b>learning</b> has become a powerful tool to initialize deep learning models to achieve faster convergence and higher performance. This is especially useful in the medical imaging analysis domain, where data scarcity limits possible performance gains for deep learning models. Some advancements have been made in boosting the <b>transfer</b> <b>learning</b> performance gain by merging models starting from the same initialization. However, in the medical imaging analysis domain, there is an opportunity in merging models starting from different initialisations, thus combining the features learnt from different tasks. In this work, we propose MedMerge, a method whereby the weights of different models can be merged, and their features can be effectively utilized to boost performance on a new task. With MedMerge, we learn kernel-level weights that can later be used to merge the models into a single model, even when starting from different initializations. Testing on various medical imaging analysis tasks, we show that our merged model can achieve significant performance gains, with up to 3% improvement on the F1 score. The code implementation of this work will be available at <a href=https://www.github.com/BioMedIA-MBZUAI/MedMerge>www.github.com/BioMedIA-MBZUAI/MedMerge</a>.</p></p class="citation"></blockquote><h3 id=104122--147350-hsemotion-team-at-the-6th-abaw-competition-facial-expressions-valence-arousal-and-emotion-intensity-prediction-andrey-v-savchenko-2024>(104/122 | 147/350) HSEmotion Team at the 6th ABAW Competition: Facial Expressions, Valence-Arousal and Emotion Intensity Prediction (Andrey V. Savchenko, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andrey V. Savchenko. (2024)<br><strong>HSEmotion Team at the 6th ABAW Competition: Facial Expressions, Valence-Arousal and Emotion Intensity Prediction</strong><br><button class=copy-to-clipboard title="HSEmotion Team at the 6th ABAW Competition: Facial Expressions, Valence-Arousal and Emotion Intensity Prediction" index=147>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-147 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: 68T10, I-4-9, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11590v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11590v1.pdf filename=2403.11590v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This article presents our results for the sixth Affective Behavior Analysis in-the-wild (ABAW) competition. To improve the trustworthiness of facial analysis, we study the possibility of using pre-trained deep models that extract reliable emotional features without the need to <b>fine-tune</b> the neural networks for a downstream task. In particular, we introduce several lightweight models based on MobileViT, MobileFaceNet, EfficientNet, and DDAMFN architectures trained in multi-task scenarios to recognize facial expressions, valence, and arousal on static photos. These neural networks extract frame-level features fed into a simple classifier, e.g., linear feed-forward neural network, to predict emotion intensity, compound expressions, action units, facial expressions, and valence/arousal. Experimental results for five tasks from the sixth ABAW challenge demonstrate that our approach lets us significantly improve quality metrics on validation sets compared to existing non-ensemble techniques.</p></p class="citation"></blockquote><h3 id=105122--148350-ourdb-ouroboric-domain-bridging-for-multi-target-domain-adaptive-semantic-segmentation-seungbeom-woo-et-al-2024>(105/122 | 148/350) OurDB: Ouroboric Domain Bridging for Multi-Target Domain Adaptive Semantic Segmentation (Seungbeom Woo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Seungbeom Woo, Geonwoo Baek, Taehoon Kim, Jaemin Na, Joong-won Hwang, Wonjun Hwang. (2024)<br><strong>OurDB: Ouroboric Domain Bridging for Multi-Target Domain Adaptive Semantic Segmentation</strong><br><button class=copy-to-clipboard title="OurDB: Ouroboric Domain Bridging for Multi-Target Domain Adaptive Semantic Segmentation" index=148>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-148 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11582v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11582v1.pdf filename=2403.11582v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multi-target <b>domain</b> <b>adaptation</b> (MTDA) for semantic segmentation poses a significant challenge, as it involves multiple target <b>domains</b> <b>with</b> varying distributions. The goal of MTDA is to minimize the <b>domain</b> <b>discrepancies</b> among a single source and multi-target <b>domains,</b> <b>aiming</b> to train a single model that excels across all target <b>domains.</b> <b>Previous</b> MTDA approaches typically employ multiple teacher architectures, where each teacher specializes in one target <b>domain</b> <b>to</b> simplify the task. However, these architectures hinder the student model from fully assimilating comprehensive knowledge from all target-specific teachers and escalate training costs with increasing target <b>domains.</b> <b>In</b> this paper, we propose an ouroboric <b>domain</b> <b>bridging</b> (OurDB) framework, offering an efficient solution to the MTDA problem using a single teacher architecture. This framework dynamically cycles through multiple target <b>domains,</b> <b>aligning</b> each <b>domain</b> <b>individually</b> to restrain the biased alignment problem, and utilizes Fisher information to minimize the forgetting of knowledge from previous target <b>domains.</b> <b>We</b> also propose a context-guided class-wise mixup (CGMix) that leverages contextual information tailored to diverse target contexts in MTDA. Experimental evaluations conducted on four urban driving datasets (i.e., GTA5, Cityscapes, IDD, and Mapillary) demonstrate the superiority of our method over existing state-of-the-art approaches.</p></p class="citation"></blockquote><h3 id=106122--149350-tarn-vist-topic-aware-reinforcement-network-for-visual-storytelling-weiran-chen-et-al-2024>(106/122 | 149/350) TARN-VIST: Topic Aware Reinforcement Network for Visual Storytelling (Weiran Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Weiran Chen, Xin Li, Jiaqi Su, Guiqian Zhu, Ying Li, Yi Ji, Chunping Liu. (2024)<br><strong>TARN-VIST: Topic Aware Reinforcement Network for Visual Storytelling</strong><br><button class=copy-to-clipboard title="TARN-VIST: Topic Aware Reinforcement Network for Visual Storytelling" index=149>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-149 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11550v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11550v1.pdf filename=2403.11550v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As a cross-modal task, visual storytelling aims to generate a story for an ordered image sequence automatically. Different from the image captioning task, visual storytelling requires not only modeling the relationships between objects in the image but also mining the connections between adjacent images. Recent approaches primarily utilize either end-to-end frameworks or multi-stage frameworks to generate relevant stories, but they usually overlook latent topic information. In this paper, in order to generate a more coherent and relevant story, we propose a novel method, Topic Aware <b>Reinforcement</b> <b>Network</b> for VIsual StoryTelling (TARN-VIST). In particular, we pre-extracted the topic information of stories from both visual and linguistic perspectives. Then we apply two topic-consistent <b>reinforcement</b> <b>learning</b> rewards to identify the discrepancy between the generated story and the human-labeled story so as to refine the whole generation process. Extensive experimental results on the VIST dataset and human evaluation demonstrate that our proposed model outperforms most of the competitive models across multiple evaluation metrics.</p></p class="citation"></blockquote><h3 id=107122--150350-end-to-end-underwater-video-enhancement-dataset-and-model-dazhao-du-et-al-2024>(107/122 | 150/350) End-To-End Underwater Video Enhancement: Dataset and Model (Dazhao Du et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dazhao Du, Enhan Li, Lingyu Si, Fanjiang Xu, Jianwei Niu. (2024)<br><strong>End-To-End Underwater Video Enhancement: Dataset and Model</strong><br><button class=copy-to-clipboard title="End-To-End Underwater Video Enhancement: Dataset and Model" index=150>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-150 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11506v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11506v1.pdf filename=2403.11506v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Underwater video enhancement (UVE) aims to improve the visibility and frame quality of underwater videos, which has significant implications for marine research and exploration. However, existing methods primarily focus on developing image enhancement algorithms to enhance each frame independently. There is a lack of <b>supervised</b> datasets and models specifically tailored for UVE tasks. To fill this gap, we construct the Synthetic Underwater Video Enhancement (SUVE) dataset, comprising 840 diverse underwater-style videos paired with ground-truth reference videos. Based on this dataset, we train a novel underwater video enhancement model, UVENet, which utilizes inter-frame relationships to achieve better enhancement performance. Through extensive experiments on both synthetic and real underwater videos, we demonstrate the effectiveness of our approach. This study represents the first comprehensive exploration of UVE to our knowledge. The code is available at <a href=https://anonymous.4open.science/r/UVENet>https://anonymous.4open.science/r/UVENet</a>.</p></p class="citation"></blockquote><h3 id=108122--151350-generative-motion-stylization-within-canonical-motion-space-jiaxu-zhang-et-al-2024>(108/122 | 151/350) Generative Motion Stylization within Canonical Motion Space (Jiaxu Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiaxu Zhang, Xin Chen, Gang Yu, Zhigang Tu. (2024)<br><strong>Generative Motion Stylization within Canonical Motion Space</strong><br><button class=copy-to-clipboard title="Generative Motion Stylization within Canonical Motion Space" index=151>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-151 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-GR, cs.CV<br>Keyword Score: 10<br>Keywords: Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11469v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11469v1.pdf filename=2403.11469v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Stylized motion breathes life into characters. However, the fixed skeleton structure and style representation hinder existing data-driven motion synthesis methods from generating stylized motion for various characters. In this work, we propose a generative motion stylization pipeline, named MotionS, for synthesizing diverse and stylized motion on cross-structure characters using cross-modality style <b>prompts.</b> Our key insight is to embed motion style into a cross-modality latent space and perceive the cross-structure skeleton topologies, allowing for motion stylization within a canonical motion space. Specifically, the large-scale Contrastive-Language-Image-Pre-training (CLIP) model is leveraged to construct the cross-modality latent space, enabling flexible style representation within this space. Additionally, two topology-encoded tokens are learned to capture the canonical and specific skeleton topologies, facilitating cross-structure topology shifting. Subsequently, the topology-shifted stylization diffusion is designed to generate motion content for the specific skeleton and stylize it in the shifted canonical motion space using multi-modality style descriptions. Through an extensive set of examples, we demonstrate the flexibility and generalizability of our pipeline across various characters and style descriptions. Qualitative and quantitative experiments underscore the superiority of our pipeline over state-of-the-art methods, consistently delivering high-quality stylized motion across a broad spectrum of skeletal structures.</p></p class="citation"></blockquote><h3 id=109122--152350-defense-against-adversarial-attacks-on-no-reference-image-quality-models-with-gradient-norm-regularization-yujia-liu-et-al-2024>(109/122 | 152/350) Defense Against Adversarial Attacks on No-Reference Image Quality Models with Gradient Norm Regularization (Yujia Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yujia Liu, Chenxi Yang, Dingquan Li, Jianhao Ding, Tingting Jiang. (2024)<br><strong>Defense Against Adversarial Attacks on No-Reference Image Quality Models with Gradient Norm Regularization</strong><br><button class=copy-to-clipboard title="Defense Against Adversarial Attacks on No-Reference Image Quality Models with Gradient Norm Regularization" index=152>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-152 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV<br>Keyword Score: 10<br>Keywords: Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11397v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11397v1.pdf filename=2403.11397v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The task of No-Reference Image Quality Assessment (NR-IQA) is to estimate the quality score of an input image without additional information. NR-IQA models play a crucial role in the media industry, aiding in performance evaluation and optimization guidance. However, these models are found to be vulnerable to <b>adversarial</b> <b>attacks,</b> which introduce imperceptible perturbations to input images, resulting in significant changes in predicted scores. In this paper, we propose a defense method to improve the stability in predicted scores when attacked by small perturbations, thus enhancing the <b>adversarial</b> <b>robustness</b> of NR-IQA models. To be specific, we present theoretical evidence showing that the magnitude of score changes is related to the $\ell_1$ norm of the model&rsquo;s gradient with respect to the input image. Building upon this theoretical foundation, we propose a norm regularization training strategy aimed at reducing the $\ell_1$ norm of the gradient, thereby boosting the robustness of NR-IQA models. Experiments conducted on four NR-IQA baseline models demonstrate the effectiveness of our strategy in reducing score changes in the presence of <b>adversarial</b> <b>attacks.</b> To the best of our knowledge, this work marks the first attempt to defend against <b>adversarial</b> <b>attacks</b> on NR-IQA models. Our study offers valuable insights into the <b>adversarial</b> <b>robustness</b> of NR-IQA models and provides a foundation for future research in this area.</p></p class="citation"></blockquote><h3 id=110122--153350-boosting-order-preserving-and-transferability-for-neural-architecture-search-a-joint-architecture-refined-search-and-fine-tuning-approach-beichen-zhang-et-al-2024>(110/122 | 153/350) Boosting Order-Preserving and Transferability for Neural Architecture Search: a Joint Architecture Refined Search and Fine-tuning Approach (Beichen Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Beichen Zhang, Xiaoxing Wang, Xiaohan Qin, Junchi Yan. (2024)<br><strong>Boosting Order-Preserving and Transferability for Neural Architecture Search: a Joint Architecture Refined Search and Fine-tuning Approach</strong><br><button class=copy-to-clipboard title="Boosting Order-Preserving and Transferability for Neural Architecture Search: a Joint Architecture Refined Search and Fine-tuning Approach" index=153>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-153 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11380v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11380v1.pdf filename=2403.11380v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Supernet is a core component in many recent Neural Architecture Search (NAS) methods. It not only helps embody the search space but also provides a (relative) estimation of the final performance of candidate architectures. Thus, it is critical that the top architectures ranked by a supernet should be consistent with those ranked by true performance, which is known as the order-preserving ability. In this work, we analyze the order-preserving ability on the whole search space (global) and a sub-space of top architectures (local), and empirically show that the local order-preserving for current two-stage NAS methods still need to be improved. To rectify this, we propose a novel concept of Supernet Shifting, a refined search strategy combining architecture searching with supernet <b>fine-tuning.</b> Specifically, apart from evaluating, the training loss is also accumulated in searching and the supernet is updated every iteration. Since superior architectures are sampled more frequently in evolutionary searching, the supernet is encouraged to focus on top architectures, thus improving local order-preserving. Besides, a pre-trained supernet is often un-reusable for one-shot methods. We show that Supernet Shifting can fulfill transferring supernet to a new dataset. Specifically, the last classifier layer will be unset and trained through evolutionary searching. Comprehensive experiments show that our method has better order-preserving ability and can find a dominating architecture. Moreover, the pre-trained supernet can be easily transferred into a new dataset with no loss of performance.</p></p class="citation"></blockquote><h3 id=111122--154350-diffusion-based-environment-aware-trajectory-prediction-theodor-westny-et-al-2024>(111/122 | 154/350) Diffusion-Based Environment-Aware Trajectory Prediction (Theodor Westny et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Theodor Westny, Björn Olofsson, Erik Frisk. (2024)<br><strong>Diffusion-Based Environment-Aware Trajectory Prediction</strong><br><button class=copy-to-clipboard title="Diffusion-Based Environment-Aware Trajectory Prediction" index=154>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-154 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs-RO, cs.CV<br>Keyword Score: 6<br>Keywords: Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11643v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11643v1.pdf filename=2403.11643v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The ability to predict the future trajectories of traffic participants is crucial for the safe and efficient operation of autonomous vehicles. In this paper, a diffusion-based generative model for multi-agent trajectory prediction is proposed. The model is capable of capturing the complex interactions between traffic participants and the environment, accurately learning the <b>multimodal</b> nature of the data. The effectiveness of the approach is assessed on large-scale datasets of real-world traffic scenarios, showing that our model outperforms several well-established methods in terms of prediction accuracy. By the incorporation of differential motion constraints on the model output, we illustrate that our model is capable of generating a diverse set of realistic future trajectories. Through the use of an interaction-aware guidance signal, we further demonstrate that the model can be adapted to predict the behavior of less cooperative agents, emphasizing its practical applicability under uncertain traffic conditions.</p></p class="citation"></blockquote><h3 id=112122--155350-3dgs-calib-3d-gaussian-splatting-for-multimodal-spatiotemporal-calibration-quentin-herau-et-al-2024>(112/122 | 155/350) 3DGS-Calib: 3D Gaussian Splatting for Multimodal SpatioTemporal Calibration (Quentin Herau et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Quentin Herau, Moussab Bennehar, Arthur Moreau, Nathan Piasco, Luis Roldao, Dzmitry Tsishkou, Cyrille Migniot, Pascal Vasseur, Cédric Demonceaux. (2024)<br><strong>3DGS-Calib: 3D Gaussian Splatting for Multimodal SpatioTemporal Calibration</strong><br><button class=copy-to-clipboard title="3DGS-Calib: 3D Gaussian Splatting for Multimodal SpatioTemporal Calibration" index=155>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-155 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV<br>Keyword Score: 6<br>Keywords: Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11577v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11577v1.pdf filename=2403.11577v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Reliable <b>multimodal</b> sensor fusion algorithms require accurate spatiotemporal calibration. Recently, targetless calibration techniques based on implicit neural representations have proven to provide precise and robust results. Nevertheless, such methods are inherently slow to train given the high computational overhead caused by the large number of sampled points required for volume rendering. With the recent introduction of 3D Gaussian Splatting as a faster alternative to implicit representation methods, we propose to leverage this new rendering approach to achieve faster multi-sensor calibration. We introduce 3DGS-Calib, a new calibration method that relies on the speed and rendering accuracy of 3D Gaussian Splatting to achieve <b>multimodal</b> spatiotemporal calibration that is accurate, robust, and with a substantial speed-up compared to methods relying on implicit neural representations. We demonstrate the superiority of our proposal with experimental results on sequences from KITTI-360, a widely used driving dataset.</p></p class="citation"></blockquote><h3 id=113122--156350-benchmarking-the-robustness-of-uav-tracking-against-common-corruptions-xiaoqiong-liu-et-al-2024>(113/122 | 156/350) Benchmarking the Robustness of UAV Tracking Against Common Corruptions (Xiaoqiong Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaoqiong Liu, Yunhe Feng, Shu Hu, Xiaohui Yuan, Heng Fan. (2024)<br><strong>Benchmarking the Robustness of UAV Tracking Against Common Corruptions</strong><br><button class=copy-to-clipboard title="Benchmarking the Robustness of UAV Tracking Against Common Corruptions" index=156>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-156 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 6<br>Keywords: Benchmarking, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11424v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11424v1.pdf filename=2403.11424v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The robustness of unmanned aerial vehicle (UAV) tracking is crucial in many tasks like surveillance and robotics. Despite its importance, little attention is paid to the performance of UAV trackers under common corruptions due to lack of a dedicated platform. Addressing this, we propose UAV-C, a large-scale <b>benchmark</b> for assessing robustness of UAV trackers under common corruptions. Specifically, UAV-C is built upon two popular UAV datasets by introducing 18 common corruptions from 4 representative categories including adversarial, sensor, blur, and composite corruptions in different levels. Finally, UAV-C contains more than 10K sequences. To understand the robustness of existing UAV trackers against corruptions, we extensively evaluate 12 representative algorithms on UAV-C. Our study reveals several key findings: 1) Current trackers are vulnerable to corruptions, indicating more attention needed in enhancing the robustness of UAV trackers; 2) When accompanying together, composite corruptions result in more severe degradation to trackers; and 3) While each tracker has its unique performance profile, some trackers may be more sensitive to specific corruptions. By releasing UAV-C, we hope it, along with comprehensive analysis, serves as a valuable resource for advancing the robustness of UAV tracking against corruption. Our UAV-C will be available at <a href=https://github.com/Xiaoqiong-Liu/UAV-C>https://github.com/Xiaoqiong-Liu/UAV-C</a>.</p></p class="citation"></blockquote><h3 id=114122--157350-the-polar-traverse-dataset-a-dataset-of-stereo-camera-images-simulating-traverses-across-lunar-polar-terrain-under-extreme-lighting-conditions-margaret-hansen-et-al-2024>(114/122 | 157/350) The POLAR Traverse Dataset: A Dataset of Stereo Camera Images Simulating Traverses across Lunar Polar Terrain under Extreme Lighting Conditions (Margaret Hansen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Margaret Hansen, Uland Wong, Terrence Fong. (2024)<br><strong>The POLAR Traverse Dataset: A Dataset of Stereo Camera Images Simulating Traverses across Lunar Polar Terrain under Extreme Lighting Conditions</strong><br><button class=copy-to-clipboard title="The POLAR Traverse Dataset: A Dataset of Stereo Camera Images Simulating Traverses across Lunar Polar Terrain under Extreme Lighting Conditions" index=157>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-157 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12194v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12194v1.pdf filename=2403.12194v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present the POLAR Traverse Dataset: a dataset of high-fidelity stereo pair images of lunar-like terrain under polar lighting conditions designed to simulate a straight-line traverse. Images from individual traverses with different camera heights and pitches were recorded at 1 m intervals by moving a suspended stereo bar across a test bed filled with regolith simulant and shaped to mimic lunar south polar terrain. Ground truth <b>geometry</b> and camera position information was also recorded. This dataset is intended for developing and testing software algorithms that rely on stereo or monocular camera images, such as visual odometry, for use in the lunar polar environment, as well as to provide insight into the expected lighting conditions in lunar polar regions.</p></p class="citation"></blockquote><h3 id=115122--158350-roguenerf-a-robust-geometry-consistent-universal-enhancer-for-nerf-sibi-catley-chandar-et-al-2024>(115/122 | 158/350) RoGUENeRF: A Robust Geometry-Consistent Universal Enhancer for NeRF (Sibi Catley-Chandar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sibi Catley-Chandar, Richard Shaw, Gregory Slabaugh, Eduardo Perez-Pellitero. (2024)<br><strong>RoGUENeRF: A Robust Geometry-Consistent Universal Enhancer for NeRF</strong><br><button class=copy-to-clipboard title="RoGUENeRF: A Robust Geometry-Consistent Universal Enhancer for NeRF" index=158>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-158 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11909v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11909v1.pdf filename=2403.11909v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advances in neural rendering have enabled highly photorealistic 3D scene reconstruction and novel view synthesis. Despite this progress, current state-of-the-art methods struggle to reconstruct high frequency detail, due to factors such as a low-frequency bias of radiance fields and inaccurate camera calibration. One approach to mitigate this issue is to enhance images post-rendering. 2D enhancers can be pre-trained to recover some detail but are agnostic to scene <b>geometry</b> and do not easily generalize to new distributions of image degradation. Conversely, existing 3D enhancers are able to transfer detail from nearby training images in a generalizable manner, but suffer from inaccurate camera calibration and can propagate errors from the <b>geometry</b> into rendered images. We propose a neural rendering enhancer, RoGUENeRF, which exploits the best of both paradigms. Our method is pre-trained to learn a general enhancer while also leveraging information from nearby training images via robust 3D alignment and <b>geometry-aware</b> fusion. Our approach restores high-frequency textures while maintaining geometric consistency and is also robust to inaccurate camera calibration. We show that RoGUENeRF substantially enhances the rendering quality of a wide range of neural rendering baselines, e.g. improving the PSNR of MipNeRF360 by 0.63dB and Nerfacto by 1.34dB on the real world 360v2 dataset.</p></p class="citation"></blockquote><h3 id=116122--159350-emie-map-large-scale-road-surface-reconstruction-based-on-explicit-mesh-and-implicit-encoding-wenhua-wu-et-al-2024>(116/122 | 159/350) EMIE-MAP: Large-Scale Road Surface Reconstruction Based on Explicit Mesh and Implicit Encoding (Wenhua Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenhua Wu, Qi Wang, Guangming Wang, Junping Wang, Tiankun Zhao, Yang Liu, Dongchao Gao, Zhe Liu, Hesheng Wang. (2024)<br><strong>EMIE-MAP: Large-Scale Road Surface Reconstruction Based on Explicit Mesh and Implicit Encoding</strong><br><button class=copy-to-clipboard title="EMIE-MAP: Large-Scale Road Surface Reconstruction Based on Explicit Mesh and Implicit Encoding" index=159>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-159 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11789v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11789v1.pdf filename=2403.11789v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Road surface reconstruction plays a vital role in autonomous driving systems, enabling road lane perception and high-precision mapping. Recently, neural implicit encoding has achieved remarkable results in scene representation, particularly in the realistic rendering of scene textures. However, it faces challenges in directly representing geometric information for large-scale scenes. To address this, we propose EMIE-MAP, a novel method for large-scale road surface reconstruction based on explicit mesh and implicit encoding. The road <b>geometry</b> is represented using explicit mesh, where each vertex stores implicit encoding representing the color and semantic information. To overcome the difficulty in optimizing road elevation, we introduce a trajectory-based elevation initialization and an elevation residual learning method based on Multi-Layer Perceptron (MLP). Additionally, by employing implicit encoding and multi-camera color MLPs decoding, we achieve separate modeling of scene physical properties and camera characteristics, allowing surround-view reconstruction compatible with different camera models. Our method achieves remarkable road surface reconstruction performance in a variety of real-world challenging scenarios.</p></p class="citation"></blockquote><h3 id=117122--160350-gaussnav-gaussian-splatting-for-visual-navigation-xiaohan-lei-et-al-2024>(117/122 | 160/350) GaussNav: Gaussian Splatting for Visual Navigation (Xiaohan Lei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaohan Lei, Min Wang, Wengang Zhou, Houqiang Li. (2024)<br><strong>GaussNav: Gaussian Splatting for Visual Navigation</strong><br><button class=copy-to-clipboard title="GaussNav: Gaussian Splatting for Visual Navigation" index=160>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-160 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11625v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11625v2.pdf filename=2403.11625v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In embodied vision, Instance ImageGoal Navigation (IIN) requires an agent to locate a specific object depicted in a goal image within an unexplored environment. The primary difficulty of IIN stems from the necessity of recognizing the target object across varying viewpoints and rejecting potential distractors. Existing map-based navigation methods largely adopt the representation form of Bird&rsquo;s Eye View (BEV) maps, which, however, lack the representation of detailed textures in a scene. To address the above issues, we propose a new Gaussian Splatting Navigation (abbreviated as GaussNav) framework for IIN task, which constructs a novel map representation based on 3D Gaussian Splatting (3DGS). The proposed framework enables the agent to not only memorize the <b>geometry</b> and semantic information of the scene, but also retain the textural features of objects. Our GaussNav framework demonstrates a significant leap in performance, evidenced by an increase in Success weighted by Path Length (SPL) from 0.252 to 0.578 on the challenging Habitat-Matterport 3D (HM3D) dataset. Our code will be made publicly available.</p></p class="citation"></blockquote><h3 id=118122--161350-r3ds-reality-linked-3d-scenes-for-panoramic-scene-understanding-qirui-wu-et-al-2024>(118/122 | 161/350) R3DS: Reality-linked 3D Scenes for Panoramic Scene Understanding (Qirui Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qirui Wu, Sonia Raychaudhuri, Daniel Ritchie, Manolis Savva, Angel X Chang. (2024)<br><strong>R3DS: Reality-linked 3D Scenes for Panoramic Scene Understanding</strong><br><button class=copy-to-clipboard title="R3DS: Reality-linked 3D Scenes for Panoramic Scene Understanding" index=161>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-161 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12301v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12301v1.pdf filename=2403.12301v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce the Reality-linked 3D Scenes (R3DS) dataset of synthetic 3D scenes mirroring the real-world scene arrangements from Matterport3D panoramas. Compared to prior work, R3DS has more complete and densely populated scenes with objects linked to real-world observations in panoramas. R3DS also provides an object support hierarchy, and matching object sets (e.g., same chairs around a dining table) for each scene. Overall, R3DS contains 19K objects represented by 3,784 distinct CAD models from over 100 object categories. We demonstrate the effectiveness of R3DS on the Panoramic Scene Understanding task. We find that: 1) training on R3DS enables better generalization; 2) support relation prediction trained with R3DS improves performance compared to heuristically calculated support; and 3) R3DS offers a challenging <b>benchmark</b> for future work on panoramic scene understanding.</p></p class="citation"></blockquote><h3 id=119122--162350-a-unified-model-for-longitudinal-multi-modal-multi-view-prediction-with-missingness-boqi-chen-et-al-2024>(119/122 | 162/350) A Unified Model for Longitudinal Multi-Modal Multi-View Prediction with Missingness (Boqi Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Boqi Chen, Junier Oliva, Marc Niethammer. (2024)<br><strong>A Unified Model for Longitudinal Multi-Modal Multi-View Prediction with Missingness</strong><br><button class=copy-to-clipboard title="A Unified Model for Longitudinal Multi-Modal Multi-View Prediction with Missingness" index=162>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-162 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12211v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12211v1.pdf filename=2403.12211v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Medical records often consist of different modalities, such as images, text, and tabular information. Integrating all modalities offers a holistic view of a patient&rsquo;s condition, while analyzing them longitudinally provides a better understanding of disease progression. However, real-world longitudinal medical records present challenges: 1) patients may lack some or all of the data for a specific timepoint, and 2) certain modalities or views might be absent for all patients during a particular period. In this work, we introduce a unified model for longitudinal <b>multi-modal</b> multi-view (MMMV) prediction with missingness. Our method allows as many timepoints as desired for input, and aims to leverage all available data, regardless of their availability. We conduct extensive experiments on the knee osteoarthritis dataset from the Osteoarthritis Initiative (OAI) for pain and Kellgren-Lawrence grade (KLG) prediction at a future timepoint. We demonstrate the effectiveness of our method by comparing results from our unified model to specific models that use the same modality and view combinations during training and evaluation. We also show the benefit of having extended temporal data and provide post-hoc analysis for a deeper understanding of each modality/view&rsquo;s importance for different tasks.</p></p class="citation"></blockquote><h3 id=120122--163350-expandable-subspace-ensemble-for-pre-trained-model-based-class-incremental-learning-da-wei-zhou-et-al-2024>(120/122 | 163/350) Expandable Subspace Ensemble for Pre-Trained Model-Based Class-Incremental Learning (Da-Wei Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Da-Wei Zhou, Hai-Long Sun, Han-Jia Ye, De-Chuan Zhan. (2024)<br><strong>Expandable Subspace Ensemble for Pre-Trained Model-Based Class-Incremental Learning</strong><br><button class=copy-to-clipboard title="Expandable Subspace Ensemble for Pre-Trained Model-Based Class-Incremental Learning" index=163>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-163 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12030v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12030v1.pdf filename=2403.12030v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Class-Incremental Learning (CIL) requires a learning system to continually learn new classes without forgetting. Despite the strong performance of Pre-Trained Models (PTMs) in CIL, a critical issue persists: learning new classes often results in the overwriting of old ones. Excessive modification of the network causes forgetting, while minimal adjustments lead to an inadequate fit for new classes. As a result, it is desired to figure out a way of efficient model updating without harming former knowledge. In this paper, we propose ExpAndable Subspace Ensemble (EASE) for PTM-based CIL. To enable model updating without conflict, we train a distinct lightweight adapter module for each new task, aiming to create task-specific subspaces. These adapters span a high-dimensional feature space, enabling joint decision-making across multiple subspaces. As data evolves, the expanding subspaces render the old class classifiers incompatible with new-stage spaces. Correspondingly, we design a semantic-guided prototype complement strategy that synthesizes old classes&rsquo; new features without using any old class instance. Extensive experiments on seven <b>benchmark</b> datasets verify EASE&rsquo;s state-of-the-art performance. Code is available at: <a href=https://github.com/sun-hailong/CVPR24-Ease>https://github.com/sun-hailong/CVPR24-Ease</a></p></p class="citation"></blockquote><h3 id=121122--164350-ivac-p2l-leveraging-irregular-repetition-priors-for-improving-video-action-counting-hang-wang-et-al-2024>(121/122 | 164/350) IVAC-P2L: Leveraging Irregular Repetition Priors for Improving Video Action Counting (Hang Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hang Wang, Zhi-Qi Cheng, Youtian Du, Lei Zhang. (2024)<br><strong>IVAC-P2L: Leveraging Irregular Repetition Priors for Improving Video Action Counting</strong><br><button class=copy-to-clipboard title="IVAC-P2L: Leveraging Irregular Repetition Priors for Improving Video Action Counting" index=164>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-164 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-MM, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11959v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11959v2.pdf filename=2403.11959v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Video Action Counting (VAC) is crucial in analyzing sports, fitness, and everyday activities by quantifying repetitive actions in videos. However, traditional VAC methods have overlooked the complexity of action repetitions, such as interruptions and the variability in cycle duration. Our research addresses the shortfall by introducing a novel approach to VAC, called Irregular Video Action Counting (IVAC). IVAC prioritizes modeling irregular repetition patterns in videos, which we define through two primary aspects: Inter-cycle Consistency and Cycle-interval Inconsistency. Inter-cycle Consistency ensures homogeneity in the spatial-temporal representations of cycle segments, signifying action uniformity within cycles. Cycle-interval inconsistency highlights the importance of distinguishing between cycle segments and intervals based on their inherent content differences. To encapsulate these principles, we propose a new methodology that includes consistency and inconsistency modules, supported by a unique pull-push loss (P2L) mechanism. The IVAC-P2L model applies a pull loss to promote coherence among cycle segment features and a push loss to clearly distinguish features of cycle segments from interval segments. Empirical evaluations conducted on the RepCount dataset demonstrate that the IVAC-P2L model sets a new <b>benchmark</b> in VAC task performance. Furthermore, the model demonstrates exceptional adaptability and generalization across various video contents, outperforming existing models on two additional datasets, UCFRep and Countix, without the need for dataset-specific optimization. These results confirm the efficacy of our approach in addressing irregular repetitions in videos and pave the way for further advancements in video analysis and understanding.</p></p class="citation"></blockquote><h3 id=122122--165350-n-modal-contrastive-losses-with-applications-to-social-media-data-in-trimodal-space-william-theisen-et-al-2024>(122/122 | 165/350) N-Modal Contrastive Losses with Applications to Social Media Data in Trimodal Space (William Theisen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>William Theisen, Walter Scheirer. (2024)<br><strong>N-Modal Contrastive Losses with Applications to Social Media Data in Trimodal Space</strong><br><button class=copy-to-clipboard title="N-Modal Contrastive Losses with Applications to Social Media Data in Trimodal Space" index=165>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-165 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12747v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12747v1.pdf filename=2403.12747v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The social media landscape of conflict dynamics has grown increasingly <b>multi-modal.</b> Recent advancements in model architectures such as CLIP have enabled researchers to begin studying the interplay between the modalities of text and images in a shared latent space. However, CLIP models fail to handle situations on social media when modalities present in a post expand above two. Social media dynamics often require understanding the interplay between not only text and images, but video as well. In this paper we explore an extension of the contrastive loss function to allow for any number of modalities, and demonstrate its usefulness in trimodal spaces on social media. By extending CLIP into three dimensions we can further aide understanding social media landscapes where all three modalities are present (an increasingly common situation). We use a newly collected public data set of Telegram posts containing all three modalities to train, and then demonstrate the usefulness of, a trimodal model in two OSINT scenarios: classifying a social media artifact post as either pro-Russian or pro-Ukrainian and identifying which account a given artifact originated from. While trimodal CLIP models have been explored before (though not on social media data), we also display a novel quadmodal CLIP model. This model can learn the interplay between text, image, video, and audio. We demonstrate new state-of-the-art baseline results on retrieval for quadmodel models moving forward.</p></p class="citation"></blockquote><h2 id=cscr-8>cs.CR (8)</h2><h3 id=18--166350-shifting-the-lens-detecting-malware-in-npm-ecosystem-with-large-language-models-nusrat-zahan-et-al-2024>(1/8 | 166/350) Shifting the Lens: Detecting Malware in npm Ecosystem with Large Language Models (Nusrat Zahan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nusrat Zahan, Philipp Burckhardt, Mikola Lysenko, Feross Aboukhadijeh, Laurie Williams. (2024)<br><strong>Shifting the Lens: Detecting Malware in npm Ecosystem with Large Language Models</strong><br><button class=copy-to-clipboard title="Shifting the Lens: Detecting Malware in npm Ecosystem with Large Language Models" index=166>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-166 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs.CR<br>Keyword Score: 80<br>Keywords: Zero-shot, ChatGPT, GPT, GPT-3, GPT-4, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12196v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12196v1.pdf filename=2403.12196v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Gartner 2022 report predicts that 45% of organizations worldwide will encounter software supply chain attacks by 2025, highlighting the urgency to improve software supply chain security for community and national interests. Current malware detection techniques aid in the manual review process by filtering benign and malware packages, yet such techniques have high false-positive rates and limited automation support. Therefore, malware detection techniques could benefit from advanced, more automated approaches for accurate and minimally false-positive results. The goal of this study is to assist security analysts in identifying malicious packages through the empirical study of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to detect potential malware in the npm ecosystem. We present SocketAI Scanner, a multi-stage decision-maker malware detection workflow using iterative self-refinement and <b>zero-shot-role-play-Chain</b> of Thought (CoT) <b>prompting</b> techniques for <b>ChatGPT.</b> We studied 5,115 npm packages (of which 2,180 are malicious) and performed a baseline comparison of the <b>GPT-3</b> and <b>GPT-4</b> models with a static analysis tool. Our findings showed promising results for <b>GPT</b> models with low misclassification alert rates. Our baseline comparison demonstrates a notable improvement over static analysis in precision scores above 25% and F1 scores above 15%. We attained precision and F1 scores of 91% and 94%, respectively, for the <b>GPT-3</b> model. Overall, <b>GPT-4</b> demonstrates superior performance in precision (99%) and F1 (97%) scores, while <b>GPT-3</b> presents a cost-effective balance between performance and expenditure.</p></p class="citation"></blockquote><h3 id=28--167350-problem-space-structural-adversarial-attacks-for-network-intrusion-detection-systems-based-on-graph-neural-networks-andrea-venturi-et-al-2024>(2/8 | 167/350) Problem space structural adversarial attacks for Network Intrusion Detection Systems based on Graph Neural Networks (Andrea Venturi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andrea Venturi, Dario Stabili, Mirco Marchetti. (2024)<br><strong>Problem space structural adversarial attacks for Network Intrusion Detection Systems based on Graph Neural Networks</strong><br><button class=copy-to-clipboard title="Problem space structural adversarial attacks for Network Intrusion Detection Systems based on Graph Neural Networks" index=167>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-167 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs.CR<br>Keyword Score: 33<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11830v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11830v1.pdf filename=2403.11830v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Machine Learning (ML) algorithms have become increasingly popular for supporting Network Intrusion Detection Systems (NIDS). Nevertheless, extensive research has shown their vulnerability to <b>adversarial</b> <b>attacks,</b> which involve subtle perturbations to the inputs of the models aimed at compromising their performance. Recent proposals have effectively leveraged <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNN)</b> to produce predictions based also on the structural patterns exhibited by intrusions to enhance the detection robustness. However, the adoption of <b>GNN-based</b> NIDS introduces new types of risks. In this paper, we propose the first formalization of <b>adversarial</b> <b>attacks</b> specifically tailored for <b>GNN</b> in network intrusion detection. Moreover, we outline and model the problem space constraints that attackers need to consider to carry out feasible structural attacks in real-world scenarios. As a final contribution, we conduct an extensive experimental campaign in which we launch the proposed attacks against state-of-the-art <b>GNN-based</b> NIDS. Our findings demonstrate the increased robustness of the models against classical feature-based <b>adversarial</b> <b>attacks,</b> while highlighting their susceptibility to structure-based attacks.</p></p class="citation"></blockquote><h3 id=38--168350-large-language-models-in-6g-security-challenges-and-opportunities-tri-nguyen-et-al-2024>(3/8 | 168/350) Large language models in 6G security: challenges and opportunities (Tri Nguyen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tri Nguyen, Huong Nguyen, Ahmad Ijaz, Saeid Sheikhi, Athanasios V. Vasilakos, Panos Kostakos. (2024)<br><strong>Large language models in 6G security: challenges and opportunities</strong><br><button class=copy-to-clipboard title="Large language models in 6G security: challenges and opportunities" index=168>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-168 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-DC, cs.CR<br>Keyword Score: 30<br>Keywords: Generative AI, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12239v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12239v1.pdf filename=2403.12239v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rapid integration of <b>Generative</b> <b>AI</b> (GenAI) and <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> in sectors such as education and healthcare have marked a significant advancement in technology. However, this growth has also led to a largely unexplored aspect: their security vulnerabilities. As the ecosystem that includes both offline and online models, various tools, browser plugins, and third-party applications continues to expand, it significantly widens the attack surface, thereby escalating the potential for security breaches. These expansions in the 6G and beyond landscape provide new avenues for adversaries to manipulate <b>LLMs</b> for malicious purposes. We focus on the security aspects of <b>LLMs</b> from the viewpoint of potential adversaries. We aim to dissect their objectives and methodologies, providing an in-depth analysis of known security weaknesses. This will include the development of a comprehensive threat taxonomy, categorizing various adversary behaviors. Also, our research will concentrate on how <b>LLMs</b> can be integrated into cybersecurity efforts by defense teams, also known as blue teams. We will explore the potential synergy between <b>LLMs</b> and blockchain technology, and how this combination could lead to the development of next-generation, fully autonomous security solutions. This approach aims to establish a unified cybersecurity strategy across the entire computing continuum, enhancing overall digital security infrastructure.</p></p class="citation"></blockquote><h3 id=48--169350-parasitic-circuson-the-feasibility-of-golden-free-pcb-verification-maryam-saadat-safa-et-al-2024>(4/8 | 169/350) Parasitic Circus:On the Feasibility of Golden Free PCB Verification (Maryam Saadat Safa et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Maryam Saadat Safa, Patrick Schaumont, Shahin Tajik. (2024)<br><strong>Parasitic Circus:On the Feasibility of Golden Free PCB Verification</strong><br><button class=copy-to-clipboard title="Parasitic Circus:On the Feasibility of Golden Free PCB Verification" index=169>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-169 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12252v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12252v1.pdf filename=2403.12252v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Printed circuit boards (PCBs) are an integral part of electronic systems. Hence, verifying their physical integrity in the presence of supply chain attacks (e.g., tampering and counterfeiting) is of utmost importance. Recently, tamper detection techniques grounded in impedance characterization of PCB&rsquo;s Power Delivery Network (PDN) have gained prominence due to their global detection coverage, non-invasive, and low-cost nature. Similar to other physical verification methods, these techniques rely on the existence of a physical golden sample for signature comparisons. However, having access to a physical golden sample for golden signature extraction is not feasible in many real-world scenarios. In this work, we assess the feasibility of eliminating a physical golden sample and replacing it with a simulated golden signature obtained by the PCB design files. By performing extensive <b>simulation</b> and measurements on an in-house designed PCB, we demonstrate how the parasitic impedance of the PCB components plays a major role in reaching a successful verification. Based on the obtained results and using statistical metrics, we show that we can mitigate the discrepancy between collected signatures from <b>simulation</b> and measurements.</p></p class="citation"></blockquote><h3 id=58--170350-diffusion-denoising-as-a-certified-defense-against-clean-label-poisoning-sanghyun-hong-et-al-2024>(5/8 | 170/350) Diffusion Denoising as a Certified Defense against Clean-label Poisoning (Sanghyun Hong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sanghyun Hong, Nicholas Carlini, Alexey Kurakin. (2024)<br><strong>Diffusion Denoising as a Certified Defense against Clean-label Poisoning</strong><br><button class=copy-to-clipboard title="Diffusion Denoising as a Certified Defense against Clean-label Poisoning" index=170>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-170 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-CV, cs-LG, cs.CR<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11981v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11981v1.pdf filename=2403.11981v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a certified defense to clean-label poisoning attacks. These attacks work by injecting a small number of poisoning samples (e.g., 1%) that contain $p$-norm bounded adversarial perturbations into the training data to induce a targeted misclassification of a test-time input. Inspired by the adversarial robustness achieved by $denoised$ $smoothing$, we show how an off-the-shelf <b>diffusion</b> <b>model</b> can sanitize the tampered training data. We extensively test our defense against seven clean-label poisoning attacks and reduce their attack success to 0-16% with only a negligible drop in the test time accuracy. We compare our defense with existing countermeasures against clean-label poisoning, showing that the defense reduces the attack success the most and offers the best model utility. Our results highlight the need for future work on developing stronger clean-label attacks and using our certified yet practical defense as a strong baseline to evaluate these attacks.</p></p class="citation"></blockquote><h3 id=68--171350-is-it-really-you-who-forgot-the-password-when-account-recovery-meets-risk-based-authentication-andre-büttner-et-al-2024>(6/8 | 171/350) Is It Really You Who Forgot the Password? When Account Recovery Meets Risk-Based Authentication (Andre Büttner et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andre Büttner, Andreas Thue Pedersen, Stephan Wiefling, Nils Gruschka, Luigi Lo Iacono. (2024)<br><strong>Is It Really You Who Forgot the Password? When Account Recovery Meets Risk-Based Authentication</strong><br><button class=copy-to-clipboard title="Is It Really You Who Forgot the Password? When Account Recovery Meets Risk-Based Authentication" index=171>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-171 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 10<br>Keywords: Anomaly Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11798v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11798v1.pdf filename=2403.11798v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Risk-based authentication (RBA) is used in online services to protect user accounts from unauthorized takeover. RBA commonly uses contextual features that indicate a suspicious login attempt when the characteristic attributes of the login context deviate from known and thus expected values. Previous research on RBA and <b>anomaly</b> <b>detection</b> in authentication has mainly focused on the login process. However, recent attacks have revealed vulnerabilities in other parts of the authentication process, specifically in the account recovery function. Consequently, to ensure comprehensive authentication security, the use of <b>anomaly</b> <b>detection</b> in the context of account recovery must also be investigated. This paper presents the first study to investigate risk-based account recovery (RBAR) in the wild. We analyzed the adoption of RBAR by five prominent online services (that are known to use RBA). Our findings confirm the use of RBAR at Google, LinkedIn, and Amazon. Furthermore, we provide insights into the different RBAR mechanisms of these services and explore the impact of multi-factor authentication on them. Based on our findings, we create a first maturity model for RBAR challenges. The goal of our work is to help developers, administrators, and policy-makers gain an initial understanding of RBAR and to encourage further research in this direction.</p></p class="citation"></blockquote><h3 id=78--172350-efficient-and-privacy-preserving-federated-learning-based-on-full-homomorphic-encryption-yuqi-guo-et-al-2024>(7/8 | 172/350) Efficient and Privacy-Preserving Federated Learning based on Full Homomorphic Encryption (Yuqi Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuqi Guo, Lin Li, Zhongxiang Zheng, Hanrui Yun, Ruoyan Zhang, Xiaolin Chang, Zhixuan Gao. (2024)<br><strong>Efficient and Privacy-Preserving Federated Learning based on Full Homomorphic Encryption</strong><br><button class=copy-to-clipboard title="Efficient and Privacy-Preserving Federated Learning based on Full Homomorphic Encryption" index=172>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-172 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11519v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11519v1.pdf filename=2403.11519v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Since the first theoretically feasible full homomorphic encryption (FHE) scheme was proposed in 2009, great progress has been achieved. These improvements have made FHE schemes come off the paper and become quite useful in solving some practical problems. In this paper, we propose a set of novel <b>Federated</b> <b>Learning</b> Schemes by utilizing the latest homomorphic encryption technologies, so as to improve the security, functionality and practicality at the same time. Comparisons have been given in four practical data sets separately from medical, business, biometric and financial fields, covering both horizontal and vertical <b>federated</b> <b>learning</b> scenarios. The experiment results show that our scheme achieves significant improvements in security, efficiency and practicality, compared with classical horizontal and vertical <b>federated</b> <b>learning</b> schemes.</p></p class="citation"></blockquote><h3 id=88--173350-budget-recycling-differential-privacy-bo-jiang-et-al-2024>(8/8 | 173/350) Budget Recycling Differential Privacy (Bo Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bo Jiang, Jian Du, Sagar Shamar, Qiang Yan. (2024)<br><strong>Budget Recycling Differential Privacy</strong><br><button class=copy-to-clipboard title="Budget Recycling Differential Privacy" index=173>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-173 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-DS, cs.CR, eess-SP<br>Keyword Score: 10<br>Keywords: Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11445v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11445v1.pdf filename=2403.11445v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Differential</b> <b>Privacy</b> (DP) mechanisms usually {force} reduction in data utility by producing <code>out-of-bound'' noisy results for a tight privacy budget. We introduce the Budget Recycling &lt;b>Differential&lt;/b> &lt;b>Privacy&lt;/b> (BR-DP) framework, designed to provide soft-bounded noisy outputs for a broad range of existing DP mechanisms. By </code>soft-bounded," we refer to the mechanism&rsquo;s ability to release most outputs within a predefined error boundary, thereby improving utility and maintaining privacy simultaneously. The core of BR-DP consists of two components: a DP kernel responsible for generating a noisy answer per iteration, and a recycler that probabilistically recycles/regenerates or releases the noisy answer. We delve into the privacy accounting of BR-DP, culminating in the development of a budgeting principle that optimally sub-allocates the available budget between the DP kernel and the recycler. Furthermore, we introduce algorithms for tight BR-DP accounting in composition scenarios, and our findings indicate that BR-DP achieves reduced privacy leakage post-composition compared to DP. Additionally, we explore the concept of privacy amplification via subsampling within the BR-DP framework and propose optimal sampling rates for BR-DP across various queries. We experiment with real data, and the results demonstrate BR-DP&rsquo;s effectiveness in lifting the utility-privacy tradeoff provided by DP mechanisms.</p></p class="citation"></blockquote><h2 id=csne-2>cs.NE (2)</h2><h3 id=12--174350-unsupervised-end-to-end-training-with-a-self-defined-bio-inspired-target-dongshu-liu-et-al-2024>(1/2 | 174/350) Unsupervised End-to-End Training with a Self-Defined Bio-Inspired Target (Dongshu Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dongshu Liu, Jérémie Laydevant, Adrien Pontlevy, Damien Querlioz, Julie Grollier. (2024)<br><strong>Unsupervised End-to-End Training with a Self-Defined Bio-Inspired Target</strong><br><button class=copy-to-clipboard title="Unsupervised End-to-End Training with a Self-Defined Bio-Inspired Target" index=174>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-174 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-ET, cs-LG, cs-NE, cs.NE<br>Keyword Score: 80<br>Keywords: MNIST, Self-supervised Learning, Self-supervised Learning, Semi-Supervised Learning, Supervised Learning, Supervised Learning, Unsupervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12116v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12116v1.pdf filename=2403.12116v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Current <b>unsupervised</b> <b>learning</b> methods depend on end-to-end training via deep learning techniques such as <b>self-supervised</b> <b>learning,</b> with high computational requirements, or employ layer-by-layer training using bio-inspired approaches like Hebbian learning, using local learning rules incompatible with <b>supervised</b> <b>learning.</b> Both approaches are problematic for edge AI hardware that relies on sparse computational resources and would strongly benefit from alternating between <b>unsupervised</b> <b>and</b> <b>supervised</b> <b>learning</b> phases - thus leveraging widely available unlabeled data from the environment as well as labeled training datasets. To solve this challenge, in this work, we introduce a &lsquo;self-defined target&rsquo; that uses Winner-Take-All (WTA) selectivity at the network&rsquo;s final layer, complemented by regularization through biologically inspired homeostasis mechanism. This approach, framework-agnostic and compatible with both global (Backpropagation) and local (Equilibrium propagation) learning rules, achieves a 97.6% test accuracy on the <b>MNIST</b> dataset. Furthermore, we demonstrate that incorporating a hidden layer enhances classification accuracy and the quality of learned features across all training methods, showcasing the advantages of end-to-end <b>unsupervised</b> <b>training.</b> Extending to <b>semi-supervised</b> <b>learning,</b> our method dynamically adjusts the target according to data availability, reaching a 96.6% accuracy with just 600 labeled <b>MNIST</b> samples. This result highlights our <b>&lsquo;unsupervised</b> <b>target&rsquo;</b> strategy&rsquo;s efficacy and flexibility in scenarios ranging from abundant to no labeled data availability.</p></p class="citation"></blockquote><h3 id=22--175350-llm-guided-evolution----the-automation-of-models-advancing-models-clint-morris-et-al-2024>(2/2 | 175/350) LLM Guided Evolution &ndash; The Automation of Models Advancing Models (Clint Morris et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Clint Morris, Michael Jurado, Jason Zutty. (2024)<br><strong>LLM Guided Evolution &ndash; The Automation of Models Advancing Models</strong><br><button class=copy-to-clipboard title="LLM Guided Evolution -- The Automation of Models Advancing Models" index=175>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-175 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-LG, cs-NE, cs.NE<br>Keyword Score: 40<br>Keywords: Supervised Learning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11446v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11446v1.pdf filename=2403.11446v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the realm of machine learning, traditional model development and automated approaches like AutoML typically rely on layers of abstraction, such as tree-based or Cartesian genetic programming. Our study introduces &ldquo;Guided Evolution&rdquo; (GE), a novel framework that diverges from these methods by utilizing <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> to directly modify code. GE leverages <b>LLMs</b> for a more intelligent, <b>supervised</b> evolutionary process, guiding mutations and crossovers. Our unique &ldquo;Evolution of Thought&rdquo; (EoT) technique further enhances GE by enabling <b>LLMs</b> to reflect on and learn from the outcomes of previous mutations. This results in a self-sustaining feedback loop that augments decision-making in model evolution. GE maintains genetic diversity, crucial for evolutionary algorithms, by leveraging <b>LLMs&rsquo;</b> capability to generate diverse responses from expertly crafted <b>prompts</b> and modulate model temperature. This not only accelerates the evolution process but also injects expert like creativity and insight into the process. Our application of GE in evolving the ExquisiteNetV2 model demonstrates its efficacy: the <b>LLM-driven</b> GE autonomously produced variants with improved accuracy, increasing from 92.52% to 93.34%, without compromising model compactness. This underscores the potential of <b>LLMs</b> to accelerate the traditional model design pipeline, enabling models to autonomously evolve and enhance their own designs.</p></p class="citation"></blockquote><h2 id=csro-32>cs.RO (32)</h2><h3 id=132--176350-can-llms-generate-human-like-wayfinding-instructions-towards-platform-agnostic-embodied-instruction-synthesis-vishnu-sashank-dorbala-et-al-2024>(1/32 | 176/350) Can LLMs Generate Human-Like Wayfinding Instructions? Towards Platform-Agnostic Embodied Instruction Synthesis (Vishnu Sashank Dorbala et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vishnu Sashank Dorbala, Sanjoy Chowdhury, Dinesh Manocha. (2024)<br><strong>Can LLMs Generate Human-Like Wayfinding Instructions? Towards Platform-Agnostic Embodied Instruction Synthesis</strong><br><button class=copy-to-clipboard title="Can LLMs Generate Human-Like Wayfinding Instructions? Towards Platform-Agnostic Embodied Instruction Synthesis" index=176>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-176 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs.RO<br>Keyword Score: 80<br>Keywords: Simulation, Simulator, Zero-shot, Question Answering, Visual Question Answering, In-context Learning, In-context Learning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11487v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11487v1.pdf filename=2403.11487v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a novel approach to automatically synthesize &ldquo;wayfinding instructions&rdquo; for an embodied robot agent. In contrast to prior approaches that are heavily reliant on human-annotated datasets designed exclusively for specific <b>simulation</b> platforms, our algorithm uses <b>in-context</b> <b>learning</b> to condition an <b>LLM</b> to generate instructions using just a few references. Using an <b>LLM-based</b> <b>Visual</b> <b>Question</b> <b>Answering</b> strategy, we gather detailed information about the environment which is used by the <b>LLM</b> for instruction synthesis. We implement our approach on multiple <b>simulation</b> platforms including Matterport3D, AI Habitat and ThreeDWorld, thereby demonstrating its platform-agnostic nature. We subjectively evaluate our approach via a user study and observe that 83.3% of users find the synthesized instructions accurately capture the details of the environment and show characteristics similar to those of human-generated instructions. Further, we conduct <b>zero-shot</b> navigation with multiple approaches on the REVERIE dataset using the generated instructions, and observe very close correlation with the baseline on standard success metrics (&lt; 1% change in SR), quantifying the viability of generated instructions in replacing human-annotated data. To the best of our knowledge, ours is the first <b>LLM-driven</b> approach capable of generating &ldquo;human-like&rdquo; instructions in a platform-agnostic manner, without requiring any form of training.</p></p class="citation"></blockquote><h3 id=232--177350-llm3large-language-model-based-task-and-motion-planning-with-motion-failure-reasoning-shu-wang-et-al-2024>(2/32 | 177/350) LLM3:Large Language Model-based Task and Motion Planning with Motion Failure Reasoning (Shu Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shu Wang, Muzhi Han, Ziyuan Jiao, Zeyu Zhang, Ying Nian Wu, Song-Chun Zhu, Hangxin Liu. (2024)<br><strong>LLM3:Large Language Model-based Task and Motion Planning with Motion Failure Reasoning</strong><br><button class=copy-to-clipboard title="LLM3:Large Language Model-based Task and Motion Planning with Motion Failure Reasoning" index=177>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-177 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs.RO<br>Keyword Score: 60<br>Keywords: Simulation, Simulator, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11552v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11552v2.pdf filename=2403.11552v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Conventional Task and Motion Planning (TAMP) approaches rely on manually crafted interfaces connecting symbolic task planning with continuous motion generation. These domain-specific and labor-intensive modules are limited in addressing emerging tasks in real-world settings. Here, we present LLM^3, a novel <b>Large</b> <b>Language</b> <b>Model</b> <b>(LLM)-based</b> TAMP framework featuring a domain-independent interface. Specifically, we leverage the powerful <b>reasoning</b> and planning capabilities of pre-trained <b>LLMs</b> to propose symbolic action sequences and select continuous action parameters for motion planning. Crucially, LLM^3 incorporates motion planning feedback through <b>prompting,</b> allowing the <b>LLM</b> to iteratively refine its proposals by <b>reasoning</b> about motion failure. Consequently, LLM^3 interfaces between task planning and motion planning, alleviating the intricate design process of handling domain-specific messages between them. Through a series of <b>simulations</b> in a box-packing domain, we quantitatively demonstrate the effectiveness of LLM^3 in solving TAMP problems and the efficiency in selecting action parameters. Ablation studies underscore the significant contribution of motion failure <b>reasoning</b> to the success of LLM^3. Furthermore, we conduct qualitative experiments on a physical manipulator, demonstrating the practical applicability of our approach in real-world settings.</p></p class="citation"></blockquote><h3 id=332--178350-multimodal-human-autonomous-agents-interaction-using-pre-trained-language-and-visual-foundation-models-linus-nwankwo-et-al-2024>(3/32 | 178/350) Multimodal Human-Autonomous Agents Interaction Using Pre-Trained Language and Visual Foundation Models (Linus Nwankwo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Linus Nwankwo, Elmar Rueckert. (2024)<br><strong>Multimodal Human-Autonomous Agents Interaction Using Pre-Trained Language and Visual Foundation Models</strong><br><button class=copy-to-clipboard title="Multimodal Human-Autonomous Agents Interaction Using Pre-Trained Language and Visual Foundation Models" index=178>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-178 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 46<br>Keywords: Foundation Model, Multi-modal, Multi-modal, Automatic Speech Recognition, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12273v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12273v1.pdf filename=2403.12273v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we extended the method proposed in [17] to enable humans to interact naturally with autonomous agents through vocal and textual conversations. Our extended method exploits the inherent capabilities of pre-trained <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> <b>multimodal</b> visual language models (VLMs), and <b>speech</b> <b>recognition</b> (SR) models to decode the high-level natural language conversations and semantic understanding of the robot&rsquo;s task environment, and abstract them to the robot&rsquo;s actionable commands or queries. We performed a quantitative evaluation of our framework&rsquo;s natural vocal conversation understanding with participants from different racial backgrounds and English language accents. The participants interacted with the robot using both spoken and textual instructional commands. Based on the logged interaction data, our framework achieved 87.55% vocal commands decoding accuracy, 86.27% commands execution success, and an average latency of 0.89 seconds from receiving the participants&rsquo; vocal chat commands to initiating the robot&rsquo;s actual physical action. The video demonstrations of this paper can be found at <a href=https://linusnep.github.io/MTCC-IRoNL/>https://linusnep.github.io/MTCC-IRoNL/</a>.</p></p class="citation"></blockquote><h3 id=432--179350-sim-to-real-grasp-detection-with-global-to-local-rgb-d-adaptation-haoxiang-ma-et-al-2024>(4/32 | 179/350) Sim-to-Real Grasp Detection with Global-to-Local RGB-D Adaptation (Haoxiang Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haoxiang Ma, Ran Qin, Modi shi, Boyang Gao, Di Huang. (2024)<br><strong>Sim-to-Real Grasp Detection with Global-to-Local RGB-D Adaptation</strong><br><button class=copy-to-clipboard title="Sim-to-Real Grasp Detection with Global-to-Local RGB-D Adaptation" index=179>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-179 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-CV, cs-RO, cs.RO<br>Keyword Score: 46<br>Keywords: Benchmarking, Multi-modal, Self-supervised Learning, Simulation, Simulator, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11511v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11511v1.pdf filename=2403.11511v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper focuses on the sim-to-real issue of RGB-D grasp detection and formulates it as a <b>domain</b> <b>adaptation</b> problem. In this case, we present a global-to-local method to address hybrid <b>domain</b> <b>gaps</b> in RGB and depth data and insufficient <b>multi-modal</b> feature alignment. First, a <b>self-supervised</b> rotation pre-training strategy is adopted to deliver robust initialization for RGB and depth networks. We then propose a global-to-local alignment pipeline with individual global <b>domain</b> <b>classifiers</b> for scene features of RGB and depth images as well as a local one specifically working for grasp features in the two modalities. In particular, we propose a grasp prototype adaptation module, which aims to facilitate fine-grained local feature alignment by dynamically updating and matching the grasp prototypes from the <b>simulation</b> and real-world scenarios throughout the training process. Due to such designs, the proposed method substantially reduces the <b>domain</b> <b>shift</b> and thus leads to consistent performance improvements. Extensive experiments are conducted on the GraspNet-Planar <b>benchmark</b> and physical environment, and superior results are achieved which demonstrate the effectiveness of our method.</p></p class="citation"></blockquote><h3 id=532--180350-continual-domain-randomization-josip-josifovski-et-al-2024>(5/32 | 180/350) Continual Domain Randomization (Josip Josifovski et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Josip Josifovski, Sayantan Auddy, Mohammadhossein Malmir, Justus Piater, Alois Knoll, Nicolás Navarro-Guerrero. (2024)<br><strong>Continual Domain Randomization</strong><br><button class=copy-to-clipboard title="Continual Domain Randomization" index=180>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-180 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 40<br>Keywords: Continual Learning, Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12193v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12193v1.pdf filename=2403.12193v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Domain Randomization (DR) is commonly used for sim2real transfer of <b>reinforcement</b> <b>learning</b> (RL) policies in robotics. Most DR approaches require a simulator with a fixed set of tunable parameters from the start of the training, from which the parameters are randomized simultaneously to train a robust model for use in the real world. However, the combined randomization of many parameters increases the task difficulty and might result in sub-optimal policies. To address this problem and to provide a more flexible training process, we propose <b>Continual</b> <b>Domain</b> Randomization (CDR) for RL that combines domain randomization with <b>continual</b> <b>learning</b> to enable sequential training in <b>simulation</b> on a subset of randomization parameters at a time. Starting from a model trained in a non-randomized <b>simulation</b> where the task is easier to solve, the model is trained on a sequence of randomizations, and <b>continual</b> <b>learning</b> is employed to remember the effects of previous randomizations. Our robotic reaching and grasping tasks experiments show that the model trained in this fashion learns effectively in <b>simulation</b> and performs robustly on the real robot while matching or outperforming baselines that employ combined randomization or sequential randomization without <b>continual</b> <b>learning.</b> Our code and videos are available at <a href=https://continual-dr.github.io/>https://continual-dr.github.io/</a>.</p></p class="citation"></blockquote><h3 id=632--181350-synthesizing-multi-log-grasp-poses-arvid-fälldin-et-al-2024>(6/32 | 181/350) Synthesizing multi-log grasp poses (Arvid Fälldin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Arvid Fälldin, Erik Wallin, Tommy Löfstedt, Martin Servin. (2024)<br><strong>Synthesizing multi-log grasp poses</strong><br><button class=copy-to-clipboard title="Synthesizing multi-log grasp poses" index=181>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-181 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 40<br>Keywords: Convolution, Convolutional Neural Network, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11623v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11623v1.pdf filename=2403.11623v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multi-object grasping is a challenging task. It is important for energy and cost-efficient operation of industrial crane manipulators, such as those used to collect tree logs off the forest floor and onto forest machines. In this work, we used synthetic data from physics <b>simulations</b> to explore how data-driven modeling can be used to infer multi-object grasp poses from images. We showed that <b>convolutional</b> <b>neural</b> <b>networks</b> can be trained specifically for synthesizing multi-object grasps. Using RGB-Depth images and instance segmentation masks as input, a U-Net model outputs grasp maps with corresponding grapple orientation and opening width. Given an observation of a pile of logs, the model can be used to synthesize and rate the possible grasp poses and select the most suitable one, with the possibility to respect changing operational constraints such as lift capacity and reach. When tested on previously unseen data, the proposed model found successful grasp poses with an accuracy of 95%.</p></p class="citation"></blockquote><h3 id=732--182350-visual-preference-inference-an-image-sequence-based-preference-reasoning-in-tabletop-object-manipulation-joonhyung-lee-et-al-2024>(7/32 | 182/350) Visual Preference Inference: An Image Sequence-Based Preference Reasoning in Tabletop Object Manipulation (Joonhyung Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Joonhyung Lee, Sangbeom Park, Yongin Kwon, Jemin Lee, Minwook Ahn, Sungjoon Choi. (2024)<br><strong>Visual Preference Inference: An Image Sequence-Based Preference Reasoning in Tabletop Object Manipulation</strong><br><button class=copy-to-clipboard title="Visual Preference Inference: An Image Sequence-Based Preference Reasoning in Tabletop Object Manipulation" index=182>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-182 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 40<br>Keywords: Simulation, Simulator, Reasoning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11513v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11513v1.pdf filename=2403.11513v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In robotic object manipulation, human preferences can often be influenced by the visual attributes of objects, such as color and shape. These properties play a crucial role in operating a robot to interact with objects and align with human intention. In this paper, we focus on the problem of inferring underlying human preferences from a sequence of raw visual observations in tabletop manipulation environments with a variety of object types, named Visual Preference Inference (VPI). To facilitate visual <b>reasoning</b> in the context of manipulation, we introduce the Chain-of-Visual-Residuals (CoVR) method. CoVR employs a <b>prompting</b> mechanism that describes the difference between the consecutive images (i.e., visual residuals) and incorporates such texts with a sequence of images to infer the user&rsquo;s preference. This approach significantly enhances the ability to understand and adapt to dynamic changes in its visual environment during manipulation tasks. Furthermore, we incorporate such texts along with a sequence of images to infer the user&rsquo;s preferences. Our method outperforms baseline methods in terms of extracting human preferences from visual sequences in both <b>simulation</b> and real-world environments. Code and videos are available at: \href{https://joonhyung-lee.github.io/vpi/}{https://joonhyung-lee.github.io/vpi/}</p></p class="citation"></blockquote><h3 id=832--183350-aldm-grasping-diffusion-aided-zero-shot-sim-to-real-transfer-for-robot-grasping-yiwei-li-et-al-2024>(8/32 | 183/350) ALDM-Grasping: Diffusion-aided Zero-Shot Sim-to-Real Transfer for Robot Grasping (Yiwei Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yiwei Li, Zihao Wu, Huaqin Zhao, Tianze Yang, Zhengliang Liu, Peng Shu, Jin Sun, Ramviyas Parasuraman, Tianming Liu. (2024)<br><strong>ALDM-Grasping: Diffusion-aided Zero-Shot Sim-to-Real Transfer for Robot Grasping</strong><br><button class=copy-to-clipboard title="ALDM-Grasping: Diffusion-aided Zero-Shot Sim-to-Real Transfer for Robot Grasping" index=183>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-183 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 40<br>Keywords: Simulation, Simulator, Zero-shot, Zero-shot Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11459v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11459v1.pdf filename=2403.11459v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To tackle the &ldquo;reality gap&rdquo; encountered in Sim-to-Real transfer, this study proposes a diffusion-based framework that minimizes inconsistencies in grasping actions between the <b>simulation</b> settings and realistic environments. The process begins by training an adversarial supervision layout-to-image diffusion model(ALDM). Then, leverage the ALDM approach to enhance the <b>simulation</b> environment, rendering it with photorealistic fidelity, thereby optimizing robotic grasp task training. Experimental results indicate this framework outperforms existing models in both success rates and adaptability to new environments through improvements in the accuracy and reliability of visual grasping actions under a variety of conditions. Specifically, it achieves a 75% success rate in grasping tasks under plain backgrounds and maintains a 65% success rate in more complex scenarios. This performance demonstrates this framework excels at generating controlled image content based on text descriptions, identifying object grasp points, and demonstrating <b>zero-shot</b> <b>learning</b> in complex, unseen scenarios.</p></p class="citation"></blockquote><h3 id=932--184350-demystifying-deep-reinforcement-learning-based-autonomous-vehicle-decision-making-hanxi-wan-et-al-2024>(9/32 | 184/350) Demystifying Deep Reinforcement Learning-Based Autonomous Vehicle Decision-Making (Hanxi Wan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hanxi Wan, Pei Li, Arpan Kusari. (2024)<br><strong>Demystifying Deep Reinforcement Learning-Based Autonomous Vehicle Decision-Making</strong><br><button class=copy-to-clipboard title="Demystifying Deep Reinforcement Learning-Based Autonomous Vehicle Decision-Making" index=184>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-184 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-LG, cs-RO, cs.RO<br>Keyword Score: 35<br>Keywords: Black Box, Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11432v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11432v1.pdf filename=2403.11432v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the advent of universal function approximators in the domain of <b>reinforcement</b> <b>learning,</b> the number of practical applications leveraging deep <b>reinforcement</b> <b>learning</b> (DRL) has exploded. Decision-making in automated driving tasks has emerged as a chief application among them, taking the sensor data or the higher-order kinematic variables as the input and providing a discrete choice or continuous control output. However, the <b>black-box</b> <b>nature</b> of the models presents an overwhelming limitation that restricts the real-world deployment of DRL in autonomous vehicles (AVs). Therefore, in this research work, we focus on the interpretability of an attention-based DRL framework. We use a continuous proximal policy optimization-based DRL algorithm as the baseline model and add a multi-head attention framework in an open-source AV <b>simulation</b> environment. We provide some analytical techniques for discussing the interpretability of the trained models in terms of explainability and causality for spatial and temporal correlations. We show that the weights in the first head encode the positions of the neighboring vehicles while the second head focuses on the leader vehicle exclusively. Also, the ego vehicle&rsquo;s action is causally dependent on the vehicles in the target lane spatially and temporally. Through these findings, we reliably show that these techniques can help practitioners decipher the results of the DRL algorithms.</p></p class="citation"></blockquote><h3 id=1032--185350-improving-out-of-distribution-generalization-of-learned-dynamics-by-learning-pseudometrics-and-constraint-manifolds-yating-lin-et-al-2024>(10/32 | 185/350) Improving Out-of-Distribution Generalization of Learned Dynamics by Learning Pseudometrics and Constraint Manifolds (Yating Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yating Lin, Glen Chou, Dmitry Berenson. (2024)<br><strong>Improving Out-of-Distribution Generalization of Learned Dynamics by Learning Pseudometrics and Constraint Manifolds</strong><br><button class=copy-to-clipboard title="Improving Out-of-Distribution Generalization of Learned Dynamics by Learning Pseudometrics and Constraint Manifolds" index=185>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-185 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Contrastive Learning, Gaussian Process, Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12245v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12245v2.pdf filename=2403.12245v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose a method for improving the prediction accuracy of learned robot dynamics models on <b>out-of-distribution</b> (OOD) states. We achieve this by leveraging two key sources of structure often present in robot dynamics: 1) sparsity, i.e., some components of the state may not affect the dynamics, and 2) physical limits on the set of possible motions, in the form of nonholonomic constraints. Crucially, we do not assume this structure is known a priori, and instead learn it from data. We use <b>contrastive</b> <b>learning</b> to obtain a distance pseudometric that uncovers the sparsity pattern in the dynamics, and use it to reduce the input space when learning the dynamics. We then learn the unknown constraint manifold by approximating the normal space of possible motions from the data, which we use to train a <b>Gaussian</b> <b>process</b> (GP) representation of the constraint manifold. We evaluate our approach on a physical differential-drive robot and a simulated quadrotor, showing improved prediction accuracy on OOD data relative to baselines.</p></p class="citation"></blockquote><h3 id=1132--186350-bootstrapping-reinforcement-learning-with-imitation-for-vision-based-agile-flight-jiaxu-xing-et-al-2024>(11/32 | 186/350) Bootstrapping Reinforcement Learning with Imitation for Vision-Based Agile Flight (Jiaxu Xing et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiaxu Xing, Angel Romero, Leonard Bauersfeld, Davide Scaramuzza. (2024)<br><strong>Bootstrapping Reinforcement Learning with Imitation for Vision-Based Agile Flight</strong><br><button class=copy-to-clipboard title="Bootstrapping Reinforcement Learning with Imitation for Vision-Based Agile Flight" index=186>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-186 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-CV, cs-LG, cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Fine-tuning, Knowledge Distillation, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12203v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12203v1.pdf filename=2403.12203v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We combine the effectiveness of <b>Reinforcement</b> <b>Learning</b> (RL) and the efficiency of Imitation Learning (IL) in the context of vision-based, autonomous drone racing. We focus on directly processing visual input without explicit state estimation. While RL offers a general framework for learning complex controllers through trial and error, it faces challenges regarding sample efficiency and computational demands due to the high dimensionality of visual inputs. Conversely, IL demonstrates efficiency in learning from visual demonstrations but is limited by the quality of those demonstrations and faces issues like covariate shift. To overcome these limitations, we propose a novel training framework combining RL and IL&rsquo;s advantages. Our framework involves three stages: initial training of a teacher policy using privileged state information, <b>distilling</b> this policy into a student policy using IL, and performance-constrained adaptive RL <b>fine-tuning.</b> Our experiments in both simulated and real-world environments demonstrate that our approach achieves superior performance and robustness than IL or RL alone in navigating a quadrotor through a racing course using only visual information without explicit state estimation.</p></p class="citation"></blockquote><h3 id=1232--187350-sim2real-manipulation-on-unknown-objects-with-tactile-based-reinforcement-learning-entong-su-et-al-2024>(12/32 | 187/350) Sim2Real Manipulation on Unknown Objects with Tactile-based Reinforcement Learning (Entong Su et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Entong Su, Chengzhe Jia, Yuzhe Qin, Wenxuan Zhou, Annabella Macaluso, Binghao Huang, Xiaolong Wang. (2024)<br><strong>Sim2Real Manipulation on Unknown Objects with Tactile-based Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Sim2Real Manipulation on Unknown Objects with Tactile-based Reinforcement Learning" index=187>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-187 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12170v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12170v1.pdf filename=2403.12170v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Using tactile sensors for manipulation remains one of the most challenging problems in robotics. At the heart of these challenges is generalization: How can we train a tactile-based policy that can manipulate unseen and diverse objects? In this paper, we propose to perform <b>Reinforcement</b> <b>Learning</b> with only visual tactile sensing inputs on diverse objects in a physical simulator. By training with diverse objects in <b>simulation,</b> it enables the policy to generalize to unseen objects. However, leveraging <b>simulation</b> introduces the Sim2Real transfer problem. To mitigate this problem, we study different tactile representations and evaluate how each affects real-robot manipulation results after transfer. We conduct our experiments on diverse real-world objects and show significant improvements over baselines for the pivoting task. Our project page is available at <a href=https://tactilerl.github.io/>https://tactilerl.github.io/</a>.</p></p class="citation"></blockquote><h3 id=1332--188350-inferring-belief-states-in-partially-observable-human-robot-teams-jack-kolb-et-al-2024>(13/32 | 188/350) Inferring Belief States in Partially-Observable Human-Robot Teams (Jack Kolb et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jack Kolb, Karen M. Feigh. (2024)<br><strong>Inferring Belief States in Partially-Observable Human-Robot Teams</strong><br><button class=copy-to-clipboard title="Inferring Belief States in Partially-Observable Human-Robot Teams" index=188>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-188 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-HC, cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Simulation, Simulator, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11955v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11955v1.pdf filename=2403.11955v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We investigate the real-time estimation of human situation awareness using observations from a robot teammate with limited visibility. In human factors and human-autonomy teaming, it is recognized that individuals navigate their environments using an internal mental <b>simulation,</b> or mental model. The mental model informs cognitive processes including situation awareness, contextual <b>reasoning,</b> and task planning. In teaming domains, the mental model includes a team model of each teammate&rsquo;s beliefs and capabilities, enabling fluent teamwork without the need for explicit communication. However, little work has applied team models to human-robot teaming. We compare the performance of two current methods at estimating user situation awareness over varying visibility conditions. Our results indicate that the methods are largely resilient to low-visibility conditions in our domain, however opportunities exist to improve their overall performance.</p></p class="citation"></blockquote><h3 id=1432--189350-deep-bayesian-future-fusion-for-self-supervised-high-resolution-off-road-mapping-shubhra-aich-et-al-2024>(14/32 | 189/350) Deep Bayesian Future Fusion for Self-Supervised, High-Resolution, Off-Road Mapping (Shubhra Aich et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shubhra Aich, Wenshan Wang, Parv Maheshwari, Matthew Sivaprakasam, Samuel Triest, Cherie Ho, Jason M. Gregory, John G. Rogers III, Sebastian Scherer. (2024)<br><strong>Deep Bayesian Future Fusion for Self-Supervised, High-Resolution, Off-Road Mapping</strong><br><button class=copy-to-clipboard title="Deep Bayesian Future Fusion for Self-Supervised, High-Resolution, Off-Road Mapping" index=189>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-189 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-CV, cs-LG, cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11876v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11876v1.pdf filename=2403.11876v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The limited sensing resolution of resource-constrained off-road vehicles poses significant challenges towards reliable off-road autonomy. To overcome this limitation, we propose a general framework based on fusing the future information (i.e. future fusion) for self-supervision. Recent approaches exploit this future information alongside the hand-crafted heuristics to directly supervise the targeted downstream tasks (e.g. traversability estimation). However, in this paper, we opt for a more general line of development - time-efficient completion of the highest resolution (i.e. 2cm per pixel) BEV map in a <b>self-supervised</b> manner via future fusion, which can be used for any downstream tasks for better longer range prediction. To this end, first, we create a high-resolution future-fusion dataset containing pairs of (RGB / height) raw sparse and noisy inputs and map-based dense labels. Next, to accommodate the noise and sparsity of the sensory information, especially in the distal regions, we design an efficient realization of the Bayes filter onto the vanilla <b>convolutional</b> <b>network</b> via the recurrent mechanism. Equipped with the ideas from SOTA generative models, our Bayesian structure effectively predicts high-quality BEV maps in the distal regions. Extensive evaluation on both the quality of completion and downstream task on our future-fusion dataset demonstrates the potential of our approach.</p></p class="citation"></blockquote><h3 id=1532--190350-reinforcement-learning-with-latent-state-inference-for-autonomous-on-ramp-merging-under-observation-delay-amin-tabrizian-et-al-2024>(15/32 | 190/350) Reinforcement Learning with Latent State Inference for Autonomous On-ramp Merging under Observation Delay (Amin Tabrizian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amin Tabrizian, Zhitong Huang, Peng Wei. (2024)<br><strong>Reinforcement Learning with Latent State Inference for Autonomous On-ramp Merging under Observation Delay</strong><br><button class=copy-to-clipboard title="Reinforcement Learning with Latent State Inference for Autonomous On-ramp Merging under Observation Delay" index=190>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-190 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11852v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11852v2.pdf filename=2403.11852v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a novel approach to address the challenging problem of autonomous on-ramp merging, where a self-driving vehicle needs to seamlessly integrate into a flow of vehicles on a multi-lane highway. We introduce the Lane-keeping, Lane-changing with Latent-state Inference and Safety Controller (L3IS) agent, designed to perform the on-ramp merging task safely without comprehensive knowledge about surrounding vehicles&rsquo; intents or driving styles. We also present an augmentation of this agent called AL3IS that accounts for observation delays, allowing the agent to make more robust decisions in real-world environments with vehicle-to-vehicle (V2V) communication delays. By modeling the unobservable aspects of the environment through latent states, such as other drivers&rsquo; intents, our approach enhances the agent&rsquo;s ability to adapt to dynamic traffic conditions, optimize merging maneuvers, and ensure safe interactions with other vehicles. We demonstrate the effectiveness of our method through extensive <b>simulations</b> generated from real traffic data and compare its performance with existing approaches. L3IS shows a 99.90% success rate in a challenging on-ramp merging case generated from the real US Highway 101 data. We further perform a sensitivity analysis on AL3IS to evaluate its robustness against varying observation delays, which demonstrates an acceptable performance of 93.84% success rate in 1-second V2V communication delay.</p></p class="citation"></blockquote><h3 id=1632--191350-locomotion-generation-for-a-rat-robot-based-on-environmental-changes-via-reinforcement-learning-xinhui-shan-et-al-2024>(16/32 | 191/350) Locomotion Generation for a Rat Robot based on Environmental Changes via Reinforcement Learning (Xinhui Shan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinhui Shan, Yuhong Huang, Zhenshan Bing, Zitao Zhang, Xiangtong Yao, Kai Huang, Alois Knoll. (2024)<br><strong>Locomotion Generation for a Rat Robot based on Environmental Changes via Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Locomotion Generation for a Rat Robot based on Environmental Changes via Reinforcement Learning" index=191>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-191 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11788v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11788v2.pdf filename=2403.11788v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This research focuses on developing <b>reinforcement</b> <b>learning</b> approaches for the locomotion generation of small-size quadruped robots. The rat robot NeRmo is employed as the experimental platform. Due to the constrained volume, small-size quadruped robots typically possess fewer and weaker sensors, resulting in difficulty in accurately perceiving and responding to environmental changes. In this context, insufficient and imprecise feedback data from sensors makes it difficult to generate adaptive locomotion based on <b>reinforcement</b> <b>learning.</b> To overcome these challenges, this paper proposes a novel <b>reinforcement</b> <b>learning</b> approach that focuses on extracting effective perceptual information to enhance the environmental adaptability of small-size quadruped robots. According to the frequency of a robot&rsquo;s gait stride, key information of sensor data is analyzed utilizing sinusoidal functions derived from Fourier transform results. Additionally, a multifunctional reward mechanism is proposed to generate adaptive locomotion in different tasks. Extensive <b>simulations</b> are conducted to assess the effectiveness of the proposed <b>reinforcement</b> <b>learning</b> approach in generating rat robot locomotion in various environments. The experiment results illustrate the capability of the proposed approach to maintain stable locomotion of a rat robot across different terrains, including ramps, stairs, and spiral stairs.</p></p class="citation"></blockquote><h3 id=1732--192350-combining-local-and-global-perception-for-autonomous-navigation-on-nano-uavs-lorenzo-lamberti-et-al-2024>(17/32 | 192/350) Combining Local and Global Perception for Autonomous Navigation on Nano-UAVs (Lorenzo Lamberti et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lorenzo Lamberti, Georg Rutishauser, Francesco Conti, Luca Benini. (2024)<br><strong>Combining Local and Global Perception for Autonomous Navigation on Nano-UAVs</strong><br><button class=copy-to-clipboard title="Combining Local and Global Perception for Autonomous Navigation on Nano-UAVs" index=192>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-192 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs-SY, cs.RO, eess-SY<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Information Retrieval<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11661v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11661v1.pdf filename=2403.11661v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A critical challenge in deploying unmanned aerial vehicles (UAVs) for autonomous tasks is their ability to navigate in an unknown environment. This paper introduces a novel vision-depth fusion approach for autonomous navigation on nano-UAVs. We combine the visual-based PULP-Dronet <b>convolutional</b> <b>neural</b> <b>network</b> for semantic <b>information</b> <b>extraction,</b> i.e., serving as the global perception, with 8x8px depth maps for close-proximity maneuvers, i.e., the local perception. When tested in-field, our integration strategy highlights the complementary strengths of both visual and depth sensory <b>information.</b> <b>We</b> achieve a 100% success rate over 15 flights in a complex navigation scenario, encompassing straight pathways, static obstacle avoidance, and 90{\deg} turns.</p></p class="citation"></blockquote><h3 id=1832--193350-scalable-networked-feature-selection-with-randomized-algorithm-for-robot-navigation-vivek-pandey-et-al-2024>(18/32 | 193/350) Scalable Networked Feature Selection with Randomized Algorithm for Robot Navigation (Vivek Pandey et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vivek Pandey, Arash Amini, Guangyi Liu, Ufuk Topcu, Qiyu Sun, Kostas Daniilidis, Nader Motee. (2024)<br><strong>Scalable Networked Feature Selection with Randomized Algorithm for Robot Navigation</strong><br><button class=copy-to-clipboard title="Scalable Networked Feature Selection with Randomized Algorithm for Robot Navigation" index=193>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-193 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 23<br>Keywords: Graph, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12279v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12279v1.pdf filename=2403.12279v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We address the problem of sparse selection of visual features for localizing a team of robots navigating an unknown environment, where robots can exchange relative position measurements with neighbors. We select a set of the most informative features by anticipating their importance in robots localization by simulating trajectories of robots over a prediction horizon. Through theoretical proofs, we establish a crucial connection between <b>graph</b> Laplacian and the importance of features. We show that strong network connectivity translates to uniformity in feature importance, which enables uniform random sampling of features and reduces the overall computational complexity. We leverage a scalable randomized algorithm for sparse sums of positive semidefinite matrices to efficiently select the set of the most informative features and significantly improve the probabilistic performance bounds. Finally, we support our findings with extensive <b>simulations.</b></p></p class="citation"></blockquote><h3 id=1932--194350-robot-navigation-in-unknown-and-cluttered-workspace-with-dynamical-system-modulation-in-starshaped-roadmap-kai-chen-et-al-2024>(19/32 | 194/350) Robot Navigation in Unknown and Cluttered Workspace with Dynamical System Modulation in Starshaped Roadmap (Kai Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kai Chen, Haichao Liu, Yulin Li, Jianghua Duan, Lei Zhu, Jun Ma. (2024)<br><strong>Robot Navigation in Unknown and Cluttered Workspace with Dynamical System Modulation in Starshaped Roadmap</strong><br><button class=copy-to-clipboard title="Robot Navigation in Unknown and Cluttered Workspace with Dynamical System Modulation in Starshaped Roadmap" index=194>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-194 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 23<br>Keywords: Benchmarking, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11484v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11484v1.pdf filename=2403.11484v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a novel reactive motion planning framework for navigating robots in unknown and cluttered 2D workspace. Typical existing methods are developed by enforcing the robot staying in free regions represented by the locally extracted ellipse or polygon. Instead, we navigate the robot in free space with an alternate starshaped decomposition, which is calculated directly from real-time sensor data. Additionally, a roadmap is constructed incrementally to maintain the connectivity information of the starshaped regions. Compared to the roadmap built upon connected polygons or ellipses in the conventional approaches, the concave starshaped region is better suited to capture the natural distribution of sensor data, so that the perception information can be fully exploited for robot navigation. In this sense, conservative and myopic behaviors are avoided with the proposed approach, and intricate obstacle configurations can be suitably accommodated in unknown and cluttered environments. Then, we design a heuristic exploration algorithm on the roadmap to determine the frontier points of the starshaped regions, from which short-term goals are selected to attract the robot towards the goal configuration. It is noteworthy that, a recovery mechanism is developed on the roadmap that is triggered once a non-extendable short-term goal is reached. This mechanism renders it possible to deal with dead-end situations that can be typically encountered in unknown and cluttered environments. Furthermore, safe and smooth motion within the starshaped regions is generated by employing the Dynamical System Modulation (DSM) approach on the constructed roadmap. Through comprehensive evaluation in both <b>simulations</b> and real-world experiments, the proposed method outperforms the <b>benchmark</b> methods in terms of success rate and traveling time.</p></p class="citation"></blockquote><h3 id=2032--195350-ikspark-an-inverse-kinematics-solver-using-semidefinite-relaxation-and-rank-minimization-liangting-wu-et-al-2024>(20/32 | 195/350) IKSPARK: An Inverse Kinematics Solver using Semidefinite Relaxation and Rank Minimization (Liangting Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Liangting Wu, Roberto Tron. (2024)<br><strong>IKSPARK: An Inverse Kinematics Solver using Semidefinite Relaxation and Rank Minimization</strong><br><button class=copy-to-clipboard title="IKSPARK: An Inverse Kinematics Solver using Semidefinite Relaxation and Rank Minimization" index=195>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-195 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs-SY, cs.RO, eess-SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12235v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12235v1.pdf filename=2403.12235v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Inverse kinematics (IK) is a fundamental problem frequently occurred in robot control and motion planning. However, the problem is nonconvex because the kinematic map between the configuration and task spaces is generally nonlinear, which makes it challenging for fast and accurate solutions. The problem can be more complicated with the existence of different physical constraints imposed by the robot structure. In this paper, we develop an inverse kinematics solver named IKSPARK (Inverse Kinematics using Semidefinite Programming And RanK minimization) that can find solutions for robots with various structures, including open/closed kinematic chains, spherical, revolute, and/or prismatic joints. The solver works in the space of rotation matrices of the link reference frames and involves solving only convex semidefinite problems (SDPs). Specifically, the IK problem is formulated as an SDP with an additional rank-1 constraint on symmetric matrices with constant traces. The solver first solves this SDP disregarding the rank constraint to get a start point and then finds the rank-1 solution iteratively via a rank minimization algorithm with proven local convergence. Compared to other work that performs SDP relaxation for IK problems, our formulation is simpler, and uses variables with smaller sizes. We validate our approach via <b>simulations</b> on different robots, comparing against a standard IK method.</p></p class="citation"></blockquote><h3 id=2132--196350-frontier-based-exploration-for-multi-robot-rendezvous-in-communication-restricted-unknown-environments-mauro-tellaroli-et-al-2024>(21/32 | 196/350) Frontier-Based Exploration for Multi-Robot Rendezvous in Communication-Restricted Unknown Environments (Mauro Tellaroli et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mauro Tellaroli, Matteo Luperto, Michele Antonazzi, Nicola Basilico. (2024)<br><strong>Frontier-Based Exploration for Multi-Robot Rendezvous in Communication-Restricted Unknown Environments</strong><br><button class=copy-to-clipboard title="Frontier-Based Exploration for Multi-Robot Rendezvous in Communication-Restricted Unknown Environments" index=196>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-196 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11617v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11617v1.pdf filename=2403.11617v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multi-robot rendezvous and exploration are fundamental challenges in the domain of mobile robotic systems. This paper addresses multi-robot rendezvous within an initially unknown environment where communication is only possible after the rendezvous. Traditionally, exploration has been focused on rapidly mapping the environment, often leading to suboptimal rendezvous performance in later stages. We adapt a standard frontier-based exploration technique to integrate exploration and rendezvous into a unified strategy, with a mechanism that allows robots to re-visit previously explored regions thus enhancing rendezvous opportunities. We validate our approach in 3D realistic <b>simulations</b> using ROS, showcasing its effectiveness in achieving faster rendezvous times compared to exploration strategies.</p></p class="citation"></blockquote><h3 id=2232--197350-r2snet-scalable-domain-adaptation-for-object-detection-in-cloud-based-robots-ecosystems-via-proposal-refinement-michele-antonazzi-et-al-2024>(22/32 | 197/350) R2SNet: Scalable Domain Adaptation for Object Detection in Cloud-Based Robots Ecosystems via Proposal Refinement (Michele Antonazzi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michele Antonazzi, Matteo Luperto, N. Alberto Borghese, Nicola Basilico. (2024)<br><strong>R2SNet: Scalable Domain Adaptation for Object Detection in Cloud-Based Robots Ecosystems via Proposal Refinement</strong><br><button class=copy-to-clipboard title="R2SNet: Scalable Domain Adaptation for Object Detection in Cloud-Based Robots Ecosystems via Proposal Refinement" index=197>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-197 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Object Detection, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11567v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11567v1.pdf filename=2403.11567v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce a novel approach for scalable <b>domain</b> <b>adaptation</b> in cloud robotics scenarios where robots rely on third-party AI inference services powered by large pre-trained deep neural networks. Our method is based on a downstream proposal-refinement stage running locally on the robots, exploiting a new lightweight DNN architecture, R2SNet. This architecture aims to mitigate performance degradation from <b>domain</b> <b>shifts</b> by adapting the <b>object</b> <b>detection</b> process to the target environment, focusing on relabeling, rescoring, and suppression of bounding-box proposals. Our method allows for local execution on robots, addressing the scalability challenges of <b>domain</b> <b>adaptation</b> without incurring significant computational costs. Real-world results on mobile service robots performing door detection show the effectiveness of the proposed method in achieving scalable <b>domain</b> <b>adaptation.</b></p></p class="citation"></blockquote><h3 id=2332--198350-on-the-benefits-of-gpu-sample-based-stochastic-predictive-controllers-for-legged-locomotion-giulio-turrisi-et-al-2024>(23/32 | 198/350) On the Benefits of GPU Sample-Based Stochastic Predictive Controllers for Legged Locomotion (Giulio Turrisi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Giulio Turrisi, Valerio Modugno, Lorenzo Amatucci, Dimitrios Kanoulas, Claudio Semini. (2024)<br><strong>On the Benefits of GPU Sample-Based Stochastic Predictive Controllers for Legged Locomotion</strong><br><button class=copy-to-clipboard title="On the Benefits of GPU Sample-Based Stochastic Predictive Controllers for Legged Locomotion" index=198>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-198 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11383v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11383v1.pdf filename=2403.11383v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Quadrupedal robots excel in mobility, navigating complex terrains with agility. However, their complex control systems present challenges that are still far from being fully addressed. In this paper, we introduce the use of Sample-Based Stochastic control strategies for quadrupedal robots, as an alternative to traditional optimal control laws. We show that Sample-Based Stochastic methods, supported by GPU acceleration, can be effectively applied to real quadruped robots. In particular, in this work, we focus on achieving gait frequency adaptation, a notable challenge in quadrupedal locomotion for gradient-based methods. To validate the effectiveness of Sample-Based Stochastic controllers we test two distinct approaches for quadrupedal robots and compare them against a conventional gradient-based Model Predictive Control system. Our findings, validated both in <b>simulation</b> and on a real 21Kg Aliengo quadruped, demonstrate that our method is on par with a traditional Model Predictive Control strategy when the robot is subject to zero or moderate disturbance, while it surpasses gradient-based methods in handling sustained external disturbances, thanks to the straightforward gait adaptation strategy that is possible to achieve within their formulation.</p></p class="citation"></blockquote><h3 id=2432--199350-scenesense-diffusion-models-for-3d-occupancy-synthesis-from-partial-observation-alec-reed-et-al-2024>(24/32 | 199/350) SceneSense: Diffusion Models for 3D Occupancy Synthesis from Partial Observation (Alec Reed et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alec Reed, Brendan Crowe, Doncey Albin, Lorin Achey, Bradley Hayes, Christoffer Heckman. (2024)<br><strong>SceneSense: Diffusion Models for 3D Occupancy Synthesis from Partial Observation</strong><br><button class=copy-to-clipboard title="SceneSense: Diffusion Models for 3D Occupancy Synthesis from Partial Observation" index=199>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-199 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 15<br>Keywords: Diffusion Model, Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11985v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11985v1.pdf filename=2403.11985v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>When exploring new areas, robotic systems generally exclusively plan and execute controls over <b>geometry</b> that has been directly measured. When entering space that was previously obstructed from view such as turning corners in hallways or entering new rooms, robots often pause to plan over the newly observed space. To address this we present SceneScene, a real-time 3D <b>diffusion</b> <b>model</b> for synthesizing 3D occupancy information from partial observations that effectively predicts these occluded or out of view geometries for use in future planning and control frameworks. SceneSense uses a running occupancy map and a single RGB-D camera to generate predicted <b>geometry</b> around the platform at runtime, even when the <b>geometry</b> is occluded or out of view. Our architecture ensures that SceneSense never overwrites observed free or occupied space. By preserving the integrity of the observed map, SceneSense mitigates the risk of corrupting the observed space with generative predictions. While SceneSense is shown to operate well using a single RGB-D camera, the framework is flexible enough to extend to additional modalities. SceneSense operates as part of any system that generates a running occupancy map `out of the box&rsquo;, removing conditioning from the framework. Alternatively, for maximum performance in new modalities, the perception backbone can be replaced and the model retrained for inference in new applications. Unlike existing models that necessitate multiple views and offline scene synthesis, or are focused on filling gaps in observed data, our findings demonstrate that SceneSense is an effective approach to estimating unobserved local occupancy information at runtime. Local occupancy predictions from SceneSense are shown to better represent the ground truth occupancy distribution during the test exploration trajectories than the running occupancy map.</p></p class="citation"></blockquote><h3 id=2532--200350-agrnav-efficient-and-energy-saving-autonomous-navigation-for-air-ground-robots-in-occlusion-prone-environments-junming-wang-et-al-2024>(25/32 | 200/350) AGRNav: Efficient and Energy-Saving Autonomous Navigation for Air-Ground Robots in Occlusion-Prone Environments (Junming Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junming Wang, Zekai Sun, Xiuxian Guan, Tianxiang Shen, Zongyuan Zhang, Tianyang Duan, Dong Huang, Shixiong Zhao, Heming Cui. (2024)<br><strong>AGRNav: Efficient and Energy-Saving Autonomous Navigation for Air-Ground Robots in Occlusion-Prone Environments</strong><br><button class=copy-to-clipboard title="AGRNav: Efficient and Energy-Saving Autonomous Navigation for Air-Ground Robots in Occlusion-Prone Environments" index=200>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-200 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 13<br>Keywords: Benchmarking, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11607v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11607v1.pdf filename=2403.11607v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The exceptional mobility and long endurance of air-ground robots are raising interest in their usage to navigate complex environments (e.g., forests and large buildings). However, such environments often contain occluded and unknown regions, and without accurate prediction of unobserved obstacles, the movement of the air-ground robot often suffers a suboptimal trajectory under existing mapping-based and learning-based navigation methods. In this work, we present AGRNav, a novel framework designed to search for safe and energy-saving air-ground hybrid paths. AGRNav contains a lightweight semantic scene completion network (SCONet) with <b>self-attention</b> to enable accurate obstacle predictions by capturing contextual information and occlusion area features. The framework subsequently employs a query-based method for low-latency updates of prediction results to the grid map. Finally, based on the updated map, the hierarchical path planner efficiently searches for energy-saving paths for navigation. We validate AGRNav&rsquo;s performance through <b>benchmarks</b> in both simulated and real-world environments, demonstrating its superiority over classical and state-of-the-art methods. The open-source code is available at <a href=https://github.com/jmwang0117/AGRNav>https://github.com/jmwang0117/AGRNav</a>.</p></p class="citation"></blockquote><h3 id=2632--201350-reachability-based-trajectory-design-via-exact-formulation-of-implicit-neural-signed-distance-functions-jonathan-michaux-et-al-2024>(26/32 | 201/350) Reachability-based Trajectory Design via Exact Formulation of Implicit Neural Signed Distance Functions (Jonathan Michaux et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jonathan Michaux, Qingyi Chen, Challen Enninful Adu, Jinsun Liu, Ram Vasudevan. (2024)<br><strong>Reachability-based Trajectory Design via Exact Formulation of Implicit Neural Signed Distance Functions</strong><br><button class=copy-to-clipboard title="Reachability-based Trajectory Design via Exact Formulation of Implicit Neural Signed Distance Functions" index=201>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-201 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Continuous Time, Continuous Time<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12280v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12280v1.pdf filename=2403.12280v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Generating receding-horizon motion trajectories for autonomous vehicles in real-time while also providing safety guarantees is challenging. This is because a future trajectory needs to be planned before the previously computed trajectory is completely executed. This becomes even more difficult if the trajectory is required to satisfy <b>continuous-time</b> <b>collision-avoidance</b> constraints while accounting for a large number of obstacles. To address these challenges, this paper proposes a novel real-time, receding-horizon motion planning algorithm named REachability-based trajectory Design via Exact Formulation of Implicit NEural signed Distance functions (REDEFINED). REDEFINED first applies offline reachability analysis to compute zonotope-based reachable sets that overapproximate the motion of the ego vehicle. During online planning, REDEFINED leverages zonotope arithmetic to construct a neural implicit representation that computes the exact signed distance between a parameterized swept volume of the ego vehicle and obstacle vehicles. REDEFINED then implements a novel, real-time optimization framework that utilizes the neural network to construct a collision avoidance constraint. REDEFINED is compared to a variety of state-of-the-art techniques and is demonstrated to successfully enable the vehicle to safely navigate through complex environments. Code, data, and video demonstrations can be found at <a href=https://roahmlab.github.io/redefined/>https://roahmlab.github.io/redefined/</a>.</p></p class="citation"></blockquote><h3 id=2732--202350-safety-implications-of-explainable-artificial-intelligence-in-end-to-end-autonomous-driving-shahin-atakishiyev-et-al-2024>(27/32 | 202/350) Safety Implications of Explainable Artificial Intelligence in End-to-End Autonomous Driving (Shahin Atakishiyev et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shahin Atakishiyev, Mohammad Salameh, Randy Goebel. (2024)<br><strong>Safety Implications of Explainable Artificial Intelligence in End-to-End Autonomous Driving</strong><br><button class=copy-to-clipboard title="Safety Implications of Explainable Artificial Intelligence in End-to-End Autonomous Driving" index=202>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-202 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Explainable AI<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12176v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12176v1.pdf filename=2403.12176v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The end-to-end learning pipeline is gradually creating a paradigm shift in the ongoing development of highly autonomous vehicles, largely due to advances in deep learning, the availability of large-scale training datasets, and improvements in integrated sensor devices. However, a lack of interpretability in real-time decisions with contemporary learning methods impedes user trust and attenuates the widespread deployment and commercialization of such vehicles. Moreover, the issue is exacerbated when these cars are involved in or cause traffic accidents. Such drawback raises serious safety concerns from societal and legal perspectives. Consequently, explainability in end-to-end autonomous driving is essential to enable the safety of vehicular automation. However, the safety and explainability aspects of autonomous driving have generally been investigated disjointly by researchers in today&rsquo;s state of the art. In this paper, we aim to bridge the gaps between these topics and seek to answer the following research question: When and how can explanations improve safety of autonomous driving? In this regard, we first revisit established safety and state-of-the-art explainability techniques in autonomous driving. Furthermore, we present three critical case studies and show the pivotal role of explanations in enhancing self-driving safety. Finally, we describe our empirical investigation and reveal potential value, limitations, and caveats with practical <b>explainable</b> <b>AI</b> methods on their role of assuring safety and transparency for vehicle autonomy.</p></p class="citation"></blockquote><h3 id=2832--203350-ergonomic-optimization-in-worker-robot-bimanual-object-handover-implementing-reba-using-reinforcement-learning-in-virtual-reality-mani-amani-et-al-2024>(28/32 | 203/350) Ergonomic Optimization in Worker-Robot Bimanual Object Handover: Implementing REBA Using Reinforcement Learning in Virtual Reality (Mani Amani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mani Amani, Reza Akhavian. (2024)<br><strong>Ergonomic Optimization in Worker-Robot Bimanual Object Handover: Implementing REBA Using Reinforcement Learning in Virtual Reality</strong><br><button class=copy-to-clipboard title="Ergonomic Optimization in Worker-Robot Bimanual Object Handover: Implementing REBA Using Reinforcement Learning in Virtual Reality" index=203>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-203 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12149v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12149v1.pdf filename=2403.12149v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Robots can serve as safety catalysts on construction job sites by taking over hazardous and repetitive tasks while alleviating the risks associated with existing manual workflows. Research on the safety of physical human-robot interaction (pHRI) is traditionally focused on addressing the risks associated with potential collisions. However, it is equally important to ensure that the workflows involving a collaborative robot are inherently safe, even though they may not result in an accident. For example, pHRI may require the human counterpart to use non-ergonomic body postures to conform to the robot hardware and physical configurations. Frequent and long-term exposure to such situations may result in chronic health issues. Safety and ergonomics assessment measures can be understood by robots if they are presented in algorithmic fashions so optimization for body postures is attainable. While frameworks such as Rapid Entire Body Assessment (REBA) have been an industry standard for many decades, they lack a rigorous mathematical structure which poses challenges in using them immediately for pHRI safety optimization purposes. Furthermore, learnable approaches have limited robustness outside of their training data, reducing generalizability. In this paper, we propose a novel framework that approaches optimization through <b>Reinforcement</b> <b>Learning,</b> ensuring precise, online ergonomic scores as compared to approximations, while being able to generalize and tune the regiment to any human and any task. To ensure practicality, the training is done in virtual reality utilizing Inverse Kinematics to simulate human movement mechanics. Experimental findings are compared to ergonomically naive object handover heuristics and indicate promising results where the developed framework can find the optimal object handover coordinates in pHRI contexts for manual material handling exemplary situations.</p></p class="citation"></blockquote><h3 id=2932--204350-mcd-diverse-large-scale-multi-campus-dataset-for-robot-perception-thien-minh-nguyen-et-al-2024>(29/32 | 204/350) MCD: Diverse Large-Scale Multi-Campus Dataset for Robot Perception (Thien-Minh Nguyen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Thien-Minh Nguyen, Shenghai Yuan, Thien Hoang Nguyen, Pengyu Yin, Haozhi Cao, Lihua Xie, Maciej Wozniak, Patric Jensfelt, Marko Thiel, Justin Ziegenbein, Noel Blunder. (2024)<br><strong>MCD: Diverse Large-Scale Multi-Campus Dataset for Robot Perception</strong><br><button class=copy-to-clipboard title="MCD: Diverse Large-Scale Multi-Campus Dataset for Robot Perception" index=204>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-204 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Continuous Time, Continuous Time<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11496v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11496v1.pdf filename=2403.11496v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Perception plays a crucial role in various robot applications. However, existing well-annotated datasets are biased towards autonomous driving scenarios, while unlabelled SLAM datasets are quickly over-fitted, and often lack environment and domain variations. To expand the frontier of these fields, we introduce a comprehensive dataset named MCD (Multi-Campus Dataset), featuring a wide range of sensing modalities, high-accuracy ground truth, and diverse challenging environments across three Eurasian university campuses. MCD comprises both CCS (Classical Cylindrical Spinning) and NRE (Non-Repetitive Epicyclic) lidars, high-quality IMUs (Inertial Measurement Units), cameras, and UWB (Ultra-WideBand) sensors. Furthermore, in a pioneering effort, we introduce semantic annotations of 29 classes over 59k sparse NRE lidar scans across three domains, thus providing a novel challenge to existing semantic segmentation research upon this largely unexplored lidar modality. Finally, we propose, for the first time to the best of our knowledge, <b>continuous-time</b> <b>ground</b> truth based on optimization-based registration of lidar-inertial data on large survey-grade prior maps, which are also publicly released, each several times the size of existing ones. We conduct a rigorous evaluation of numerous state-of-the-art algorithms on MCD, report their performance, and highlight the challenges awaiting solutions from the research community.</p></p class="citation"></blockquote><h3 id=3032--205350-vihe-virtual-in-hand-eye-transformer-for-3d-robotic-manipulation-weiyao-wang-et-al-2024>(30/32 | 205/350) VIHE: Virtual In-Hand Eye Transformer for 3D Robotic Manipulation (Weiyao Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Weiyao Wang, Yutian Lei, Shiyu Jin, Gregory D. Hager, Liangjun Zhang. (2024)<br><strong>VIHE: Virtual In-Hand Eye Transformer for 3D Robotic Manipulation</strong><br><button class=copy-to-clipboard title="VIHE: Virtual In-Hand Eye Transformer for 3D Robotic Manipulation" index=205>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-205 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11461v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11461v2.pdf filename=2403.11461v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we introduce the Virtual In-Hand Eye <b>Transformer</b> (VIHE), a novel method designed to enhance 3D manipulation capabilities through action-aware view rendering. VIHE autoregressively refines actions in multiple stages by conditioning on rendered views posed from action predictions in the earlier stages. These virtual in-hand views provide a strong inductive bias for effectively recognizing the correct pose for the hand, especially for challenging high-precision tasks such as peg insertion. On 18 manipulation tasks in RLBench simulated environments, VIHE achieves a new state-of-the-art, with a 12% absolute improvement, increasing from 65% to 77% over the existing state-of-the-art model using 100 demonstrations per task. In real-world scenarios, VIHE can learn manipulation tasks with just a handful of demonstrations, highlighting its practical utility. Videos and code implementation can be found at our project site: <a href=https://vihe-3d.github.io>https://vihe-3d.github.io</a>.</p></p class="citation"></blockquote><h3 id=3132--206350-smt-based-dynamic-multi-robot-task-allocation-victoria-marie-tuck-et-al-2024>(31/32 | 206/350) SMT-Based Dynamic Multi-Robot Task Allocation (Victoria Marie Tuck et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Victoria Marie Tuck, Pei-Wei Chen, Georgios Fainekos, Bardh Hoxha, Hideki Okamoto, S. Shankar Sastry, Sanjit A. Seshia. (2024)<br><strong>SMT-Based Dynamic Multi-Robot Task Allocation</strong><br><button class=copy-to-clipboard title="SMT-Based Dynamic Multi-Robot Task Allocation" index=206>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-206 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs-SY, cs.RO, eess-SY<br>Keyword Score: 6<br>Keywords: Graph, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11737v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11737v1.pdf filename=2403.11737v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multi-Robot Task Allocation (MRTA) is a problem that arises in many application domains including package delivery, warehouse robotics, and healthcare. In this work, we consider the problem of MRTA for a dynamic stream of tasks with task deadlines and capacitated agents (capacity for more than one simultaneous task). Previous work commonly focuses on the static case, uses specialized algorithms for restrictive task specifications, or lacks guarantees. We propose an approach to Dynamic MRTA for capacitated robots that is based on Satisfiability Modulo Theories (SMT) solving and addresses these concerns. We show our approach is both sound and complete, and that the SMT encoding is general, enabling extension to a broader class of task specifications. We show how to leverage the incremental solving capabilities of SMT solvers, keeping learned information when allocating new tasks arriving online, and to solve non-incrementally, which we provide runtime comparisons of. Additionally, we provide an algorithm to start with a smaller but potentially incomplete encoding that can iteratively be adjusted to the complete encoding. We evaluate our method on a parameterized set of <b>benchmarks</b> encoding multi-robot delivery created from a <b>graph</b> abstraction of a hospital-like environment. The effectiveness of our approach is demonstrated using a range of encodings, including quantifier-free theories of uninterpreted functions and linear or bitvector arithmetic across multiple solvers.</p></p class="citation"></blockquote><h3 id=3232--207350-masstar-a-multi-modal-and-large-scale-scene-dataset-with-a-versatile-toolchain-for-surface-prediction-and-completion-guiyong-zheng-et-al-2024>(32/32 | 207/350) MASSTAR: A Multi-Modal and Large-Scale Scene Dataset with a Versatile Toolchain for Surface Prediction and Completion (Guiyong Zheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guiyong Zheng, Jinqi Jiang, Chen Feng, Shaojie Shen, Boyu Zhou. (2024)<br><strong>MASSTAR: A Multi-Modal and Large-Scale Scene Dataset with a Versatile Toolchain for Surface Prediction and Completion</strong><br><button class=copy-to-clipboard title="MASSTAR: A Multi-Modal and Large-Scale Scene Dataset with a Versatile Toolchain for Surface Prediction and Completion" index=207>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-207 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-CV, cs-RO, cs.RO<br>Keyword Score: 6<br>Keywords: Benchmarking, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11681v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11681v1.pdf filename=2403.11681v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Surface prediction and completion have been widely studied in various applications. Recently, research in surface completion has evolved from small objects to complex large-scale scenes. As a result, researchers have begun increasing the volume of data and leveraging a greater variety of data modalities including rendered RGB images, descriptive texts, depth images, etc, to enhance algorithm performance. However, existing datasets suffer from a deficiency in the amounts of scene-level models along with the corresponding <b>multi-modal</b> information. Therefore, a method to scale the datasets and generate <b>multi-modal</b> information in them efficiently is essential. To bridge this research gap, we propose MASSTAR: a <b>Multi-modal</b> lArge-scale Scene dataset with a verSatile Toolchain for surfAce pRediction and completion. We develop a versatile and efficient toolchain for processing the raw 3D data from the environments. It screens out a set of fine-grained scene models and generates the corresponding <b>multi-modal</b> data. Utilizing the toolchain, we then generate an example dataset composed of over a thousand scene-level models with partial real-world data added. We compare MASSTAR with the existing datasets, which validates its superiority: the ability to efficiently extract high-quality models from complex scenarios to expand the dataset. Additionally, several representative surface completion algorithms are <b>benchmarked</b> on MASSTAR, which reveals that existing algorithms can hardly deal with scene-level completion. We will release the source code of our toolchain and the dataset. For more details, please see our project page at <a href=https://sysu-star.github.io/MASSTAR>https://sysu-star.github.io/MASSTAR</a>.</p></p class="citation"></blockquote><h2 id=cslg-50>cs.LG (50)</h2><h3 id=150--208350-graph-partial-label-learning-with-potential-cause-discovering-hang-gao-et-al-2024>(1/50 | 208/350) Graph Partial Label Learning with Potential Cause Discovering (Hang Gao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hang Gao, Jiaguo Yuan, Jiangmeng Li, Chengyu Yao, Fengge Wu, Junsuo Zhao, Changwen Zheng. (2024)<br><strong>Graph Partial Label Learning with Potential Cause Discovering</strong><br><button class=copy-to-clipboard title="Graph Partial Label Learning with Potential Cause Discovering" index=208>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-208 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 78<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Representation Learning, Supervised Learning, Supervised Learning, Weakly-supervised Learning, Weakly-supervised Learning, Weakly Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11449v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11449v1.pdf filename=2403.11449v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> have gained considerable attention for their potential in addressing challenges posed by complex <b>graph-structured</b> <b>data</b> <b>in</b> diverse domains. However, accurately annotating <b>graph</b> <b>data</b> <b>for</b> training is difficult due to the inherent complexity and interconnectedness of <b>graphs.</b> <b>To</b> <b>tackle</b> this issue, we propose a novel <b>graph</b> <b>representation</b> <b>learning</b> method that enables <b>GNN</b> models to effectively learn discriminative information even in the presence of noisy labels within the context of Partially Labeled Learning (PLL). PLL is a critical <b>weakly</b> <b>supervised</b> <b>learning</b> problem, where each training instance is associated with a set of candidate labels, including both the true label and additional noisy labels. Our approach leverages potential cause extraction to obtain <b>graph</b> <b>data</b> <b>that</b> exhibit a higher likelihood of possessing a causal relationship with the labels. By incorporating auxiliary training based on the extracted <b>graph</b> <b>data,</b> <b>our</b> model can effectively filter out the noise contained in the labels. We support the rationale behind our approach with a series of theoretical analyses. Moreover, we conduct extensive evaluations and ablation studies on multiple datasets, demonstrating the superiority of our proposed method.</p></p class="citation"></blockquote><h3 id=250--209350-improving-lora-in-privacy-preserving-federated-learning-youbang-sun-et-al-2024>(2/50 | 209/350) Improving LoRA in Privacy-preserving Federated Learning (Youbang Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Youbang Sun, Zitao Li, Yaliang Li, Bolin Ding. (2024)<br><strong>Improving LoRA in Privacy-preserving Federated Learning</strong><br><button class=copy-to-clipboard title="Improving LoRA in Privacy-preserving Federated Learning" index=209>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-209 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-DC, cs-LG, cs.LG<br>Keyword Score: 60<br>Keywords: Federated Learning, Fine-tuning, Fine-tuning, Large Language Model, Pre-trained Language Model, Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12313v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12313v1.pdf filename=2403.12313v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Low-rank adaptation (LoRA) is one of the most popular task-specific parameter-efficient <b>fine-tuning</b> (PEFT) methods on <b>pre-trained</b> <b>language</b> <b>models</b> for its good performance and computational efficiency. LoRA injects a product of two trainable rank decomposition matrices over the top of each frozen <b>pre-trained</b> <b>model</b> <b>module.</b> However, when applied in the setting of privacy-preserving <b>federated</b> <b>learning</b> (FL), LoRA may become unstable due to the following facts: 1) the effects of data heterogeneity and multi-step local updates are non-negligible, 2) additive noise enforced on updating gradients to guarantee <b>differential</b> <b>privacy</b> (DP) can be amplified and 3) the final performance is susceptible to hyper-parameters. A key factor leading to these phenomena is the discordance between jointly optimizing the two low-rank matrices by local clients and separately aggregating them by the central server. Thus, this paper proposes an efficient and effective version of LoRA, <b>Federated</b> <b>Freeze</b> A LoRA (FFA-LoRA), to alleviate these challenges and further halve the communication cost of <b>federated</b> <b>fine-tuning</b> <b>LLMs.</b> The core idea of FFA-LoRA is to fix the randomly initialized non-zero matrices and only <b>fine-tune</b> the zero-initialized matrices. Compared to LoRA, FFA-LoRA is motivated by practical and theoretical benefits in privacy-preserved FL. Our experiments demonstrate that FFA-LoRA provides more consistent performance with better computational efficiency over vanilla LoRA in various FL tasks.</p></p class="citation"></blockquote><h3 id=350--210350-efficient-transformer-based-hyper-parameter-optimization-for-resource-constrained-iot-environments-ibrahim-shaer-et-al-2024>(3/50 | 210/350) Efficient Transformer-based Hyper-parameter Optimization for Resource-constrained IoT Environments (Ibrahim Shaer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ibrahim Shaer, Soodeh Nikan, Abdallah Shami. (2024)<br><strong>Efficient Transformer-based Hyper-parameter Optimization for Resource-constrained IoT Environments</strong><br><button class=copy-to-clipboard title="Efficient Transformer-based Hyper-parameter Optimization for Resource-constrained IoT Environments" index=210>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-210 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 60<br>Keywords: MNIST, Convolution, Convolutional Neural Network, Convolutional Neural Network, Reinforcement Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12237v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12237v1.pdf filename=2403.12237v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The hyper-parameter optimization (HPO) process is imperative for finding the best-performing <b>Convolutional</b> <b>Neural</b> <b>Networks</b> <b>(CNNs).</b> The automation process of HPO is characterized by its sizable computational footprint and its lack of transparency; both important factors in a resource-constrained Internet of Things (IoT) environment. In this paper, we address these problems by proposing a novel approach that combines <b>transformer</b> architecture and actor-critic <b>Reinforcement</b> <b>Learning</b> (RL) model, TRL-HPO, equipped with multi-headed attention that enables parallelization and progressive generation of layers. These assumptions are founded empirically by evaluating TRL-HPO on the <b>MNIST</b> dataset and comparing it with state-of-the-art approaches that build <b>CNN</b> models from scratch. The results show that TRL-HPO outperforms the classification results of these approaches by 6.8% within the same time frame, demonstrating the efficiency of TRL-HPO for the HPO process. The analysis of the results identifies the main culprit for performance degradation attributed to stacking fully connected layers. This paper identifies new avenues for improving RL-based HPO processes in resource-constrained environments.</p></p class="citation"></blockquote><h3 id=450--211350-narrative-feature-or-structured-feature-a-study-of-large-language-models-to-identify-cancer-patients-at-risk-of-heart-failure-ziyi-chen-et-al-2024>(4/50 | 211/350) Narrative Feature or Structured Feature? A Study of Large Language Models to Identify Cancer Patients at Risk of Heart Failure (Ziyi Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ziyi Chen, Mengyuan Zhang, Mustafa Mohammed Ahmed, Yi Guo, Thomas J. George, Jiang Bian, Yonghui Wu. (2024)<br><strong>Narrative Feature or Structured Feature? A Study of Large Language Models to Identify Cancer Patients at Risk of Heart Failure</strong><br><button class=copy-to-clipboard title="Narrative Feature or Structured Feature? A Study of Large Language Models to Identify Cancer Patients at Risk of Heart Failure" index=211>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-211 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG<br>Keyword Score: 60<br>Keywords: BERT, LSTM, LSTM, Transformer, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11425v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11425v1.pdf filename=2403.11425v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Cancer treatments are known to introduce cardiotoxicity, negatively impacting outcomes and survivorship. Identifying cancer patients at risk of heart failure (HF) is critical to improving cancer treatment outcomes and safety. This study examined machine learning (ML) models to identify cancer patients at risk of HF using electronic health records (EHRs), including traditional ML, Time-Aware <b>long</b> <b>short-term</b> <b>memory</b> <b>(T-LSTM),</b> and <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> using novel narrative features derived from the structured medical codes. We identified a cancer cohort of 12,806 patients from the University of Florida Health, diagnosed with lung, breast, and colorectal cancers, among which 1,602 individuals developed HF after cancer. The <b>LLM,</b> GatorTron-3.9B, achieved the best F1 scores, outperforming the traditional support vector machines by 39%, the T-LSTM deep learning model by 7%, and a widely used <b>transformer</b> model, <b>BERT,</b> by 5.6%. The analysis shows that the proposed narrative features remarkably increased feature density and improved performance.</p></p class="citation"></blockquote><h3 id=550--212350-learning-useful-representations-of-recurrent-neural-network-weight-matrices-vincent-herrmann-et-al-2024>(5/50 | 212/350) Learning Useful Representations of Recurrent Neural Network Weight Matrices (Vincent Herrmann et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vincent Herrmann, Francesco Faccio, Jürgen Schmidhuber. (2024)<br><strong>Learning Useful Representations of Recurrent Neural Network Weight Matrices</strong><br><button class=copy-to-clipboard title="Learning Useful Representations of Recurrent Neural Network Weight Matrices" index=212>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-212 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: I-2-6, cs-LG, cs.LG<br>Keyword Score: 55<br>Keywords: MNIST, Representation Learning, Self-supervised Learning, Self-supervised Learning, Recurrent Neural Network, Recurrent Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11998v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11998v1.pdf filename=2403.11998v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Recurrent</b> <b>Neural</b> <b>Networks</b> <b>(RNNs)</b> are general-purpose parallel-sequential computers. The program of an <b>RNN</b> is its weight matrix. How to learn useful <b>representations</b> <b>of</b> <b>RNN</b> weights that facilitate <b>RNN</b> analysis as well as downstream tasks? While the mechanistic approach directly looks at some <b>RNN&rsquo;s</b> weights to predict its behavior, the functionalist approach analyzes its overall functionality &ndash; specifically, its input-output mapping. We consider several mechanistic approaches for <b>RNN</b> weights and adapt the permutation equivariant Deep Weight Space layer for <b>RNNs.</b> Our two novel functionalist approaches extract information from <b>RNN</b> weights by &lsquo;interrogating&rsquo; the <b>RNN</b> through probing inputs. We develop a theoretical framework that demonstrates conditions under which the functionalist approach can generate rich <b>representations</b> <b>that</b> help determine <b>RNN</b> behavior. We create and release the first two &lsquo;model zoo&rsquo; datasets for <b>RNN</b> weight <b>representation</b> <b>learning.</b> One consists of generative models of a class of formal languages, and the other one of classifiers of sequentially processed <b>MNIST</b> digits. With the help of an emulation-based <b>self-supervised</b> <b>learning</b> technique we compare and evaluate the different <b>RNN</b> weight encoding techniques on multiple downstream applications. On the most challenging one, namely predicting which exact task the <b>RNN</b> was trained on, functionalist approaches show clear superiority.</p></p class="citation"></blockquote><h3 id=650--213350-investigating-the-benefits-of-projection-head-for-representation-learning-yihao-xue-et-al-2024>(6/50 | 213/350) Investigating the Benefits of Projection Head for Representation Learning (Yihao Xue et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yihao Xue, Eric Gan, Jiayi Ni, Siddharth Joshi, Baharan Mirzasoleiman. (2024)<br><strong>Investigating the Benefits of Projection Head for Representation Learning</strong><br><button class=copy-to-clipboard title="Investigating the Benefits of Projection Head for Representation Learning" index=213>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-213 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 55<br>Keywords: Contrastive Learning, Data Augmentation, Representation Learning, Self-supervised Learning, Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11391v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11391v1.pdf filename=2403.11391v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>An effective technique for obtaining high-quality <b>representations</b> <b>is</b> adding a projection head on top of the encoder during training, then discarding it and using the pre-projection <b>representations.</b> <b>Despite</b> its proven practical effectiveness, the reason behind the success of this technique is poorly understood. The pre-projection <b>representations</b> <b>are</b> not directly optimized by the loss function, raising the question: what makes them better? In this work, we provide a rigorous theoretical answer to this question. We start by examining linear models trained with <b>self-supervised</b> <b>contrastive</b> <b>loss.</b> We reveal that the implicit bias of training algorithms leads to layer-wise progressive feature weighting, where features become increasingly unequal as we go deeper into the layers. Consequently, lower layers tend to have more normalized and less specialized <b>representations.</b> <b>We</b> theoretically characterize scenarios where such <b>representations</b> <b>are</b> more beneficial, highlighting the intricate interplay between <b>data</b> <b>augmentation</b> and input features. Additionally, we demonstrate that introducing non-linearity into the network allows lower layers to learn features that are completely absent in higher layers. Finally, we show how this mechanism improves the robustness in <b>supervised</b> <b>contrastive</b> <b>learning</b> and <b>supervised</b> <b>learning.</b> We empirically validate our results through various experiments on CIFAR-10/100, UrbanCars and shifted versions of ImageNet. We also introduce a potential alternative to projection head, which offers a more interpretable and controllable design.</p></p class="citation"></blockquote><h3 id=750--214350-supervised-fine-tuning-as-inverse-reinforcement-learning-hao-sun-2024>(7/50 | 214/350) Supervised Fine-Tuning as Inverse Reinforcement Learning (Hao Sun, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hao Sun. (2024)<br><strong>Supervised Fine-Tuning as Inverse Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Supervised Fine-Tuning as Inverse Reinforcement Learning" index=214>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-214 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 50<br>Keywords: Fine-tuning, Reinforcement Learning, Supervised Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12017v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12017v1.pdf filename=2403.12017v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The prevailing approach to aligning <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> typically relies on human or AI feedback and assumes access to specific types of preference datasets. In our work, we question the efficacy of such datasets and explore various scenarios where alignment with expert demonstrations proves more realistic. We build a sequential decision-making framework to formulate the problem of aligning <b>LLMs</b> using demonstration datasets. Drawing insights from inverse <b>reinforcement</b> <b>learning</b> and imitation learning, we introduce various approaches for divergence minimization in the <b>LLM</b> alignment tasks. Our analysis highlights the mass-covering and mode-seeking behaviors of these different approaches. Inclusively, we examine the pros and cons of the classical <b>supervised</b> <b>fine-tuning</b> method, elaborating on scenarios where different methods shine.</p></p class="citation"></blockquote><h3 id=850--215350-transfer-learning-beyond-bounded-density-ratios-alkis-kalavasis-et-al-2024>(8/50 | 215/350) Transfer Learning Beyond Bounded Density Ratios (Alkis Kalavasis et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alkis Kalavasis, Ilias Zadik, Manolis Zampetakis. (2024)<br><strong>Transfer Learning Beyond Bounded Density Ratios</strong><br><button class=copy-to-clipboard title="Transfer Learning Beyond Bounded Density Ratios" index=215>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-215 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-DS, cs-LG, cs.LG, math-ST, stat-ML, stat-TH<br>Keyword Score: 50<br>Keywords: Out-of-distribution, Transfer Learning, Transformer, In-context Learning, In-context Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11963v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11963v1.pdf filename=2403.11963v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the fundamental problem of <b>transfer</b> <b>learning</b> where a learning algorithm collects data from some source distribution $P$ but needs to perform well with respect to a different target distribution $Q$. A standard change of measure argument implies that <b>transfer</b> <b>learning</b> happens when the density ratio $dQ/dP$ is bounded. Yet, prior thought-provoking works by Kpotufe and Martinet (COLT, 2018) and Hanneke and Kpotufe (NeurIPS, 2019) demonstrate cases where the ratio $dQ/dP$ is unbounded, but <b>transfer</b> <b>learning</b> is possible. In this work, we focus on <b>transfer</b> <b>learning</b> over the class of low-degree polynomial estimators. Our main result is a general <b>transfer</b> <b>inequality</b> over the domain $\mathbb{R}^n$, proving that non-trivial <b>transfer</b> <b>learning</b> for low-degree polynomials is possible under very mild assumptions, going well beyond the classical assumption that $dQ/dP$ is bounded. For instance, it always applies if $Q$ is a log-concave measure and the inverse ratio $dP/dQ$ is bounded. To demonstrate the applicability of our inequality, we obtain new results in the settings of: (1) the classical truncated regression setting, where $dQ/dP$ equals infinity, and (2) the more recent <b>out-of-distribution</b> generalization setting for <b>in-context</b> <b>learning</b> linear functions with <b>transformers.</b> We also provide a discrete analogue of our <b>transfer</b> <b>inequality</b> on the Boolean Hypercube ${-1,1}^n$, and study its connections with the recent problem of Generalization on the Unseen of Abbe, Bengio, Lotfi and Rizk (ICML, 2023). Our main conceptual contribution is that the maximum influence of the error of the estimator $\widehat{f}-f^<em>$ under $Q$, $\mathrm{I}_{\max}(\widehat{f}-f^</em>)$, acts as a sufficient condition for transferability; when $\mathrm{I}_{\max}(\widehat{f}-f^*)$ is appropriately bounded, <b>transfer</b> <b>is</b> possible over the Boolean domain.</p></p class="citation"></blockquote><h3 id=950--216350-knfu-effective-knowledge-fusion-s-jamal-seyedmohammadi-et-al-2024>(9/50 | 216/350) KnFu: Effective Knowledge Fusion (S. Jamal Seyedmohammadi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>S. Jamal Seyedmohammadi, S. Kawa Atapour, Jamshid Abouei, Arash Mohammadi. (2024)<br><strong>KnFu: Effective Knowledge Fusion</strong><br><button class=copy-to-clipboard title="KnFu: Effective Knowledge Fusion" index=216>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-216 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-DC, cs-LG, cs.LG<br>Keyword Score: 50<br>Keywords: MNIST, Federated Learning, Knowledge Distillation, Knowledge Distillation, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11892v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11892v1.pdf filename=2403.11892v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Federated</b> <b>Learning</b> (FL) has emerged as a prominent alternative to the traditional centralized learning approach. Generally speaking, FL is a decentralized approach that allows for collaborative training of Machine Learning (ML) models across multiple local nodes, ensuring data privacy and security while leveraging diverse datasets. Conventional FL, however, is susceptible to gradient inversion attacks, restrictively enforces a uniform architecture on local models, and suffers from model heterogeneity (model drift) due to non-IID local datasets. To mitigate some of these challenges, the new paradigm of <b>Federated</b> <b>Knowledge</b> <b>Distillation</b> (FKD) has emerged. FDK is developed based on the concept of <b>Knowledge</b> <b>Distillation</b> <b>(KD),</b> which involves extraction and transfer of a large and well-trained teacher model&rsquo;s <b>knowledge</b> <b>to</b> lightweight student models. FKD, however, still faces the model drift issue. Intuitively speaking, not all <b>knowledge</b> <b>is</b> universally beneficial due to the inherent diversity of data among local nodes. This calls for innovative mechanisms to evaluate the relevance and effectiveness of each client&rsquo;s <b>knowledge</b> <b>for</b> others, to prevent propagation of adverse <b>knowledge.</b> <b>In</b> this context, the paper proposes Effective <b>Knowledge</b> <b>Fusion</b> (KnFu) algorithm that evaluates <b>knowledge</b> <b>of</b> local models to only fuse semantic neighbors&rsquo; effective <b>knowledge</b> <b>for</b> each client. The KnFu is a personalized effective <b>knowledge</b> <b>fusion</b> scheme for each client, that analyzes effectiveness of different local models&rsquo; <b>knowledge</b> <b>prior</b> to the aggregation phase. Comprehensive experiments were performed on <b>MNIST</b> and CIFAR10 datasets illustrating effectiveness of the proposed KnFu in comparison to its state-of-the-art counterparts. A key conclusion of the work is that in scenarios with large and highly heterogeneous local datasets, local training could be preferable to <b>knowledge</b> <b>fusion-based</b> solutions.</p></p class="citation"></blockquote><h3 id=1050--217350-linguacodus-a-synergistic-framework-for-transformative-code-generation-in-machine-learning-pipelines-ekaterina-trofimova-et-al-2024>(10/50 | 217/350) Linguacodus: A Synergistic Framework for Transformative Code Generation in Machine Learning Pipelines (Ekaterina Trofimova et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ekaterina Trofimova, Emil Sataev, Andrey E. Ustyuzhanin. (2024)<br><strong>Linguacodus: A Synergistic Framework for Transformative Code Generation in Machine Learning Pipelines</strong><br><button class=copy-to-clipboard title="Linguacodus: A Synergistic Framework for Transformative Code Generation in Machine Learning Pipelines" index=217>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-217 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs-PL, cs-SE, cs.LG<br>Keyword Score: 50<br>Keywords: Fine-tuning, Fine-tuning, Code Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11585v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11585v1.pdf filename=2403.11585v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the ever-evolving landscape of machine learning, seamless translation of natural language descriptions into executable <b>code</b> <b>remains</b> a formidable challenge. This paper introduces Linguacodus, an innovative framework designed to tackle this challenge by deploying a dynamic pipeline that iteratively transforms natural language task descriptions into <b>code</b> <b>through</b> high-level data-shaping instructions. The core of Linguacodus is a <b>fine-tuned</b> <b>large</b> <b>language</b> <b>model</b> <b>(LLM),</b> empowered to evaluate diverse solutions for various problems and select the most fitting one for a given task. This paper details the <b>fine-tuning</b> process, and sheds light on how natural language descriptions can be translated into functional <b>code.</b> <b>Linguacodus</b> represents a substantial leap towards automated <b>code</b> <b>generation,</b> effectively bridging the gap between task descriptions and executable <b>code.</b> <b>It</b> holds great promise for advancing machine learning applications across diverse domains. Additionally, we propose an algorithm capable of transforming a natural description of an ML task into <b>code</b> <b>with</b> minimal human interaction. In extensive experiments on a vast machine learning <b>code</b> <b>dataset</b> originating from Kaggle, we showcase the effectiveness of Linguacodus. The investigations highlight its potential applications across diverse domains, emphasizing its impact on applied machine learning in various scientific fields.</p></p class="citation"></blockquote><h3 id=1150--218350-state-separated-sarsa-a-practical-sequential-decision-making-algorithm-with-recovering-rewards-yuto-tanimoto-et-al-2024>(11/50 | 218/350) State-Separated SARSA: A Practical Sequential Decision-Making Algorithm with Recovering Rewards (Yuto Tanimoto et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuto Tanimoto, Kenji Fukumizu. (2024)<br><strong>State-Separated SARSA: A Practical Sequential Decision-Making Algorithm with Recovering Rewards</strong><br><button class=copy-to-clipboard title="State-Separated SARSA: A Practical Sequential Decision-Making Algorithm with Recovering Rewards" index=218>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-218 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 50<br>Keywords: Bandit Algorithm, Bandit Algorithm, Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11520v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11520v1.pdf filename=2403.11520v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While many multi-armed <b>bandit</b> <b>algorithms</b> assume that rewards for all arms are constant across rounds, this assumption does not hold in many real-world scenarios. This paper considers the setting of recovering <b>bandits</b> <b>(Pike-Burke</b> & Grunewalder, 2019), where the reward depends on the number of rounds elapsed since the last time an arm was pulled. We propose a new <b>reinforcement</b> <b>learning</b> (RL) algorithm tailored to this setting, named the State-Separate SARSA (SS-SARSA) algorithm, which treats rounds as states. The SS-SARSA algorithm achieves efficient learning by reducing the number of state combinations required for Q-learning/SARSA, which often suffers from combinatorial issues for large-scale RL problems. Additionally, it makes minimal assumptions about the reward structure and offers lower computational complexity. Furthermore, we prove asymptotic convergence to an optimal policy under mild assumptions. <b>Simulation</b> studies demonstrate the superior performance of our algorithm across various settings.</p></p class="citation"></blockquote><h3 id=1250--219350-variational-sampling-of-temporal-trajectories-jurijs-nazarovs-et-al-2024>(12/50 | 219/350) Variational Sampling of Temporal Trajectories (Jurijs Nazarovs et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jurijs Nazarovs, Zhichun Huang, Xingjian Zhen, Sourav Pal, Rudrasis Chakraborty, Vikas Singh. (2024)<br><strong>Variational Sampling of Temporal Trajectories</strong><br><button class=copy-to-clipboard title="Variational Sampling of Temporal Trajectories" index=219>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-219 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 50<br>Keywords: Out-of-distribution, Reinforcement Learning, Simulation, Simulator, Recurrent Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11418v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11418v1.pdf filename=2403.11418v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A deterministic temporal process can be determined by its trajectory, an element in the product space of (a) initial condition $z_0 \in \mathcal{Z}$ and (b) transition function $f: (\mathcal{Z}, \mathcal{T}) \to \mathcal{Z}$ often influenced by the control of the underlying dynamical system. Existing methods often model the transition function as a differential equation or as a <b>recurrent</b> <b>neural</b> <b>network.</b> Despite their effectiveness in predicting future measurements, few results have successfully established a method for sampling and statistical inference of trajectories using neural networks, partially due to constraints in the parameterization. In this work, we introduce a mechanism to learn the distribution of trajectories by parameterizing the transition function $f$ explicitly as an element in a function space. Our framework allows efficient synthesis of novel trajectories, while also directly providing a convenient tool for inference, i.e., uncertainty estimation, likelihood evaluations and out of distribution detection for abnormal trajectories. These capabilities can have implications for various downstream tasks, e.g., <b>simulation</b> and evaluation for <b>reinforcement</b> <b>learning.</b></p></p class="citation"></blockquote><h3 id=1350--220350-removing-undesirable-concepts-in-text-to-image-generative-models-with-learnable-prompts-anh-bui-et-al-2024>(13/50 | 220/350) Removing Undesirable Concepts in Text-to-Image Generative Models with Learnable Prompts (Anh Bui et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anh Bui, Khanh Doan, Trung Le, Paul Montague, Tamas Abraham, Dinh Phung. (2024)<br><strong>Removing Undesirable Concepts in Text-to-Image Generative Models with Learnable Prompts</strong><br><button class=copy-to-clipboard title="Removing Undesirable Concepts in Text-to-Image Generative Models with Learnable Prompts" index=220>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-220 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Diffusion Model, Knowledge Transfer, Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12326v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12326v1.pdf filename=2403.12326v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Generative models have demonstrated remarkable potential in generating visually impressive content from textual descriptions. However, training these models on unfiltered internet data poses the risk of learning and subsequently propagating undesirable concepts, such as copyrighted or unethical content. In this paper, we propose a novel method to remove undesirable concepts from <b>text-to-image</b> generative models by incorporating a learnable <b>prompt</b> into the cross-attention module. This learnable <b>prompt</b> acts as additional memory to transfer the <b>knowledge</b> <b>of</b> undesirable concepts into it and reduce the dependency of these concepts on the model parameters and corresponding textual inputs. Because of this <b>knowledge</b> <b>transfer</b> into the <b>prompt,</b> erasing these undesirable concepts is more stable and has minimal negative impact on other concepts. We demonstrate the effectiveness of our method on the Stable <b>Diffusion</b> <b>model,</b> showcasing its superiority over state-of-the-art erasure methods in terms of removing undesirable content while preserving other unrelated elements.</p></p class="citation"></blockquote><h3 id=1450--221350-parmesan-parameter-free-memory-search-and-transduction-for-dense-prediction-tasks-philip-matthias-winter-et-al-2024>(14/50 | 221/350) PARMESAN: Parameter-Free Memory Search and Transduction for Dense Prediction Tasks (Philip Matthias Winter et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Philip Matthias Winter, Maria Wimmer, David Major, Dimitrios Lenis, Astrid Berg, Theresa Neubauer, Gaia Romana De Paolis, Johannes Novotny, Sophia Ulonska, Katja Bühler. (2024)<br><strong>PARMESAN: Parameter-Free Memory Search and Transduction for Dense Prediction Tasks</strong><br><button class=copy-to-clipboard title="PARMESAN: Parameter-Free Memory Search and Transduction for Dense Prediction Tasks" index=221>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-221 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 40<br>Keywords: Few-shot, Few-shot Learning, Fine-tuning, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11743v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11743v1.pdf filename=2403.11743v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work we address flexibility in deep learning by means of transductive <b>reasoning.</b> For adaptation to new tasks or new data, existing methods typically involve tuning of learnable parameters or even complete re-training from scratch, rendering such approaches unflexible in practice. We argue that the notion of separating computation from memory by the means of transduction can act as a stepping stone for solving these issues. We therefore propose PARMESAN (parameter-free memory search and transduction), a scalable transduction method which leverages a memory module for solving dense prediction tasks. At inference, hidden representations in memory are being searched to find corresponding examples. In contrast to other methods, PARMESAN learns without the requirement for any continuous training or <b>fine-tuning</b> of learnable parameters simply by modifying the memory content. Our method is compatible with commonly used neural architectures and canonically transfers to 1D, 2D, and 3D grid-based data. We demonstrate the capabilities of our approach at complex tasks such as continual and <b>few-shot</b> <b>learning.</b> PARMESAN learns up to 370 times faster than common baselines while being on par in terms of predictive performance, knowledge retention, and data-efficiency.</p></p class="citation"></blockquote><h3 id=1550--222350-open-world-semi-supervised-learning-for-node-classification-yanling-wang-et-al-2024>(15/50 | 222/350) Open-World Semi-Supervised Learning for Node Classification (Yanling Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yanling Wang, Jing Zhang, Lingxi Zhang, Lixin Liu, Yuxiao Dong, Cuiping Li, Hong Chen, Hongzhi Yin. (2024)<br><strong>Open-World Semi-Supervised Learning for Node Classification</strong><br><button class=copy-to-clipboard title="Open-World Semi-Supervised Learning for Node Classification" index=222>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-222 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-SI, cs.LG<br>Keyword Score: 36<br>Keywords: Node Classification, Graph, Benchmarking, Contrastive Learning, Semi-Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11483v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11483v1.pdf filename=2403.11483v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Open-world <b>semi-supervised</b> <b>learning</b> (Open-world SSL) for <b>node</b> <b>classification,</b> that classifies unlabeled <b>nodes</b> <b>into</b> seen classes or multiple novel classes, is a practical but under-explored problem in the <b>graph</b> community. As only seen classes have human labels, they are usually better learned than novel classes, and thus exhibit smaller intra-class variances within the embedding space (named as imbalance of intra-class variances between seen and novel classes). Based on empirical and theoretical analysis, we find the variance imbalance can negatively impact the model performance. Pre-trained feature encoders can alleviate this issue via producing compact representations for novel classes. However, creating general pre-trained encoders for various types of <b>graph</b> data has been proven to be challenging. As such, there is a demand for an effective method that does not rely on pre-trained <b>graph</b> encoders. In this paper, we propose an IMbalance-Aware method named OpenIMA for Open-world <b>semi-supervised</b> <b>node</b> <b>classification,</b> which trains the <b>node</b> <b>classification</b> model from scratch via <b>contrastive</b> <b>learning</b> with bias-reduced pseudo labels. Extensive experiments on seven popular <b>graph</b> <b>benchmarks</b> demonstrate the effectiveness of OpenIMA, and the source code has been available on GitHub.</p></p class="citation"></blockquote><h3 id=1650--223350-accelerating-scientific-discovery-with-generative-knowledge-extraction-graph-based-representation-and-multimodal-intelligent-graph-reasoning-markus-j-buehler-2024>(16/50 | 223/350) Accelerating Scientific Discovery with Generative Knowledge Extraction, Graph-Based Representation, and Multimodal Intelligent Graph Reasoning (Markus J. Buehler, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Markus J. Buehler. (2024)<br><strong>Accelerating Scientific Discovery with Generative Knowledge Extraction, Graph-Based Representation, and Multimodal Intelligent Graph Reasoning</strong><br><button class=copy-to-clipboard title="Accelerating Scientific Discovery with Generative Knowledge Extraction, Graph-Based Representation, and Multimodal Intelligent Graph Reasoning" index=223>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-223 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cond-mat-mes-hall, cond-mat-mtrl-sci, cond-mat-soft, cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 34<br>Keywords: Graph, Generative AI, Knowledge Graph, Multi-modal, Multi-modal, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11996v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11996v1.pdf filename=2403.11996v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Using <b>generative</b> <b>Artificial</b> Intelligence (AI), we transformed a set of 1,000 scientific papers in the area of biological materials into detailed ontological <b>knowledge</b> <b>graphs,</b> revealing their inherently scale-free nature. Using <b>graph</b> traversal path detection between dissimilar concepts based on combinatorial ranking of node similarity and betweenness centrality, we reveal deep insights into unprecedented interdisciplinary relationships that can be used to answer queries, identify gaps in <b>knowledge,</b> <b>and</b> propose never-before-seen material designs and their behaviors. One comparison revealed detailed structural parallels between biological materials and Beethoven&rsquo;s 9th Symphony, highlighting shared patterns of complexity through isomorphic mapping. The algorithm further created an innovative hierarchical mycelium-based composite that incorporates joint synthesis of <b>graph</b> sampling with principles extracted from Kandinsky&rsquo;s Composition VII painting, where the resulting composite reflects a balance of chaos and order, with features like adjustable porosity, mechanical strength, and complex patterned chemical functionalization. We uncover other isomorphisms across physical, biological, and artistic spheres, revealing a nuanced ontology of immanence and material flux that resonates with postmodern philosophy, and positions these interconnections within a heterarchical framework. Our findings reveal the dynamic, context-dependent interplay of entities beyond traditional hierarchical paradigms, emphasizing the significant role of individual components and their fluctuative relationships within the system. Our predictions achieve a far higher degree of novelty, technical detail and explorative capacity than conventional <b>generative</b> <b>AI</b> methods. The approach establishes a widely useful framework for innovation by revealing hidden connections that facilitate discovery.</p></p class="citation"></blockquote><h3 id=1750--224350-molecular-classification-using-hyperdimensional-graph-classification-pere-verges-et-al-2024>(17/50 | 224/350) Molecular Classification Using Hyperdimensional Graph Classification (Pere Verges et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pere Verges, Igor Nunes, Mike Heddes, Tony Givargis, Alexandru Nicolau. (2024)<br><strong>Molecular Classification Using Hyperdimensional Graph Classification</strong><br><button class=copy-to-clipboard title="Molecular Classification Using Hyperdimensional Graph Classification" index=224>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-224 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-NE, cs.LG, q-bio-QM<br>Keyword Score: 33<br>Keywords: Graph Classification, Graph, Graph Neural Network, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12307v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12307v1.pdf filename=2403.12307v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Our work introduces an innovative approach to <b>graph</b> <b>learning</b> <b>by</b> leveraging Hyperdimensional Computing. <b>Graphs</b> <b>serve</b> <b>as</b> a widely embraced method for conveying information, and their utilization in learning has gained significant attention. This is notable in the field of chemoinformatics, where learning from <b>graph</b> <b>representations</b> <b>plays</b> a pivotal role. An important application within this domain involves the identification of cancerous cells across diverse molecular structures. We propose an HDC-based model that demonstrates comparable Area Under the Curve results when compared to state-of-the-art models like <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> or the Weisfieler-Lehman <b>graph</b> <b>kernel</b> <b>(WL).</b> Moreover, it outperforms previously proposed hyperdimensional computing <b>graph</b> <b>learning</b> <b>methods.</b> Furthermore, it achieves noteworthy speed enhancements, boasting a 40x acceleration in the training phase and a 15x improvement in inference time compared to <b>GNN</b> and WL models. This not only underscores the efficacy of the HDC-based method, but also highlights its potential for expedited and resource-efficient <b>graph</b> <b>learning.</b></p></p class="citation"></blockquote><h3 id=1850--225350-larimar-large-language-models-with-episodic-memory-control-payel-das-et-al-2024>(18/50 | 225/350) Larimar: Large Language Models with Episodic Memory Control (Payel Das et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Payel Das, Subhajit Chaudhury, Elliot Nelson, Igor Melnyk, Sarath Swaminathan, Sihui Dai, Aurélie Lozano, Georgios Kollias, Vijil Chenthamarakshan, Jiří, Navrátil, Soham Dan, Pin-Yu Chen. (2024)<br><strong>Larimar: Large Language Models with Episodic Memory Control</strong><br><button class=copy-to-clipboard title="Larimar: Large Language Models with Episodic Memory Control" index=225>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-225 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 33<br>Keywords: Benchmarking, Fine-tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11901v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11901v1.pdf filename=2403.11901v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Efficient and accurate updating of knowledge stored in <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> is one of the most pressing research challenges today. This paper presents Larimar - a novel, brain-inspired architecture for enhancing <b>LLMs</b> with a distributed episodic memory. Larimar&rsquo;s memory allows for dynamic, one-shot updates of knowledge without the need for computationally expensive re-training or <b>fine-tuning.</b> Experimental results on multiple fact editing <b>benchmarks</b> demonstrate that Larimar attains accuracy comparable to most competitive baselines, even in the challenging sequential editing setup, but also excels in speed - yielding speed-ups of 4-10x depending on the base <b>LLM</b> - as well as flexibility due to the proposed architecture being simple, <b>LLM-agnostic,</b> and hence general. We further provide mechanisms for selective fact forgetting and input context length generalization with Larimar and show their effectiveness.</p></p class="citation"></blockquote><h3 id=1950--226350-layer-diverse-negative-sampling-for-graph-neural-networks-wei-duan-et-al-2024>(19/50 | 226/350) Layer-diverse Negative Sampling for Graph Neural Networks (Wei Duan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wei Duan, Jie Lu, Yu Guang Wang, Junyu Xuan. (2024)<br><strong>Layer-diverse Negative Sampling for Graph Neural Networks</strong><br><button class=copy-to-clipboard title="Layer-diverse Negative Sampling for Graph Neural Networks" index=226>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-226 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 33<br>Keywords: Message-Passing, Graph, Graph Neural Network, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11408v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11408v1.pdf filename=2403.11408v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>neural</b> <b>networks</b> <b>(GNNs)</b> are a powerful solution for various structure learning applications due to their strong representation capabilities for <b>graph</b> <b>data.</b> <b>However,</b> traditional <b>GNNs,</b> relying on <b>message-passing</b> mechanisms that gather information exclusively from first-order neighbours (known as positive samples), can lead to issues such as over-smoothing and over-squashing. To mitigate these issues, we propose a layer-diverse negative sampling method for <b>message-passing</b> propagation. This method employs a sampling matrix within a determinantal point process, which transforms the candidate set into a space and selectively samples from this space to generate negative samples. To further enhance the diversity of the negative samples during each forward pass, we develop a space-squeezing method to achieve layer-wise diversity in multi-layer <b>GNNs.</b> Experiments on various real-world <b>graph</b> <b>datasets</b> <b>demonstrate</b> the effectiveness of our approach in improving the diversity of negative samples and overall learning performance. Moreover, adding negative samples dynamically changes the <b>graph&rsquo;s</b> <b>topology,</b> <b>thus</b> with the strong potential to improve the expressiveness of <b>GNNs</b> and reduce the risk of over-squashing.</p></p class="citation"></blockquote><h3 id=2050--227350-improving-generalization-via-meta-learning-on-hard-samples-nishant-jain-et-al-2024>(20/50 | 227/350) Improving Generalization via Meta-Learning on Hard Samples (Nishant Jain et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nishant Jain, Arun S. Suggala, Pradeep Shenoy. (2024)<br><strong>Improving Generalization via Meta-Learning on Hard Samples</strong><br><button class=copy-to-clipboard title="Improving Generalization via Meta-Learning on Hard Samples" index=227>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-227 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Meta Learning, Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12236v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12236v1.pdf filename=2403.12236v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Learned reweighting (LRW) approaches to <b>supervised</b> <b>learning</b> use an optimization criterion to assign weights for training instances, in order to maximize performance on a representative validation dataset. We pose and formalize the problem of optimized selection of the validation set used in LRW training, to improve classifier generalization. In particular, we show that using hard-to-classify instances in the validation set has both a theoretical connection to, and strong empirical evidence of generalization. We provide an efficient algorithm for training this <b>meta-optimized</b> <b>model,</b> as well as a simple train-twice heuristic for careful comparative study. We demonstrate that LRW with easy validation data performs consistently worse than LRW with hard validation data, establishing the validity of our <b>meta-optimization</b> <b>problem.</b> Our proposed algorithm outperforms a wide range of baselines on a range of datasets and domain shift challenges (Imagenet-1K, CIFAR-100, Clothing-1M, CAMELYON, WILDS, etc.), with ~1% gains using VIT-B on Imagenet. We also show that using naturally hard examples for validation (Imagenet-R / Imagenet-A) in LRW training for Imagenet improves performance on both clean and naturally hard test instances by 1-2%. Secondary analyses show that using hard validation data in an LRW framework improves margins on test data, hinting at the mechanism underlying our empirical gains. We believe this work opens up new research directions for the <b>meta-optimization</b> <b>of</b> <b>meta-learning</b> <b>in</b> a <b>supervised</b> <b>learning</b> context.</p></p class="citation"></blockquote><h3 id=2150--228350-unveil-conditional-diffusion-models-with-classifier-free-guidance-a-sharp-statistical-theory-hengyu-fu-et-al-2024>(21/50 | 228/350) Unveil Conditional Diffusion Models with Classifier-free Guidance: A Sharp Statistical Theory (Hengyu Fu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hengyu Fu, Zhuoran Yang, Mengdi Wang, Minshuo Chen. (2024)<br><strong>Unveil Conditional Diffusion Models with Classifier-free Guidance: A Sharp Statistical Theory</strong><br><button class=copy-to-clipboard title="Unveil Conditional Diffusion Models with Classifier-free Guidance: A Sharp Statistical Theory" index=228>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-228 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-ST, stat-ML, stat-TH<br>Keyword Score: 30<br>Keywords: Diffusion Model, Reinforcement Learning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11968v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11968v1.pdf filename=2403.11968v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Conditional <b>diffusion</b> <b>models</b> serve as the foundation of modern image synthesis and find extensive application in fields like computational biology and <b>reinforcement</b> <b>learning.</b> In these applications, conditional <b>diffusion</b> <b>models</b> incorporate various conditional information, such as <b>prompt</b> input, to guide the sample generation towards desired properties. Despite the empirical success, theory of conditional <b>diffusion</b> <b>models</b> is largely missing. This paper bridges this gap by presenting a sharp statistical theory of distribution estimation using conditional <b>diffusion</b> <b>models.</b> Our analysis yields a sample complexity bound that adapts to the smoothness of the data distribution and matches the minimax lower bound. The key to our theoretical development lies in an approximation result for the conditional score function, which relies on a novel diffused Taylor approximation technique. Moreover, we demonstrate the utility of our statistical theory in elucidating the performance of conditional <b>diffusion</b> <b>models</b> across diverse applications, including model-based transition kernel estimation in <b>reinforcement</b> <b>learning,</b> solving inverse problems, and reward conditioned sample generation.</p></p class="citation"></blockquote><h3 id=2250--229350-s-jepa-towards-seamless-cross-dataset-transfer-through-dynamic-spatial-attention-pierre-guetschel-et-al-2024>(22/50 | 229/350) S-JEPA: towards seamless cross-dataset transfer through dynamic spatial attention (Pierre Guetschel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pierre Guetschel, Thomas Moreau, Michael Tangermann. (2024)<br><strong>S-JEPA: towards seamless cross-dataset transfer through dynamic spatial attention</strong><br><button class=copy-to-clipboard title="S-JEPA: towards seamless cross-dataset transfer through dynamic spatial attention" index=229>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-229 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Self-supervised Learning, Self-supervised Learning, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11772v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11772v1.pdf filename=2403.11772v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Motivated by the challenge of seamless cross-dataset <b>transfer</b> <b>in</b> EEG signal processing, this article presents an exploratory study on the use of Joint Embedding Predictive Architectures (JEPAs). In recent years, <b>self-supervised</b> <b>learning</b> has emerged as a promising approach for <b>transfer</b> <b>learning</b> in various domains. However, its application to EEG signals remains largely unexplored. In this article, we introduce Signal-JEPA for representing EEG recordings which includes a novel domain-specific spatial block masking strategy and three novel architectures for downstream classification. The study is conducted on a 54~subjects dataset and the downstream performance of the models is evaluated on three different BCI paradigms: motor imagery, ERP and SSVEP. Our study provides preliminary evidence for the potential of JEPAs in EEG signal encoding. Notably, our results highlight the importance of spatial filtering for accurate downstream classification and reveal an influence of the length of the pre-training examples but not of the mask size on the downstream performance.</p></p class="citation"></blockquote><h3 id=2350--230350-pita-physics-informed-trajectory-autoencoder-johannes-fischer-et-al-2024>(23/50 | 230/350) PITA: Physics-Informed Trajectory Autoencoder (Johannes Fischer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Johannes Fischer, Kevin Rösch, Martin Lauer, Christoph Stiller. (2024)<br><strong>PITA: Physics-Informed Trajectory Autoencoder</strong><br><button class=copy-to-clipboard title="PITA: Physics-Informed Trajectory Autoencoder" index=230>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-230 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-RO, cs.LG<br>Keyword Score: 30<br>Keywords: Autoencoder, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11728v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11728v1.pdf filename=2403.11728v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Validating robotic systems in safety-critical appli-cations requires testing in many scenarios including rare edgecases that are unlikely to occur, requiring to complement real-world testing with testing in <b>simulation.</b> Generative models canbe used to augment real-world datasets with generated data toproduce edge case scenarios by sampling in a learned latentspace. <b>Autoencoders</b> can learn said latent representation for aspecific domain by learning to reconstruct the input data froma lower-dimensional intermediate representation. However, theresulting trajectories are not necessarily physically plausible, butinstead typically contain noise that is not present in the inputtrajectory. To resolve this issue, we propose the novel Physics-Informed Trajectory <b>Autoencoder</b> (PITA) architecture, whichincorporates a physical dynamics model into the loss functionof the <b>autoencoder.</b> This results in smooth trajectories that notonly reconstruct the input trajectory but also adhere to thephysical model. We evaluate PITA on a real-world dataset ofvehicle trajectories and compare its performance to a normalautoencoder and a state-of-the-art action-space <b>autoencoder.</b></p></p class="citation"></blockquote><h3 id=2450--231350-seisfusion-constrained-diffusion-model-with-input-guidance-for-3d-seismic-data-interpolation-and-reconstruction-shuang-wang-et-al-2024>(24/50 | 231/350) SeisFusion: Constrained Diffusion Model with Input Guidance for 3D Seismic Data Interpolation and Reconstruction (Shuang Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shuang Wang, Fei Deng, Peifan Jiang, Zishan Gong, Xiaolin Wei, Yuqing Wang. (2024)<br><strong>SeisFusion: Constrained Diffusion Model with Input Guidance for 3D Seismic Data Interpolation and Reconstruction</strong><br><button class=copy-to-clipboard title="SeisFusion: Constrained Diffusion Model with Input Guidance for 3D Seismic Data Interpolation and Reconstruction" index=231>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-231 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, physics-geo-ph<br>Keyword Score: 30<br>Keywords: Diffusion Model, Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11482v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11482v1.pdf filename=2403.11482v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Geographical, physical, or economic constraints often result in missing traces within seismic data, making the reconstruction of complete seismic data a crucial step in seismic data processing. Traditional methods for seismic data reconstruction require the selection of multiple empirical parameters and struggle to handle large-scale continuous missing data. With the development of deep learning, various neural networks have demonstrated powerful reconstruction capabilities. However, these <b>convolutional</b> <b>neural</b> <b>networks</b> represent a point-to-point reconstruction approach that may not cover the entire distribution of the dataset. Consequently, when dealing with seismic data featuring complex missing patterns, such networks may experience varying degrees of performance degradation. In response to this challenge, we propose a novel <b>diffusion</b> <b>model</b> reconstruction framework tailored for 3D seismic data. To constrain the results generated by the <b>diffusion</b> <b>model,</b> we introduce conditional supervision constraints into the <b>diffusion</b> <b>model,</b> constraining the generated data of the <b>diffusion</b> <b>model</b> based on the input data to be reconstructed. We introduce a 3D neural network architecture into the <b>diffusion</b> <b>model,</b> successfully extending the 2D <b>diffusion</b> <b>model</b> to 3D space. Additionally, we refine the model&rsquo;s generation process by incorporating missing data into the generation process, resulting in reconstructions with higher consistency. Through ablation studies determining optimal parameter values, our method exhibits superior reconstruction accuracy when applied to both field datasets and synthetic datasets, effectively addressing a wide range of complex missing patterns. Our implementation is available at <a href=https://github.com/WAL-l/SeisFusion>https://github.com/WAL-l/SeisFusion</a>.</p></p class="citation"></blockquote><h3 id=2550--232350-gcam-gaussian-and-causal-attention-model-of-food-fine-grained-recognition-guohang-zhuang-et-al-2024>(25/50 | 232/350) GCAM: Gaussian and causal-attention model of food fine-grained recognition (Guohang Zhuang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guohang Zhuang, Yue Hu, Tianxing Yan, JiaZhan Gao. (2024)<br><strong>GCAM: Gaussian and causal-attention model of food fine-grained recognition</strong><br><button class=copy-to-clipboard title="GCAM: Gaussian and causal-attention model of food fine-grained recognition" index=232>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-232 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Counter-factual, Counterfactual Reasoning, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12109v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12109v1.pdf filename=2403.12109v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Currently, most food recognition relies on deep learning for category classification. However, these approaches struggle to effectively distinguish between visually similar food samples, highlighting the pressing need to address fine-grained issues in food recognition. To mitigate these challenges, we propose the adoption of a Gaussian and causal-attention model for fine-grained object recognition.In particular, we train to obtain Gaussian features over target regions, followed by the extraction of fine-grained features from the objects, thereby enhancing the feature mapping capabilities of the target regions. To counteract data drift resulting from uneven data distributions, we employ a <b>counterfactual</b> <b>reasoning</b> approach. By using <b>counterfactual</b> <b>interventions,</b> we analyze the impact of the learned image attention mechanism on network predictions, enabling the network to acquire more useful attention weights for fine-grained image recognition. Finally, we design a learnable loss strategy to balance training stability across various modules, ultimately improving the accuracy of the final target recognition. We validate our approach on four relevant datasets, demonstrating its excellent performance across these four datasets.We experimentally show that GCAM surpasses state-of-the-art methods on the ETH-FOOD101, UECFOOD256, and Vireo-FOOD172 datasets. Furthermore, our approach also achieves state-of-the-art performance on the CUB-200 dataset.</p></p class="citation"></blockquote><h3 id=2650--233350-automated-data-processing-and-feature-engineering-for-deep-learning-and-big-data-applications-a-survey-alhassan-mumuni-et-al-2024>(26/50 | 233/350) Automated data processing and feature engineering for deep learning and big data applications: a survey (Alhassan Mumuni et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alhassan Mumuni, Fuseini Mumuni. (2024)<br><strong>Automated data processing and feature engineering for deep learning and big data applications: a survey</strong><br><button class=copy-to-clipboard title="Automated data processing and feature engineering for deep learning and big data applications: a survey" index=233>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-233 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-DB, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Data Augmentation, Generative AI, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11395v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11395v2.pdf filename=2403.11395v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Modern approach to artificial intelligence (AI) aims to design algorithms that learn directly from <b>data.</b> <b>This</b> approach has achieved impressive results and has contributed significantly to the progress of AI, particularly in the sphere of <b>supervised</b> deep learning. It has also simplified the design of machine learning systems as the learning process is highly automated. However, not all <b>data</b> <b>processing</b> tasks in conventional deep learning pipelines have been automated. In most cases <b>data</b> <b>has</b> to be manually collected, preprocessed and further extended through <b>data</b> <b>augmentation</b> before they can be effective for training. Recently, special techniques for automating these tasks have emerged. The automation of <b>data</b> <b>processing</b> tasks is driven by the need to utilize large volumes of complex, heterogeneous <b>data</b> <b>for</b> machine learning and big <b>data</b> <b>applications.</b> Today, end-to-end automated <b>data</b> <b>processing</b> systems based on automated machine learning (AutoML) techniques are capable of taking raw <b>data</b> <b>and</b> transforming them into useful features for Big <b>Data</b> <b>tasks</b> by automating all intermediate processing stages. In this work, we present a thorough review of approaches for automating <b>data</b> <b>processing</b> tasks in deep learning pipelines, including automated <b>data</b> <b>preprocessing&ndash;e.g.,</b> <b>data</b> <b>cleaning,</b> labeling, missing <b>data</b> <b>imputation,</b> and categorical <b>data</b> <b>encoding&ndash;as</b> well as <b>data</b> <b>augmentation</b> (including synthetic <b>data</b> <b>generation</b> using <b>generative</b> <b>AI</b> methods) and feature engineering&ndash;specifically, automated feature extraction, feature construction and feature selection. In addition to automating specific <b>data</b> <b>processing</b> tasks, we discuss the use of AutoML methods and tools to simultaneously optimize all stages of the machine learning pipeline.</p></p class="citation"></blockquote><h3 id=2750--234350-semantic-enhanced-representation-learning-for-road-networks-with-temporal-dynamics-yile-chen-et-al-2024>(27/50 | 234/350) Semantic-Enhanced Representation Learning for Road Networks with Temporal Dynamics (Yile Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yile Chen, Xiucheng Li, Gao Cong, Zhifeng Bao, Cheng Long. (2024)<br><strong>Semantic-Enhanced Representation Learning for Road Networks with Temporal Dynamics</strong><br><button class=copy-to-clipboard title="Semantic-Enhanced Representation Learning for Road Networks with Temporal Dynamics" index=234>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-234 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 25<br>Keywords: Knowledge Distillation, Representation Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11495v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11495v1.pdf filename=2403.11495v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this study, we introduce a novel framework called Toast for learning general-purpose <b>representations</b> <b>of</b> road networks, along with its advanced counterpart DyToast, designed to enhance the integration of temporal dynamics to boost the performance of various time-sensitive downstream tasks. Specifically, we propose to encode two pivotal semantic characteristics intrinsic to road networks: traffic patterns and traveling semantics. To achieve this, we refine the skip-gram module by incorporating auxiliary objectives aimed at predicting the traffic context associated with a target road segment. Moreover, we leverage trajectory data and design pre-training strategies based on <b>Transformer</b> to <b>distill</b> traveling semantics on road networks. DyToast further augments this framework by employing unified trigonometric functions characterized by their beneficial properties, enabling the capture of temporal evolution and dynamic nature of road networks more effectively. With these proposed techniques, we can obtain <b>representations</b> <b>that</b> encode multi-faceted aspects of knowledge within road networks, applicable across both road segment-based applications and trajectory-based applications. Extensive experiments on two real-world datasets across three tasks demonstrate that our proposed framework consistently outperforms the state-of-the-art baselines by a significant margin.</p></p class="citation"></blockquote><h3 id=2850--235350-graph-neural-networks-for-learning-equivariant-representations-of-neural-networks-miltiadis-kofinas-et-al-2024>(28/50 | 235/350) Graph Neural Networks for Learning Equivariant Representations of Neural Networks (Miltiadis Kofinas et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Miltiadis Kofinas, Boris Knyazev, Yan Zhang, Yunlu Chen, Gertjan J. Burghouts, Efstratios Gavves, Cees G. M. Snoek, David W. Zhang. (2024)<br><strong>Graph Neural Networks for Learning Equivariant Representations of Neural Networks</strong><br><button class=copy-to-clipboard title="Graph Neural Networks for Learning Equivariant Representations of Neural Networks" index=235>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-235 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, stat-ML<br>Keyword Score: 23<br>Keywords: Graph, Graph Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12143v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12143v2.pdf filename=2403.12143v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neural networks that process the parameters of other neural networks find applications in domains as diverse as classifying implicit neural representations, generating neural network weights, and predicting generalization errors. However, existing approaches either overlook the inherent permutation symmetry in the neural network or rely on intricate weight-sharing patterns to achieve equivariance, while ignoring the impact of the network architecture itself. In this work, we propose to represent neural networks as computational <b>graphs</b> <b>of</b> <b>parameters,</b> which allows us to harness powerful <b>graph</b> <b>neural</b> <b>networks</b> and <b>transformers</b> that preserve permutation symmetry. Consequently, our approach enables a single model to encode neural computational <b>graphs</b> <b>with</b> <b>diverse</b> architectures. We showcase the effectiveness of our method on a wide range of tasks, including classification and editing of implicit neural representations, predicting generalization performance, and learning to optimize, while consistently outperforming state-of-the-art methods. The source code is open-sourced at <a href=https://github.com/mkofinas/neural-graphs>https://github.com/mkofinas/neural-graphs</a>.</p></p class="citation"></blockquote><h3 id=2950--236350-routerbench-a-benchmark-for-multi-llm-routing-system-qitian-jason-hu-et-al-2024>(29/50 | 236/350) ROUTERBENCH: A Benchmark for Multi-LLM Routing System (Qitian Jason Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qitian Jason Hu, Jacob Bieker, Xiuyu Li, Nan Jiang, Benjamin Keigwin, Gaurav Ranganath, Kurt Keutzer, Shriyash Kaustubh Upadhyay. (2024)<br><strong>ROUTERBENCH: A Benchmark for Multi-LLM Routing System</strong><br><button class=copy-to-clipboard title="ROUTERBENCH: A Benchmark for Multi-LLM Routing System" index=236>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-236 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Benchmarking, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12031v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12031v1.pdf filename=2403.12031v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As the range of applications for <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> continues to grow, the demand for effective serving solutions becomes increasingly critical. Despite the versatility of <b>LLMs,</b> no single model can optimally address all tasks and applications, particularly when balancing performance with cost. This limitation has led to the development of <b>LLM</b> routing systems, which combine the strengths of various models to overcome the constraints of individual <b>LLMs.</b> Yet, the absence of a standardized <b>benchmark</b> for evaluating the performance of <b>LLM</b> routers hinders progress in this area. To bridge this gap, we present ROUTERBENCH, a novel evaluation framework designed to systematically assess the efficacy of <b>LLM</b> routing systems, along with a comprehensive dataset comprising over 405k inference outcomes from representative <b>LLMs</b> to support the development of routing strategies. We further propose a theoretical framework for <b>LLM</b> routing, and deliver a comparative analysis of various routing approaches through ROUTERBENCH, highlighting their potentials and limitations within our evaluation framework. This work not only formalizes and advances the development of <b>LLM</b> routing systems but also sets a standard for their assessment, paving the way for more accessible and economically viable <b>LLM</b> deployments. The code and data are available at <a href=https://github.com/withmartian/routerbench>https://github.com/withmartian/routerbench</a>.</p></p class="citation"></blockquote><h3 id=3050--237350-casper-causality-aware-spatiotemporal-graph-neural-networks-for-spatiotemporal-time-series-imputation-baoyu-jing-et-al-2024>(30/50 | 237/350) CASPER: Causality-Aware Spatiotemporal Graph Neural Networks for Spatiotemporal Time Series Imputation (Baoyu Jing et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Baoyu Jing, Dawei Zhou, Kan Ren, Carl Yang. (2024)<br><strong>CASPER: Causality-Aware Spatiotemporal Graph Neural Networks for Spatiotemporal Time Series Imputation</strong><br><button class=copy-to-clipboard title="CASPER: Causality-Aware Spatiotemporal Graph Neural Networks for Spatiotemporal Time Series Imputation" index=237>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-237 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 23<br>Keywords: Graph, Graph Neural Network, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11960v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11960v1.pdf filename=2403.11960v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Spatiotemporal time series is the foundation of understanding human activities and their impacts, which is usually collected via monitoring sensors placed at different locations. The collected data usually contains missing values due to various failures, which have significant impact on data analysis. To impute the missing values, a lot of methods have been introduced. When recovering a specific data point, most existing methods tend to take into consideration all the information relevant to that point regardless of whether they have a cause-and-effect relationship. During data collection, it is inevitable that some unknown confounders are included, e.g., background noise in time series and non-causal shortcut edges in the constructed sensor network. These confounders could open backdoor paths between the input and output, in other words, they establish non-causal correlations between the input and output. Over-exploiting these non-causal correlations could result in overfitting and make the model vulnerable to noises. In this paper, we first revisit spatiotemporal time series imputation from a causal perspective, which shows the causal relationships among the input, output, embeddings and confounders. Next, we show how to block the confounders via the frontdoor adjustment. Based on the results of the frontdoor adjustment, we introduce a novel Causality-Aware SPatiotEmpoRal <b>graph</b> <b>neural</b> <b>network</b> (CASPER), which contains a novel Spatiotemporal Causal Attention (SCA) and a <b>Prompt</b> Based Decoder (PBD). PBD could reduce the impact of confounders and SCA could discover the sparse causal relationships among embeddings. Theoretical analysis reveals that SCA discovers causal relationships based on the values of gradients. We evaluate Casper on three real-world datasets, and the experimental results show that Casper outperforms the baselines and effectively discovers causal relationships.</p></p class="citation"></blockquote><h3 id=3150--238350-time-series-compression-using-quaternion-valued-neural-networks-and-quaternion-backpropagation-johannes-pöppelbaum-et-al-2024>(31/50 | 238/350) Time Series Compression using Quaternion Valued Neural Networks and Quaternion Backpropagation (Johannes Pöppelbaum et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Johannes Pöppelbaum, Andreas Schwung. (2024)<br><strong>Time Series Compression using Quaternion Valued Neural Networks and Quaternion Backpropagation</strong><br><button class=copy-to-clipboard title="Time Series Compression using Quaternion Valued Neural Networks and Quaternion Backpropagation" index=238>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-238 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Benchmarking, Contrastive Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11722v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11722v1.pdf filename=2403.11722v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose a novel quaternionic time-series compression methodology where we divide a long time-series into segments of data, extract the min, max, mean and standard deviation of these chunks as representative features and encapsulate them in a quaternion, yielding a quaternion valued time-series. This time-series is processed using quaternion valued neural network layers, where we aim to preserve the relation between these features through the usage of the Hamilton product. To train this quaternion neural network, we derive quaternion backpropagation employing the GHR calculus, which is required for a valid product and chain rule in quaternion space. Furthermore, we investigate the connection between the derived update rules and automatic differentiation. We apply our proposed compression method on the Tennessee Eastman Dataset, where we perform fault classification using the compressed data in two settings: a fully <b>supervised</b> one and in a semi <b>supervised,</b> <b>contrastive</b> <b>learning</b> setting. Both times, we were able to outperform real valued counterparts as well as two baseline models: one with the uncompressed time-series as the input and the other with a regular downsampling using the mean. Further, we could improve the classification <b>benchmark</b> set by SimCLR-TS from 81.43% to 83.90%.</p></p class="citation"></blockquote><h3 id=3250--239350-complete-and-efficient-graph-transformers-for-crystal-material-property-prediction-keqiang-yan-et-al-2024>(32/50 | 239/350) Complete and Efficient Graph Transformers for Crystal Material Property Prediction (Keqiang Yan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Keqiang Yan, Cong Fu, Xiaofeng Qian, Xiaoning Qian, Shuiwang Ji. (2024)<br><strong>Complete and Efficient Graph Transformers for Crystal Material Property Prediction</strong><br><button class=copy-to-clipboard title="Complete and Efficient Graph Transformers for Crystal Material Property Prediction" index=239>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-239 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cond-mat-mtrl-sci, cs-LG, cs.LG<br>Keyword Score: 21<br>Keywords: Graph, Benchmarking, Representation Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11857v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11857v1.pdf filename=2403.11857v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Crystal structures are characterized by atomic bases within a primitive unit cell that repeats along a regular lattice throughout 3D space. The periodic and infinite nature of crystals poses unique challenges for geometric <b>graph</b> <b>representation</b> <b>learning.</b> Specifically, constructing <b>graphs</b> that effectively capture the complete geometric information of crystals and handle chiral crystals remains an unsolved and challenging problem. In this paper, we introduce a novel approach that utilizes the periodic patterns of unit cells to establish the lattice-based <b>representation</b> <b>for</b> each atom, enabling efficient and expressive <b>graph</b> <b>representations</b> <b>of</b> crystals. Furthermore, we propose ComFormer, a SE(3) <b>transformer</b> designed specifically for crystalline materials. ComFormer includes two variants; namely, iComFormer that employs invariant geometric descriptors of Euclidean distances and angles, and eComFormer that utilizes equivariant vector <b>representations.</b> <b>Experimental</b> results demonstrate the state-of-the-art predictive accuracy of ComFormer variants on various tasks across three widely-used crystal <b>benchmarks.</b> Our code is publicly available as part of the AIRS library (<a href=https://github.com/divelab/AIRS)>https://github.com/divelab/AIRS)</a>.</p></p class="citation"></blockquote><h3 id=3350--240350-reinforcement-learning-from-delayed-observations-via-world-models-armin-karamzade-et-al-2024>(33/50 | 240/350) Reinforcement Learning from Delayed Observations via World Models (Armin Karamzade et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Armin Karamzade, Kyungmin Kim, Montek Kalsi, Roy Fox. (2024)<br><strong>Reinforcement Learning from Delayed Observations via World Models</strong><br><button class=copy-to-clipboard title="Reinforcement Learning from Delayed Observations via World Models" index=240>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-240 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Markov Decision Process, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12309v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12309v1.pdf filename=2403.12309v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In standard <b>Reinforcement</b> <b>Learning</b> settings, agents typically assume immediate feedback about the effects of their actions after taking them. However, in practice, this assumption may not hold true due to physical constraints and can significantly impact the performance of RL algorithms. In this paper, we focus on addressing observation delays in partially observable environments. We propose leveraging world models, which have shown success in integrating past observations and learning dynamics, to handle observation delays. By reducing delayed POMDPs to delayed <b>MDPs</b> with world models, our methods can effectively handle partial observability, where existing approaches achieve sub-optimal performance or even degrade quickly as observability decreases. Experiments suggest that one of our methods can outperform a naive model-based approach by up to %30. Moreover, we evaluate our methods on visual input based delayed environment, for the first time showcasing delay-aware <b>reinforcement</b> <b>learning</b> on visual observations.</p></p class="citation"></blockquote><h3 id=3450--241350-multistep-inverse-is-not-all-you-need-alexander-levine-et-al-2024>(34/50 | 241/350) Multistep Inverse Is Not All You Need (Alexander Levine et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alexander Levine, Peter Stone, Amy Zhang. (2024)<br><strong>Multistep Inverse Is Not All You Need</strong><br><button class=copy-to-clipboard title="Multistep Inverse Is Not All You Need" index=241>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-241 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-SY, cs.LG, eess-SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11940v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11940v1.pdf filename=2403.11940v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In real-world control settings, the observation space is often unnecessarily high-dimensional and subject to time-correlated noise. However, the controllable dynamics of the system are often far simpler than the dynamics of the raw observations. It is therefore desirable to learn an encoder to map the observation space to a simpler space of control-relevant variables. In this work, we consider the Ex-BMDP model, first proposed by Efroni et al. (2022), which formalizes control problems where observations can be factorized into an action-dependent latent state which evolves deterministically, and action-independent time-correlated noise. Lamb et al. (2022) proposes the &ldquo;AC-State&rdquo; method for learning an encoder to extract a complete action-dependent latent state representation from the observations in such problems. AC-State is a multistep-inverse method, in that it uses the encoding of the the first and last state in a path to predict the first action in the path. However, we identify cases where AC-State will fail to learn a correct latent representation of the agent-controllable factor of the state. We therefore propose a new algorithm, ACDF, which combines multistep-inverse prediction with a latent forward model. ACDF is guaranteed to correctly infer an action-dependent latent state encoder for a large class of Ex-BMDP models. We demonstrate the effectiveness of ACDF on tabular Ex-BMDPs through numerical <b>simulations;</b> as well as high-dimensional environments using neural-network-based encoders. Code is available at <a href=https://github.com/midi-lab/acdf>https://github.com/midi-lab/acdf</a>.</p></p class="citation"></blockquote><h3 id=3550--242350-global-optimality-without-mixing-time-oracles-in-average-reward-rl-via-multi-level-actor-critic-bhrij-patel-et-al-2024>(35/50 | 242/350) Global Optimality without Mixing Time Oracles in Average-reward RL via Multi-level Actor-Critic (Bhrij Patel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bhrij Patel, Wesley A. Suttle, Alec Koppel, Vaneet Aggarwal, Brian M. Sadler, Amrit Singh Bedi, Dinesh Manocha. (2024)<br><strong>Global Optimality without Mixing Time Oracles in Average-reward RL via Multi-level Actor-Critic</strong><br><button class=copy-to-clipboard title="Global Optimality without Mixing Time Oracles in Average-reward RL via Multi-level Actor-Critic" index=242>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-242 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Markov Decision Process, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11925v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11925v1.pdf filename=2403.11925v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the context of average-reward <b>reinforcement</b> <b>learning,</b> the requirement for oracle knowledge of the mixing time, a measure of the duration a Markov chain under a fixed policy needs to achieve its stationary distribution-poses a significant challenge for the global convergence of policy gradient methods. This requirement is particularly problematic due to the difficulty and expense of estimating mixing time in environments with large state spaces, leading to the necessity of impractically long trajectories for effective gradient estimation in practical applications. To address this limitation, we consider the Multi-level Actor-Critic (MAC) framework, which incorporates a Multi-level Monte Carlo (MLMC) gradient estimator. With our approach, we effectively alleviate the dependency on mixing time knowledge, a first for average-reward <b>MDPs</b> global convergence. Furthermore, our approach exhibits the tightest-available dependence of $\mathcal{O}\left( \sqrt{\tau_{mix}} \right)$ relative to prior work. With a 2D gridworld goal-reaching navigation experiment, we demonstrate that MAC achieves higher reward than a previous PG-based method for average reward, Parameterized Policy Gradient with Advantage Estimation (PPGAE), especially in cases with relatively small training sample budget restricting trajectory length.</p></p class="citation"></blockquote><h3 id=3650--243350-efficient-training-of-learning-based-thermal-power-flow-for-4th-generation-district-heating-grids-andreas-bott-et-al-2024>(36/50 | 243/350) Efficient Training of Learning-Based Thermal Power Flow for 4th Generation District Heating Grids (Andreas Bott et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andreas Bott, Mario Beykirch, Florian Steinke. (2024)<br><strong>Efficient Training of Learning-Based Thermal Power Flow for 4th Generation District Heating Grids</strong><br><button class=copy-to-clipboard title="Efficient Training of Learning-Based Thermal Power Flow for 4th Generation District Heating Grids" index=243>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-243 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CE, cs-LG, cs-SY, cs.LG, eess-SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11877v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11877v1.pdf filename=2403.11877v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Thermal power flow (TPF) is an important task for various control purposes in 4 Th generation district heating grids with multiple decentral heat sources and meshed grid structures. Computing the TPF, i.e., determining the grid state consisting of temperatures, pressures, and mass flows for given supply and demand values, is classically done by solving the nonlinear heat grid equations, but can be sped up by orders of magnitude using learned models such as neural networks. We propose a novel, efficient scheme to generate a sufficiently large training data set covering relevant supply and demand values. Instead of sampling supply and demand values, our approach generates training examples from a proxy distribution over generator and consumer mass flows, omitting the iterations needed for solving the heat grid equations. The exact, but slightly different, training examples can be weighted to represent the original training distribution. We show with <b>simulations</b> for typical grid structures that the new approach can reduce training set generation times by two orders of magnitude compared to sampling supply and demand values directly, without loss of relevance for the training samples. Moreover, learning TPF with a training data set is shown to outperform sample-free, physics-aware training approaches significantly.</p></p class="citation"></blockquote><h3 id=3750--244350-near-optimal-solutions-of-constrained-learning-problems-juan-elenter-et-al-2024>(37/50 | 244/350) Near-Optimal Solutions of Constrained Learning Problems (Juan Elenter et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Juan Elenter, Luiz F. O. Chamon, Alejandro Ribeiro. (2024)<br><strong>Near-Optimal Solutions of Constrained Learning Problems</strong><br><button class=copy-to-clipboard title="Near-Optimal Solutions of Constrained Learning Problems" index=244>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-244 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, eess-SP, math-OC<br>Keyword Score: 20<br>Keywords: Constrained Learning, Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11844v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11844v1.pdf filename=2403.11844v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the widespread adoption of machine learning systems, the need to curtail their behavior has become increasingly apparent. This is evidenced by recent advancements towards developing models that satisfy robustness, safety, and <b>fairness</b> requirements. These requirements can be imposed (with generalization guarantees) by formulating <b>constrained</b> <b>learning</b> problems that can then be tackled by dual ascent algorithms. Yet, though these algorithms converge in objective value, even in non-convex settings, they cannot guarantee that their outcome is feasible. Doing so requires randomizing over all iterates, which is impractical in virtually any modern applications. Still, final iterates have been observed to perform well in practice. In this work, we address this gap between theory and practice by characterizing the constraint violation of Lagrangian minimizers associated with optimal dual variables, despite lack of convexity. To do this, we leverage the fact that non-convex, finite-dimensional <b>constrained</b> <b>learning</b> problems can be seen as parametrizations of convex, functional problems. Our results show that rich parametrizations effectively mitigate the issue of feasibility in dual methods, shedding light on prior empirical successes of dual learning. We illustrate our findings in fair learning tasks.</p></p class="citation"></blockquote><h3 id=3850--245350-uncertainty-calibrated-test-time-model-adaptation-without-forgetting-mingkui-tan-et-al-2024>(38/50 | 245/350) Uncertainty-Calibrated Test-Time Model Adaptation without Forgetting (Mingkui Tan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mingkui Tan, Guohao Chen, Jiaxiang Wu, Yifan Zhang, Yaofo Chen, Peilin Zhao, Shuaicheng Niu. (2024)<br><strong>Uncertainty-Calibrated Test-Time Model Adaptation without Forgetting</strong><br><button class=copy-to-clipboard title="Uncertainty-Calibrated Test-Time Model Adaptation without Forgetting" index=245>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-245 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Distribution Shift, Distribution Shift, Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11491v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11491v1.pdf filename=2403.11491v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Test-time adaptation (TTA) seeks to tackle potential <b>distribution</b> <b>shifts</b> between training and test data by adapting a given model w.r.t. any test sample. Although recent TTA has shown promising performance, we still face two key challenges: 1) prior methods perform backpropagation for each test sample, resulting in unbearable optimization costs to many applications; 2) while existing TTA can significantly improve the test performance on <b>out-of-distribution</b> data, they often suffer from severe performance degradation on in-distribution data after TTA (known as forgetting). To this end, we have proposed an Efficient Anti-Forgetting Test-Time Adaptation (EATA) method which develops an active sample selection criterion to identify reliable and non-redundant samples for test-time entropy minimization. To alleviate forgetting, EATA introduces a Fisher regularizer estimated from test samples to constrain important model parameters from drastic changes. However, in EATA, the adopted entropy loss consistently assigns higher confidence to predictions even for samples that are underlying uncertain, leading to overconfident predictions. To tackle this, we further propose EATA with Calibration (EATA-C) to separately exploit the reducible model uncertainty and the inherent data uncertainty for calibrated TTA. Specifically, we measure the model uncertainty by the divergence between predictions from the full network and its sub-networks, on which we propose a divergence loss to encourage consistent predictions instead of overconfident ones. To further recalibrate prediction confidence, we utilize the disagreement among predicted labels as an indicator of the data uncertainty, and then devise a min-max entropy regularizer to selectively increase and decrease prediction confidence for different samples. Experiments on image classification and semantic segmentation verify the effectiveness of our methods.</p></p class="citation"></blockquote><h3 id=3950--246350-span-based-optimal-sample-complexity-for-weakly-communicating-and-general-average-reward-mdps-matthew-zurek-et-al-2024>(39/50 | 246/350) Span-Based Optimal Sample Complexity for Weakly Communicating and General Average Reward MDPs (Matthew Zurek et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Matthew Zurek, Yudong Chen. (2024)<br><strong>Span-Based Optimal Sample Complexity for Weakly Communicating and General Average Reward MDPs</strong><br><button class=copy-to-clipboard title="Span-Based Optimal Sample Complexity for Weakly Communicating and General Average Reward MDPs" index=246>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-246 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-IT, cs-LG, cs.LG, math-IT, math-OC, stat-ML<br>Keyword Score: 20<br>Keywords: Markov Decision Process, Markov Decision Process<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11477v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11477v1.pdf filename=2403.11477v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the sample complexity of learning an $\epsilon$-optimal policy in an average-reward <b>Markov</b> <b>decision</b> <b>process</b> (MDP) under a generative model. For weakly communicating <b>MDPs,</b> we establish the complexity bound $\tilde{O}(SA\frac{H}{\epsilon^2})$, where $H$ is the span of the bias function of the optimal policy and $SA$ is the cardinality of the state-action space. Our result is the first that is minimax optimal (up to log factors) in all parameters $S,A,H$ and $\epsilon$, improving on existing work that either assumes uniformly bounded mixing times for all policies or has suboptimal dependence on the parameters. We further investigate sample complexity in general (non-weakly-communicating) average-reward <b>MDPs.</b> We argue a new transient time parameter $B$ is necessary, establish an $\tilde{O}(SA\frac{B+H}{\epsilon^2})$ complexity bound, and prove a matching (up to log factors) minimax lower bound. Both results are based on reducing the average-reward MDP to a discounted MDP, which requires new ideas in the general setting. To establish the optimality of this reduction, we develop improved bounds for $\gamma$-discounted <b>MDPs,</b> showing that $\tilde{\Omega}\left(SA\frac{H}{(1-\gamma)^2\epsilon^2}\right)$ samples suffice to learn an $\epsilon$-optimal policy in weakly communicating <b>MDPs</b> under the regime that $\gamma\geq 1-1/H$, and $\tilde{\Omega}\left(SA\frac{B+H}{(1-\gamma)^2\epsilon^2}\right)$ samples suffice in general <b>MDPs</b> when $\gamma\geq 1-\frac{1}{B+H}$. Both these results circumvent the well-known lower bound of $\tilde{\Omega}\left(SA\frac{1}{(1-\gamma)^3\epsilon^2}\right)$ for arbitrary $\gamma$-discounted <b>MDPs.</b> Our analysis develops upper bounds on certain instance-dependent variance parameters in terms of the span and transient time parameters. The weakly communicating bounds are tighter than those based on the mixing time or diameter of the MDP and may be of broader use.</p></p class="citation"></blockquote><h3 id=4050--247350-fedspu-personalized-federated-learning-for-resource-constrained-devices-with-stochastic-parameter-update-ziru-niu-et-al-2024>(40/50 | 247/350) FedSPU: Personalized Federated Learning for Resource-constrained Devices with Stochastic Parameter Update (Ziru Niu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ziru Niu, Hai Dong, A. K. Qin. (2024)<br><strong>FedSPU: Personalized Federated Learning for Resource-constrained Devices with Stochastic Parameter Update</strong><br><button class=copy-to-clipboard title="FedSPU: Personalized Federated Learning for Resource-constrained Devices with Stochastic Parameter Update" index=247>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-247 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: 68U35, C-2-4; I-2-11, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Federated Learning, Model Pruning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11464v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11464v1.pdf filename=2403.11464v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Personalized <b>Federated</b> <b>Learning</b> (PFL) is widely employed in IoT applications to handle high-volume, non-iid client data while ensuring data privacy. However, heterogeneous edge devices owned by clients may impose varying degrees of resource constraints, causing computation and communication bottlenecks for PFL. <b>Federated</b> <b>Dropout</b> has emerged as a popular strategy to address this challenge, wherein only a subset of the global <b>model,</b> <b>i.e.</b> a \textit{sub-model}, is trained on a client&rsquo;s device, thereby reducing computation and communication overheads. Nevertheless, the dropout-based <b>model-pruning</b> <b>strategy</b> may introduce bias, particularly towards non-iid local data. When biased sub-models absorb highly divergent parameters from other clients, performance degradation becomes inevitable. In response, we propose <b>federated</b> <b>learning</b> with stochastic parameter update (FedSPU). Unlike dropout that tailors the global <b>model</b> <b>to</b> small-size local sub-models, FedSPU maintains the full <b>model</b> <b>architecture</b> on each device but randomly freezes a certain percentage of neurons in the local <b>model</b> <b>during</b> training while updating the remaining neurons. This approach ensures that a portion of the local <b>model</b> <b>remains</b> personalized, thereby enhancing the <b>model&rsquo;s</b> <b>robustness</b> against biased parameters from other clients. Experimental results demonstrate that FedSPU outperforms <b>federated</b> <b>dropout</b> by 7.57% on average in terms of accuracy. Furthermore, an introduced early stopping scheme leads to a significant reduction of the training time by (24.8%\sim70.4%) while maintaining high accuracy.</p></p class="citation"></blockquote><h3 id=4150--248350-large-scale-flood-modeling-and-forecasting-with-floodcast-qingsong-xu-et-al-2024>(41/50 | 248/350) Large-scale flood modeling and forecasting with FloodCast (Qingsong Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qingsong Xu, Yilei Shi, Jonathan Bamber, Chaojun Ouyang, Xiao Xiang Zhu. (2024)<br><strong>Large-scale flood modeling and forecasting with FloodCast</strong><br><button class=copy-to-clipboard title="Large-scale flood modeling and forecasting with FloodCast" index=248>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-248 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG, physics-flu-dyn<br>Keyword Score: 18<br>Keywords: Benchmarking, Geometry, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12226v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12226v1.pdf filename=2403.12226v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Large-scale hydrodynamic models generally rely on fixed-resolution spatial grids and model parameters as well as incurring a high computational cost. This limits their ability to accurately forecast flood crests and issue time-critical hazard warnings. In this work, we build a fast, stable, accurate, resolution-invariant, and <b>geometry-adaptative</b> flood modeling and forecasting framework that can perform at large scales, namely FloodCast. The framework comprises two main modules: multi-satellite observation and hydrodynamic modeling. In the multi-satellite observation module, a real-time <b>unsupervised</b> change detection method and a rainfall processing and analysis tool are proposed to harness the full potential of multi-satellite observations in large-scale flood prediction. In the hydrodynamic modeling module, a <b>geometry-adaptive</b> physics-informed neural solver (GeoPINS) is introduced, benefiting from the absence of a requirement for training data in physics-informed neural networks and featuring a fast, accurate, and resolution-invariant architecture with Fourier neural operators. GeoPINS demonstrates impressive performance on popular PDEs across regular and irregular domains. Building upon GeoPINS, we propose a sequence-to-sequence GeoPINS model to handle long-term temporal series and extensive spatial domains in large-scale flood modeling. Next, we establish a <b>benchmark</b> dataset in the 2022 Pakistan flood to assess various flood prediction methods. Finally, we validate the model in three dimensions - flood inundation range, depth, and transferability of spatiotemporal downscaling. Traditional hydrodynamics and sequence-to-sequence GeoPINS exhibit exceptional agreement during high water levels, while comparative assessments with SAR-based flood depth data show that sequence-to-sequence GeoPINS outperforms traditional hydrodynamics, with smaller prediction errors.</p></p class="citation"></blockquote><h3 id=4250--249350-offline-multitask-representation-learning-for-reinforcement-learning-haque-ishfaq-et-al-2024>(42/50 | 249/350) Offline Multitask Representation Learning for Reinforcement Learning (Haque Ishfaq et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haque Ishfaq, Thanh Nguyen-Tang, Songtao Feng, Raman Arora, Mengdi Wang, Ming Yin, Doina Precup. (2024)<br><strong>Offline Multitask Representation Learning for Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Offline Multitask Representation Learning for Reinforcement Learning" index=249>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-249 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 15<br>Keywords: Reinforcement Learning, Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11574v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11574v1.pdf filename=2403.11574v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study offline multitask <b>representation</b> <b>learning</b> in <b>reinforcement</b> <b>learning</b> (RL), where a learner is provided with an offline dataset from different tasks that share a common <b>representation</b> <b>and</b> is asked to learn the shared <b>representation.</b> <b>We</b> theoretically investigate offline multitask low-rank RL, and propose a new algorithm called MORL for offline multitask <b>representation</b> <b>learning.</b> Furthermore, we examine downstream RL in reward-free, offline and online scenarios, where a new task is introduced to the agent that shares the same <b>representation</b> <b>as</b> the upstream offline tasks. Our theoretical results demonstrate the benefits of using the learned <b>representation</b> <b>from</b> the upstream offline task instead of directly learning the <b>representation</b> <b>of</b> the low-rank model.</p></p class="citation"></blockquote><h3 id=4350--250350-petscml-second-order-solvers-for-training-regression-problems-in-scientific-machine-learning-stefano-zampini-et-al-2024>(43/50 | 250/350) PETScML: Second-order solvers for training regression problems in Scientific Machine Learning (Stefano Zampini et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Stefano Zampini, Umberto Zerbinati, George Turkiyyah, David Keyes. (2024)<br><strong>PETScML: Second-order solvers for training regression problems in Scientific Machine Learning</strong><br><button class=copy-to-clipboard title="PETScML: Second-order solvers for training regression problems in Scientific Machine Learning" index=250>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-250 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: 65K10, 68T07, 65M70, 65Y05, I-2-5; D-2-m; G-4; G-1-6; J-2, cs-LG, cs-MS, cs.LG, math-OC<br>Keyword Score: 10<br>Keywords: Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12188v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12188v1.pdf filename=2403.12188v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, we have witnessed the emergence of scientific machine learning as a data-driven tool for the analysis, by means of deep-learning techniques, of data produced by computational science and engineering applications. At the core of these methods is the <b>supervised</b> training algorithm to learn the neural network realization, a highly non-convex optimization problem that is usually solved using stochastic gradient methods. However, distinct from deep-learning practice, scientific machine-learning training problems feature a much larger volume of smooth data and better characterizations of the empirical risk functions, which make them suited for conventional solvers for unconstrained optimization. We introduce a lightweight software framework built on top of the Portable and Extensible Toolkit for Scientific computation to bridge the gap between deep-learning software and conventional solvers for unconstrained minimization. We empirically demonstrate the superior efficacy of a trust region method based on the Gauss-Newton approximation of the Hessian in improving the generalization errors arising from regression tasks when learning surrogate models for a wide range of scientific machine-learning techniques and test cases. All the conventional second-order solvers tested, including L-BFGS and inexact Newton with line-search, compare favorably, either in terms of cost or accuracy, with the adaptive first-order methods used to validate the surrogate models.</p></p class="citation"></blockquote><h3 id=4450--251350-informed-spectral-normalized-gaussian-processes-for-trajectory-prediction-christian-schlauch-et-al-2024>(44/50 | 251/350) Informed Spectral Normalized Gaussian Processes for Trajectory Prediction (Christian Schlauch et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Christian Schlauch, Christian Wirth, Nadja Klein. (2024)<br><strong>Informed Spectral Normalized Gaussian Processes for Trajectory Prediction</strong><br><button class=copy-to-clipboard title="Informed Spectral Normalized Gaussian Processes for Trajectory Prediction" index=251>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-251 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Continual Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11966v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11966v1.pdf filename=2403.11966v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Prior parameter distributions provide an elegant way to represent prior expert and world knowledge for informed learning. Previous work has shown that using such informative priors to regularize probabilistic deep learning (DL) models increases their performance and data-efficiency. However, commonly used sampling-based approximations for probabilistic DL models can be computationally expensive, requiring multiple inference passes and longer training times. Promising alternatives are compute-efficient last layer kernel approximations like spectral normalized Gaussian processes (SNGPs). We propose a novel regularization-based <b>continual</b> <b>learning</b> method for SNGPs, which enables the use of informative priors that represent prior knowledge learned from previous tasks. Our proposal builds upon well-established methods and requires no rehearsal memory or parameter expansion. We apply our informed SNGP model to the trajectory prediction problem in autonomous driving by integrating prior drivability knowledge. On two public datasets, we investigate its performance under diminishing training data and across locations, and thereby demonstrate an increase in data-efficiency and robustness to location-transfers over non-informed and informed baselines.</p></p class="citation"></blockquote><h3 id=4550--252350-single-agent-actor-critic-for-decentralized-cooperative-driving-shengchao-yan-et-al-2024>(45/50 | 252/350) Single-Agent Actor Critic for Decentralized Cooperative Driving (Shengchao Yan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shengchao Yan, Lukas König, Wolfram Burgard. (2024)<br><strong>Single-Agent Actor Critic for Decentralized Cooperative Driving</strong><br><button class=copy-to-clipboard title="Single-Agent Actor Critic for Decentralized Cooperative Driving" index=252>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-252 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-RO, cs.LG<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11914v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11914v1.pdf filename=2403.11914v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Active traffic management incorporating autonomous vehicles (AVs) promises a future with diminished congestion and enhanced traffic flow. However, developing algorithms for real-world application requires addressing the challenges posed by continuous traffic flow and partial observability. To bridge this gap and advance the field of active traffic management towards greater decentralization, we introduce a novel asymmetric actor-critic model aimed at learning decentralized cooperative driving policies for autonomous vehicles using single-agent <b>reinforcement</b> <b>learning.</b> Our approach employs attention neural networks with masking to handle the dynamic nature of real-world traffic flow and partial observability. Through extensive evaluations against baseline controllers across various traffic scenarios, our model shows great potential for improving traffic flow at diverse bottleneck locations within the road system. Additionally, we explore the challenge associated with the conservative driving behaviors of autonomous vehicles that adhere strictly to traffic regulations. The experiment results illustrate that our proposed cooperative policy can mitigate potential traffic slowdowns without compromising safety.</p></p class="citation"></blockquote><h3 id=4650--253350-crystalformer-infinitely-connected-attention-for-periodic-structure-encoding-tatsunori-taniai-et-al-2024>(46/50 | 253/350) Crystalformer: Infinitely Connected Attention for Periodic Structure Encoding (Tatsunori Taniai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tatsunori Taniai, Ryo Igarashi, Yuta Suzuki, Naoya Chiba, Kotaro Saito, Yoshitaka Ushiku, Kanta Ono. (2024)<br><strong>Crystalformer: Infinitely Connected Attention for Periodic Structure Encoding</strong><br><button class=copy-to-clipboard title="Crystalformer: Infinitely Connected Attention for Periodic Structure Encoding" index=253>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-253 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cond-mat-mtrl-sci, cs-LG, cs.LG, physics-comp-ph<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11686v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11686v1.pdf filename=2403.11686v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Predicting physical properties of materials from their crystal structures is a fundamental problem in materials science. In peripheral areas such as the prediction of molecular properties, fully connected attention networks have been shown to be successful. However, unlike these finite atom arrangements, crystal structures are infinitely repeating, periodic arrangements of atoms, whose fully connected attention results in infinitely connected attention. In this work, we show that this infinitely connected attention can lead to a computationally tractable formulation, interpreted as neural potential summation, that performs infinite interatomic potential summations in a deeply learned feature space. We then propose a simple yet effective <b>Transformer-based</b> encoder architecture for crystal structures called Crystalformer. Compared to an existing <b>Transformer-based</b> model, the proposed model requires only 29.4% of the number of parameters, with minimal modifications to the original <b>Transformer</b> architecture. Despite the architectural simplicity, the proposed method outperforms state-of-the-art methods for various property regression tasks on the Materials Project and JARVIS-DFT datasets.</p></p class="citation"></blockquote><h3 id=4750--254350-the-value-of-reward-lookahead-in-reinforcement-learning-nadav-merlis-et-al-2024>(47/50 | 254/350) The Value of Reward Lookahead in Reinforcement Learning (Nadav Merlis et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nadav Merlis, Dorian Baudry, Vianney Perchet. (2024)<br><strong>The Value of Reward Lookahead in Reinforcement Learning</strong><br><button class=copy-to-clipboard title="The Value of Reward Lookahead in Reinforcement Learning" index=254>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-254 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11637v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11637v1.pdf filename=2403.11637v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In <b>reinforcement</b> <b>learning</b> (RL), agents sequentially interact with changing environments while aiming to maximize the obtained rewards. Usually, rewards are observed only after acting, and so the goal is to maximize the expected cumulative reward. Yet, in many practical settings, reward information is observed in advance &ndash; prices are observed before performing transactions; nearby traffic information is partially known; and goals are oftentimes given to agents prior to the interaction. In this work, we aim to quantifiably analyze the value of such future reward information through the lens of competitive analysis. In particular, we measure the ratio between the value of standard RL agents and that of agents with partial future-reward lookahead. We characterize the worst-case reward distribution and derive exact ratios for the worst-case reward expectations. Surprisingly, the resulting ratios relate to known quantities in offline RL and reward-free exploration. We further provide tight bounds for the ratio given the worst-case dynamics. Our results cover the full spectrum between observing the immediate rewards before acting to observing all the rewards before the interaction starts.</p></p class="citation"></blockquote><h3 id=4850--255350-rl-in-markov-games-with-independent-function-approximation-improved-sample-complexity-bound-under-the-local-access-model-junyi-fan-et-al-2024>(48/50 | 255/350) RL in Markov Games with Independent Function Approximation: Improved Sample Complexity Bound under the Local Access Model (Junyi Fan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junyi Fan, Yuxuan Han, Jialin Zeng, Jian-Feng Cai, Yang Wang, Yang Xiang, Jiheng Zhang. (2024)<br><strong>RL in Markov Games with Independent Function Approximation: Improved Sample Complexity Bound under the Local Access Model</strong><br><button class=copy-to-clipboard title="RL in Markov Games with Independent Function Approximation: Improved Sample Complexity Bound under the Local Access Model" index=255>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-255 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Markov Game<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11544v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11544v2.pdf filename=2403.11544v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Efficiently learning equilibria with large state and action spaces in general-sum <b>Markov</b> <b>games</b> while overcoming the curse of multi-agency is a challenging problem. Recent works have attempted to solve this problem by employing independent linear function classes to approximate the marginal $Q$-value for each agent. However, existing sample complexity bounds under such a framework have a suboptimal dependency on the desired accuracy $\varepsilon$ or the action space. In this work, we introduce a new algorithm, Lin-Confident-FTRL, for learning coarse correlated equilibria (CCE) with local access to the simulator, i.e., one can interact with the underlying environment on the visited states. Up to a logarithmic dependence on the size of the state space, Lin-Confident-FTRL learns $\epsilon$-CCE with a provable optimal accuracy bound $O(\epsilon^{-2})$ and gets rids of the linear dependency on the action space, while scaling polynomially with relevant problem parameters (such as the number of agents and time horizon). Moreover, our analysis of Linear-Confident-FTRL generalizes the virtual policy iteration technique in the single-agent local planning literature, which yields a new computationally efficient algorithm with a tighter sample complexity bound when assuming random access to the simulator.</p></p class="citation"></blockquote><h3 id=4950--256350-methods-for-generating-drift-in-text-streams-cristiano-mesquita-garcia-et-al-2024>(49/50 | 256/350) Methods for Generating Drift in Text Streams (Cristiano Mesquita Garcia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Cristiano Mesquita Garcia, Alessandro Lameiras Koerich, Alceu de Souza Britto Jr, Jean Paul Barddal. (2024)<br><strong>Methods for Generating Drift in Text Streams</strong><br><button class=copy-to-clipboard title="Methods for Generating Drift in Text Streams" index=256>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-256 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-IR, cs-LG, cs.LG<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12328v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12328v1.pdf filename=2403.12328v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Systems and individuals produce data continuously. On the Internet, people share their knowledge, sentiments, and opinions, provide reviews about services and products, and so on. Automatically learning from these textual data can provide insights to organizations and institutions, thus preventing financial impacts, for example. To learn from textual data over time, the machine learning system must account for concept drift. Concept drift is a frequent phenomenon in real-world datasets and corresponds to changes in data distribution over time. For instance, a concept drift occurs when sentiments change or a word&rsquo;s meaning is adjusted over time. Although concept drift is frequent in real-world applications, <b>benchmark</b> datasets with labeled drifts are rare in the literature. To bridge this gap, this paper provides four textual drift generation methods to ease the production of datasets with labeled drifts. These methods were applied to Yelp and Airbnb datasets and tested using incremental classifiers respecting the stream mining paradigm to evaluate their ability to recover from the drifts. Results show that all methods have their performance degraded right after the drifts, and the incremental SVM is the fastest to run and recover the previous performance levels regarding accuracy and Macro F1-Score.</p></p class="citation"></blockquote><h3 id=5050--257350-accelerating-string-key-learned-index-structures-via-memoization-based-incremental-training-minsu-kim-et-al-2024>(50/50 | 257/350) Accelerating String-Key Learned Index Structures via Memoization-based Incremental Training (Minsu Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Minsu Kim, Jinwoo Hwang, Guseul Heo, Seiyeon Cho, Divya Mahajan, Jongse Park. (2024)<br><strong>Accelerating String-Key Learned Index Structures via Memoization-based Incremental Training</strong><br><button class=copy-to-clipboard title="Accelerating String-Key Learned Index Structures via Memoization-based Incremental Training" index=257>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-257 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AR, cs-DB, cs-LG, cs.LG<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11472v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11472v1.pdf filename=2403.11472v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Learned indexes use machine learning models to learn the mappings between keys and their corresponding positions in key-value indexes. These indexes use the mapping information as training data. Learned indexes require frequent retrainings of their models to incorporate the changes introduced by update queries. To efficiently retrain the models, existing learned index systems often harness a linear algebraic QR factorization technique that performs matrix decomposition. This factorization approach processes all key-position pairs during each retraining, resulting in compute operations that grow linearly with the total number of keys and their lengths. Consequently, the retrainings create a severe performance bottleneck, especially for variable-length string keys, while the retrainings are crucial for maintaining high prediction accuracy and in turn, ensuring low query service latency. To address this performance problem, we develop an algorithm-hardware co-designed string-key learned index system, dubbed SIA. In designing SIA, we leverage a unique algorithmic property of the matrix decomposition-based training method. Exploiting the property, we develop a memoization-based incremental training scheme, which only requires computation over updated keys, while decomposition results of non-updated keys from previous computations can be reused. We further enhance SIA to offload a portion of this training process to an FPGA accelerator to not only relieve CPU resources for serving index queries (i.e., inference), but also accelerate the training itself. Our evaluation shows that compared to ALEX, LIPP, and SIndex, a state-of-the-art learned index systems, SIA-accelerated learned indexes offer 2.6x and 3.4x higher throughput on the two real-world <b>benchmark</b> suites, YCSB and Twitter cache trace, respectively.</p></p class="citation"></blockquote><h2 id=csai-9>cs.AI (9)</h2><h3 id=19--258350-how-far-are-we-on-the-decision-making-of-llms-evaluating-llms-gaming-ability-in-multi-agent-environments-jen-tse-huang-et-al-2024>(1/9 | 258/350) How Far Are We on the Decision-Making of LLMs? Evaluating LLMs&rsquo; Gaming Ability in Multi-Agent Environments (Jen-tse Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jen-tse Huang, Eric John Li, Man Ho Lam, Tian Liang, Wenxuan Wang, Youliang Yuan, Wenxiang Jiao, Xing Wang, Zhaopeng Tu, Michael R. Lyu. (2024)<br><strong>How Far Are We on the Decision-Making of LLMs? Evaluating LLMs&rsquo; Gaming Ability in Multi-Agent Environments</strong><br><button class=copy-to-clipboard title="How Far Are We on the Decision-Making of LLMs? Evaluating LLMs' Gaming Ability in Multi-Agent Environments" index=258>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-258 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs.AI<br>Keyword Score: 60<br>Keywords: GPT, GPT-3, GPT-3.5, GPT-4, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11807v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11807v1.pdf filename=2403.11807v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Decision-making, a complicated task requiring various types of abilities, presents an excellent framework for assessing <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> Our research investigates <b>LLMs&rsquo;</b> decision-making capabilities through the lens of a well-established field, Game Theory. We focus specifically on games that support the participation of more than two agents simultaneously. Subsequently, we introduce our framework, GAMA-Bench, including eight classical multi-agent games. We design a scoring scheme to assess a model&rsquo;s performance in these games quantitatively. Through GAMA-Bench, we investigate <b>LLMs&rsquo;</b> robustness, generalizability, and enhancement strategies. Results reveal that while <b>GPT-3.5</b> shows satisfying robustness, its generalizability is relatively limited. However, its performance can be improved through approaches such as Chain-of-Thought. Additionally, we conduct evaluations across various <b>LLMs</b> and find that <b>GPT-4</b> outperforms other models on GAMA-Bench, achieving a score of 72.5. Moreover, the increasingly higher scores across the three iterations of <b>GPT-3.5</b> (0613, 1106, 0125) demonstrate marked advancements in the model&rsquo;s intelligence with each update. The code and experimental results are made publicly available via <a href=https://github.com/CUHK-ARISE/GAMABench>https://github.com/CUHK-ARISE/GAMABench</a>.</p></p class="citation"></blockquote><h3 id=29--259350-can-llm-augmented-autonomous-agents-cooperate-an-evaluation-of-their-cooperative-capabilities-through-melting-pot-manuel-mosquera-et-al-2024>(2/9 | 259/350) Can LLM-Augmented autonomous agents cooperate?, An evaluation of their cooperative capabilities through Melting Pot (Manuel Mosquera et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Manuel Mosquera, Juan Sebastian Pinzon, Manuel Rios, Yesid Fonseca, Luis Felipe Giraldo, Nicanor Quijano, Ruben Manrique. (2024)<br><strong>Can LLM-Augmented autonomous agents cooperate?, An evaluation of their cooperative capabilities through Melting Pot</strong><br><button class=copy-to-clipboard title="Can LLM-Augmented autonomous agents cooperate?, An evaluation of their cooperative capabilities through Melting Pot" index=259>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-259 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs.AI<br>Keyword Score: 50<br>Keywords: GPT-3, GPT-3.5, GPT-4, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11381v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11381v1.pdf filename=2403.11381v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As the field of AI continues to evolve, a significant dimension of this progression is the development of <b>Large</b> <b>Language</b> <b>Models</b> and their potential to enhance multi-agent artificial intelligence systems. This paper explores the cooperative capabilities of <b>Large</b> <b>Language</b> <b>Model-augmented</b> Autonomous Agents (LAAs) using the well-known Meltin Pot environments along with reference models such as <b>GPT4</b> and <b>GPT3.5.</b> Preliminary results suggest that while these agents demonstrate a propensity for cooperation, they still struggle with effective collaboration in given environments, emphasizing the need for more robust architectures. The study&rsquo;s contributions include an abstraction layer to adapt Melting Pot game scenarios for <b>LLMs,</b> the implementation of a reusable architecture for <b>LLM-mediated</b> agent development - which includes short and long-term memories and different cognitive modules, and the evaluation of cooperation capabilities using a set of metrics tied to the Melting Pot&rsquo;s &ldquo;Commons Harvest&rdquo; game. The paper closes, by discussing the limitations of the current architectural framework and the potential of a new set of modules that fosters better cooperation among LAAs.</p></p class="citation"></blockquote><h3 id=39--260350-compositional-learning-of-functions-in-humans-and-machines-yanli-zhou-et-al-2024>(3/9 | 260/350) Compositional learning of functions in humans and machines (Yanli Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yanli Zhou, Brenden M. Lake, Adina Williams. (2024)<br><strong>Compositional learning of functions in humans and machines</strong><br><button class=copy-to-clipboard title="Compositional learning of functions in humans and machines" index=260>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-260 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-HC, cs-LG, cs.AI<br>Keyword Score: 40<br>Keywords: Meta Learning, Zero-shot, Transformer, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12201v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12201v1.pdf filename=2403.12201v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The ability to learn and compose functions is foundational to efficient learning and <b>reasoning</b> in humans, enabling flexible generalizations such as creating new dishes from known cooking processes. Beyond sequential chaining of functions, existing linguistics literature indicates that humans can grasp more complex compositions with interacting functions, where output production depends on context changes induced by different function orderings. Extending the investigation into the visual domain, we developed a function learning paradigm to explore the capacity of humans and neural network models in learning and <b>reasoning</b> with compositional functions under varied interaction conditions. Following brief training on individual functions, human participants were assessed on composing two learned functions, in ways covering four main interaction types, including instances in which the application of the first function creates or removes the context for applying the second function. Our findings indicate that humans can make <b>zero-shot</b> generalizations on novel visual function compositions across interaction conditions, demonstrating sensitivity to contextual changes. A comparison with a neural network model on the same task reveals that, through the <b>meta-learning</b> <b>for</b> compositionality (MLC) approach, a standard sequence-to-sequence <b>Transformer</b> can mimic human generalization patterns in composing functions.</p></p class="citation"></blockquote><h3 id=49--261350-fusing-domain-specific-content-from-large-language-models-into-knowledge-graphs-for-enhanced-zero-shot-object-state-classification-filippos-gouidis-et-al-2024>(4/9 | 261/350) Fusing Domain-Specific Content from Large Language Models into Knowledge Graphs for Enhanced Zero Shot Object State Classification (Filippos Gouidis et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Filippos Gouidis, Katerina Papantoniou, Konstantinos Papoutsakis Theodore Patkos, Antonis Argyros, Dimitris Plexousakis. (2024)<br><strong>Fusing Domain-Specific Content from Large Language Models into Knowledge Graphs for Enhanced Zero Shot Object State Classification</strong><br><button class=copy-to-clipboard title="Fusing Domain-Specific Content from Large Language Models into Knowledge Graphs for Enhanced Zero Shot Object State Classification" index=261>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-261 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs-CV, cs-LG, cs.AI<br>Keyword Score: 38<br>Keywords: Graph, Knowledge Graph, Zero-shot, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12151v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12151v1.pdf filename=2403.12151v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Domain-specific <b>knowledge</b> <b>can</b> significantly contribute to addressing a wide variety of vision tasks. However, the generation of such <b>knowledge</b> <b>entails</b> considerable human labor and time costs. This study investigates the potential of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> in generating and providing domain-specific information through semantic embeddings. To achieve this, an <b>LLM</b> is integrated into a pipeline that utilizes <b>Knowledge</b> <b>Graphs</b> and pre-trained semantic vectors in the context of the Vision-based <b>Zero-shot</b> Object State Classification task. We thoroughly examine the behavior of the <b>LLM</b> through an extensive ablation study. Our findings reveal that the integration of <b>LLM-based</b> embeddings, in combination with general-purpose pre-trained embeddings, leads to substantial performance improvements. Drawing insights from this ablation study, we conduct a comparative analysis against competing models, thereby highlighting the state-of-the-art performance achieved by the proposed approach.</p></p class="citation"></blockquote><h3 id=59--262350-gradient-based-fuzzy-system-optimisation-via-automatic-differentiation----fuzzyr-as-a-use-case-chao-chen-et-al-2024>(5/9 | 262/350) Gradient-based Fuzzy System Optimisation via Automatic Differentiation &ndash; FuzzyR as a Use Case (Chao Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chao Chen, Christian Wagner, Jonathan M. Garibaldi. (2024)<br><strong>Gradient-based Fuzzy System Optimisation via Automatic Differentiation &ndash; FuzzyR as a Use Case</strong><br><button class=copy-to-clipboard title="Gradient-based Fuzzy System Optimisation via Automatic Differentiation -- FuzzyR as a Use Case" index=262>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-262 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 20<br>Keywords: Explainable AI, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12308v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12308v1.pdf filename=2403.12308v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Since their introduction, fuzzy sets and systems have become an important area of research known for its versatility in modelling, knowledge representation and <b>reasoning,</b> and increasingly its potential within the context <b>explainable</b> <b>AI.</b> While the applications of fuzzy systems are diverse, there has been comparatively little advancement in their design from a machine learning perspective. In other words, while representations such as neural networks have benefited from a boom in learning capability driven by an increase in computational performance in combination with advances in their training mechanisms and available tool, in particular gradient descent, the impact on fuzzy system design has been limited. In this paper, we discuss gradient-descent-based optimisation of fuzzy systems, focussing in particular on automatic differentiation &ndash; crucial to neural network learning &ndash; with a view to free fuzzy system designers from intricate derivative computations, allowing for more focus on the functional and explainability aspects of their design. As a starting point, we present a use case in FuzzyR which demonstrates how current fuzzy inference system implementations can be adjusted to leverage powerful features of automatic differentiation tools sets, discussing its potential for the future of fuzzy system design.</p></p class="citation"></blockquote><h3 id=69--263350-learning-general-policies-for-classical-planning-domains-getting-beyond-c_2-simon-ståhlberg-et-al-2024>(6/9 | 263/350) Learning General Policies for Classical Planning Domains: Getting Beyond C$_2$ (Simon Ståhlberg et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Simon Ståhlberg, Blai Bonet, Hector Geffner. (2024)<br><strong>Learning General Policies for Classical Planning Domains: Getting Beyond C$_2$</strong><br><button class=copy-to-clipboard title="Learning General Policies for Classical Planning Domains: Getting Beyond C$_2$" index=263>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-263 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LG, cs.AI<br>Keyword Score: 20<br>Keywords: Graph Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11734v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11734v1.pdf filename=2403.11734v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>GNN-based</b> approaches for learning general policies across planning domains are limited by the expressive power of $C_2$, namely; first-order logic with two variables and counting. This limitation can be overcomed by transitioning to $k$-GNNs, for $k=3$, wherein object embeddings are substituted with triplet embeddings. Yet, while $3$-GNNs have the expressive power of $C_3$, unlike $1$- and $2$-GNNs that are confined to $C_2$, they require quartic time for message exchange and cubic space for embeddings, rendering them impractical. In this work, we introduce a parameterized version of relational <b>GNNs.</b> When $t$ is infinity, R-GNN[$t$] approximates $3$-GNNs using only quadratic space for embeddings. For lower values of $t$, such as $t=1$ and $t=2$, R-GNN[$t$] achieves a weaker approximation by exchanging fewer messages, yet interestingly, often yield the $C_3$ features required in several planning domains. Furthermore, the new R-GNN[$t$] architecture is the original R-GNN architecture with a suitable transformation applied to the input states only. Experimental results illustrate the clear performance gains of R-GNN[$1$] and R-GNN[$2$] over plain R-GNNs, and also over edge <b>transformers</b> that also approximate $3$-GNNs.</p></p class="citation"></blockquote><h3 id=79--264350-turkingbench-a-challenge-benchmark-for-web-agents-kevin-xu-et-al-2024>(7/9 | 264/350) Tur[k]ingBench: A Challenge Benchmark for Web Agents (Kevin Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kevin Xu, Yeganeh Kordi, Kate Sanders, Yizhong Wang, Adam Byerly, Jack Zhang, Benjamin Van Durme, Daniel Khashabi. (2024)<br><strong>Tur[k]ingBench: A Challenge Benchmark for Web Agents</strong><br><button class=copy-to-clipboard title="Tur[k]ingBench: A Challenge Benchmark for Web Agents" index=264>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-264 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs-CV, cs-HC, cs.AI<br>Keyword Score: 16<br>Keywords: Benchmarking, Multi-modal, Chatbot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11905v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11905v1.pdf filename=2403.11905v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent <b>chatbots</b> have demonstrated impressive ability to understand and communicate in raw-text form. However, there is more to the world than raw text. For example, humans spend long hours of their time on web pages, where text is intertwined with other modalities and tasks are accomplished in the form of various complex interactions. Can state-of-the-art <b>multi-modal</b> models generalize to such complex domains? To address this question, we introduce TurkingBench, a <b>benchmark</b> of tasks formulated as web pages containing textual instructions with <b>multi-modal</b> context. Unlike existing work which employs artificially synthesized web pages, here we use natural HTML pages that were originally designed for crowdsourcing workers for various annotation purposes. The HTML instructions of each task are also instantiated with various values (obtained from the crowdsourcing tasks) to form new instances of the task. This <b>benchmark</b> contains 32.2K instances distributed across 158 tasks. Additionally, to facilitate the evaluation on TurkingBench, we develop an evaluation framework that connects the responses of <b>chatbots</b> to modifications on web pages (modifying a text box, checking a radio, etc.). We evaluate the performance of state-of-the-art models, including language-only, vision-only, and layout-only models, and their combinations, on this <b>benchmark.</b> Our findings reveal that these models perform significantly better than random chance, yet considerable room exists for improvement. We hope this <b>benchmark</b> will help facilitate the evaluation and development of web-based agents.</p></p class="citation"></blockquote><h3 id=89--265350-guiding-the-generation-of-counterfactual-explanations-through-temporal-background-knowledge-for-predictive-process-monitoring-andrei-buliga-et-al-2024>(8/9 | 265/350) Guiding the generation of counterfactual explanations through temporal background knowledge for Predictive Process Monitoring (Andrei Buliga et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andrei Buliga, Chiara Di Francescomarino, Chiara Ghidini, Ivan Donadello, Fabrizio Maria Maggi. (2024)<br><strong>Guiding the generation of counterfactual explanations through temporal background knowledge for Predictive Process Monitoring</strong><br><button class=copy-to-clipboard title="Guiding the generation of counterfactual explanations through temporal background knowledge for Predictive Process Monitoring" index=265>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-265 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LG, cs.AI<br>Keyword Score: 10<br>Keywords: Counter-factual<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11642v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11642v1.pdf filename=2403.11642v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Counterfactual</b> explanations suggest what should be different in the input instance to change the outcome of an AI system. When dealing with <b>counterfactual</b> explanations in the field of Predictive Process Monitoring, however, control flow relationships among events have to be carefully considered. A <b>counterfactual,</b> indeed, should not violate control flow relationships among activities (temporal background knowledege). Within the field of Explainability in Predictive Process Monitoring, there have been a series of works regarding <b>counterfactual</b> explanations for outcome-based predictions. However, none of them consider the inclusion of temporal background knowledge when generating these <b>counterfactuals.</b> In this work, we adapt state-of-the-art techniques for <b>counterfactual</b> generation in the domain of XAI that are based on genetic algorithms to consider a series of temporal constraints at runtime. We assume that this temporal background knowledge is given, and we adapt the fitness function, as well as the crossover and mutation operators, to maintain the satisfaction of the constraints. The proposed methods are evaluated with respect to state-of-the-art genetic algorithms for <b>counterfactual</b> generation and the results are presented. We showcase that the inclusion of temporal background knowledge allows the generation of <b>counterfactuals</b> more conformant to the temporal background knowledge, without however losing in terms of the <b>counterfactual</b> traditional quality metrics.</p></p class="citation"></blockquote><h3 id=99--266350-does-ai-help-humans-make-better-decisions-a-methodological-framework-for-experimental-evaluation-eli-ben-michael-et-al-2024>(9/9 | 266/350) Does AI help humans make better decisions? A methodological framework for experimental evaluation (Eli Ben-Michael et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Eli Ben-Michael, D. James Greiner, Melody Huang, Kosuke Imai, Zhichao Jiang, Sooahn Shin. (2024)<br><strong>Does AI help humans make better decisions? A methodological framework for experimental evaluation</strong><br><button class=copy-to-clipboard title="Does AI help humans make better decisions? A methodological framework for experimental evaluation" index=266>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-266 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI, econ-GN, q-fin-EC, stat-AP, stat-ME<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12108v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12108v1.pdf filename=2403.12108v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The use of Artificial Intelligence (AI) based on data-driven algorithms has become ubiquitous in today&rsquo;s society. Yet, in many cases and especially when stakes are high, humans still make final decisions. The critical question, therefore, is whether AI helps humans make better decisions as compared to a human alone or AI an alone. We introduce a new methodological framework that can be used to answer experimentally this question with no additional assumptions. We measure a decision maker&rsquo;s ability to make correct decisions using standard classification metrics based on the baseline potential outcome. We consider a single-blinded experimental design, in which the provision of AI-generated <b>recommendations</b> is randomized across cases with a human making final decisions. Under this experimental design, we show how to compare the performance of three alternative decision-making systems&ndash;human-alone, human-with-AI, and AI-alone. We apply the proposed methodology to the data from our own randomized controlled trial of a pretrial risk assessment instrument. We find that AI <b>recommendations</b> do not improve the classification accuracy of a judge&rsquo;s decision to impose cash bail. Our analysis also shows that AI-alone decisions generally perform worse than human decisions with or without AI assistance. Finally, AI <b>recommendations</b> tend to impose cash bail on non-white arrestees more often than necessary when compared to white arrestees.</p></p class="citation"></blockquote><h2 id=cscy-5>cs.CY (5)</h2><h3 id=15--267350-embracing-the-generative-ai-revolution-advancing-tertiary-education-in-cybersecurity-with-gpt-raza-nowrozy-et-al-2024>(1/5 | 267/350) Embracing the Generative AI Revolution: Advancing Tertiary Education in Cybersecurity with GPT (Raza Nowrozy et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Raza Nowrozy, David Jam. (2024)<br><strong>Embracing the Generative AI Revolution: Advancing Tertiary Education in Cybersecurity with GPT</strong><br><button class=copy-to-clipboard title="Embracing the Generative AI Revolution: Advancing Tertiary Education in Cybersecurity with GPT" index=267>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-267 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-AI, cs-CY, cs.CY<br>Keyword Score: 60<br>Keywords: Generative AI, Recommendation, BLOOM, ChatGPT, GPT, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11402v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11402v1.pdf filename=2403.11402v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rapid advancement of <b>generative</b> <b>Artificial</b> Intelligence (AI) technologies, particularly <b>Generative</b> <b>Pre-trained</b> <b>Transformer</b> <b>(GPT)</b> models such as <b>ChatGPT,</b> has the potential to significantly impact cybersecurity. In this study, we investigated the impact of <b>GPTs,</b> specifically <b>ChatGPT,</b> on tertiary education in cybersecurity, and provided <b>recommendations</b> for universities to adapt their curricula to meet the evolving needs of the industry. Our research highlighted the importance of understanding the alignment between <b>GPT&rsquo;s</b> ``mental model&rsquo;&rsquo; and human cognition, as well as the enhancement of <b>GPT</b> capabilities to human skills based on <b>Bloom&rsquo;s</b> taxonomy. By analyzing current educational practices and the alignment of curricula with industry requirements, we concluded that universities providing practical degrees like cybersecurity should align closely with industry demand and embrace the inevitable <b>generative</b> <b>AI</b> revolution, while applying stringent ethics oversight to safeguard responsible <b>GPT</b> usage. We proposed a set of <b>recommendations</b> focused on updating university curricula, promoting agility within universities, fostering collaboration between academia, industry, and policymakers, and evaluating and assessing educational outcomes.</p></p class="citation"></blockquote><h3 id=25--268350-synthetic-image-generation-in-cyber-influence-operations-an-emergent-threat-melanie-mathys-et-al-2024>(2/5 | 268/350) Synthetic Image Generation in Cyber Influence Operations: An Emergent Threat? (Melanie Mathys et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Melanie Mathys, Marco Willi, Michael Graber, Raphael Meier. (2024)<br><strong>Synthetic Image Generation in Cyber Influence Operations: An Emergent Threat?</strong><br><button class=copy-to-clipboard title="Synthetic Image Generation in Cyber Influence Operations: An Emergent Threat?" index=268>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-268 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: K-4-0; I-2-0; I-4-0, cs-AI, cs-CV, cs-CY, cs.CY<br>Keyword Score: 30<br>Keywords: Diffusion Model, Recommendation, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12207v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12207v1.pdf filename=2403.12207v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The evolution of artificial intelligence (AI) has catalyzed a transformation in digital content generation, with profound implications for cyber influence operations. This report delves into the potential and limitations of generative deep learning models, such as <b>diffusion</b> <b>models,</b> in fabricating convincing synthetic images. We critically assess the accessibility, practicality, and output quality of these tools and their implications in threat scenarios of deception, influence, and subversion. Notably, the report generates content for several hypothetical cyber influence operations to demonstrate the current capabilities and limitations of these AI-driven methods for threat actors. While generative models excel at producing illustrations and non-realistic imagery, creating convincing photo-realistic content remains a significant challenge, limited by computational resources and the necessity for human-guided refinement. Our exploration underscores the delicate balance between technological advancement and its potential for misuse, <b>prompting</b> <b>recommendations</b> for ongoing research, defense mechanisms, multi-disciplinary collaboration, and policy development. These <b>recommendations</b> aim to leverage AI&rsquo;s potential for positive impact while safeguarding against its risks to the integrity of information, especially in the context of cyber influence.</p></p class="citation"></blockquote><h3 id=35--269350-a-toolbox-for-surfacing-health-equity-harms-and-biases-in-large-language-models-stephen-r-pfohl-et-al-2024>(3/5 | 269/350) A Toolbox for Surfacing Health Equity Harms and Biases in Large Language Models (Stephen R. Pfohl et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Stephen R. Pfohl, Heather Cole-Lewis, Rory Sayres, Darlene Neal, Mercy Asiedu, Awa Dieng, Nenad Tomasev, Qazi Mamunur Rashid, Shekoofeh Azizi, Negar Rostamzadeh, Liam G. McCoy, Leo Anthony Celi, Yun Liu, Mike Schaekermann, Alanna Walton, Alicia Parrish, Chirag Nagpal, Preeti Singh, Akeiylah Dewitt, Philip Mansfield, Sushant Prakash, Katherine Heller, Alan Karthikesalingam, Christopher Semturs, Joelle Barral, Greg Corrado, Yossi Matias, Jamila Smith-Loud, Ivor Horn, Karan Singhal. (2024)<br><strong>A Toolbox for Surfacing Health Equity Harms and Biases in Large Language Models</strong><br><button class=copy-to-clipboard title="A Toolbox for Surfacing Health Equity Harms and Biases in Large Language Models" index=269>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-269 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CL, cs-CY, cs-LG, cs.CY<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12025v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12025v1.pdf filename=2403.12025v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> hold immense promise to serve complex health information needs but also have the potential to introduce harm and exacerbate health disparities. Reliably evaluating equity-related model failures is a critical step toward developing systems that promote health equity. In this work, we present resources and methodologies for surfacing biases with potential to precipitate equity-related harms in long-form, <b>LLM-generated</b> answers to medical questions and then conduct an empirical case study with Med-PaLM 2, resulting in the largest human evaluation study in this area to date. Our contributions include a multifactorial framework for human assessment of <b>LLM-generated</b> answers for biases, and EquityMedQA, a collection of seven newly-released datasets comprising both manually-curated and <b>LLM-generated</b> questions enriched for adversarial queries. Both our human assessment framework and dataset design process are grounded in an iterative participatory approach and review of possible biases in Med-PaLM 2 answers to adversarial queries. Through our empirical study, we find that the use of a collection of datasets curated through a variety of methodologies, coupled with a thorough evaluation protocol that leverages multiple assessment rubric designs and diverse rater groups, surfaces biases that may be missed via narrower evaluation approaches. Our experience underscores the importance of using diverse assessment methodologies and involving raters of varying backgrounds and expertise. We emphasize that while our framework can identify specific forms of bias, it is not sufficient to holistically assess whether the deployment of an AI system promotes equitable health outcomes. We hope the broader community leverages and builds on these tools and methods towards realizing a shared goal of <b>LLMs</b> that promote accessible and equitable healthcare for all.</p></p class="citation"></blockquote><h3 id=45--270350-bangladesh-agricultural-knowledge-graph-enabling-semantic-integration-and-data-driven-analysis--full-version-rudra-pratap-deb-nath-et-al-2024>(4/5 | 270/350) Bangladesh Agricultural Knowledge Graph: Enabling Semantic Integration and Data-driven Analysis&ndash;Full Version (Rudra Pratap Deb Nath et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rudra Pratap Deb Nath, Tithi Rani Das, Tonmoy Chandro Das, S. M. Shafkat Raihan. (2024)<br><strong>Bangladesh Agricultural Knowledge Graph: Enabling Semantic Integration and Data-driven Analysis&ndash;Full Version</strong><br><button class=copy-to-clipboard title="Bangladesh Agricultural Knowledge Graph: Enabling Semantic Integration and Data-driven Analysis--Full Version" index=270>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-270 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: H-3, cs-CY, cs-DB, cs.CY<br>Keyword Score: 18<br>Keywords: Graph, Fairness, Knowledge Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11920v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11920v2.pdf filename=2403.11920v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In Bangladesh, agriculture is a crucial driver for addressing Sustainable Development Goal 1 (No Poverty) and 2 (Zero Hunger), playing a fundamental role in the economy and people&rsquo;s livelihoods. To enhance the sustainability and resilience of the agriculture industry through data-driven insights, the Bangladesh Bureau of Statistics and other organizations consistently collect and publish agricultural data on the Web. Nevertheless, the current datasets encounter various challenges: 1) they are presented in an unsustainable, static, read-only, and aggregated format, 2) they do not conform to the Findability, Accessibility, Interoperability, and Reusability (FAIR) principles, and 3) they do not facilitate interactive analysis and integration with other data sources. In this paper, we present a thorough solution, delineating a systematic procedure for developing BDAKG: a <b>knowledge</b> <b>graph</b> that semantically and analytically integrates agriculture data in Bangladesh. BDAKG incorporates multidimensional semantics, is linked with external <b>knowledge</b> <b>graphs,</b> is compatible with OLAP, and adheres to the FAIR principles. Our experimental evaluation centers on evaluating the integration process and assessing the quality of the resultant <b>knowledge</b> <b>graph</b> in terms of completeness, timeliness, <b>FAIRness,</b> OLAP compatibility and data-driven analysis. Our federated data analysis recommend a strategic approach focused on decreasing CO$_2$ emissions, fostering economic growth, and promoting sustainable forestry.</p></p class="citation"></blockquote><h3 id=55--271350-analyzing-evaluating-creating-assessing-computational-thinking-and-problem-solving-in-visual-programming-domains-ahana-ghosh-et-al-2024>(5/5 | 271/350) Analyzing-Evaluating-Creating: Assessing Computational Thinking and Problem Solving in Visual Programming Domains (Ahana Ghosh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ahana Ghosh, Liina Malva, Adish Singla. (2024)<br><strong>Analyzing-Evaluating-Creating: Assessing Computational Thinking and Problem Solving in Visual Programming Domains</strong><br><button class=copy-to-clipboard title="Analyzing-Evaluating-Creating: Assessing Computational Thinking and Problem Solving in Visual Programming Domains" index=271>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-271 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CY, cs.CY<br>Keyword Score: 10<br>Keywords: BLOOM<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12227v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12227v1.pdf filename=2403.12227v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Computational thinking (CT) and problem-solving skills are increasingly integrated into K-8 school curricula worldwide. Consequently, there is a growing need to develop reliable assessments for measuring students&rsquo; proficiency in these skills. Recent works have proposed tests for assessing these skills across various CT concepts and practices, in particular, based on multi-choice items enabling psychometric validation and usage in large-scale studies. Despite their practical relevance, these tests are limited in how they measure students&rsquo; computational creativity, a crucial ability when applying CT and problem solving in real-world settings. In our work, we have developed ACE, a novel test focusing on the three higher cognitive levels in <b>Bloom&rsquo;s</b> Taxonomy, i.e., Analyze, Evaluate, and Create. ACE comprises a diverse set of 7x3 multi-choice items spanning these three levels, grounded in elementary block-based visual programming. We evaluate the psychometric properties of ACE through a study conducted with 371 students in grades 3-7 from 10 schools. Based on several psychometric analysis frameworks, our results confirm the reliability and validity of ACE. Our study also shows a positive correlation between students&rsquo; performance on ACE and performance on Hour of Code: Maze Challenge by Code.org.</p></p class="citation"></blockquote><h2 id=eessiv-13>eess.IV (13)</h2><h3 id=113--272350-hypervq-mlr-based-vector-quantization-in-hyperbolic-space-nabarun-goswami-et-al-2024>(1/13 | 272/350) HyperVQ: MLR-based Vector Quantization in Hyperbolic Space (Nabarun Goswami et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nabarun Goswami, Yusuke Mukuta, Tatsuya Harada. (2024)<br><strong>HyperVQ: MLR-based Vector Quantization in Hyperbolic Space</strong><br><button class=copy-to-clipboard title="HyperVQ: MLR-based Vector Quantization in Hyperbolic Space" index=272>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-272 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-LG, eess-IV, eess.IV<br>Keyword Score: 58<br>Keywords: Autoencoder, Clustering, Logistic Regression, Quantization, Representation Learning, Variational Autoencoder, Tokenization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13015v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13015v1.pdf filename=2403.13015v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The success of models operating on tokenized data has led to an increased demand for effective <b>tokenization</b> methods, particularly when applied to vision or auditory tasks, which inherently involve non-discrete data. One of the most popular <b>tokenization</b> methods is Vector <b>Quantization</b> (VQ), a key component of several recent state-of-the-art methods across various domains. Typically, a VQ <b>Variational</b> <b>Autoencoder</b> (VQVAE) is trained to transform data to and from its tokenized <b>representation.</b> <b>However,</b> since the VQVAE is trained with a reconstruction objective, there is no constraint for the embeddings to be well disentangled, a crucial aspect for using them in discriminative tasks. Recently, several works have demonstrated the benefits of utilizing hyperbolic spaces for <b>representation</b> <b>learning.</b> Hyperbolic spaces induce compact latent <b>representations</b> <b>due</b> to their exponential volume growth and inherent ability to model hierarchical and structured data. In this work, we explore the use of hyperbolic spaces for vector <b>quantization</b> (HyperVQ), formulating the VQ operation as a hyperbolic Multinomial <b>Logistic</b> <b>Regression</b> (MLR) problem, in contrast to the Euclidean K-Means <b>clustering</b> used in VQVAE. Through extensive experiments, we demonstrate that hyperVQ performs comparably in reconstruction and generative tasks while outperforming VQ in discriminative tasks and learning a highly disentangled latent space.</p></p class="citation"></blockquote><h3 id=213--273350-mlvicx-multi-level-variance-covariance-exploration-for-chest-x-ray-self-supervised-representation-learning-azad-singh-et-al-2024>(2/13 | 273/350) MLVICX: Multi-Level Variance-Covariance Exploration for Chest X-ray Self-Supervised Representation Learning (Azad Singh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Azad Singh, Vandan Gorade, Deepak Mishra. (2024)<br><strong>MLVICX: Multi-Level Variance-Covariance Exploration for Chest X-ray Self-Supervised Representation Learning</strong><br><button class=copy-to-clipboard title="MLVICX: Multi-Level Variance-Covariance Exploration for Chest X-ray Self-Supervised Representation Learning" index=273>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-273 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-AI, cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 35<br>Keywords: Fine-tuning, Representation Learning, Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11504v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11504v1.pdf filename=2403.11504v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Self-supervised</b> <b>learning</b> (SSL) is potentially useful in reducing the need for manual annotation and making deep learning models accessible for medical image analysis tasks. By leveraging the <b>representations</b> <b>learned</b> from unlabeled data, <b>self-supervised</b> <b>models</b> perform well on tasks that require little to no <b>fine-tuning.</b> However, for medical images, like chest X-rays, which are characterized by complex anatomical structures and diverse clinical conditions, there arises a need for <b>representation</b> <b>learning</b> techniques that can encode fine-grained details while preserving the broader contextual information. In this context, we introduce MLVICX (Multi-Level Variance-Covariance Exploration for Chest X-ray <b>Self-Supervised</b> <b>Representation</b> <b>Learning),</b> an approach to capture rich <b>representations</b> <b>in</b> the form of embeddings from chest X-ray images. Central to our approach is a novel multi-level variance and covariance exploration strategy that empowers the model to detect diagnostically meaningful patterns while reducing redundancy effectively. By enhancing the variance and covariance of the learned embeddings, MLVICX promotes the retention of critical medical insights by adapting both global and local contextual details. We demonstrate the performance of MLVICX in advancing <b>self-supervised</b> <b>chest</b> X-ray <b>representation</b> <b>learning</b> through comprehensive experiments. The performance enhancements we observe across various downstream tasks highlight the significance of the proposed approach in enhancing the utility of chest X-ray embeddings for precision medical diagnosis and comprehensive image analysis. For pertaining, we used the NIH-Chest X-ray dataset, while for downstream tasks, we utilized NIH-Chest X-ray, Vinbig-CXR, RSNA pneumonia, and SIIM-ACR Pneumothorax datasets. Overall, we observe more than 3% performance gains over SOTA SSL approaches in various downstream tasks.</p></p class="citation"></blockquote><h3 id=313--274350-oucopula-bi-channel-multi-label-copula-enhanced-adapter-based-cnn-for-myopia-screening-based-on-ou-uwf-images-yang-li-et-al-2024>(3/13 | 274/350) OUCopula: Bi-Channel Multi-Label Copula-Enhanced Adapter-Based CNN for Myopia Screening Based on OU-UWF Images (Yang Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yang Li, Qiuyi Huang, Chong Zhong, Danjuan Yang, Meiyan Li, A. H. Welsh, Aiyi Liu, Bo Fu, Catherien C. Liu, Xingtao Zhou. (2024)<br><strong>OUCopula: Bi-Channel Multi-Label Copula-Enhanced Adapter-Based CNN for Myopia Screening Based on OU-UWF Images</strong><br><button class=copy-to-clipboard title="OUCopula: Bi-Channel Multi-Label Copula-Enhanced Adapter-Based CNN for Myopia Screening Based on OU-UWF Images" index=274>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-274 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11974v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11974v1.pdf filename=2403.11974v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Myopia screening using cutting-edge ultra-widefield (UWF) fundus imaging is potentially significant for ophthalmic outcomes. Current multidisciplinary research between ophthalmology and deep learning (DL) concentrates primarily on disease classification and diagnosis using single-eye images, largely ignoring joint modeling and prediction for Oculus Uterque (OU, both eyes). Inspired by the complex relationships between OU and the high correlation between the (continuous) outcome labels (Spherical Equivalent and Axial Length), we propose a framework of copula-enhanced adapter <b>convolutional</b> <b>neural</b> <b>network</b> <b>(CNN)</b> learning with OU UWF fundus images (OUCopula) for joint prediction of multiple clinical scores. We design a novel bi-channel multi-label <b>CNN</b> that can (1) take bi-channel image inputs subject to both high correlation and heterogeneity (by sharing the same backbone network and employing adapters to parameterize the channel-wise discrepancy), and (2) incorporate correlation information between continuous output labels (using a copula). Solid experiments show that OUCopula achieves satisfactory performance in myopia score prediction compared to backbone models. Moreover, OUCopula can far exceed the performance of models constructed for single-eye inputs. Importantly, our study also hints at the potential extension of the bi-channel model to a multi-channel paradigm and the generalizability of OUCopula across various backbone <b>CNNs.</b></p></p class="citation"></blockquote><h3 id=413--275350-paon-a-new-neuron-model-using-padé-approximants-onur-keleş-et-al-2024>(4/13 | 275/350) PAON: A New Neuron Model using Padé Approximants (Onur Keleş et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Onur Keleş, A. Murat Tekalp. (2024)<br><strong>PAON: A New Neuron Model using Padé Approximants</strong><br><button class=copy-to-clipboard title="PAON: A New Neuron Model using Padé Approximants" index=275>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-275 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11791v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11791v1.pdf filename=2403.11791v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Convolutional</b> <b>neural</b> <b>networks</b> <b>(CNN)</b> are built upon the classical McCulloch-Pitts neuron model, which is essentially a linear model, where the nonlinearity is provided by a separate activation function. Several researchers have proposed enhanced neuron models, including quadratic neurons, generalized operational neurons, generative neurons, and super neurons, with stronger nonlinearity than that provided by the pointwise activation function. There has also been a proposal to use Pade approximation as a generalized activation function. In this paper, we introduce a brand new neuron model called Pade neurons (Paons), inspired by the Pade approximants, which is the best mathematical approximation of a transcendental function as a ratio of polynomials with different orders. We show that Paons are a super set of all other proposed neuron models. Hence, the basic neuron in any known <b>CNN</b> model can be replaced by Paons. In this paper, we extend the well-known ResNet to PadeNet (built by Paons) to demonstrate the concept. Our experiments on the single-image super-resolution task show that PadeNets can obtain better results than competing architectures.</p></p class="citation"></blockquote><h3 id=513--276350-morestyle-relax-low-frequency-constraint-of-fourier-based-image-reconstruction-in-generalizable-medical-image-segmentation-haoyu-zhao-et-al-2024>(5/13 | 276/350) MoreStyle: Relax Low-frequency Constraint of Fourier-based Image Reconstruction in Generalizable Medical Image Segmentation (Haoyu Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haoyu Zhao, Wenhui Dong, Rui Yu, Zhou Zhao, Du Bo, Yongchao Xu. (2024)<br><strong>MoreStyle: Relax Low-frequency Constraint of Fourier-based Image Reconstruction in Generalizable Medical Image Segmentation</strong><br><button class=copy-to-clipboard title="MoreStyle: Relax Low-frequency Constraint of Fourier-based Image Reconstruction in Generalizable Medical Image Segmentation" index=276>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-276 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 23<br>Keywords: Adversarial Learning, Benchmarking, Data Augmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11689v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11689v2.pdf filename=2403.11689v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The task of single-source domain generalization (SDG) in medical image segmentation is crucial due to frequent domain shifts in clinical image datasets. To address the challenge of poor generalization across different domains, we introduce a Plug-and-Play module for <b>data</b> <b>augmentation</b> called MoreStyle. MoreStyle diversifies image styles by relaxing low-frequency constraints in Fourier space, guiding the image reconstruction network. With the help of <b>adversarial</b> <b>learning,</b> MoreStyle further expands the style range and pinpoints the most intricate style combinations within latent features. To handle significant style variations, we introduce an uncertainty-weighted loss. This loss emphasizes hard-to-classify pixels resulting only from style shifts while mitigating true hard-to-classify pixels in both MoreStyle-generated and original images. Extensive experiments on two widely used <b>benchmarks</b> demonstrate that the proposed MoreStyle effectively helps to achieve good domain generalization ability, and has the potential to further boost the performance of some state-of-the-art SDG methods.</p></p class="citation"></blockquote><h3 id=613--277350-estimation-and-analysis-of-slice-propagation-uncertainty-in-3d-anatomy-segmentation-rachaell-nihalaani-et-al-2024>(6/13 | 277/350) Estimation and Analysis of Slice Propagation Uncertainty in 3D Anatomy Segmentation (Rachaell Nihalaani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rachaell Nihalaani, Tushar Kataria, Jadie Adams, Shireen Y. Elhabian. (2024)<br><strong>Estimation and Analysis of Slice Propagation Uncertainty in 3D Anatomy Segmentation</strong><br><button class=copy-to-clipboard title="Estimation and Analysis of Slice Propagation Uncertainty in 3D Anatomy Segmentation" index=277>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-277 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 20<br>Keywords: Self-supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12290v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12290v1.pdf filename=2403.12290v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Supervised</b> methods for 3D anatomy segmentation demonstrate superior performance but are often limited by the availability of annotated data. This limitation has led to a growing interest in <b>self-supervised</b> approaches in tandem with the abundance of available un-annotated data. Slice propagation has emerged as an <b>self-supervised</b> approach that leverages slice registration as a <b>self-supervised</b> task to achieve full anatomy segmentation with minimal supervision. This approach significantly reduces the need for domain expertise, time, and the cost associated with building fully annotated datasets required for training segmentation networks. However, this shift toward reduced supervision via deterministic networks raises concerns about the trustworthiness and reliability of predictions, especially when compared with more accurate <b>supervised</b> approaches. To address this concern, we propose the integration of calibrated uncertainty quantification (UQ) into slice propagation methods, providing insights into the model&rsquo;s predictive reliability and confidence levels. Incorporating uncertainty measures enhances user confidence in <b>self-supervised</b> approaches, thereby improving their practical applicability. We conducted experiments on three datasets for 3D abdominal segmentation using five UQ methods. The results illustrate that incorporating UQ improves not only model trustworthiness, but also segmentation accuracy. Furthermore, our analysis reveals various failure modes of slice propagation methods that might not be immediately apparent to end-users. This study opens up new research avenues to improve the accuracy and trustworthiness of slice propagation methods.</p></p class="citation"></blockquote><h3 id=713--278350-wia-ld2nd-wavelet-based-image-alignment-for-self-supervised-low-dose-ct-denoising-haoyu-zhao-et-al-2024>(7/13 | 278/350) WIA-LD2ND: Wavelet-based Image Alignment for Self-supervised Low-Dose CT Denoising (Haoyu Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haoyu Zhao, Yuliang Gu, Zhou Zhao, Bo Du, Yongchao Xu, Rui Yu. (2024)<br><strong>WIA-LD2ND: Wavelet-based Image Alignment for Self-supervised Low-Dose CT Denoising</strong><br><button class=copy-to-clipboard title="WIA-LD2ND: Wavelet-based Image Alignment for Self-supervised Low-Dose CT Denoising" index=278>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-278 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 20<br>Keywords: Self-supervised Learning, Weakly-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11672v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11672v2.pdf filename=2403.11672v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In clinical examinations and diagnoses, low-dose computed tomography (LDCT) is crucial for minimizing health risks compared with normal-dose computed tomography (NDCT). However, reducing the radiation dose compromises the signal-to-noise ratio, leading to degraded quality of CT images. To address this, we analyze LDCT denoising task based on experimental results from the frequency perspective, and then introduce a novel <b>self-supervised</b> CT image denoising method called WIA-LD2ND, only using NDCT data. The proposed WIA-LD2ND comprises two modules: Wavelet-based Image Alignment (WIA) and Frequency-Aware Multi-scale Loss (FAM). First, WIA is introduced to align NDCT with LDCT by mainly adding noise to the high-frequency components, which is the main difference between LDCT and NDCT. Second, to better capture high-frequency components and detailed information, Frequency-Aware Multi-scale Loss (FAM) is proposed by effectively utilizing multi-scale feature space. Extensive experiments on two public LDCT denoising datasets demonstrate that our WIA-LD2ND, only uses NDCT, outperforms existing several state-of-the-art <b>weakly-supervised</b> and <b>self-supervised</b> methods.</p></p class="citation"></blockquote><h3 id=813--279350-covid-19-detection-from-ct-scans-using-efficientnet-and-attention-mechanism-ramy-farag-et-al-2024>(8/13 | 279/350) Covid-19 detection from CT scans using EfficientNet and Attention mechanism (Ramy Farag et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ramy Farag, Parth Upadhyay, Guilhermen DeSouza. (2024)<br><strong>Covid-19 detection from CT scans using EfficientNet and Attention mechanism</strong><br><button class=copy-to-clipboard title="Covid-19 detection from CT scans using EfficientNet and Attention mechanism" index=279>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-279 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 20<br>Keywords: Fairness, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11505v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11505v1.pdf filename=2403.11505v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Manual diagnosis and analysis of COVID-19 through the examination of lung Computed Tomography (CT) scan images by physicians tends to result in inefficiency, especially with high patient volumes and numerous images per patient. We address the need for automation by developing a deep learning model-based pipeline for COVID-19 detection from CT scan images of the lungs. The <b>Domain</b> <b>adaptation,</b> Explainability, and <b>Fairness</b> in AI for Medical Image Analysis Workshop and COVID-19 Diagnosis Competition (DEF-AI-MIA COV19D) provides an opportunity to assess our designed pipeline for COVID-19 detection from CT scan images. The proposed pipeline incorporates EfficientNet with an Attention mechanism with a pre-processing step. Our pipeline outperforms last year&rsquo;s teams on the validation set of the competition dataset.</p></p class="citation"></blockquote><h3 id=913--280350-denoisplit-a-method-for-joint-image-splitting-and-unsupervised-denoising-ashesh-ashesh-et-al-2024>(9/13 | 280/350) denoiSplit: a method for joint image splitting and unsupervised denoising (Ashesh Ashesh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ashesh Ashesh, Florian Jug. (2024)<br><strong>denoiSplit: a method for joint image splitting and unsupervised denoising</strong><br><button class=copy-to-clipboard title="denoiSplit: a method for joint image splitting and unsupervised denoising" index=280>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-280 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 13<br>Keywords: Benchmarking, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11854v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11854v1.pdf filename=2403.11854v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work we present denoiSplit, a method to tackle a new analysis task, i.e. the challenge of joint semantic image splitting and <b>unsupervised</b> denoising. This dual approach has important applications in fluorescence microscopy, where semantic image splitting has important applications but noise does generally hinder the downstream analysis of image content. Image splitting involves dissecting an image into its distinguishable semantic structures. We show that the current state-of-the-art method for this task struggles in the presence of image noise, inadvertently also distributing the noise across the predicted outputs. The method we present here can deal with image noise by integrating an <b>unsupervised</b> denoising sub-task. This integration results in improved semantic image unmixing, even in the presence of notable and realistic levels of imaging noise. A key innovation in denoiSplit is the use of specifically formulated noise models and the suitable adjustment of KL-divergence loss for the high-dimensional hierarchical latent space we are training. We showcase the performance of denoiSplit across 4 tasks on real-world microscopy images. Additionally, we perform qualitative and quantitative evaluations and compare results to existing <b>benchmarks,</b> demonstrating the effectiveness of using denoiSplit: a single Variational Splitting Encoder-Decoder (VSE) Network using two suitable noise models to jointly perform semantic splitting and denoising.</p></p class="citation"></blockquote><h3 id=1013--281350-hierarchical-frequency-based-upsampling-and-refining-for-compressed-video-quality-enhancement-qianyu-zhang-et-al-2024>(10/13 | 281/350) Hierarchical Frequency-based Upsampling and Refining for Compressed Video Quality Enhancement (Qianyu Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qianyu Zhang, Bolun Zheng, Xinying Chen, Quan Chen, Zhunjie Zhu, Canjin Wang, Zongpeng Li, Chengang Yan. (2024)<br><strong>Hierarchical Frequency-based Upsampling and Refining for Compressed Video Quality Enhancement</strong><br><button class=copy-to-clipboard title="Hierarchical Frequency-based Upsampling and Refining for Compressed Video Quality Enhancement" index=281>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-281 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 13<br>Keywords: Benchmarking, Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11556v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11556v1.pdf filename=2403.11556v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Video compression artifacts arise due to the <b>quantization</b> operation in the frequency domain. The goal of video quality enhancement is to reduce compression artifacts and reconstruct a visually-pleasant result. In this work, we propose a hierarchical frequency-based upsampling and refining neural network (HFUR) for compressed video quality enhancement. HFUR consists of two modules: implicit frequency upsampling module (ImpFreqUp) and hierarchical and iterative refinement module (HIR). ImpFreqUp exploits DCT-domain prior derived through implicit DCT transform, and accurately reconstructs the DCT-domain loss via a coarse-to-fine transfer. Consequently, HIR is introduced to facilitate cross-collaboration and information compensation between the scales, thus further refine the feature maps and promote the visual quality of the final output. We demonstrate the effectiveness of the proposed modules via ablation experiments and visualized results. Extensive experiments on public <b>benchmarks</b> show that HFUR achieves state-of-the-art performance for both constant bit rate and constant QP modes.</p></p class="citation"></blockquote><h3 id=1113--282350-domain-adaptation-using-pseudo-labels-for-covid-19-detection-runtian-yuan-et-al-2024>(11/13 | 282/350) Domain Adaptation Using Pseudo Labels for COVID-19 Detection (Runtian Yuan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Runtian Yuan, Qingqiu Li, Junlin Hou, Jilan Xu, Yuejie Zhang, Rui Feng, Hao Chen. (2024)<br><strong>Domain Adaptation Using Pseudo Labels for COVID-19 Detection</strong><br><button class=copy-to-clipboard title="Domain Adaptation Using Pseudo Labels for COVID-19 Detection" index=282>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-282 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11498v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11498v1.pdf filename=2403.11498v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In response to the need for rapid and accurate COVID-19 diagnosis during the global pandemic, we present a two-stage framework that leverages pseudo labels for <b>domain</b> <b>adaptation</b> to enhance the detection of COVID-19 from CT scans. By utilizing annotated data from one <b>domain</b> <b>and</b> non-annotated data from another, the model overcomes the challenge of data scarcity and variability, common in emergent health crises. The innovative approach of generating pseudo labels enables the model to iteratively refine its learning process, thereby improving its accuracy and adaptability across different hospitals and medical centres. Experimental results on COV19-CT-DB database showcase the model&rsquo;s potential to achieve high diagnostic precision, significantly contributing to efficient patient management and alleviating the strain on healthcare systems. Our method achieves 0.92 Macro F1 Score on the validation set of Covid-19 <b>domain</b> <b>adaptation</b> challenge.</p></p class="citation"></blockquote><h3 id=1213--283350-divide-and-conquer-posterior-sampling-for-denoising-diffusion-priors-yazid-janati-et-al-2024>(12/13 | 283/350) Divide-and-Conquer Posterior Sampling for Denoising Diffusion Priors (Yazid Janati et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yazid Janati, Alain Durmus, Eric Moulines, Jimmy Olsson. (2024)<br><strong>Divide-and-Conquer Posterior Sampling for Denoising Diffusion Priors</strong><br><button class=copy-to-clipboard title="Divide-and-Conquer Posterior Sampling for Denoising Diffusion Priors" index=283>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-283 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-LG, eess-IV, eess.IV, stat-ML<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11407v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11407v1.pdf filename=2403.11407v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Interest in the use of Denoising <b>Diffusion</b> <b>Models</b> (DDM) as priors for solving inverse Bayesian problems has recently increased significantly. However, sampling from the resulting posterior distribution poses a challenge. To solve this problem, previous works have proposed approximations to bias the drift term of the <b>diffusion.</b> <b>In</b> this work, we take a different approach and utilize the specific structure of the DDM prior to define a set of intermediate and simpler posterior sampling problems, resulting in a lower approximation error compared to previous methods. We empirically demonstrate the reconstruction capability of our method for general linear inverse problems using synthetic examples and various image restoration tasks.</p></p class="citation"></blockquote><h3 id=1313--284350-generalizing-deep-learning-models-for-medical-image-classification-matta-sarah-et-al-2024>(13/13 | 284/350) Generalizing deep learning models for medical image classification (Matta Sarah et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Matta Sarah, Lamard Mathieu, Zhang Philippe, Alexandre Le Guilcher, Laurent Borderie, Béatrice Cochener, Gwenolé Quellec. (2024)<br><strong>Generalizing deep learning models for medical image classification</strong><br><button class=copy-to-clipboard title="Generalizing deep learning models for medical image classification" index=284>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-284 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12167v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12167v1.pdf filename=2403.12167v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Numerous Deep Learning (DL) models have been developed for a large spectrum of medical image analysis applications, which promises to reshape various facets of medical practice. Despite early advances in DL model validation and implementation, which encourage healthcare institutions to adopt them, some fundamental questions remain: are the DL models capable of generalizing? What causes a drop in DL model performances? How to overcome the DL model performance drop? Medical data are dynamic and prone to domain shift, due to multiple factors such as updates to medical equipment, new imaging workflow, and shifts in patient demographics or populations can induce this drift over time. In this paper, we review recent developments in generalization methods for DL-based classification models. We also discuss future challenges, including the need for improved evaluation protocols and <b>benchmarks,</b> and envisioned future developments to achieve robust, generalized models for medical image classification.</p></p class="citation"></blockquote><h2 id=eesssy-9>eess.SY (9)</h2><h3 id=19--285350-context-aware-llm-based-safe-control-against-latent-risks-quan-khanh-luu-et-al-2024>(1/9 | 285/350) Context-aware LLM-based Safe Control Against Latent Risks (Quan Khanh Luu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Quan Khanh Luu, Xiyu Deng, Anh Van Ho, Yorie Nakahira. (2024)<br><strong>Context-aware LLM-based Safe Control Against Latent Risks</strong><br><button class=copy-to-clipboard title="Context-aware LLM-based Safe Control Against Latent Risks" index=285>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-285 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-RO, cs-SY, eess-SY, eess.SY<br>Keyword Score: 50<br>Keywords: Fine-tuning, Stochastic Gradient Descent, Stochastic Gradient Descent, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11863v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11863v1.pdf filename=2403.11863v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>It is challenging for autonomous control systems to perform complex tasks in the presence of latent risks. Motivated by this challenge, this paper proposes an integrated framework that involves <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> <b>stochastic</b> <b>gradient</b> <b>descent</b> <b>(SGD),</b> and optimization-based control. In the first phrase, the proposed framework breaks down complex tasks into a sequence of smaller subtasks, whose specifications account for contextual information and latent risks. In the second phase, these subtasks and their parameters are refined through a dual process involving <b>LLMs</b> and <b>SGD.</b> <b>LLMs</b> are used to generate rough guesses and failure explanations, and <b>SGD</b> is used to <b>fine-tune</b> parameters. The proposed framework is tested using simulated case studies of robots and vehicles. The experiments demonstrate that the proposed framework can mediate actions based on the context and latent risks and learn complex behaviors efficiently.</p></p class="citation"></blockquote><h3 id=29--286350-distill2explain-differentiable-decision-trees-for-explainable-reinforcement-learning-in-energy-application-controllers-gargya-gokhale-et-al-2024>(2/9 | 286/350) Distill2Explain: Differentiable decision trees for explainable reinforcement learning in energy application controllers (Gargya Gokhale et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gargya Gokhale, Seyed Soroush Karimi Madahi, Bert Claessens, Chris Develder. (2024)<br><strong>Distill2Explain: Differentiable decision trees for explainable reinforcement learning in energy application controllers</strong><br><button class=copy-to-clipboard title="Distill2Explain: Differentiable decision trees for explainable reinforcement learning in energy application controllers" index=286>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-286 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-LG, cs-SY, eess-SY, eess.SY<br>Keyword Score: 30<br>Keywords: Human Intervention, Knowledge Distillation, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11907v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11907v1.pdf filename=2403.11907v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Demand-side flexibility is gaining importance as a crucial element in the energy transition process. Accounting for about 25% of final energy consumption globally, the residential sector is an important (potential) source of energy flexibility. However, unlocking this flexibility requires developing a control framework that (1) easily scales across different houses, (2) is easy to maintain, and (3) is simple to understand for end-users. A potential control framework for such a task is data-driven control, specifically model-free <b>reinforcement</b> <b>learning</b> (RL). Such RL-based controllers learn a good control policy by interacting with their environment, learning purely based on data and with minimal <b>human</b> <b>intervention.</b> Yet, they lack explainability, which hampers user acceptance. Moreover, limited hardware capabilities of residential assets forms a hurdle (e.g., using deep neural networks). To overcome both those challenges, we propose a novel method to obtain explainable RL policies by using differentiable decision trees. Using a policy <b>distillation</b> approach, we train these differentiable decision trees to mimic standard RL-based controllers, leading to a decision tree-based control policy that is data-driven and easy to explain. As a proof-of-concept, we examine the performance and explainability of our proposed approach in a battery-based home energy management system to reduce energy costs. For this use case, we show that our proposed approach can outperform baseline rule-based policies by about 20-25%, while providing simple, explainable control policies. We further compare these explainable policies with standard RL policies and examine the performance trade-offs associated with this increased explainability.</p></p class="citation"></blockquote><h3 id=39--287350-the-role-of-extended-horizon-methodology-in-renewable-dense-grids-with-inter-day-long-duration-energy-storage-amogh-a-thatte-et-al-2024>(3/9 | 287/350) The Role of Extended Horizon Methodology in Renewable-Dense Grids With Inter-Day Long-Duration Energy Storage (Amogh A. Thatte et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amogh A. Thatte, Sourabh Dalvi, Vincent Carag, Jiazi Zhang, Jennie Jorgenson, Omar J. Guerra. (2024)<br><strong>The Role of Extended Horizon Methodology in Renewable-Dense Grids With Inter-Day Long-Duration Energy Storage</strong><br><button class=copy-to-clipboard title="The Role of Extended Horizon Methodology in Renewable-Dense Grids With Inter-Day Long-Duration Energy Storage" index=287>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-287 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11379v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11379v1.pdf filename=2403.11379v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study addresses the challenges in optimizing long-duration energy storage (LDES) dispatch within future power systems featuring high integration of variable renewable energy (VRE). The research focuses on conducting a comparative analysis between traditional and extended horizon methods for the optimization of LDES dispatch, using open-source and commercial production cost models (PCMs), tested on a futuristic Electric Reliability Council of Texas (ERCOT) grid. The findings indicate that, despite its complexity and longer solution times, the extended horizon approach demonstrates superior performance in LDES dispatch and effectively reduces the impact of degenerate solutions in sequential <b>simulations.</b> This study underscores the trade-offs between computational efficiency and improvement in storage dispatch, which is crucial for future energy systems. The analysis highlights the necessity of addressing the degeneracy issue in storage dispatch in grids dominated by zero operating cost VRE generators and low operating cost energy storage devices. Additionally, the research reveals revenue discrepancies for LDES operators across different models, a consequence of the persistent presence of degeneracy in high VRE systems. These findings suggest an urgent need for refined modeling techniques in the planning and operation of future energy systems.</p></p class="citation"></blockquote><h3 id=49--288350-decomposing-control-lyapunov-functions-for-efficient-reinforcement-learning-antonio-lopez-et-al-2024>(4/9 | 288/350) Decomposing Control Lyapunov Functions for Efficient Reinforcement Learning (Antonio Lopez et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Antonio Lopez, David Fridovich-Keil. (2024)<br><strong>Decomposing Control Lyapunov Functions for Efficient Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Decomposing Control Lyapunov Functions for Efficient Reinforcement Learning" index=288>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-288 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-LG, cs-SY, eess-SY, eess.SY<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12210v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12210v1.pdf filename=2403.12210v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent methods using <b>Reinforcement</b> <b>Learning</b> (RL) have proven to be successful for training intelligent agents in unknown environments. However, RL has not been applied widely in real-world robotics scenarios. This is because current state-of-the-art RL methods require large amounts of data to learn a specific task, leading to unreasonable costs when deploying the agent to collect data in real-world applications. In this paper, we build from existing work that reshapes the reward function in RL by introducing a Control Lyapunov Function (CLF), which is demonstrated to reduce the sample complexity. Still, this formulation requires knowing a CLF of the system, but due to the lack of a general method, it is often a challenge to identify a suitable CLF. Existing work can compute low-dimensional CLFs via a Hamilton-Jacobi reachability procedure. However, this class of methods becomes intractable on high-dimensional systems, a problem that we address by using a system decomposition technique to compute what we call Decomposed Control Lyapunov Functions (DCLFs). We use the computed DCLF for reward shaping, which we show improves RL performance. Through multiple examples, we demonstrate the effectiveness of this approach, where our method finds a policy to successfully land a quadcopter in less than half the amount of real-world data required by the state-of-the-art Soft-Actor Critic algorithm.</p></p class="citation"></blockquote><h3 id=59--289350-explainable-reinforcement-learning-based-home-energy-management-systems-using-differentiable-decision-trees-gargya-gokhale-et-al-2024>(5/9 | 289/350) Explainable Reinforcement Learning-based Home Energy Management Systems using Differentiable Decision Trees (Gargya Gokhale et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gargya Gokhale, Bert Claessens, Chris Develder. (2024)<br><strong>Explainable Reinforcement Learning-based Home Energy Management Systems using Differentiable Decision Trees</strong><br><button class=copy-to-clipboard title="Explainable Reinforcement Learning-based Home Energy Management Systems using Differentiable Decision Trees" index=289>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-289 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-LG, cs-SY, eess-SY, eess.SY<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11947v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11947v1.pdf filename=2403.11947v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the ongoing energy transition, demand-side flexibility has become an important aspect of the modern power grid for providing grid support and allowing further integration of sustainable energy sources. Besides traditional sources, the residential sector is another major and largely untapped source of flexibility, driven by the increased adoption of solar PV, home batteries, and EVs. However, unlocking this residential flexibility is challenging as it requires a control framework that can effectively manage household energy consumption, and maintain user comfort while being readily scalable across different, diverse houses. We aim to address this challenging problem and introduce a <b>reinforcement</b> <b>learning-based</b> approach using differentiable decision trees. This approach integrates the scalability of data-driven <b>reinforcement</b> <b>learning</b> with the explainability of (differentiable) decision trees. This leads to a controller that can be easily adapted across different houses and provides a simple control policy that can be explained to end-users, further improving user acceptance. As a proof-of-concept, we analyze our method using a home energy management problem, comparing its performance with commercially available rule-based baseline and standard neural network-based RL controllers. Through this preliminary study, we show that the performance of our proposed method is comparable to standard RL-based controllers, outperforming baseline controllers by ~20% in terms of daily cost savings while being straightforward to explain.</p></p class="citation"></blockquote><h3 id=69--290350-state-space-representations-of-the-roesser-type-for-convolutional-layers-patricia-pauli-et-al-2024>(6/9 | 290/350) State space representations of the Roesser type for convolutional layers (Patricia Pauli et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Patricia Pauli, Dennis Gramlich, Fran Allgöwer. (2024)<br><strong>State space representations of the Roesser type for convolutional layers</strong><br><button class=copy-to-clipboard title="State space representations of the Roesser type for convolutional layers" index=290>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-290 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-LG, cs-SY, eess-IV, eess-SP, eess-SY, eess.SY<br>Keyword Score: 10<br>Keywords: Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11938v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11938v1.pdf filename=2403.11938v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>From the perspective of control theory, <b>convolutional</b> layers (of neural networks) are 2-D (or N-D) linear time-invariant dynamical systems. The usual representation of <b>convolutional</b> layers by the <b>convolution</b> kernel corresponds to the representation of a dynamical system by its impulse response. However, many analysis tools from control theory, e.g., involving linear matrix inequalities, require a state space representation. For this reason, we explicitly provide a state space representation of the Roesser type for 2-D <b>convolutional</b> layers with $c_\mathrm{in}r_1 + c_\mathrm{out}r_2$ states, where $c_\mathrm{in}$/$c_\mathrm{out}$ is the number of input/output channels of the layer and $r_1$/$r_2$ characterizes the width/length of the <b>convolution</b> kernel. This representation is shown to be minimal for $c_\mathrm{in} = c_\mathrm{out}$. We further construct state space representations for dilated, strided, and N-D <b>convolutions.</b></p></p class="citation"></blockquote><h3 id=79--291350-symmetry-based-abstraction-algorithm-for-accelerating-symbolic-control-synthesis-hussein-sibai-et-al-2024>(7/9 | 291/350) Symmetry-based Abstraction Algorithm for Accelerating Symbolic Control Synthesis (Hussein Sibai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hussein Sibai, Sacha Huriot, Tyler Martin, Murat Arcak. (2024)<br><strong>Symmetry-based Abstraction Algorithm for Accelerating Symbolic Control Synthesis</strong><br><button class=copy-to-clipboard title="Symmetry-based Abstraction Algorithm for Accelerating Symbolic Control Synthesis" index=291>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-291 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 10<br>Keywords: Continuous Time, Continuous Time<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11816v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11816v1.pdf filename=2403.11816v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose an efficient symbolic control synthesis algorithm for equivariant <b>continuous-time</b> <b>dynamical</b> systems to satisfy reach-avoid specifications. The algorithm exploits dynamical symmetries to construct lean abstractions to avoid redundant computations during synthesis. Our proposed algorithm adds another layer of abstraction over the common grid-based discrete abstraction before solving the synthesis problem. It combines each set of grid cells that are at a similar relative position from the targets and nearby obstacles, defined by the symmetries, into a single abstract state. It uses this layer of abstraction to guide the order by which actions are explored during synthesis over the grid-based abstraction. We demonstrate the potential of our algorithm by synthesizing a reach-avoid controller for a 3-dimensional ship model with translation and rotation symmetries in the special Euclidean group SE(2).</p></p class="citation"></blockquote><h3 id=89--292350-topology-data-analysis-based-error-detection-for-semantic-image-transmission-with-incremental-knowledge-based-harq-fei-ni-et-al-2024>(8/9 | 292/350) Topology Data Analysis-based Error Detection for Semantic Image Transmission with Incremental Knowledge-based HARQ (Fei Ni et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fei Ni, Rongpeng Li, Zhifeng Zhao, Honggang Zhang. (2024)<br><strong>Topology Data Analysis-based Error Detection for Semantic Image Transmission with Incremental Knowledge-based HARQ</strong><br><button class=copy-to-clipboard title="Topology Data Analysis-based Error Detection for Semantic Image Transmission with Incremental Knowledge-based HARQ" index=292>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-292 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11542v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11542v1.pdf filename=2403.11542v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Semantic communication (SemCom) aims to achieve high fidelity information delivery under low communication consumption by only guaranteeing semantic accuracy. Nevertheless, semantic communication still suffers from unexpected channel volatility and thus developing a re-transmission mechanism (e.g., hybrid automatic repeat request [HARQ]) is indispensable. In that regard, instead of discarding previously transmitted information, the incremental knowledge-based HARQ (IK-HARQ) is deemed as a more effective mechanism that could sufficiently utilize the information semantics. However, considering the possible existence of semantic ambiguity in image transmission, a simple bit-level cyclic redundancy check (CRC) might compromise the performance of IK-HARQ. Therefore, it emerges a strong incentive to revolutionize the CRC mechanism, so as to reap the benefits of both SemCom and HARQ. In this paper, built on top of swin <b>transformer-based</b> joint source-channel coding (JSCC) and IK-HARQ, we propose a semantic image transmission framework SC-TDA-HARQ. In particular, different from the conventional CRC, we introduce a topological data analysis (TDA)-based error detection method, which capably digs out the inner topological and geometric information of images, so as to capture semantic information and determine the necessity for re-transmission. Extensive numerical results validate the effectiveness and efficiency of the proposed SC-TDA-HARQ framework, especially under the limited bandwidth condition, and manifest the superiority of TDA-based error detection method in image transmission.</p></p class="citation"></blockquote><h3 id=99--293350-secure-synchronization-of-heterogeneous-pulse-coupled-oscillators-jiaqi-yan-et-al-2024>(9/9 | 293/350) Secure Synchronization of Heterogeneous Pulse-Coupled Oscillators (Jiaqi Yan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiaqi Yan, Hideaki Ishii. (2024)<br><strong>Secure Synchronization of Heterogeneous Pulse-Coupled Oscillators</strong><br><button class=copy-to-clipboard title="Secure Synchronization of Heterogeneous Pulse-Coupled Oscillators" index=293>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-293 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12218v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12218v2.pdf filename=2403.12218v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we consider the synchronization of heterogeneous pulse-coupled oscillators (PCOs), where some of the oscillators might be faulty or malicious. The oscillators interact through identical pulses at discrete instants and evolve continuously with different frequencies otherwise. Despite the presence of misbehaviors, benign oscillators aim to reach synchronization. To achieve this objective, two resilient synchronization protocols are developed in this paper by adapting the real-valued mean-subsequence reduced (MSR) algorithm to pulse-based interactions. The first protocol relies on packet-based communication to transmit absolute frequencies, while the second protocol operates purely with pulses to calculate relative frequencies. In both protocols, each normal oscillator periodically counts the received pulses to detect possible malicious behaviors. By disregarding suspicious pulses from its neighbors, the oscillator updates both its phases and frequencies. The paper establishes sufficient conditions on the initial states and <b>graph</b> structure under which resilient synchronization is achieved in the PCO network. Specifically, the normal oscillators can either detect the presence of malicious nodes or synchronize in both phases and frequencies. Additionally, a comparison between the two algorithms reveals a trade-off between relaxed initial conditions and reduced communication burden.</p></p class="citation"></blockquote><h2 id=csar-1>cs.AR (1)</h2><h3 id=11--294350-hdldebugger-streamlining-hdl-debugging-with-large-language-models-xufeng-yao-et-al-2024>(1/1 | 294/350) HDLdebugger: Streamlining HDL debugging with Large Language Models (Xufeng Yao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xufeng Yao, Haoyang Li, Tsz Ho Chan, Wenyi Xiao, Mingxuan Yuan, Yu Huang, Lei Chen, Bei Yu. (2024)<br><strong>HDLdebugger: Streamlining HDL debugging with Large Language Models</strong><br><button class=copy-to-clipboard title="HDLdebugger: Streamlining HDL debugging with Large Language Models" index=294>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-294 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AR<br>Categories: cs-AI, cs-AR, cs-CE, cs-LG, cs-SE, cs.AR<br>Keyword Score: 50<br>Keywords: Fine-tuning, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11671v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11671v1.pdf filename=2403.11671v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the domain of chip design, Hardware Description Languages (HDLs) play a pivotal role. However, due to the complex syntax of HDLs and the limited availability of online resources, debugging HDL codes remains a difficult and time-intensive task, even for seasoned engineers. Consequently, there is a pressing need to develop automated HDL code debugging models, which can alleviate the burden on hardware engineers. Despite the strong capabilities of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> in generating, completing, and debugging software code, their utilization in the specialized field of HDL debugging has been limited and, to date, has not yielded satisfactory results. In this paper, we propose an <b>LLM-assisted</b> HDL debugging framework, namely HDLdebugger, which consists of HDL debugging data generation via a reverse engineering approach, a search engine for <b>retrieval-augmented</b> <b>generation,</b> <b>and</b> a <b>retrieval-augmented</b> <b>LLM</b> <b>fine-tuning</b> approach. Through the integration of these components, HDLdebugger can automate and streamline HDL debugging for chip design. Our comprehensive experiments, conducted on an HDL code dataset sourced from Huawei, reveal that HDLdebugger outperforms 13 cutting-edge <b>LLM</b> baselines, displaying exceptional effectiveness in HDL code debugging.</p></p class="citation"></blockquote><h2 id=csdc-4>cs.DC (4)</h2><h3 id=14--295350-fair-distributed-cooperative-bandit-learning-on-networks-for-intelligent-internet-of-things-systems-technical-report-ziqun-chen-et-al-2024>(1/4 | 295/350) Fair Distributed Cooperative Bandit Learning on Networks for Intelligent Internet of Things Systems (Technical Report) (Ziqun Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ziqun Chen, Kechao Cai, Jinbei Zhang, Zhigang Yu. (2024)<br><strong>Fair Distributed Cooperative Bandit Learning on Networks for Intelligent Internet of Things Systems (Technical Report)</strong><br><button class=copy-to-clipboard title="Fair Distributed Cooperative Bandit Learning on Networks for Intelligent Internet of Things Systems (Technical Report)" index=295>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-295 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs-LG, cs.DC<br>Keyword Score: 50<br>Keywords: Bandit Algorithm, Bandit Algorithm, Fairness, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11603v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11603v1.pdf filename=2403.11603v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In intelligent Internet of Things (IoT) systems, edge servers within a network exchange information with their neighbors and collect data from sensors to complete delivered tasks. In this paper, we propose a multiplayer multi-armed <b>bandit</b> <b>model</b> for intelligent IoT systems to facilitate data collection and incorporate <b>fairness</b> considerations. In our model, we establish an effective communication protocol that helps servers cooperate with their neighbors. Then we design a distributed cooperative <b>bandit</b> <b>algorithm,</b> DC-ULCB, enabling servers to collaboratively select sensors to maximize data rates while maintaining <b>fairness</b> in their choices. We conduct an analysis of the reward regret and <b>fairness</b> regret of DC-ULCB, and prove that both regrets have logarithmic instance-dependent upper bounds. Additionally, through extensive <b>simulations,</b> we validate that DC-ULCB outperforms existing algorithms in maximizing reward and ensuring <b>fairness.</b></p></p class="citation"></blockquote><h3 id=24--296350-fastdecode-high-throughput-gpu-efficient-llm-serving-using-heterogeneous-pipelines-jiaao-he-et-al-2024>(2/4 | 296/350) FastDecode: High-Throughput GPU-Efficient LLM Serving using Heterogeneous Pipelines (Jiaao He et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiaao He, Jidong Zhai. (2024)<br><strong>FastDecode: High-Throughput GPU-Efficient LLM Serving using Heterogeneous Pipelines</strong><br><button class=copy-to-clipboard title="FastDecode: High-Throughput GPU-Efficient LLM Serving using Heterogeneous Pipelines" index=296>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-296 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: C-4, cs-DC, cs-PF, cs.DC<br>Keyword Score: 30<br>Keywords: Transformer, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11421v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11421v1.pdf filename=2403.11421v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Cost of serving <b>large</b> <b>language</b> <b>models</b> <b>(LLM)</b> is high, but the expensive and scarce GPUs are poorly efficient when generating tokens sequentially, unless the batch of sequences is enlarged. However, the batch size is limited by some constantly reused intermediate results, namely KV-Cache. They occupy too much memory to fit more sequences into a GPU simultaneously. While they could be offloaded to host memory, the CPU-GPU bandwidth is an inevitable bottleneck. We find a way to decompose the <b>transformer</b> models into two parts of different characteristics, one of which includes the memory-bound KV-Cache accessing. Our key insight is that the aggregated memory capacity, bandwidth, and computing power of CPUs across multiple nodes is an efficient option to process this part. Performance improvement comes from reduced data transmission overhead and boosted GPU throughput to process the other model part. Moreover, we address efficiency challenges brought by heterogeneity at both temporal and inter-device scopes using scheduling and performance modeling techniques. Evaluation results show that our system achieves 1.88x - 5.04x the throughput of vLLM when serving modern <b>LLMs</b> with the same GPU.</p></p class="citation"></blockquote><h3 id=34--297350-amrex-and-pyamrex-looking-beyond-ecp-andrew-myers-et-al-2024>(3/4 | 297/350) AMReX and pyAMReX: Looking Beyond ECP (Andrew Myers et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andrew Myers, Weiqun Zhang, Ann Almgren, Thierry Antoun, John Bell, Axel Huebl, Alexander Sinn. (2024)<br><strong>AMReX and pyAMReX: Looking Beyond ECP</strong><br><button class=copy-to-clipboard title="AMReX and pyAMReX: Looking Beyond ECP" index=297>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-297 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs.DC<br>Keyword Score: 10<br>Keywords: Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12179v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12179v1.pdf filename=2403.12179v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>AMReX is a software framework for the development of block-structured mesh applications with adaptive mesh refinement (AMR). AMReX was initially developed and supported by the AMReX Co-Design Center as part of the U.S. DOE Exascale Computing Project, and is continuing to grow post-ECP. In addition to adding new functionality and performance improvements to the core AMReX framework, we have also developed a Python binding, pyAMReX, that provides a bridge between AMReX-based application codes and the data science ecosystem. pyAMReX provides zero-copy application GPU data access for AI/ML, in situ analysis and application coupling, and enables rapid, massively parallel prototyping. In this paper we review the overall functionality of AMReX and pyAMReX, focusing on new developments, new functionality, and optimizations of key operations. We also <b>summarize</b> capabilities of ECP projects that used AMReX and provide an overview of new, non-ECP applications.</p></p class="citation"></blockquote><h3 id=44--298350-berger-byzantine-robust-geometric-routing-brown-zaz-et-al-2024>(4/4 | 298/350) BeRGeR: Byzantine-Robust Geometric Routing (Brown Zaz et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Brown Zaz, Mikhail Nesterenko, Gokarna Sharma. (2024)<br><strong>BeRGeR: Byzantine-Robust Geometric Routing</strong><br><button class=copy-to-clipboard title="BeRGeR: Byzantine-Robust Geometric Routing" index=298>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-298 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs.DC<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12256v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12256v1.pdf filename=2403.12256v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present BeRGeR: the first asynchronous geometric routing algorithm that guarantees delivery of a message despite a Byzantine fault without relying on cryptographic primitives or randomization. The communication <b>graph</b> is a planar embedding that remains three-connected if all edges intersecting the source-target line segment are removed. We prove the algorithm correct and estimate its message complexity.</p></p class="citation"></blockquote><h2 id=physicsdata-an-1>physics.data-an (1)</h2><h3 id=11--299350-nugraph2-a-graph-neural-network-for-neutrino-physics-event-reconstruction-v-hewes-et-al-2024>(1/1 | 299/350) NuGraph2: A Graph Neural Network for Neutrino Physics Event Reconstruction (V Hewes et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>V Hewes, Adam Aurisano, Giuseppe Cerati, Jim Kowalkowski, Claire Lee, Wei-keng Liao, Daniel Grzenda, Kaushal Gumpula, Xiaohe Zhang. (2024)<br><strong>NuGraph2: A Graph Neural Network for Neutrino Physics Event Reconstruction</strong><br><button class=copy-to-clipboard title="NuGraph2: A Graph Neural Network for Neutrino Physics Event Reconstruction" index=299>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-299 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.data-an<br>Categories: cs-LG, hep-ex, physics-data-an, physics.data-an<br>Keyword Score: 48<br>Keywords: Message-Passing, Graph, Graph Neural Network, Graph Neural Network, Convolution, Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11872v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11872v1.pdf filename=2403.11872v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Liquid Argon Time Projection Chamber (LArTPC) detector technology offers a wealth of high-resolution information on particle interactions, and leveraging that information to its full potential requires sophisticated automated reconstruction techniques. This article describes NuGraph2, a <b>Graph</b> <b>Neural</b> <b>Network</b> <b>(GNN)</b> for low-level reconstruction of simulated neutrino interactions in a LArTPC detector. Simulated neutrino interactions in the MicroBooNE detector <b>geometry</b> are described as heterogeneous <b>graphs,</b> <b>with</b> <b>energy</b> depositions on each detector plane forming nodes on planar subgraphs. The network utilizes a multi-head attention <b>message-passing</b> mechanism to perform background filtering and semantic labelling on these <b>graph</b> <b>nodes,</b> <b>identifying</b> those associated with the primary physics interaction with 98.0% efficiency and labelling them according to particle type with 94.9% efficiency. The network operates directly on detector observables across multiple 2D representations, but utilizes a 3D-context-aware mechanism to encourage consistency between these representations. Model inference takes 0.12 s/event on a CPU, and 0.005 s/event batched on a GPU. This architecture is designed to be a general-purpose solution for particle reconstruction in neutrino physics, with the potential for deployment across a broad range of detector technologies, and offers a core <b>convolution</b> engine that can be leveraged for a variety of tasks beyond the two described in this article.</p></p class="citation"></blockquote><h2 id=statml-5>stat.ML (5)</h2><h3 id=15--300350-pessimistic-causal-reinforcement-learning-with-mediators-for-confounded-offline-data-danyang-wang-et-al-2024>(1/5 | 300/350) Pessimistic Causal Reinforcement Learning with Mediators for Confounded Offline Data (Danyang Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Danyang Wang, Chengchun Shi, Shikai Luo, Will Wei Sun. (2024)<br><strong>Pessimistic Causal Reinforcement Learning with Mediators for Confounded Offline Data</strong><br><button class=copy-to-clipboard title="Pessimistic Causal Reinforcement Learning with Mediators for Confounded Offline Data" index=300>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-300 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-AI, cs-LG, stat-ML, stat.ML<br>Keyword Score: 40<br>Keywords: Offline Reinforcement Learning, Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11841v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11841v1.pdf filename=2403.11841v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In real-world scenarios, datasets collected from randomized experiments are often constrained by size, due to limitations in time and budget. As a result, leveraging large observational datasets becomes a more attractive option for achieving high-quality policy learning. However, most existing <b>offline</b> <b>reinforcement</b> <b>learning</b> (RL) methods depend on two key assumptions&ndash;unconfoundedness and positivity&ndash;which frequently do not hold in observational data contexts. Recognizing these challenges, we propose a novel policy learning algorithm, PESsimistic CAusal Learning (PESCAL). We utilize the mediator variable based on front-door criterion to remove the confounding bias; additionally, we adopt the pessimistic principle to address the distributional shift between the action distributions induced by candidate policies, and the behavior policy that generates the observational data. Our key observation is that, by incorporating auxiliary variables that mediate the effect of actions on system dynamics, it is sufficient to learn a lower bound of the mediator distribution function, instead of the Q-function, to partially mitigate the issue of distributional shift. This insight significantly simplifies our algorithm, by circumventing the challenging task of sequential uncertainty quantification for the estimated Q-function. Moreover, we provide theoretical guarantees for the algorithms we propose, and demonstrate their efficacy through <b>simulations,</b> as well as real-world experiments utilizing <b>offline</b> <b>datasets</b> <b>from</b> a leading ride-hailing platform.</p></p class="citation"></blockquote><h3 id=25--301350-posterior-uncertainty-quantification-in-neural-networks-using-data-augmentation-luhuan-wu-et-al-2024>(2/5 | 301/350) Posterior Uncertainty Quantification in Neural Networks using Data Augmentation (Luhuan Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Luhuan Wu, Sinead Williamson. (2024)<br><strong>Posterior Uncertainty Quantification in Neural Networks using Data Augmentation</strong><br><button class=copy-to-clipboard title="Posterior Uncertainty Quantification in Neural Networks using Data Augmentation" index=301>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-301 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 30<br>Keywords: Data Augmentation, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12729v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12729v1.pdf filename=2403.12729v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we approach the problem of uncertainty quantification in deep learning through a predictive framework, which captures uncertainty in model parameters by specifying our assumptions about the predictive distribution of unseen future <b>data.</b> <b>Under</b> this view, we show that deep ensembling (Lakshminarayanan et al., 2017) is a fundamentally mis-specified model class, since it assumes that future <b>data</b> <b>are</b> supported on existing observations only &ndash; a situation rarely encountered in practice. To address this limitation, we propose MixupMP, a method that constructs a more realistic predictive distribution using popular <b>data</b> <b>augmentation</b> techniques. MixupMP operates as a drop-in replacement for deep ensembles, where each ensemble member is trained on a random <b>simulation</b> from this predictive distribution. Grounded in the recently-proposed framework of Martingale posteriors (Fong et al., 2023), MixupMP returns samples from an implicitly defined Bayesian posterior. Our empirical analysis showcases that MixupMP achieves superior predictive performance and uncertainty quantification on various image classification datasets, when compared with existing Bayesian and non-Bayesian approaches.</p></p class="citation"></blockquote><h3 id=35--302350-out-of-distribution-detection-should-use-conformal-prediction-and-vice-versa-paul-novello-et-al-2024>(3/5 | 302/350) Out-of-Distribution Detection Should Use Conformal Prediction (and Vice-versa?) (Paul Novello et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Paul Novello, Joseba Dalmau, Léo Andeol. (2024)<br><strong>Out-of-Distribution Detection Should Use Conformal Prediction (and Vice-versa?)</strong><br><button class=copy-to-clipboard title="Out-of-Distribution Detection Should Use Conformal Prediction (and Vice-versa?)" index=302>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-302 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-CV, cs-LG, stat-ML, stat.ML<br>Keyword Score: 26<br>Keywords: Anomaly Detection, Benchmarking, Out-of-distribution, Sample Size<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11532v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11532v1.pdf filename=2403.11532v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Research on <b>Out-Of-Distribution</b> (OOD) detection focuses mainly on building scores that efficiently distinguish OOD data from In Distribution (ID) data. On the other hand, Conformal Prediction (CP) uses non-conformity scores to construct prediction sets with probabilistic coverage guarantees. In this work, we propose to use CP to better assess the efficiency of OOD scores. Specifically, we emphasize that in standard OOD <b>benchmark</b> settings, evaluation metrics can be overly optimistic due to the finite <b>sample</b> <b>size</b> of the test dataset. Based on the work of (Bates et al., 2022), we define new conformal AUROC and conformal FRP@TPR95 metrics, which are corrections that provide probabilistic conservativeness guarantees on the variability of these metrics. We show the effect of these corrections on two reference OOD and <b>anomaly</b> <b>detection</b> <b>benchmarks,</b> OpenOOD (Yang et al., 2022) and ADBench (Han et al., 2022). We also show that the benefits of using OOD together with CP apply the other way around by using OOD scores as non-conformity scores, which results in improving upon current CP methods. One of the key messages of these contributions is that since OOD is concerned with designing scores and CP with interpreting these scores, the two fields may be inherently intertwined.</p></p class="citation"></blockquote><h3 id=45--303350-nonsmooth-implicit-differentiation-deterministic-and-stochastic-convergence-rates-riccardo-grazzi-et-al-2024>(4/5 | 303/350) Nonsmooth Implicit Differentiation: Deterministic and Stochastic Convergence Rates (Riccardo Grazzi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Riccardo Grazzi, Massimiliano Pontil, Saverio Salzo. (2024)<br><strong>Nonsmooth Implicit Differentiation: Deterministic and Stochastic Convergence Rates</strong><br><button class=copy-to-clipboard title="Nonsmooth Implicit Differentiation: Deterministic and Stochastic Convergence Rates" index=303>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-303 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, math-OC, stat-ML, stat.ML<br>Keyword Score: 10<br>Keywords: Meta Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11687v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11687v1.pdf filename=2403.11687v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the problem of efficiently computing the derivative of the fixed-point of a parametric non-differentiable contraction map. This problem has wide applications in machine learning, including hyperparameter optimization, <b>meta-learning</b> <b>and</b> data poisoning attacks. We analyze two popular approaches: iterative differentiation (ITD) and approximate implicit differentiation (AID). A key challenge behind the nonsmooth setting is that the chain rule does not hold anymore. Building upon the recent work by Bolte et al. (2022), who proved the linear convergence of non-differentiable ITD, we provide refined linear convergence rates for both ITD and AID in the deterministic case. We further introduce NSID, a new method to compute the implicit derivative when the fixed point is defined as the composition of an outer map and an inner map which is accessible only through a stochastic unbiased estimator. We establish rates for the convergence of NSID to the true derivative, encompassing the best available rates in the smooth setting. We present illustrative experiments confirming our analysis.</p></p class="citation"></blockquote><h3 id=55--304350-variational-approach-for-efficient-kl-divergence-estimation-in-dirichlet-mixture-models-samyajoy-pal-et-al-2024>(5/5 | 304/350) Variational Approach for Efficient KL Divergence Estimation in Dirichlet Mixture Models (Samyajoy Pal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Samyajoy Pal, Christian Heumann. (2024)<br><strong>Variational Approach for Efficient KL Divergence Estimation in Dirichlet Mixture Models</strong><br><button class=copy-to-clipboard title="Variational Approach for Efficient KL Divergence Estimation in Dirichlet Mixture Models" index=304>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-304 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, math-ST, stat-ML, stat-TH, stat.ML<br>Keyword Score: 3<br>Keywords: Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12158v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12158v1.pdf filename=2403.12158v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study tackles the efficient estimation of Kullback-Leibler (KL) Divergence in Dirichlet Mixture Models (DMM), crucial for <b>clustering</b> compositional data. Despite the significance of DMMs, obtaining an analytically tractable solution for KL Divergence has proven elusive. Past approaches relied on computationally demanding Monte Carlo methods, motivating our introduction of a novel variational approach. Our method offers a closed-form solution, significantly enhancing computational efficiency for swift model comparisons and robust estimation evaluations. Validation using real and simulated data showcases its superior efficiency and accuracy over traditional Monte Carlo-based methods, opening new avenues for rapid exploration of diverse DMM models and advancing statistical analyses of compositional data.</p></p class="citation"></blockquote><h2 id=csir-1>cs.IR (1)</h2><h3 id=11--305350-dual-channel-multiplex-graph-neural-networks-for-recommendation-xiang-li-et-al-2024>(1/1 | 305/350) Dual-Channel Multiplex Graph Neural Networks for Recommendation (Xiang Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiang Li, Chaofan Fu, Zhongying Zhao, Guanjie Zheng, Chao Huang, Junyu Dong, Yanwei Yu. (2024)<br><strong>Dual-Channel Multiplex Graph Neural Networks for Recommendation</strong><br><button class=copy-to-clipboard title="Dual-Channel Multiplex Graph Neural Networks for Recommendation" index=305>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-305 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs-LG, cs.IR<br>Keyword Score: 38<br>Keywords: Graph, Graph Neural Network, Recommendation, Recommender System, Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11624v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11624v1.pdf filename=2403.11624v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Efficient <b>recommender</b> <b>systems</b> play a crucial role in accurately capturing user and item attributes that mirror individual preferences. Some existing <b>recommendation</b> techniques have started to shift their focus towards modeling various types of interaction relations between users and items in real-world <b>recommendation</b> scenarios, such as clicks, marking favorites, and purchases on online shopping platforms. Nevertheless, these approaches still grapple with two significant shortcomings: (1) Insufficient modeling and exploitation of the impact of various behavior patterns formed by multiplex relations between users and items on <b>representation</b> <b>learning,</b> and (2) ignoring the effect of different relations in the behavior patterns on the target relation in <b>recommender</b> <b>system</b> scenarios. In this study, we introduce a novel <b>recommendation</b> framework, Dual-Channel Multiplex <b>Graph</b> <b>Neural</b> <b>Network</b> (DCMGNN), which addresses the aforementioned challenges. It incorporates an explicit behavior pattern <b>representation</b> <b>learner</b> to capture the behavior patterns composed of multiplex user-item interaction relations, and includes a relation chain <b>representation</b> <b>learning</b> and a relation chain-aware encoder to discover the impact of various auxiliary relations on the target relation, the dependencies between different relations, and mine the appropriate order of relations in a behavior pattern. Extensive experiments on three real-world datasets demonstrate that our \model surpasses various state-of-the-art <b>recommendation</b> methods. It outperforms the best baselines by 10.06% and 12.15% on average across all datasets in terms of R@10 and N@10 respectively.</p></p class="citation"></blockquote><h2 id=hep-ph-1>hep-ph (1)</h2><h3 id=11--306350-capslorentznet-integrating-physics-inspired-features-with-graph-convolution-rameswar-sahu-2024>(1/1 | 306/350) CapsLorentzNet: Integrating Physics Inspired Features with Graph Convolution (Rameswar Sahu, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rameswar Sahu. (2024)<br><strong>CapsLorentzNet: Integrating Physics Inspired Features with Graph Convolution</strong><br><button class=copy-to-clipboard title="CapsLorentzNet: Integrating Physics Inspired Features with Graph Convolution" index=306>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-306 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: hep-ph<br>Categories: cs-LG, hep-ex, hep-ph, hep-ph<br>Keyword Score: 33<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11826v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11826v1.pdf filename=2403.11826v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the advent of advanced machine learning techniques, boosted object tagging has witnessed significant progress. In this article, we take this field further by introducing novel architectural modifications compatible with a wide array of <b>Graph</b> <b>Neural</b> <b>Network</b> <b>(GNN)</b> architectures. Our approach advocates for integrating capsule layers, replacing the conventional decoding blocks in standard <b>GNNs.</b> These capsules are a group of neurons with vector activations. The orientation of these vectors represents important properties of the objects under study, with their magnitude characterizing whether the object under study belongs to the class represented by the capsule. Moreover, capsule networks incorporate a regularization by reconstruction mechanism, facilitating the seamless integration of expert-designed high-level features into the analysis. We have studied the usefulness of our architecture with the LorentzNet architecture for quark-gluon tagging. Here, we have replaced the decoding block of LorentzNet with a capsulated decoding block and have called the resulting architecture CapsLorentzNet. Our new architecture can enhance the performance of LorentzNet by 20 % for the quark-gluon tagging task.</p></p class="citation"></blockquote><h2 id=q-biocb-1>q-bio.CB (1)</h2><h3 id=11--307350-transfer-learning-for-t-cell-response-prediction-josua-stadelmaier-et-al-2024>(1/1 | 307/350) Transfer Learning for T-Cell Response Prediction (Josua Stadelmaier et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Josua Stadelmaier, Brandon Malone, Ralf Eggeling. (2024)<br><strong>Transfer Learning for T-Cell Response Prediction</strong><br><button class=copy-to-clipboard title="Transfer Learning for T-Cell Response Prediction" index=307>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-307 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.CB<br>Categories: cs-LG, q-bio-CB, q-bio.CB<br>Keyword Score: 30<br>Keywords: Fine-tuning, Transfer Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12117v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12117v1.pdf filename=2403.12117v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the prediction of T-cell response for specific given peptides, which could, among other applications, be a crucial step towards the development of personalized cancer vaccines. It is a challenging task due to limited, heterogeneous training data featuring a multi-domain structure; such data entail the danger of shortcut learning, where models learn general characteristics of peptide sources, such as the source organism, rather than specific peptide characteristics associated with T-cell response. Using a <b>transformer</b> model for T-cell response prediction, we show that the danger of inflated predictive performance is not merely theoretical but occurs in practice. Consequently, we propose a domain-aware evaluation scheme. We then study different <b>transfer</b> <b>learning</b> techniques to deal with the multi-domain structure and shortcut learning. We demonstrate a per-source fine tuning approach to be effective across a wide range of peptide sources and further show that our final model outperforms existing state-of-the-art approaches for predicting T-cell responses for human peptides.</p></p class="citation"></blockquote><h2 id=cssd-6>cs.SD (6)</h2><h3 id=16--308350-unimodal-multi-task-fusion-for-emotional-mimicry-prediciton-tobias-hallmen-et-al-2024>(1/6 | 308/350) Unimodal Multi-Task Fusion for Emotional Mimicry Prediciton (Tobias Hallmen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tobias Hallmen, Fabian Deuser, Norbert Oswald, Elisabeth André. (2024)<br><strong>Unimodal Multi-Task Fusion for Emotional Mimicry Prediciton</strong><br><button class=copy-to-clipboard title="Unimodal Multi-Task Fusion for Emotional Mimicry Prediciton" index=308>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-308 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-AI, cs-SD, cs.SD, eess-AS<br>Keyword Score: 30<br>Keywords: LSTM, LSTM, LSTM<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11879v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11879v1.pdf filename=2403.11879v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this study, we propose a methodology for the Emotional Mimicry Intensity (EMI) Estimation task within the context of the 6th Workshop and Competition on Affective Behavior Analysis in-the-wild. Our approach leverages the Wav2Vec 2.0 framework, pre-trained on a comprehensive podcast dataset, to extract a broad range of audio features encompassing both linguistic and paralinguistic elements. We enhance feature representation through a fusion technique that integrates individual features with a global mean vector, introducing global contextual insights into our analysis. Additionally, we incorporate a pre-trained valence-arousal-dominance (VAD) module from the Wav2Vec 2.0 model. Our fusion employs a <b>Long</b> <b>Short-Term</b> <b>Memory</b> <b>(LSTM)</b> architecture for efficient temporal analysis of audio data. Utilizing only the provided audio data, our approach demonstrates significant improvements over the established baseline.</p></p class="citation"></blockquote><h3 id=26--309350-prompt-singer-controllable-singing-voice-synthesis-with-natural-language-prompt-yongqi-wang-et-al-2024>(2/6 | 309/350) Prompt-Singer: Controllable Singing-Voice-Synthesis with Natural Language Prompt (Yongqi Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yongqi Wang, Ruofan Hu, Rongjie Huang, Zhiqing Hong, Ruiqi Li, Wenrui Liu, Fuming You, Tao Jin, Zhou Zhao. (2024)<br><strong>Prompt-Singer: Controllable Singing-Voice-Synthesis with Natural Language Prompt</strong><br><button class=copy-to-clipboard title="Prompt-Singer: Controllable Singing-Voice-Synthesis with Natural Language Prompt" index=309>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-309 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-AI, cs-LG, cs-SD, cs.SD, eess-AS<br>Keyword Score: 30<br>Keywords: Fine-tuning, Transformer, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11780v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11780v1.pdf filename=2403.11780v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent singing-voice-synthesis (SVS) methods have achieved remarkable audio quality and naturalness, yet they lack the capability to control the style attributes of the synthesized singing explicitly. We propose <b>Prompt-Singer,</b> the first SVS method that enables attribute controlling on singer gender, vocal range and volume with natural language. We adopt a model architecture based on a decoder-only <b>transformer</b> with a multi-scale hierarchy, and design a range-melody decoupled pitch representation that enables text-conditioned vocal range control while keeping melodic accuracy. Furthermore, we explore various experiment settings, including different types of text representations, text encoder <b>fine-tuning,</b> and introducing speech data to alleviate data scarcity, aiming to facilitate further research. Experiments show that our model achieves favorable controlling ability and audio quality. Audio samples are available at <a href=http://prompt-singer.github.io>http://prompt-singer.github.io</a> .</p></p class="citation"></blockquote><h3 id=36--310350-generalized-multi-source-inference-for-text-conditioned-music-diffusion-models-emilian-postolache-et-al-2024>(3/6 | 310/350) Generalized Multi-Source Inference for Text Conditioned Music Diffusion Models (Emilian Postolache et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Emilian Postolache, Giorgio Mariani, Luca Cosmo, Emmanouil Benetos, Emanuele Rodolà. (2024)<br><strong>Generalized Multi-Source Inference for Text Conditioned Music Diffusion Models</strong><br><button class=copy-to-clipboard title="Generalized Multi-Source Inference for Text Conditioned Music Diffusion Models" index=310>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-310 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-LG, cs-SD, cs.SD, eess-AS<br>Keyword Score: 20<br>Keywords: Diffusion Model, Text Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11706v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11706v1.pdf filename=2403.11706v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multi-Source <b>Diffusion</b> <b>Models</b> (MSDM) allow for compositional musical generation tasks: generating a set of coherent sources, creating accompaniments, and performing source separation. Despite their versatility, they require estimating the joint distribution over the sources, necessitating pre-separated musical data, which is rarely available, and fixing the number and type of sources at training time. This paper generalizes MSDM to arbitrary time-domain <b>diffusion</b> <b>models</b> conditioned on <b>text</b> <b>embeddings.</b> These models do not require separated data as they are trained on mixtures, can parameterize an arbitrary number of sources, and allow for rich semantic control. We propose an inference procedure enabling the coherent generation of sources and accompaniments. Additionally, we adapt the Dirac separator of MSDM to perform source separation. We experiment with <b>diffusion</b> <b>models</b> trained on Slakh2100 and MTG-Jamendo, showcasing competitive generation and separation results in a relaxed data setting.</p></p class="citation"></blockquote><h3 id=46--311350-notochord-a-flexible-probabilistic-model-for-real-time-midi-performance-victor-shepardson-et-al-2024>(4/6 | 311/350) Notochord: a Flexible Probabilistic Model for Real-Time MIDI Performance (Victor Shepardson et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Victor Shepardson, Jack Armitage, Thor Magnusson. (2024)<br><strong>Notochord: a Flexible Probabilistic Model for Real-Time MIDI Performance</strong><br><button class=copy-to-clipboard title="Notochord: a Flexible Probabilistic Model for Real-Time MIDI Performance" index=311>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-311 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-AI, cs-SD, cs.SD, eess-AS<br>Keyword Score: 10<br>Keywords: Probabilistic Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12000v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12000v1.pdf filename=2403.12000v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep learning-based <b>probabilistic</b> <b>models</b> of musical data are producing increasingly realistic results and promise to enter creative workflows of many kinds. Yet they have been little-studied in a performance setting, where the results of user actions typically ought to feel instantaneous. To enable such study, we designed Notochord, a deep <b>probabilistic</b> <b>model</b> for sequences of structured events, and trained an instance of it on the Lakh MIDI dataset. Our <b>probabilistic</b> <b>formulation</b> allows interpretable interventions at a sub-event level, which enables one model to act as a backbone for diverse interactive musical functions including steerable generation, harmonization, machine improvisation, and likelihood-based interfaces. Notochord can generate polyphonic and multi-track MIDI, and respond to inputs with latency below ten milliseconds. Training code, model checkpoints and interactive examples are provided as open source software.</p></p class="citation"></blockquote><h3 id=56--312350-sound-event-detection-and-localization-with-distance-estimation-daniel-aleksander-krause-et-al-2024>(5/6 | 312/350) Sound Event Detection and Localization with Distance Estimation (Daniel Aleksander Krause et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daniel Aleksander Krause, Archontis Politis, Annamaria Mesaros. (2024)<br><strong>Sound Event Detection and Localization with Distance Estimation</strong><br><button class=copy-to-clipboard title="Sound Event Detection and Localization with Distance Estimation" index=312>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-312 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-LG, cs-SD, cs.SD, eess-AS<br>Keyword Score: 10<br>Keywords: Event Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11827v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11827v1.pdf filename=2403.11827v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Sound <b>Event</b> <b>Detection</b> and Localization (SELD) is a combined task of identifying sound <b>events</b> <b>and</b> their corresponding direction-of-arrival (DOA). While this task has numerous applications and has been extensively researched in recent years, it fails to provide full information about the sound source position. In this paper, we overcome this problem by extending the task to Sound <b>Event</b> <b>Detection,</b> Localization with Distance Estimation (3D SELD). We study two ways of integrating distance estimation within the SELD core - a multi-task approach, in which the problem is tackled by a separate model output, and a single-task approach obtained by extending the multi-ACCDOA method to include distance information. We investigate both methods for the Ambisonic and binaural versions of STARSS23: Sony-TAU Realistic Spatial Soundscapes 2023. Moreover, our study involves experiments on the loss function related to the distance estimation part. Our results show that it is possible to perform 3D SELD without any degradation of performance in sound <b>event</b> <b>detection</b> and DOA estimation.</p></p class="citation"></blockquote><h3 id=66--313350-towards-the-development-of-a-real-time-deepfake-audio-detection-system-in-communication-platforms-jonat-john-mathew-et-al-2024>(6/6 | 313/350) Towards the Development of a Real-Time Deepfake Audio Detection System in Communication Platforms (Jonat John Mathew et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jonat John Mathew, Rakin Ahsan, Sae Furukawa, Jagdish Gautham Krishna Kumar, Huzaifa Pallan, Agamjeet Singh Padda, Sara Adamski, Madhu Reddiboina, Arjun Pankajakshan. (2024)<br><strong>Towards the Development of a Real-Time Deepfake Audio Detection System in Communication Platforms</strong><br><button class=copy-to-clipboard title="Towards the Development of a Real-Time Deepfake Audio Detection System in Communication Platforms" index=313>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-313 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-CR, cs-LG, cs-SD, cs.SD, eess-AS<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11778v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11778v1.pdf filename=2403.11778v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deepfake audio poses a rising threat in communication platforms, necessitating real-time detection for audio stream integrity. Unlike traditional non-real-time approaches, this study assesses the viability of employing static deepfake audio detection models in real-time communication platforms. An executable software is developed for cross-platform compatibility, enabling real-time execution. Two deepfake audio detection models based on Resnet and LCNN architectures are implemented using the ASVspoof 2019 dataset, achieving <b>benchmark</b> performances compared to ASVspoof 2019 challenge baselines. The study proposes strategies and frameworks for enhancing these models, paving the way for real-time deepfake audio detection in communication platforms. This work contributes to the advancement of audio stream security, ensuring robust detection capabilities in dynamic, real-time communication scenarios.</p></p class="citation"></blockquote><h2 id=csit-6>cs.IT (6)</h2><h3 id=16--314350-ris-aided-single-frequency-3d-imaging-by-exploiting-multi-view-image-correlations-yixuan-huang-et-al-2024>(1/6 | 314/350) RIS-aided Single-frequency 3D Imaging by Exploiting Multi-view Image Correlations (Yixuan Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yixuan Huang, Jie Yang, Chao-Kai Wen, Shi Jin. (2024)<br><strong>RIS-aided Single-frequency 3D Imaging by Exploiting Multi-view Image Correlations</strong><br><button class=copy-to-clipboard title="RIS-aided Single-frequency 3D Imaging by Exploiting Multi-view Image Correlations" index=314>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-314 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 30<br>Keywords: Message-Passing, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11764v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11764v1.pdf filename=2403.11764v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Retrieving range information in three-dimensional (3D) radio imaging is particularly challenging due to the limited communication bandwidth and pilot resources. To address this issue, we consider a reconfigurable intelligent surface (RIS)-aided uplink communication scenario, generating multiple measurements through RIS phase adjustment. This study successfully realizes 3D single-frequency imaging by exploiting the near-field multi-view image correlations deduced from user mobility. We first highlight the significance of considering anisotropy in multi-view image formation by investigating radar cross-section properties and diffraction resolution limits. We then propose a novel model for joint multi-view 3D imaging that incorporates occlusion effects and anisotropic scattering. These factors lead to slow image support variation and smooth coefficient evolution, which are mathematically modeled as Markov processes. Based on this model, we employ the Expectation Maximization-Turbo-Generalized Approximate Message Passing algorithm for joint multi-view single-frequency 3D imaging with limited measurements. <b>Simulation</b> results reveal the superiority of joint multi-view imaging in terms of enhanced imaging ranges, accuracies, and anisotropy characterization compared to single-view imaging. Combining adjacent observations for joint multi-view imaging enables a reduction in the measurement overhead by 80%.</p></p class="citation"></blockquote><h3 id=26--315350-comparative-analysis-of-sub-band-allocation-algorithms-in-in-body-sub-networks-supporting-xr-applications-saeed-bagherinejad-et-al-2024>(2/6 | 315/350) Comparative Analysis of Sub-band Allocation Algorithms in In-body Sub-networks Supporting XR Applications (Saeed Bagherinejad et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Saeed Bagherinejad, Thomas Jacobsen, Nuno K. Pratas, Ramoni O. Adeogun. (2024)<br><strong>Comparative Analysis of Sub-band Allocation Algorithms in In-body Sub-networks Supporting XR Applications</strong><br><button class=copy-to-clipboard title="Comparative Analysis of Sub-band Allocation Algorithms in In-body Sub-networks Supporting XR Applications" index=315>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-315 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 23<br>Keywords: Graph, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11891v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11891v1.pdf filename=2403.11891v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In-body subnetworks (IBS) are envisioned to support reliable wireless connectivity for emerging applications including extended reality (XR) in the human body. As the deployment of in-body sub-networks is uncontrollable by nature, the dynamic radio resource allocation scheme in place becomes of the uttermost importance for the performance of the in-body sub-networks. This paper provides a comparative study on the performance of the state-of-the-art interference-aware sub-band allocation algorithms in in-body sub-networks supporting the XR applications. The study identified suitable models for characterizing in-body sub-networks which are used in a snapshot-based <b>simulation</b> framework to perform a comprehensive evaluation of the performance of state-of-art sub-band allocation algorithms, including greedy selection, sequential greedy selection (SG), centralized <b>graph</b> coloring (CGC), and sequential iterative sub-band allocation (SISA). The study shows that for XR requirements, the SISA and SG algorithms can support IBS densities up to 75% higher than CGC.</p></p class="citation"></blockquote><h3 id=36--316350-full-duplex-mu-mimo-systems-with-coarse-quantization-how-many-bits-do-we-need-seunghyeong-yoo-et-al-2024>(3/6 | 316/350) Full-Duplex MU-MIMO Systems with Coarse Quantization: How Many Bits Do We Need? (Seunghyeong Yoo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Seunghyeong Yoo, Seokjun Park, Mintaek Oh, Namyoon Lee, Jinseok Choi. (2024)<br><strong>Full-Duplex MU-MIMO Systems with Coarse Quantization: How Many Bits Do We Need?</strong><br><button class=copy-to-clipboard title="Full-Duplex MU-MIMO Systems with Coarse Quantization: How Many Bits Do We Need?" index=316>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-316 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 23<br>Keywords: Benchmarking, Quantization, Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11762v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11762v2.pdf filename=2403.11762v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper investigates full-duplex (FD) multi-user multiple-input multiple-output (MU-MIMO) system design with coarse <b>quantization.</b> We first analyze the impact of self-interference (SI) on <b>quantization</b> in FD single-input single-output systems. The analysis elucidates that the minimum required number of analog-to-digital converter (ADC) bits is logarithmically proportional to the ratio of total received power to the received power of desired signals. Motivated by this, we design a FD MIMO beamforming method that effectively manages the SI. Dividing a spectral efficiency maximization beamforming problem into two sub-problems for alternating optimization, we address the first by optimizing the precoder: obtaining a generalized eigenvalue problem from the first-order optimality condition, where the principal eigenvector is the optimal stationary solution, and adopting a power iteration method to identify this eigenvector. Subsequently, a <b>quantization-aware</b> minimum mean square error combiner is computed for the derived precoder. Through numerical studies, we observe that the proposed beamformer reduces the minimum required number of ADC bits for achieving higher spectral efficiency than that of half-duplex (HD) systems, compared to FD <b>benchmarks.</b> The overall analysis shows that, unlike with <b>quantized</b> HD systems, more than 6 bits are required for the ADC to fully realize the potential of the <b>quantized</b> FD system.</p></p class="citation"></blockquote><h3 id=46--317350-sensing-enhanced-channel-estimation-for-near-field-xl-mimo-systems-shicong-liu-et-al-2024>(4/6 | 317/350) Sensing-Enhanced Channel Estimation for Near-Field XL-MIMO Systems (Shicong Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shicong Liu, Xianghao Yu, Zhen Gao, Jie Xu, Derrick Wing Kwan Ng, Shuguang Cui. (2024)<br><strong>Sensing-Enhanced Channel Estimation for Near-Field XL-MIMO Systems</strong><br><button class=copy-to-clipboard title="Sensing-Enhanced Channel Estimation for Near-Field XL-MIMO Systems" index=317>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-317 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11809v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11809v1.pdf filename=2403.11809v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Future sixth-generation (6G) systems are expected to leverage extremely large-scale multiple-input multiple-output (XL-MIMO) technology, which significantly expands the range of the near-field region. The spherical wavefront characteristics in the near field introduce additional degrees of freedom (DoFs), namely distance and angle, into the channel model, which leads to unique challenges in channel estimation (CE). In this paper, we propose a new sensing-enhanced uplink CE scheme for near-field XL-MIMO, which notably reduces the required quantity of baseband samples and the dictionary size. In particular, we first propose a sensing method that can be accomplished in a single time slot. It employs power sensors embedded within the antenna elements to measure the received power pattern rather than baseband samples. A time inversion algorithm is then proposed to precisely estimate the locations of users and scatterers, which offers a substantially lower computational complexity. Based on the estimated locations from sensing, a novel dictionary is then proposed by considering the eigen-problem based on the near-field transmission model, which facilitates efficient near-field CE with less baseband sampling and a more lightweight dictionary. Moreover, we derive the general form of the eigenvectors associated with the near-field channel matrix, revealing their noteworthy connection to the discrete prolate spheroidal sequence (DPSS). <b>Simulation</b> results unveil that the proposed time inversion algorithm achieves accurate localization with power measurements only, and remarkably outperforms various widely-adopted algorithms in terms of computational complexity. Furthermore, the proposed eigen-dictionary considerably improves the accuracy in CE with a compact dictionary size and a drastic reduction in baseband samples by up to 77%.</p></p class="citation"></blockquote><h3 id=56--318350-beamforming-design-for-semantic-bit-coexisting-communication-system-maojun-zhang-et-al-2024>(5/6 | 318/350) Beamforming Design for Semantic-Bit Coexisting Communication System (Maojun Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Maojun Zhang, Guangxu Zhu, Richeng Jin, Xiaoming Chen, Qingjiang Shi, Caijun Zhong, Kaibing Huang. (2024)<br><strong>Beamforming Design for Semantic-Bit Coexisting Communication System</strong><br><button class=copy-to-clipboard title="Beamforming Design for Semantic-Bit Coexisting Communication System" index=318>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-318 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11693v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11693v1.pdf filename=2403.11693v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Semantic communication (SemCom) is emerging as a key technology for future sixth-generation (6G) systems. Unlike traditional bit-level communication (BitCom), SemCom directly optimizes performance at the semantic level, leading to superior communication efficiency. Nevertheless, the task-oriented nature of SemCom renders it challenging to completely replace BitCom. Consequently, it is desired to consider a semantic-bit coexisting communication system, where a base station (BS) serves SemCom users (sem-users) and BitCom users (bit-users) simultaneously. Such a system faces severe and heterogeneous inter-user interference. In this context, this paper provides a new semantic-bit coexisting communication framework and proposes a spatial beamforming scheme to accommodate both types of users. Specifically, we consider maximizing the semantic rate for semantic users while ensuring the quality-of-service (QoS) requirements for bit-users. Due to the intractability of obtaining the exact closed-form expression of the semantic rate, a data driven method is first applied to attain an approximated expression via data fitting. With the resulting complex transcendental function, majorization minimization (MM) is adopted to convert the original formulated problem into a multiple-ratio problem, which allows fractional programming (FP) to be used to further transform the problem into an inhomogeneous quadratically constrained quadratic programs (QCQP) problem. Solving the problem leads to a semi-closed form solution with undetermined Lagrangian factors that can be updated by a fixed point algorithm. Extensive <b>simulation</b> results demonstrate that the proposed beamforming scheme significantly outperforms conventional beamforming algorithms such as zero-forcing (ZF), maximum ratio transmission (MRT), and weighted minimum mean-square error (WMMSE).</p></p class="citation"></blockquote><h3 id=66--319350-towards-a-theory-of-pragmatic-information-edward-d-weinberger-2024>(6/6 | 319/350) Towards a Theory of Pragmatic Information (Edward D. Weinberger, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Edward D. Weinberger. (2024)<br><strong>Towards a Theory of Pragmatic Information</strong><br><button class=copy-to-clipboard title="Towards a Theory of Pragmatic Information" index=319>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-319 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: H-1-1, cs-IT, cs.IT, math-IT<br>Keyword Score: 10<br>Keywords: Bandit Algorithm<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12324v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12324v1.pdf filename=2403.12324v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The subject generally known as <code>information theory'' has nothing to say about how much meaning is conveyed by the information. Accordingly, we fill this gap with the first rigorously justifiable, quantitative definition of </code>pragmatic information&rsquo;&rsquo; as the amount of information that becomes meaningful because it is used in making a decision. We posit that such information updates a <code>state of the world'' random variable, $\omega$, that informs the decision. The pragmatic information of a single message is then defined as the Kulbach-Leibler divergence between the a priori and updated probability distributions of $\omega$, and the pragmatic information of a message ensemble is defined as the expected value of the pragmatic information values of the ensemble's component messages. We justify these definitions by showing, first, that the pragmatic information of a single message is the expected difference between the shortest binary encoding of $\omega$ under the {\it a priori} and updated probability distributions, and, second, that the average of the pragmatic values of individual messages, when sampled a large number of times from the ensemble, approaches its expected value. The resulting pragmatic information formulas have many hoped-for properties, such as non-negativity and additivity for independent decisions and </code>pragmatically independent&rsquo;&rsquo; messages. We also sketch two applications of these formulas: The first is the single play of a slot machine, a.k.a. a ``one armed <b>bandit&rsquo;&rsquo;,</b> with an unknown probability of payout; the second being the reformulation of the efficient market hypothesis of financial economics as the claim that the pragmatic information content of all available data about a given security is zero.</p></p class="citation"></blockquote><h2 id=cshc-5>cs.HC (5)</h2><h3 id=15--320350-simulating-wearable-urban-augmented-reality-experiences-in-vr-lessons-learnt-from-designing-two-future-urban-interfaces-tram-thi-minh-tran-et-al-2024>(1/5 | 320/350) Simulating Wearable Urban Augmented Reality Experiences in VR: Lessons Learnt from Designing Two Future Urban Interfaces (Tram Thi Minh Tran et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tram Thi Minh Tran, Callum Parker, Marius Hoggenmüller, Luke Hespanhol, Martin Tomitsch. (2024)<br><strong>Simulating Wearable Urban Augmented Reality Experiences in VR: Lessons Learnt from Designing Two Future Urban Interfaces</strong><br><button class=copy-to-clipboard title="Simulating Wearable Urban Augmented Reality Experiences in VR: Lessons Learnt from Designing Two Future Urban Interfaces" index=320>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-320 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 30<br>Keywords: Recommendation, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11377v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11377v1.pdf filename=2403.11377v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Augmented reality (AR) has the potential to fundamentally change how people engage with increasingly interactive urban environments. However, many challenges exist in designing and evaluating these new urban AR experiences, such as technical constraints and safety concerns associated with outdoor AR. We contribute to this domain by assessing the use of virtual reality (VR) for simulating wearable urban AR experiences, allowing participants to interact with future AR interfaces in a realistic, safe and controlled setting. This paper describes two wearable urban AR applications (pedestrian navigation and autonomous mobility) simulated in VR. Based on a thematic analysis of interview data collected across the two studies, we found that the VR <b>simulation</b> successfully elicited feedback on the functional benefits of AR concepts and the potential impact of urban contextual factors, such as safety concerns, attentional capacity, and social considerations. At the same time, we highlighted the limitations of this approach in terms of assessing the AR interface&rsquo;s visual quality and providing exhaustive contextual information. The paper concludes with <b>recommendations</b> for simulating wearable urban AR experiences in VR.</p></p class="citation"></blockquote><h3 id=25--321350-the-value-benefits-and-concerns-of-generative-ai-powered-assistance-in-writing-zhuoyan-li-et-al-2024>(2/5 | 321/350) The Value, Benefits, and Concerns of Generative AI-Powered Assistance in Writing (Zhuoyan Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhuoyan Li, Chen Liang, Jing Peng, Ming Yin. (2024)<br><strong>The Value, Benefits, and Concerns of Generative AI-Powered Assistance in Writing</strong><br><button class=copy-to-clipboard title="The Value, Benefits, and Concerns of Generative AI-Powered Assistance in Writing" index=321>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-321 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 20<br>Keywords: Generative AI, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12004v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12004v1.pdf filename=2403.12004v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advances in <b>generative</b> <b>AI</b> technologies like <b>large</b> <b>language</b> <b>models</b> raise both excitement and concerns about the future of human-AI co-creation in writing. To unpack people&rsquo;s attitude towards and experience with <b>generative</b> <b>AI-powered</b> writing assistants, in this paper, we conduct an experiment to understand whether and how much value people attach to AI assistance, and how the incorporation of AI assistance in writing workflows changes people&rsquo;s writing perceptions and performance. Our results suggest that people are willing to forgo financial payments to receive writing assistance from AI, especially if AI can provide direct content generation assistance and the writing task is highly creative. <b>Generative</b> <b>AI-powered</b> assistance is found to offer benefits in increasing people&rsquo;s productivity and confidence in writing. However, direct content generation assistance offered by AI also comes with risks, including decreasing people&rsquo;s sense of accountability and diversity in writing. We conclude by discussing the implications of our findings.</p></p class="citation"></blockquote><h3 id=35--322350-explainable-agency-human-preferences-for-simple-or-complex-explanations-michelle-blom-et-al-2024>(3/5 | 322/350) Explainable agency: human preferences for simple or complex explanations (Michelle Blom et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michelle Blom, Ronal Singh, Tim Miller, Liz Sonenberg, Kerry Trentelman, Adam Saulwick. (2024)<br><strong>Explainable agency: human preferences for simple or complex explanations</strong><br><button class=copy-to-clipboard title="Explainable agency: human preferences for simple or complex explanations" index=322>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-322 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12321v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12321v1.pdf filename=2403.12321v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Research in cognitive psychology has established that whether people prefer simpler explanations to complex ones is context dependent, but the question of `simple vs. complex&rsquo; becomes critical when an artificial agent seeks to explain its decisions or predictions to humans. We present a model for abstracting causal <b>reasoning</b> chains for the purpose of explanation. This model uses a set of rules to progressively abstract different types of causal information in causal proof traces. We perform online studies using 123 Amazon MTurk participants and with five industry experts over two domains: maritime patrol and weather prediction. We found participants&rsquo; satisfaction with generated explanations was based on the consistency of relationships among the causes (coherence) that explain an event; and that the important question is not whether people prefer simple or complex explanations, but what types of causal information are relevant to individuals in specific contexts.</p></p class="citation"></blockquote><h3 id=45--323350-a-systematic-review-of-xr-based-remote-human-robot-interaction-systems-xian-wang-et-al-2024>(4/5 | 323/350) A Systematic Review of XR-based Remote Human-Robot Interaction Systems (Xian Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xian Wang, Luyao Shen, Lik-Hang Lee. (2024)<br><strong>A Systematic Review of XR-based Remote Human-Robot Interaction Systems</strong><br><button class=copy-to-clipboard title="A Systematic Review of XR-based Remote Human-Robot Interaction Systems" index=323>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-323 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs-RO, cs.HC<br>Keyword Score: 10<br>Keywords: Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11384v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11384v1.pdf filename=2403.11384v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This survey provides an exhaustive review of the applications of extended reality (XR) technologies in the field of remote human-computer interaction (HRI). We developed a systematic search strategy based on the PRISMA methodology. From the initial 2,561 articles selected, 100 research papers that met our inclusion criteria were included. We categorized and <b>summarized</b> the domain in detail, delving into XR technologies, including augmented reality (AR), virtual reality (VR), and mixed reality (MR), and their applications in facilitating intuitive and effective remote control and interaction with robotic systems.The survey highlights existing articles on the application of XR technologies, user experience enhancement, and various interaction designs for XR in remote HRI, providing insights into current trends and future directions. We also identified potential gaps and opportunities for future research to improve remote HRI systems through XR technology to guide and inform future XR and robotics research.</p></p class="citation"></blockquote><h3 id=55--324350-a-review-of-virtual-reality-studies-on-autonomous-vehicle--pedestrian-interaction-tram-thi-minh-tran-et-al-2024>(5/5 | 324/350) A Review of Virtual Reality Studies on Autonomous Vehicle&ndash;Pedestrian Interaction (Tram Thi Minh Tran et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tram Thi Minh Tran, Callum Parker, Martin Tomitsch. (2024)<br><strong>A Review of Virtual Reality Studies on Autonomous Vehicle&ndash;Pedestrian Interaction</strong><br><button class=copy-to-clipboard title="A Review of Virtual Reality Studies on Autonomous Vehicle--Pedestrian Interaction" index=324>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-324 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11378v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11378v1.pdf filename=2403.11378v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>An increasing number of studies employ virtual reality (VR) to evaluate interactions between autonomous vehicles (AVs) and pedestrians. VR simulators are valued for their cost-effectiveness, flexibility in developing various traffic scenarios, safe conduct of user studies, and acceptable ecological validity. Reviewing the literature between 2010 and 2020, we found 31 empirical studies using VR as a testing apparatus for both implicit and explicit communication. By performing a systematic analysis, we identified current coverage of critical use cases, obtained a comprehensive account of factors influencing pedestrian behavior in simulated traffic scenarios, and assessed evaluation measures. Based on the findings, we present a set of <b>recommendations</b> for implementing VR pedestrian simulators and propose directions for future research.</p></p class="citation"></blockquote><h2 id=eesssp-1>eess.SP (1)</h2><h3 id=11--325350-adaptive-lpd-radar-waveform-design-with-generative-deep-learning-matthew-r-ziemann-et-al-2024>(1/1 | 325/350) Adaptive LPD Radar Waveform Design with Generative Deep Learning (Matthew R. Ziemann et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Matthew R. Ziemann, Christopher A. Metzler. (2024)<br><strong>Adaptive LPD Radar Waveform Design with Generative Deep Learning</strong><br><button class=copy-to-clipboard title="Adaptive LPD Radar Waveform Design with Generative Deep Learning" index=325>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-325 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SP<br>Categories: cs-LG, eess-SP, eess.SP<br>Keyword Score: 20<br>Keywords: Adversarial Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12254v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12254v1.pdf filename=2403.12254v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose a novel, learning-based method for adaptively generating low probability of detection (LPD) radar waveforms that blend into their operating environment. Our waveforms are designed to follow a distribution that is indistinguishable from the ambient radio frequency (RF) background &ndash; while still being effective at ranging and sensing. To do so, we use an <b>unsupervised,</b> <b>adversarial</b> <b>learning</b> framework; our generator network produces waveforms designed to confuse a critic network, which is optimized to differentiate generated waveforms from the background. To ensure our generated waveforms are still effective for sensing, we introduce and minimize an ambiguity function-based loss on the generated waveforms. We evaluate the performance of our method by comparing the single-pulse detectability of our generated waveforms with traditional LPD waveforms using a separately trained detection neural network. We find that our method can generate LPD waveforms that reduce detectability by up to 90% while simultaneously offering improved ambiguity function (sensing) characteristics. Our framework also provides a mechanism to trade-off detectability and sensing performance.</p></p class="citation"></blockquote><h2 id=mathna-4>math.NA (4)</h2><h3 id=14--326350-on-the-convergence-of-a-data-driven-regularized-stochastic-gradient-descent-for-nonlinear-ill-posed-problems-zehui-zhou-2024>(1/4 | 326/350) On the Convergence of A Data-Driven Regularized Stochastic Gradient Descent for Nonlinear Ill-Posed Problems (Zehui Zhou, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zehui Zhou. (2024)<br><strong>On the Convergence of A Data-Driven Regularized Stochastic Gradient Descent for Nonlinear Ill-Posed Problems</strong><br><button class=copy-to-clipboard title="On the Convergence of A Data-Driven Regularized Stochastic Gradient Descent for Nonlinear Ill-Posed Problems" index=326>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-326 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: 65J20, 65J22, 47J06, cs-NA, math-NA, math-OC, math.NA<br>Keyword Score: 20<br>Keywords: Stochastic Gradient Descent, Stochastic Gradient Descent<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11787v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11787v1.pdf filename=2403.11787v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Stochastic</b> <b>gradient</b> <b>descent</b> <b>(SGD)</b> is a promising method for solving large-scale inverse problems, due to its excellent scalability with respect to data size. In this work, we analyze a new data-driven regularized <b>stochastic</b> <b>gradient</b> <b>descent</b> for the efficient numerical solution of a class of nonlinear ill-posed inverse problems in infinite dimensional Hilbert spaces. At each step of the iteration, the method randomly selects one equation from the nonlinear system combined with a corresponding equation from the learned system based on training data to obtain a <b>stochastic</b> <b>estimate</b> <b>of</b> the gradient and then performs a descent step with the estimated gradient. We prove the regularizing property of this method under the tangential cone condition and a priori parameter choice and then derive the convergence rates under the additional source condition and range invariance conditions. Several numerical experiments are provided to complement the analysis.</p></p class="citation"></blockquote><h3 id=24--327350-a-physics-informed-neural-network-method-for-the-approximation-of-slow-invariant-manifolds-for-the-general-class-of-stiff-systems-of-odes-dimitrios-g-patsatzis-et-al-2024>(2/4 | 327/350) A physics-informed neural network method for the approximation of slow invariant manifolds for the general class of stiff systems of ODEs (Dimitrios G. Patsatzis et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dimitrios G. Patsatzis, Lucia Russo, Constantinos Siettos. (2024)<br><strong>A physics-informed neural network method for the approximation of slow invariant manifolds for the general class of stiff systems of ODEs</strong><br><button class=copy-to-clipboard title="A physics-informed neural network method for the approximation of slow invariant manifolds for the general class of stiff systems of ODEs" index=327>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-327 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: 65P99, 65L04, 37M21, 68T07, cs-LG, cs-NA, math-DS, math-NA, math.NA<br>Keyword Score: 8<br>Keywords: Benchmarking, Black Box<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11591v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11591v1.pdf filename=2403.11591v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a physics-informed neural network (PINN) approach for the discovery of slow invariant manifolds (SIMs), for the most general class of fast/slow dynamical systems of ODEs. In contrast to other machine learning (ML) approaches that construct reduced order <b>black</b> <b>box</b> surrogate models using simple regression, and/or require a priori knowledge of the fast and slow variables, our approach, simultaneously decomposes the vector field into fast and slow components and provides a functional of the underlying SIM in a closed form. The decomposition is achieved by finding a transformation of the state variables to the fast and slow ones, which enables the derivation of an explicit, in terms of fast variables, SIM functional. The latter is obtained by solving a PDE corresponding to the invariance equation within the Geometric Singular Perturbation Theory (GSPT) using a single-layer feedforward neural network with symbolic differentiation. The performance of the proposed physics-informed ML framework is assessed via three <b>benchmark</b> problems: the Michaelis-Menten, the target mediated drug disposition (TMDD) reaction model and a fully competitive substrate-inhibitor(fCSI) mechanism. We also provide a comparison with other GPST methods, namely the quasi steady state approximation (QSSA), the partial equilibrium approximation (PEA) and CSP with one and two iterations. We show that the proposed PINN scheme provides SIM approximations, of equivalent or even higher accuracy, than those provided by QSSA, PEA and CSP, especially close to the boundaries of the underlying SIMs.</p></p class="citation"></blockquote><h3 id=34--328350-a-restricted-additive-smoother-for-finite-cell-flow-problems-s-saberi-et-al-2024>(3/4 | 328/350) A restricted additive smoother for finite cell flow problems (S. Saberi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>S. Saberi, A. Vogel. (2024)<br><strong>A restricted additive smoother for finite cell flow problems</strong><br><button class=copy-to-clipboard title="A restricted additive smoother for finite cell flow problems" index=328>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-328 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-MP, math-NA, math-ph, math.NA<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11636v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11636v1.pdf filename=2403.11636v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we propose an adaptive geometric multigrid method for the solution of large-scale finite cell flow problems. The finite cell method seeks to circumvent the need for a boundary-conforming mesh through the embedding of the physical domain in a regular background mesh. As a result of the intersection between the physical domain and the background computational mesh, the resultant systems of equations are typically numerically ill-conditioned, rendering the appropriate treatment of cutcells a crucial aspect of the solver. To this end, we propose a smoother operator with favorable parallel properties and discuss its memory footprint and parallelization aspects. We propose three cache policies that offer a balance between cached and on-the-fly computation and discuss the optimization opportunities offered by the smoother operator. It is shown that the smoother operator, on account of its additive nature, can be replicated in parallel exactly with little communication overhead, which offers a major advantage in parallel settings as the geometric multigrid solver is consequently independent of the number of processes. The convergence and scalability of the geometric multigrid method is studied using numerical examples. It is shown that the iteration count of the solver remains bounded independent of the problem size and depth of the grid hierarchy. The solver is shown to obtain excellent weak and strong scaling using numerical <b>benchmarks</b> with more than 665 million degrees of freedom. The presented geometric multigrid solver is, therefore, an attractive option for the solution of large-scale finite cell problems in massively parallel high-performance computing environments.</p></p class="citation"></blockquote><h3 id=44--329350-data-driven-stabilization-of-nitsches-method-s-saberi-et-al-2024>(4/4 | 329/350) Data-driven Stabilization of Nitsche&rsquo;s Method (S. Saberi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>S. Saberi, L. Zhang, A. Vogel. (2024)<br><strong>Data-driven Stabilization of Nitsche&rsquo;s Method</strong><br><button class=copy-to-clipboard title="Data-driven Stabilization of Nitsche's Method" index=329>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-329 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-MP, math-NA, math-ph, math.NA<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11632v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11632v1.pdf filename=2403.11632v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The weak imposition of essential boundary conditions is an integral aspect of unfitted finite element methods, where the physical boundary does not in general coincide with the computational domain. In this regard, the symmetric Nitsche&rsquo;s method is a powerful technique that preserves the symmetry and variational consistency of the unmodified weak formulation. The stabilization parameter in Nitsche&rsquo;s method plays a crucial role in the stability of the resultant formulation, whose estimation is computationally intensive and dependent on the particular cut configuration using the conventional eigenvalue-based approach. In this work, we employ as model problem the finite cell method in which the need for the generation of a boundary-conforming mesh is circumvented by embedding the physical domain in a, typically regular, background mesh. We propose a data-driven estimate based on machine learning methods for the estimation of the stabilization parameter in Nitsche&rsquo;s method that offers an efficient constant-complexity alternative to the eigenvalue-based approach independent of the cut configuration. It is shown, using numerical <b>benchmarks,</b> that the proposed method can estimate the stabilization parameter accurately and is by far more computationally efficient. The data-driven estimate can be integrated into existing numerical codes with minimal modifications and thanks to the wide adoption of accelerators such as GPUs by machine learning frameworks, can be used with virtually no extra implementation cost on GPU devices, further increasing the potential for computational gains over the conventional eigenvalue-based estimate.</p></p class="citation"></blockquote><h2 id=cond-matstr-el-1>cond-mat.str-el (1)</h2><h3 id=11--330350-coarsening-of-chiral-domains-in-itinerant-electron-magnets-a-machine-learning-force-field-approach-yunhao-fan-et-al-2024>(1/1 | 330/350) Coarsening of chiral domains in itinerant electron magnets: A machine learning force field approach (Yunhao Fan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yunhao Fan, Sheng Zhang, Gia-Wei Chern. (2024)<br><strong>Coarsening of chiral domains in itinerant electron magnets: A machine learning force field approach</strong><br><button class=copy-to-clipboard title="Coarsening of chiral domains in itinerant electron magnets: A machine learning force field approach" index=330>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-330 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cond-mat.str-el<br>Categories: cond-mat-str-el, cond-mat.str-el, cs-LG<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11705v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11705v1.pdf filename=2403.11705v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Frustrated itinerant magnets often exhibit complex noncollinear or noncoplanar magnetic orders which support topological electronic structures. A canonical example is the anomalous quantum Hall state with a chiral spin order stabilized by electron-spin interactions on a triangular lattice. While a long-range magnetic order cannot survive thermal fluctuations in two dimensions, the chiral order which results from the breaking of a discrete Ising symmetry persists even at finite temperatures. We present a scalable machine learning (ML) framework to model the complex electron-mediated spin-spin interactions that stabilize the chiral magnetic domains in a triangular lattice. Large-scale dynamical <b>simulations,</b> enabled by the ML force-field models, are performed to investigate the coarsening of chiral domains after a thermal quench. While the chiral phase is described by a broken $Z_2$ Ising-type symmetry, we find that the characteristic size of chiral domains increases linearly with time, in stark contrast to the expected Allen-Cahn domain growth law for a non-conserved Ising order parameter field. The linear growth of the chiral domains is attributed to the orientational anisotropy of domain boundaries. Our work also demonstrates the promising potential of ML models for large-scale spin dynamics of itinerant magnets.</p></p class="citation"></blockquote><h2 id=csgr-2>cs.GR (2)</h2><h3 id=12--331350-qean-quaternion-enhanced-attention-network-for-visual-dance-generation-zhizhen-zhou-et-al-2024>(1/2 | 331/350) QEAN: Quaternion-Enhanced Attention Network for Visual Dance Generation (Zhizhen Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhizhen Zhou, Yejing Huo, Guoheng Huang, An Zeng, Xuhang Chen, Lian Huang, Zinuo Li. (2024)<br><strong>QEAN: Quaternion-Enhanced Attention Network for Visual Dance Generation</strong><br><button class=copy-to-clipboard title="QEAN: Quaternion-Enhanced Attention Network for Visual Dance Generation" index=331>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-331 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GR<br>Categories: cs-AI, cs-CV, cs-GR, cs-MM, cs-SD, cs.GR, eess-AS<br>Keyword Score: 20<br>Keywords: Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11626v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11626v1.pdf filename=2403.11626v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The study of music-generated dance is a novel and challenging Image generation task. It aims to input a piece of music and seed motions, then generate natural dance movements for the subsequent music. <b>Transformer-based</b> methods face challenges in time series prediction tasks related to human movements and music due to their struggle in capturing the nonlinear relationship and temporal aspects. This can lead to issues like joint deformation, role deviation, floating, and inconsistencies in dance movements generated in response to the music. In this paper, we propose a Quaternion-Enhanced Attention Network (QEAN) for visual dance synthesis from a quaternion perspective, which consists of a Spin Position Embedding (SPE) module and a Quaternion Rotary Attention (QRA) module. First, SPE embeds position information into <b>self-attention</b> in a rotational manner, leading to better learning of features of movement sequences and audio sequences, and improved understanding of the connection between music and dance. Second, QRA represents and fuses 3D motion features and audio features in the form of a series of quaternions, enabling the model to better learn the temporal coordination of music and dance under the complex temporal cycle conditions of dance generation. Finally, we conducted experiments on the dataset AIST++, and the results show that our approach achieves better and more robust performance in generating accurate, high-quality dance movements. Our source code and dataset can be available from <a href=https://github.com/MarasyZZ/QEAN>https://github.com/MarasyZZ/QEAN</a> and <a href=https://google.github.io/aistplusplus_dataset>https://google.github.io/aistplusplus_dataset</a> respectively.</p></p class="citation"></blockquote><h3 id=22--332350-bridging-3d-gaussian-and-mesh-for-freeview-video-rendering-yuting-xiao-et-al-2024>(2/2 | 332/350) Bridging 3D Gaussian and Mesh for Freeview Video Rendering (Yuting Xiao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuting Xiao, Xuan Wang, Jiafei Li, Hongrui Cai, Yanbo Fan, Nan Xue, Minghui Yang, Yujun Shen, Shenghua Gao. (2024)<br><strong>Bridging 3D Gaussian and Mesh for Freeview Video Rendering</strong><br><button class=copy-to-clipboard title="Bridging 3D Gaussian and Mesh for Freeview Video Rendering" index=332>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-332 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GR<br>Categories: cs-CV, cs-GR, cs.GR<br>Keyword Score: 15<br>Keywords: Geometry, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11453v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11453v1.pdf filename=2403.11453v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This is only a preview version of GauMesh. Recently, primitive-based rendering has been proven to achieve convincing results in solving the problem of modeling and rendering the 3D dynamic scene from 2D images. Despite this, in the context of novel view synthesis, each type of primitive has its inherent defects in terms of representation ability. It is difficult to exploit the mesh to depict the fuzzy <b>geometry.</b> Meanwhile, the point-based splatting (e.g. the 3D Gaussian Splatting) method usually produces artifacts or blurry pixels in the area with smooth <b>geometry</b> and sharp textures. As a result, it is difficult, even not impossible, to represent the complex and dynamic scene with a single type of primitive. To this end, we propose a novel approach, GauMesh, to bridge the 3D Gaussian and Mesh for modeling and rendering the dynamic scenes. Given a sequence of tracked mesh as initialization, our goal is to simultaneously optimize the mesh <b>geometry,</b> color texture, opacity maps, a set of 3D Gaussians, and the deformation field. At a specific time, we perform $\alpha$-blending on the RGB and opacity values based on the merged and re-ordered z-buffers from mesh and 3D Gaussian rasterizations. This produces the final rendering, which is <b>supervised</b> by the ground-truth image. Experiments demonstrate that our approach adapts the appropriate type of primitives to represent the different parts of the dynamic scene and outperforms all the baseline methods in both quantitative and qualitative comparisons without losing render speed.</p></p class="citation"></blockquote><h2 id=csgt-1>cs.GT (1)</h2><h3 id=11--333350-information-compression-in-dynamic-information-disclosure-games-dengwang-tang-et-al-2024>(1/1 | 333/350) Information Compression in Dynamic Information Disclosure Games (Dengwang Tang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dengwang Tang, Vijay G. Subramanian. (2024)<br><strong>Information Compression in Dynamic Information Disclosure Games</strong><br><button class=copy-to-clipboard title="Information Compression in Dynamic Information Disclosure Games" index=333>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-333 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-GT, cs-MA, cs-SY, cs.GT, eess-SY, math-OC<br>Keyword Score: 10<br>Keywords: Information Compression<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12204v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12204v1.pdf filename=2403.12204v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider a two-player dynamic <b>information</b> <b>design</b> problem between a principal and a receiver &ndash; a game is played between the two agents on top of a Markovian system controlled by the receiver&rsquo;s actions, where the principal obtains and strategically shares some <b>information</b> <b>about</b> the underlying system with the receiver in order to influence their actions. In our setting, both players have long-term objectives, and the principal sequentially commits to their strategies instead of committing at the beginning. Further, the principal cannot directly observe the system state, but at every turn they can choose randomized experiments to observe the system partially. The principal can share details about the experiments to the receiver. For our analysis we impose the truthful disclosure rule: the principal is required to truthfully announce the details and the result of each experiment to the receiver immediately after the experiment result is revealed. Based on the received <b>information,</b> <b>the</b> receiver takes an action when its their turn, with the action influencing the state of the underlying system. We show that there exist Perfect Bayesian equilibria in this game where both agents play Canonical Belief Based (CBB) strategies using a compressed version of their <b>information,</b> <b>rather</b> than full <b>information,</b> <b>to</b> choose experiments (for the principal) or actions (for the receiver). We also provide a backward inductive procedure to solve for an equilibrium in CBB strategies.</p></p class="citation"></blockquote><h2 id=mathco-2>math.CO (2)</h2><h3 id=12--334350-packit-gamified-rectangle-packing-thomas-garrison-et-al-2024>(1/2 | 334/350) PackIt! Gamified Rectangle Packing (Thomas Garrison et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Thomas Garrison, Marijn J. H. Heule, Bernardo Subercaseaux. (2024)<br><strong>PackIt! Gamified Rectangle Packing</strong><br><button class=copy-to-clipboard title="PackIt! Gamified Rectangle Packing" index=334>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-334 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.CO<br>Categories: cs-DS, cs-GT, math-CO, math.CO<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12195v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12195v1.pdf filename=2403.12195v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present and analyze PackIt!, a turn-based game consisting of packing rectangles on an $n \times n$ grid. PackIt! can be easily played on paper, either as a competitive two-player game or in \emph{solitaire} fashion. On the $t$-th turn, a rectangle of area $t$ or $t+1$ must be placed in the grid. In the two-player format of PackIt! whichever player places a rectangle last wins, whereas the goal in the solitaire variant is to perfectly pack the $n \times n$ grid. We analyze conditions for the existence of a perfect packing over $n \times n$, then present an automated <b>reasoning</b> approach that allows finding perfect games of PackIt! up to $n = 50$ which includes a novel SAT-encoding technique of independent interest, and conclude by proving an NP-hardness result.</p></p class="citation"></blockquote><h3 id=22--335350-the-real-tropical-geometry-of-neural-networks-marie-charlotte-brandenburg-et-al-2024>(2/2 | 335/350) The Real Tropical Geometry of Neural Networks (Marie-Charlotte Brandenburg et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marie-Charlotte Brandenburg, Georg Loho, Guido Montúfar. (2024)<br><strong>The Real Tropical Geometry of Neural Networks</strong><br><button class=copy-to-clipboard title="The Real Tropical Geometry of Neural Networks" index=335>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-335 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.CO<br>Categories: 14T90, 52C45, 68T07 (Primary), 14P10, 52C35 (Secondary), cs-LG, math-CO, math.CO<br>Keyword Score: 8<br>Keywords: Graph, Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11871v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11871v1.pdf filename=2403.11871v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider a binary classifier defined as the sign of a tropical rational function, that is, as the difference of two convex piecewise linear functions. The parameter space of ReLU neural networks is contained as a semialgebraic set inside the parameter space of tropical rational functions. We initiate the study of two different subdivisions of this parameter space: a subdivision into semialgebraic sets, on which the combinatorial type of the decision boundary is fixed, and a subdivision into a polyhedral fan, capturing the combinatorics of the partitions of the dataset. The sublevel sets of the 0/1-loss function arise as subfans of this classification fan, and we show that the level-sets are not necessarily connected. We describe the classification fan i) geometrically, as normal fan of the activation polytope, and ii) combinatorially through a list of properties of associated bipartite <b>graphs,</b> in analogy to covector axioms of oriented matroids and tropical oriented matroids. Our findings extend and refine the connection between neural networks and tropical <b>geometry</b> by observing structures established in real tropical <b>geometry,</b> such as positive tropicalizations of hypersurfaces and tropical semialgebraic sets.</p></p class="citation"></blockquote><h2 id=csos-1>cs.OS (1)</h2><h3 id=11--336350-llm-as-a-system-service-on-mobile-devices-wangsong-yin-et-al-2024>(1/1 | 336/350) LLM as a System Service on Mobile Devices (Wangsong Yin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wangsong Yin, Mengwei Xu, Yuanchun Li, Xuanzhe Liu. (2024)<br><strong>LLM as a System Service on Mobile Devices</strong><br><button class=copy-to-clipboard title="LLM as a System Service on Mobile Devices" index=336>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-336 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.OS<br>Categories: cs-OS, cs.OS<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11805v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11805v1.pdf filename=2403.11805v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Being more powerful and intrusive into user-device interactions, <b>LLMs</b> are eager for on-device execution to better preserve user privacy. In this work, we propose a new paradigm of mobile AI: <b>LLM</b> as a system service on mobile devices (LLMaaS). Unlike traditional DNNs that execute in a stateless manner, such a system service is stateful: <b>LLMs</b> execution often needs to maintain persistent states (mainly KV cache) across multiple invocations. To minimize the <b>LLM</b> context switching overhead under tight device memory budget, this work presents <b>LLMS,</b> which decouples the memory management of app and <b>LLM</b> contexts with a key idea of fine-grained, chunk-wise, globally-optimized KV cache compression and swapping. By fully leveraging KV cache&rsquo;s unique characteristics, it proposes three novel techniques: (1) Tolerance-Aware Compression: it compresses chunks based on their measured accuracy tolerance to compression. (2) IO-Recompute Pipelined Loading: it introduces recompute to swapping-in for acceleration. (3) Chunk Lifecycle Management: it optimizes the memory activities of chunks with an ahead-of-time swapping-out and an LCTRU (Least Compression-Tolerable and Recently-Used) queue based eviction. In evaluations conducted on well-established traces and various edge devices, \sys reduces context switching latency by up to 2 orders of magnitude when compared to competitive baseline solutions.</p></p class="citation"></blockquote><h2 id=csse-3>cs.SE (3)</h2><h3 id=13--337350-empowerability-a-portal-for-employment--scholarships-for-differently-abled-himanshu-raj-et-al-2024>(1/3 | 337/350) EmpowerAbility: A portal for employment & scholarships for differently-abled (Himanshu Raj et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Himanshu Raj, Shubham Kumar, Dr. J Kalaivani. (2024)<br><strong>EmpowerAbility: A portal for employment & scholarships for differently-abled</strong><br><button class=copy-to-clipboard title="EmpowerAbility: A portal for employment & scholarships for differently-abled" index=337>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-337 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 10<br>Keywords: Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11769v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11769v1.pdf filename=2403.11769v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The internet has become a vital resource for job seekers in today&rsquo;s technologically advanced world, particularly for those with impairments. They mainly rely on internet resources to find jobs that fit their particular requirements and skill set. Though some disabled candidates receive <b>prompt</b> responses and job offers, others find it difficult to traverse the intricate world of job portals, the efficacy of this process frequently varies. This discrepancy results from a typical error: a failure to completely comprehend and utilize the accessibility features and functions that can significantly expedite and simplify the job search process for people with impairments.This project is a job and scholarship portal that empowers individuals with diverse abilities. Through inspiring success stories, user-centric features, and practical opportunities, it fosters resilience and inclusivity while reshaping narratives. This platform&rsquo;s dual-pronged strategy instills pride and offers real-world solutions, making a lasting impact on the lives it touches.</p></p class="citation"></blockquote><h3 id=23--338350-on-the-integration-of-spectrum-based-fault-localization-tools-into-ides-attila-szatmári-et-al-2024>(2/3 | 338/350) On the Integration of Spectrum-Based Fault Localization Tools into IDEs (Attila Szatmári et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Attila Szatmári, Qusay Idrees Sarhan, Gergő Balogh, Péter Attila Soha, Árpád Beszédes. (2024)<br><strong>On the Integration of Spectrum-Based Fault Localization Tools into IDEs</strong><br><button class=copy-to-clipboard title="On the Integration of Spectrum-Based Fault Localization Tools into IDEs" index=338>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-338 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11538v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11538v1.pdf filename=2403.11538v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Spectrum-Based Fault Localization (SBFL) is a technique to be used during debugging, the premise of which is that, based on the test case outcomes and code coverage, faulty code elements can be automatically detected. SBFL is popular among researchers because it is lightweight and easy to implement, and there is a lot of potential in it when it comes to research that aims to improve its effectiveness. Despite this, the technique cannot be found in contemporary development and debugging tools, only a handful of research prototypes are available. Reasons for this can be multiple, including the algortihms&rsquo; sub-optimal effectiveness and other technical weaknesses. But, also the lack of clear functional and non-functional requirements for such a tool, either standalone or integrated into IDEs. In this paper, we attempt to provide such a list in form of <b>recommendations,</b> based on surveying the most popular SBFL tools and on our own researchers&rsquo; and tool builders&rsquo; experience.</p></p class="citation"></blockquote><h3 id=33--339350-empirical-analysis-on-cicd-pipeline-evolution-in-machine-learning-projects-alaa-houerbi-et-al-2024>(3/3 | 339/350) Empirical Analysis on CI/CD Pipeline Evolution in Machine Learning Projects (Alaa Houerbi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alaa Houerbi, Chadha Siala, Alexis Tucker, Dhia Elhaq Rzig, Foyzul Hassan. (2024)<br><strong>Empirical Analysis on CI/CD Pipeline Evolution in Machine Learning Projects</strong><br><button class=copy-to-clipboard title="Empirical Analysis on CI/CD Pipeline Evolution in Machine Learning Projects" index=339>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-339 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 3<br>Keywords: Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12199v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12199v1.pdf filename=2403.12199v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The growing popularity of machine learning (ML) and the integration of ML components with other software artifacts has led to the use of continuous integration and delivery (CI/CD) tools, such as Travis CI, GitHub Actions, etc. that enable faster integration and testing for ML projects. Such CI/CD configurations and services require synchronization during the life cycle of the projects. Several works discussed how CI/CD configuration and services change during their usage in traditional software systems. However, there is very limited knowledge of how CI/CD configuration and services change in ML projects. To fill this knowledge gap, this work presents the first empirical analysis of how CI/CD configuration evolves for ML software systems. We manually analyzed 343 commits collected from 508 open-source ML projects to identify common CI/CD configuration change categories in ML projects and devised a taxonomy of 14 co-changes in CI/CD and ML components. Moreover, we developed a CI/CD configuration change <b>clustering</b> tool that identified frequent CI/CD configuration change patterns in 15,634 commits. Furthermore, we measured the expertise of ML developers who modify CI/CD configurations. Based on this analysis, we found that 61.8% of commits include a change to the build policy and minimal changes related to performance and maintainability compared to general open-source projects. Additionally, the co-evolution analysis identified that CI/CD configurations, in many cases, changed unnecessarily due to bad practices such as the direct inclusion of dependencies and a lack of usage of standardized testing frameworks. More practices were found through the change patterns analysis consisting of using deprecated settings and reliance on a generic build language. Finally, our developer&rsquo;s expertise analysis suggests that experienced developers are more inclined to modify CI/CD configurations.</p></p class="citation"></blockquote><h2 id=mathoc-2>math.OC (2)</h2><h3 id=12--340350-convex-co-design-of-control-barrier-function-and-safe-feedback-controller-under-input-constraints-han-wang-et-al-2024>(1/2 | 340/350) Convex Co-Design of Control Barrier Function and Safe Feedback Controller Under Input Constraints (Han Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Han Wang, Kostas Margellos, Antonis Papachristodoulou, Claudio De Persis. (2024)<br><strong>Convex Co-Design of Control Barrier Function and Safe Feedback Controller Under Input Constraints</strong><br><button class=copy-to-clipboard title="Convex Co-Design of Control Barrier Function and Safe Feedback Controller Under Input Constraints" index=340>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-340 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-SY, eess-SY, math-OC, math.OC<br>Keyword Score: 10<br>Keywords: Continuous Time, Continuous Time<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11763v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11763v1.pdf filename=2403.11763v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the problem of co-designing control barrier functions (CBF) and linear state feedback controllers for <b>continuous-time</b> <b>linear</b> systems. We achieve this by means of a single semi-definite optimization program. Our formulation can handle mixed-relative degree problems without requiring an explicit safe controller. Different L-norm based input limitations can be introduced as convex constraints in the proposed program. We demonstrate our results on an omni-directional car numerical example.</p></p class="citation"></blockquote><h3 id=22--341350-a-first-order-gradient-approach-for-the-connectivity-analysis-of-weighted-graphs-christian-p-c-franssen-et-al-2024>(2/2 | 341/350) A First-Order Gradient Approach for the Connectivity Analysis of Weighted Graphs (Christian P. C. Franssen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Christian P. C. Franssen, Alessandro Zocca, Bernd F. Heidergott. (2024)<br><strong>A First-Order Gradient Approach for the Connectivity Analysis of Weighted Graphs</strong><br><button class=copy-to-clipboard title="A First-Order Gradient Approach for the Connectivity Analysis of Weighted Graphs" index=341>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-341 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-SY, eess-SY, math-OC, math.OC<br>Keyword Score: 6<br>Keywords: Graph, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11744v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11744v1.pdf filename=2403.11744v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Weighted <b>graphs</b> are commonly used to model various complex systems, including social networks, power grids, transportation networks, and biological systems. In many applications, the connectivity of these networks can be expressed through the Mean First Passage Times (MFPTs) of a Markov chain modeling a random walker on the <b>graph.</b> In this paper, we generalize the network metrics based on Markov chains&rsquo; MFPTs and extend them to networks affected by uncertainty, in which edges may fail and hence not be present according to a pre-determined stochastic model. To find optimally connected <b>graphs,</b> we present a parameterization-free method for optimizing the MFPTs of the underlying Markov chain. More specifically, we show how to extend the Simultaneous Perturbation Stochastic Approximation (SPSA) algorithm in the context of Markov chain optimization. The proposed algorithm is suitable for both fixed and random networks. Using various numerical experiments, we demonstrate scalability compared to established <b>benchmarks.</b> Importantly, our algorithm finds an optimal solution without requiring prior knowledge of edge failure probabilities, allowing for an online optimization approach.</p></p class="citation"></blockquote><h2 id=hep-th-1>hep-th (1)</h2><h3 id=11--342350-neural-network-representation-of-quantum-systems-koji-hashimoto-et-al-2024>(1/1 | 342/350) Neural network representation of quantum systems (Koji Hashimoto et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Koji Hashimoto, Yuji Hirono, Jun Maeda, Jojiro Totsuka-Yoshinaka. (2024)<br><strong>Neural network representation of quantum systems</strong><br><button class=copy-to-clipboard title="Neural network representation of quantum systems" index=342>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-342 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: hep-th<br>Categories: cond-mat-dis-nn, cs-AI, cs-LG, hep-th, hep-th, quant-ph<br>Keyword Score: 10<br>Keywords: Gaussian Process<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11420v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11420v1.pdf filename=2403.11420v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>It has been proposed that random wide neural networks near <b>Gaussian</b> <b>process</b> are quantum field theories around <b>Gaussian</b> <b>fixed</b> points. In this paper, we provide a novel map with which a wide class of quantum mechanical systems can be cast into the form of a neural network with a statistical summation over network parameters. Our simple idea is to use the universal approximation theorem of neural networks to generate arbitrary paths in the Feynman&rsquo;s path integral. The map can be applied to interacting quantum systems / field theories, even away from the <b>Gaussian</b> <b>limit.</b> Our findings bring machine learning closer to the quantum world.</p></p class="citation"></blockquote><h2 id=csdb-1>cs.DB (1)</h2><h3 id=11--343350-benchmarking-analytical-query-processing-in-intel-sgxv2-adrian-lutsch-et-al-2024>(1/1 | 343/350) Benchmarking Analytical Query Processing in Intel SGXv2 (Adrian Lutsch et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Adrian Lutsch, Muhammad El-Hindi, Matthias Heinrich, Daniel Ritter, Zsolt István, Carsten Binnig. (2024)<br><strong>Benchmarking Analytical Query Processing in Intel SGXv2</strong><br><button class=copy-to-clipboard title="Benchmarking Analytical Query Processing in Intel SGXv2" index=343>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-343 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DB<br>Categories: H-2; B-8, cs-DB, cs.DB<br>Keyword Score: 6<br>Keywords: Benchmarking, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11874v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11874v1.pdf filename=2403.11874v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The recently introduced second generation of Intel SGX (SGXv2) lifts memory size limitations of the first generation. Theoretically, this promises to enable secure and highly efficient analytical DBMSs in the cloud. To validate this promise, in this paper, we conduct the first in-depth evaluation study of running analytical query processing algorithms inside SGXv2. Our study reveals that state-of-the-art query operators like radix joins and SIMD-based scans can indeed achieve high performance inside SGXv2 enclaves. These operations are orders of magnitude faster than joins optimized for the discontinued SGXv1 hardware. However, substantial performance overheads are still caused by subtle hardware and software differences influencing code execution inside an SGX enclave. We investigate these differences and propose new optimizations to bring the performance inside the enclave on par with native code execution outside an enclave.</p></p class="citation"></blockquote><h2 id=csmm-1>cs.MM (1)</h2><h3 id=11--344350-virbo-multimodal-multilingual-avatar-video-generation-in-digital-marketing-juan-zhang-et-al-2024>(1/1 | 344/350) Virbo: Multimodal Multilingual Avatar Video Generation in Digital Marketing (Juan Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Juan Zhang, Jiahao Chen, Cheng Wang, Zhiwang Yu, Can Liu, Tangquan Qi, Di Wu. (2024)<br><strong>Virbo: Multimodal Multilingual Avatar Video Generation in Digital Marketing</strong><br><button class=copy-to-clipboard title="Virbo: Multimodal Multilingual Avatar Video Generation in Digital Marketing" index=344>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-344 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MM<br>Categories: cs-MM, cs.MM<br>Keyword Score: 6<br>Keywords: Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11700v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11700v1.pdf filename=2403.11700v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the widespread popularity of internet celebrity marketing all over the world, short video production has gradually become a popular way of presenting products information. However, the traditional video production industry usually includes series of procedures as script writing, video filming in a professional studio, video clipping, special effects rendering, customized post-processing, and so forth. Not to mention that multilingual videos is not accessible for those who could not speak multilingual languages. These complicated procedures usually needs a professional team to complete, and this made short video production costly in both time and money. This paper presents an intelligent system that supports the automatic generation of talking avatar videos, namely Virbo. With simply a user-specified script, Virbo could use a deep generative model to generate a target talking videos. Meanwhile, the system also supports <b>multimodal</b> inputs to customize the video with specified face, specified voice and special effects. This system also integrated a multilingual customization module that supports generate multilingual talking avatar videos in a batch with hundreds of delicate templates and creative special effects. Through a series of user studies and demo tests, we found that Virbo can generate talking avatar videos that maintained a high quality of videos as those from a professional team while reducing the entire production costs significantly. This intelligent system will effectively promote the video production industry and facilitate the internet marketing neglecting of language barriers and cost challenges.</p></p class="citation"></blockquote><h2 id=csce-1>cs.CE (1)</h2><h3 id=11--345350-vehicle-single-track-modeling-using-physics-guided-neural-differential-equations-stephan-rhode-et-al-2024>(1/1 | 345/350) Vehicle single track modeling using physics guided neural differential equations (Stephan Rhode et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Stephan Rhode, Fabian Jarmolowitz, Felix Berkel. (2024)<br><strong>Vehicle single track modeling using physics guided neural differential equations</strong><br><button class=copy-to-clipboard title="Vehicle single track modeling using physics guided neural differential equations" index=345>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-345 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CE<br>Categories: cs-CE, cs.CE<br>Keyword Score: 5<br>Keywords: Black Box<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11648v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11648v1.pdf filename=2403.11648v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we follow the physics guided modeling approach and integrate a neural differential equation network into the physical structure of a vehicle single track model. By relying on the kinematic relations of the single track ordinary differential equations (ODE), a small neural network and few training samples are sufficient to substantially improve the model accuracy compared with a pure physics based vehicle single track model. To be more precise, the sum of squared error is reduced by 68% in the considered scenario. In addition, it is demonstrated that the prediction capabilities of the physics guided neural ODE model are superior compared with a pure <b>black</b> <b>box</b> neural differential equation approach.</p></p class="citation"></blockquote><h2 id=csni-1>cs.NI (1)</h2><h3 id=11--346350-edge-disjoint-spanning-trees-on-star-product-networks-aleyah-dawkins-et-al-2024>(1/1 | 346/350) Edge-Disjoint Spanning Trees on Star-Product Networks (Aleyah Dawkins et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aleyah Dawkins, Kelly Isham, Ales Kubicek, Kartik Lakhotia, Laura Monroe. (2024)<br><strong>Edge-Disjoint Spanning Trees on Star-Product Networks</strong><br><button class=copy-to-clipboard title="Edge-Disjoint Spanning Trees on Star-Product Networks" index=346>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-346 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-DC, cs-NI, cs.NI, math-CO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12231v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12231v1.pdf filename=2403.12231v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Star-product <b>graphs</b> are a natural extension of the Cartesian product, but have not been well-studied. We show that many important established and emerging network topologies, including HyperX, SlimFly, BundleFly, PolarStar, mesh, and torus, are in fact star-product <b>graphs.</b> While this connection was known for BundleFly and PolarStar, it was not for the others listed. We extend a method of constructing maximal and near-maximal sets of edge-disjoint spanning trees on Cartesian products to the star product, thus obtain maximal or near-maximal sets of edge-disjoint spanning trees on new networks of importance, where such sets can improve bandwidth of collective operations and therefore accelerate many important workloads in high-performance computing.</p></p class="citation"></blockquote><h2 id=mathat-1>math.AT (1)</h2><h3 id=11--347350-probabilistic-analysis-of-multiparameter-persistence-decompositions-ángel-javier-alonso-et-al-2024>(1/1 | 347/350) Probabilistic Analysis of Multiparameter Persistence Decompositions (Ángel Javier Alonso et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ángel Javier Alonso, Michael Kerber, Primoz Skraba. (2024)<br><strong>Probabilistic Analysis of Multiparameter Persistence Decompositions</strong><br><button class=copy-to-clipboard title="Probabilistic Analysis of Multiparameter Persistence Decompositions" index=347>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-347 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.AT<br>Categories: cs-CG, math-AT, math.AT<br>Keyword Score: 3<br>Keywords: Sample Size<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11939v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11939v1.pdf filename=2403.11939v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multiparameter persistence modules can be uniquely decomposed into indecomposable summands. Among these indecomposables, intervals stand out for their simplicity, making them preferable for their ease of interpretation in practical applications and their computational efficiency. Empirical observations indicate that modules that decompose into only intervals are rare. To support this observation, we show that for numerous common multiparameter constructions, such as density- or degree-Rips bifiltrations, and across a general category of point <b>samples,</b> <b>the</b> probability of the homology-induced persistence module decomposing into intervals goes to zero as the <b>sample</b> <b>size</b> goes to infinity.</p></p class="citation"></blockquote><h2 id=cspl-1>cs.PL (1)</h2><h3 id=11--348350-looper-a-learned-automatic-code-optimizer-for-polyhedral-compilers-massinissa-merouani-et-al-2024>(1/1 | 348/350) LOOPer: A Learned Automatic Code Optimizer For Polyhedral Compilers (Massinissa Merouani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Massinissa Merouani, Khaled Afif Boudaoud, Iheb Nassim Aouadj, Nassim Tchoulak, Islam Kara Bernou, Hamza Benyamina, Fatima Benbouzid-Si Tayeb, Karima Benatchba, Hugh Leather, Riyadh Baghdadi. (2024)<br><strong>LOOPer: A Learned Automatic Code Optimizer For Polyhedral Compilers</strong><br><button class=copy-to-clipboard title="LOOPer: A Learned Automatic Code Optimizer For Polyhedral Compilers" index=348>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-348 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.PL<br>Categories: cs-DC, cs-LG, cs-PL, cs.PL<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11522v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11522v1.pdf filename=2403.11522v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While polyhedral compilers have shown success in implementing advanced code transformations, they still have challenges in selecting the most profitable transformations that lead to the best speedups. This has motivated the use of machine learning to build cost models to guide the search for polyhedral optimizations. State-of-the-art polyhedral compilers have demonstrated a viable proof-of-concept of this approach. While such a proof-of-concept has shown promise, it still has significant limitations. State-of-the-art polyhedral compilers that use a deep-learning cost model only support a small subset of affine transformations, limiting their ability to apply complex code transformations. They also only support simple programs that have a single loop nest and a rectangular iteration domain, limiting their applicability to many programs. These limitations significantly impact the generality of such compilers and autoschedulers and put into question the whole approach. In this paper, we introduce LOOPer, the first polyhedral autoscheduler that uses a deep-learning based cost model and covers a large set of affine transformations and programs. It supports the exploration of a large set of affine transformations, allowing the application of complex sequences of polyhedral transformations. It also supports the optimization of programs with multiple loop nests and with rectangular and non-rectangular iteration domains, allowing the optimization of an extensive set of programs. We implement and evaluate LOOPer and show that it achieves speedups over the state-of-the-art. On the Polybench <b>benchmark,</b> LOOPer achieves a geometric mean speedup of 1.59x over Tiramisu. LOOPer also achieves competitive speedups with a geometric mean speedup of 1.34x over Pluto, a state-of-the-art polyhedral compiler that does not use a machine-learning based cost model.</p></p class="citation"></blockquote><h2 id=q-bionc-1>q-bio.NC (1)</h2><h3 id=11--349350-towards-understanding-the-nature-of-direct-functional-connectivity-in-visual-brain-network-debanjali-bhattacharya-et-al-2024>(1/1 | 349/350) Towards understanding the nature of direct functional connectivity in visual brain network (Debanjali Bhattacharya et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Debanjali Bhattacharya, Neelam Sinha. (2024)<br><strong>Towards understanding the nature of direct functional connectivity in visual brain network</strong><br><button class=copy-to-clipboard title="Towards understanding the nature of direct functional connectivity in visual brain network" index=349>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-349 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.NC<br>Categories: cs-CV, q-bio-NC, q-bio.NC<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11480v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11480v1.pdf filename=2403.11480v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advances in neuroimaging have enabled studies in functional connectivity (FC) of human brain, alongside investigation of the neuronal basis of cognition. One important FC study is the representation of vision in human brain. The release of publicly available dataset BOLD5000 has made it possible to study the brain dynamics during visual tasks in greater detail. In this paper, a comprehensive analysis of fMRI time series (TS) has been performed to explore different types of visual brain networks (VBN). The novelty of this work lies in (1) constructing VBN with consistently significant direct connectivity using both marginal and partial correlation, which is further analyzed using <b>graph</b> theoretic measures, (2) classification of VBNs as formed by image complexity-specific TS, using graphical features. In image complexity-specific VBN classification, XGBoost yields average accuracy in the range of 86.5% to 91.5% for positively correlated VBN, which is 2% greater than that using negative correlation. This result not only reflects the distinguishing graphical characteristics of each image complexity-specific VBN, but also highlights the importance of studying both positively correlated and negatively correlated VBN to understand the how differently brain functions while viewing different complexities of real-world images.</p></p class="citation"></blockquote><h2 id=csds-1>cs.DS (1)</h2><h3 id=11--350350-eth-tight-algorithm-for-cycle-packing-on-unit-disk-graphs-shinwoo-an-et-al-2024>(1/1 | 350/350) ETH-Tight Algorithm for Cycle Packing on Unit Disk Graphs (Shinwoo An et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shinwoo An, Eunjin Oh. (2024)<br><strong>ETH-Tight Algorithm for Cycle Packing on Unit Disk Graphs</strong><br><button class=copy-to-clipboard title="ETH-Tight Algorithm for Cycle Packing on Unit Disk Graphs" index=350>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-350 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-CG, cs-DS, cs.DS<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.11426v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.11426v1.pdf filename=2403.11426v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we consider the Cycle Packing problem on unit disk <b>graphs</b> defined as follows. Given a unit disk <b>graph</b> G with n vertices and an integer k, the goal is to find a set of $k$ vertex-disjoint cycles of G if it exists. Our algorithm runs in time $2^{O(\sqrt k)}n^{O(1)}$. This improves the $2^{O(\sqrt k\log k)}n^{O(1)}$-time algorithm by Fomin et al. [SODA 2012, ICALP 2017]. Moreover, our algorithm is optimal assuming the exponential-time hypothesis.</p></p class="citation"></blockquote></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/akitenkrad-blog/posts/arxiv/202403/20240319000000/ title="arXiv @ 2024.03.19" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Prev</div><div class=next-prev-text>arXiv @ 2024.03.19</div></a></div><div class="col-md-6 next-article"><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/ title="arXiv @ 2024.03.21" class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>arXiv @ 2024.03.21</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#primary-categories>Primary Categories</a></li><li><a href=#keywords>Keywords</a></li><li><a href=#cscl-43>cs.CL (43)</a><ul><li><a href=#143--1350-embedded-named-entity-recognition-using-probing-classifiers-nicholas-popovič-et-al-2024>(1/43 | 1/350) Embedded Named Entity Recognition using Probing Classifiers (Nicholas Popovič et al., 2024)</a></li><li><a href=#243--2350-cicle-conformal-in-context-learning-for-largescale-multi-class-food-risk-classification-korbinian-randl-et-al-2024>(2/43 | 2/350) CICLe: Conformal In-Context Learning for Largescale Multi-Class Food Risk Classification (Korbinian Randl et al., 2024)</a></li><li><a href=#343--3350-ensuring-safe-and-high-quality-outputs-a-guideline-library-approach-for-language-models-yi-luo-et-al-2024>(3/43 | 3/350) Ensuring Safe and High-Quality Outputs: A Guideline Library Approach for Language Models (Yi Luo et al., 2024)</a></li><li><a href=#443--4350-finllama-financial-sentiment-classification-for-algorithmic-trading-applications-thanos-konstantinidis-et-al-2024>(4/43 | 4/350) FinLlama: Financial Sentiment Classification for Algorithmic Trading Applications (Thanos Konstantinidis et al., 2024)</a></li><li><a href=#543--5350-evaluating-named-entity-recognition-comparative-analysis-of-mono--and-multilingual-transformer-models-on-brazilian-corporate-earnings-call-transcriptions-ramon-abilio-et-al-2024>(5/43 | 5/350) Evaluating Named Entity Recognition: Comparative Analysis of Mono- and Multilingual Transformer Models on Brazilian Corporate Earnings Call Transcriptions (Ramon Abilio et al., 2024)</a></li><li><a href=#643--6350-tnt-llm-text-mining-at-scale-with-large-language-models-mengting-wan-et-al-2024>(6/43 | 6/350) TnT-LLM: Text Mining at Scale with Large Language Models (Mengting Wan et al., 2024)</a></li><li><a href=#743--7350-a-novel-paradigm-boosting-translation-capabilities-of-large-language-models-jiaxin-guo-et-al-2024>(7/43 | 7/350) A Novel Paradigm Boosting Translation Capabilities of Large Language Models (Jiaxin Guo et al., 2024)</a></li><li><a href=#843--8350-construction-of-hyper-relational-knowledge-graphs-using-pre-trained-large-language-models-preetha-datta-et-al-2024>(8/43 | 8/350) Construction of Hyper-Relational Knowledge Graphs Using Pre-Trained Large Language Models (Preetha Datta et al., 2024)</a></li><li><a href=#943--9350-hatecot-an-explanation-enhanced-dataset-for-generalizable-offensive-speech-detection-via-large-language-models-huy-nghiem-et-al-2024>(9/43 | 9/350) HateCOT: An Explanation-Enhanced Dataset for Generalizable Offensive Speech Detection via Large Language Models (Huy Nghiem et al., 2024)</a></li><li><a href=#1043--10350-easyjailbreak-a-unified-framework-for-jailbreaking-large-language-models-weikang-zhou-et-al-2024>(10/43 | 10/350) EasyJailbreak: A Unified Framework for Jailbreaking Large Language Models (Weikang Zhou et al., 2024)</a></li><li><a href=#1143--11350-enhancing-hokkien-dual-translation-by-exploring-and-standardizing-of-four-writing-systems-bo-han-lu-et-al-2024>(11/43 | 11/350) Enhancing Hokkien Dual Translation by Exploring and Standardizing of Four Writing Systems (Bo-Han Lu et al., 2024)</a></li><li><a href=#1243--12350-envgen-generating-and-adapting-environments-via-llms-for-training-embodied-agents-abhay-zala-et-al-2024>(12/43 | 12/350) EnvGen: Generating and Adapting Environments via LLMs for Training Embodied Agents (Abhay Zala et al., 2024)</a></li><li><a href=#1343--13350-gpt-4-as-evaluator-evaluating-large-language-models-on-pest-management-in-agriculture-shanglong-yang-et-al-2024>(13/43 | 13/350) GPT-4 as Evaluator: Evaluating Large Language Models on Pest Management in Agriculture (Shanglong Yang et al., 2024)</a></li><li><a href=#1443--14350-metaphor-understanding-challenge-dataset-for-llms-xiaoyu-tong-et-al-2024>(14/43 | 14/350) Metaphor Understanding Challenge Dataset for LLMs (Xiaoyu Tong et al., 2024)</a></li><li><a href=#1543--15350-counting-stars-a-simple-efficient-and-reasonable-strategy-for-evaluating-long-context-large-language-models-mingyang-song-et-al-2024>(15/43 | 15/350) Counting-Stars: A Simple, Efficient, and Reasonable Strategy for Evaluating Long-Context Large Language Models (Mingyang Song et al., 2024)</a></li><li><a href=#1643--16350-co3-low-resource-contrastive-co-training-for-generative-conversational-query-rewrite-yifei-yuan-et-al-2024>(16/43 | 16/350) CO3: Low-resource Contrastive Co-training for Generative Conversational Query Rewrite (Yifei Yuan et al., 2024)</a></li><li><a href=#1743--17350-towards-understanding-the-relationship-between-in-context-learning-and-compositional-generalization-sungjun-han-et-al-2024>(17/43 | 17/350) Towards Understanding the Relationship between In-context Learning and Compositional Generalization (Sungjun Han et al., 2024)</a></li><li><a href=#1843--18350-lets-focus-on-neuron-neuron-level-supervised-fine-tuning-for-large-language-model-haoyun-xu-et-al-2024>(18/43 | 18/350) Let&rsquo;s Focus on Neuron: Neuron-Level Supervised Fine-tuning for Large Language Model (Haoyun Xu et al., 2024)</a></li><li><a href=#1943--19350-openeval-benchmarking-chinese-llms-across-capability-alignment-and-safety-chuang-liu-et-al-2024>(19/43 | 19/350) OpenEval: Benchmarking Chinese LLMs across Capability, Alignment and Safety (Chuang Liu et al., 2024)</a></li><li><a href=#2043--20350-leveraging-large-language-models-to-extract-information-on-substance-use-disorder-severity-from-clinical-notes-a-zero-shot-learning-approach-maria-mahbub-et-al-2024>(20/43 | 20/350) Leveraging Large Language Models to Extract Information on Substance Use Disorder Severity from Clinical Notes: A Zero-shot Learning Approach (Maria Mahbub et al., 2024)</a></li><li><a href=#2143--21350-queryagent-a-reliable-and-efficient-reasoning-framework-with-environmental-feedback-based-self-correction-xiang-huang-et-al-2024>(21/43 | 21/350) QueryAgent: A Reliable and Efficient Reasoning Framework with Environmental Feedback based Self-Correction (Xiang Huang et al., 2024)</a></li><li><a href=#2243--22350-reinforcement-learning-with-token-level-feedback-for-controllable-text-generation-wendi-li-et-al-2024>(22/43 | 22/350) Reinforcement Learning with Token-level Feedback for Controllable Text Generation (Wendi Li et al., 2024)</a></li><li><a href=#2343--23350-dee-dual-stage-explainable-evaluation-method-for-text-generation-shenyu-zhang-et-al-2024>(23/43 | 23/350) DEE: Dual-stage Explainable Evaluation Method for Text Generation (Shenyu Zhang et al., 2024)</a></li><li><a href=#2443--24350-inscl-a-data-efficient-continual-learning-paradigm-for-fine-tuning-large-language-models-with-instructions-yifan-wang-et-al-2024>(24/43 | 24/350) InsCL: A Data-efficient Continual Learning Paradigm for Fine-tuning Large Language Models with Instructions (Yifan Wang et al., 2024)</a></li><li><a href=#2543--25350-dynamic-contexts-for-generating-suggestion-questions-in-rag-based-conversational-systems-anuja-tayal-et-al-2024>(25/43 | 25/350) Dynamic Contexts for Generating Suggestion Questions in RAG Based Conversational Systems (Anuja Tayal et al., 2024)</a></li><li><a href=#2643--26350-x-llava-optimizing-bilingual-large-vision-language-alignment-dongjae-shin-et-al-2024>(26/43 | 26/350) X-LLaVA: Optimizing Bilingual Large Vision-Language Alignment (Dongjae Shin et al., 2024)</a></li><li><a href=#2743--27350-novelqa-a-benchmark-for-long-range-novel-question-answering-cunxiang-wang-et-al-2024>(27/43 | 27/350) NovelQA: A Benchmark for Long-Range Novel Question Answering (Cunxiang Wang et al., 2024)</a></li><li><a href=#2843--28350-stylechat-learning-recitation-augmented-memory-in-llms-for-stylized-dialogue-generation-jinpeng-li-et-al-2024>(28/43 | 28/350) StyleChat: Learning Recitation-Augmented Memory in LLMs for Stylized Dialogue Generation (Jinpeng Li et al., 2024)</a></li><li><a href=#2943--29350-zero-shot-multi-task-hallucination-detection-patanjali-bhamidipati-et-al-2024>(29/43 | 29/350) Zero-Shot Multi-task Hallucination Detection (Patanjali Bhamidipati et al., 2024)</a></li><li><a href=#3043--30350-using-generative-text-models-to-create-qualitative-codebooks-for-student-evaluations-of-teaching-andrew-katz-et-al-2024>(30/43 | 30/350) Using Generative Text Models to Create Qualitative Codebooks for Student Evaluations of Teaching (Andrew Katz et al., 2024)</a></li><li><a href=#3143--31350-language-evolution-with-deep-learning-mathieu-rita-et-al-2024>(31/43 | 31/350) Language Evolution with Deep Learning (Mathieu Rita et al., 2024)</a></li><li><a href=#3243--32350-reference-based-metrics-disprove-themselves-in-question-generation-bang-nguyen-et-al-2024>(32/43 | 32/350) Reference-based Metrics Disprove Themselves in Question Generation (Bang Nguyen et al., 2024)</a></li><li><a href=#3343--33350-reasoning-abilities-of-large-language-models-in-depth-analysis-on-the-abstraction-and-reasoning-corpus-seungpil-lee-et-al-2024>(33/43 | 33/350) Reasoning Abilities of Large Language Models: In-Depth Analysis on the Abstraction and Reasoning Corpus (Seungpil Lee et al., 2024)</a></li><li><a href=#3443--34350-syn-qa2-evaluating-false-assumptions-in-long-tail-questions-with-synthetic-qa-datasets-ashwin-daswani-et-al-2024>(34/43 | 34/350) Syn-QA2: Evaluating False Assumptions in Long-tail Questions with Synthetic QA Datasets (Ashwin Daswani et al., 2024)</a></li><li><a href=#3543--35350-from-pixels-to-insights-a-survey-on-automatic-chart-understanding-in-the-era-of-large-foundation-models-kung-hsiang-huang-et-al-2024>(35/43 | 35/350) From Pixels to Insights: A Survey on Automatic Chart Understanding in the Era of Large Foundation Models (Kung-Hsiang Huang et al., 2024)</a></li><li><a href=#3643--36350-investigating-markers-and-drivers-of-gender-bias-in-machine-translations-peter-j-barclay-et-al-2024>(36/43 | 36/350) Investigating Markers and Drivers of Gender Bias in Machine Translations (Peter J Barclay et al., 2024)</a></li><li><a href=#3743--37350-revisiting-the-classics-a-study-on-identifying-and-rectifying-gender-stereotypes-in-rhymes-and-poems-aditya-narayan-sankaran-et-al-2024>(37/43 | 37/350) Revisiting The Classics: A Study on Identifying and Rectifying Gender Stereotypes in Rhymes and Poems (Aditya Narayan Sankaran et al., 2024)</a></li><li><a href=#3843--38350-from-explainable-to-interpretable-deep-learning-for-natural-language-processing-in-healthcare-how-far-from-reality-guangming-huang-et-al-2024>(38/43 | 38/350) From explainable to interpretable deep learning for natural language processing in healthcare: how far from reality? (Guangming Huang et al., 2024)</a></li><li><a href=#3943--39350-sscae----semantic-syntactic-and-context-aware-natural-language-adversarial-examples-generator-javad-rafiei-asl-et-al-2024>(39/43 | 39/350) SSCAE &ndash; Semantic, Syntactic, and Context-aware natural language Adversarial Examples generator (Javad Rafiei Asl et al., 2024)</a></li><li><a href=#4043--40350-a-comparative-investigation-of-compositional-syntax-and-semantics-in-dall-e-2-elliot-murphy-et-al-2024>(40/43 | 40/350) A Comparative Investigation of Compositional Syntax and Semantics in DALL-E 2 (Elliot Murphy et al., 2024)</a></li><li><a href=#4143--41350-adaptative-bilingual-aligning-using-multilingual-sentence-embedding-olivier-kraif-2024>(41/43 | 41/350) Adaptative Bilingual Aligning Using Multilingual Sentence Embedding (Olivier Kraif, 2024)</a></li><li><a href=#4243--42350-a-closer-look-at-claim-decomposition-miriam-wanner-et-al-2024>(42/43 | 42/350) A Closer Look at Claim Decomposition (Miriam Wanner et al., 2024)</a></li><li><a href=#4343--43350-word-orders-impacts-insights-from-reordering-and-generation-analysis-qinghua-zhao-et-al-2024>(43/43 | 43/350) Word Order&rsquo;s Impacts: Insights from Reordering and Generation Analysis (Qinghua Zhao et al., 2024)</a></li></ul></li><li><a href=#cscv-122>cs.CV (122)</a><ul><li><a href=#1122--44350-leveraging-spatial-and-semantic-feature-extraction-for-skin-cancer-diagnosis-with-capsule-networks-and-graph-neural-networks-k-p-santoso-et-al-2024>(1/122 | 44/350) Leveraging Spatial and Semantic Feature Extraction for Skin Cancer Diagnosis with Capsule Networks and Graph Neural Networks (K. P. Santoso et al., 2024)</a></li><li><a href=#2122--45350-seta-semantic-aware-token-augmentation-for-domain-generalization-jintao-guo-et-al-2024>(2/122 | 45/350) SETA: Semantic-Aware Token Augmentation for Domain Generalization (Jintao Guo et al., 2024)</a></li><li><a href=#3122--46350-ttt-kd-test-time-training-for-3d-semantic-segmentation-through-knowledge-distillation-from-foundation-models-lisa-weijler-et-al-2024>(3/122 | 46/350) TTT-KD: Test-Time Training for 3D Semantic Segmentation through Knowledge Distillation from Foundation Models (Lisa Weijler et al., 2024)</a></li><li><a href=#4122--47350-flexcap-generating-rich-localized-and-flexible-captions-in-images-debidatta-dwibedi-et-al-2024>(4/122 | 47/350) FlexCap: Generating Rich, Localized, and Flexible Captions in Images (Debidatta Dwibedi et al., 2024)</a></li><li><a href=#5122--48350-arc2face-a-foundation-model-of-human-faces-foivos-paraperas-papantoniou-et-al-2024>(5/122 | 48/350) Arc2Face: A Foundation Model of Human Faces (Foivos Paraperas Papantoniou et al., 2024)</a></li><li><a href=#6122--49350-vmambair-visual-state-space-model-for-image-restoration-yuan-shi-et-al-2024>(6/122 | 49/350) VmambaIR: Visual State Space Model for Image Restoration (Yuan Shi et al., 2024)</a></li><li><a href=#7122--50350-videoagent-a-memory-augmented-multimodal-agent-for-video-understanding-yue-fan-et-al-2024>(7/122 | 50/350) VideoAgent: A Memory-augmented Multimodal Agent for Video Understanding (Yue Fan et al., 2024)</a></li><li><a href=#8122--51350-geowizard-unleashing-the-diffusion-priors-for-3d-geometry-estimation-from-a-single-image-xiao-fu-et-al-2024>(8/122 | 51/350) GeoWizard: Unleashing the Diffusion Priors for 3D Geometry Estimation from a Single Image (Xiao Fu et al., 2024)</a></li><li><a href=#9122--52350-meta-prompting-for-automating-zero-shot-visual-recognition-with-llms-m-jehanzeb-mirza-et-al-2024>(9/122 | 52/350) Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs (M. Jehanzeb Mirza et al., 2024)</a></li><li><a href=#10122--53350-one-step-image-translation-with-text-to-image-models-gaurav-parmar-et-al-2024>(10/122 | 53/350) One-Step Image Translation with Text-to-Image Models (Gaurav Parmar et al., 2024)</a></li><li><a href=#11122--54350-idf-cr-iterative-diffusion-process-for-divide-and-conquer-cloud-removal-in-remote-sensing-images-meilin-wang-et-al-2024>(11/122 | 54/350) IDF-CR: Iterative Diffusion Process for Divide-and-Conquer Cloud Removal in Remote-sensing Images (Meilin Wang et al., 2024)</a></li><li><a href=#12122--55350-agent3d-zero-an-agent-for-zero-shot-3d-understanding-sha-zhang-et-al-2024>(12/122 | 55/350) Agent3D-Zero: An Agent for Zero-shot 3D Understanding (Sha Zhang et al., 2024)</a></li><li><a href=#13122--56350-boosting-continuous-emotion-recognition-with-self-pretraining-using-masked-autoencoders-temporal-convolutional-networks-and-transformers-weiwei-zhou-et-al-2024>(13/122 | 56/350) Boosting Continuous Emotion Recognition with Self-Pretraining using Masked Autoencoders, Temporal Convolutional Networks, and Transformers (Weiwei Zhou et al., 2024)</a></li><li><a href=#14122--57350-ocr-is-all-you-need-importing-multi-modality-into-image-based-defect-detection-system-chih-chung-hsu-et-al-2024>(14/122 | 57/350) OCR is All you need: Importing Multi-Modality into Image-based Defect Detection System (Chih-Chung Hsu et al., 2024)</a></li><li><a href=#15122--58350-hiri-vit-scaling-vision-transformer-with-high-resolution-inputs-ting-yao-et-al-2024>(15/122 | 58/350) HIRI-ViT: Scaling Vision Transformer with High Resolution Inputs (Ting Yao et al., 2024)</a></li><li><a href=#16122--59350-better-pseudo-labels-for-semi-supervised-instance-segmentation-françois-porcher-et-al-2024>(16/122 | 59/350) Better (pseudo-)labels for semi-supervised instance segmentation (François Porcher et al., 2024)</a></li><li><a href=#17122--60350-binary-noise-for-binary-tasks-masked-bernoulli-diffusion-for-unsupervised-anomaly-detection-julia-wolleb-et-al-2024>(17/122 | 60/350) Binary Noise for Binary Tasks: Masked Bernoulli Diffusion for Unsupervised Anomaly Detection (Julia Wolleb et al., 2024)</a></li><li><a href=#18122--61350-ssap-a-shape-sensitive-adversarial-patch-for-comprehensive-disruption-of-monocular-depth-estimation-in-autonomous-navigation-applications-amira-guesmi-et-al-2024>(18/122 | 61/350) SSAP: A Shape-Sensitive Adversarial Patch for Comprehensive Disruption of Monocular Depth Estimation in Autonomous Navigation Applications (Amira Guesmi et al., 2024)</a></li><li><a href=#19122--62350-siamese-learning-with-joint-alignment-and-regression-for-weakly-supervised-video-paragraph-grounding-chaolei-tan-et-al-2024>(19/122 | 62/350) Siamese Learning with Joint Alignment and Regression for Weakly-Supervised Video Paragraph Grounding (Chaolei Tan et al., 2024)</a></li><li><a href=#20122--63350-minedreamer-learning-to-follow-instructions-via-chain-of-imagination-for-simulated-world-control-enshen-zhou-et-al-2024>(20/122 | 63/350) MineDreamer: Learning to Follow Instructions via Chain-of-Imagination for Simulated-World Control (Enshen Zhou et al., 2024)</a></li><li><a href=#21122--64350-hvdistill-transferring-knowledge-from-images-to-point-clouds-via-unsupervised-hybrid-view-distillation-sha-zhang-et-al-2024>(21/122 | 64/350) HVDistill: Transferring Knowledge from Images to Point Clouds via Unsupervised Hybrid-View Distillation (Sha Zhang et al., 2024)</a></li><li><a href=#22122--65350-evaluating-text-to-image-synthesis-survey-and-taxonomy-of-image-quality-metrics-sebastian-hartwig-et-al-2024>(22/122 | 65/350) Evaluating Text to Image Synthesis: Survey and Taxonomy of Image Quality Metrics (Sebastian Hartwig et al., 2024)</a></li><li><a href=#23122--66350-dynamic-tuning-towards-parameter-and-inference-efficiency-for-vit-adaptation-wangbo-zhao-et-al-2024>(23/122 | 66/350) Dynamic Tuning Towards Parameter and Inference Efficiency for ViT Adaptation (Wangbo Zhao et al., 2024)</a></li><li><a href=#24122--67350-ln3diff-scalable-latent-neural-fields-diffusion-for-speedy-3d-generation-yushi-lan-et-al-2024>(24/122 | 67/350) LN3Diff: Scalable Latent Neural Fields Diffusion for Speedy 3D Generation (Yushi Lan et al., 2024)</a></li><li><a href=#25122--68350-videomv-consistent-multi-view-generation-based-on-large-video-generative-model-qi-zuo-et-al-2024>(25/122 | 68/350) VideoMV: Consistent Multi-View Generation Based on Large Video Generative Model (Qi Zuo et al., 2024)</a></li><li><a href=#26122--69350-genview-enhancing-view-quality-with-pretrained-generative-model-for-self-supervised-learning-xiaojie-li-et-al-2024>(26/122 | 69/350) GenView: Enhancing View Quality with Pretrained Generative Model for Self-Supervised Learning (Xiaojie Li et al., 2024)</a></li><li><a href=#27122--70350-superlora-parameter-efficient-unified-adaptation-of-multi-layer-attention-modules-xiangyu-chen-et-al-2024>(27/122 | 70/350) SuperLoRA: Parameter-Efficient Unified Adaptation of Multi-Layer Attention Modules (Xiangyu Chen et al., 2024)</a></li><li><a href=#28122--71350-boosting-continual-learning-of-vision-language-models-via-mixture-of-experts-adapters-jiazuo-yu-et-al-2024>(28/122 | 71/350) Boosting Continual Learning of Vision-Language Models via Mixture-of-Experts Adapters (Jiazuo Yu et al., 2024)</a></li><li><a href=#29122--72350-continual-forgetting-for-pre-trained-vision-models-hongbo-zhao-et-al-2024>(29/122 | 72/350) Continual Forgetting for Pre-trained Vision Models (Hongbo Zhao et al., 2024)</a></li><li><a href=#30122--73350-scene-llm-extending-language-model-for-3d-visual-understanding-and-reasoning-rao-fu-et-al-2024>(30/122 | 73/350) Scene-LLM: Extending Language Model for 3D Visual Understanding and Reasoning (Rao Fu et al., 2024)</a></li><li><a href=#31122--74350-urban-scene-diffusion-through-semantic-occupancy-map-junge-zhang-et-al-2024>(31/122 | 74/350) Urban Scene Diffusion through Semantic Occupancy Map (Junge Zhang et al., 2024)</a></li><li><a href=#32122--75350-decotr-enhancing-depth-completion-with-2d-and-3d-attentions-yunxiao-shi-et-al-2024>(32/122 | 75/350) DeCoTR: Enhancing Depth Completion with 2D and 3D Attentions (Yunxiao Shi et al., 2024)</a></li><li><a href=#33122--76350-end-to-end-multi-modal-product-matching-in-fashion-e-commerce-sándor-tóth-et-al-2024>(33/122 | 76/350) End-to-end multi-modal product matching in fashion e-commerce (Sándor Tóth et al., 2024)</a></li><li><a href=#34122--77350-do-clips-always-generalize-better-than-imagenet-models-qizhou-wang-et-al-2024>(34/122 | 77/350) Do CLIPs Always Generalize Better than ImageNet Models? (Qizhou Wang et al., 2024)</a></li><li><a href=#35122--78350-collage-prompting-budget-friendly-visual-recognition-with-gpt-4v-siyu-xu-et-al-2024>(35/122 | 78/350) Collage Prompting: Budget-Friendly Visual Recognition with GPT-4V (Siyu Xu et al., 2024)</a></li><li><a href=#36122--79350-effiperception-an-efficient-framework-for-various-perception-tasks-xinhao-xiang-et-al-2024>(36/122 | 79/350) EffiPerception: an Efficient Framework for Various Perception Tasks (Xinhao Xiang et al., 2024)</a></li><li><a href=#37122--80350-e2f-net-eyes-to-face-inpainting-via-stylegan-latent-space-ahmad-hassanpour-et-al-2024>(37/122 | 80/350) E2F-Net: Eyes-to-Face Inpainting via StyleGAN Latent Space (Ahmad Hassanpour et al., 2024)</a></li><li><a href=#38122--81350-generic-3d-diffusion-adapter-using-controlled-multi-view-editing-hansheng-chen-et-al-2024>(38/122 | 81/350) Generic 3D Diffusion Adapter Using Controlled Multi-View Editing (Hansheng Chen et al., 2024)</a></li><li><a href=#39122--82350-fast-high-resolution-image-synthesis-with-latent-adversarial-diffusion-distillation-axel-sauer-et-al-2024>(39/122 | 82/350) Fast High-Resolution Image Synthesis with Latent Adversarial Diffusion Distillation (Axel Sauer et al., 2024)</a></li><li><a href=#40122--83350-layerdiff-exploring-text-guided-multi-layered-composable-image-synthesis-via-layer-collaborative-diffusion-model-runhui-huang-et-al-2024>(40/122 | 83/350) LayerDiff: Exploring Text-guided Multi-layered Composable Image Synthesis via Layer-Collaborative Diffusion Model (Runhui Huang et al., 2024)</a></li><li><a href=#41122--84350-deep-medial-voxels-learned-medial-axis-approximations-for-anatomical-shape-modeling-antonio-pepe-et-al-2024>(41/122 | 84/350) Deep Medial Voxels: Learned Medial Axis Approximations for Anatomical Shape Modeling (Antonio Pepe et al., 2024)</a></li><li><a href=#42122--85350-infinite-id-identity-preserved-personalization-via-id-semantics-decoupling-paradigm-yi-wu-et-al-2024>(42/122 | 85/350) Infinite-ID: Identity-preserved Personalization via ID-semantics Decoupling Paradigm (Yi Wu et al., 2024)</a></li><li><a href=#43122--86350-effivedefficient-video-editing-via-text-instruction-diffusion-models-zhenghao-zhang-et-al-2024>(43/122 | 86/350) EffiVED:Efficient Video Editing via Text-instruction Diffusion Models (Zhenghao Zhang et al., 2024)</a></li><li><a href=#44122--87350-echoreel-enhancing-action-generation-of-existing-video-diffusion-models-jianzhi-liu-et-al-2024>(44/122 | 87/350) EchoReel: Enhancing Action Generation of Existing Video Diffusion Models (Jianzhi liu et al., 2024)</a></li><li><a href=#45122--88350-cassr-activating-image-power-for-real-world-image-super-resolution-haolan-chen-et-al-2024>(45/122 | 88/350) CasSR: Activating Image Power for Real-World Image Super-Resolution (Haolan Chen et al., 2024)</a></li><li><a href=#46122--89350-robust-overfitting-does-matter-test-time-adversarial-purification-with-fgsm-linyu-tang-et-al-2024>(46/122 | 89/350) Robust Overfitting Does Matter: Test-Time Adversarial Purification With FGSM (Linyu Tang et al., 2024)</a></li><li><a href=#47122--90350-llava-uhd-an-lmm-perceiving-any-aspect-ratio-and-high-resolution-images-ruyi-xu-et-al-2024>(47/122 | 90/350) LLaVA-UHD: an LMM Perceiving Any Aspect Ratio and High-Resolution Images (Ruyi Xu et al., 2024)</a></li><li><a href=#48122--91350-align-and-distill-unifying-and-improving-domain-adaptive-object-detection-justin-kay-et-al-2024>(48/122 | 91/350) Align and Distill: Unifying and Improving Domain Adaptive Object Detection (Justin Kay et al., 2024)</a></li><li><a href=#49122--92350-openocc-open-vocabulary-3d-scene-reconstruction-via-occupancy-representation-haochen-jiang-et-al-2024>(49/122 | 92/350) OpenOcc: Open Vocabulary 3D Scene Reconstruction via Occupancy Representation (Haochen Jiang et al., 2024)</a></li><li><a href=#50122--93350-localstylefool-regional-video-style-transfer-attack-using-segment-anything-model-yuxin-cao-et-al-2024>(50/122 | 93/350) LocalStyleFool: Regional Video Style Transfer Attack Using Segment Anything Model (Yuxin Cao et al., 2024)</a></li><li><a href=#51122--94350-diffusion-models-are-geometry-critics-single-image-3d-editing-using-pre-trained-diffusion-priors-ruicheng-wang-et-al-2024>(51/122 | 94/350) Diffusion Models are Geometry Critics: Single Image 3D Editing Using Pre-Trained Diffusion Priors (Ruicheng Wang et al., 2024)</a></li><li><a href=#52122--95350-graph-jigsaw-conditioned-diffusion-model-for-skeleton-based-video-anomaly-detection-ali-karami-et-al-2024>(52/122 | 95/350) Graph-Jigsaw Conditioned Diffusion Model for Skeleton-based Video Anomaly Detection (Ali Karami et al., 2024)</a></li><li><a href=#53122--96350-exploring-pre-trained-text-to-video-diffusion-models-for-referring-video-object-segmentation-zixin-zhu-et-al-2024>(53/122 | 96/350) Exploring Pre-trained Text-to-Video Diffusion Models for Referring Video Object Segmentation (Zixin Zhu et al., 2024)</a></li><li><a href=#54122--97350-just-add-100-more-augmenting-nerf-based-pseudo-lidar-point-cloud-for-resolving-class-imbalance-problem-mincheol-chang-et-al-2024>(54/122 | 97/350) Just Add $100 More: Augmenting NeRF-based Pseudo-LiDAR Point Cloud for Resolving Class-imbalance Problem (Mincheol Chang et al., 2024)</a></li><li><a href=#55122--98350-semantic-prompting-with-image-token-for-continual-learning-jisu-han-et-al-2024>(55/122 | 98/350) Semantic Prompting with Image-Token for Continual Learning (Jisu Han et al., 2024)</a></li><li><a href=#56122--99350-fed3dgs-scalable-3d-gaussian-splatting-with-federated-learning-teppei-suzuki-2024>(56/122 | 99/350) Fed3DGS: Scalable 3D Gaussian Splatting with Federated Learning (Teppei Suzuki, 2024)</a></li><li><a href=#57122--100350-hiker-sgg-hierarchical-knowledge-enhanced-robust-scene-graph-generation-ce-zhang-et-al-2024>(57/122 | 100/350) HiKER-SGG: Hierarchical Knowledge Enhanced Robust Scene Graph Generation (Ce Zhang et al., 2024)</a></li><li><a href=#58122--101350-development-of-automated-neural-network-prediction-for-echocardiographic-left-ventricular-ejection-fraction-yuting-zhang-et-al-2024>(58/122 | 101/350) Development of Automated Neural Network Prediction for Echocardiographic Left ventricular Ejection Fraction (Yuting Zhang et al., 2024)</a></li><li><a href=#59122--102350-distilling-datasets-into-less-than-one-image-asaf-shul-et-al-2024>(59/122 | 102/350) Distilling Datasets Into Less Than One Image (Asaf Shul et al., 2024)</a></li><li><a href=#60122--103350-vfusion3d-learning-scalable-3d-generative-models-from-video-diffusion-models-junlin-han-et-al-2024>(60/122 | 103/350) VFusion3D: Learning Scalable 3D Generative Models from Video Diffusion Models (Junlin Han et al., 2024)</a></li><li><a href=#61122--104350-dreammotion-space-time-self-similarity-score-distillation-for-zero-shot-video-editing-hyeonho-jeong-et-al-2024>(61/122 | 104/350) DreamMotion: Space-Time Self-Similarity Score Distillation for Zero-Shot Video Editing (Hyeonho Jeong et al., 2024)</a></li><li><a href=#62122--105350-subjective-aligned-dateset-and-metric-for-text-to-video-quality-assessment-tengchuan-kou-et-al-2024>(62/122 | 105/350) Subjective-Aligned Dateset and Metric for Text-to-Video Quality Assessment (Tengchuan Kou et al., 2024)</a></li><li><a href=#63122--106350-prioritized-semantic-learning-for-zero-shot-instance-navigation-xander-sun-et-al-2024>(63/122 | 106/350) Prioritized Semantic Learning for Zero-shot Instance Navigation (Xander Sun et al., 2024)</a></li><li><a href=#64122--107350-compositional-kronecker-context-optimization-for-vision-language-models-kun-ding-et-al-2024>(64/122 | 107/350) Compositional Kronecker Context Optimization for Vision-Language Models (Kun Ding et al., 2024)</a></li><li><a href=#65122--108350-lora-composer-leveraging-low-rank-adaptation-for-multi-concept-customization-in-training-free-diffusion-models-yang-yang-et-al-2024>(65/122 | 108/350) LoRA-Composer: Leveraging Low-Rank Adaptation for Multi-Concept Customization in Training-Free Diffusion Models (Yang Yang et al., 2024)</a></li><li><a href=#66122--109350-crs-diff-controllable-generative-remote-sensing-foundation-model-datao-tang-et-al-2024>(66/122 | 109/350) CRS-Diff: Controllable Generative Remote Sensing Foundation Model (Datao Tang et al., 2024)</a></li><li><a href=#67122--110350-dynosurf-neural-deformation-based-temporally-consistent-dynamic-surface-reconstruction-yuxin-yao-et-al-2024>(67/122 | 110/350) DynoSurf: Neural Deformation-based Temporally Consistent Dynamic Surface Reconstruction (Yuxin Yao et al., 2024)</a></li><li><a href=#68122--111350-augment-before-copy-paste-data-and-memory-efficiency-oriented-instance-segmentation-framework-for-sport-scenes-chih-chung-hsu-et-al-2024>(68/122 | 111/350) Augment Before Copy-Paste: Data and Memory Efficiency-Oriented Instance Segmentation Framework for Sport-scenes (Chih-Chung Hsu et al., 2024)</a></li><li><a href=#69122--112350-logicaldefender-discovering-extracting-and-utilizing-common-sense-knowledge-yuhe-liu-et-al-2024>(69/122 | 112/350) LogicalDefender: Discovering, Extracting, and Utilizing Common-Sense Knowledge (Yuhe Liu et al., 2024)</a></li><li><a href=#70122--113350-learning-unified-reference-representation-for-unsupervised-multi-class-anomaly-detection-liren-he-et-al-2024>(70/122 | 113/350) Learning Unified Reference Representation for Unsupervised Multi-class Anomaly Detection (Liren He et al., 2024)</a></li><li><a href=#71122--114350-hierarchical-spatial-proximity-reasoning-for-vision-and-language-navigation-ming-xu-et-al-2024>(71/122 | 114/350) Hierarchical Spatial Proximity Reasoning for Vision-and-Language Navigation (Ming Xu et al., 2024)</a></li><li><a href=#72122--115350-zero-shot-compound-expression-recognition-with-visual-language-model-at-the-6th-abaw-challenge-jiahe-wang-et-al-2024>(72/122 | 115/350) Zero-shot Compound Expression Recognition with Visual Language Model at the 6th ABAW Challenge (Jiahe Wang et al., 2024)</a></li><li><a href=#73122--116350-dreamsampler-unifying-diffusion-sampling-and-score-distillation-for-image-manipulation-jeongsol-kim-et-al-2024>(73/122 | 116/350) DreamSampler: Unifying Diffusion Sampling and Score Distillation for Image Manipulation (Jeongsol Kim et al., 2024)</a></li><li><a href=#74122--117350-federated-modality-specific-encoders-and-multimodal-anchors-for-personalized-brain-tumor-segmentation-qian-dai-et-al-2024>(74/122 | 117/350) Federated Modality-specific Encoders and Multimodal Anchors for Personalized Brain Tumor Segmentation (Qian Dai et al., 2024)</a></li><li><a href=#75122--118350-graphbev-towards-robust-bev-feature-alignment-for-multi-modal-3d-object-detection-ziying-song-et-al-2024>(75/122 | 118/350) GraphBEV: Towards Robust BEV Feature Alignment for Multi-Modal 3D Object Detection (Ziying Song et al., 2024)</a></li><li><a href=#76122--119350-modality-agnostic-fmri-decoding-of-vision-and-language-mitja-nikolaus-et-al-2024>(76/122 | 119/350) Modality-Agnostic fMRI Decoding of Vision and Language (Mitja Nikolaus et al., 2024)</a></li><li><a href=#77122--120350-circle-representation-for-medical-instance-object-segmentation-juming-xiong-et-al-2024>(77/122 | 120/350) Circle Representation for Medical Instance Object Segmentation (Juming Xiong et al., 2024)</a></li><li><a href=#78122--121350-gnerp-gaussian-guided-neural-reconstruction-of-reflective-objects-with-noisy-polarization-priors-li-yang-et-al-2024>(78/122 | 121/350) GNeRP: Gaussian-guided Neural Reconstruction of Reflective Objects with Noisy Polarization Priors (LI Yang et al., 2024)</a></li><li><a href=#79122--122350-relational-representation-learning-network-for-cross-spectral-image-patch-matching-chuang-yu-et-al-2024>(79/122 | 122/350) Relational Representation Learning Network for Cross-Spectral Image Patch Matching (Chuang Yu et al., 2024)</a></li><li><a href=#80122--123350-zero-shot-image-feature-consensus-with-deep-functional-maps-xinle-cheng-et-al-2024>(80/122 | 123/350) Zero-Shot Image Feature Consensus with Deep Functional Maps (Xinle Cheng et al., 2024)</a></li><li><a href=#81122--124350-regennet-towards-human-action-reaction-synthesis-liang-xu-et-al-2024>(81/122 | 124/350) ReGenNet: Towards Human Action-Reaction Synthesis (Liang Xu et al., 2024)</a></li><li><a href=#82122--125350-exploring-multi-modal-neural-scene-representations-with-applications-on-thermal-imaging-mert-özer-et-al-2024>(82/122 | 125/350) Exploring Multi-modal Neural Scene Representations With Applications on Thermal Imaging (Mert Özer et al., 2024)</a></li><li><a href=#83122--126350-lsknet-a-foundation-lightweight-backbone-for-remote-sensing-yuxuan-li-et-al-2024>(83/122 | 126/350) LSKNet: A Foundation Lightweight Backbone for Remote Sensing (Yuxuan Li et al., 2024)</a></li><li><a href=#84122--127350-towards-generalizing-to-unseen-domains-with-few-labels-chamuditha-jayanga-galappaththige-et-al-2024>(84/122 | 127/350) Towards Generalizing to Unseen Domains with Few Labels (Chamuditha Jayanga Galappaththige et al., 2024)</a></li><li><a href=#85122--128350-video-object-segmentation-with-dynamic-query-modulation-hantao-zhou-et-al-2024>(85/122 | 128/350) Video Object Segmentation with Dynamic Query Modulation (Hantao Zhou et al., 2024)</a></li><li><a href=#86122--129350-genflow-generalizable-recurrent-flow-for-6d-pose-refinement-of-novel-objects-sungphill-moon-et-al-2024>(86/122 | 129/350) GenFlow: Generalizable Recurrent Flow for 6D Pose Refinement of Novel Objects (Sungphill Moon et al., 2024)</a></li><li><a href=#87122--130350-shapeformer-shape-prior-visible-to-amodal-transformer-based-amodal-instance-segmentation-minh-tran-et-al-2024>(87/122 | 130/350) ShapeFormer: Shape Prior Visible-to-Amodal Transformer-based Amodal Instance Segmentation (Minh Tran et al., 2024)</a></li><li><a href=#88122--131350-path-gptomic-a-balanced-multi-modal-learning-framework-for-survival-outcome-prediction-hongxiao-wang-et-al-2024>(88/122 | 131/350) Path-GPTOmic: A Balanced Multi-modal Learning Framework for Survival Outcome Prediction (Hongxiao Wang et al., 2024)</a></li><li><a href=#89122--132350-thermonerf-multimodal-neural-radiance-fields-for-thermal-novel-view-synthesis-mariam-hassan-et-al-2024>(89/122 | 132/350) ThermoNeRF: Multimodal Neural Radiance Fields for Thermal Novel View Synthesis (Mariam Hassan et al., 2024)</a></li><li><a href=#90122--133350-prototipo-de-un-contador-bidireccional-automático-de-personas-basado-en-sensores-de-visión-3d-benjamín-ojeda-magaña-et-al-2024>(90/122 | 133/350) Prototipo de un Contador Bidireccional Automático de Personas basado en sensores de visión 3D (Benjamín Ojeda-Magaña et al., 2024)</a></li><li><a href=#91122--134350-data-efficient-contrastive-language-image-pretraining-prioritizing-data-quality-over-quantity-siddharth-joshi-et-al-2024>(91/122 | 134/350) Data-Efficient Contrastive Language-Image Pretraining: Prioritizing Data Quality over Quantity (Siddharth Joshi et al., 2024)</a></li><li><a href=#92122--135350-fusion-transformer-with-object-mask-guidance-for-image-forgery-analysis-dimitrios-karageorgiou-et-al-2024>(92/122 | 135/350) Fusion Transformer with Object Mask Guidance for Image Forgery Analysis (Dimitrios Karageorgiou et al., 2024)</a></li><li><a href=#93122--136350-hoidiffusion-generating-realistic-3d-hand-object-interaction-data-mengqi-zhang-et-al-2024>(93/122 | 136/350) HOIDiffusion: Generating Realistic 3D Hand-Object Interaction Data (Mengqi Zhang et al., 2024)</a></li><li><a href=#94122--137350-sv3d-novel-multi-view-synthesis-and-3d-generation-from-a-single-image-using-latent-video-diffusion-vikram-voleti-et-al-2024>(94/122 | 137/350) SV3D: Novel Multi-view Synthesis and 3D Generation from a Single Image using Latent Video Diffusion (Vikram Voleti et al., 2024)</a></li><li><a href=#95122--138350-see-imagine-plan-discovering-and-hallucinating-tasks-from-a-single-image-chenyang-ma-et-al-2024>(95/122 | 138/350) See, Imagine, Plan: Discovering and Hallucinating Tasks from a Single Image (Chenyang Ma et al., 2024)</a></li><li><a href=#96122--139350-exploring-facial-expression-recognition-through-semi-supervised-pretraining-and-temporal-modeling-jun-yu-et-al-2024>(96/122 | 139/350) Exploring Facial Expression Recognition through Semi-Supervised Pretraining and Temporal Modeling (Jun Yu et al., 2024)</a></li><li><a href=#97122--140350-intex-interactive-text-to-texture-synthesis-via-unified-depth-aware-inpainting-jiaxiang-tang-et-al-2024>(97/122 | 140/350) InTeX: Interactive Text-to-texture Synthesis via Unified Depth-aware Inpainting (Jiaxiang Tang et al., 2024)</a></li><li><a href=#98122--141350-towards-real-time-fast-unmanned-aerial-vehicle-detection-using-dynamic-vision-sensors-jakub-mandula-et-al-2024>(98/122 | 141/350) Towards Real-Time Fast Unmanned Aerial Vehicle Detection Using Dynamic Vision Sensors (Jakub Mandula et al., 2024)</a></li><li><a href=#99122--142350-tcnet-continuous-sign-language-recognition-from-trajectories-and-correlated-regions-hui-lu-et-al-2024>(99/122 | 142/350) TCNet: Continuous Sign Language Recognition from Trajectories and Correlated Regions (Hui Lu et al., 2024)</a></li><li><a href=#100122--143350-implicit-discriminative-knowledge-learning-for-visible-infrared-person-re-identification-kaijie-ren-et-al-2024>(100/122 | 143/350) Implicit Discriminative Knowledge Learning for Visible-Infrared Person Re-Identification (Kaijie Ren et al., 2024)</a></li><li><a href=#101122--144350-trajectorynas-a-neural-architecture-search-for-trajectory-prediction-ali-asghar-sharifi-et-al-2024>(101/122 | 144/350) TrajectoryNAS: A Neural Architecture Search for Trajectory Prediction (Ali Asghar Sharifi et al., 2024)</a></li><li><a href=#102122--145350-neds-slam-a-novel-neural-explicit-dense-semantic-slam-framework-using-3d-gaussian-splatting-yiming-ji-et-al-2024>(102/122 | 145/350) NEDS-SLAM: A Novel Neural Explicit Dense Semantic SLAM Framework using 3D Gaussian Splatting (Yiming Ji et al., 2024)</a></li><li><a href=#103122--146350-medmerge-merging-models-for-effective-transfer-learning-to-medical-imaging-tasks-ibrahim-almakky-et-al-2024>(103/122 | 146/350) MedMerge: Merging Models for Effective Transfer Learning to Medical Imaging Tasks (Ibrahim Almakky et al., 2024)</a></li><li><a href=#104122--147350-hsemotion-team-at-the-6th-abaw-competition-facial-expressions-valence-arousal-and-emotion-intensity-prediction-andrey-v-savchenko-2024>(104/122 | 147/350) HSEmotion Team at the 6th ABAW Competition: Facial Expressions, Valence-Arousal and Emotion Intensity Prediction (Andrey V. Savchenko, 2024)</a></li><li><a href=#105122--148350-ourdb-ouroboric-domain-bridging-for-multi-target-domain-adaptive-semantic-segmentation-seungbeom-woo-et-al-2024>(105/122 | 148/350) OurDB: Ouroboric Domain Bridging for Multi-Target Domain Adaptive Semantic Segmentation (Seungbeom Woo et al., 2024)</a></li><li><a href=#106122--149350-tarn-vist-topic-aware-reinforcement-network-for-visual-storytelling-weiran-chen-et-al-2024>(106/122 | 149/350) TARN-VIST: Topic Aware Reinforcement Network for Visual Storytelling (Weiran Chen et al., 2024)</a></li><li><a href=#107122--150350-end-to-end-underwater-video-enhancement-dataset-and-model-dazhao-du-et-al-2024>(107/122 | 150/350) End-To-End Underwater Video Enhancement: Dataset and Model (Dazhao Du et al., 2024)</a></li><li><a href=#108122--151350-generative-motion-stylization-within-canonical-motion-space-jiaxu-zhang-et-al-2024>(108/122 | 151/350) Generative Motion Stylization within Canonical Motion Space (Jiaxu Zhang et al., 2024)</a></li><li><a href=#109122--152350-defense-against-adversarial-attacks-on-no-reference-image-quality-models-with-gradient-norm-regularization-yujia-liu-et-al-2024>(109/122 | 152/350) Defense Against Adversarial Attacks on No-Reference Image Quality Models with Gradient Norm Regularization (Yujia Liu et al., 2024)</a></li><li><a href=#110122--153350-boosting-order-preserving-and-transferability-for-neural-architecture-search-a-joint-architecture-refined-search-and-fine-tuning-approach-beichen-zhang-et-al-2024>(110/122 | 153/350) Boosting Order-Preserving and Transferability for Neural Architecture Search: a Joint Architecture Refined Search and Fine-tuning Approach (Beichen Zhang et al., 2024)</a></li><li><a href=#111122--154350-diffusion-based-environment-aware-trajectory-prediction-theodor-westny-et-al-2024>(111/122 | 154/350) Diffusion-Based Environment-Aware Trajectory Prediction (Theodor Westny et al., 2024)</a></li><li><a href=#112122--155350-3dgs-calib-3d-gaussian-splatting-for-multimodal-spatiotemporal-calibration-quentin-herau-et-al-2024>(112/122 | 155/350) 3DGS-Calib: 3D Gaussian Splatting for Multimodal SpatioTemporal Calibration (Quentin Herau et al., 2024)</a></li><li><a href=#113122--156350-benchmarking-the-robustness-of-uav-tracking-against-common-corruptions-xiaoqiong-liu-et-al-2024>(113/122 | 156/350) Benchmarking the Robustness of UAV Tracking Against Common Corruptions (Xiaoqiong Liu et al., 2024)</a></li><li><a href=#114122--157350-the-polar-traverse-dataset-a-dataset-of-stereo-camera-images-simulating-traverses-across-lunar-polar-terrain-under-extreme-lighting-conditions-margaret-hansen-et-al-2024>(114/122 | 157/350) The POLAR Traverse Dataset: A Dataset of Stereo Camera Images Simulating Traverses across Lunar Polar Terrain under Extreme Lighting Conditions (Margaret Hansen et al., 2024)</a></li><li><a href=#115122--158350-roguenerf-a-robust-geometry-consistent-universal-enhancer-for-nerf-sibi-catley-chandar-et-al-2024>(115/122 | 158/350) RoGUENeRF: A Robust Geometry-Consistent Universal Enhancer for NeRF (Sibi Catley-Chandar et al., 2024)</a></li><li><a href=#116122--159350-emie-map-large-scale-road-surface-reconstruction-based-on-explicit-mesh-and-implicit-encoding-wenhua-wu-et-al-2024>(116/122 | 159/350) EMIE-MAP: Large-Scale Road Surface Reconstruction Based on Explicit Mesh and Implicit Encoding (Wenhua Wu et al., 2024)</a></li><li><a href=#117122--160350-gaussnav-gaussian-splatting-for-visual-navigation-xiaohan-lei-et-al-2024>(117/122 | 160/350) GaussNav: Gaussian Splatting for Visual Navigation (Xiaohan Lei et al., 2024)</a></li><li><a href=#118122--161350-r3ds-reality-linked-3d-scenes-for-panoramic-scene-understanding-qirui-wu-et-al-2024>(118/122 | 161/350) R3DS: Reality-linked 3D Scenes for Panoramic Scene Understanding (Qirui Wu et al., 2024)</a></li><li><a href=#119122--162350-a-unified-model-for-longitudinal-multi-modal-multi-view-prediction-with-missingness-boqi-chen-et-al-2024>(119/122 | 162/350) A Unified Model for Longitudinal Multi-Modal Multi-View Prediction with Missingness (Boqi Chen et al., 2024)</a></li><li><a href=#120122--163350-expandable-subspace-ensemble-for-pre-trained-model-based-class-incremental-learning-da-wei-zhou-et-al-2024>(120/122 | 163/350) Expandable Subspace Ensemble for Pre-Trained Model-Based Class-Incremental Learning (Da-Wei Zhou et al., 2024)</a></li><li><a href=#121122--164350-ivac-p2l-leveraging-irregular-repetition-priors-for-improving-video-action-counting-hang-wang-et-al-2024>(121/122 | 164/350) IVAC-P2L: Leveraging Irregular Repetition Priors for Improving Video Action Counting (Hang Wang et al., 2024)</a></li><li><a href=#122122--165350-n-modal-contrastive-losses-with-applications-to-social-media-data-in-trimodal-space-william-theisen-et-al-2024>(122/122 | 165/350) N-Modal Contrastive Losses with Applications to Social Media Data in Trimodal Space (William Theisen et al., 2024)</a></li></ul></li><li><a href=#cscr-8>cs.CR (8)</a><ul><li><a href=#18--166350-shifting-the-lens-detecting-malware-in-npm-ecosystem-with-large-language-models-nusrat-zahan-et-al-2024>(1/8 | 166/350) Shifting the Lens: Detecting Malware in npm Ecosystem with Large Language Models (Nusrat Zahan et al., 2024)</a></li><li><a href=#28--167350-problem-space-structural-adversarial-attacks-for-network-intrusion-detection-systems-based-on-graph-neural-networks-andrea-venturi-et-al-2024>(2/8 | 167/350) Problem space structural adversarial attacks for Network Intrusion Detection Systems based on Graph Neural Networks (Andrea Venturi et al., 2024)</a></li><li><a href=#38--168350-large-language-models-in-6g-security-challenges-and-opportunities-tri-nguyen-et-al-2024>(3/8 | 168/350) Large language models in 6G security: challenges and opportunities (Tri Nguyen et al., 2024)</a></li><li><a href=#48--169350-parasitic-circuson-the-feasibility-of-golden-free-pcb-verification-maryam-saadat-safa-et-al-2024>(4/8 | 169/350) Parasitic Circus:On the Feasibility of Golden Free PCB Verification (Maryam Saadat Safa et al., 2024)</a></li><li><a href=#58--170350-diffusion-denoising-as-a-certified-defense-against-clean-label-poisoning-sanghyun-hong-et-al-2024>(5/8 | 170/350) Diffusion Denoising as a Certified Defense against Clean-label Poisoning (Sanghyun Hong et al., 2024)</a></li><li><a href=#68--171350-is-it-really-you-who-forgot-the-password-when-account-recovery-meets-risk-based-authentication-andre-büttner-et-al-2024>(6/8 | 171/350) Is It Really You Who Forgot the Password? When Account Recovery Meets Risk-Based Authentication (Andre Büttner et al., 2024)</a></li><li><a href=#78--172350-efficient-and-privacy-preserving-federated-learning-based-on-full-homomorphic-encryption-yuqi-guo-et-al-2024>(7/8 | 172/350) Efficient and Privacy-Preserving Federated Learning based on Full Homomorphic Encryption (Yuqi Guo et al., 2024)</a></li><li><a href=#88--173350-budget-recycling-differential-privacy-bo-jiang-et-al-2024>(8/8 | 173/350) Budget Recycling Differential Privacy (Bo Jiang et al., 2024)</a></li></ul></li><li><a href=#csne-2>cs.NE (2)</a><ul><li><a href=#12--174350-unsupervised-end-to-end-training-with-a-self-defined-bio-inspired-target-dongshu-liu-et-al-2024>(1/2 | 174/350) Unsupervised End-to-End Training with a Self-Defined Bio-Inspired Target (Dongshu Liu et al., 2024)</a></li><li><a href=#22--175350-llm-guided-evolution----the-automation-of-models-advancing-models-clint-morris-et-al-2024>(2/2 | 175/350) LLM Guided Evolution &ndash; The Automation of Models Advancing Models (Clint Morris et al., 2024)</a></li></ul></li><li><a href=#csro-32>cs.RO (32)</a><ul><li><a href=#132--176350-can-llms-generate-human-like-wayfinding-instructions-towards-platform-agnostic-embodied-instruction-synthesis-vishnu-sashank-dorbala-et-al-2024>(1/32 | 176/350) Can LLMs Generate Human-Like Wayfinding Instructions? Towards Platform-Agnostic Embodied Instruction Synthesis (Vishnu Sashank Dorbala et al., 2024)</a></li><li><a href=#232--177350-llm3large-language-model-based-task-and-motion-planning-with-motion-failure-reasoning-shu-wang-et-al-2024>(2/32 | 177/350) LLM3:Large Language Model-based Task and Motion Planning with Motion Failure Reasoning (Shu Wang et al., 2024)</a></li><li><a href=#332--178350-multimodal-human-autonomous-agents-interaction-using-pre-trained-language-and-visual-foundation-models-linus-nwankwo-et-al-2024>(3/32 | 178/350) Multimodal Human-Autonomous Agents Interaction Using Pre-Trained Language and Visual Foundation Models (Linus Nwankwo et al., 2024)</a></li><li><a href=#432--179350-sim-to-real-grasp-detection-with-global-to-local-rgb-d-adaptation-haoxiang-ma-et-al-2024>(4/32 | 179/350) Sim-to-Real Grasp Detection with Global-to-Local RGB-D Adaptation (Haoxiang Ma et al., 2024)</a></li><li><a href=#532--180350-continual-domain-randomization-josip-josifovski-et-al-2024>(5/32 | 180/350) Continual Domain Randomization (Josip Josifovski et al., 2024)</a></li><li><a href=#632--181350-synthesizing-multi-log-grasp-poses-arvid-fälldin-et-al-2024>(6/32 | 181/350) Synthesizing multi-log grasp poses (Arvid Fälldin et al., 2024)</a></li><li><a href=#732--182350-visual-preference-inference-an-image-sequence-based-preference-reasoning-in-tabletop-object-manipulation-joonhyung-lee-et-al-2024>(7/32 | 182/350) Visual Preference Inference: An Image Sequence-Based Preference Reasoning in Tabletop Object Manipulation (Joonhyung Lee et al., 2024)</a></li><li><a href=#832--183350-aldm-grasping-diffusion-aided-zero-shot-sim-to-real-transfer-for-robot-grasping-yiwei-li-et-al-2024>(8/32 | 183/350) ALDM-Grasping: Diffusion-aided Zero-Shot Sim-to-Real Transfer for Robot Grasping (Yiwei Li et al., 2024)</a></li><li><a href=#932--184350-demystifying-deep-reinforcement-learning-based-autonomous-vehicle-decision-making-hanxi-wan-et-al-2024>(9/32 | 184/350) Demystifying Deep Reinforcement Learning-Based Autonomous Vehicle Decision-Making (Hanxi Wan et al., 2024)</a></li><li><a href=#1032--185350-improving-out-of-distribution-generalization-of-learned-dynamics-by-learning-pseudometrics-and-constraint-manifolds-yating-lin-et-al-2024>(10/32 | 185/350) Improving Out-of-Distribution Generalization of Learned Dynamics by Learning Pseudometrics and Constraint Manifolds (Yating Lin et al., 2024)</a></li><li><a href=#1132--186350-bootstrapping-reinforcement-learning-with-imitation-for-vision-based-agile-flight-jiaxu-xing-et-al-2024>(11/32 | 186/350) Bootstrapping Reinforcement Learning with Imitation for Vision-Based Agile Flight (Jiaxu Xing et al., 2024)</a></li><li><a href=#1232--187350-sim2real-manipulation-on-unknown-objects-with-tactile-based-reinforcement-learning-entong-su-et-al-2024>(12/32 | 187/350) Sim2Real Manipulation on Unknown Objects with Tactile-based Reinforcement Learning (Entong Su et al., 2024)</a></li><li><a href=#1332--188350-inferring-belief-states-in-partially-observable-human-robot-teams-jack-kolb-et-al-2024>(13/32 | 188/350) Inferring Belief States in Partially-Observable Human-Robot Teams (Jack Kolb et al., 2024)</a></li><li><a href=#1432--189350-deep-bayesian-future-fusion-for-self-supervised-high-resolution-off-road-mapping-shubhra-aich-et-al-2024>(14/32 | 189/350) Deep Bayesian Future Fusion for Self-Supervised, High-Resolution, Off-Road Mapping (Shubhra Aich et al., 2024)</a></li><li><a href=#1532--190350-reinforcement-learning-with-latent-state-inference-for-autonomous-on-ramp-merging-under-observation-delay-amin-tabrizian-et-al-2024>(15/32 | 190/350) Reinforcement Learning with Latent State Inference for Autonomous On-ramp Merging under Observation Delay (Amin Tabrizian et al., 2024)</a></li><li><a href=#1632--191350-locomotion-generation-for-a-rat-robot-based-on-environmental-changes-via-reinforcement-learning-xinhui-shan-et-al-2024>(16/32 | 191/350) Locomotion Generation for a Rat Robot based on Environmental Changes via Reinforcement Learning (Xinhui Shan et al., 2024)</a></li><li><a href=#1732--192350-combining-local-and-global-perception-for-autonomous-navigation-on-nano-uavs-lorenzo-lamberti-et-al-2024>(17/32 | 192/350) Combining Local and Global Perception for Autonomous Navigation on Nano-UAVs (Lorenzo Lamberti et al., 2024)</a></li><li><a href=#1832--193350-scalable-networked-feature-selection-with-randomized-algorithm-for-robot-navigation-vivek-pandey-et-al-2024>(18/32 | 193/350) Scalable Networked Feature Selection with Randomized Algorithm for Robot Navigation (Vivek Pandey et al., 2024)</a></li><li><a href=#1932--194350-robot-navigation-in-unknown-and-cluttered-workspace-with-dynamical-system-modulation-in-starshaped-roadmap-kai-chen-et-al-2024>(19/32 | 194/350) Robot Navigation in Unknown and Cluttered Workspace with Dynamical System Modulation in Starshaped Roadmap (Kai Chen et al., 2024)</a></li><li><a href=#2032--195350-ikspark-an-inverse-kinematics-solver-using-semidefinite-relaxation-and-rank-minimization-liangting-wu-et-al-2024>(20/32 | 195/350) IKSPARK: An Inverse Kinematics Solver using Semidefinite Relaxation and Rank Minimization (Liangting Wu et al., 2024)</a></li><li><a href=#2132--196350-frontier-based-exploration-for-multi-robot-rendezvous-in-communication-restricted-unknown-environments-mauro-tellaroli-et-al-2024>(21/32 | 196/350) Frontier-Based Exploration for Multi-Robot Rendezvous in Communication-Restricted Unknown Environments (Mauro Tellaroli et al., 2024)</a></li><li><a href=#2232--197350-r2snet-scalable-domain-adaptation-for-object-detection-in-cloud-based-robots-ecosystems-via-proposal-refinement-michele-antonazzi-et-al-2024>(22/32 | 197/350) R2SNet: Scalable Domain Adaptation for Object Detection in Cloud-Based Robots Ecosystems via Proposal Refinement (Michele Antonazzi et al., 2024)</a></li><li><a href=#2332--198350-on-the-benefits-of-gpu-sample-based-stochastic-predictive-controllers-for-legged-locomotion-giulio-turrisi-et-al-2024>(23/32 | 198/350) On the Benefits of GPU Sample-Based Stochastic Predictive Controllers for Legged Locomotion (Giulio Turrisi et al., 2024)</a></li><li><a href=#2432--199350-scenesense-diffusion-models-for-3d-occupancy-synthesis-from-partial-observation-alec-reed-et-al-2024>(24/32 | 199/350) SceneSense: Diffusion Models for 3D Occupancy Synthesis from Partial Observation (Alec Reed et al., 2024)</a></li><li><a href=#2532--200350-agrnav-efficient-and-energy-saving-autonomous-navigation-for-air-ground-robots-in-occlusion-prone-environments-junming-wang-et-al-2024>(25/32 | 200/350) AGRNav: Efficient and Energy-Saving Autonomous Navigation for Air-Ground Robots in Occlusion-Prone Environments (Junming Wang et al., 2024)</a></li><li><a href=#2632--201350-reachability-based-trajectory-design-via-exact-formulation-of-implicit-neural-signed-distance-functions-jonathan-michaux-et-al-2024>(26/32 | 201/350) Reachability-based Trajectory Design via Exact Formulation of Implicit Neural Signed Distance Functions (Jonathan Michaux et al., 2024)</a></li><li><a href=#2732--202350-safety-implications-of-explainable-artificial-intelligence-in-end-to-end-autonomous-driving-shahin-atakishiyev-et-al-2024>(27/32 | 202/350) Safety Implications of Explainable Artificial Intelligence in End-to-End Autonomous Driving (Shahin Atakishiyev et al., 2024)</a></li><li><a href=#2832--203350-ergonomic-optimization-in-worker-robot-bimanual-object-handover-implementing-reba-using-reinforcement-learning-in-virtual-reality-mani-amani-et-al-2024>(28/32 | 203/350) Ergonomic Optimization in Worker-Robot Bimanual Object Handover: Implementing REBA Using Reinforcement Learning in Virtual Reality (Mani Amani et al., 2024)</a></li><li><a href=#2932--204350-mcd-diverse-large-scale-multi-campus-dataset-for-robot-perception-thien-minh-nguyen-et-al-2024>(29/32 | 204/350) MCD: Diverse Large-Scale Multi-Campus Dataset for Robot Perception (Thien-Minh Nguyen et al., 2024)</a></li><li><a href=#3032--205350-vihe-virtual-in-hand-eye-transformer-for-3d-robotic-manipulation-weiyao-wang-et-al-2024>(30/32 | 205/350) VIHE: Virtual In-Hand Eye Transformer for 3D Robotic Manipulation (Weiyao Wang et al., 2024)</a></li><li><a href=#3132--206350-smt-based-dynamic-multi-robot-task-allocation-victoria-marie-tuck-et-al-2024>(31/32 | 206/350) SMT-Based Dynamic Multi-Robot Task Allocation (Victoria Marie Tuck et al., 2024)</a></li><li><a href=#3232--207350-masstar-a-multi-modal-and-large-scale-scene-dataset-with-a-versatile-toolchain-for-surface-prediction-and-completion-guiyong-zheng-et-al-2024>(32/32 | 207/350) MASSTAR: A Multi-Modal and Large-Scale Scene Dataset with a Versatile Toolchain for Surface Prediction and Completion (Guiyong Zheng et al., 2024)</a></li></ul></li><li><a href=#cslg-50>cs.LG (50)</a><ul><li><a href=#150--208350-graph-partial-label-learning-with-potential-cause-discovering-hang-gao-et-al-2024>(1/50 | 208/350) Graph Partial Label Learning with Potential Cause Discovering (Hang Gao et al., 2024)</a></li><li><a href=#250--209350-improving-lora-in-privacy-preserving-federated-learning-youbang-sun-et-al-2024>(2/50 | 209/350) Improving LoRA in Privacy-preserving Federated Learning (Youbang Sun et al., 2024)</a></li><li><a href=#350--210350-efficient-transformer-based-hyper-parameter-optimization-for-resource-constrained-iot-environments-ibrahim-shaer-et-al-2024>(3/50 | 210/350) Efficient Transformer-based Hyper-parameter Optimization for Resource-constrained IoT Environments (Ibrahim Shaer et al., 2024)</a></li><li><a href=#450--211350-narrative-feature-or-structured-feature-a-study-of-large-language-models-to-identify-cancer-patients-at-risk-of-heart-failure-ziyi-chen-et-al-2024>(4/50 | 211/350) Narrative Feature or Structured Feature? A Study of Large Language Models to Identify Cancer Patients at Risk of Heart Failure (Ziyi Chen et al., 2024)</a></li><li><a href=#550--212350-learning-useful-representations-of-recurrent-neural-network-weight-matrices-vincent-herrmann-et-al-2024>(5/50 | 212/350) Learning Useful Representations of Recurrent Neural Network Weight Matrices (Vincent Herrmann et al., 2024)</a></li><li><a href=#650--213350-investigating-the-benefits-of-projection-head-for-representation-learning-yihao-xue-et-al-2024>(6/50 | 213/350) Investigating the Benefits of Projection Head for Representation Learning (Yihao Xue et al., 2024)</a></li><li><a href=#750--214350-supervised-fine-tuning-as-inverse-reinforcement-learning-hao-sun-2024>(7/50 | 214/350) Supervised Fine-Tuning as Inverse Reinforcement Learning (Hao Sun, 2024)</a></li><li><a href=#850--215350-transfer-learning-beyond-bounded-density-ratios-alkis-kalavasis-et-al-2024>(8/50 | 215/350) Transfer Learning Beyond Bounded Density Ratios (Alkis Kalavasis et al., 2024)</a></li><li><a href=#950--216350-knfu-effective-knowledge-fusion-s-jamal-seyedmohammadi-et-al-2024>(9/50 | 216/350) KnFu: Effective Knowledge Fusion (S. Jamal Seyedmohammadi et al., 2024)</a></li><li><a href=#1050--217350-linguacodus-a-synergistic-framework-for-transformative-code-generation-in-machine-learning-pipelines-ekaterina-trofimova-et-al-2024>(10/50 | 217/350) Linguacodus: A Synergistic Framework for Transformative Code Generation in Machine Learning Pipelines (Ekaterina Trofimova et al., 2024)</a></li><li><a href=#1150--218350-state-separated-sarsa-a-practical-sequential-decision-making-algorithm-with-recovering-rewards-yuto-tanimoto-et-al-2024>(11/50 | 218/350) State-Separated SARSA: A Practical Sequential Decision-Making Algorithm with Recovering Rewards (Yuto Tanimoto et al., 2024)</a></li><li><a href=#1250--219350-variational-sampling-of-temporal-trajectories-jurijs-nazarovs-et-al-2024>(12/50 | 219/350) Variational Sampling of Temporal Trajectories (Jurijs Nazarovs et al., 2024)</a></li><li><a href=#1350--220350-removing-undesirable-concepts-in-text-to-image-generative-models-with-learnable-prompts-anh-bui-et-al-2024>(13/50 | 220/350) Removing Undesirable Concepts in Text-to-Image Generative Models with Learnable Prompts (Anh Bui et al., 2024)</a></li><li><a href=#1450--221350-parmesan-parameter-free-memory-search-and-transduction-for-dense-prediction-tasks-philip-matthias-winter-et-al-2024>(14/50 | 221/350) PARMESAN: Parameter-Free Memory Search and Transduction for Dense Prediction Tasks (Philip Matthias Winter et al., 2024)</a></li><li><a href=#1550--222350-open-world-semi-supervised-learning-for-node-classification-yanling-wang-et-al-2024>(15/50 | 222/350) Open-World Semi-Supervised Learning for Node Classification (Yanling Wang et al., 2024)</a></li><li><a href=#1650--223350-accelerating-scientific-discovery-with-generative-knowledge-extraction-graph-based-representation-and-multimodal-intelligent-graph-reasoning-markus-j-buehler-2024>(16/50 | 223/350) Accelerating Scientific Discovery with Generative Knowledge Extraction, Graph-Based Representation, and Multimodal Intelligent Graph Reasoning (Markus J. Buehler, 2024)</a></li><li><a href=#1750--224350-molecular-classification-using-hyperdimensional-graph-classification-pere-verges-et-al-2024>(17/50 | 224/350) Molecular Classification Using Hyperdimensional Graph Classification (Pere Verges et al., 2024)</a></li><li><a href=#1850--225350-larimar-large-language-models-with-episodic-memory-control-payel-das-et-al-2024>(18/50 | 225/350) Larimar: Large Language Models with Episodic Memory Control (Payel Das et al., 2024)</a></li><li><a href=#1950--226350-layer-diverse-negative-sampling-for-graph-neural-networks-wei-duan-et-al-2024>(19/50 | 226/350) Layer-diverse Negative Sampling for Graph Neural Networks (Wei Duan et al., 2024)</a></li><li><a href=#2050--227350-improving-generalization-via-meta-learning-on-hard-samples-nishant-jain-et-al-2024>(20/50 | 227/350) Improving Generalization via Meta-Learning on Hard Samples (Nishant Jain et al., 2024)</a></li><li><a href=#2150--228350-unveil-conditional-diffusion-models-with-classifier-free-guidance-a-sharp-statistical-theory-hengyu-fu-et-al-2024>(21/50 | 228/350) Unveil Conditional Diffusion Models with Classifier-free Guidance: A Sharp Statistical Theory (Hengyu Fu et al., 2024)</a></li><li><a href=#2250--229350-s-jepa-towards-seamless-cross-dataset-transfer-through-dynamic-spatial-attention-pierre-guetschel-et-al-2024>(22/50 | 229/350) S-JEPA: towards seamless cross-dataset transfer through dynamic spatial attention (Pierre Guetschel et al., 2024)</a></li><li><a href=#2350--230350-pita-physics-informed-trajectory-autoencoder-johannes-fischer-et-al-2024>(23/50 | 230/350) PITA: Physics-Informed Trajectory Autoencoder (Johannes Fischer et al., 2024)</a></li><li><a href=#2450--231350-seisfusion-constrained-diffusion-model-with-input-guidance-for-3d-seismic-data-interpolation-and-reconstruction-shuang-wang-et-al-2024>(24/50 | 231/350) SeisFusion: Constrained Diffusion Model with Input Guidance for 3D Seismic Data Interpolation and Reconstruction (Shuang Wang et al., 2024)</a></li><li><a href=#2550--232350-gcam-gaussian-and-causal-attention-model-of-food-fine-grained-recognition-guohang-zhuang-et-al-2024>(25/50 | 232/350) GCAM: Gaussian and causal-attention model of food fine-grained recognition (Guohang Zhuang et al., 2024)</a></li><li><a href=#2650--233350-automated-data-processing-and-feature-engineering-for-deep-learning-and-big-data-applications-a-survey-alhassan-mumuni-et-al-2024>(26/50 | 233/350) Automated data processing and feature engineering for deep learning and big data applications: a survey (Alhassan Mumuni et al., 2024)</a></li><li><a href=#2750--234350-semantic-enhanced-representation-learning-for-road-networks-with-temporal-dynamics-yile-chen-et-al-2024>(27/50 | 234/350) Semantic-Enhanced Representation Learning for Road Networks with Temporal Dynamics (Yile Chen et al., 2024)</a></li><li><a href=#2850--235350-graph-neural-networks-for-learning-equivariant-representations-of-neural-networks-miltiadis-kofinas-et-al-2024>(28/50 | 235/350) Graph Neural Networks for Learning Equivariant Representations of Neural Networks (Miltiadis Kofinas et al., 2024)</a></li><li><a href=#2950--236350-routerbench-a-benchmark-for-multi-llm-routing-system-qitian-jason-hu-et-al-2024>(29/50 | 236/350) ROUTERBENCH: A Benchmark for Multi-LLM Routing System (Qitian Jason Hu et al., 2024)</a></li><li><a href=#3050--237350-casper-causality-aware-spatiotemporal-graph-neural-networks-for-spatiotemporal-time-series-imputation-baoyu-jing-et-al-2024>(30/50 | 237/350) CASPER: Causality-Aware Spatiotemporal Graph Neural Networks for Spatiotemporal Time Series Imputation (Baoyu Jing et al., 2024)</a></li><li><a href=#3150--238350-time-series-compression-using-quaternion-valued-neural-networks-and-quaternion-backpropagation-johannes-pöppelbaum-et-al-2024>(31/50 | 238/350) Time Series Compression using Quaternion Valued Neural Networks and Quaternion Backpropagation (Johannes Pöppelbaum et al., 2024)</a></li><li><a href=#3250--239350-complete-and-efficient-graph-transformers-for-crystal-material-property-prediction-keqiang-yan-et-al-2024>(32/50 | 239/350) Complete and Efficient Graph Transformers for Crystal Material Property Prediction (Keqiang Yan et al., 2024)</a></li><li><a href=#3350--240350-reinforcement-learning-from-delayed-observations-via-world-models-armin-karamzade-et-al-2024>(33/50 | 240/350) Reinforcement Learning from Delayed Observations via World Models (Armin Karamzade et al., 2024)</a></li><li><a href=#3450--241350-multistep-inverse-is-not-all-you-need-alexander-levine-et-al-2024>(34/50 | 241/350) Multistep Inverse Is Not All You Need (Alexander Levine et al., 2024)</a></li><li><a href=#3550--242350-global-optimality-without-mixing-time-oracles-in-average-reward-rl-via-multi-level-actor-critic-bhrij-patel-et-al-2024>(35/50 | 242/350) Global Optimality without Mixing Time Oracles in Average-reward RL via Multi-level Actor-Critic (Bhrij Patel et al., 2024)</a></li><li><a href=#3650--243350-efficient-training-of-learning-based-thermal-power-flow-for-4th-generation-district-heating-grids-andreas-bott-et-al-2024>(36/50 | 243/350) Efficient Training of Learning-Based Thermal Power Flow for 4th Generation District Heating Grids (Andreas Bott et al., 2024)</a></li><li><a href=#3750--244350-near-optimal-solutions-of-constrained-learning-problems-juan-elenter-et-al-2024>(37/50 | 244/350) Near-Optimal Solutions of Constrained Learning Problems (Juan Elenter et al., 2024)</a></li><li><a href=#3850--245350-uncertainty-calibrated-test-time-model-adaptation-without-forgetting-mingkui-tan-et-al-2024>(38/50 | 245/350) Uncertainty-Calibrated Test-Time Model Adaptation without Forgetting (Mingkui Tan et al., 2024)</a></li><li><a href=#3950--246350-span-based-optimal-sample-complexity-for-weakly-communicating-and-general-average-reward-mdps-matthew-zurek-et-al-2024>(39/50 | 246/350) Span-Based Optimal Sample Complexity for Weakly Communicating and General Average Reward MDPs (Matthew Zurek et al., 2024)</a></li><li><a href=#4050--247350-fedspu-personalized-federated-learning-for-resource-constrained-devices-with-stochastic-parameter-update-ziru-niu-et-al-2024>(40/50 | 247/350) FedSPU: Personalized Federated Learning for Resource-constrained Devices with Stochastic Parameter Update (Ziru Niu et al., 2024)</a></li><li><a href=#4150--248350-large-scale-flood-modeling-and-forecasting-with-floodcast-qingsong-xu-et-al-2024>(41/50 | 248/350) Large-scale flood modeling and forecasting with FloodCast (Qingsong Xu et al., 2024)</a></li><li><a href=#4250--249350-offline-multitask-representation-learning-for-reinforcement-learning-haque-ishfaq-et-al-2024>(42/50 | 249/350) Offline Multitask Representation Learning for Reinforcement Learning (Haque Ishfaq et al., 2024)</a></li><li><a href=#4350--250350-petscml-second-order-solvers-for-training-regression-problems-in-scientific-machine-learning-stefano-zampini-et-al-2024>(43/50 | 250/350) PETScML: Second-order solvers for training regression problems in Scientific Machine Learning (Stefano Zampini et al., 2024)</a></li><li><a href=#4450--251350-informed-spectral-normalized-gaussian-processes-for-trajectory-prediction-christian-schlauch-et-al-2024>(44/50 | 251/350) Informed Spectral Normalized Gaussian Processes for Trajectory Prediction (Christian Schlauch et al., 2024)</a></li><li><a href=#4550--252350-single-agent-actor-critic-for-decentralized-cooperative-driving-shengchao-yan-et-al-2024>(45/50 | 252/350) Single-Agent Actor Critic for Decentralized Cooperative Driving (Shengchao Yan et al., 2024)</a></li><li><a href=#4650--253350-crystalformer-infinitely-connected-attention-for-periodic-structure-encoding-tatsunori-taniai-et-al-2024>(46/50 | 253/350) Crystalformer: Infinitely Connected Attention for Periodic Structure Encoding (Tatsunori Taniai et al., 2024)</a></li><li><a href=#4750--254350-the-value-of-reward-lookahead-in-reinforcement-learning-nadav-merlis-et-al-2024>(47/50 | 254/350) The Value of Reward Lookahead in Reinforcement Learning (Nadav Merlis et al., 2024)</a></li><li><a href=#4850--255350-rl-in-markov-games-with-independent-function-approximation-improved-sample-complexity-bound-under-the-local-access-model-junyi-fan-et-al-2024>(48/50 | 255/350) RL in Markov Games with Independent Function Approximation: Improved Sample Complexity Bound under the Local Access Model (Junyi Fan et al., 2024)</a></li><li><a href=#4950--256350-methods-for-generating-drift-in-text-streams-cristiano-mesquita-garcia-et-al-2024>(49/50 | 256/350) Methods for Generating Drift in Text Streams (Cristiano Mesquita Garcia et al., 2024)</a></li><li><a href=#5050--257350-accelerating-string-key-learned-index-structures-via-memoization-based-incremental-training-minsu-kim-et-al-2024>(50/50 | 257/350) Accelerating String-Key Learned Index Structures via Memoization-based Incremental Training (Minsu Kim et al., 2024)</a></li></ul></li><li><a href=#csai-9>cs.AI (9)</a><ul><li><a href=#19--258350-how-far-are-we-on-the-decision-making-of-llms-evaluating-llms-gaming-ability-in-multi-agent-environments-jen-tse-huang-et-al-2024>(1/9 | 258/350) How Far Are We on the Decision-Making of LLMs? Evaluating LLMs&rsquo; Gaming Ability in Multi-Agent Environments (Jen-tse Huang et al., 2024)</a></li><li><a href=#29--259350-can-llm-augmented-autonomous-agents-cooperate-an-evaluation-of-their-cooperative-capabilities-through-melting-pot-manuel-mosquera-et-al-2024>(2/9 | 259/350) Can LLM-Augmented autonomous agents cooperate?, An evaluation of their cooperative capabilities through Melting Pot (Manuel Mosquera et al., 2024)</a></li><li><a href=#39--260350-compositional-learning-of-functions-in-humans-and-machines-yanli-zhou-et-al-2024>(3/9 | 260/350) Compositional learning of functions in humans and machines (Yanli Zhou et al., 2024)</a></li><li><a href=#49--261350-fusing-domain-specific-content-from-large-language-models-into-knowledge-graphs-for-enhanced-zero-shot-object-state-classification-filippos-gouidis-et-al-2024>(4/9 | 261/350) Fusing Domain-Specific Content from Large Language Models into Knowledge Graphs for Enhanced Zero Shot Object State Classification (Filippos Gouidis et al., 2024)</a></li><li><a href=#59--262350-gradient-based-fuzzy-system-optimisation-via-automatic-differentiation----fuzzyr-as-a-use-case-chao-chen-et-al-2024>(5/9 | 262/350) Gradient-based Fuzzy System Optimisation via Automatic Differentiation &ndash; FuzzyR as a Use Case (Chao Chen et al., 2024)</a></li><li><a href=#69--263350-learning-general-policies-for-classical-planning-domains-getting-beyond-c_2-simon-ståhlberg-et-al-2024>(6/9 | 263/350) Learning General Policies for Classical Planning Domains: Getting Beyond C$_2$ (Simon Ståhlberg et al., 2024)</a></li><li><a href=#79--264350-turkingbench-a-challenge-benchmark-for-web-agents-kevin-xu-et-al-2024>(7/9 | 264/350) Tur[k]ingBench: A Challenge Benchmark for Web Agents (Kevin Xu et al., 2024)</a></li><li><a href=#89--265350-guiding-the-generation-of-counterfactual-explanations-through-temporal-background-knowledge-for-predictive-process-monitoring-andrei-buliga-et-al-2024>(8/9 | 265/350) Guiding the generation of counterfactual explanations through temporal background knowledge for Predictive Process Monitoring (Andrei Buliga et al., 2024)</a></li><li><a href=#99--266350-does-ai-help-humans-make-better-decisions-a-methodological-framework-for-experimental-evaluation-eli-ben-michael-et-al-2024>(9/9 | 266/350) Does AI help humans make better decisions? A methodological framework for experimental evaluation (Eli Ben-Michael et al., 2024)</a></li></ul></li><li><a href=#cscy-5>cs.CY (5)</a><ul><li><a href=#15--267350-embracing-the-generative-ai-revolution-advancing-tertiary-education-in-cybersecurity-with-gpt-raza-nowrozy-et-al-2024>(1/5 | 267/350) Embracing the Generative AI Revolution: Advancing Tertiary Education in Cybersecurity with GPT (Raza Nowrozy et al., 2024)</a></li><li><a href=#25--268350-synthetic-image-generation-in-cyber-influence-operations-an-emergent-threat-melanie-mathys-et-al-2024>(2/5 | 268/350) Synthetic Image Generation in Cyber Influence Operations: An Emergent Threat? (Melanie Mathys et al., 2024)</a></li><li><a href=#35--269350-a-toolbox-for-surfacing-health-equity-harms-and-biases-in-large-language-models-stephen-r-pfohl-et-al-2024>(3/5 | 269/350) A Toolbox for Surfacing Health Equity Harms and Biases in Large Language Models (Stephen R. Pfohl et al., 2024)</a></li><li><a href=#45--270350-bangladesh-agricultural-knowledge-graph-enabling-semantic-integration-and-data-driven-analysis--full-version-rudra-pratap-deb-nath-et-al-2024>(4/5 | 270/350) Bangladesh Agricultural Knowledge Graph: Enabling Semantic Integration and Data-driven Analysis&ndash;Full Version (Rudra Pratap Deb Nath et al., 2024)</a></li><li><a href=#55--271350-analyzing-evaluating-creating-assessing-computational-thinking-and-problem-solving-in-visual-programming-domains-ahana-ghosh-et-al-2024>(5/5 | 271/350) Analyzing-Evaluating-Creating: Assessing Computational Thinking and Problem Solving in Visual Programming Domains (Ahana Ghosh et al., 2024)</a></li></ul></li><li><a href=#eessiv-13>eess.IV (13)</a><ul><li><a href=#113--272350-hypervq-mlr-based-vector-quantization-in-hyperbolic-space-nabarun-goswami-et-al-2024>(1/13 | 272/350) HyperVQ: MLR-based Vector Quantization in Hyperbolic Space (Nabarun Goswami et al., 2024)</a></li><li><a href=#213--273350-mlvicx-multi-level-variance-covariance-exploration-for-chest-x-ray-self-supervised-representation-learning-azad-singh-et-al-2024>(2/13 | 273/350) MLVICX: Multi-Level Variance-Covariance Exploration for Chest X-ray Self-Supervised Representation Learning (Azad Singh et al., 2024)</a></li><li><a href=#313--274350-oucopula-bi-channel-multi-label-copula-enhanced-adapter-based-cnn-for-myopia-screening-based-on-ou-uwf-images-yang-li-et-al-2024>(3/13 | 274/350) OUCopula: Bi-Channel Multi-Label Copula-Enhanced Adapter-Based CNN for Myopia Screening Based on OU-UWF Images (Yang Li et al., 2024)</a></li><li><a href=#413--275350-paon-a-new-neuron-model-using-padé-approximants-onur-keleş-et-al-2024>(4/13 | 275/350) PAON: A New Neuron Model using Padé Approximants (Onur Keleş et al., 2024)</a></li><li><a href=#513--276350-morestyle-relax-low-frequency-constraint-of-fourier-based-image-reconstruction-in-generalizable-medical-image-segmentation-haoyu-zhao-et-al-2024>(5/13 | 276/350) MoreStyle: Relax Low-frequency Constraint of Fourier-based Image Reconstruction in Generalizable Medical Image Segmentation (Haoyu Zhao et al., 2024)</a></li><li><a href=#613--277350-estimation-and-analysis-of-slice-propagation-uncertainty-in-3d-anatomy-segmentation-rachaell-nihalaani-et-al-2024>(6/13 | 277/350) Estimation and Analysis of Slice Propagation Uncertainty in 3D Anatomy Segmentation (Rachaell Nihalaani et al., 2024)</a></li><li><a href=#713--278350-wia-ld2nd-wavelet-based-image-alignment-for-self-supervised-low-dose-ct-denoising-haoyu-zhao-et-al-2024>(7/13 | 278/350) WIA-LD2ND: Wavelet-based Image Alignment for Self-supervised Low-Dose CT Denoising (Haoyu Zhao et al., 2024)</a></li><li><a href=#813--279350-covid-19-detection-from-ct-scans-using-efficientnet-and-attention-mechanism-ramy-farag-et-al-2024>(8/13 | 279/350) Covid-19 detection from CT scans using EfficientNet and Attention mechanism (Ramy Farag et al., 2024)</a></li><li><a href=#913--280350-denoisplit-a-method-for-joint-image-splitting-and-unsupervised-denoising-ashesh-ashesh-et-al-2024>(9/13 | 280/350) denoiSplit: a method for joint image splitting and unsupervised denoising (Ashesh Ashesh et al., 2024)</a></li><li><a href=#1013--281350-hierarchical-frequency-based-upsampling-and-refining-for-compressed-video-quality-enhancement-qianyu-zhang-et-al-2024>(10/13 | 281/350) Hierarchical Frequency-based Upsampling and Refining for Compressed Video Quality Enhancement (Qianyu Zhang et al., 2024)</a></li><li><a href=#1113--282350-domain-adaptation-using-pseudo-labels-for-covid-19-detection-runtian-yuan-et-al-2024>(11/13 | 282/350) Domain Adaptation Using Pseudo Labels for COVID-19 Detection (Runtian Yuan et al., 2024)</a></li><li><a href=#1213--283350-divide-and-conquer-posterior-sampling-for-denoising-diffusion-priors-yazid-janati-et-al-2024>(12/13 | 283/350) Divide-and-Conquer Posterior Sampling for Denoising Diffusion Priors (Yazid Janati et al., 2024)</a></li><li><a href=#1313--284350-generalizing-deep-learning-models-for-medical-image-classification-matta-sarah-et-al-2024>(13/13 | 284/350) Generalizing deep learning models for medical image classification (Matta Sarah et al., 2024)</a></li></ul></li><li><a href=#eesssy-9>eess.SY (9)</a><ul><li><a href=#19--285350-context-aware-llm-based-safe-control-against-latent-risks-quan-khanh-luu-et-al-2024>(1/9 | 285/350) Context-aware LLM-based Safe Control Against Latent Risks (Quan Khanh Luu et al., 2024)</a></li><li><a href=#29--286350-distill2explain-differentiable-decision-trees-for-explainable-reinforcement-learning-in-energy-application-controllers-gargya-gokhale-et-al-2024>(2/9 | 286/350) Distill2Explain: Differentiable decision trees for explainable reinforcement learning in energy application controllers (Gargya Gokhale et al., 2024)</a></li><li><a href=#39--287350-the-role-of-extended-horizon-methodology-in-renewable-dense-grids-with-inter-day-long-duration-energy-storage-amogh-a-thatte-et-al-2024>(3/9 | 287/350) The Role of Extended Horizon Methodology in Renewable-Dense Grids With Inter-Day Long-Duration Energy Storage (Amogh A. Thatte et al., 2024)</a></li><li><a href=#49--288350-decomposing-control-lyapunov-functions-for-efficient-reinforcement-learning-antonio-lopez-et-al-2024>(4/9 | 288/350) Decomposing Control Lyapunov Functions for Efficient Reinforcement Learning (Antonio Lopez et al., 2024)</a></li><li><a href=#59--289350-explainable-reinforcement-learning-based-home-energy-management-systems-using-differentiable-decision-trees-gargya-gokhale-et-al-2024>(5/9 | 289/350) Explainable Reinforcement Learning-based Home Energy Management Systems using Differentiable Decision Trees (Gargya Gokhale et al., 2024)</a></li><li><a href=#69--290350-state-space-representations-of-the-roesser-type-for-convolutional-layers-patricia-pauli-et-al-2024>(6/9 | 290/350) State space representations of the Roesser type for convolutional layers (Patricia Pauli et al., 2024)</a></li><li><a href=#79--291350-symmetry-based-abstraction-algorithm-for-accelerating-symbolic-control-synthesis-hussein-sibai-et-al-2024>(7/9 | 291/350) Symmetry-based Abstraction Algorithm for Accelerating Symbolic Control Synthesis (Hussein Sibai et al., 2024)</a></li><li><a href=#89--292350-topology-data-analysis-based-error-detection-for-semantic-image-transmission-with-incremental-knowledge-based-harq-fei-ni-et-al-2024>(8/9 | 292/350) Topology Data Analysis-based Error Detection for Semantic Image Transmission with Incremental Knowledge-based HARQ (Fei Ni et al., 2024)</a></li><li><a href=#99--293350-secure-synchronization-of-heterogeneous-pulse-coupled-oscillators-jiaqi-yan-et-al-2024>(9/9 | 293/350) Secure Synchronization of Heterogeneous Pulse-Coupled Oscillators (Jiaqi Yan et al., 2024)</a></li></ul></li><li><a href=#csar-1>cs.AR (1)</a><ul><li><a href=#11--294350-hdldebugger-streamlining-hdl-debugging-with-large-language-models-xufeng-yao-et-al-2024>(1/1 | 294/350) HDLdebugger: Streamlining HDL debugging with Large Language Models (Xufeng Yao et al., 2024)</a></li></ul></li><li><a href=#csdc-4>cs.DC (4)</a><ul><li><a href=#14--295350-fair-distributed-cooperative-bandit-learning-on-networks-for-intelligent-internet-of-things-systems-technical-report-ziqun-chen-et-al-2024>(1/4 | 295/350) Fair Distributed Cooperative Bandit Learning on Networks for Intelligent Internet of Things Systems (Technical Report) (Ziqun Chen et al., 2024)</a></li><li><a href=#24--296350-fastdecode-high-throughput-gpu-efficient-llm-serving-using-heterogeneous-pipelines-jiaao-he-et-al-2024>(2/4 | 296/350) FastDecode: High-Throughput GPU-Efficient LLM Serving using Heterogeneous Pipelines (Jiaao He et al., 2024)</a></li><li><a href=#34--297350-amrex-and-pyamrex-looking-beyond-ecp-andrew-myers-et-al-2024>(3/4 | 297/350) AMReX and pyAMReX: Looking Beyond ECP (Andrew Myers et al., 2024)</a></li><li><a href=#44--298350-berger-byzantine-robust-geometric-routing-brown-zaz-et-al-2024>(4/4 | 298/350) BeRGeR: Byzantine-Robust Geometric Routing (Brown Zaz et al., 2024)</a></li></ul></li><li><a href=#physicsdata-an-1>physics.data-an (1)</a><ul><li><a href=#11--299350-nugraph2-a-graph-neural-network-for-neutrino-physics-event-reconstruction-v-hewes-et-al-2024>(1/1 | 299/350) NuGraph2: A Graph Neural Network for Neutrino Physics Event Reconstruction (V Hewes et al., 2024)</a></li></ul></li><li><a href=#statml-5>stat.ML (5)</a><ul><li><a href=#15--300350-pessimistic-causal-reinforcement-learning-with-mediators-for-confounded-offline-data-danyang-wang-et-al-2024>(1/5 | 300/350) Pessimistic Causal Reinforcement Learning with Mediators for Confounded Offline Data (Danyang Wang et al., 2024)</a></li><li><a href=#25--301350-posterior-uncertainty-quantification-in-neural-networks-using-data-augmentation-luhuan-wu-et-al-2024>(2/5 | 301/350) Posterior Uncertainty Quantification in Neural Networks using Data Augmentation (Luhuan Wu et al., 2024)</a></li><li><a href=#35--302350-out-of-distribution-detection-should-use-conformal-prediction-and-vice-versa-paul-novello-et-al-2024>(3/5 | 302/350) Out-of-Distribution Detection Should Use Conformal Prediction (and Vice-versa?) (Paul Novello et al., 2024)</a></li><li><a href=#45--303350-nonsmooth-implicit-differentiation-deterministic-and-stochastic-convergence-rates-riccardo-grazzi-et-al-2024>(4/5 | 303/350) Nonsmooth Implicit Differentiation: Deterministic and Stochastic Convergence Rates (Riccardo Grazzi et al., 2024)</a></li><li><a href=#55--304350-variational-approach-for-efficient-kl-divergence-estimation-in-dirichlet-mixture-models-samyajoy-pal-et-al-2024>(5/5 | 304/350) Variational Approach for Efficient KL Divergence Estimation in Dirichlet Mixture Models (Samyajoy Pal et al., 2024)</a></li></ul></li><li><a href=#csir-1>cs.IR (1)</a><ul><li><a href=#11--305350-dual-channel-multiplex-graph-neural-networks-for-recommendation-xiang-li-et-al-2024>(1/1 | 305/350) Dual-Channel Multiplex Graph Neural Networks for Recommendation (Xiang Li et al., 2024)</a></li></ul></li><li><a href=#hep-ph-1>hep-ph (1)</a><ul><li><a href=#11--306350-capslorentznet-integrating-physics-inspired-features-with-graph-convolution-rameswar-sahu-2024>(1/1 | 306/350) CapsLorentzNet: Integrating Physics Inspired Features with Graph Convolution (Rameswar Sahu, 2024)</a></li></ul></li><li><a href=#q-biocb-1>q-bio.CB (1)</a><ul><li><a href=#11--307350-transfer-learning-for-t-cell-response-prediction-josua-stadelmaier-et-al-2024>(1/1 | 307/350) Transfer Learning for T-Cell Response Prediction (Josua Stadelmaier et al., 2024)</a></li></ul></li><li><a href=#cssd-6>cs.SD (6)</a><ul><li><a href=#16--308350-unimodal-multi-task-fusion-for-emotional-mimicry-prediciton-tobias-hallmen-et-al-2024>(1/6 | 308/350) Unimodal Multi-Task Fusion for Emotional Mimicry Prediciton (Tobias Hallmen et al., 2024)</a></li><li><a href=#26--309350-prompt-singer-controllable-singing-voice-synthesis-with-natural-language-prompt-yongqi-wang-et-al-2024>(2/6 | 309/350) Prompt-Singer: Controllable Singing-Voice-Synthesis with Natural Language Prompt (Yongqi Wang et al., 2024)</a></li><li><a href=#36--310350-generalized-multi-source-inference-for-text-conditioned-music-diffusion-models-emilian-postolache-et-al-2024>(3/6 | 310/350) Generalized Multi-Source Inference for Text Conditioned Music Diffusion Models (Emilian Postolache et al., 2024)</a></li><li><a href=#46--311350-notochord-a-flexible-probabilistic-model-for-real-time-midi-performance-victor-shepardson-et-al-2024>(4/6 | 311/350) Notochord: a Flexible Probabilistic Model for Real-Time MIDI Performance (Victor Shepardson et al., 2024)</a></li><li><a href=#56--312350-sound-event-detection-and-localization-with-distance-estimation-daniel-aleksander-krause-et-al-2024>(5/6 | 312/350) Sound Event Detection and Localization with Distance Estimation (Daniel Aleksander Krause et al., 2024)</a></li><li><a href=#66--313350-towards-the-development-of-a-real-time-deepfake-audio-detection-system-in-communication-platforms-jonat-john-mathew-et-al-2024>(6/6 | 313/350) Towards the Development of a Real-Time Deepfake Audio Detection System in Communication Platforms (Jonat John Mathew et al., 2024)</a></li></ul></li><li><a href=#csit-6>cs.IT (6)</a><ul><li><a href=#16--314350-ris-aided-single-frequency-3d-imaging-by-exploiting-multi-view-image-correlations-yixuan-huang-et-al-2024>(1/6 | 314/350) RIS-aided Single-frequency 3D Imaging by Exploiting Multi-view Image Correlations (Yixuan Huang et al., 2024)</a></li><li><a href=#26--315350-comparative-analysis-of-sub-band-allocation-algorithms-in-in-body-sub-networks-supporting-xr-applications-saeed-bagherinejad-et-al-2024>(2/6 | 315/350) Comparative Analysis of Sub-band Allocation Algorithms in In-body Sub-networks Supporting XR Applications (Saeed Bagherinejad et al., 2024)</a></li><li><a href=#36--316350-full-duplex-mu-mimo-systems-with-coarse-quantization-how-many-bits-do-we-need-seunghyeong-yoo-et-al-2024>(3/6 | 316/350) Full-Duplex MU-MIMO Systems with Coarse Quantization: How Many Bits Do We Need? (Seunghyeong Yoo et al., 2024)</a></li><li><a href=#46--317350-sensing-enhanced-channel-estimation-for-near-field-xl-mimo-systems-shicong-liu-et-al-2024>(4/6 | 317/350) Sensing-Enhanced Channel Estimation for Near-Field XL-MIMO Systems (Shicong Liu et al., 2024)</a></li><li><a href=#56--318350-beamforming-design-for-semantic-bit-coexisting-communication-system-maojun-zhang-et-al-2024>(5/6 | 318/350) Beamforming Design for Semantic-Bit Coexisting Communication System (Maojun Zhang et al., 2024)</a></li><li><a href=#66--319350-towards-a-theory-of-pragmatic-information-edward-d-weinberger-2024>(6/6 | 319/350) Towards a Theory of Pragmatic Information (Edward D. Weinberger, 2024)</a></li></ul></li><li><a href=#cshc-5>cs.HC (5)</a><ul><li><a href=#15--320350-simulating-wearable-urban-augmented-reality-experiences-in-vr-lessons-learnt-from-designing-two-future-urban-interfaces-tram-thi-minh-tran-et-al-2024>(1/5 | 320/350) Simulating Wearable Urban Augmented Reality Experiences in VR: Lessons Learnt from Designing Two Future Urban Interfaces (Tram Thi Minh Tran et al., 2024)</a></li><li><a href=#25--321350-the-value-benefits-and-concerns-of-generative-ai-powered-assistance-in-writing-zhuoyan-li-et-al-2024>(2/5 | 321/350) The Value, Benefits, and Concerns of Generative AI-Powered Assistance in Writing (Zhuoyan Li et al., 2024)</a></li><li><a href=#35--322350-explainable-agency-human-preferences-for-simple-or-complex-explanations-michelle-blom-et-al-2024>(3/5 | 322/350) Explainable agency: human preferences for simple or complex explanations (Michelle Blom et al., 2024)</a></li><li><a href=#45--323350-a-systematic-review-of-xr-based-remote-human-robot-interaction-systems-xian-wang-et-al-2024>(4/5 | 323/350) A Systematic Review of XR-based Remote Human-Robot Interaction Systems (Xian Wang et al., 2024)</a></li><li><a href=#55--324350-a-review-of-virtual-reality-studies-on-autonomous-vehicle--pedestrian-interaction-tram-thi-minh-tran-et-al-2024>(5/5 | 324/350) A Review of Virtual Reality Studies on Autonomous Vehicle&ndash;Pedestrian Interaction (Tram Thi Minh Tran et al., 2024)</a></li></ul></li><li><a href=#eesssp-1>eess.SP (1)</a><ul><li><a href=#11--325350-adaptive-lpd-radar-waveform-design-with-generative-deep-learning-matthew-r-ziemann-et-al-2024>(1/1 | 325/350) Adaptive LPD Radar Waveform Design with Generative Deep Learning (Matthew R. Ziemann et al., 2024)</a></li></ul></li><li><a href=#mathna-4>math.NA (4)</a><ul><li><a href=#14--326350-on-the-convergence-of-a-data-driven-regularized-stochastic-gradient-descent-for-nonlinear-ill-posed-problems-zehui-zhou-2024>(1/4 | 326/350) On the Convergence of A Data-Driven Regularized Stochastic Gradient Descent for Nonlinear Ill-Posed Problems (Zehui Zhou, 2024)</a></li><li><a href=#24--327350-a-physics-informed-neural-network-method-for-the-approximation-of-slow-invariant-manifolds-for-the-general-class-of-stiff-systems-of-odes-dimitrios-g-patsatzis-et-al-2024>(2/4 | 327/350) A physics-informed neural network method for the approximation of slow invariant manifolds for the general class of stiff systems of ODEs (Dimitrios G. Patsatzis et al., 2024)</a></li><li><a href=#34--328350-a-restricted-additive-smoother-for-finite-cell-flow-problems-s-saberi-et-al-2024>(3/4 | 328/350) A restricted additive smoother for finite cell flow problems (S. Saberi et al., 2024)</a></li><li><a href=#44--329350-data-driven-stabilization-of-nitsches-method-s-saberi-et-al-2024>(4/4 | 329/350) Data-driven Stabilization of Nitsche&rsquo;s Method (S. Saberi et al., 2024)</a></li></ul></li><li><a href=#cond-matstr-el-1>cond-mat.str-el (1)</a><ul><li><a href=#11--330350-coarsening-of-chiral-domains-in-itinerant-electron-magnets-a-machine-learning-force-field-approach-yunhao-fan-et-al-2024>(1/1 | 330/350) Coarsening of chiral domains in itinerant electron magnets: A machine learning force field approach (Yunhao Fan et al., 2024)</a></li></ul></li><li><a href=#csgr-2>cs.GR (2)</a><ul><li><a href=#12--331350-qean-quaternion-enhanced-attention-network-for-visual-dance-generation-zhizhen-zhou-et-al-2024>(1/2 | 331/350) QEAN: Quaternion-Enhanced Attention Network for Visual Dance Generation (Zhizhen Zhou et al., 2024)</a></li><li><a href=#22--332350-bridging-3d-gaussian-and-mesh-for-freeview-video-rendering-yuting-xiao-et-al-2024>(2/2 | 332/350) Bridging 3D Gaussian and Mesh for Freeview Video Rendering (Yuting Xiao et al., 2024)</a></li></ul></li><li><a href=#csgt-1>cs.GT (1)</a><ul><li><a href=#11--333350-information-compression-in-dynamic-information-disclosure-games-dengwang-tang-et-al-2024>(1/1 | 333/350) Information Compression in Dynamic Information Disclosure Games (Dengwang Tang et al., 2024)</a></li></ul></li><li><a href=#mathco-2>math.CO (2)</a><ul><li><a href=#12--334350-packit-gamified-rectangle-packing-thomas-garrison-et-al-2024>(1/2 | 334/350) PackIt! Gamified Rectangle Packing (Thomas Garrison et al., 2024)</a></li><li><a href=#22--335350-the-real-tropical-geometry-of-neural-networks-marie-charlotte-brandenburg-et-al-2024>(2/2 | 335/350) The Real Tropical Geometry of Neural Networks (Marie-Charlotte Brandenburg et al., 2024)</a></li></ul></li><li><a href=#csos-1>cs.OS (1)</a><ul><li><a href=#11--336350-llm-as-a-system-service-on-mobile-devices-wangsong-yin-et-al-2024>(1/1 | 336/350) LLM as a System Service on Mobile Devices (Wangsong Yin et al., 2024)</a></li></ul></li><li><a href=#csse-3>cs.SE (3)</a><ul><li><a href=#13--337350-empowerability-a-portal-for-employment--scholarships-for-differently-abled-himanshu-raj-et-al-2024>(1/3 | 337/350) EmpowerAbility: A portal for employment & scholarships for differently-abled (Himanshu Raj et al., 2024)</a></li><li><a href=#23--338350-on-the-integration-of-spectrum-based-fault-localization-tools-into-ides-attila-szatmári-et-al-2024>(2/3 | 338/350) On the Integration of Spectrum-Based Fault Localization Tools into IDEs (Attila Szatmári et al., 2024)</a></li><li><a href=#33--339350-empirical-analysis-on-cicd-pipeline-evolution-in-machine-learning-projects-alaa-houerbi-et-al-2024>(3/3 | 339/350) Empirical Analysis on CI/CD Pipeline Evolution in Machine Learning Projects (Alaa Houerbi et al., 2024)</a></li></ul></li><li><a href=#mathoc-2>math.OC (2)</a><ul><li><a href=#12--340350-convex-co-design-of-control-barrier-function-and-safe-feedback-controller-under-input-constraints-han-wang-et-al-2024>(1/2 | 340/350) Convex Co-Design of Control Barrier Function and Safe Feedback Controller Under Input Constraints (Han Wang et al., 2024)</a></li><li><a href=#22--341350-a-first-order-gradient-approach-for-the-connectivity-analysis-of-weighted-graphs-christian-p-c-franssen-et-al-2024>(2/2 | 341/350) A First-Order Gradient Approach for the Connectivity Analysis of Weighted Graphs (Christian P. C. Franssen et al., 2024)</a></li></ul></li><li><a href=#hep-th-1>hep-th (1)</a><ul><li><a href=#11--342350-neural-network-representation-of-quantum-systems-koji-hashimoto-et-al-2024>(1/1 | 342/350) Neural network representation of quantum systems (Koji Hashimoto et al., 2024)</a></li></ul></li><li><a href=#csdb-1>cs.DB (1)</a><ul><li><a href=#11--343350-benchmarking-analytical-query-processing-in-intel-sgxv2-adrian-lutsch-et-al-2024>(1/1 | 343/350) Benchmarking Analytical Query Processing in Intel SGXv2 (Adrian Lutsch et al., 2024)</a></li></ul></li><li><a href=#csmm-1>cs.MM (1)</a><ul><li><a href=#11--344350-virbo-multimodal-multilingual-avatar-video-generation-in-digital-marketing-juan-zhang-et-al-2024>(1/1 | 344/350) Virbo: Multimodal Multilingual Avatar Video Generation in Digital Marketing (Juan Zhang et al., 2024)</a></li></ul></li><li><a href=#csce-1>cs.CE (1)</a><ul><li><a href=#11--345350-vehicle-single-track-modeling-using-physics-guided-neural-differential-equations-stephan-rhode-et-al-2024>(1/1 | 345/350) Vehicle single track modeling using physics guided neural differential equations (Stephan Rhode et al., 2024)</a></li></ul></li><li><a href=#csni-1>cs.NI (1)</a><ul><li><a href=#11--346350-edge-disjoint-spanning-trees-on-star-product-networks-aleyah-dawkins-et-al-2024>(1/1 | 346/350) Edge-Disjoint Spanning Trees on Star-Product Networks (Aleyah Dawkins et al., 2024)</a></li></ul></li><li><a href=#mathat-1>math.AT (1)</a><ul><li><a href=#11--347350-probabilistic-analysis-of-multiparameter-persistence-decompositions-ángel-javier-alonso-et-al-2024>(1/1 | 347/350) Probabilistic Analysis of Multiparameter Persistence Decompositions (Ángel Javier Alonso et al., 2024)</a></li></ul></li><li><a href=#cspl-1>cs.PL (1)</a><ul><li><a href=#11--348350-looper-a-learned-automatic-code-optimizer-for-polyhedral-compilers-massinissa-merouani-et-al-2024>(1/1 | 348/350) LOOPer: A Learned Automatic Code Optimizer For Polyhedral Compilers (Massinissa Merouani et al., 2024)</a></li></ul></li><li><a href=#q-bionc-1>q-bio.NC (1)</a><ul><li><a href=#11--349350-towards-understanding-the-nature-of-direct-functional-connectivity-in-visual-brain-network-debanjali-bhattacharya-et-al-2024>(1/1 | 349/350) Towards understanding the nature of direct functional connectivity in visual brain network (Debanjali Bhattacharya et al., 2024)</a></li></ul></li><li><a href=#csds-1>cs.DS (1)</a><ul><li><a href=#11--350350-eth-tight-algorithm-for-cycle-packing-on-unit-disk-graphs-shinwoo-an-et-al-2024>(1/1 | 350/350) ETH-Tight Algorithm for Cycle Packing on Unit Disk Graphs (Shinwoo An et al., 2024)</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:contact.to.akitenkrad@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>contact.to.akitenkrad@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/akitenkrad-blog/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2020 Akitenkrad.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/akitenkrad-blog/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/jquery-3.4.1.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/popper.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/bootstrap.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/navbar.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/plyr.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=https://akitenkrad.github.io/akitenkrad-blog/js/single.js></script><script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.css><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.js></script><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!0})})</script></body></html>