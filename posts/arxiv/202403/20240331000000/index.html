<!doctype html><html><head><title>arXiv @ 2024.03.31</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/bootstrap.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/main.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/navbar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/plyr.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/flag-icon.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/custom/style.css><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM crossorigin=anonymous><script async src=https://cdn.jsdelivr.net/npm/es-module-shims@1/dist/es-module-shims.min.js crossorigin=anonymous></script><script type=importmap>
{
  "imports": {
    "@popperjs/core": "https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.8/dist/esm/popper.min.js",
    "bootstrap": "https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.esm.min.js"
  }
}
</script><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js integrity=sha384-geWF76RCwLtnZ8qwWowPQNguL3RmwHVBC9FhGdlKrxdiJJigb/j/68SIy3Te4Bkz crossorigin=anonymous></script><script type=module src=https://akitenkrad.github.io/akitenkrad-blog/js/custom/custom.js></script><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/fontawesome/css/all.min.css><link rel=icon type=image/png href=https://akitenkrad.github.io/akitenkrad-blog/images/favicons/favicon-96x96_huf1ee13f0caf27d1547f91fb46207d708_13005_42x0_resize_box_3.png><meta property="og:title" content="arXiv @ 2024.03.31"><meta property="og:description" content="Primary Categories cs.AI (12) cs.AR (3) cs.CE (1) cs.CL (42) cs.CR (3) cs.CV (68) cs.CY (1) cs.DB (3) cs.DC (1) cs.DS (2) cs.IR (5) cs.IT (6) cs.LG (18) cs.MM (1) cs.NE (1) cs.NI (4) cs.RO (10) cs.SD (1) cs.SE (1) eess.AS (1) eess.IV (7) eess.SP (1) eess.SY (2) math-ph (1) math.NA (2) math.OC (3) q-bio.BM (2) stat.CO (1) stat.ML (1) Keywords keyword cs.AI cs.CL cs.CV cs.LG cs.RO Adversarial Attack 3 Adversarial Learning 1 Anomaly Detection 2 Autoencoder 4 Automatic Evaluation 1 Automatic Speech Recognition 2 BART 1 BERT 2 Benchmarking 18 24 8 Black Box 1 2 ChatGPT 4 Chatbot 1 Clustering 1 Code Generation 1 Continual Learning 4 Continuous Graph 1 Continuous Time 2 Contrastive Learning 2 1 Convolution 9 3 Convolutional Neural Network 1 9 2 Counter-factual 2 1 Curriculum Learning 1 Data Augmentation 1 2 Dialogue System 1 Diffusion Model 6 Discrete Time 2 Distribution Shift 2 Emotion Recognition 1 Explainable AI 1 Face Recognition 1 Fact Verification 1 Fairness 1 2 Fake News Detection 1 1 Federated Learning 1 Few-shot 4 2 1 Fine-tuning 1 13 7 1 Foundation Model 1 6 GLUE 1 GPT 10 1 GPT-2 1 GPT-3 3 GPT-3."><meta property="og:type" content="article"><meta property="og:url" content="https://akitenkrad.github.io/akitenkrad-blog/posts/arxiv/202403/20240331000000/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-03-31T00:00:00+00:00"><meta property="article:modified_time" content="2024-03-31T00:00:00+00:00"><meta name=description content="arXiv @ 2024.03.31"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/single.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/sidebar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/style.css><script type=text/javascript src=//ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js></script><script language=JavaScript>$(document).ready(function(){$("a[href^='http']:not([href*='"+location.hostname+"'])").attr("target","_blank")})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-1MYYZQG0WE"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-1MYYZQG0WE",{anonymize_ip:!1})}</script></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow" style=position:fixed><div class=container><button class="navbar-toggler navbar-light navbar-for-sidebar" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span>
</button>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav mr-auto"><li class=nav-item><a class=navbar-brand href=/akitenkrad-blog><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png alt=Logo>
Akitenkrad's Blog</a></li></ul><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=/akitenkrad-blog#home>Home</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#about>About</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#recent-posts>Recent Posts</a></li><li class=nav-item><a class=nav-link id=blog-link href=https://akitenkrad.github.io/akitenkrad-blog/posts>Posts</a></li><li class=nav-item><a class=nav-link id=tag-link href=https://akitenkrad.github.io/akitenkrad-blog/tags>Tags</a></li></ul></div></div><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/akitenkrad-blog/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/>Papers</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202205/>2022.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202205/20220518224923/ title="A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks">A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220508162318/ title="A Deep Cascade Model for Multi-Document Reading Comprehension">A Deep Cascade Model for Multi-Document Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220506021208/ title="A Primer in BERTology: What We Know About How BERT Works">A Primer in BERTology: What We Know About How BERT Works</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220529131339/ title="Attention Is All You Need">Attention Is All You Need</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220503010000/ title="DeBERTa: Decoding-Enhanced BERT with Disentangled Attention">DeBERTa: Decoding-Enhanced BERT with Disentangled Attention</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220505222900/ title="Dense Passage Retrieval for Open-Domain Question Answering">Dense Passage Retrieval for Open-Domain Question Answering</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220511010217/ title="Multi-Style Generative Reading Comprehension">Multi-Style Generative Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220530102936/ title="Neural Machine Translation of Rare Words with Subword Units">Neural Machine Translation of Rare Words with Subword Units</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220523223206/ title="RoBERTa: A Robustly Optimized BERT Pretraining Approach">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220520124748/ title="Semi-Supervised Classification with Graph Convolutional Networks">Semi-Supervised Classification with Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220509110738/ title="Survey on graph embeddings and their applications to machine learning problems on graphs">Survey on graph embeddings and their applications to machine learning problems on graphs</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220514151839/ title="UnitedQA: A Hybrid Approach for Open Domain Question Answering">UnitedQA: A Hybrid Approach for Open Domain Question Answering</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202206/>2022.06</a><ul><li><a href=/akitenkrad-blog/posts/papers/202206/20220612105422/ title="Attributed Network Embedding for Learning in a Dynamic Environment">Attributed Network Embedding for Learning in a Dynamic Environment</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220608085622/ title="CodeBERT: A Pre-Trained Model for Programming and Natural Languages">CodeBERT: A Pre-Trained Model for Programming and Natural Languages</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220618223844/ title="High-order Proximity Preserved Embedding for Dynamic Networks">High-order Proximity Preserved Embedding for Dynamic Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220602171700/ title="S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension">S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202207/>2022.07</a><ul><li><a href=/akitenkrad-blog/posts/papers/202207/20220727145036/ title="Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions">Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions</a></li><li><a href=/akitenkrad-blog/posts/papers/202207/20220726163444/ title="Dynamic Network Embedding Survey">Dynamic Network Embedding Survey</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/papers/202208/20220802103319/ title="Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN">Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/papers/202209/20220909180827/ title="Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting">Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202305/>2023.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202305/20230513094804/ title="Improving Language Understanding by Generative Pre-Training">Improving Language Understanding by Generative Pre-Training</a></li><li><a href=/akitenkrad-blog/posts/papers/202305/20230514200241/ title="Language Models are Unsupervised Multitask Learners">Language Models are Unsupervised Multitask Learners</a></li></ul></li><li><a href=/akitenkrad-blog/posts/papers/202306/ title=2023.06>2023.06</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/>arXiv</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202307/>2023.07</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230701000000/ title="arXiv @ 2023.07.01">arXiv @ 2023.07.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230702000000/ title="arXiv @ 2023.07.02">arXiv @ 2023.07.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230703000000/ title="arXiv @ 2023.07.03">arXiv @ 2023.07.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230704000000/ title="arXiv @ 2023.07.04">arXiv @ 2023.07.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230705000000/ title="arXiv @ 2023.07.05">arXiv @ 2023.07.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230706000000/ title="arXiv @ 2023.07.06">arXiv @ 2023.07.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230707000000/ title="arXiv @ 2023.07.07">arXiv @ 2023.07.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230708000000/ title="arXiv @ 2023.07.08">arXiv @ 2023.07.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230709000000/ title="arXiv @ 2023.07.09">arXiv @ 2023.07.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230710000000/ title="arXiv @ 2023.07.10">arXiv @ 2023.07.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230711000000/ title="arXiv @ 2023.07.11">arXiv @ 2023.07.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230712000000/ title="arXiv @ 2023.07.12">arXiv @ 2023.07.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230713000000/ title="arXiv @ 2023.07.13">arXiv @ 2023.07.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230714000000/ title="arXiv @ 2023.07.14">arXiv @ 2023.07.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230715000000/ title="arXiv @ 2023.07.15">arXiv @ 2023.07.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230716000000/ title="arXiv @ 2023.07.16">arXiv @ 2023.07.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230717000000/ title="arXiv @ 2023.07.17">arXiv @ 2023.07.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230718000000/ title="arXiv @ 2023.07.18">arXiv @ 2023.07.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230719000000/ title="arXiv @ 2023.07.19">arXiv @ 2023.07.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230720000000/ title="arXiv @ 2023.07.20">arXiv @ 2023.07.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230721000000/ title="arXiv @ 2023.07.21">arXiv @ 2023.07.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230722000000/ title="arXiv @ 2023.07.22">arXiv @ 2023.07.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230723000000/ title="arXiv @ 2023.07.23">arXiv @ 2023.07.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230724000000/ title="arXiv @ 2023.07.24">arXiv @ 2023.07.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230725000000/ title="arXiv @ 2023.07.25">arXiv @ 2023.07.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230726000000/ title="arXiv @ 2023.07.26">arXiv @ 2023.07.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230727000000/ title="arXiv @ 2023.07.27">arXiv @ 2023.07.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230728000000/ title="arXiv @ 2023.07.28">arXiv @ 2023.07.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230729000000/ title="arXiv @ 2023.07.29">arXiv @ 2023.07.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230730000000/ title="arXiv @ 2023.07.30">arXiv @ 2023.07.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230731000000/ title="arXiv @ 2023.07.31">arXiv @ 2023.07.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202308/>2023.08</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230801000000/ title="arXiv @ 2023.08.01">arXiv @ 2023.08.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230802000000/ title="arXiv @ 2023.08.02">arXiv @ 2023.08.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230803000000/ title="arXiv @ 2023.08.03">arXiv @ 2023.08.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230804000000/ title="arXiv @ 2023.08.04">arXiv @ 2023.08.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230805000000/ title="arXiv @ 2023.08.05">arXiv @ 2023.08.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230806000000/ title="arXiv @ 2023.08.06">arXiv @ 2023.08.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230807000000/ title="arXiv @ 2023.08.07">arXiv @ 2023.08.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230808000000/ title="arXiv @ 2023.08.08">arXiv @ 2023.08.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230809000000/ title="arXiv @ 2023.08.09">arXiv @ 2023.08.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230810000000/ title="arXiv @ 2023.08.10">arXiv @ 2023.08.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230811000000/ title="arXiv @ 2023.08.11">arXiv @ 2023.08.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230812000000/ title="arXiv @ 2023.08.12">arXiv @ 2023.08.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230813000000/ title="arXiv @ 2023.08.13">arXiv @ 2023.08.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230814000000/ title="arXiv @ 2023.08.14">arXiv @ 2023.08.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230815000000/ title="arXiv @ 2023.08.15">arXiv @ 2023.08.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230816000000/ title="arXiv @ 2023.08.16">arXiv @ 2023.08.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230817000000/ title="arXiv @ 2023.08.17">arXiv @ 2023.08.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230818000000/ title="arXiv @ 2023.08.18">arXiv @ 2023.08.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230819000000/ title="arXiv @ 2023.08.19">arXiv @ 2023.08.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230820000000/ title="arXiv @ 2023.08.20">arXiv @ 2023.08.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230821000000/ title="arXiv @ 2023.08.21">arXiv @ 2023.08.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230822000000/ title="arXiv @ 2023.08.22">arXiv @ 2023.08.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230823000000/ title="arXiv @ 2023.08.23">arXiv @ 2023.08.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230824000000/ title="arXiv @ 2023.08.24">arXiv @ 2023.08.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230825000000/ title="arXiv @ 2023.08.25">arXiv @ 2023.08.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230826000000/ title="arXiv @ 2023.08.26">arXiv @ 2023.08.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230827000000/ title="arXiv @ 2023.08.27">arXiv @ 2023.08.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230828000000/ title="arXiv @ 2023.08.28">arXiv @ 2023.08.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230829000000/ title="arXiv @ 2023.08.29">arXiv @ 2023.08.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230830000000/ title="arXiv @ 2023.08.30">arXiv @ 2023.08.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230831000000/ title="arXiv @ 2023.08.31">arXiv @ 2023.08.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202309/>2023.09</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230901000000/ title="arXiv @ 2023.09.01">arXiv @ 2023.09.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230902000000/ title="arXiv @ 2023.09.02">arXiv @ 2023.09.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230903000000/ title="arXiv @ 2023.09.03">arXiv @ 2023.09.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230904000000/ title="arXiv @ 2023.09.04">arXiv @ 2023.09.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230905000000/ title="arXiv @ 2023.09.05">arXiv @ 2023.09.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230906000000/ title="arXiv @ 2023.09.06">arXiv @ 2023.09.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230907000000/ title="arXiv @ 2023.09.07">arXiv @ 2023.09.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230908000000/ title="arXiv @ 2023.09.08">arXiv @ 2023.09.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230909000000/ title="arXiv @ 2023.09.09">arXiv @ 2023.09.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230910000000/ title="arXiv @ 2023.09.10">arXiv @ 2023.09.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230911000000/ title="arXiv @ 2023.09.11">arXiv @ 2023.09.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230912000000/ title="arXiv @ 2023.09.12">arXiv @ 2023.09.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230913000000/ title="arXiv @ 2023.09.13">arXiv @ 2023.09.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230914000000/ title="arXiv @ 2023.09.14">arXiv @ 2023.09.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230915000000/ title="arXiv @ 2023.09.15">arXiv @ 2023.09.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230916000000/ title="arXiv @ 2023.09.16">arXiv @ 2023.09.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230917000000/ title="arXiv @ 2023.09.17">arXiv @ 2023.09.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230918000000/ title="arXiv @ 2023.09.18">arXiv @ 2023.09.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230919000000/ title="arXiv @ 2023.09.19">arXiv @ 2023.09.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230920000000/ title="arXiv @ 2023.09.20">arXiv @ 2023.09.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230921000000/ title="arXiv @ 2023.09.21">arXiv @ 2023.09.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230922000000/ title="arXiv @ 2023.09.22">arXiv @ 2023.09.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230923000000/ title="arXiv @ 2023.09.23">arXiv @ 2023.09.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230924000000/ title="arXiv @ 2023.09.24">arXiv @ 2023.09.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230925000000/ title="arXiv @ 2023.09.25">arXiv @ 2023.09.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230926000000/ title="arXiv @ 2023.09.26">arXiv @ 2023.09.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230927000000/ title="arXiv @ 2023.09.27">arXiv @ 2023.09.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230928000000/ title="arXiv @ 2023.09.28">arXiv @ 2023.09.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230929000000/ title="arXiv @ 2023.09.29">arXiv @ 2023.09.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230930000000/ title="arXiv @ 2023.09.30">arXiv @ 2023.09.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202310/>2023.1</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231001000000/ title="arXiv @ 2023.10.01">arXiv @ 2023.10.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231002000000/ title="arXiv @ 2023.10.02">arXiv @ 2023.10.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231003000000/ title="arXiv @ 2023.10.03">arXiv @ 2023.10.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231004000000/ title="arXiv @ 2023.10.04">arXiv @ 2023.10.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231005000000/ title="arXiv @ 2023.10.05">arXiv @ 2023.10.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231006000000/ title="arXiv @ 2023.10.06">arXiv @ 2023.10.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231007000000/ title="arXiv @ 2023.10.07">arXiv @ 2023.10.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231008000000/ title="arXiv @ 2023.10.08">arXiv @ 2023.10.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231009000000/ title="arXiv @ 2023.10.09">arXiv @ 2023.10.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231010000000/ title="arXiv @ 2023.10.10">arXiv @ 2023.10.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231011000000/ title="arXiv @ 2023.10.11">arXiv @ 2023.10.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231012000000/ title="arXiv @ 2023.10.12">arXiv @ 2023.10.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231013000000/ title="arXiv @ 2023.10.13">arXiv @ 2023.10.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231014000000/ title="arXiv @ 2023.10.14">arXiv @ 2023.10.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231015000000/ title="arXiv @ 2023.10.15">arXiv @ 2023.10.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231016000000/ title="arXiv @ 2023.10.16">arXiv @ 2023.10.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231017000000/ title="arXiv @ 2023.10.17">arXiv @ 2023.10.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231018000000/ title="arXiv @ 2023.10.18">arXiv @ 2023.10.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231019000000/ title="arXiv @ 2023.10.19">arXiv @ 2023.10.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231020000000/ title="arXiv @ 2023.10.20">arXiv @ 2023.10.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231021000000/ title="arXiv @ 2023.10.21">arXiv @ 2023.10.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231022000000/ title="arXiv @ 2023.10.22">arXiv @ 2023.10.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231023000000/ title="arXiv @ 2023.10.23">arXiv @ 2023.10.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231024000000/ title="arXiv @ 2023.10.24">arXiv @ 2023.10.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231025000000/ title="arXiv @ 2023.10.25">arXiv @ 2023.10.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231026000000/ title="arXiv @ 2023.10.26">arXiv @ 2023.10.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231027000000/ title="arXiv @ 2023.10.27">arXiv @ 2023.10.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231028000000/ title="arXiv @ 2023.10.28">arXiv @ 2023.10.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231029000000/ title="arXiv @ 2023.10.29">arXiv @ 2023.10.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231030000000/ title="arXiv @ 2023.10.30">arXiv @ 2023.10.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231031000000/ title="arXiv @ 2023.10.31">arXiv @ 2023.10.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202311/>2023.11</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231101000000/ title="arXiv @ 2023.11.01">arXiv @ 2023.11.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231102000000/ title="arXiv @ 2023.11.02">arXiv @ 2023.11.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231103000000/ title="arXiv @ 2023.11.03">arXiv @ 2023.11.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231104000000/ title="arXiv @ 2023.11.04">arXiv @ 2023.11.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231105000000/ title="arXiv @ 2023.11.05">arXiv @ 2023.11.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231106000000/ title="arXiv @ 2023.11.06">arXiv @ 2023.11.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231107000000/ title="arXiv @ 2023.11.07">arXiv @ 2023.11.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231108000000/ title="arXiv @ 2023.11.08">arXiv @ 2023.11.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231109000000/ title="arXiv @ 2023.11.09">arXiv @ 2023.11.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231110000000/ title="arXiv @ 2023.11.10">arXiv @ 2023.11.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231111000000/ title="arXiv @ 2023.11.11">arXiv @ 2023.11.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231112000000/ title="arXiv @ 2023.11.12">arXiv @ 2023.11.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231113000000/ title="arXiv @ 2023.11.13">arXiv @ 2023.11.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231114000000/ title="arXiv @ 2023.11.14">arXiv @ 2023.11.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231115000000/ title="arXiv @ 2023.11.15">arXiv @ 2023.11.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231116000000/ title="arXiv @ 2023.11.16">arXiv @ 2023.11.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231117000000/ title="arXiv @ 2023.11.17">arXiv @ 2023.11.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231118000000/ title="arXiv @ 2023.11.18">arXiv @ 2023.11.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231119000000/ title="arXiv @ 2023.11.19">arXiv @ 2023.11.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231120000000/ title="arXiv @ 2023.11.20">arXiv @ 2023.11.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231121000000/ title="arXiv @ 2023.11.21">arXiv @ 2023.11.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231122000000/ title="arXiv @ 2023.11.22">arXiv @ 2023.11.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231123000000/ title="arXiv @ 2023.11.23">arXiv @ 2023.11.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231124000000/ title="arXiv @ 2023.11.24">arXiv @ 2023.11.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231125000000/ title="arXiv @ 2023.11.25">arXiv @ 2023.11.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231126000000/ title="arXiv @ 2023.11.26">arXiv @ 2023.11.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231127000000/ title="arXiv @ 2023.11.27">arXiv @ 2023.11.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231128000000/ title="arXiv @ 2023.11.28">arXiv @ 2023.11.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231129000000/ title="arXiv @ 2023.11.29">arXiv @ 2023.11.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231130000000/ title="arXiv @ 2023.11.30">arXiv @ 2023.11.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202312/>2023.12</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231201000000/ title="arXiv @ 2023.12.01">arXiv @ 2023.12.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231202000000/ title="arXiv @ 2023.12.02">arXiv @ 2023.12.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231203000000/ title="arXiv @ 2023.12.03">arXiv @ 2023.12.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231204000000/ title="arXiv @ 2023.12.04">arXiv @ 2023.12.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231205000000/ title="arXiv @ 2023.12.05">arXiv @ 2023.12.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231206000000/ title="arXiv @ 2023.12.06">arXiv @ 2023.12.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231207000000/ title="arXiv @ 2023.12.07">arXiv @ 2023.12.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231208000000/ title="arXiv @ 2023.12.08">arXiv @ 2023.12.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231209000000/ title="arXiv @ 2023.12.09">arXiv @ 2023.12.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231210000000/ title="arXiv @ 2023.12.10">arXiv @ 2023.12.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231211000000/ title="arXiv @ 2023.12.11">arXiv @ 2023.12.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231212000000/ title="arXiv @ 2023.12.12">arXiv @ 2023.12.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231213000000/ title="arXiv @ 2023.12.13">arXiv @ 2023.12.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231214000000/ title="arXiv @ 2023.12.14">arXiv @ 2023.12.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231215000000/ title="arXiv @ 2023.12.15">arXiv @ 2023.12.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231216000000/ title="arXiv @ 2023.12.16">arXiv @ 2023.12.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231217000000/ title="arXiv @ 2023.12.17">arXiv @ 2023.12.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231218000000/ title="arXiv @ 2023.12.18">arXiv @ 2023.12.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231219000000/ title="arXiv @ 2023.12.19">arXiv @ 2023.12.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231220000000/ title="arXiv @ 2023.12.20">arXiv @ 2023.12.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231221000000/ title="arXiv @ 2023.12.21">arXiv @ 2023.12.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231222000000/ title="arXiv @ 2023.12.22">arXiv @ 2023.12.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231223000000/ title="arXiv @ 2023.12.23">arXiv @ 2023.12.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231224000000/ title="arXiv @ 2023.12.24">arXiv @ 2023.12.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231225000000/ title="arXiv @ 2023.12.25">arXiv @ 2023.12.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231226000000/ title="arXiv @ 2023.12.26">arXiv @ 2023.12.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231227000000/ title="arXiv @ 2023.12.27">arXiv @ 2023.12.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231228000000/ title="arXiv @ 2023.12.28">arXiv @ 2023.12.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231229000000/ title="arXiv @ 2023.12.29">arXiv @ 2023.12.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231230000000/ title="arXiv @ 2023.12.30">arXiv @ 2023.12.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231231000000/ title="arXiv @ 2023.12.31">arXiv @ 2023.12.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202401/>2024.01</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240101000000/ title="arXiv @ 2024.01.01">arXiv @ 2024.01.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240102000000/ title="arXiv @ 2024.01.02">arXiv @ 2024.01.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240103000000/ title="arXiv @ 2024.01.03">arXiv @ 2024.01.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240104000000/ title="arXiv @ 2024.01.04">arXiv @ 2024.01.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240105000000/ title="arXiv @ 2024.01.05">arXiv @ 2024.01.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240106000000/ title="arXiv @ 2024.01.06">arXiv @ 2024.01.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240107000000/ title="arXiv @ 2024.01.07">arXiv @ 2024.01.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240108000000/ title="arXiv @ 2024.01.08">arXiv @ 2024.01.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240109000000/ title="arXiv @ 2024.01.09">arXiv @ 2024.01.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240110000000/ title="arXiv @ 2024.01.10">arXiv @ 2024.01.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240111000000/ title="arXiv @ 2024.01.11">arXiv @ 2024.01.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240112000000/ title="arXiv @ 2024.01.12">arXiv @ 2024.01.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240113000000/ title="arXiv @ 2024.01.13">arXiv @ 2024.01.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240114000000/ title="arXiv @ 2024.01.14">arXiv @ 2024.01.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240115000000/ title="arXiv @ 2024.01.15">arXiv @ 2024.01.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240116000000/ title="arXiv @ 2024.01.16">arXiv @ 2024.01.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240117000000/ title="arXiv @ 2024.01.17">arXiv @ 2024.01.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240118000000/ title="arXiv @ 2024.01.18">arXiv @ 2024.01.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240119000000/ title="arXiv @ 2024.01.19">arXiv @ 2024.01.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240120000000/ title="arXiv @ 2024.01.20">arXiv @ 2024.01.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240121000000/ title="arXiv @ 2024.01.21">arXiv @ 2024.01.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240122000000/ title="arXiv @ 2024.01.22">arXiv @ 2024.01.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240123000000/ title="arXiv @ 2024.01.23">arXiv @ 2024.01.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240124000000/ title="arXiv @ 2024.01.24">arXiv @ 2024.01.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240125000000/ title="arXiv @ 2024.01.25">arXiv @ 2024.01.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240126000000/ title="arXiv @ 2024.01.26">arXiv @ 2024.01.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240127000000/ title="arXiv @ 2024.01.27">arXiv @ 2024.01.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240128000000/ title="arXiv @ 2024.01.28">arXiv @ 2024.01.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240129000000/ title="arXiv @ 2024.01.29">arXiv @ 2024.01.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240130000000/ title="arXiv @ 2024.01.30">arXiv @ 2024.01.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240131000000/ title="arXiv @ 2024.01.31">arXiv @ 2024.01.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202402/>2024.02</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240201000000/ title="arXiv @ 2024.02.01">arXiv @ 2024.02.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240202000000/ title="arXiv @ 2024.02.02">arXiv @ 2024.02.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240203000000/ title="arXiv @ 2024.02.03">arXiv @ 2024.02.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240204000000/ title="arXiv @ 2024.02.04">arXiv @ 2024.02.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240205000000/ title="arXiv @ 2024.02.05">arXiv @ 2024.02.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240206000000/ title="arXiv @ 2024.02.06">arXiv @ 2024.02.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/ title="arXiv @ 2024.02.07">arXiv @ 2024.02.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/ title="arXiv @ 2024.02.08">arXiv @ 2024.02.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/ title="arXiv @ 2024.02.09">arXiv @ 2024.02.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/ title="arXiv @ 2024.02.10">arXiv @ 2024.02.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240211000000/ title="arXiv @ 2024.02.11">arXiv @ 2024.02.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/ title="arXiv @ 2024.02.12">arXiv @ 2024.02.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/ title="arXiv @ 2024.02.13">arXiv @ 2024.02.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/ title="arXiv @ 2024.02.14">arXiv @ 2024.02.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/ title="arXiv @ 2024.02.15">arXiv @ 2024.02.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240216000000/ title="arXiv @ 2024.02.16">arXiv @ 2024.02.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/ title="arXiv @ 2024.02.17">arXiv @ 2024.02.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240218000000/ title="arXiv @ 2024.02.18">arXiv @ 2024.02.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/ title="arXiv @ 2024.02.19">arXiv @ 2024.02.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/ title="arXiv @ 2024.02.20">arXiv @ 2024.02.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/ title="arXiv @ 2024.02.21">arXiv @ 2024.02.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/ title="arXiv @ 2024.02.22">arXiv @ 2024.02.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/ title="arXiv @ 2024.02.23">arXiv @ 2024.02.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/ title="arXiv @ 2024.02.24">arXiv @ 2024.02.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/ title="arXiv @ 2024.02.25">arXiv @ 2024.02.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/ title="arXiv @ 2024.02.26">arXiv @ 2024.02.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/ title="arXiv @ 2024.02.27">arXiv @ 2024.02.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/ title="arXiv @ 2024.02.28">arXiv @ 2024.02.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/ title="arXiv @ 2024.02.29">arXiv @ 2024.02.29</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/202403/>2024.03</a><ul class=active><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/ title="arXiv @ 2024.03.01">arXiv @ 2024.03.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/ title="arXiv @ 2024.03.02">arXiv @ 2024.03.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/ title="arXiv @ 2024.03.03">arXiv @ 2024.03.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240304000000/ title="arXiv @ 2024.03.04">arXiv @ 2024.03.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/ title="arXiv @ 2024.03.05">arXiv @ 2024.03.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/ title="arXiv @ 2024.03.06">arXiv @ 2024.03.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/ title="arXiv @ 2024.03.07">arXiv @ 2024.03.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/ title="arXiv @ 2024.03.08">arXiv @ 2024.03.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/ title="arXiv @ 2024.03.09">arXiv @ 2024.03.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/ title="arXiv @ 2024.03.10">arXiv @ 2024.03.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/ title="arXiv @ 2024.03.11">arXiv @ 2024.03.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/ title="arXiv @ 2024.03.12">arXiv @ 2024.03.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/ title="arXiv @ 2024.03.13">arXiv @ 2024.03.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/ title="arXiv @ 2024.03.14">arXiv @ 2024.03.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/ title="arXiv @ 2024.03.15">arXiv @ 2024.03.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/ title="arXiv @ 2024.03.16">arXiv @ 2024.03.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/ title="arXiv @ 2024.03.17">arXiv @ 2024.03.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/ title="arXiv @ 2024.03.18">arXiv @ 2024.03.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240319000000/ title="arXiv @ 2024.03.19">arXiv @ 2024.03.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/ title="arXiv @ 2024.03.20">arXiv @ 2024.03.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/ title="arXiv @ 2024.03.21">arXiv @ 2024.03.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/ title="arXiv @ 2024.03.22">arXiv @ 2024.03.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240323000000/ title="arXiv @ 2024.03.23">arXiv @ 2024.03.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/ title="arXiv @ 2024.03.24">arXiv @ 2024.03.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240325000000/ title="arXiv @ 2024.03.25">arXiv @ 2024.03.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240326000000/ title="arXiv @ 2024.03.26">arXiv @ 2024.03.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240327000000/ title="arXiv @ 2024.03.27">arXiv @ 2024.03.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240328000000/ title="arXiv @ 2024.03.28">arXiv @ 2024.03.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240329000000/ title="arXiv @ 2024.03.29">arXiv @ 2024.03.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240330000000/ title="arXiv @ 2024.03.30">arXiv @ 2024.03.30</a></li><li><a class=active href=/akitenkrad-blog/posts/arxiv/202403/20240331000000/ title="arXiv @ 2024.03.31">arXiv @ 2024.03.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202404/>2024.04</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240401000000/ title="arXiv @ 2024.04.01">arXiv @ 2024.04.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240402000000/ title="arXiv @ 2024.04.02">arXiv @ 2024.04.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240403000000/ title="arXiv @ 2024.04.03">arXiv @ 2024.04.03</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/algorithms/>Algorithms</a><ul><li><a href=/akitenkrad-blog/posts/algorithms/bandit-algorithm-basic/ title="Bandit Algorithm Basic">Bandit Algorithm Basic</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/conference/>Conference</a><ul><li><a href=/akitenkrad-blog/posts/conference/acl/ title="Annual Meeting of the Association for Computational Linguistics">Annual Meeting of the Association for Computational Linguistics</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/>Figures</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/figures/202208/20220822092748/ title=1ヶ月当たりの実労働時間の推移>1ヶ月当たりの実労働時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220813115200/ title=コンビニエンスストアの店舗数の推移>コンビニエンスストアの店舗数の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220821113322/ title=地域別1世帯当たり1ヶ月間の支出額の推移>地域別1世帯当たり1ヶ月間の支出額の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220828111836/ title="夫婦別子供の有無別生活時間 (2016)">夫婦別子供の有無別生活時間 (2016)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220827103615/ title=家族類型の変遷>家族類型の変遷</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220815131016/ title="年齢別大学院入学者数 (2021)">年齢別大学院入学者数 (2021)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220818122628/ title=情報サービス業における企業特殊的人的資本（名目）の推移>情報サービス業における企業特殊的人的資本（名目）の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220824095127/ title=有業者の社会生活における活動の変化>有業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220814175022/ title=東京23区の乗用車保有台数>東京23区の乗用車保有台数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220817121117/ title="業種別企業特殊的人的資本 (2018)">業種別企業特殊的人的資本 (2018)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220825094111/ title=無業者の社会生活における活動の変化>無業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220823104100/ title=睡眠時間の推移>睡眠時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220826102652/ title=総人口の推移>総人口の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220820225528/ title=都道府県別国公立別学校数・学生数>都道府県別国公立別学校数・学生数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220816120845/ title="都道府県別消費者物価指数/教育（全国平均=100）">都道府県別消費者物価指数/教育（全国平均=100）</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220812173814/ title=金沢市の夏の月平均気温の遷移>金沢市の夏の月平均気温の遷移</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/figures/202209/20220907122906/ title="ACL-2022 WordCloud">ACL-2022 WordCloud</a></li></ul></li></ul></li><li><a href=/akitenkrad-blog/posts/latex/ title="Latex Mathematics Syntax Guide">Latex Mathematics Syntax Guide</a></li><li><a href=/akitenkrad-blog/posts/markdown/ title="Markdown Sample">Markdown Sample</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/akitenkrad-blog/posts/arxiv/202403/20240331000000/hero.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_120x120_fit_box_3.png alt="Author Image"><h5 class=author-name></h5><p>Sunday, Mar 31, 2024</p></div><div class=title><h1>arXiv @ 2024.03.31</h1></div><div class=taxonomy-terms><ul><li class=rounded><a href=/akitenkrad-blog/tags/arxiv class="btn, btn-sm">arXiv</a></li><li class=rounded><a href=/akitenkrad-blog/tags/published2024 class="btn, btn-sm">2024</a></li></ul></div><div class=post-content id=post-content><figure style=border:none;width:100%;display:flex;justify-content:center><iframe src=pie.html width=900 height=620 style=border:none></iframe></figure><h2 id=primary-categories>Primary Categories</h2><ul><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240331000000/#csai-12>cs.AI (12)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240331000000/#csar-3>cs.AR (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240331000000/#csce-1>cs.CE (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240331000000/#cscl-42>cs.CL (42)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240331000000/#cscr-3>cs.CR (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240331000000/#cscv-68>cs.CV (68)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240331000000/#cscy-1>cs.CY (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240331000000/#csdb-3>cs.DB (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240331000000/#csdc-1>cs.DC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240331000000/#csds-2>cs.DS (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240331000000/#csir-5>cs.IR (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240331000000/#csit-6>cs.IT (6)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240331000000/#cslg-18>cs.LG (18)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240331000000/#csmm-1>cs.MM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240331000000/#csne-1>cs.NE (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240331000000/#csni-4>cs.NI (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240331000000/#csro-10>cs.RO (10)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240331000000/#cssd-1>cs.SD (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240331000000/#csse-1>cs.SE (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240331000000/#eessas-1>eess.AS (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240331000000/#eessiv-7>eess.IV (7)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240331000000/#eesssp-1>eess.SP (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240331000000/#eesssy-2>eess.SY (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240331000000/#math-ph-1>math-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240331000000/#mathna-2>math.NA (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240331000000/#mathoc-3>math.OC (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240331000000/#q-biobm-2>q-bio.BM (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240331000000/#statco-1>stat.CO (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240331000000/#statml-1>stat.ML (1)</a></li></ul><h2 id=keywords>Keywords</h2><table border=1 class=dataframe><thead><tr style=text-align:right><th>keyword</th><th>cs.AI</th><th>cs.CL</th><th>cs.CV</th><th>cs.LG</th><th>cs.RO</th></tr></thead><tbody><tr><td>Adversarial Attack</td><td></td><td></td><td>3</td><td></td><td></td></tr><tr><td>Adversarial Learning</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Anomaly Detection</td><td></td><td></td><td>2</td><td></td><td></td></tr><tr><td>Autoencoder</td><td></td><td></td><td>4</td><td></td><td></td></tr><tr><td>Automatic Evaluation</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Automatic Speech Recognition</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>BART</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>BERT</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Benchmarking</td><td></td><td>18</td><td>24</td><td>8</td><td></td></tr><tr><td>Black Box</td><td>1</td><td></td><td>2</td><td></td><td></td></tr><tr><td>ChatGPT</td><td></td><td>4</td><td></td><td></td><td></td></tr><tr><td>Chatbot</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Clustering</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Code Generation</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Continual Learning</td><td></td><td></td><td>4</td><td></td><td></td></tr><tr><td>Continuous Graph</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Continuous Time</td><td></td><td></td><td></td><td></td><td>2</td></tr><tr><td>Contrastive Learning</td><td></td><td>2</td><td>1</td><td></td><td></td></tr><tr><td>Convolution</td><td></td><td></td><td>9</td><td>3</td><td></td></tr><tr><td>Convolutional Neural Network</td><td></td><td>1</td><td>9</td><td>2</td><td></td></tr><tr><td>Counter-factual</td><td></td><td></td><td>2</td><td></td><td>1</td></tr><tr><td>Curriculum Learning</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Data Augmentation</td><td></td><td>1</td><td>2</td><td></td><td></td></tr><tr><td>Dialogue System</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Diffusion Model</td><td></td><td></td><td>6</td><td></td><td></td></tr><tr><td>Discrete Time</td><td></td><td></td><td></td><td></td><td>2</td></tr><tr><td>Distribution Shift</td><td></td><td></td><td>2</td><td></td><td></td></tr><tr><td>Emotion Recognition</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Explainable AI</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Face Recognition</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Fact Verification</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Fairness</td><td>1</td><td></td><td>2</td><td></td><td></td></tr><tr><td>Fake News Detection</td><td>1</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Federated Learning</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Few-shot</td><td></td><td>4</td><td>2</td><td>1</td><td></td></tr><tr><td>Fine-tuning</td><td>1</td><td>13</td><td>7</td><td></td><td>1</td></tr><tr><td>Foundation Model</td><td></td><td>1</td><td>6</td><td></td><td></td></tr><tr><td>GLUE</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>GPT</td><td></td><td>10</td><td>1</td><td></td><td></td></tr><tr><td>GPT-2</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>GPT-3</td><td></td><td>3</td><td></td><td></td><td></td></tr><tr><td>GPT-3.5</td><td></td><td>3</td><td></td><td></td><td></td></tr><tr><td>GPT-4</td><td></td><td>7</td><td></td><td></td><td></td></tr><tr><td>GPT-4 turbo</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Gemini</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Generative AI</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Generative Adversarial Network</td><td></td><td></td><td>4</td><td></td><td></td></tr><tr><td>Geometry</td><td></td><td></td><td>5</td><td></td><td></td></tr><tr><td>Graph</td><td>1</td><td>2</td><td>2</td><td>3</td><td>1</td></tr><tr><td>Graph Attention Networks</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Graph Convolutional Network</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Graph Embedding</td><td></td><td></td><td></td><td></td><td>1</td></tr><tr><td>Graph Neural Network</td><td>2</td><td></td><td></td><td>4</td><td></td></tr><tr><td>Grounding</td><td></td><td></td><td>2</td><td></td><td></td></tr><tr><td>High-Resource</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Image2text</td><td></td><td></td><td>3</td><td></td><td></td></tr><tr><td>In-context Learning</td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><td>Information Retrieval</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>InstructGPT</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Instruction Following</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Intent Detection</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Karush-Kuhn-Tucker</td><td></td><td></td><td></td><td></td><td>1</td></tr><tr><td>Knowledge Distillation</td><td></td><td>2</td><td>2</td><td>1</td><td></td></tr><tr><td>LLaMA</td><td></td><td>1</td><td></td><td>1</td><td></td></tr><tr><td>Language Generation</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Large Language Model</td><td>9</td><td>46</td><td>6</td><td>2</td><td></td></tr><tr><td>Low-Resource</td><td></td><td>4</td><td></td><td></td><td></td></tr><tr><td>MNIST</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Markov Decision Process</td><td>1</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Masked Language Model</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Message-Passing</td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><td>Mistral</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Multi-modal</td><td></td><td>2</td><td>10</td><td>2</td><td></td></tr><tr><td>Named Entity Recognition</td><td></td><td>3</td><td></td><td></td><td></td></tr><tr><td>Natural Language Generation</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Natural Language Inference</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Natural Language Understanding</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Neural Machine Translation</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Node Classification</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Object Detection</td><td></td><td></td><td>2</td><td>1</td><td></td></tr><tr><td>Offline Reinforcement Learning</td><td></td><td></td><td></td><td></td><td>2</td></tr><tr><td>Optical Character Recognition</td><td></td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>Out-of-distribution</td><td></td><td></td><td>2</td><td>2</td><td></td></tr><tr><td>Probabilistic Model</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Prompt</td><td>2</td><td>8</td><td>9</td><td></td><td></td></tr><tr><td>Pruning</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Quantization</td><td>1</td><td>1</td><td>2</td><td>2</td><td></td></tr><tr><td>Question Answering</td><td></td><td>1</td><td>4</td><td></td><td></td></tr><tr><td>Reasoning</td><td>1</td><td>4</td><td>3</td><td></td><td></td></tr><tr><td>Recommendation</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Recurrent Neural Network</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Reinforcement Learning</td><td>1</td><td></td><td></td><td>3</td><td>6</td></tr><tr><td>Retrieval Augmentation</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Retrieval-Augmented Generation</td><td></td><td>3</td><td>2</td><td></td><td></td></tr><tr><td>Rouge</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Rouge-L</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Self-Attention</td><td></td><td></td><td>2</td><td>1</td><td></td></tr><tr><td>Self-Distillation</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Self-supervised Learning</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Sentiment Analysis</td><td>1</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Simulation</td><td>2</td><td>1</td><td>2</td><td>1</td><td>4</td></tr><tr><td>Simulator</td><td>2</td><td>1</td><td>2</td><td>1</td><td>4</td></tr><tr><td>Stochastic Gradient Descent</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Summarization</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Supervised Learning</td><td></td><td>2</td><td>6</td><td></td><td></td></tr><tr><td>Text Classification</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Text Embedding</td><td></td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>Text Generation</td><td>1</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Text Summarization</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Text2image</td><td></td><td></td><td>4</td><td></td><td></td></tr><tr><td>Transfer Learning</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Transformer</td><td></td><td>4</td><td>11</td><td>4</td><td></td></tr><tr><td>Unsupervised Learning</td><td>2</td><td></td><td>3</td><td></td><td></td></tr><tr><td>Variational Autoencoder</td><td></td><td></td><td>2</td><td></td><td></td></tr><tr><td>Vision Transformer</td><td></td><td></td><td>4</td><td></td><td></td></tr><tr><td>Vision-and-Language</td><td></td><td></td><td>7</td><td></td><td>1</td></tr><tr><td>Visual Question Answering</td><td></td><td></td><td>4</td><td></td><td></td></tr><tr><td>Weakly-supervised Learning</td><td></td><td></td><td>3</td><td></td><td></td></tr><tr><td>Word2vec</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Zero-shot</td><td>1</td><td>2</td><td>3</td><td>1</td><td></td></tr><tr><td>falcon</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>human-in-the-loop</td><td></td><td></td><td>1</td><td></td><td></td></tr></tbody></table><script>$(function(){$("table").addClass("keyword-table table-bordered border-success"),$("table thead").addClass("sticky-top"),$("table tbody td").css("text-align","")})</script><h2 id=cscl-42>cs.CL (42)</h2><h3 id=142--1204-chatgpt-vs-media-bias-a-comparative-study-of-gpt-35-and-fine-tuned-language-models-zehao-wen-et-al-2024>(1/42 | 1/204) ChatGPT v.s. Media Bias: A Comparative Study of GPT-3.5 and Fine-tuned Language Models (Zehao Wen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zehao Wen, Rabih Younes. (2024)<br><strong>ChatGPT v.s. Media Bias: A Comparative Study of GPT-3.5 and Fine-tuned Language Models</strong><br><button class=copy-to-clipboard title="ChatGPT v.s. Media Bias: A Comparative Study of GPT-3.5 and Fine-tuned Language Models" index=1>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-1 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 103<br>Keywords: Benchmarking, Fine-tuning, BART, ChatGPT, GPT, GPT-2, GPT-3, GPT-3.5, Fake News Detection, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20158v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20158v1.pdf filename=2403.20158v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In our rapidly evolving digital sphere, the ability to discern media bias becomes crucial as it can shape public sentiment and influence pivotal decisions. The advent of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> such as <b>ChatGPT,</b> noted for their broad utility in various natural language processing (NLP) tasks, invites exploration of their efficacy in media bias detection. Can <b>ChatGPT</b> detect media bias? This study seeks to answer this question by leveraging the Media Bias Identification <b>Benchmark</b> (MBIB) to assess <b>ChatGPT&rsquo;s</b> competency in distinguishing six categories of media bias, juxtaposed against <b>fine-tuned</b> models such as <b>BART,</b> ConvBERT, and <b>GPT-2.</b> The findings present a dichotomy: <b>ChatGPT</b> performs at par with <b>fine-tuned</b> models in detecting hate speech and text-level context bias, yet faces difficulties with subtler elements of other bias detections, namely, <b>fake</b> <b>news,</b> racial, gender, and cognitive biases.</p></p class="citation"></blockquote><h3 id=242--2204-enhancing-the-general-agent-capabilities-of-low-parameter-llms-through-tuning-and-multi-branch-reasoning-qinhao-zhou-et-al-2024>(2/42 | 2/204) Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning (Qinhao Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qinhao Zhou, Zihan Zhang, Xiang Xiang, Ke Wang, Yuchuan Wu, Yongbin Li. (2024)<br><strong>Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning</strong><br><button class=copy-to-clipboard title="Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning" index=2>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-2 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 90<br>Keywords: Fine-tuning, Supervised Learning, ChatGPT, GPT, GPT-4, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19962v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19962v1.pdf filename=2403.19962v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Open-source pre-trained <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> exhibit strong language understanding and generation capabilities, making them highly successful in a variety of tasks. However, when used as agents for dealing with complex problems in the real world, their performance is far inferior to <b>large</b> <b>commercial</b> <b>models</b> such as <b>ChatGPT</b> and <b>GPT-4.</b> As intelligent agents, <b>LLMs</b> need to have the capabilities of task planning, long-term memory, and the ability to leverage external tools to achieve satisfactory performance. Various methods have been proposed to enhance the agent capabilities of <b>LLMs.</b> On the one hand, methods involve constructing agent-specific data and <b>fine-tuning</b> the models. On the other hand, some methods focus on designing <b>prompts</b> that effectively activate the <b>reasoning</b> abilities of the <b>LLMs.</b> We explore both strategies on the 7B and 13B models. We propose a comprehensive method for constructing agent-specific data using <b>GPT-4.</b> Through <b>supervised</b> <b>fine-tuning</b> with constructed data, we find that for these models with a relatively small number of parameters, <b>supervised</b> <b>fine-tuning</b> can significantly reduce hallucination outputs and formatting errors in agent tasks. Furthermore, techniques such as multi-path <b>reasoning</b> and task decomposition can effectively decrease problem complexity and enhance the performance of <b>LLMs</b> as agents. We evaluate our method on five agent tasks of AgentBench and achieve satisfactory results.</p></p class="citation"></blockquote><h3 id=342--3204-dataagent-evaluating-large-language-models-ability-to-answer-zero-shot-natural-language-queries-manit-mishra-et-al-2024>(3/42 | 3/204) DataAgent: Evaluating Large Language Models&rsquo; Ability to Answer Zero-Shot, Natural Language Queries (Manit Mishra et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Manit Mishra, Abderrahman Braham, Charles Marsom, Bryan Chung, Gavin Griffin, Dakshesh Sidnerlikar, Chatanya Sarin, Arjun Rajaram. (2024)<br><strong>DataAgent: Evaluating Large Language Models&rsquo; Ability to Answer Zero-Shot, Natural Language Queries</strong><br><button class=copy-to-clipboard title="DataAgent: Evaluating Large Language Models' Ability to Answer Zero-Shot, Natural Language Queries" index=3>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-3 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 73<br>Keywords: Benchmarking, Zero-shot, GPT, GPT-3, GPT-3.5, Code Generation, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00188v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00188v1.pdf filename=2404.00188v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Conventional processes for analyzing datasets and extracting meaningful information are often time-consuming and laborious. Previous work has identified manual, repetitive coding and data collection as major obstacles that hinder data scientists from undertaking more nuanced labor and high-level projects. To combat this, we evaluated OpenAI&rsquo;s <b>GPT-3.5</b> as a &ldquo;Language Data Scientist&rdquo; (LDS) that can extrapolate key findings, including correlations and basic information, from a given dataset. The model was tested on a diverse set of <b>benchmark</b> datasets to evaluate its performance across multiple standards, including data science <b>code-generation</b> <b>based</b> tasks involving libraries such as NumPy, Pandas, Scikit-Learn, and TensorFlow, and was broadly successful in correctly answering a given data science query related to the <b>benchmark</b> dataset. The LDS used various novel <b>prompt</b> engineering techniques to effectively answer a given question, including Chain-of-Thought reinforcement and SayCan <b>prompt</b> engineering. Our findings demonstrate great potential for leveraging <b>Large</b> <b>Language</b> <b>Models</b> for low-level, <b>zero-shot</b> data analysis.</p></p class="citation"></blockquote><h3 id=442--4204-latxa-an-open-language-model-and-evaluation-suite-for-basque-julen-etxaniz-et-al-2024>(4/42 | 4/204) Latxa: An Open Language Model and Evaluation Suite for Basque (Julen Etxaniz et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Julen Etxaniz, Oscar Sainz, Naiara Perez, Itziar Aldabe, German Rigau, Eneko Agirre, Aitor Ormazabal, Mikel Artetxe, Aitor Soroa. (2024)<br><strong>Latxa: An Open Language Model and Evaluation Suite for Basque</strong><br><button class=copy-to-clipboard title="Latxa: An Open Language Model and Evaluation Suite for Basque" index=4>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-4 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 73<br>Keywords: Benchmarking, Low-Resource, GPT, GPT-4, GPT-4 turbo, LLaMA, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20266v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20266v1.pdf filename=2403.20266v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce Latxa, a family of <b>large</b> <b>language</b> <b>models</b> for Basque ranging from 7 to 70 billion parameters. Latxa is based on <b>Llama</b> 2, which we continue pretraining on a new Basque corpus comprising 4.3M documents and 4.2B tokens. Addressing the scarcity of high-quality <b>benchmarks</b> for Basque, we further introduce 4 multiple choice evaluation datasets: EusProficiency, comprising 5,169 questions from official language proficiency exams; EusReading, comprising 352 reading comprehension questions; EusTrivia, comprising 1,715 trivia questions from 5 knowledge areas; and EusExams, comprising 16,774 questions from public examinations. In our extensive evaluation, Latxa outperforms all previous open models we compare to by a <b>large</b> <b>margin.</b> <b>In</b> addition, it is competitive with <b>GPT-4</b> <b>Turbo</b> in language proficiency and understanding, despite lagging behind in reading comprehension and knowledge-intensive tasks. Both the Latxa family of models, as well as our new pretraining corpora and evaluation datasets, are publicly available under open licenses at <a href=https://github.com/hitz-zentroa/latxa>https://github.com/hitz-zentroa/latxa</a>. Our suite enables reproducible research on methods to build <b>LLMs</b> for <b>low-resource</b> languages.</p></p class="citation"></blockquote><h3 id=542--5204-on-the-fly-definition-augmentation-of-llms-for-biomedical-ner-monica-munnangi-et-al-2024>(5/42 | 5/204) On-the-fly Definition Augmentation of LLMs for Biomedical NER (Monica Munnangi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Monica Munnangi, Sergey Feldman, Byron C Wallace, Silvio Amir, Tom Hope, Aakanksha Naik. (2024)<br><strong>On-the-fly Definition Augmentation of LLMs for Biomedical NER</strong><br><button class=copy-to-clipboard title="On-the-fly Definition Augmentation of LLMs for Biomedical NER" index=5>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-5 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: Few-shot, Fine-tuning, GPT, GPT-4, Named Entity Recognition, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00152v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00152v1.pdf filename=2404.00152v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite their general capabilities, <b>LLMs</b> still struggle on biomedical <b>NER</b> tasks, which are difficult due to the presence of specialized terminology and lack of training data. In this work we set out to improve <b>LLM</b> performance on biomedical <b>NER</b> in limited data settings via a new knowledge augmentation approach which incorporates definitions of relevant concepts on-the-fly. During this process, to provide a test bed for knowledge augmentation, we perform a comprehensive exploration of <b>prompting</b> strategies. Our experiments show that definition augmentation is useful for both open source and closed <b>LLMs.</b> For example, it leads to a relative improvement of 15% (on average) in <b>GPT-4</b> performance (F1) across all (six) of our test datasets. We conduct extensive ablations and analyses to demonstrate that our performance improvements stem from adding relevant definitional knowledge. We find that careful <b>prompting</b> strategies also improve <b>LLM</b> performance, allowing them to outperform <b>fine-tuned</b> language models in <b>few-shot</b> settings. To facilitate future research in this direction, we release our code at <a href=https://github.com/allenai/beacon>https://github.com/allenai/beacon</a>.</p></p class="citation"></blockquote><h3 id=642--6204-are-llms-effective-backbones-for-fine-tuning-an-experimental-investigation-of-supervised-llms-on-chinese-short-text-matching-shulin-liu-et-al-2024>(6/42 | 6/204) Are LLMs Effective Backbones for Fine-tuning? An Experimental Investigation of Supervised LLMs on Chinese Short Text Matching (Shulin Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shulin Liu, Chengcheng Xu, Hao Liu, Tinghao Yu, Tao Yang. (2024)<br><strong>Are LLMs Effective Backbones for Fine-tuning? An Experimental Investigation of Supervised LLMs on Chinese Short Text Matching</strong><br><button class=copy-to-clipboard title="Are LLMs Effective Backbones for Fine-tuning? An Experimental Investigation of Supervised LLMs on Chinese Short Text Matching" index=6>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-6 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: Few-shot, Fine-tuning, Supervised Learning, Natural Language Understanding, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19930v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19930v1.pdf filename=2403.19930v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The recent success of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> has garnered significant attention in both academia and industry. Prior research on <b>LLMs</b> has primarily focused on enhancing or leveraging their generalization capabilities in zero- and <b>few-shot</b> settings. However, there has been limited investigation into effectively <b>fine-tuning</b> <b>LLMs</b> for a specific <b>natural</b> <b>language</b> <b>understanding</b> task in <b>supervised</b> settings. In this study, we conduct an experimental analysis by <b>fine-tuning</b> <b>LLMs</b> for the task of Chinese short text matching. We explore various factors that influence performance when <b>fine-tuning</b> <b>LLMs,</b> including task modeling methods, <b>prompt</b> formats, and output formats.</p></p class="citation"></blockquote><h3 id=742--7204-towards-a-robust-retrieval-based-summarization-system-shengjie-liu-et-al-2024>(7/42 | 7/204) Towards a Robust Retrieval-Based Summarization System (Shengjie Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shengjie Liu, Jing Wu, Jingyuan Bao, Wenyi Wang, Naira Hovakimyan, Christopher G Healey. (2024)<br><strong>Towards a Robust Retrieval-Based Summarization System</strong><br><button class=copy-to-clipboard title="Towards a Robust Retrieval-Based Summarization System" index=7>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-7 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-IR, cs-LG, cs.CL<br>Keyword Score: 70<br>Keywords: Fine-tuning, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19889v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19889v1.pdf filename=2403.19889v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper describes an investigation of the robustness of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> for <b>retrieval</b> <b>augmented</b> <b>generation</b> <b>(RAG)-based</b> <b>summarization</b> tasks. While <b>LLMs</b> provide <b>summarization</b> capabilities, their performance in complex, real-world scenarios remains under-explored. Our first contribution is LogicSumm, an innovative evaluation framework incorporating realistic scenarios to assess <b>LLM</b> robustness during <b>RAG-based</b> <b>summarization.</b> Based on limitations identified by LogiSumm, we then developed SummRAG, a comprehensive system to create training dialogues and <b>fine-tune</b> a model to enhance robustness within LogicSumm&rsquo;s scenarios. SummRAG is an example of our goal of defining structured methods to test the capabilities of an <b>LLM,</b> rather than addressing issues in a one-off fashion. Experimental results confirm the power of SummRAG, showcasing improved logical coherence and <b>summarization</b> quality. Data, corresponding model weights, and Python code are available online.</p></p class="citation"></blockquote><h3 id=842--8204-elitr-bench-a-meeting-assistant-benchmark-for-long-context-language-models-thibaut-thonet-et-al-2024>(8/42 | 8/204) ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models (Thibaut Thonet et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Thibaut Thonet, Jos Rozen, Laurent Besacier. (2024)<br><strong>ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models</strong><br><button class=copy-to-clipboard title="ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models" index=8>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-8 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 63<br>Keywords: Benchmarking, GPT, GPT-4, Automatic Speech Recognition, Automatic Speech Recognition, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20262v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20262v1.pdf filename=2403.20262v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Research on <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> has recently witnessed an increasing interest in extending models&rsquo; context size to better capture dependencies within long documents. While <b>benchmarks</b> have been proposed to assess long-range abilities, existing efforts primarily considered generic tasks that are not necessarily aligned with real-world applications. In contrast, our work proposes a new <b>benchmark</b> for long-context <b>LLMs</b> focused on a practical meeting assistant scenario. In this scenario, the long contexts consist of transcripts obtained by <b>automatic</b> <b>speech</b> <b>recognition,</b> presenting unique challenges for <b>LLMs</b> due to the inherent noisiness and oral nature of such data. Our <b>benchmark,</b> named ELITR-Bench, augments the existing ELITR corpus&rsquo; transcripts with 271 manually crafted questions and their ground-truth answers. Our experiments with recent long-context <b>LLMs</b> on ELITR-Bench highlight a gap between open-source and proprietary models, especially when questions are asked sequentially within a conversation. We also provide a thorough analysis of our <b>GPT-4-based</b> evaluation method, encompassing insights from a crowdsourcing study. Our findings suggest that while <b>GPT-4&rsquo;s</b> evaluation scores are correlated with human judges&rsquo;, its ability to differentiate among more than three score levels may be limited.</p></p class="citation"></blockquote><h3 id=942--9204-luq-long-text-uncertainty-quantification-for-llms-caiqi-zhang-et-al-2024>(9/42 | 9/204) LUQ: Long-text Uncertainty Quantification for LLMs (Caiqi Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Caiqi Zhang, Fangyu Liu, Marco Basaldella, Nigel Collier. (2024)<br><strong>LUQ: Long-text Uncertainty Quantification for LLMs</strong><br><button class=copy-to-clipboard title="LUQ: Long-text Uncertainty Quantification for LLMs" index=9>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-9 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: GPT, GPT-4, Gemini, Text Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20279v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20279v1.pdf filename=2403.20279v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have demonstrated remarkable capability in a variety of NLP tasks. Despite their effectiveness, these models are prone to generate nonfactual content. Uncertainty Quantification (UQ) is pivotal in enhancing our understanding of a model&rsquo;s confidence in its generated content, thereby aiding in the mitigation of nonfactual outputs. Existing research on UQ predominantly targets short <b>text</b> <b>generation,</b> typically yielding brief, word-limited responses. However, real-world applications frequently necessitate much longer responses. Our study first highlights the limitations of current UQ methods in handling long <b>text</b> <b>generation.</b> We then introduce \textsc{Luq}, a novel sampling-based UQ approach specifically designed for long <b>text.</b> <b>Our</b> findings reveal that \textsc{Luq} outperforms existing baseline methods in correlating with the model&rsquo;s factuality scores (negative coefficient of -0.85 observed for <b>Gemini</b> Pro). With \textsc{Luq} as the tool for UQ, we investigate behavior patterns of several popular <b>LLMs&rsquo;</b> response confidence spectrum and how that interplays with the response&rsquo; factuality. We identify that <b>LLMs</b> lack confidence in generating long <b>text</b> <b>for</b> rare facts and a factually strong model (i.e. <b>GPT-4)</b> tends to reject questions it is not sure about. To further improve the factual accuracy of <b>LLM</b> responses, we propose a method called \textsc{Luq-Ensemble} that ensembles responses from multiple models and selects the response with the least uncertainty. The ensembling method greatly improves the response factuality upon the best standalone <b>LLM.</b></p></p class="citation"></blockquote><h3 id=1042--10204-fine-tuning-large-language-models-for-automated-diagnostic-screening-summaries-manjeet-yadav-et-al-2024>(10/42 | 10/204) Fine-tuning Large Language Models for Automated Diagnostic Screening Summaries (Manjeet Yadav et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Manjeet Yadav, Nilesh Kumar Sahu, Mudita Chaturvedi, Snehil Gupta, Haroon R Lone. (2024)<br><strong>Fine-tuning Large Language Models for Automated Diagnostic Screening Summaries</strong><br><button class=copy-to-clipboard title="Fine-tuning Large Language Models for Automated Diagnostic Screening Summaries" index=10>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-10 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Fine-tuning, Fine-tuning, Large Language Model, Large Language Model, Rouge, Rouge-L<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20145v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20145v1.pdf filename=2403.20145v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Improving mental health support in developing countries is a pressing need. One potential solution is the development of scalable, automated systems to conduct diagnostic screenings, which could help alleviate the burden on mental health professionals. In this work, we evaluate several state-of-the-art <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> with and without <b>fine-tuning,</b> on our custom dataset for generating concise summaries from mental state examinations. We rigorously evaluate four different models for summary generation using established <b>ROUGE</b> metrics and input from human evaluators. The results highlight that our top-performing <b>fine-tuned</b> model outperforms existing models, achieving <b>ROUGE-1</b> and <b>ROUGE-L</b> values of 0.810 and 0.764, respectively. Furthermore, we assessed the <b>fine-tuned</b> model&rsquo;s generalizability on a publicly available D4 dataset, and the outcomes were promising, indicating its potential applicability beyond our custom dataset.</p></p class="citation"></blockquote><h3 id=1142--11204-transformer-lite-high-efficiency-deployment-of-large-language-models-on-mobile-phone-gpus-luchang-li-et-al-2024>(11/42 | 11/204) Transformer-Lite: High-efficiency Deployment of Large Language Models on Mobile Phone GPUs (Luchang Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Luchang Li, Sheng Qian, Jie Lu, Lunxi Yuan, Rui Wang, Qin Xie. (2024)<br><strong>Transformer-Lite: High-efficiency Deployment of Large Language Models on Mobile Phone GPUs</strong><br><button class=copy-to-clipboard title="Transformer-Lite: High-efficiency Deployment of Large Language Models on Mobile Phone GPUs" index=11>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-11 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Quantization, Transformer, Text Summarization, Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20041v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20041v1.pdf filename=2403.20041v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The <b>Large</b> <b>Language</b> <b>Model</b> <b>(LLM)</b> is widely employed for tasks such as intelligent assistants, <b>text</b> <b>summarization,</b> translation, and multi-modality on mobile phones. However, the current methods for on-device <b>LLM</b> deployment maintain slow inference speed, which causes poor user experience. To facilitate high-efficiency <b>LLM</b> deployment on device GPUs, we propose four optimization techniques: (a) a symbolic expression-based approach to support dynamic shape model inference; (b) operator optimizations and execution priority setting to enhance inference speed and reduce phone lagging; (c) an FP4 <b>quantization</b> method termed M0E4 to reduce dequantization overhead; (d) a sub-tensor-based technique to eliminate the need for copying KV cache after <b>LLM</b> inference. Furthermore, we implement these methods in our mobile inference engine, <b>Transformer-Lite,</b> which is compatible with both Qualcomm and MTK processors. We evaluated <b>Transformer-Lite&rsquo;s</b> performance using <b>LLMs</b> with varied architectures and parameters ranging from 2B to 14B. Specifically, we achieved prefill and decoding speeds of 121 token/s and 14 token/s for ChatGLM2 6B, and 330 token/s and 30 token/s for smaller Gemma 2B, respectively. Compared with CPU-based FastLLM and GPU-based MLC-LLM, our engine attains over 10x speedup for the prefill speed and 2~3x speedup for the decoding speed.</p></p class="citation"></blockquote><h3 id=1242--12204-realm-reference-resolution-as-language-modeling-joel-ruben-antony-moniz-et-al-2024>(12/42 | 12/204) ReALM: Reference Resolution As Language Modeling (Joel Ruben Antony Moniz et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Joel Ruben Antony Moniz, Soundarya Krishnan, Melis Ozyildirim, Prathamesh Saraf, Halim Cagri Ates, Yuan Zhang, Hong Yu, Nidhi Rajshree. (2024)<br><strong>ReALM: Reference Resolution As Language Modeling</strong><br><button class=copy-to-clipboard title="ReALM: Reference Resolution As Language Modeling" index=12>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-12 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 53<br>Keywords: Benchmarking, GPT, GPT-3, GPT-3.5, GPT-4, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20329v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20329v1.pdf filename=2403.20329v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Reference resolution is an important problem, one that is essential to understand and successfully handle context of different kinds. This context includes both previous turns and context that pertains to non-conversational entities, such as entities on the user&rsquo;s screen or those running in the background. While <b>LLMs</b> have been shown to be extremely powerful for a variety of tasks, their use in reference resolution, particularly for non-conversational entities, remains underutilized. This paper demonstrates how <b>LLMs</b> can be used to create an extremely effective system to resolve references of various types, by showing how reference resolution can be converted into a language modeling problem, despite involving forms of entities like those on screen that are not traditionally conducive to being reduced to a text-only modality. We demonstrate large improvements over an existing system with similar functionality across different types of references, with our smallest model obtaining absolute gains of over 5% for on-screen references. We also <b>benchmark</b> against <b>GPT-3.5</b> and <b>GPT-4,</b> with our smallest model achieving performance comparable to that of <b>GPT-4,</b> and our larger models substantially outperforming it.</p></p class="citation"></blockquote><h3 id=1342--13204-gecko-versatile-text-embeddings-distilled-from-large-language-models-jinhyuk-lee-et-al-2024>(13/42 | 13/204) Gecko: Versatile Text Embeddings Distilled from Large Language Models (Jinhyuk Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinhyuk Lee, Zhuyun Dai, Xiaoqi Ren, Blair Chen, Daniel Cer, Jeremy R. Cole, Kai Hui, Michael Boratko, Rajvi Kapadia, Wen Ding, Yi Luan, Sai Meher Karthik Duddu, Gustavo Hernandez Abrego, Weiqiang Shi, Nithi Gupta, Aditya Kusupati, Prateek Jain, Siddhartha Reddy Jonnalagadda, Ming-Wei Chang, Iftekhar Naim. (2024)<br><strong>Gecko: Versatile Text Embeddings Distilled from Large Language Models</strong><br><button class=copy-to-clipboard title="Gecko: Versatile Text Embeddings Distilled from Large Language Models" index=13>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-13 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 53<br>Keywords: Benchmarking, Knowledge Distillation, Knowledge Distillation, Large Language Model, Large Language Model, Text Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20327v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20327v1.pdf filename=2403.20327v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present Gecko, a compact and versatile <b>text</b> <b>embedding</b> model. Gecko achieves strong retrieval performance by leveraging a key idea: <b>distilling</b> knowledge from <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> into a retriever. Our two-step <b>distillation</b> process begins with generating diverse, synthetic paired data using an <b>LLM.</b> Next, we further refine the data quality by retrieving a set of candidate passages for each query, and relabeling the positive and hard negative passages using the same <b>LLM.</b> The effectiveness of our approach is demonstrated by the compactness of the Gecko. On the Massive <b>Text</b> <b>Embedding</b> <b>Benchmark</b> (MTEB), Gecko with 256 embedding dimensions outperforms all existing entries with 768 embedding size. Gecko with 768 embedding dimensions achieves an average score of 66.31, competing with 7x larger models and 5x higher dimensional embeddings.</p></p class="citation"></blockquote><h3 id=1442--14204-layernorm-a-key-component-in-parameter-efficient-fine-tuning-taha-valizadehaslani-et-al-2024>(14/42 | 14/204) LayerNorm: A key component in parameter-efficient fine-tuning (Taha ValizadehAslani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Taha ValizadehAslani, Hualou Liang. (2024)<br><strong>LayerNorm: A key component in parameter-efficient fine-tuning</strong><br><button class=copy-to-clipboard title="LayerNorm: A key component in parameter-efficient fine-tuning" index=14>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-14 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 53<br>Keywords: Benchmarking, Fine-tuning, Fine-tuning, BERT, Transformer, GLUE<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20284v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20284v1.pdf filename=2403.20284v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Fine-tuning</b> a pre-trained model, such as Bidirectional Encoder Representations from <b>Transformers</b> <b>(BERT),</b> has been proven to be an effective method for solving many natural language processing (NLP) tasks. However, due to the large number of parameters in many state-of-the-art NLP models, including <b>BERT,</b> the process of <b>fine-tuning</b> is computationally expensive. One attractive solution to this issue is parameter-efficient <b>fine-tuning,</b> which involves modifying only a minimal segment of the model while keeping the remainder unchanged. Yet, it remains unclear which segment of the <b>BERT</b> model is crucial for <b>fine-tuning.</b> In this paper, we first analyze different components in the <b>BERT</b> model to pinpoint which one undergoes the most significant changes after <b>fine-tuning.</b> We find that output LayerNorm changes more than any other components when <b>fine-tuned</b> for different General Language Understanding Evaluation <b>(GLUE)</b> tasks. Then we show that only <b>fine-tuning</b> the LayerNorm can reach comparable, or in some cases better, performance to full <b>fine-tuning</b> and other parameter-efficient <b>fine-tuning</b> methods. Moreover, we use Fisher information to determine the most critical subset of LayerNorm and demonstrate that many NLP tasks in the <b>GLUE</b> <b>benchmark</b> can be solved by <b>fine-tuning</b> only a small portion of LayerNorm with negligible performance degradation.</p></p class="citation"></blockquote><h3 id=1542--15204-can-llms-learn-from-previous-mistakes-investigating-llms-errors-to-boost-for-reasoning-yongqi-tong-et-al-2024>(15/42 | 15/204) Can LLMs Learn from Previous Mistakes? Investigating LLMs&rsquo; Errors to Boost for Reasoning (Yongqi Tong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yongqi Tong, Dawei Li, Sizhe Wang, Yujia Wang, Fei Teng, Jingbo Shang. (2024)<br><strong>Can LLMs Learn from Previous Mistakes? Investigating LLMs&rsquo; Errors to Boost for Reasoning</strong><br><button class=copy-to-clipboard title="Can LLMs Learn from Previous Mistakes? Investigating LLMs' Errors to Boost for Reasoning" index=15>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-15 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 53<br>Keywords: Benchmarking, Few-shot, Fine-tuning, Reasoning, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20046v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20046v1.pdf filename=2403.20046v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent works have shown the benefits to <b>LLMs</b> from <b>fine-tuning</b> golden-standard Chain-of-Thought (CoT) rationales or using them as correct examples in <b>few-shot</b> <b>prompting.</b> While humans can indeed imitate correct examples, learning from our mistakes is another vital aspect of human cognition. Hence, a question naturally arises: \textit{can <b>LLMs</b> learn and benefit from their mistakes, especially for their reasoning? } This study investigates this problem from both the <b>prompting</b> and model-tuning perspectives. We begin by introducing \textsc{CoTErrorSet}, a new <b>benchmark</b> with 609,432 questions, each designed with both correct and error references, and demonstrating the types and reasons for making such mistakes. To explore the effectiveness of those mistakes, we design two methods: (1) \textbf{Self-rethinking} <b>prompting</b> guides <b>LLMs</b> to rethink whether they have made similar previous mistakes; and (2) \textbf{Mistake tuning} involves <b>finetuning</b> models in both correct and incorrect <b>reasoning</b> domains, rather than only tuning models to learn ground truth in traditional methodology. We conduct a series of experiments to prove <b>LLMs</b> can obtain benefits from mistakes in both directions. Our two methods offer potentially cost-effective strategies by leveraging errors to enhance <b>reasoning</b> capabilities, which costs significantly less than creating meticulously hand-crafted golden references. We ultimately make a thorough analysis of the reasons behind <b>LLMs&rsquo;</b> errors, which provides directions that future research needs to overcome. \textsc{CoTErrorSet} will be published soon on \texttt{Anonymity Link}.</p></p class="citation"></blockquote><h3 id=1642--16204-mango-a-benchmark-for-evaluating-mapping-and-navigation-abilities-of-large-language-models-peng-ding-et-al-2024>(16/42 | 16/204) MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models (Peng Ding et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Peng Ding, Jiading Fang, Peng Li, Kangrui Wang, Xiaochen Zhou, Mo Yu, Jing Li, Matthew R. Walter, Hongyuan Mei. (2024)<br><strong>MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models</strong><br><button class=copy-to-clipboard title="MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models" index=16>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-16 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs-RO, cs.CL<br>Keyword Score: 53<br>Keywords: Benchmarking, ChatGPT, GPT, GPT-4, Question Answering, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19913v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19913v1.pdf filename=2403.19913v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> such as <b>ChatGPT</b> and <b>GPT-4</b> have recently achieved astonishing performance on a variety of natural language processing tasks. In this paper, we propose MANGO, a <b>benchmark</b> to evaluate their capabilities to perform text-based mapping and navigation. Our <b>benchmark</b> includes 53 mazes taken from a suite of textgames: each maze is paired with a walkthrough that visits every location but does not cover all possible paths. The task is <b>question-answering:</b> <b>for</b> each maze, a <b>large</b> <b>language</b> <b>model</b> reads the walkthrough and answers hundreds of mapping and navigation <b>questions</b> <b>such</b> as &ldquo;How should you go to Attic from West of House?&rdquo; and &ldquo;Where are we if we go north and east from Cellar?&rdquo;. Although these <b>questions</b> <b>are</b> easy to humans, it turns out that even <b>GPT-4,</b> the best-to-date language model, performs poorly at answering them. Further, our experiments suggest that a strong mapping and navigation ability would benefit <b>large</b> <b>language</b> <b>models</b> in performing relevant downstream tasks, such as playing textgames. Our MANGO <b>benchmark</b> will facilitate future research on methods that improve the mapping and navigation capabilities of language models. We host our leaderboard, data, code, and evaluation program at <a href=https://mango.ttic.edu>https://mango.ttic.edu</a> and <a href=https://github.com/oaklight/mango/>https://github.com/oaklight/mango/</a>.</p></p class="citation"></blockquote><h3 id=1742--17204-wait-its-all-token-noise-always-has-been-interpreting-llm-behavior-using-shapley-value-behnam-mohammadi-2024>(17/42 | 17/204) Wait, It&rsquo;s All Token Noise? Always Has Been: Interpreting LLM Behavior Using Shapley Value (Behnam Mohammadi, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Behnam Mohammadi. (2024)<br><strong>Wait, It&rsquo;s All Token Noise? Always Has Been: Interpreting LLM Behavior Using Shapley Value</strong><br><button class=copy-to-clipboard title="Wait, It's All Token Noise? Always Has Been: Interpreting LLM Behavior Using Shapley Value" index=17>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-17 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 50<br>Keywords: Simulation, Simulator, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01332v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01332v1.pdf filename=2404.01332v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The emergence of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> has opened up exciting possibilities for simulating human behavior and cognitive processes, with potential applications in various domains, including marketing research and consumer behavior analysis. However, the validity of utilizing <b>LLMs</b> as stand-ins for human subjects remains uncertain due to glaring divergences that suggest fundamentally different underlying processes at play and the sensitivity of <b>LLM</b> responses to <b>prompt</b> variations. This paper presents a novel approach based on Shapley values from cooperative game theory to interpret <b>LLM</b> behavior and quantify the relative contribution of each <b>prompt</b> component to the model&rsquo;s output. Through two applications-a discrete choice experiment and an investigation of cognitive biases-we demonstrate how the Shapley value method can uncover what we term &ldquo;token noise&rdquo; effects, a phenomenon where <b>LLM</b> decisions are disproportionately influenced by tokens providing minimal informative content. This phenomenon raises concerns about the robustness and generalizability of insights obtained from <b>LLMs</b> in the context of human behavior <b>simulation.</b> Our model-agnostic approach extends its utility to proprietary <b>LLMs,</b> providing a valuable tool for marketers and researchers to strategically optimize <b>prompts</b> and mitigate apparent cognitive biases. Our findings underscore the need for a more nuanced understanding of the factors driving <b>LLM</b> responses before relying on them as substitutes for human subjects in research settings. We emphasize the importance of researchers reporting results conditioned on specific <b>prompt</b> templates and exercising caution when drawing parallels between human behavior and <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=1842--18204-classifying-conspiratorial-narratives-at-scale-false-alarms-and-erroneous-connections-ahmad-diab-et-al-2024>(18/42 | 18/204) Classifying Conspiratorial Narratives At Scale: False Alarms and Erroneous Connections (Ahmad Diab et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ahmad Diab, Rr. Nefriana, Yu-Ru Lin. (2024)<br><strong>Classifying Conspiratorial Narratives At Scale: False Alarms and Erroneous Connections</strong><br><button class=copy-to-clipboard title="Classifying Conspiratorial Narratives At Scale: False Alarms and Erroneous Connections" index=18>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-18 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-SI, cs.CL<br>Keyword Score: 50<br>Keywords: BERT, GPT, Transformer, Reasoning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00141v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00141v1.pdf filename=2404.00141v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Online discussions frequently involve conspiracy theories, which can contribute to the proliferation of belief in them. However, not all discussions surrounding conspiracy theories promote them, as some are intended to debunk them. Existing research has relied on simple proxies or focused on a constrained set of signals to identify conspiracy theories, which limits our understanding of conspiratorial discussions across different topics and online communities. This work establishes a general scheme for classifying discussions related to conspiracy theories based on authors&rsquo; perspectives on the conspiracy belief, which can be expressed explicitly through narrative elements, such as the agent, action, or objective, or implicitly through references to known theories, such as chemtrails or the New World Order. We leverage human-labeled ground truth to train a <b>BERT-based</b> model for classifying online CTs, which we then compared to the Generative Pre-trained <b>Transformer</b> machine <b>(GPT)</b> for detecting online conspiratorial content. Despite <b>GPT&rsquo;s</b> known strengths in its expressiveness and contextual understanding, our study revealed significant flaws in its logical <b>reasoning,</b> while also demonstrating comparable strengths from our classifiers. We present the first <b>large-scale</b> <b>classification</b> <b>study</b> using posts from the most active conspiracy-related Reddit forums and find that only one-third of the posts are classified as positive. This research sheds light on the potential applications of <b>large</b> <b>language</b> <b>models</b> in tasks demanding nuanced contextual comprehension.</p></p class="citation"></blockquote><h3 id=1942--19204-large-language-model-based-situational-dialogues-for-second-language-learning-shuyao-xu-et-al-2024>(19/42 | 19/204) Large Language Model based Situational Dialogues for Second Language Learning (Shuyao Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shuyao Xu, Long Qin, Tianyang Chen, Zhenzhou Zha, Bingxue Qiu, Weizhi Wang. (2024)<br><strong>Large Language Model based Situational Dialogues for Second Language Learning</strong><br><button class=copy-to-clipboard title="Large Language Model based Situational Dialogues for Second Language Learning" index=19>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-19 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Automatic Evaluation, Fine-tuning, Dialogue System, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20005v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20005v1.pdf filename=2403.20005v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In second language learning, scenario-based conversation practice is important for language learners to achieve fluency in speaking, but students often lack sufficient opportunities to practice their conversational skills with qualified instructors or native speakers. To bridge this gap, we propose situational <b>dialogue</b> <b>models</b> for students to engage in conversational practice. Our situational <b>dialogue</b> <b>models</b> are <b>fine-tuned</b> on <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> with the aim of combining the engaging nature of an open-ended conversation with the focused practice of scenario-based tasks. Leveraging the generalization capabilities of <b>LLMs,</b> we demonstrate that our situational <b>dialogue</b> <b>models</b> perform effectively not only on training topics but also on topics not encountered during training. This offers a promising solution to support a wide range of conversational topics without extensive manual work. Additionally, research in the field of <b>dialogue</b> <b>systems</b> still lacks reliable <b>automatic</b> <b>evaluation</b> metrics, leading to human evaluation as the gold standard (Smith et al., 2022), which is typically expensive. To address the limitations of existing evaluation methods, we present a novel <b>automatic</b> <b>evaluation</b> method that employs <b>fine-tuned</b> <b>LLMs</b> to efficiently and effectively assess the performance of situational <b>dialogue</b> <b>models.</b></p></p class="citation"></blockquote><h3 id=2042--20204-indibias-a-benchmark-dataset-to-measure-social-biases-in-language-models-for-indian-context-nihar-ranjan-sahoo-et-al-2024>(20/42 | 20/204) IndiBias: A Benchmark Dataset to Measure Social Biases in Language Models for Indian Context (Nihar Ranjan Sahoo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nihar Ranjan Sahoo, Pranamya Prashant Kulkarni, Narjis Asad, Arif Ahmad, Tanu Goyal, Aparna Garimella, Pushpak Bhattacharyya. (2024)<br><strong>IndiBias: A Benchmark Dataset to Measure Social Biases in Language Models for Indian Context</strong><br><button class=copy-to-clipboard title="IndiBias: A Benchmark Dataset to Measure Social Biases in Language Models for Indian Context" index=20>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-20 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 46<br>Keywords: Benchmarking, Benchmarking, ChatGPT, InstructGPT, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20147v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20147v1.pdf filename=2403.20147v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The pervasive influence of social biases in language data has sparked the need for <b>benchmark</b> datasets that capture and evaluate these biases in <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> Existing efforts predominantly focus on English language and the Western context, leaving a void for a reliable dataset that encapsulates India&rsquo;s unique socio-cultural nuances. To bridge this gap, we introduce IndiBias, a comprehensive <b>benchmarking</b> dataset designed specifically for evaluating social biases in the Indian context. We filter and translate the existing CrowS-Pairs dataset to create a <b>benchmark</b> dataset suited to the Indian context in Hindi language. Additionally, we leverage <b>LLMs</b> including <b>ChatGPT</b> and <b>InstructGPT</b> to augment our dataset with diverse societal biases and stereotypes prevalent in India. The included bias dimensions encompass gender, religion, caste, age, region, physical appearance, and occupation. We also build a resource to address intersectional biases along three intersectional dimensions. Our dataset contains 800 filtered sentences from the CrowS-Pairs dataset and tuples for bias measurement across different demographics. It is made available in English and Hindi languages, providing a size comparable to existing <b>benchmark</b> datasets. Furthermore, using IndiBias we compare ten different language models on multiple bias measurement metrics. We observed that the language models exhibit more bias across a majority of the intersectional groups.</p></p class="citation"></blockquote><h3 id=2142--21204-gpta-generative-prompt-tuning-assistant-for-synergistic-downstream-neural-network-enhancement-with-llms-xiao-liu-et-al-2024>(21/42 | 21/204) GPTA: Generative Prompt Tuning Assistant for Synergistic Downstream Neural Network Enhancement with LLMs (Xiao Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiao Liu, Jiawei Zhang. (2024)<br><strong>GPTA: Generative Prompt Tuning Assistant for Synergistic Downstream Neural Network Enhancement with LLMs</strong><br><button class=copy-to-clipboard title="GPTA: Generative Prompt Tuning Assistant for Synergistic Downstream Neural Network Enhancement with LLMs" index=21>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-21 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Benchmarking, Low-Resource, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00189v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00189v1.pdf filename=2404.00189v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study introduces GPTA, a <b>Large</b> <b>Language</b> <b>Model</b> assistance training framework, that enhances the training of downstream task models via prefix <b>prompt.</b> By minimizing data exposure to <b>LLM,</b> the framework addresses the security and legal challenges of applying <b>LLM</b> in downstream task model training. GPTA utilizes a new synergistic training approach, optimizing the downstream models with parameter gradients and <b>LLMs</b> with the novel ``dialogue gradient&rsquo;&rsquo;. The framework not only demonstrates significant improvements in model performance across six NLP <b>benchmark</b> datasets, but also reduces overfitting in <b>low-resource</b> scenarios effectively. The detailed analyses further validate that our pioneer framework provides a cost-efficient and adaptive method for downstream task model training with <b>LLM</b> support.</p></p class="citation"></blockquote><h3 id=2242--22204-measuring-taiwanese-mandarin-language-understanding-po-heng-chen-et-al-2024>(22/42 | 22/204) Measuring Taiwanese Mandarin Language Understanding (Po-Heng Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Po-Heng Chen, Sijia Cheng, Wei-Lin Chen, Yen-Ting Lin, Yun-Nung Chen. (2024)<br><strong>Measuring Taiwanese Mandarin Language Understanding</strong><br><button class=copy-to-clipboard title="Measuring Taiwanese Mandarin Language Understanding" index=22>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-22 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Benchmarking, Few-shot, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20180v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20180v1.pdf filename=2403.20180v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The evaluation of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> has drawn substantial attention in the field recently. This work focuses on evaluating <b>LLMs</b> in a Chinese context, specifically, for Traditional Chinese which has been largely underrepresented in existing <b>benchmarks.</b> We present TMLU, a holistic evaluation suit tailored for assessing the advanced knowledge and <b>reasoning</b> capability in <b>LLMs,</b> under the context of Taiwanese Mandarin. TMLU consists of an array of 37 subjects across social science, STEM, humanities, Taiwan-specific content, and others, ranging from middle school to professional levels. In addition, we curate chain-of-thought-like <b>few-shot</b> explanations for each subject to facilitate the evaluation of complex <b>reasoning</b> skills. To establish a comprehensive baseline, we conduct extensive experiments and analysis on 24 advanced <b>LLMs.</b> The results suggest that Chinese open-weight models demonstrate inferior performance comparing to multilingual proprietary ones, and open-weight models tailored for Taiwanese Mandarin lag behind the Simplified-Chinese counterparts. The findings indicate great headrooms for improvement, and emphasize the goal of TMLU to foster the development of localized Taiwanese-Mandarin <b>LLMs.</b> We release the <b>benchmark</b> and evaluation scripts for the community to promote future research.</p></p class="citation"></blockquote><h3 id=2342--23204-can-llms-correct-physicians-yet-investigating-effective-interaction-methods-in-the-medical-domain-burcu-sayin-et-al-2024>(23/42 | 23/204) Can LLMs Correct Physicians, Yet? Investigating Effective Interaction Methods in the Medical Domain (Burcu Sayin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Burcu Sayin, Pasquale Minervini, Jacopo Staiano, Andrea Passerini. (2024)<br><strong>Can LLMs Correct Physicians, Yet? Investigating Effective Interaction Methods in the Medical Domain</strong><br><button class=copy-to-clipboard title="Can LLMs Correct Physicians, Yet? Investigating Effective Interaction Methods in the Medical Domain" index=23>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-23 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Mistral, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20288v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20288v1.pdf filename=2403.20288v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We explore the potential of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> to assist and potentially correct physicians in medical decision-making tasks. We evaluate several <b>LLMs,</b> including Meditron, Llama2, and <b>Mistral,</b> to analyze the ability of these models to interact effectively with physicians across different scenarios. We consider questions from PubMedQA and several tasks, ranging from binary (yes/no) responses to long answer generation, where the answer of the model is produced after an interaction with a physician. Our findings suggest that <b>prompt</b> design significantly influences the downstream accuracy of <b>LLMs</b> and that <b>LLMs</b> can provide valuable feedback to physicians, challenging incorrect diagnoses and contributing to more accurate decision-making. For example, when the physician is accurate 38% of the time, <b>Mistral</b> can produce the correct answer, improving accuracy up to 74% depending on the <b>prompt</b> being used, while Llama2 and Meditron models exhibit greater sensitivity to <b>prompt</b> choice. Our analysis also uncovers the challenges of ensuring that <b>LLM-generated</b> suggestions are pertinent and useful, emphasizing the need for further research in this area.</p></p class="citation"></blockquote><h3 id=2442--24204-entertainment-chatbot-for-the-digital-inclusion-of-elderly-people-without-abstraction-capabilities-silvia-garcía-méndez-et-al-2024>(24/42 | 24/204) Entertainment chatbot for the digital inclusion of elderly people without abstraction capabilities (Silvia García-Méndez et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Silvia García-Méndez, Francisco de Arriba-Pérez, Francisco J. González-Castaño, José A. Regueiro-Janeiro, Felipe Gil-Castiñeira. (2024)<br><strong>Entertainment chatbot for the digital inclusion of elderly people without abstraction capabilities</strong><br><button class=copy-to-clipboard title="Entertainment chatbot for the digital inclusion of elderly people without abstraction capabilities" index=24>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-24 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-HC, cs-LG, cs.CL<br>Keyword Score: 40<br>Keywords: Chatbot, Language Generation, Natural Language Generation, Sentiment Analysis<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01327v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01327v1.pdf filename=2404.01327v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Current <b>language</b> <b>processing</b> technologies allow the creation of conversational <b>chatbot</b> platforms. Even though artificial intelligence is still too immature to support satisfactory user experience in many mass market domains, conversational interfaces have found their way into ad hoc applications such as call centres and online shopping assistants. However, they have not been applied so far to social inclusion of elderly people, who are particularly vulnerable to the digital divide. Many of them relieve their loneliness with traditional media such as TV and radio, which are known to create a feeling of companionship. In this paper we present the EBER <b>chatbot,</b> designed to reduce the digital gap for the elderly. EBER reads news in the background and adapts its responses to the user&rsquo;s mood. Its novelty lies in the concept of &ldquo;intelligent radio&rdquo;, according to which, instead of simplifying a digital information system to make it accessible to the elderly, a traditional channel they find familiar &ndash; background news &ndash; is augmented with interactions via voice dialogues. We make it possible by combining Artificial Intelligence Modelling <b>Language,</b> <b>automatic</b> <b>Natural</b> <b>Language</b> <b>Generation</b> and <b>Sentiment</b> <b>Analysis.</b> The system allows accessing digital content of interest by combining words extracted from user answers to <b>chatbot</b> questions with keywords extracted from the news items. This approach permits defining metrics of the abstraction capabilities of the users depending on a spatial representation of the word space. To prove the suitability of the proposed solution we present results of real experiments conducted with elderly people that provided valuable insights. Our approach was considered satisfactory during the tests and improved the information search capabilities of the participants.</p></p class="citation"></blockquote><h3 id=2542--25204-cross-lingual-transfer-robustness-to-lower-resource-languages-on-adversarial-datasets-shadi-manafi-et-al-2024>(25/42 | 25/204) Cross-Lingual Transfer Robustness to Lower-Resource Languages on Adversarial Datasets (Shadi Manafi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shadi Manafi, Nikhil Krishnaswamy. (2024)<br><strong>Cross-Lingual Transfer Robustness to Lower-Resource Languages on Adversarial Datasets</strong><br><button class=copy-to-clipboard title="Cross-Lingual Transfer Robustness to Lower-Resource Languages on Adversarial Datasets" index=25>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-25 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: High-Resource, Low-Resource, Named Entity Recognition, Named Entity Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20056v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20056v1.pdf filename=2403.20056v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multilingual Language Models (MLLMs) exhibit robust cross-lingual transfer capabilities, or the ability to leverage information acquired in a source language and apply it to a target language. These capabilities find practical applications in well-established Natural Language Processing (NLP) tasks such as <b>Named</b> <b>Entity</b> <b>Recognition</b> <b>(NER).</b> This study aims to investigate the effectiveness of a source language when applied to a target language, particularly in the context of perturbing the input test set. We evaluate on 13 pairs of languages, each including one <b>high-resource</b> language (HRL) and one <b>low-resource</b> language (LRL) with a geographic, genetic, or borrowing relationship. We evaluate two well-known MLLMs&ndash;MBERT and XLM-R&ndash;on these pairs, in native LRL and cross-lingual transfer settings, in two tasks, under a set of different perturbations. Our findings indicate that <b>NER</b> cross-lingual transfer depends largely on the overlap of entity chunks. If a source and target language have more entities in common, the transfer ability is stronger. Models using cross-lingual transfer also appear to be somewhat more robust to certain perturbations of the input, perhaps indicating an ability to leverage stronger representations derived from the HRL. Our research provides valuable insights into cross-lingual transfer and its implications for NLP applications, and underscores the need to consider linguistic nuances and potential limitations when employing MLLMs across distinct languages.</p></p class="citation"></blockquote><h3 id=2642--26204-llava-gemma-accelerating-multimodal-foundation-models-with-a-compact-language-model-musashi-hinck-et-al-2024>(26/42 | 26/204) LLaVA-Gemma: Accelerating Multimodal Foundation Models with a Compact Language Model (Musashi Hinck et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Musashi Hinck, Matthew L. Olson, David Cobbley, Shao-Yen Tseng, Vasudev Lal. (2024)<br><strong>LLaVA-Gemma: Accelerating Multimodal Foundation Models with a Compact Language Model</strong><br><button class=copy-to-clipboard title="LLaVA-Gemma: Accelerating Multimodal Foundation Models with a Compact Language Model" index=26>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-26 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 36<br>Keywords: Foundation Model, Multi-modal, Multi-modal, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01331v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01331v1.pdf filename=2404.01331v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We train a suite of <b>multimodal</b> <b>foundation</b> <b>models</b> (MMFM) using the popular LLaVA framework with the recently released Gemma family of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> Of particular interest is the 2B parameter Gemma model, which provides opportunities to construct capable small-scale MMFMs. In line with findings from other papers in this space, we test the effect of ablating three design features: pretraining the connector, utilizing a more powerful image backbone, and increasing the size of the language backbone. The resulting models, which we call LLaVA-Gemma, exhibit moderate performance on an array of evaluations, but fail to improve past the current comparably sized SOTA models. Closer analysis of performance shows mixed effects; skipping pretraining tends to reduce performance, larger vision models sometimes improve performance, and increasing language model size has inconsistent effects. We publicly release training recipes, code and weights for our models for the LLaVA-Gemma models.</p></p class="citation"></blockquote><h3 id=2742--27204-using-llms-to-model-the-beliefs-and-preferences-of-targeted-populations-keiichi-namikoshi-et-al-2024>(27/42 | 27/204) Using LLMs to Model the Beliefs and Preferences of Targeted Populations (Keiichi Namikoshi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Keiichi Namikoshi, Alex Filipowicz, David A. Shamma, Rumen Iliev, Candice L. Hogan, Nikos Arechiga. (2024)<br><strong>Using LLMs to Model the Beliefs and Preferences of Targeted Populations</strong><br><button class=copy-to-clipboard title="Using LLMs to Model the Beliefs and Preferences of Targeted Populations" index=27>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-27 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, Fine-tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20252v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20252v1.pdf filename=2403.20252v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider the problem of aligning a <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> to model the preferences of a human population. Modeling the beliefs, preferences, and behaviors of a specific population can be useful for a variety of different applications, such as conducting simulated focus groups for new products, conducting virtual surveys, and testing behavioral interventions, especially for interventions that are expensive, impractical, or unethical. Existing work has had mixed success using <b>LLMs</b> to accurately model human behavior in different contexts. We <b>benchmark</b> and evaluate two well-known <b>fine-tuning</b> approaches and evaluate the resulting populations on their ability to match the preferences of real human respondents on a survey of preferences for battery electric vehicles (BEVs). We evaluate our models against their ability to match population-wide statistics as well as their ability to match individual responses, and we investigate the role of temperature in controlling the trade-offs between these two. Additionally, we propose and evaluate a novel loss term to improve model performance on responses that require a numeric response.</p></p class="citation"></blockquote><h3 id=2842--28204-a-systematic-analysis-of-subwords-and-cross-lingual-transfer-in-multilingual-translation-francois-meyer-et-al-2024>(28/42 | 28/204) A Systematic Analysis of Subwords and Cross-Lingual Transfer in Multilingual Translation (Francois Meyer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Francois Meyer, Jan Buys. (2024)<br><strong>A Systematic Analysis of Subwords and Cross-Lingual Transfer in Multilingual Translation</strong><br><button class=copy-to-clipboard title="A Systematic Analysis of Subwords and Cross-Lingual Transfer in Multilingual Translation" index=28>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-28 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Fine-tuning, Low-Resource, Neural Machine Translation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20157v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20157v1.pdf filename=2403.20157v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multilingual modelling can improve <b>machine</b> <b>translation</b> for <b>low-resource</b> languages, partly through shared subword representations. This paper studies the role of subword segmentation in cross-lingual transfer. We systematically compare the efficacy of several subword methods in promoting synergy and preventing interference across different linguistic typologies. Our findings show that subword regularisation boosts synergy in multilingual modelling, whereas BPE more effectively facilitates transfer during cross-lingual <b>fine-tuning.</b> Notably, our results suggest that differences in orthographic word boundary conventions (the morphological granularity of written words) may impede cross-lingual transfer more significantly than linguistic unrelatedness. Our study confirms that decisions around subword modelling can be key to optimising the benefits of multilingual modelling.</p></p class="citation"></blockquote><h3 id=2942--29204-adverb-is-the-key-simple-text-data-augmentation-with-adverb-deletion-juhwan-choi-et-al-2024>(29/42 | 29/204) Adverb Is the Key: Simple Text Data Augmentation with Adverb Deletion (Juhwan Choi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Juhwan Choi, YoungBin Kim. (2024)<br><strong>Adverb Is the Key: Simple Text Data Augmentation with Adverb Deletion</strong><br><button class=copy-to-clipboard title="Adverb Is the Key: Simple Text Data Augmentation with Adverb Deletion" index=29>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-29 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Data Augmentation, Natural Language Inference, Text Classification<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20015v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20015v1.pdf filename=2403.20015v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the field of <b>text</b> <b>data</b> <b>augmentation,</b> rule-based methods are widely adopted for real-world applications owing to their cost-efficiency. However, conventional rule-based approaches suffer from the possibility of losing the original semantics of the given <b>text.</b> <b>We</b> propose a novel <b>text</b> <b>data</b> <b>augmentation</b> strategy that avoids such phenomena through a straightforward deletion of adverbs, which play a subsidiary role in the sentence. Our comprehensive experiments demonstrate the efficiency and effectiveness of our proposed approach for not just single <b>text</b> <b>classification,</b> but also <b>natural</b> <b>language</b> <b>inference</b> that requires semantic preservation. We publicly released our source code for reproducibility.</p></p class="citation"></blockquote><h3 id=3042--30204-realkie-five-novel-datasets-for-enterprise-key-information-extraction-benjamin-townsend-et-al-2024>(30/42 | 30/204) RealKIE: Five Novel Datasets for Enterprise Key Information Extraction (Benjamin Townsend et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Benjamin Townsend, Madison May, Christopher Wells. (2024)<br><strong>RealKIE: Five Novel Datasets for Enterprise Key Information Extraction</strong><br><button class=copy-to-clipboard title="RealKIE: Five Novel Datasets for Enterprise Key Information Extraction" index=30>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-30 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CV, cs-LG, cs.CL<br>Keyword Score: 23<br>Keywords: Optical Character Recognition, Benchmarking, Information Retrieval<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20101v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20101v1.pdf filename=2403.20101v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce RealKIE, a <b>benchmark</b> of five challenging datasets aimed at advancing key <b>information</b> <b>extraction</b> methods, with an emphasis on enterprise applications. The datasets include a diverse range of documents including SEC S1 Filings, US Non-disclosure Agreements, UK Charity Reports, FCC Invoices, and Resource Contracts. Each presents unique challenges: poor text serialization, sparse annotations in long documents, and complex tabular layouts. These datasets provide a realistic testing ground for key <b>information</b> <b>extraction</b> tasks like investment analysis and legal data processing. In addition to presenting these datasets, we offer an in-depth description of the annotation process, document processing techniques, and baseline modeling approaches. This contribution facilitates the development of NLP models capable of handling practical challenges and supports further research into <b>information</b> <b>extraction</b> technologies applicable to industry-specific problems. The annotated data and <b>OCR</b> outputs are available to download at <a href=https://indicodatasolutions.github.io/RealKIE/>https://indicodatasolutions.github.io/RealKIE/</a> code to reproduce the baselines will be available shortly.</p></p class="citation"></blockquote><h3 id=3142--31204-dijiang-efficient-large-language-models-through-compact-kernelization-hanting-chen-et-al-2024>(31/42 | 31/204) DiJiang: Efficient Large Language Models through Compact Kernelization (Hanting Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hanting Chen, Zhicheng Liu, Xutao Wang, Yuchuan Tian, Yunhe Wang. (2024)<br><strong>DiJiang: Efficient Large Language Models through Compact Kernelization</strong><br><button class=copy-to-clipboard title="DiJiang: Efficient Large Language Models through Compact Kernelization" index=31>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-31 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 23<br>Keywords: Benchmarking, Transformer, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19928v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19928v2.pdf filename=2403.19928v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In an effort to reduce the computational load of <b>Transformers,</b> research on linear attention has gained significant momentum. However, the improvement strategies for attention mechanisms typically necessitate extensive retraining, which is impractical for <b>large</b> <b>language</b> <b>models</b> with a vast array of parameters. In this paper, we present DiJiang, a novel Frequency Domain Kernelization approach that enables the transformation of a pre-trained vanilla <b>Transformer</b> into a linear complexity model with little training costs. By employing a weighted Quasi-Monte Carlo method for sampling, the proposed approach theoretically offers superior approximation efficiency. To further reduce the training computational complexity, our kernelization is based on Discrete Cosine Transform (DCT) operations. Extensive experiments demonstrate that the proposed method achieves comparable performance to the original <b>Transformer,</b> but with significantly reduced training costs and much faster inference speeds. Our DiJiang-7B achieves comparable performance with LLaMA2-7B on various <b>benchmark</b> while requires only about 1/50 training cost. Code is available at <a href=https://github.com/YuchuanTian/DiJiang>https://github.com/YuchuanTian/DiJiang</a>.</p></p class="citation"></blockquote><h3 id=3242--32204-where-are-you-from-let-me-guess-subdialect-recognition-of-speeches-in-sorani-kurdish-sana-isam-et-al-2024>(32/42 | 32/204) Where Are You From? Let Me Guess! Subdialect Recognition of Speeches in Sorani Kurdish (Sana Isam et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sana Isam, Hossein Hassani. (2024)<br><strong>Where Are You From? Let Me Guess! Subdialect Recognition of Speeches in Sorani Kurdish</strong><br><button class=copy-to-clipboard title="Where Are You From? Let Me Guess! Subdialect Recognition of Speeches in Sorani Kurdish" index=32>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-32 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-SD, cs.CL, eess-AS<br>Keyword Score: 20<br>Keywords: Convolutional Neural Network, Recurrent Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00124v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00124v1.pdf filename=2404.00124v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Classifying Sorani Kurdish subdialects poses a challenge due to the need for publicly available datasets or reliable resources like social media or websites for data collection. We conducted field visits to various cities and villages to address this issue, connecting with native speakers from different age groups, genders, academic backgrounds, and professions. We recorded their voices while engaging in conversations covering diverse topics such as lifestyle, background history, hobbies, interests, vacations, and life lessons. The target area of the research was the Kurdistan Region of Iraq. As a result, we accumulated 29 hours, 16 minutes, and 40 seconds of audio recordings from 107 interviews, constituting an unbalanced dataset encompassing six subdialects. Subsequently, we adapted three deep learning models: ANN, <b>CNN,</b> and <b>RNN-LSTM.</b> We explored various configurations, including different track durations, dataset splitting, and imbalanced dataset handling techniques such as oversampling and undersampling. Two hundred and twenty-five(225) experiments were conducted, and the outcomes were evaluated. The results indicated that the <b>RNN-LSTM</b> outperforms the other methods by achieving an accuracy of 96%. <b>CNN</b> achieved an accuracy of 93%, and ANN 75%. All three models demonstrated improved performance when applied to balanced datasets, primarily when we followed the oversampling approach. Future studies can explore additional future research directions to include other Kurdish dialects.</p></p class="citation"></blockquote><h3 id=3342--33204-emotion-anchored-contrastive-learning-framework-for-emotion-recognition-in-conversation-fangxu-yu-et-al-2024>(33/42 | 33/204) Emotion-Anchored Contrastive Learning Framework for Emotion Recognition in Conversation (Fangxu Yu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fangxu Yu, Junjie Guo, Zhen Wu, Xinyu Dai. (2024)<br><strong>Emotion-Anchored Contrastive Learning Framework for Emotion Recognition in Conversation</strong><br><button class=copy-to-clipboard title="Emotion-Anchored Contrastive Learning Framework for Emotion Recognition in Conversation" index=33>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-33 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-SD, cs.CL, eess-AS<br>Keyword Score: 20<br>Keywords: Contrastive Learning, Emotion Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20289v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20289v1.pdf filename=2403.20289v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Emotion</b> <b>Recognition</b> in Conversation (ERC) involves detecting the underlying <b>emotion</b> <b>behind</b> each utterance within a conversation. Effectively generating representations for utterances remains a significant challenge in this task. Recent works propose various models to address this issue, but they still struggle with differentiating similar <b>emotions</b> <b>such</b> as excitement and happiness. To alleviate this problem, We propose an <b>Emotion-Anchored</b> <b>Contrastive</b> <b>Learning</b> (EACL) framework that can generate more distinguishable utterance representations for similar <b>emotions.</b> <b>To</b> achieve this, we utilize label encodings as anchors to guide the learning of utterance representations and design an auxiliary loss to ensure the effective separation of anchors for similar <b>emotions.</b> <b>Moreover,</b> an additional adaptation process is proposed to adapt anchors to serve as effective classifiers to improve classification performance. Across extensive experiments, our proposed EACL achieves state-of-the-art <b>emotion</b> <b>recognition</b> performance and exhibits superior performance on similar <b>emotions.</b> <b>Our</b> code is available at <a href=https://github.com/Yu-Fangxu/EACL>https://github.com/Yu-Fangxu/EACL</a>.</p></p class="citation"></blockquote><h3 id=3442--34204-an-efficient-approach-for-studying-cross-lingual-transfer-in-multilingual-language-models-fahim-faisal-et-al-2024>(34/42 | 34/204) An Efficient Approach for Studying Cross-Lingual Transfer in Multilingual Language Models (Fahim Faisal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fahim Faisal, Antonios Anastasopoulos. (2024)<br><strong>An Efficient Approach for Studying Cross-Lingual Transfer in Multilingual Language Models</strong><br><button class=copy-to-clipboard title="An Efficient Approach for Studying Cross-Lingual Transfer in Multilingual Language Models" index=34>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-34 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Zero-shot, Masked Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20088v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20088v1.pdf filename=2403.20088v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The capacity and effectiveness of pre-trained multilingual models <b>(MLMs)</b> for <b>zero-shot</b> cross-lingual transfer is well established. However, phenomena of positive or negative transfer, and the effect of language choice still need to be fully understood, especially in the complex setting of massively multilingual LMs. We propose an \textit{efficient} method to study transfer language influence in <b>zero-shot</b> performance on another target language. Unlike previous work, our approach disentangles downstream tasks from language, using dedicated adapter units. Our findings suggest that some languages do not largely affect others, while some languages, especially ones unseen during pre-training, can be extremely beneficial or detrimental for different target languages. We find that no transfer language is beneficial for all target languages. We do, curiously, observe languages previously unseen by <b>MLMs</b> consistently benefit from transfer from almost any language. We additionally use our modular approach to quantify negative interference efficiently and categorize languages accordingly. Furthermore, we provide a list of promising transfer-target language configurations that consistently lead to target language performance improvements. Code and data are publicly available: <a href=https://github.com/ffaisal93/neg_inf>https://github.com/ffaisal93/neg_inf</a></p></p class="citation"></blockquote><h3 id=3542--35204-on-large-language-models-hallucination-with-regard-to-known-facts-che-jiang-et-al-2024>(35/42 | 35/204) On Large Language Models&rsquo; Hallucination with Regard to Known Facts (Che Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Che Jiang, Biqing Qi, Xiangyu Hong, Dayuan Fu, Yang Cheng, Fandong Meng, Mo Yu, Bowen Zhou, Jie Zhou. (2024)<br><strong>On Large Language Models&rsquo; Hallucination with Regard to Known Facts</strong><br><button class=copy-to-clipboard title="On Large Language Models' Hallucination with Regard to Known Facts" index=35>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-35 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20009v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20009v1.pdf filename=2403.20009v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> are successful in answering factoid questions but are also prone to hallucination.We investigate the phenomenon of <b>LLMs</b> possessing correct answer knowledge yet still hallucinating from the perspective of inference dynamics, an area not previously covered in studies on hallucinations.We are able to conduct this analysis via two key ideas.First, we identify the factual questions that query the same triplet knowledge but result in different answers. The difference between the model behaviors on the correct and incorrect outputs hence suggests the patterns when hallucinations happen. Second, to measure the pattern, we utilize mappings from the residual streams to vocabulary space. We reveal the different dynamics of the output token probabilities along the depths of layers between the correct and hallucinated cases. In hallucinated cases, the output token&rsquo;s information rarely demonstrates abrupt increases and consistent superiority in the later stages of the model. Leveraging the dynamic curve as a feature, we build a classifier capable of accurately detecting hallucinatory predictions with an 88% success rate. Our study shed light on understanding the reasons for <b>LLMs&rsquo;</b> hallucinations on their known facts, and more importantly, on accurately predicting when they are hallucinating.</p></p class="citation"></blockquote><h3 id=3642--36204-individual-text-corpora-predict-openness-interests-knowledge-and-level-of-education-markus-j-hofmann-et-al-2024>(36/42 | 36/204) Individual Text Corpora Predict Openness, Interests, Knowledge and Level of Education (Markus J. Hofmann et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Markus J. Hofmann, Markus T. Jansen, Christoph Wigbels, Benny Briesemeister, Arthur M. Jacobs. (2024)<br><strong>Individual Text Corpora Predict Openness, Interests, Knowledge and Level of Education</strong><br><button class=copy-to-clipboard title="Individual Text Corpora Predict Openness, Interests, Knowledge and Level of Education" index=36>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-36 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 10<br>Keywords: Word2vec<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00165v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00165v1.pdf filename=2404.00165v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Here we examine whether the personality dimension of openness to experience can be predicted from the individual google search history. By web scraping, individual text corpora (ICs) were generated from 214 participants with a mean number of 5 million word tokens. We trained <b>word2vec</b> models and used the similarities of each IC to label words, which were derived from a lexical approach of personality. These IC-label-word similarities were utilized as predictive features in neural models. For training and validation, we relied on 179 participants and held out a test sample of 35 participants. A grid search with varying number of predictive features, hidden units and boost factor was performed. As model selection criterion, we used R2 in the validation samples penalized by the absolute R2 difference between training and validation. The selected neural model explained 35% of the openness variance in the test sample, while an ensemble model with the same architecture often provided slightly more stable predictions for intellectual interests, knowledge in humanities and level of education. Finally, a learning curve analysis suggested that around 500 training participants are required for generalizable predictions. We discuss ICs as a complement or replacement of survey-based psychodiagnostics.</p></p class="citation"></blockquote><h3 id=3742--37204-towards-a-framework-for-evaluating-explanations-in-automated-fact-verification-neema-kotonya-et-al-2024>(37/42 | 37/204) Towards a Framework for Evaluating Explanations in Automated Fact Verification (Neema Kotonya et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Neema Kotonya, Francesca Toni. (2024)<br><strong>Towards a Framework for Evaluating Explanations in Automated Fact Verification</strong><br><button class=copy-to-clipboard title="Towards a Framework for Evaluating Explanations in Automated Fact Verification" index=37>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-37 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Fact Verification<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20322v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20322v1.pdf filename=2403.20322v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As deep neural models in NLP become more complex, and as a consequence opaque, the necessity to interpret them becomes greater. A burgeoning interest has emerged in rationalizing explanations to provide short and coherent justifications for predictions. In this position paper, we advocate for a formal framework for key concepts and properties about rationalizing explanations to support their evaluation systematically. We also outline one such formal framework, tailored to rationalizing explanations of increasingly complex structures, from free-form explanations to deductive explanations, to argumentative explanations (with the richest structure). Focusing on the automated <b>fact</b> <b>verification</b> task, we provide illustrations of the use and usefulness of our formalization for evaluating explanations, tailored to their varying structures.</p></p class="citation"></blockquote><h3 id=3842--38204-automatic-alignment-of-discourse-relations-of-different-discourse-annotation-frameworks-yingxue-fu-2024>(38/42 | 38/204) Automatic Alignment of Discourse Relations of Different Discourse Annotation Frameworks (Yingxue Fu, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yingxue Fu. (2024)<br><strong>Automatic Alignment of Discourse Relations of Different Discourse Annotation Frameworks</strong><br><button class=copy-to-clipboard title="Automatic Alignment of Discourse Relations of Different Discourse Annotation Frameworks" index=38>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-38 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Contrastive Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20196v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20196v1.pdf filename=2403.20196v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing discourse corpora are annotated based on different frameworks, which show significant dissimilarities in definitions of arguments and relations and structural constraints. Despite surface differences, these frameworks share basic understandings of discourse relations. The relationship between these frameworks has been an open research question, especially the correlation between relation inventories utilized in different frameworks. Better understanding of this question is helpful for integrating discourse theories and enabling interoperability of discourse corpora annotated under different frameworks. However, studies that explore correlations between discourse relation inventories are hindered by different criteria of discourse segmentation, and expert knowledge and manual examination are typically needed. Some semi-automatic methods have been proposed, but they rely on corpora annotated in multiple frameworks in parallel. In this paper, we introduce a fully automatic approach to address the challenges. Specifically, we extend the label-anchored <b>contrastive</b> <b>learning</b> method introduced by Zhang et al. (2022b) to learn label embeddings during a classification task. These embeddings are then utilized to map discourse relations from different frameworks. We show experimental results on RST-DT (Carlson et al., 2001) and PDTB 3.0 (Prasad et al., 2018).</p></p class="citation"></blockquote><h3 id=3942--39204-user-modeling-challenges-in-interactive-ai-assistant-systems-megan-su-et-al-2024>(39/42 | 39/204) User Modeling Challenges in Interactive AI Assistant Systems (Megan Su et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Megan Su, Yuwei Bao. (2024)<br><strong>User Modeling Challenges in Interactive AI Assistant Systems</strong><br><button class=copy-to-clipboard title="User Modeling Challenges in Interactive AI Assistant Systems" index=39>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-39 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20134v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20134v1.pdf filename=2403.20134v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Interactive Artificial Intelligent(AI) assistant systems are designed to offer timely guidance to help human users to complete a variety tasks. One of the remaining challenges is to understand user&rsquo;s mental states during the task for more personalized guidance. In this work, we analyze users&rsquo; mental states during task executions and investigate the capabilities and challenges for <b>large</b> <b>language</b> <b>models</b> to interpret user profiles for more personalized user guidance.</p></p class="citation"></blockquote><h3 id=4042--40204-the-lscd-benchmark-a-testbed-for-diachronic-word-meaning-tasks-dominik-schlechtweg-et-al-2024>(40/42 | 40/204) The LSCD Benchmark: a Testbed for Diachronic Word Meaning Tasks (Dominik Schlechtweg et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dominik Schlechtweg, Shafqat Mumtaz Virk, Nikolay Arefyev. (2024)<br><strong>The LSCD Benchmark: a Testbed for Diachronic Word Meaning Tasks</strong><br><button class=copy-to-clipboard title="The LSCD Benchmark: a Testbed for Diachronic Word Meaning Tasks" index=40>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-40 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 6<br>Keywords: Graph, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00176v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00176v1.pdf filename=2404.00176v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Lexical Semantic Change Detection (LSCD) is a complex, lemma-level task, which is usually operationalized based on two subsequently applied usage-level tasks: First, Word-in-Context (WiC) labels are derived for pairs of usages. Then, these labels are represented in a <b>graph</b> on which Word Sense Induction (WSI) is applied to derive sense clusters. Finally, LSCD labels are derived by comparing sense clusters over time. This modularity is reflected in most LSCD datasets and models. It also leads to a large heterogeneity in modeling options and task definitions, which is exacerbated by a variety of dataset versions, preprocessing options and evaluation metrics. This heterogeneity makes it difficult to evaluate models under comparable conditions, to choose optimal model combinations or to reproduce results. Hence, we provide a <b>benchmark</b> repository standardizing LSCD evaluation. Through transparent implementation results become easily reproducible and by standardization different components can be freely combined. The repository reflects the task&rsquo;s modularity by allowing model evaluation for WiC, WSI and LSCD. This allows for careful evaluation of increasingly complex model components providing new ways of model optimization.</p></p class="citation"></blockquote><h3 id=4142--41204-ipa-transcription-of-bengali-texts-kanij-fatema-et-al-2024>(41/42 | 41/204) IPA Transcription of Bengali Texts (Kanij Fatema et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kanij Fatema, Fazle Dawood Haider, Nirzona Ferdousi Turpa, Tanveer Azmal, Sourav Ahmed, Navid Hasan, Mohammad Akhlaqur Rahman, Biplab Kumar Sarkar, Afrar Jahin, Md. Rezuwan Hassan, Md Foriduzzaman Zihad, Rubayet Sabbir Faruque, Asif Sushmit, Mashrur Imtiaz, Farig Sadeque, Syed Shahrier Rahman. (2024)<br><strong>IPA Transcription of Bengali Texts</strong><br><button class=copy-to-clipboard title="IPA Transcription of Bengali Texts" index=41>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-41 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20084v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20084v1.pdf filename=2403.20084v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The International Phonetic Alphabet (IPA) serves to systematize phonemes in language, enabling precise textual representation of pronunciation. In Bengali phonology and phonetics, ongoing scholarly deliberations persist concerning the IPA standard and core Bengali phonemes. This work examines prior research, identifies current and potential issues, and suggests a framework for a Bengali IPA standard, facilitating linguistic analysis and NLP resource creation and downstream technology development. In this work, we present a comprehensive study of Bengali IPA transcription and introduce a novel IPA transcription framework incorporating a novel dataset with DL-based <b>benchmarks.</b></p></p class="citation"></blockquote><h3 id=4242--42204-slfnet-generating-semantic-logic-forms-from-natural-language-using-semantic-probability-graphs-hao-wu-et-al-2024>(42/42 | 42/204) SLFNet: Generating Semantic Logic Forms from Natural Language Using Semantic Probability Graphs (Hao Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hao Wu, Fan Xu. (2024)<br><strong>SLFNet: Generating Semantic Logic Forms from Natural Language Using Semantic Probability Graphs</strong><br><button class=copy-to-clipboard title="SLFNet: Generating Semantic Logic Forms from Natural Language Using Semantic Probability Graphs" index=42>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-42 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19936v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19936v1.pdf filename=2403.19936v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Building natural language interfaces typically uses a semantic parser to parse the user&rsquo;s natural language and convert it into structured \textbf{S}emantic \textbf{L}ogic \textbf{F}orms (SLFs). The mainstream approach is to adopt a sequence-to-sequence framework, which requires that natural language commands and SLFs must be represented serially. Since a single natural language may have multiple SLFs or multiple natural language commands may have the same SLF, training a sequence-to-sequence model is sensitive to the choice among them, a phenomenon recorded as &ldquo;order matters&rdquo;. To solve this problem, we propose a novel neural network, SLFNet, which firstly incorporates dependent syntactic information as prior knowledge and can capture the long-range interactions between contextual information and words. Secondly construct semantic probability <b>graphs</b> to obtain local dependencies between predictor variables. Finally we propose the Multi-Head SLF Attention mechanism to synthesize SLFs from natural language commands based on Sequence-to-Slots. Experiments show that SLFNet achieves state-of-the-art performance on the ChineseQCI-TS and Okapi datasets, and competitive performance on the ATIS dataset.</p></p class="citation"></blockquote><h2 id=cscv-68>cs.CV (68)</h2><h3 id=168--43204-uncovering-bias-in-large-vision-language-models-with-counterfactuals-phillip-howard-et-al-2024>(1/68 | 43/204) Uncovering Bias in Large Vision-Language Models with Counterfactuals (Phillip Howard et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Phillip Howard, Anahita Bhiwandiwalla, Kathleen C. Fraser, Svetlana Kiritchenko. (2024)<br><strong>Uncovering Bias in Large Vision-Language Models with Counterfactuals</strong><br><button class=copy-to-clipboard title="Uncovering Bias in Large Vision-Language Models with Counterfactuals" index=43>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-43 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 76<br>Keywords: Counter-factual, Multi-modal, Multi-modal, Question Answering, Visual Question Answering, Large Language Model, Large Language Model, Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00166v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00166v1.pdf filename=2404.00166v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the advent of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> possessing increasingly impressive capabilities, a number of <b>Large</b> <b>Vision-Language</b> <b>Models</b> (LVLMs) have been proposed to augment <b>LLMs</b> with <b>visual</b> <b>inputs.</b> <b>Such</b> models condition generated text on both an input image and a text <b>prompt,</b> enabling a variety of use cases such as <b>visual</b> <b>question</b> <b>answering</b> and <b>multimodal</b> chat. While prior studies have examined the social biases contained in text generated by <b>LLMs,</b> this topic has been relatively unexplored in LVLMs. Examining social biases in LVLMs is particularly challenging due to the confounding contributions of bias induced by information contained across the text and <b>visual</b> <b>modalities.</b> <b>To</b> address this challenging problem, we conduct a <b>large-scale</b> <b>study</b> <b>of</b> text generated by different LVLMs under <b>counterfactual</b> changes to input images. Specifically, we present LVLMs with identical open-ended text <b>prompts</b> while conditioning on images from different <b>counterfactual</b> sets, where each set contains images which are largely identical in their depiction of a common subject (e.g., a doctor), but vary only in terms of intersectional social attributes (e.g., race and gender). We comprehensively evaluate the text produced by different LVLMs under this <b>counterfactual</b> generation setting and find that social attributes such as race, gender, and physical characteristics depicted in input images can significantly influence toxicity and the generation of competency-associated words.</p></p class="citation"></blockquote><h3 id=268--44204-relation-rectification-in-diffusion-model-yinwei-wu-et-al-2024>(2/68 | 44/204) Relation Rectification in Diffusion Model (Yinwei Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yinwei Wu, Xingyi Yang, Xinchao Wang. (2024)<br><strong>Relation Rectification in Diffusion Model</strong><br><button class=copy-to-clipboard title="Relation Rectification in Diffusion Model" index=44>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-44 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 73<br>Keywords: Diffusion Model, Graph Convolutional Network, Graph, Convolution, Convolutional Neural Network, Text2image, Prompt, Text Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20249v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20249v1.pdf filename=2403.20249v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite their exceptional generative abilities, large <b>text-to-image</b> <b>diffusion</b> <b>models,</b> much like skilled but careless artists, often struggle with accurately depicting visual relationships between objects. This issue, as we uncover through careful analysis, arises from a misaligned <b>text</b> <b>encoder</b> that struggles to interpret specific relationships and differentiate the logical order of associated objects. To resolve this, we introduce a novel task termed Relation Rectification, aiming to refine the model to accurately represent a given relationship it initially fails to generate. To address this, we propose an innovative solution utilizing a Heterogeneous <b>Graph</b> <b>Convolutional</b> <b>Network</b> (HGCN). It models the directional relationships between relation terms and corresponding objects within the input <b>prompts.</b> Specifically, we optimize the HGCN on a pair of <b>prompts</b> with identical relational words but reversed object orders, supplemented by a few reference images. The lightweight HGCN adjusts the <b>text</b> <b>embeddings</b> generated by the <b>text</b> <b>encoder,</b> ensuring the accurate reflection of the textual relation in the embedding space. Crucially, our method retains the parameters of the <b>text</b> <b>encoder</b> and <b>diffusion</b> <b>model,</b> preserving the model&rsquo;s robust performance on unrelated descriptions. We validated our approach on a newly curated dataset of diverse relational data, demonstrating both quantitative and qualitative enhancements in generating images with precise visual relations. Project page: <a href=https://wuyinwei-hah.github.io/rrnet.github.io/>https://wuyinwei-hah.github.io/rrnet.github.io/</a>.</p></p class="citation"></blockquote><h3 id=368--45204-convolutional-prompting-meets-language-models-for-continual-learning-anurag-roy-et-al-2024>(3/68 | 45/204) Convolutional Prompting meets Language Models for Continual Learning (Anurag Roy et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anurag Roy, Riddhiman Moulick, Vinay K. Verma, Saptarshi Ghosh, Abir Das. (2024)<br><strong>Convolutional Prompting meets Language Models for Continual Learning</strong><br><button class=copy-to-clipboard title="Convolutional Prompting meets Language Models for Continual Learning" index=45>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-45 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 70<br>Keywords: Vision Transformer, Continual Learning, Convolution, Transformer, Large Language Model, Prompt, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20317v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20317v1.pdf filename=2403.20317v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Continual</b> <b>Learning</b> (CL) enables machine learning models to learn from continuously shifting new training data in absence of data from old tasks. Recently, pretrained <b>vision</b> <b>transformers</b> combined with <b>prompt</b> tuning have shown promise for overcoming catastrophic forgetting in CL. These approaches rely on a pool of learnable <b>prompts</b> which can be inefficient in sharing knowledge across tasks leading to inferior performance. In addition, the lack of fine-grained layer specific <b>prompts</b> does not allow these to fully express the strength of the <b>prompts</b> for CL. We address these limitations by proposing ConvPrompt, a novel <b>convolutional</b> <b>prompt</b> creation mechanism that maintains layer-wise shared embeddings, enabling both layer-specific learning and better concept transfer across tasks. The intelligent use of <b>convolution</b> enables us to maintain a low parameter overhead without compromising performance. We further leverage <b>Large</b> <b>Language</b> <b>Models</b> to generate fine-grained text descriptions of each category which are used to get task similarity and dynamically decide the number of <b>prompts</b> to be learned. Extensive experiments demonstrate the superiority of ConvPrompt and improves SOTA by ~3% with significantly less parameter overhead. We also perform strong ablation over various modules to disentangle the importance of different components.</p></p class="citation"></blockquote><h3 id=468--46204-draw-and-understand-leveraging-visual-prompts-to-enable-mllms-to-comprehend-what-you-want-weifeng-lin-et-al-2024>(4/68 | 46/204) Draw-and-Understand: Leveraging Visual Prompts to Enable MLLMs to Comprehend What You Want (Weifeng Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Weifeng Lin, Xinyu Wei, Ruichuan An, Peng Gao, Bocheng Zou, Yulin Luo, Siyuan Huang, Shanghang Zhang, Hongsheng Li. (2024)<br><strong>Draw-and-Understand: Leveraging Visual Prompts to Enable MLLMs to Comprehend What You Want</strong><br><button class=copy-to-clipboard title="Draw-and-Understand: Leveraging Visual Prompts to Enable MLLMs to Comprehend What You Want" index=46>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-46 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 69<br>Keywords: Optical Character Recognition, Benchmarking, Multi-modal, Multi-modal, Instruction Following, Question Answering, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20271v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20271v2.pdf filename=2403.20271v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The interaction between humans and artificial intelligence (AI) is a crucial factor that reflects the effectiveness of <b>multimodal</b> <b>large</b> <b>language</b> <b>models</b> (MLLMs). However, current MLLMs primarily focus on image-level comprehension and limit interaction to textual <b>instructions,</b> <b>thereby</b> constraining their flexibility in usage and depth of response. In this paper, we introduce the Draw-and-Understand project: a new model, a multi-domain dataset, and a challenging <b>benchmark</b> for visual <b>prompting.</b> Specifically, we propose SPHINX-V, a new end-to-end trained <b>Multimodal</b> <b>Large</b> <b>Language</b> <b>Model</b> (MLLM) that connects a vision encoder, a visual <b>prompt</b> encoder and an <b>LLM</b> for various visual <b>prompts</b> (points, bounding boxes, and free-form shape) and language understanding. To advance visual <b>prompting</b> research for MLLMs, we introduce MDVP-Data and MDVP-Bench. MDVP-Data features a multi-domain dataset containing 1.6M unique image-visual <b>prompt-text</b> <b>instruction-following</b> <b>samples,</b> including natural images, document images, <b>OCR</b> images, mobile screenshots, web screenshots, and multi-panel images. Furthermore, we present MDVP-Bench, a comprehensive and challenging <b>benchmark</b> to assess a model&rsquo;s capability in understanding visual <b>prompting</b> <b>instructions.</b> <b>Our</b> experiments demonstrate SPHINX-V&rsquo;s impressive <b>multimodal</b> interaction capabilities through visual <b>prompting,</b> revealing significant improvements in detailed pixel-level description and <b>question-answering</b> <b>abilities.</b></p></p class="citation"></blockquote><h3 id=568--47204-medclip-sam-bridging-text-and-image-towards-universal-medical-image-segmentation-taha-koleilat-et-al-2024>(5/68 | 47/204) MedCLIP-SAM: Bridging Text and Image Towards Universal Medical Image Segmentation (Taha Koleilat et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Taha Koleilat, Hojat Asgariandehkordi, Hassan Rivaz, Yiming Xiao. (2024)<br><strong>MedCLIP-SAM: Bridging Text and Image Towards Universal Medical Image Segmentation</strong><br><button class=copy-to-clipboard title="MedCLIP-SAM: Bridging Text and Image Towards Universal Medical Image Segmentation" index=47>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-47 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 60<br>Keywords: Fine-tuning, Foundation Model, Supervised Learning, Weakly-supervised Learning, Zero-shot, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20253v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20253v1.pdf filename=2403.20253v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Medical image segmentation of anatomical structures and pathology is crucial in modern clinical diagnosis, disease study, and treatment planning. To date, great progress has been made in deep learning-based segmentation techniques, but most methods still lack data efficiency, generalizability, and interactability. Consequently, the development of new, precise segmentation methods that demand fewer labeled datasets is of utmost importance in medical image analysis. Recently, the emergence of <b>foundation</b> <b>models,</b> such as CLIP and Segment-Anything-Model (SAM), with comprehensive cross-domain representation opened the door for interactive and universal image segmentation. However, exploration of these models for data-efficient medical image segmentation is still limited, but is highly necessary. In this paper, we propose a novel framework, called MedCLIP-SAM that combines CLIP and SAM models to generate segmentation of clinical scans using text <b>prompts</b> in both <b>zero-shot</b> and weakly <b>supervised</b> settings. To achieve this, we employed a new Decoupled Hard Negative Noise Contrastive Estimation (DHN-NCE) loss to <b>fine-tune</b> the BiomedCLIP model and the recent gScoreCAM to generate <b>prompts</b> to obtain segmentation masks from SAM in a <b>zero-shot</b> setting. Additionally, we explored the use of <b>zero-shot</b> segmentation labels in a weakly <b>supervised</b> paradigm to improve the segmentation quality further. By extensively testing three diverse segmentation tasks and medical image modalities (breast tumor ultrasound, brain tumor MRI, and lung X-ray), our proposed framework has demonstrated excellent accuracy.</p></p class="citation"></blockquote><h3 id=668--48204-fairrag-fair-human-generation-via-fair-retrieval-augmentation-robik-shrestha-et-al-2024>(6/68 | 48/204) FairRAG: Fair Human Generation via Fair Retrieval Augmentation (Robik Shrestha et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Robik Shrestha, Yang Zou, Qiuyu Chen, Zhiheng Li, Yusheng Xie, Siqi Deng. (2024)<br><strong>FairRAG: Fair Human Generation via Fair Retrieval Augmentation</strong><br><button class=copy-to-clipboard title="FairRAG: Fair Human Generation via Fair Retrieval Augmentation" index=48>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-48 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-CY, cs-LG, cs.CV<br>Keyword Score: 60<br>Keywords: Fairness, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Image2text, Text2image, Retrieval Augmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19964v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19964v2.pdf filename=2403.19964v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing <b>text-to-image</b> generative models reflect or even amplify societal biases ingrained in their training data. This is especially concerning for human image generation where models are biased against certain demographic groups. Existing attempts to rectify this issue are hindered by the inherent limitations of the pre-trained models and fail to substantially improve demographic diversity. In this work, we introduce Fair <b>Retrieval</b> <b>Augmented</b> <b>Generation</b> (FairRAG), a novel framework that conditions pre-trained generative models on reference images retrieved from an external image database to improve <b>fairness</b> in human generation. FairRAG enables conditioning through a lightweight linear module that projects reference images into the textual space. To enhance <b>fairness,</b> FairRAG applies simple-yet-effective debiasing strategies, providing images from diverse demographic groups during the generative process. Extensive experiments demonstrate that FairRAG outperforms existing methods in terms of demographic diversity, <b>image-text</b> alignment, and image fidelity while incurring minimal computational overhead during inference.</p></p class="citation"></blockquote><h3 id=768--49204-unsolvable-problem-detection-evaluating-trustworthiness-of-vision-language-models-atsuyuki-miyai-et-al-2024>(7/68 | 49/204) Unsolvable Problem Detection: Evaluating Trustworthiness of Vision Language Models (Atsuyuki Miyai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Atsuyuki Miyai, Jingkang Yang, Jingyang Zhang, Yifei Ming, Qing Yu, Go Irie, Yixuan Li, Hai Li, Ziwei Liu, Kiyoharu Aizawa. (2024)<br><strong>Unsolvable Problem Detection: Evaluating Trustworthiness of Vision Language Models</strong><br><button class=copy-to-clipboard title="Unsolvable Problem Detection: Evaluating Trustworthiness of Vision Language Models" index=49>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-49 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CL, cs-CV, cs-LG, cs.CV<br>Keyword Score: 53<br>Keywords: Benchmarking, GPT, Question Answering, Visual Question Answering, Visual Question Answering, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20331v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20331v1.pdf filename=2403.20331v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces a novel and significant challenge for Vision Language Models (VLMs), termed Unsolvable Problem Detection (UPD). UPD examines the VLM&rsquo;s ability to withhold answers when faced with unsolvable problems in the context of <b>Visual</b> <b>Question</b> <b>Answering</b> <b>(VQA)</b> tasks. UPD encompasses three distinct settings: Absent Answer Detection (AAD), Incompatible Answer Set Detection (IASD), and Incompatible <b>Visual</b> <b>Question</b> <b>Detection</b> (IVQD). To deeply investigate the UPD problem, extensive experiments indicate that most VLMs, including <b>GPT-4V</b> and LLaVA-Next-34B, struggle with our <b>benchmarks</b> to varying extents, highlighting significant room for the improvements. To address UPD, we explore both training-free and training-based solutions, offering new insights into their effectiveness and limitations. We hope our insights, together with future efforts within the proposed UPD settings, will enhance the broader understanding and development of more practical and reliable VLMs.</p></p class="citation"></blockquote><h3 id=868--50204-learn-no-to-say-yes-better-improving-vision-language-models-via-negations-jaisidh-singh-et-al-2024>(8/68 | 50/204) Learn &lsquo;No&rsquo; to Say &lsquo;Yes&rsquo; Better: Improving Vision-Language Models via Negations (Jaisidh Singh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jaisidh Singh, Ishaan Shrivastava, Mayank Vatsa, Richa Singh, Aparna Bharati. (2024)<br><strong>Learn &lsquo;No&rsquo; to Say &lsquo;Yes&rsquo; Better: Improving Vision-Language Models via Negations</strong><br><button class=copy-to-clipboard title="Learn 'No' to Say 'Yes' Better: Improving Vision-Language Models via Negations" index=50>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-50 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 53<br>Keywords: Benchmarking, Foundation Model, Zero-shot, Reasoning, Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20312v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20312v1.pdf filename=2403.20312v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing <b>vision-language</b> models (VLMs) treat text descriptions as a unit, confusing individual concepts in a <b>prompt</b> and impairing visual semantic matching and <b>reasoning.</b> An important aspect of <b>reasoning</b> in logic and language is negations. This paper highlights the limitations of popular VLMs such as CLIP, at understanding the implications of negations, i.e., the effect of the word &ldquo;not&rdquo; in a given <b>prompt.</b> To enable evaluation of VLMs on fluent <b>prompts</b> with negations, we present CC-Neg, a dataset containing 228,246 images, true captions and their corresponding negated captions. Using CC-Neg along with modifications to the contrastive loss of CLIP, our proposed CoN-CLIP framework, has an improved understanding of negations. This training paradigm improves CoN-CLIP&rsquo;s ability to encode semantics reliably, resulting in 3.85% average gain in top-1 accuracy for <b>zero-shot</b> image classification across 8 datasets. Further, CoN-CLIP outperforms CLIP on challenging compositionality <b>benchmarks</b> such as SugarCREPE by 4.4%, showcasing emergent compositional understanding of objects, relations, and attributes in text. Overall, our work addresses a crucial limitation of VLMs by introducing a dataset and framework that strengthens semantic associations between images and text, demonstrating improved large-scale <b>foundation</b> <b>models</b> with significantly reduced computational cost, promoting efficiency and accessibility.</p></p class="citation"></blockquote><h3 id=968--51204-eclipse-efficient-continual-learning-in-panoptic-segmentation-with-visual-prompt-tuning-beomyoung-kim-et-al-2024>(9/68 | 51/204) ECLIPSE: Efficient Continual Learning in Panoptic Segmentation with Visual Prompt Tuning (Beomyoung Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Beomyoung Kim, Joonsang Yu, Sung Ju Hwang. (2024)<br><strong>ECLIPSE: Efficient Continual Learning in Panoptic Segmentation with Visual Prompt Tuning</strong><br><button class=copy-to-clipboard title="ECLIPSE: Efficient Continual Learning in Panoptic Segmentation with Visual Prompt Tuning" index=51>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-51 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 53<br>Keywords: Benchmarking, Continual Learning, Fine-tuning, Knowledge Distillation, Knowledge Distillation, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20126v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20126v1.pdf filename=2403.20126v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Panoptic segmentation, combining semantic and instance segmentation, stands as a cutting-edge computer vision task. Despite recent progress with deep learning models, the dynamic nature of real-world applications necessitates <b>continual</b> <b>learning,</b> where models adapt to new classes (plasticity) over time without forgetting old ones (catastrophic forgetting). Current <b>continual</b> <b>segmentation</b> methods often rely on <b>distillation</b> strategies like <b>knowledge</b> <b>distillation</b> and pseudo-labeling, which are effective but result in increased training complexity and computational overhead. In this paper, we introduce a novel and efficient method for <b>continual</b> <b>panoptic</b> segmentation based on Visual <b>Prompt</b> Tuning, dubbed ECLIPSE. Our approach involves freezing the base model parameters and <b>fine-tuning</b> only a small set of <b>prompt</b> embeddings, addressing both catastrophic forgetting and plasticity and significantly reducing the trainable parameters. To mitigate inherent challenges such as error propagation and semantic drift in <b>continual</b> <b>segmentation,</b> we propose logit manipulation to effectively leverage common <b>knowledge</b> <b>across</b> the classes. Experiments on ADE20K <b>continual</b> <b>panoptic</b> segmentation <b>benchmark</b> demonstrate the superiority of ECLIPSE, notably its robustness against catastrophic forgetting and its reasonable plasticity, achieving a new state-of-the-art. The code is available at <a href=https://github.com/clovaai/ECLIPSE>https://github.com/clovaai/ECLIPSE</a>.</p></p class="citation"></blockquote><h3 id=1068--52204-sgd-street-view-synthesis-with-gaussian-splatting-and-diffusion-prior-zhongrui-yu-et-al-2024>(10/68 | 52/204) SGD: Street View Synthesis with Gaussian Splatting and Diffusion Prior (Zhongrui Yu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhongrui Yu, Haoran Wang, Jinze Yang, Hanzhang Wang, Zeke Xie, Yunfeng Cai, Jiale Cao, Zhong Ji, Mingming Sun. (2024)<br><strong>SGD: Street View Synthesis with Gaussian Splatting and Diffusion Prior</strong><br><button class=copy-to-clipboard title="SGD: Street View Synthesis with Gaussian Splatting and Diffusion Prior" index=52>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-52 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 53<br>Keywords: Diffusion Model, Fine-tuning, Multi-modal, Simulation, Simulator, Stochastic Gradient Descent<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20079v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20079v1.pdf filename=2403.20079v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Novel View Synthesis (NVS) for street scenes play a critical role in the autonomous driving <b>simulation.</b> The current mainstream technique to achieve it is neural rendering, such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). Although thrilling progress has been made, when handling street scenes, current methods struggle to maintain rendering quality at the viewpoint that deviates significantly from the training viewpoints. This issue stems from the sparse training views captured by a fixed camera on a moving vehicle. To tackle this problem, we propose a novel approach that enhances the capacity of 3DGS by leveraging prior from a <b>Diffusion</b> <b>Model</b> along with complementary <b>multi-modal</b> data. Specifically, we first <b>fine-tune</b> a <b>Diffusion</b> <b>Model</b> by adding images from adjacent frames as condition, meanwhile exploiting depth data from LiDAR point clouds to supply additional spatial information. Then we apply the <b>Diffusion</b> <b>Model</b> to regularize the 3DGS at unseen views during training. Experimental results validate the effectiveness of our method compared with current state-of-the-art models, and demonstrate its advance in rendering images from broader views.</p></p class="citation"></blockquote><h3 id=1168--53204-agileformer-spatially-agile-transformer-unet-for-medical-image-segmentation-peijie-qiu-et-al-2024>(11/68 | 53/204) AgileFormer: Spatially Agile Transformer UNet for Medical Image Segmentation (Peijie Qiu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Peijie Qiu, Jin Yang, Sayantan Kumar, Soumyendu Sekhar Ghosh, Aristeidis Sotiras. (2024)<br><strong>AgileFormer: Spatially Agile Transformer UNet for Medical Image Segmentation</strong><br><button class=copy-to-clipboard title="AgileFormer: Spatially Agile Transformer UNet for Medical Image Segmentation" index=53>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-53 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV<br>Keyword Score: 50<br>Keywords: Vision Transformer, Convolution, Convolutional Neural Network, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00122v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00122v1.pdf filename=2404.00122v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the past decades, deep neural networks, particularly <b>convolutional</b> <b>neural</b> <b>networks,</b> have achieved state-of-the-art performance in a variety of medical image segmentation tasks. Recently, the introduction of the <b>vision</b> <b>transformer</b> (ViT) has significantly altered the landscape of deep segmentation models. There has been a growing focus on ViTs, driven by their excellent performance and scalability. However, we argue that the current design of the <b>vision</b> <b>transformer-based</b> UNet (ViT-UNet) segmentation models may not effectively handle the heterogeneous appearance (e.g., varying shapes and sizes) of objects of interest in medical image segmentation tasks. To tackle this challenge, we present a structured approach to introduce spatially dynamic components to the ViT-UNet. This adaptation enables the model to effectively capture features of target objects with diverse appearances. This is achieved by three main components: \textbf{(i)} deformable patch embedding; \textbf{(ii)} spatially dynamic multi-head attention; \textbf{(iii)} deformable positional encoding. These components were integrated into a novel architecture, termed AgileFormer. AgileFormer is a spatially agile ViT-UNet designed for medical image segmentation. Experiments in three segmentation tasks using publicly available datasets demonstrated the effectiveness of the proposed method. The code is available at \href{https://github.com/sotiraslab/AgileFormer}{https://github.com/sotiraslab/AgileFormer}.</p></p class="citation"></blockquote><h3 id=1268--54204-deepfake-sentry-harnessing-ensemble-intelligence-for-resilient-detection-and-generalisation-liviu-daniel-ştefan-et-al-2024>(12/68 | 54/204) Deepfake Sentry: Harnessing Ensemble Intelligence for Resilient Detection and Generalisation (Liviu-Daniel Ştefan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Liviu-Daniel Ştefan, Dan-Cristian Stanciu, Mihai Dogariu, Mihai Gabriel Constantin, Andrei Cosmin Jitaru, Bogdan Ionescu. (2024)<br><strong>Deepfake Sentry: Harnessing Ensemble Intelligence for Resilient Detection and Generalisation</strong><br><button class=copy-to-clipboard title="Deepfake Sentry: Harnessing Ensemble Intelligence for Resilient Detection and Generalisation" index=54>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-54 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Autoencoder, Data Augmentation, Generative Adversarial Network, Generative Adversarial Network, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00114v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00114v1.pdf filename=2404.00114v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in <b>Generative</b> <b>Adversarial</b> <b>Networks</b> <b>(GANs)</b> have enabled photorealistic image generation with high quality. However, the malicious use of such generated media has raised concerns regarding visual misinformation. Although deepfake detection research has demonstrated high accuracy, it is vulnerable to advances in generation techniques and <b>adversarial</b> <b>iterations</b> on detection countermeasures. To address this, we propose a proactive and sustainable deepfake training augmentation solution that introduces artificial fingerprints into models. We achieve this by employing an ensemble learning approach that incorporates a pool of <b>autoencoders</b> that mimic the effect of the artefacts introduced by the deepfake generator models. Experiments on three datasets reveal that our proposed ensemble <b>autoencoder-based</b> <b>data</b> <b>augmentation</b> learning approach offers improvements in terms of generalisation, resistance against basic <b>data</b> <b>perturbations</b> such as noise, blurring, sharpness enhancement, and affine transforms, resilience to commonly used lossy compression algorithms such as JPEG, and enhanced resistance against <b>adversarial</b> <b>attacks.</b></p></p class="citation"></blockquote><h3 id=1368--55204-long-tailed-anomaly-detection-with-learnable-class-names-chih-hui-ho-et-al-2024>(13/68 | 55/204) Long-Tailed Anomaly Detection with Learnable Class Names (Chih-Hui Ho et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chih-Hui Ho, Kuan-Chuan Peng, Nuno Vasconcelos. (2024)<br><strong>Long-Tailed Anomaly Detection with Learnable Class Names</strong><br><button class=copy-to-clipboard title="Long-Tailed Anomaly Detection with Learnable Class Names" index=55>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-55 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Anomaly Detection, Autoencoder, Foundation Model, Variational Autoencoder, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20236v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20236v1.pdf filename=2403.20236v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Anomaly</b> <b>detection</b> (AD) aims to identify defective images and localize their defects (if any). Ideally, AD models should be able to detect defects over many image classes; without relying on hard-coded class names that can be uninformative or inconsistent across datasets; learn without <b>anomaly</b> <b>supervision;</b> and be robust to the long-tailed distributions of real-world applications. To address these challenges, we formulate the problem of long-tailed AD by introducing several datasets with different levels of class imbalance and metrics for performance evaluation. We then propose a novel method, LTAD, to detect defects from multiple and long-tailed classes, without relying on dataset class names. LTAD combines AD by reconstruction and semantic AD modules. AD by reconstruction is implemented with a <b>transformer-based</b> reconstruction module. Semantic AD is implemented with a binary classifier, which relies on learned pseudo class names and a pretrained <b>foundation</b> <b>model.</b> These modules are learned over two phases. Phase 1 learns the pseudo-class names and a <b>variational</b> <b>autoencoder</b> (VAE) for feature synthesis that augments the training data to combat long-tails. Phase 2 then learns the parameters of the reconstruction and classification modules of LTAD. Extensive experiments using the proposed long-tailed datasets show that LTAD substantially outperforms the state-of-the-art methods for most forms of dataset imbalance. The long-tailed dataset split is available at <a href=https://zenodo.org/records/10854201>https://zenodo.org/records/10854201</a> .</p></p class="citation"></blockquote><h3 id=1468--56204-freeseg-diff-training-free-open-vocabulary-segmentation-with-diffusion-models-barbara-toniella-corradini-et-al-2024>(14/68 | 56/204) FreeSeg-Diff: Training-Free Open-Vocabulary Segmentation with Diffusion Models (Barbara Toniella Corradini et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Barbara Toniella Corradini, Mustafa Shukor, Paul Couairon, Guillaume Couairon, Franco Scarselli, Matthieu Cord. (2024)<br><strong>FreeSeg-Diff: Training-Free Open-Vocabulary Segmentation with Diffusion Models</strong><br><button class=copy-to-clipboard title="FreeSeg-Diff: Training-Free Open-Vocabulary Segmentation with Diffusion Models" index=56>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-56 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Diffusion Model, Foundation Model, Weakly-supervised Learning, Zero-shot, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20105v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20105v1.pdf filename=2403.20105v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Foundation</b> <b>models</b> have exhibited unprecedented capabilities in tackling many domains and tasks. Models such as CLIP are currently widely used to bridge cross-modal representations, and <b>text-to-image</b> <b>diffusion</b> <b>models</b> are arguably the leading models in terms of realistic image generation. Image generative models are trained on massive datasets that provide them with powerful internal spatial representations. In this work, we explore the potential benefits of such representations, beyond image generation, in particular, for dense visual prediction tasks. We focus on the task of image segmentation, which is traditionally solved by training models on closed-vocabulary datasets, with pixel-level annotations. To avoid the annotation cost or training large <b>diffusion</b> <b>models,</b> we constraint our setup to be <b>zero-shot</b> and training-free. In a nutshell, our pipeline leverages different and relatively small-sized, open-source <b>foundation</b> <b>models</b> for <b>zero-shot</b> open-vocabulary segmentation. The pipeline is as follows: the image is passed to both a captioner model (i.e. BLIP) and a <b>diffusion</b> <b>model</b> (i.e., Stable <b>Diffusion</b> <b>Model)</b> to generate a text description and visual representation, respectively. The features are clustered and binarized to obtain class agnostic masks for each object. These masks are then mapped to a textual class, using the CLIP model to support open-vocabulary. Finally, we add a refinement step that allows to obtain a more precise segmentation mask. Our approach (dubbed FreeSeg-Diff), which does not rely on any training, outperforms many training-based approaches on both Pascal VOC and COCO datasets. In addition, we show very competitive results compared to the recent <b>weakly-supervised</b> segmentation approaches. We provide comprehensive experiments showing the superiority of <b>diffusion</b> <b>model</b> features compared to other pretrained models. Project page: <a href=https://bcorrad.github.io/freesegdiff/>https://bcorrad.github.io/freesegdiff/</a></p></p class="citation"></blockquote><h3 id=1568--57204-on-inherent-adversarial-robustness-of-active-vision-systems-amitangshu-mukherjee-et-al-2024>(15/68 | 57/204) On Inherent Adversarial Robustness of Active Vision Systems (Amitangshu Mukherjee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amitangshu Mukherjee, Timur Ibrayev, Kaushik Roy. (2024)<br><strong>On Inherent Adversarial Robustness of Active Vision Systems</strong><br><button class=copy-to-clipboard title="On Inherent Adversarial Robustness of Active Vision Systems" index=57>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-57 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 45<br>Keywords: Black Box, Convolution, Convolutional Neural Network, falcon, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00185v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00185v1.pdf filename=2404.00185v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Current Deep Neural Networks are vulnerable to <b>adversarial</b> <b>examples,</b> which alter their predictions by adding carefully crafted noise. Since human eyes are robust to such inputs, it is possible that the vulnerability stems from the standard way of processing inputs in one shot by processing every pixel with the same importance. In contrast, neuroscience suggests that the human vision system can differentiate salient features by (1) switching between multiple fixation points (saccades) and (2) processing the surrounding with a non-uniform external resolution (foveation). In this work, we advocate that the integration of such active vision mechanisms into current deep learning systems can offer robustness benefits. Specifically, we empirically demonstrate the inherent robustness of two active vision methods - GFNet and <b>FALcon</b> - under a <b>black</b> <b>box</b> threat model. By learning and inferencing based on downsampled glimpses obtained from multiple distinct fixation points within an input, we show that these active methods achieve (2-3) times greater robustness compared to a standard passive <b>convolutional</b> <b>network</b> under state-of-the-art <b>adversarial</b> <b>attacks.</b> More importantly, we provide illustrative and interpretable visualization analysis that demonstrates how performing inference from distinct fixation points makes active vision methods less vulnerable to malicious inputs.</p></p class="citation"></blockquote><h3 id=1668--58204-heterogeneous-network-based-contrastive-learning-method-for-polsar-land-cover-classification-jianfeng-cai-et-al-2024>(16/68 | 58/204) Heterogeneous Network Based Contrastive Learning Method for PolSAR Land Cover Classification (Jianfeng Cai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianfeng Cai, Yue Ma, Zhixi Feng, Shuyuan Yang. (2024)<br><strong>Heterogeneous Network Based Contrastive Learning Method for PolSAR Land Cover Classification</strong><br><button class=copy-to-clipboard title="Heterogeneous Network Based Contrastive Learning Method for PolSAR Land Cover Classification" index=58>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-58 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 43<br>Keywords: Benchmarking, Contrastive Learning, Few-shot, Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19902v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19902v1.pdf filename=2403.19902v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Polarimetric synthetic aperture radar (PolSAR) image interpretation is widely used in various fields. Recently, deep learning has made significant progress in PolSAR image classification. <b>Supervised</b> <b>learning</b> (SL) requires a large amount of labeled PolSAR data with high quality to achieve better performance, however, manually labeled data is insufficient. This causes the SL to fail into overfitting and degrades its generalization performance. Furthermore, the scattering confusion problem is also a significant challenge that attracts more attention. To solve these problems, this article proposes a Heterogeneous Network based <b>Contrastive</b> <b>Learning</b> method(HCLNet). It aims to learn high-level representation from unlabeled PolSAR data for <b>few-shot</b> classification according to multi-features and superpixels. Beyond the conventional CL, HCLNet introduces the heterogeneous architecture for the first time to utilize heterogeneous PolSAR features better. And it develops two easy-to-use plugins to narrow the domain gap between optics and PolSAR, including feature filter and superpixel-based instance discrimination, which the former is used to enhance the complementarity of multi-features, and the latter is used to increase the diversity of negative samples. Experiments demonstrate the superiority of HCLNet on three widely used PolSAR <b>benchmark</b> datasets compared with state-of-the-art methods. Ablation studies also verify the importance of each component. Besides, this work has implications for how to efficiently utilize the multi-features of PolSAR data to learn better high-level representation in CL and how to construct networks suitable for PolSAR data better.</p></p class="citation"></blockquote><h3 id=1768--59204-ct-respiratory-motion-synthesis-using-joint-supervised-and-adversarial-learning-yi-heng-cao-et-al-2024>(17/68 | 59/204) CT respiratory motion synthesis using joint supervised and adversarial learning (Yi-Heng Cao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yi-Heng Cao, Vincent Bourbonne, François Lucia, Ulrike Schick, Julien Bert, Vincent Jaouen, Dimitris Visvikis. (2024)<br><strong>CT respiratory motion synthesis using joint supervised and adversarial learning</strong><br><button class=copy-to-clipboard title="CT respiratory motion synthesis using joint supervised and adversarial learning" index=59>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-59 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Adversarial Learning, Supervised Learning, Unsupervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00163v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00163v1.pdf filename=2404.00163v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Objective: Four-dimensional computed tomography (4DCT) imaging consists in reconstructing a CT acquisition into multiple phases to track internal organ and tumor motion. It is commonly used in radiotherapy treatment planning to establish planning target volumes. However, 4DCT increases protocol complexity, may not align with patient breathing during treatment, and lead to higher radiation delivery. Approach: In this study, we propose a deep synthesis method to generate pseudo respiratory CT phases from static images for motion-aware treatment planning. The model produces patient-specific deformation vector fields (DVFs) by conditioning synthesis on external patient surface-based estimation, mimicking respiratory monitoring devices. A key methodological contribution is to encourage DVF realism through <b>supervised</b> DVF training while using an <b>adversarial</b> <b>term</b> jointly not only on the warped image but also on the magnitude of the DVF itself. This way, we avoid excessive smoothness typically obtained through deep <b>unsupervised</b> <b>learning,</b> and encourage correlations with the respiratory amplitude. Main results: Performance is evaluated using real 4DCT acquisitions with smaller tumor volumes than previously reported. Results demonstrate for the first time that the generated pseudo-respiratory CT phases can capture organ and tumor motion with similar accuracy to repeated 4DCT scans of the same patient. Mean inter-scans tumor center-of-mass distances and Dice similarity coefficients were $1.97$mm and $0.63$, respectively, for real 4DCT phases and $2.35$mm and $0.71$ for synthetic phases, and compares favorably to a state-of-the-art technique (RMSim).</p></p class="citation"></blockquote><h3 id=1868--60204-vsrd-instance-aware-volumetric-silhouette-rendering-for-weakly-supervised-3d-object-detection-zihua-liu-et-al-2024>(18/68 | 60/204) VSRD: Instance-Aware Volumetric Silhouette Rendering for Weakly Supervised 3D Object Detection (Zihua Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zihua Liu, Hiroki Sakuma, Masatoshi Okutomi. (2024)<br><strong>VSRD: Instance-Aware Volumetric Silhouette Rendering for Weakly Supervised 3D Object Detection</strong><br><button class=copy-to-clipboard title="VSRD: Instance-Aware Volumetric Silhouette Rendering for Weakly Supervised 3D Object Detection" index=60>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-60 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Object Detection, Supervised Learning, Supervised Learning, Weakly-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00149v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00149v1.pdf filename=2404.00149v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Monocular 3D <b>object</b> <b>detection</b> poses a significant challenge in 3D scene understanding due to its inherently ill-posed nature in monocular depth estimation. Existing methods heavily rely on <b>supervised</b> <b>learning</b> using abundant 3D labels, typically obtained through expensive and labor-intensive annotation on LiDAR point clouds. To tackle this problem, we propose a novel weakly <b>supervised</b> <b>3D</b> <b>object</b> <b>detection</b> framework named VSRD (Volumetric Silhouette Rendering for Detection) to train 3D <b>object</b> <b>detectors</b> without any 3D supervision but only weak 2D supervision. VSRD consists of multi-view 3D auto-labeling and subsequent training of monocular 3D <b>object</b> <b>detectors</b> using the pseudo labels generated in the auto-labeling stage. In the auto-labeling stage, we represent the surface of each instance as a signed distance field (SDF) and render its silhouette as an instance mask through our proposed instance-aware volumetric silhouette rendering. To directly optimize the 3D bounding boxes through rendering, we decompose the SDF of each instance into the SDF of a cuboid and the residual distance field (RDF) that represents the residual from the cuboid. This mechanism enables us to optimize the 3D bounding boxes in an end-to-end manner by comparing the rendered instance masks with the ground truth instance masks. The optimized 3D bounding boxes serve as effective training data for 3D <b>object</b> <b>detection.</b> We conduct extensive experiments on the KITTI-360 dataset, demonstrating that our method outperforms the existing weakly <b>supervised</b> <b>3D</b> <b>object</b> <b>detection</b> methods. The code is available at <a href=https://github.com/skmhrk1209/VSRD>https://github.com/skmhrk1209/VSRD</a>.</p></p class="citation"></blockquote><h3 id=1968--61204-mixed-precision-supernet-training-from-vision-foundation-models-using-low-rank-adapter-yuiko-sakuma-et-al-2024>(19/68 | 61/204) Mixed-precision Supernet Training from Vision Foundation Models using Low Rank Adapter (Yuiko Sakuma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuiko Sakuma, Masakazu Yoshimura, Junji Otsuka, Atsushi Irie, Takeshi Ohashi. (2024)<br><strong>Mixed-precision Supernet Training from Vision Foundation Models using Low Rank Adapter</strong><br><button class=copy-to-clipboard title="Mixed-precision Supernet Training from Vision Foundation Models using Low Rank Adapter" index=61>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-61 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Fine-tuning, Fine-tuning, Foundation Model, Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20080v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20080v1.pdf filename=2403.20080v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Compression of large and performant vision <b>foundation</b> <b>models</b> (VFMs) into arbitrary bit-wise operations (BitOPs) allows their deployment on various hardware. We propose to <b>fine-tune</b> a VFM to a mixed-precision <b>quantized</b> supernet. The supernet-based neural architecture search (NAS) can be adopted for this purpose, which trains a supernet, and then subnets within arbitrary hardware budgets can be extracted. However, existing methods face difficulties in optimizing the mixed-precision search space and incurring large memory costs during training. To tackle these challenges, first, we study the effective search space design for <b>fine-tuning</b> a VFM by comparing different operators (such as resolution, feature size, width, depth, and bit-widths) in terms of performance and BitOPs reduction. Second, we propose memory-efficient supernet training using a low-rank adapter (LoRA) and a progressive training strategy. The proposed method is evaluated for the recently proposed VFM, Segment Anything Model, <b>fine-tuned</b> on segmentation tasks. The searched model yields about a 95% reduction in BitOPs without incurring performance degradation.</p></p class="citation"></blockquote><h3 id=2068--62204-classification-of-diabetic-retinopathy-using-pre-trained-deep-learning-models-inas-al-kamachy-et-al-2024>(20/68 | 62/204) Classification of Diabetic Retinopathy using Pre-Trained Deep Learning Models (Inas Al-Kamachy et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Inas Al-Kamachy, Prof. Dr. Reza Hassanpour, Prof. Roya Choupani. (2024)<br><strong>Classification of Diabetic Retinopathy using Pre-Trained Deep Learning Models</strong><br><button class=copy-to-clipboard title="Classification of Diabetic Retinopathy using Pre-Trained Deep Learning Models" index=62>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-62 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: 68T07, cs-AI, cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19905v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19905v1.pdf filename=2403.19905v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Diabetic Retinopathy (DR) stands as the leading cause of blindness globally, particularly affecting individuals between the ages of 20 and 70. This paper presents a Computer-Aided Diagnosis (CAD) system designed for the automatic classification of retinal images into five distinct classes: Normal, Mild, Moderate, Severe, and Proliferative Diabetic Retinopathy (PDR). The proposed system leverages <b>Convolutional</b> <b>Neural</b> <b>Networks</b> <b>(CNNs)</b> employing pre-trained deep learning models. Through the application of <b>fine-tuning</b> techniques, our model is trained on fundus images of diabetic retinopathy with resolutions of 350x350x3 and 224x224x3. Experimental results obtained on the Kaggle platform, utilizing resources comprising 4 CPUs, 17 GB RAM, and 1 GB Disk, demonstrate the efficacy of our approach. The achieved Area Under the Curve (AUC) values for <b>CNN,</b> MobileNet, VGG-16, InceptionV3, and InceptionResNetV2 models are 0.50, 0.70, 0.53, 0.63, and 0.69, respectively.</p></p class="citation"></blockquote><h3 id=2168--63204-gda-generalized-diffusion-for-robust-test-time-adaptation-yun-yun-tsai-et-al-2024>(21/68 | 63/204) GDA: Generalized Diffusion for Robust Test-time Adaptation (Yun-Yun Tsai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yun-Yun Tsai, Fu-Chen Chen, Albert Y. C. Chen, Junfeng Yang, Che-Chun Su, Min Sun, Cheng-Hao Kuo. (2024)<br><strong>GDA: Generalized Diffusion for Robust Test-time Adaptation</strong><br><button class=copy-to-clipboard title="GDA: Generalized Diffusion for Robust Test-time Adaptation" index=63>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-63 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Diffusion Model, Benchmarking, Distribution Shift, Distribution Shift, Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00095v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00095v2.pdf filename=2404.00095v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Machine learning models struggle with generalization when encountering <b>out-of-distribution</b> (OOD) samples with unexpected <b>distribution</b> <b>shifts.</b> For vision tasks, recent studies have shown that test-time adaptation employing <b>diffusion</b> <b>models</b> can achieve state-of-the-art accuracy improvements on OOD samples by generating new samples that align with the model&rsquo;s domain without the need to modify the model&rsquo;s weights. Unfortunately, those studies have primarily focused on pixel-level corruptions, thereby lacking the generalization to adapt to a broader range of OOD types. We introduce Generalized <b>Diffusion</b> <b>Adaptation</b> (GDA), a novel <b>diffusion-based</b> <b>test-time</b> adaptation method robust against diverse OOD types. Specifically, GDA iteratively guides the <b>diffusion</b> <b>by</b> applying a marginal entropy loss derived from the model, in conjunction with style and content preservation losses during the reverse sampling process. In other words, GDA considers the model&rsquo;s output behavior with the semantic information of the samples as a whole, which can reduce ambiguity in downstream tasks during the generation process. Evaluation across various popular model architectures and OOD <b>benchmarks</b> shows that GDA consistently outperforms prior work on <b>diffusion-driven</b> <b>adaptation.</b> Notably, it achieves the highest classification accuracy improvements, ranging from 4.4% to 5.02% on ImageNet-C and 2.5% to 7.4% on Rendition, Sketch, and Stylized <b>benchmarks.</b> This performance highlights GDA&rsquo;s generalization to a broader range of OOD <b>benchmarks.</b></p></p class="citation"></blockquote><h3 id=2268--64204-context-aware-integration-of-language-and-visual-references-for-natural-language-tracking-yanyan-shao-et-al-2024>(22/68 | 64/204) Context-Aware Integration of Language and Visual References for Natural Language Tracking (Yanyan Shao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yanyan Shao, Shuting He, Qi Ye, Yuchao Feng, Wenhan Luo, Jiming Chen. (2024)<br><strong>Context-Aware Integration of Language and Visual References for Natural Language Tracking</strong><br><button class=copy-to-clipboard title="Context-Aware Integration of Language and Visual References for Natural Language Tracking" index=64>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-64 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Multi-modal, Grounding, Reasoning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19975v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19975v1.pdf filename=2403.19975v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Tracking by natural language specification (TNL) aims to consistently localize a target in a video sequence given a linguistic description in the initial frame. Existing methodologies perform language-based and template-based matching for target <b>reasoning</b> separately and merge the matching results from two sources, which suffer from tracking drift when language and visual templates miss-align with the dynamic target state and ambiguity in the later merging stage. To tackle the issues, we propose a joint <b>multi-modal</b> tracking framework with 1) a <b>prompt</b> modulation module to leverage the complementarity between temporal visual templates and language expressions, enabling precise and context-aware appearance and linguistic cues, and 2) a unified target decoding module to integrate the <b>multi-modal</b> reference cues and executes the integrated queries on the search image to predict the target location in an end-to-end manner directly. This design ensures spatio-temporal consistency by leveraging historical visual information and introduces an integrated solution, generating predictions in a single step. Extensive experiments conducted on TNL2K, OTB-Lang, LaSOT, and RefCOCOg validate the efficacy of our proposed approach. The results demonstrate competitive performance against state-of-the-art methods for both tracking and <b>grounding.</b></p></p class="citation"></blockquote><h3 id=2368--65204-robust-ensemble-person-re-identification-via-orthogonal-fusion-with-occlusion-handling-syeda-nyma-ferdous-et-al-2024>(23/68 | 65/204) Robust Ensemble Person Re-Identification via Orthogonal Fusion with Occlusion Handling (Syeda Nyma Ferdous et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Syeda Nyma Ferdous, Xin Li. (2024)<br><strong>Robust Ensemble Person Re-Identification via Orthogonal Fusion with Occlusion Handling</strong><br><button class=copy-to-clipboard title="Robust Ensemble Person Re-Identification via Orthogonal Fusion with Occlusion Handling" index=65>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-65 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Autoencoder, Convolutional Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00107v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00107v1.pdf filename=2404.00107v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Occlusion remains one of the major challenges in person reidentification (ReID) as a result of the diversity of poses and the variation of appearances. Developing novel architectures to improve the robustness of occlusion-aware person Re-ID requires new insights, especially on low-resolution edge cameras. We propose a deep ensemble model that harnesses both <b>CNN</b> and <b>Transformer</b> architectures to generate robust feature representations. To achieve robust Re-ID without the need to manually label occluded regions, we propose to take an ensemble learning-based approach derived from the analogy between arbitrarily shaped occluded regions and robust feature representation. Using the orthogonality principle, our developed deep <b>CNN</b> model makes use of masked <b>autoencoder</b> (MAE) and global-local feature fusion for robust person identification. Furthermore, we present a part occlusion-aware <b>transformer</b> capable of learning feature space that is robust to occluded regions. Experimental results are reported on several Re-ID datasets to show the effectiveness of our developed ensemble model named orthogonal fusion with occlusion handling (OFOH). Compared to competing methods, the proposed OFOH approach has achieved competent rank-1 and mAP performance.</p></p class="citation"></blockquote><h3 id=2468--66204-holo-vqvae-vq-vae-for-phase-only-holograms-joohyun-park-et-al-2024>(24/68 | 66/204) Holo-VQVAE: VQ-VAE for phase-only holograms (Joohyun Park et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Joohyun Park, Hyeongyeop Kang. (2024)<br><strong>Holo-VQVAE: VQ-VAE for phase-only holograms</strong><br><button class=copy-to-clipboard title="Holo-VQVAE: VQ-VAE for phase-only holograms" index=66>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-66 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-GR, cs-LG, cs.CV, eess-IV<br>Keyword Score: 30<br>Keywords: Autoencoder, Quantization, Variational Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.01330v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.01330v1.pdf filename=2404.01330v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Holography stands at the forefront of visual technology innovation, offering immersive, three-dimensional visualizations through the manipulation of light wave amplitude and phase. Contemporary research in hologram generation has predominantly focused on image-to-hologram conversion, producing holograms from existing images. These approaches, while effective, inherently limit the scope of innovation and creativity in hologram generation. In response to this limitation, we present Holo-VQVAE, a novel generative framework tailored for phase-only holograms (POHs). Holo-VQVAE leverages the architecture of Vector <b>Quantized</b> <b>Variational</b> <b>AutoEncoders,</b> enabling it to learn the complex distributions of POHs. Furthermore, it integrates the Angular Spectrum Method into the training process, facilitating learning in the image domain. This framework allows for the generation of unseen, diverse holographic content directly from its intricately learned latent space without requiring pre-existing images. This pioneering work paves the way for groundbreaking applications and methodologies in holographic content creation, opening a new era in the exploration of holographic content.</p></p class="citation"></blockquote><h3 id=2568--67204-h2rsvlm-towards-helpful-and-honest-remote-sensing-large-vision-language-model-chao-pang-et-al-2024>(25/68 | 67/204) H2RSVLM: Towards Helpful and Honest Remote Sensing Large Vision Language Model (Chao Pang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chao Pang, Jiang Wu, Jiayu Li, Yi Liu, Jiaxing Sun, Weijia Li, Xingxing Weng, Shuai Wang, Litong Feng, Gui-Song Xia, Conghui He. (2024)<br><strong>H2RSVLM: Towards Helpful and Honest Remote Sensing Large Vision Language Model</strong><br><button class=copy-to-clipboard title="H2RSVLM: Towards Helpful and Honest Remote Sensing Large Vision Language Model" index=67>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-67 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Question Answering, Visual Question Answering, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20213v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20213v1.pdf filename=2403.20213v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The generic large <b>Vision-Language</b> Models (VLMs) is rapidly developing, but still perform poorly in Remote Sensing (RS) domain, which is due to the unique and specialized nature of RS imagery and the comparatively limited spatial perception of current VLMs. Existing Remote Sensing specific Vision Language Models (RSVLMs) still have considerable potential for improvement, primarily owing to the lack of large-scale, high-quality RS <b>vision-language</b> datasets. We constructed HqDC-1.4M, the large scale High quality and Detailed Captions for RS images, containing 1.4 million image-caption pairs, which not only enhance the RSVLM&rsquo;s understanding of RS images but also significantly improve the model&rsquo;s spatial perception abilities, such as localization and counting, thereby increasing the helpfulness of the RSVLM. Moreover, to address the inevitable &ldquo;hallucination&rdquo; problem in RSVLM, we developed RSSA, the first dataset aimed at enhancing the Self-Awareness capability of RSVLMs. By incorporating a variety of unanswerable <b>questions</b> <b>into</b> typical RS <b>visual</b> <b>question-answering</b> <b>tasks,</b> RSSA effectively improves the truthfulness and reduces the hallucinations of the model&rsquo;s outputs, thereby enhancing the honesty of the RSVLM. Based on these datasets, we proposed the H2RSVLM, the Helpful and Honest Remote Sensing Vision Language Model. H2RSVLM has achieved outstanding performance on multiple RS public datasets and is capable of recognizing and refusing to answer the unanswerable <b>questions,</b> <b>effectively</b> mitigating the incorrect generations. We will release the code, data and model weights at <a href=https://github.com/opendatalab/H2RSVLM>https://github.com/opendatalab/H2RSVLM</a> .</p></p class="citation"></blockquote><h3 id=2668--68204-motion-inversion-for-video-customization-luozhou-wang-et-al-2024>(26/68 | 68/204) Motion Inversion for Video Customization (Luozhou Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Luozhou Wang, Guibao Shen, Yixun Liang, Xin Tao, Pengfei Wan, Di Zhang, Yijun Li, Yingcong Chen. (2024)<br><strong>Motion Inversion for Video Customization</strong><br><button class=copy-to-clipboard title="Motion Inversion for Video Customization" index=68>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-68 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Diffusion Model, Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20193v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20193v1.pdf filename=2403.20193v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this research, we present a novel approach to motion customization in video generation, addressing the widespread gap in the thorough exploration of motion representation within video generative models. Recognizing the unique challenges posed by video&rsquo;s spatiotemporal nature, our method introduces Motion Embeddings, a set of explicit, temporally coherent one-dimensional embeddings derived from a given video. These embeddings are designed to integrate seamlessly with the temporal <b>transformer</b> modules of video <b>diffusion</b> <b>models,</b> modulating <b>self-attention</b> computations across frames without compromising spatial integrity. Our approach offers a compact and efficient solution to motion representation and enables complex manipulations of motion characteristics through vector arithmetic in the embedding space. Furthermore, we identify the Temporal Discrepancy in video generative models, which refers to variations in how different motion modules process temporal relationships between frames. We leverage this understanding to optimize the integration of our motion embeddings. Our contributions include the introduction of a tailored motion embedding for customization tasks, insights into the temporal processing differences in video models, and a demonstration of the practical advantages and effectiveness of our method through extensive experiments.</p></p class="citation"></blockquote><h3 id=2768--69204-fairclip-harnessing-fairness-in-vision-language-learning-yan-luo-et-al-2024>(27/68 | 69/204) FairCLIP: Harnessing Fairness in Vision-Language Learning (Yan Luo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yan Luo, Min Shi, Muhammad Osama Khan, Muhammad Muneeb Afzal, Hao Huang, Shuaihang Yuan, Yu Tian, Luo Song, Ava Kouhana, Tobias Elze, Yi Fang, Mengyu Wang. (2024)<br><strong>FairCLIP: Harnessing Fairness in Vision-Language Learning</strong><br><button class=copy-to-clipboard title="FairCLIP: Harnessing Fairness in Vision-Language Learning" index=69>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-69 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Fairness, Foundation Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19949v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19949v1.pdf filename=2403.19949v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Fairness</b> is a critical concern in deep learning, especially in healthcare, where these models influence diagnoses and treatment decisions. Although <b>fairness</b> has been investigated in the vision-only domain, the <b>fairness</b> of medical <b>vision-language</b> (VL) models remains unexplored due to the scarcity of medical VL datasets for studying <b>fairness.</b> To bridge this research gap, we introduce the first fair <b>vision-language</b> medical dataset FairVLMed that provides detailed demographic attributes, ground-truth labels, and clinical notes to facilitate an in-depth examination of <b>fairness</b> within VL <b>foundation</b> <b>models.</b> Using FairVLMed, we conduct a comprehensive <b>fairness</b> analysis of two widely-used VL models (CLIP and BLIP2), pre-trained on both natural and medical domains, across four different protected attributes. Our results highlight significant biases in all VL models, with Asian, Male, Non-Hispanic, and Spanish being the preferred subgroups across the protected attributes of race, gender, ethnicity, and language, respectively. In order to alleviate these biases, we propose FairCLIP, an optimal-transport-based approach that achieves a favorable trade-off between performance and <b>fairness</b> by reducing the Sinkhorn distance between the overall sample distribution and the distributions corresponding to each demographic group. As the first VL dataset of its kind, FairVLMed holds the potential to catalyze advancements in the development of machine learning models that are both ethically aware and clinically effective. Our dataset and code are available at <a href=https://ophai.hms.harvard.edu/datasets/fairvlmed10k>https://ophai.hms.harvard.edu/datasets/fairvlmed10k</a>.</p></p class="citation"></blockquote><h3 id=2868--70204-are-we-on-the-right-way-for-evaluating-large-vision-language-models-lin-chen-et-al-2024>(28/68 | 70/204) Are We on the Right Way for Evaluating Large Vision-Language Models? (Lin Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, Feng Zhao. (2024)<br><strong>Are We on the Right Way for Evaluating Large Vision-Language Models?</strong><br><button class=copy-to-clipboard title="Are We on the Right Way for Evaluating Large Vision-Language Models?" index=70>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-70 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 26<br>Keywords: Benchmarking, Multi-modal, Large Language Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20330v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20330v1.pdf filename=2403.20330v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Large <b>vision-language</b> models (LVLMs) have recently achieved rapid progress, sparking numerous studies to evaluate their <b>multi-modal</b> capabilities. However, we dig into current evaluation works and identify two primary issues: 1) Visual content is unnecessary for many samples. The answers can be directly inferred from the questions and options, or the world knowledge embedded in <b>LLMs.</b> This phenomenon is prevalent across current <b>benchmarks.</b> For instance, GeminiPro achieves 42.9% on the MMMU <b>benchmark</b> without any visual input, and outperforms the random choice baseline across six <b>benchmarks</b> over 20% on average. 2) Unintentional data leakage exists in <b>LLM</b> and LVLM training. <b>LLM</b> and LVLM could still answer some visual-necessary questions without visual content, indicating the memorizing of these samples within large-scale training data. For example, Sphinx-X-MoE gets 43.6% on MMMU without accessing images, surpassing its <b>LLM</b> backbone with 17.9%. Both problems lead to misjudgments of actual <b>multi-modal</b> gains and potentially misguide the study of LVLM. To this end, we present MMStar, an elite vision-indispensable <b>multi-modal</b> <b>benchmark</b> comprising 1,500 samples meticulously selected by humans. MMStar <b>benchmarks</b> 6 core capabilities and 18 detailed axes, aiming to evaluate LVLMs&rsquo; <b>multi-modal</b> capacities with carefully balanced and purified samples. These samples are first roughly selected from current <b>benchmarks</b> with an automated pipeline, human review is then involved to ensure each curated sample exhibits visual dependency, minimal data leakage, and requires advanced <b>multi-modal</b> capabilities. Moreover, two metrics are developed to measure data leakage and actual performance gain in <b>multi-modal</b> training. We evaluate 16 leading LVLMs on MMStar to assess their <b>multi-modal</b> capabilities, and on 7 <b>benchmarks</b> with the proposed metrics to investigate their data leakage and actual <b>multi-modal</b> gain.</p></p class="citation"></blockquote><h3 id=2968--71204-negative-label-guided-ood-detection-with-pretrained-vision-language-models-xue-jiang-et-al-2024>(29/68 | 71/204) Negative Label Guided OOD Detection with Pretrained Vision-Language Models (Xue Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xue Jiang, Feng Liu, Zhen Fang, Hong Chen, Tongliang Liu, Feng Zheng, Bo Han. (2024)<br><strong>Negative Label Guided OOD Detection with Pretrained Vision-Language Models</strong><br><button class=copy-to-clipboard title="Negative Label Guided OOD Detection with Pretrained Vision-Language Models" index=71>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-71 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 26<br>Keywords: Benchmarking, Multi-modal, Out-of-distribution, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20078v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20078v1.pdf filename=2403.20078v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Out-of-distribution</b> (OOD) detection aims at identifying samples from unknown classes, playing a crucial role in trustworthy models against errors on unexpected inputs. Extensive research has been dedicated to exploring OOD detection in the vision modality. <b>Vision-language</b> models (VLMs) can leverage both textual and visual information for various <b>multi-modal</b> applications, whereas few OOD detection methods take into account information from the text modality. In this paper, we propose a novel post hoc OOD detection method, called NegLabel, which takes a vast number of negative labels from extensive corpus databases. We design a novel scheme for the OOD score collaborated with negative labels. Theoretical analysis helps to understand the mechanism of negative labels. Extensive experiments demonstrate that our method NegLabel achieves state-of-the-art performance on various OOD detection <b>benchmarks</b> and generalizes well on multiple VLM architectures. Furthermore, our method NegLabel exhibits remarkable robustness against diverse domain shifts. The codes are available at <a href=https://github.com/tmlr-group/NegLabel>https://github.com/tmlr-group/NegLabel</a>.</p></p class="citation"></blockquote><h3 id=3068--72204-dvis-daq-improving-video-segmentation-via-dynamic-anchor-queries-yikang-zhou-et-al-2024>(30/68 | 72/204) DVIS-DAQ: Improving Video Segmentation via Dynamic Anchor Queries (Yikang Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yikang Zhou, Tao Zhang, Shunping JI, Shuicheng Yan, Xiangtai Li. (2024)<br><strong>DVIS-DAQ: Improving Video Segmentation via Dynamic Anchor Queries</strong><br><button class=copy-to-clipboard title="DVIS-DAQ: Improving Video Segmentation via Dynamic Anchor Queries" index=72>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-72 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Benchmarking, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00086v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00086v1.pdf filename=2404.00086v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Modern video segmentation methods adopt object queries to perform inter-frame association and demonstrate satisfactory performance in tracking continuously appearing objects despite large-scale motion and transient occlusion. However, they all underperform on newly emerging and disappearing objects that are common in the real world because they attempt to model object emergence and disappearance through feature transitions between background and foreground queries that have significant feature gaps. We introduce Dynamic Anchor Queries (DAQ) to shorten the transition gap between the anchor and target queries by dynamically generating anchor queries based on the features of potential candidates. Furthermore, we introduce a query-level object Emergence and Disappearance <b>Simulation</b> (EDS) strategy, which unleashes DAQ&rsquo;s potential without any additional cost. Finally, we combine our proposed DAQ and EDS with DVIS~\cite{zhang2023dvis} to obtain DVIS-DAQ. Extensive experiments demonstrate that DVIS-DAQ achieves a new state-of-the-art (SOTA) performance on five mainstream video segmentation <b>benchmarks.</b> Code and models are available at \url{https://github.com/SkyworkAI/DAQ-VS}.</p></p class="citation"></blockquote><h3 id=3168--73204-aggregating-local-and-global-features-via-selective-state-spaces-model-for-efficient-image-deblurring-hu-gao-et-al-2024>(31/68 | 73/204) Aggregating Local and Global Features via Selective State Spaces Model for Efficient Image Deblurring (Hu Gao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hu Gao, Depeng Dang. (2024)<br><strong>Aggregating Local and Global Features via Selective State Spaces Model for Efficient Image Deblurring</strong><br><button class=copy-to-clipboard title="Aggregating Local and Global Features via Selective State Spaces Model for Efficient Image Deblurring" index=73>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-73 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 23<br>Keywords: Benchmarking, Convolutional Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20106v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20106v1.pdf filename=2403.20106v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Image deblurring is a process of restoring a high quality image from the corresponding blurred image. Significant progress in this field has been made possible by the emergence of various effective deep learning models, including <b>CNNs</b> and <b>Transformers.</b> However, these methods often face the dilemma between eliminating long-range blur degradation perturbations and maintaining computational efficiency, which hinders their practical application. To address this issue, we propose an efficient image deblurring network that leverages selective structured state spaces model to aggregate enriched and accurate features. Specifically, we design an aggregate local and global block (ALGBlock) to capture and fuse both local invariant properties and non-local information. The ALGBlock consists of two blocks: (1) The local block models local connectivity using simplified channel attention. (2) The global block captures long-range dependency features with linear complexity through selective structured state spaces. Nevertheless, we note that the image details are local features of images, we accentuate the local part for restoration by recalibrating the weight when aggregating the two branches for recovery. Experimental results demonstrate that the proposed method outperforms state-of-the-art approaches on widely used <b>benchmarks,</b> highlighting its superior performance.</p></p class="citation"></blockquote><h3 id=3268--74204-fsmr-a-feature-swapping-multi-modal-reasoning-approach-with-joint-textual-and-visual-clues-shuang-li-et-al-2024>(32/68 | 74/204) FSMR: A Feature Swapping Multi-modal Reasoning Approach with Joint Textual and Visual Clues (Shuang Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shuang Li, Jiahua Wang, Lijie Wen. (2024)<br><strong>FSMR: A Feature Swapping Multi-modal Reasoning Approach with Joint Textual and Visual Clues</strong><br><button class=copy-to-clipboard title="FSMR: A Feature Swapping Multi-modal Reasoning Approach with Joint Textual and Visual Clues" index=74>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-74 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CL, cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Multi-modal, Image2text, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20026v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20026v1.pdf filename=2403.20026v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multi-modal</b> <b>reasoning</b> plays a vital role in bridging the gap between textual and visual information, enabling a deeper understanding of the context. This paper presents the Feature Swapping <b>Multi-modal</b> <b>Reasoning</b> (FSMR) model, designed to enhance <b>multi-modal</b> <b>reasoning</b> through feature swapping. FSMR leverages a pre-trained visual-language model as an encoder, accommodating both text and image inputs for effective feature representation from both modalities. It introduces a unique feature swapping module, enabling the exchange of features between identified objects in images and corresponding vocabulary words in text, thereby enhancing the model&rsquo;s comprehension of the interplay between images and text. To further bolster its <b>multi-modal</b> alignment capabilities, FSMR incorporates a <b>multi-modal</b> cross-attention mechanism, facilitating the joint modeling of textual and visual information. During training, we employ <b>image-text</b> matching and cross-entropy losses to ensure semantic consistency between visual and language elements. Extensive experiments on the PMR dataset demonstrate FSMR&rsquo;s superiority over state-of-the-art baseline models across various performance metrics.</p></p class="citation"></blockquote><h3 id=3368--75204-semantically-shifted-incremental-adapter-tuning-is-a-continual-vitransformer-yuwen-tan-et-al-2024>(33/68 | 75/204) Semantically-Shifted Incremental Adapter-Tuning is A Continual ViTransformer (Yuwen Tan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuwen Tan, Qinhao Zhou, Xiang Xiang, Ke Wang, Yuchuan Wu, Yongbin Li. (2024)<br><strong>Semantically-Shifted Incremental Adapter-Tuning is A Continual ViTransformer</strong><br><button class=copy-to-clipboard title="Semantically-Shifted Incremental Adapter-Tuning is A Continual ViTransformer" index=75>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-75 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 23<br>Keywords: Benchmarking, Continual Learning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19979v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19979v1.pdf filename=2403.19979v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Class-incremental learning (CIL) aims to enable models to continuously learn new classes while overcoming catastrophic forgetting. The introduction of pre-trained models has brought new tuning paradigms to CIL. In this paper, we revisit different parameter-efficient tuning (PET) methods within the context of <b>continual</b> <b>learning.</b> We observe that adapter tuning demonstrates superiority over <b>prompt-based</b> methods, even without parameter expansion in each learning session. Motivated by this, we propose incrementally tuning the shared adapter without imposing parameter update constraints, enhancing the learning capacity of the backbone. Additionally, we employ feature sampling from stored prototypes to retrain a unified classifier, further improving its performance. We estimate the semantic shift of old prototypes without access to past samples and update stored prototypes session by session. Our proposed method eliminates model expansion and avoids retaining any image samples. It surpasses previous pre-trained model-based CIL methods and demonstrates remarkable <b>continual</b> <b>learning</b> capabilities. Experimental results on five CIL <b>benchmarks</b> validate the effectiveness of our approach, achieving state-of-the-art (SOTA) performance.</p></p class="citation"></blockquote><h3 id=3468--76204-efficient-modulation-for-vision-networks-xu-ma-et-al-2024>(34/68 | 76/204) Efficient Modulation for Vision Networks (Xu Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xu Ma, Xiyang Dai, Jianwei Yang, Bin Xiao, Yinpeng Chen, Yun Fu, Lu Yuan. (2024)<br><strong>Efficient Modulation for Vision Networks</strong><br><button class=copy-to-clipboard title="Efficient Modulation for Vision Networks" index=76>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-76 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Benchmarking, Convolution, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19963v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19963v1.pdf filename=2403.19963v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we present efficient modulation, a novel design for efficient vision networks. We revisit the modulation mechanism, which operates input through <b>convolutional</b> context modeling and feature projection layers, and fuses features via element-wise multiplication and an MLP block. We demonstrate that the modulation mechanism is particularly well suited for efficient networks and further tailor the modulation design by proposing the efficient modulation (EfficientMod) block, which is considered the essential building block for our networks. Benefiting from the prominent representational ability of modulation mechanism and the proposed efficient design, our network can accomplish better trade-offs between accuracy and efficiency and set new state-of-the-art performance in the zoo of efficient networks. When integrating EfficientMod with the vanilla <b>self-attention</b> block, we obtain the hybrid architecture which further improves the performance without loss of efficiency. We carry out comprehensive experiments to verify EfficientMod&rsquo;s performance. With fewer parameters, our EfficientMod-s performs 0.6 top-1 accuracy better than EfficientFormerV2-s2 and is 25% faster on GPU, and 2.9 better than MobileViTv2-1.0 at the same GPU latency. Additionally, our method presents a notable improvement in downstream tasks, outperforming EfficientFormerV2-s by 3.6 mIoU on the ADE20K <b>benchmark.</b> Code and checkpoints are available at <a href=https://github.com/ma-xu/EfficientMod>https://github.com/ma-xu/EfficientMod</a>.</p></p class="citation"></blockquote><h3 id=3568--77204-catsnet-a-context-aware-network-for-height-estimation-in-a-forested-area-based-on-pol-tomosar-data-wenyu-yang-et-al-2024>(35/68 | 77/204) CATSNet: a context-aware network for Height Estimation in a Forested Area based on Pol-TomoSAR data (Wenyu Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenyu Yang, Sergio Vitale, Hossein Aghababaei, Giampaolo Ferraioli, Vito Pascazio, Gilda Schirinzi. (2024)<br><strong>CATSNet: a context-aware network for Height Estimation in a Forested Area based on Pol-TomoSAR data</strong><br><button class=copy-to-clipboard title="CATSNet: a context-aware network for Height Estimation in a Forested Area based on Pol-TomoSAR data" index=77>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-77 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20273v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20273v1.pdf filename=2403.20273v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Tropical forests are a key component of the global carbon cycle. With plans for upcoming space-borne missions like BIOMASS to monitor forestry, several airborne missions, including TropiSAR and AfriSAR campaigns, have been successfully launched and experimented. Typical Synthetic Aperture Radar Tomography (TomoSAR) methods involve complex models with low accuracy and high computation costs. In recent years, deep learning methods have also gained attention in the TomoSAR framework, showing interesting performance. Recently, a solution based on a fully connected Tomographic Neural Network (TSNN) has demonstrated its effectiveness in accurately estimating forest and ground heights by exploiting the pixel-wise elements of the covariance matrix derived from TomoSAR data. This work instead goes beyond the pixel-wise approach to define a context-aware deep learning-based solution named CATSNet. A <b>convolutional</b> <b>neural</b> <b>network</b> is considered to leverage patch-based information and extract features from a neighborhood rather than focus on a single pixel. The training is conducted by considering TomoSAR data as the input and Light Detection and Ranging (LiDAR) values as the ground truth. The experimental results show striking advantages in both performance and generalization ability by leveraging context information within Multiple Baselines (MB) TomoSAR data across different polarimetric modalities, surpassing existing techniques.</p></p class="citation"></blockquote><h3 id=3668--78204-stegogan-leveraging-steganography-for-non-bijective-image-to-image-translation-sidi-wu-et-al-2024>(36/68 | 78/204) StegoGAN: Leveraging Steganography for Non-Bijective Image-to-Image Translation (Sidi Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sidi Wu, Yizi Chen, Samuel Mermet, Lorenz Hurni, Konrad Schindler, Nicolas Gonthier, Loic Landrieu. (2024)<br><strong>StegoGAN: Leveraging Steganography for Non-Bijective Image-to-Image Translation</strong><br><button class=copy-to-clipboard title="StegoGAN: Leveraging Steganography for Non-Bijective Image-to-Image Translation" index=78>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-78 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV<br>Keyword Score: 20<br>Keywords: Generative Adversarial Network, Image2text<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20142v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20142v1.pdf filename=2403.20142v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Most <b>image-to-image</b> <b>translation</b> models postulate that a unique correspondence exists between the semantic classes of the source and target domains. However, this assumption does not always hold in real-world scenarios due to divergent distributions, different class sets, and asymmetrical information representation. As conventional <b>GANs</b> attempt to generate <b>images</b> <b>that</b> match the distribution of the target domain, they may hallucinate spurious instances of classes absent from the source domain, thereby diminishing the usefulness and reliability of translated <b>images.</b> <b>CycleGAN-based</b> methods are also known to hide the mismatched information in the generated <b>images</b> <b>to</b> bypass cycle consistency objectives, a process known as steganography. In response to the challenge of non-bijective <b>image</b> <b>translation,</b> we introduce StegoGAN, a novel model that leverages steganography to prevent spurious features in generated <b>images.</b> <b>Our</b> approach enhances the semantic consistency of the translated <b>images</b> <b>without</b> requiring additional postprocessing or supervision. Our experimental evaluations demonstrate that StegoGAN outperforms existing <b>GAN-based</b> models across various non-bijective <b>image-to-image</b> <b>translation</b> tasks, both qualitatively and quantitatively. Our code and pretrained models are accessible at <a href=https://github.com/sian-wusidi/StegoGAN>https://github.com/sian-wusidi/StegoGAN</a>.</p></p class="citation"></blockquote><h3 id=3768--79204-selective-attention-based-modulation-for-continual-learning-giovanni-bellitto-et-al-2024>(37/68 | 79/204) Selective Attention-based Modulation for Continual Learning (Giovanni Bellitto et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Giovanni Bellitto, Federica Proietto Salanitri, Matteo Pennisi, Matteo Boschini, Angelo Porrello, Simone Calderara, Simone Palazzo, Concetto Spampinato. (2024)<br><strong>Selective Attention-based Modulation for Continual Learning</strong><br><button class=copy-to-clipboard title="Selective Attention-based Modulation for Continual Learning" index=79>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-79 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Continual Learning, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20086v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20086v1.pdf filename=2403.20086v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present SAM, a biologically-plausible selective attention-driven modulation approach to enhance classification models in a <b>continual</b> <b>learning</b> setting. Inspired by neurophysiological evidence that the primary visual cortex does not contribute to object manifold untangling for categorization and that primordial attention biases are still embedded in the modern brain, we propose to employ auxiliary saliency prediction features as a modulation signal to drive and stabilize the learning of a sequence of non-i.i.d. classification tasks. Experimental results confirm that SAM effectively enhances the performance (in some cases up to about twenty percent points) of state-of-the-art <b>continual</b> <b>learning</b> methods, both in class-incremental and task-incremental settings. Moreover, we show that attention-based modulation successfully encourages the learning of features that are more robust to the presence of spurious features and to <b>adversarial</b> <b>attacks</b> than baseline methods. Code is available at: <a href=https://github.com/perceivelab/SAM>https://github.com/perceivelab/SAM</a>.</p></p class="citation"></blockquote><h3 id=3868--80204-colorful-cutout-enhancing-image-data-augmentation-with-curriculum-learning-juhwan-choi-et-al-2024>(38/68 | 80/204) Colorful Cutout: Enhancing Image Data Augmentation with Curriculum Learning (Juhwan Choi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Juhwan Choi, YoungBin Kim. (2024)<br><strong>Colorful Cutout: Enhancing Image Data Augmentation with Curriculum Learning</strong><br><button class=copy-to-clipboard title="Colorful Cutout: Enhancing Image Data Augmentation with Curriculum Learning" index=80>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-80 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Curriculum Learning, Data Augmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20012v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20012v1.pdf filename=2403.20012v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Data</b> <b>augmentation</b> is one of the regularization strategies for the training of deep learning models, which enhances generalizability and prevents overfitting, leading to performance improvement. Although researchers have proposed various <b>data</b> <b>augmentation</b> techniques, they often lack consideration for the difficulty of augmented <b>data.</b> <b>Recently,</b> another line of research suggests incorporating the concept of <b>curriculum</b> <b>learning</b> with <b>data</b> <b>augmentation</b> in the field of natural language processing. In this study, we adopt <b>curriculum</b> <b>data</b> <b>augmentation</b> for image <b>data</b> <b>augmentation</b> and propose colorful cutout, which gradually increases the noise and difficulty introduced in the augmented image. Our experimental results highlight the possibility of <b>curriculum</b> <b>data</b> <b>augmentation</b> for image <b>data.</b> <b>We</b> publicly released our source code to improve the reproducibility of our study.</p></p class="citation"></blockquote><h3 id=3968--81204-a-parallel-attention-network-for-cattle-face-recognition-jiayu-li-et-al-2024>(39/68 | 81/204) A Parallel Attention Network for Cattle Face Recognition (Jiayu Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiayu Li, Xuechao Zou, Shiying Wang, Ben Chen, Junliang Xing, Pin Tao. (2024)<br><strong>A Parallel Attention Network for Cattle Face Recognition</strong><br><button class=copy-to-clipboard title="A Parallel Attention Network for Cattle Face Recognition" index=81>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-81 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Face Recognition, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19980v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19980v1.pdf filename=2403.19980v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Cattle <b>face</b> <b>recognition</b> holds paramount significance in domains such as animal husbandry and behavioral research. Despite significant progress in confined environments, applying these accomplishments in wild settings remains challenging. Thus, we create the first large-scale cattle <b>face</b> <b>recognition</b> dataset, ICRWE, for wild environments. It encompasses 483 cattle and 9,816 high-resolution image samples. Each sample undergoes annotation for <b>face</b> <b>features,</b> light conditions, and <b>face</b> <b>orientation.</b> Furthermore, we introduce a novel parallel attention network, PANet. Comprising several cascaded <b>Transformer</b> modules, each module incorporates two parallel Position Attention Modules (PAM) and Feature Mapping Modules (FMM). PAM focuses on local and global features at each image position through parallel channel attention, and FMM captures intricate feature patterns through non-linear mappings. Experimental results indicate that PANet achieves a recognition accuracy of 88.03% on the ICRWE dataset, establishing itself as the current state-of-the-art approach. The source code is available in the supplementary materials.</p></p class="citation"></blockquote><h3 id=4068--82204-separate-dynamic-and-differentiable-smart-pruner-for-blockoutput-channel-pruning-on-computer-vision-tasks-guanhua-ding-et-al-2024>(40/68 | 82/204) Separate, Dynamic and Differentiable (SMART) Pruner for Block/Output Channel Pruning on Computer Vision Tasks (Guanhua Ding et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guanhua Ding, Zexi Ye, Zhen Zhong, Gang Li, David Shao. (2024)<br><strong>Separate, Dynamic and Differentiable (SMART) Pruner for Block/Output Channel Pruning on Computer Vision Tasks</strong><br><button class=copy-to-clipboard title="Separate, Dynamic and Differentiable (SMART) Pruner for Block/Output Channel Pruning on Computer Vision Tasks" index=82>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-82 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 20<br>Keywords: Pruning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19969v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19969v1.pdf filename=2403.19969v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep Neural Network (DNN) <b>pruning</b> has emerged as a key strategy to reduce model size, improve inference latency, and lower power consumption on DNN accelerators. Among various <b>pruning</b> techniques, block and output channel <b>pruning</b> have shown significant potential in accelerating hardware performance. However, their accuracy often requires further improvement. In response to this challenge, we introduce a separate, dynamic and differentiable (SMART) pruner. This pruner stands out by utilizing a separate, learnable probability mask for weight importance ranking, employing a differentiable Top k operator to achieve target sparsity, and leveraging a dynamic temperature parameter trick to escape from non-sparse local minima. In our experiments, the SMART pruner consistently demonstrated its superiority over existing <b>pruning</b> methods across a wide range of tasks and models on block and output channel <b>pruning.</b> Additionally, we extend our testing to <b>Transformer-based</b> models in N:M <b>pruning</b> scenarios, where SMART pruner also yields state-of-the-art results, demonstrating its adaptability and robustness across various neural network architectures, and <b>pruning</b> types.</p></p class="citation"></blockquote><h3 id=4168--83204-structure-matters-tackling-the-semantic-discrepancy-in-diffusion-models-for-image-inpainting-haipeng-liu-et-al-2024>(41/68 | 83/204) Structure Matters: Tackling the Semantic Discrepancy in Diffusion Models for Image Inpainting (Haipeng Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haipeng Liu, Yang Wang, Biao Qian, Meng Wang, Yong Rui. (2024)<br><strong>Structure Matters: Tackling the Semantic Discrepancy in Diffusion Models for Image Inpainting</strong><br><button class=copy-to-clipboard title="Structure Matters: Tackling the Semantic Discrepancy in Diffusion Models for Image Inpainting" index=83>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-83 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Probabilistic Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19898v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19898v2.pdf filename=2403.19898v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Denoising <b>diffusion</b> <b>probabilistic</b> <b>models</b> for image inpainting aim to add the noise to the texture of image during the forward process and recover masked regions with unmasked ones of the texture via the reverse denoising process. Despite the meaningful semantics generation, the existing arts suffer from the semantic discrepancy between masked and unmasked regions, since the semantically dense unmasked texture fails to be completely degraded while the masked regions turn to the pure noise in <b>diffusion</b> <b>process,</b> leading to the large discrepancy between them. In this paper, we aim to answer how unmasked semantics guide texture denoising process;together with how to tackle the semantic discrepancy, to facilitate the consistent and meaningful semantics generation. To this end, we propose a novel structure-guided <b>diffusion</b> <b>model</b> named StrDiffusion, to reformulate the conventional texture denoising process under structure guidance to derive a simplified denoising objective for image inpainting, while revealing: 1) the semantically sparse structure is beneficial to tackle semantic discrepancy in early stage, while dense texture generates reasonable semantics in late stage; 2) the semantics from unmasked regions essentially offer the time-dependent structure guidance for the texture denoising process, benefiting from the time-dependent sparsity of the structure semantics. For the denoising process, a structure-guided neural network is trained to estimate the simplified denoising objective by exploiting the consistency of the denoised structure between masked and unmasked regions. Besides, we devise an adaptive resampling strategy as a formal criterion as whether structure is competent to guide the texture denoising process, while regulate their semantic correlations. Extensive experiments validate the merits of StrDiffusion over the state-of-the-arts. Our code is available at <a href=https://github.com/htyjers/StrDiffusion>https://github.com/htyjers/StrDiffusion</a>.</p></p class="citation"></blockquote><h3 id=4268--84204-benchmarking-counterfactual-image-generation-thomas-melistas-et-al-2024>(42/68 | 84/204) Benchmarking Counterfactual Image Generation (Thomas Melistas et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Thomas Melistas, Nikos Spyrou, Nefeli Gkouti, Pedro Sanchez, Athanasios Vlontzos, Giorgos Papanastasiou, Sotirios A. Tsaftaris. (2024)<br><strong>Benchmarking Counterfactual Image Generation</strong><br><button class=copy-to-clipboard title="Benchmarking Counterfactual Image Generation" index=84>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-84 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 16<br>Keywords: Benchmarking, Benchmarking, Counter-factual<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20287v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20287v1.pdf filename=2403.20287v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Counterfactual</b> image generation is pivotal for understanding the causal relations of variables, with applications in interpretability and generation of unbiased synthetic data. However, evaluating image generation is a long-standing challenge in itself. The need to evaluate <b>counterfactual</b> generation compounds on this challenge, precisely because <b>counterfactuals,</b> by definition, are hypothetical scenarios without observable ground truths. In this paper, we present a novel comprehensive framework aimed at <b>benchmarking</b> <b>counterfactual</b> image generation methods. We incorporate metrics that focus on evaluating diverse aspects of <b>counterfactuals,</b> such as composition, effectiveness, minimality of interventions, and image realism. We assess the performance of three distinct conditional image generation model types, based on the Structural Causal Model paradigm. Our work is accompanied by a user-friendly Python package which allows to further evaluate and <b>benchmark</b> existing and future <b>counterfactual</b> image generation methods. Our framework is extendable to additional SCM and other causal methods, generative models, and datasets.</p></p class="citation"></blockquote><h3 id=4368--85204-latent-embedding-clustering-for-occlusion-robust-head-pose-estimation-josé-celestino-et-al-2024>(43/68 | 85/204) Latent Embedding Clustering for Occlusion Robust Head Pose Estimation (José Celestino et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>José Celestino, Manuel Marques, Jacinto C. Nascimento. (2024)<br><strong>Latent Embedding Clustering for Occlusion Robust Head Pose Estimation</strong><br><button class=copy-to-clipboard title="Latent Embedding Clustering for Occlusion Robust Head Pose Estimation" index=85>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-85 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 16<br>Keywords: Benchmarking, Clustering, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20251v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20251v1.pdf filename=2403.20251v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Head pose estimation has become a crucial area of research in computer vision given its usefulness in a wide range of applications, including robotics, surveillance, or driver attention monitoring. One of the most difficult challenges in this field is managing head occlusions that frequently take place in real-world scenarios. In this paper, we propose a novel and efficient framework that is robust in real world head occlusion scenarios. In particular, we propose an <b>unsupervised</b> latent embedding <b>clustering</b> with regression and classification components for each pose angle. The model optimizes latent feature representations for occluded and non-occluded images through a <b>clustering</b> term while improving fine-grained angle predictions. Experimental evaluation on in-the-wild head pose <b>benchmark</b> datasets reveal competitive performance in comparison to state-of-the-art methodologies with the advantage of having a significant data reduction. We observe a substantial improvement in occluded head pose estimation. Also, an ablation study is conducted to ascertain the impact of the <b>clustering</b> term within our proposed framework.</p></p class="citation"></blockquote><h3 id=4468--86204-mtmmc-a-large-scale-real-world-multi-modal-camera-tracking-benchmark-sanghyun-woo-et-al-2024>(44/68 | 86/204) MTMMC: A Large-Scale Real-World Multi-Modal Camera Tracking Benchmark (Sanghyun Woo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sanghyun Woo, Kwanyong Park, Inkyu Shin, Myungchul Kim, In So Kweon. (2024)<br><strong>MTMMC: A Large-Scale Real-World Multi-Modal Camera Tracking Benchmark</strong><br><button class=copy-to-clipboard title="MTMMC: A Large-Scale Real-World Multi-Modal Camera Tracking Benchmark" index=86>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-86 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 16<br>Keywords: Anomaly Detection, Benchmarking, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20225v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20225v1.pdf filename=2403.20225v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multi-target multi-camera tracking is a crucial task that involves identifying and tracking individuals over time using video streams from multiple cameras. This task has practical applications in various fields, such as visual surveillance, crowd behavior analysis, and <b>anomaly</b> <b>detection.</b> However, due to the difficulty and cost of collecting and labeling data, existing datasets for this task are either synthetically generated or artificially constructed within a controlled camera network setting, which limits their ability to model real-world dynamics and generalize to diverse camera configurations. To address this issue, we present MTMMC, a real-world, large-scale dataset that includes long video sequences captured by 16 <b>multi-modal</b> cameras in two different environments - campus and factory - across various time, weather, and season conditions. This dataset provides a challenging test-bed for studying multi-camera tracking under diverse real-world complexities and includes an additional input modality of spatially aligned and temporally synchronized RGB and thermal cameras, which enhances the accuracy of multi-camera tracking. MTMMC is a super-set of existing datasets, benefiting independent fields such as person detection, re-identification, and multiple object tracking. We provide baselines and new learning setups on this dataset and set the reference scores for future studies. The datasets, models, and test server will be made publicly available.</p></p class="citation"></blockquote><h3 id=4568--87204-neslam-neural-implicit-mapping-and-self-supervised-feature-tracking-with-depth-completion-and-denoising-tianchen-deng-et-al-2024>(45/68 | 87/204) NeSLAM: Neural Implicit Mapping and Self-Supervised Feature Tracking With Depth Completion and Denoising (Tianchen Deng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianchen Deng, Yanbo Wang, Hongle Xie, Hesheng Wang, Jingchuan Wang, Danwei Wang, Weidong Chen. (2024)<br><strong>NeSLAM: Neural Implicit Mapping and Self-Supervised Feature Tracking With Depth Completion and Denoising</strong><br><button class=copy-to-clipboard title="NeSLAM: Neural Implicit Mapping and Self-Supervised Feature Tracking With Depth Completion and Denoising" index=87>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-87 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV<br>Keyword Score: 15<br>Keywords: Geometry, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20034v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20034v1.pdf filename=2403.20034v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, there have been significant advancements in 3D reconstruction and dense RGB-D SLAM systems. One notable development is the application of Neural Radiance Fields (NeRF) in these systems, which utilizes implicit neural representation to encode 3D scenes. This extension of NeRF to SLAM has shown promising results. However, the depth images obtained from consumer-grade RGB-D sensors are often sparse and noisy, which poses significant challenges for 3D reconstruction and affects the accuracy of the representation of the scene <b>geometry.</b> Moreover, the original hierarchical feature grid with occupancy value is inaccurate for scene <b>geometry</b> representation. Furthermore, the existing methods select random pixels for camera tracking, which leads to inaccurate localization and is not robust in real-world indoor environments. To this end, we present NeSLAM, an advanced framework that achieves accurate and dense depth estimation, robust camera tracking, and realistic synthesis of novel views. First, a depth completion and denoising network is designed to provide dense <b>geometry</b> prior and guide the neural implicit representation optimization. Second, the occupancy scene representation is replaced with Signed Distance Field (SDF) hierarchical scene representation for high-quality reconstruction and view synthesis. Furthermore, we also propose a NeRF-based <b>self-supervised</b> feature tracking algorithm for robust real-time tracking. Experiments on various indoor datasets demonstrate the effectiveness and accuracy of the system in reconstruction, tracking quality, and novel view synthesis.</p></p class="citation"></blockquote><h3 id=4668--88204-stable-surface-regularization-for-fast-few-shot-nerf-byeongin-joung-et-al-2024>(46/68 | 88/204) Stable Surface Regularization for Fast Few-Shot NeRF (Byeongin Joung et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Byeongin Joung, Byeong-Uk Lee, Jaesung Choe, Ukcheol Shin, Minjun Kang, Taeyeop Lee, In So Kweon, Kuk-Jin Yoon. (2024)<br><strong>Stable Surface Regularization for Fast Few-Shot NeRF</strong><br><button class=copy-to-clipboard title="Stable Surface Regularization for Fast Few-Shot NeRF" index=88>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-88 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 15<br>Keywords: Few-shot, Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19985v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19985v1.pdf filename=2403.19985v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper proposes an algorithm for synthesizing novel views under <b>few-shot</b> setup. The main concept is to develop a stable surface regularization technique called Annealing Signed Distance Function (ASDF), which anneals the surface in a coarse-to-fine manner to accelerate convergence speed. We observe that the Eikonal loss - which is a widely known geometric regularization - requires dense training signal to shape different level-sets of SDF, leading to low-fidelity results under <b>few-shot</b> training. In contrast, the proposed surface regularization successfully reconstructs scenes and produce high-fidelity <b>geometry</b> with stable training. Our method is further accelerated by utilizing grid representation and monocular geometric priors. Finally, the proposed approach is up to 45 times faster than existing <b>few-shot</b> novel view synthesis methods, and it produces comparable results in the ScanNet dataset and NeRF-Real dataset.</p></p class="citation"></blockquote><h3 id=4768--89204-multi-region-transfer-learning-for-segmentation-of-crop-field-boundaries-in-satellite-images-with-limited-labels-hannah-kerner-et-al-2024>(47/68 | 89/204) Multi-Region Transfer Learning for Segmentation of Crop Field Boundaries in Satellite Images with Limited Labels (Hannah Kerner et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hannah Kerner, Saketh Sundar, Mathan Satish. (2024)<br><strong>Multi-Region Transfer Learning for Segmentation of Crop Field Boundaries in Satellite Images with Limited Labels</strong><br><button class=copy-to-clipboard title="Multi-Region Transfer Learning for Segmentation of Crop Field Boundaries in Satellite Images with Limited Labels" index=89>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-89 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00179v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00179v1.pdf filename=2404.00179v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The goal of field boundary delineation is to predict the polygonal boundaries and interiors of individual crop fields in overhead remotely sensed images (e.g., from satellites or drones). Automatic delineation of field boundaries is a necessary task for many real-world use cases in agriculture, such as estimating cultivated area in a region or predicting end-of-season yield in a field. Field boundary delineation can be framed as an instance segmentation problem, but presents unique research challenges compared to traditional computer vision datasets used for instance segmentation. The practical applicability of previous work is also limited by the assumption that a sufficiently-large labeled dataset is available where field boundary delineation models will be applied, which is not the reality for most regions (especially under-resourced regions such as Sub-Saharan Africa). We present an approach for segmentation of crop field boundaries in satellite images in regions lacking labeled data that uses multi-region <b>transfer</b> <b>learning</b> to adapt model weights for the target region. We show that our approach outperforms existing methods and that multi-region <b>transfer</b> <b>learning</b> substantially boosts performance for multiple model architectures. Our implementation and datasets are publicly available to enable use of the approach by end-users and serve as a <b>benchmark</b> for future work.</p></p class="citation"></blockquote><h3 id=4868--90204-modeling-weather-uncertainty-for-multi-weather-co-presence-estimation-qi-bi-et-al-2024>(48/68 | 90/204) Modeling Weather Uncertainty for Multi-weather Co-Presence Estimation (Qi Bi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qi Bi, Shaodi You, Theo Gevers. (2024)<br><strong>Modeling Weather Uncertainty for Multi-weather Co-Presence Estimation</strong><br><button class=copy-to-clipboard title="Modeling Weather Uncertainty for Multi-weather Co-Presence Estimation" index=90>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-90 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20092v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20092v1.pdf filename=2403.20092v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Images from outdoor scenes may be taken under various weather conditions. It is well studied that weather impacts the performance of computer vision algorithms and needs to be handled properly. However, existing algorithms model weather condition as a discrete status and estimate it using multi-label classification. The fact is that, physically, specifically in meteorology, weather are modeled as a continuous and transitional status. Instead of directly implementing hard classification as existing multi-weather classification methods do, we consider the physical formulation of multi-weather conditions and model the impact of physical-related parameter on learning from the image appearance. In this paper, we start with solid revisit of the physics definition of weather and how it can be described as a continuous machine learning and computer vision task. Namely, we propose to model the weather uncertainty, where the level of probability and co-existence of multiple weather conditions are both considered. A Gaussian mixture model is used to encapsulate the weather uncertainty and a uncertainty-aware multi-weather learning scheme is proposed based on prior-posterior learning. A novel multi-weather co-presence estimation <b>transformer</b> (MeFormer) is proposed. In addition, a new multi-weather co-presence estimation (MePe) dataset, along with 14 fine-grained weather categories and 16,078 samples, is proposed to <b>benchmark</b> both conventional multi-label weather classification task and multi-weather co-presence estimation task. Large scale experiments show that the proposed method achieves state-of-the-art performance and substantial generalization capabilities on both the conventional multi-label weather classification task and the proposed multi-weather co-presence estimation task. Besides, modeling weather uncertainty also benefits adverse-weather semantic segmentation.</p></p class="citation"></blockquote><h3 id=4968--91204-optimal-blackjack-strategy-recommender-a-comprehensive-study-on-computer-vision-integration-for-enhanced-gameplay-krishnanshu-gupta-et-al-2024>(49/68 | 91/204) Optimal Blackjack Strategy Recommender: A Comprehensive Study on Computer Vision Integration for Enhanced Gameplay (Krishnanshu Gupta et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Krishnanshu Gupta, Devon Bolt, Ben Hinchliff. (2024)<br><strong>Optimal Blackjack Strategy Recommender: A Comprehensive Study on Computer Vision Integration for Enhanced Gameplay</strong><br><button class=copy-to-clipboard title="Optimal Blackjack Strategy Recommender: A Comprehensive Study on Computer Vision Integration for Enhanced Gameplay" index=91>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-91 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: I-4-9; I-5-3; I-5-4, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00191v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00191v1.pdf filename=2404.00191v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This research project investigates the application of several computer vision techniques for playing card detection and recognition in the context of the popular casino game, blackjack. The primary objective is to develop a robust system that is capable of detecting and accurately classifying playing cards in real-time, and displaying the optimal move <b>recommendation</b> based on the given image of the current game. The proposed methodology involves using K-Means for image segmentation, card reprojection and feature extraction, training of the KNN classifier using a labeled dataset, and integration of the detection system into a Blackjack Basic Strategy <b>recommendation</b> algorithm. Further, the study aims to observe the effectiveness of this approach in detecting various card designs under different lighting conditions and occlusions. Overall, the project examines the potential benefits of incorporating computer vision techniques, with a specific focus on card detection, into commonly played games aiming to enhance player decision-making and optimize strategic outcomes. The results obtained from our experimental evaluations with models developed under considerable time constraints, highlight the potential for practical implementation in real-world casino environments and across other similarly structured games.</p></p class="citation"></blockquote><h3 id=5068--92204-universal-bovine-identification-via-depth-data-and-deep-metric-learning-asheesh-sharma-et-al-2024>(50/68 | 92/204) Universal Bovine Identification via Depth Data and Deep Metric Learning (Asheesh Sharma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Asheesh Sharma, Lucy Randewich, William Andrew, Sion Hannuna, Neill Campbell, Siobhan Mullan, Andrew W. Dowsey, Melvyn Smith, Mark Hansen, Tilo Burghardt. (2024)<br><strong>Universal Bovine Identification via Depth Data and Deep Metric Learning</strong><br><button class=copy-to-clipboard title="Universal Bovine Identification via Depth Data and Deep Metric Learning" index=92>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-92 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 10<br>Keywords: Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00172v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00172v1.pdf filename=2404.00172v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper proposes and evaluates, for the first time, a top-down (dorsal view), depth-only deep learning system for accurately identifying individual cattle and provides associated code, datasets, and training weights for immediate reproducibility. An increase in herd size skews the cow-to-human ratio at the farm and makes the manual monitoring of individuals more challenging. Therefore, real-time cattle identification is essential for the farms and a crucial step towards precision livestock farming. Underpinned by our previous work, this paper introduces a deep-metric learning method for cattle identification using depth data from an off-the-shelf 3D camera. The method relies on <b>CNN</b> and MLP backbones that learn well-generalised embedding spaces from the body shape to differentiate individuals &ndash; requiring neither species-specific coat patterns nor close-up muzzle prints for operation. The network embeddings are clustered using a simple algorithm such as $k$-NN for highly accurate identification, thus eliminating the need to retrain the network for enrolling new individuals. We evaluate two backbone architectures, ResNet, as previously used to identify Holstein Friesians using RGB images, and PointNet, which is specialised to operate on 3D point clouds. We also present CowDepth2023, a new dataset containing 21,490 synchronised colour-depth image pairs of 99 cows, to evaluate the backbones. Both ResNet and PointNet architectures, which consume depth maps and point clouds, respectively, led to high accuracy that is on par with the coat pattern-based backbone.</p></p class="citation"></blockquote><h3 id=5168--93204-mtlora-a-low-rank-adaptation-approach-for-efficient-multi-task-learning-ahmed-agiza-et-al-2024>(51/68 | 93/204) MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning (Ahmed Agiza et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ahmed Agiza, Marina Neseem, Sherief Reda. (2024)<br><strong>MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning</strong><br><button class=copy-to-clipboard title="MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning" index=93>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-93 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 10<br>Keywords: Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20320v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20320v1.pdf filename=2403.20320v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Adapting models pre-trained on large-scale datasets to a variety of downstream tasks is a common strategy in deep learning. Consequently, parameter-efficient <b>fine-tuning</b> methods have emerged as a promising way to adapt pre-trained models to different tasks while training only a minimal number of parameters. While most of these methods are designed for single-task adaptation, parameter-efficient training in Multi-Task Learning (MTL) architectures is still unexplored. In this paper, we introduce MTLoRA, a novel framework for parameter-efficient training of MTL models. MTLoRA employs Task-Agnostic and Task-Specific Low-Rank Adaptation modules, which effectively disentangle the parameter space in MTL <b>fine-tuning,</b> thereby enabling the model to adeptly handle both task specialization and interaction within MTL contexts. We applied MTLoRA to hierarchical-transformer-based MTL architectures, adapting them to multiple downstream dense prediction tasks. Our extensive experiments on the PASCAL dataset show that MTLoRA achieves higher accuracy on downstream tasks compared to fully <b>fine-tuning</b> the MTL model while reducing the number of trainable parameters by 3.6x. Furthermore, MTLoRA establishes a Pareto-optimal trade-off between the number of trainable parameters and the accuracy of the downstream tasks, outperforming current state-of-the-art parameter-efficient training methods in both accuracy and efficiency. Our code is publicly available.</p></p class="citation"></blockquote><h3 id=5268--94204-u-vap-user-specified-visual-appearance-personalization-via-decoupled-self-augmentation-you-wu-et-al-2024>(52/68 | 94/204) U-VAP: User-specified Visual Appearance Personalization via Decoupled Self Augmentation (You Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>You Wu, Kean Liu, Xiaoyue Mi, Fan Tang, Juan Cao, Jintao Li. (2024)<br><strong>U-VAP: User-specified Visual Appearance Personalization via Decoupled Self Augmentation</strong><br><button class=copy-to-clipboard title="U-VAP: User-specified Visual Appearance Personalization via Decoupled Self Augmentation" index=94>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-94 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20231v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20231v1.pdf filename=2403.20231v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Concept personalization methods enable large <b>text-to-image</b> models to learn specific subjects (e.g., objects/poses/3D models) and synthesize renditions in new contexts. Given that the image references are highly biased towards visual attributes, state-of-the-art personalization models tend to overfit the whole subject and cannot disentangle visual characteristics in pixel space. In this study, we proposed a more challenging setting, namely fine-grained visual appearance personalization. Different from existing methods, we allow users to provide a sentence describing the desired attributes. A novel decoupled self-augmentation strategy is proposed to generate target-related and non-target samples to learn user-specified visual attributes. These augmented data allow for refining the model&rsquo;s understanding of the target attribute while mitigating the impact of unrelated attributes. At the inference stage, adjustments are conducted on semantic space through the learned target and non-target embeddings to further enhance the disentanglement of target attributes. Extensive experiments on various kinds of visual attributes with SOTA personalization methods show the ability of the proposed method to mimic target visual appearance in novel contexts, thus improving the controllability and flexibility of personalization.</p></p class="citation"></blockquote><h3 id=5368--95204-sketch-to-architecture-generative-ai-aided-architectural-design-pengzhi-li-et-al-2024>(53/68 | 95/204) Sketch-to-Architecture: Generative AI-aided Architectural Design (Pengzhi Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pengzhi Li, Baijuan Li, Zhiheng Li. (2024)<br><strong>Sketch-to-Architecture: Generative AI-aided Architectural Design</strong><br><button class=copy-to-clipboard title="Sketch-to-Architecture: Generative AI-aided Architectural Design" index=95>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-95 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Generative AI<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20186v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20186v1.pdf filename=2403.20186v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, the development of large-scale models has paved the way for various interdisciplinary research, including architecture. By using <b>generative</b> <b>AI,</b> we present a novel workflow that utilizes AI models to generate conceptual floorplans and 3D models from simple sketches, enabling rapid ideation and controlled generation of architectural renderings based on textual descriptions. Our work demonstrates the potential of <b>generative</b> <b>AI</b> in the architectural design process, pointing towards a new direction of computer-aided architectural design. Our project website is available at: <a href=https://zrealli.github.io/sketch2arc>https://zrealli.github.io/sketch2arc</a></p></p class="citation"></blockquote><h3 id=5468--96204-harmamba-efficient-wearable-sensor-human-activity-recognition-based-on-bidirectional-selective-ssm-shuangjian-li-et-al-2024>(54/68 | 96/204) HARMamba: Efficient Wearable Sensor Human Activity Recognition Based on Bidirectional Selective SSM (Shuangjian Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shuangjian Li, Tao Zhu, Furong Duan, Liming Chen, Huansheng Ning, Yaping Wan. (2024)<br><strong>HARMamba: Efficient Wearable Sensor Human Activity Recognition Based on Bidirectional Selective SSM</strong><br><button class=copy-to-clipboard title="HARMamba: Efficient Wearable Sensor Human Activity Recognition Based on Bidirectional Selective SSM" index=96>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-96 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20183v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20183v1.pdf filename=2403.20183v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Wearable sensor human activity recognition (HAR) is a crucial area of research in activity sensing. While <b>transformer-based</b> temporal deep learning models have been extensively studied and implemented, their large number of parameters present significant challenges in terms of system computing load and memory usage, rendering them unsuitable for real-time mobile activity recognition applications. Recently, an efficient hardware-aware state space model (SSM) called Mamba has emerged as a promising alternative. Mamba demonstrates strong potential in long sequence modeling, boasts a simpler network architecture, and offers an efficient hardware-aware design. Leveraging SSM for activity recognition represents an appealing avenue for exploration. In this study, we introduce HARMamba, which employs a more lightweight selective SSM as the foundational model architecture for activity recognition. The goal is to address the computational resource constraints encountered in real-time activity recognition scenarios. Our approach involves processing sensor data flow by independently learning each channel and segmenting the data into &ldquo;patches&rdquo;. The marked sensor sequence&rsquo;s position embedding serves as the input token for the bidirectional state space model, ultimately leading to activity categorization through the classification head. Compared to established activity recognition frameworks like <b>Transformer-based</b> models, HARMamba achieves superior performance while also reducing computational and memory overhead. Furthermore, our proposed method has been extensively tested on four public activity datasets: PAMAP2, WISDM, UNIMIB, and UCI, demonstrating impressive performance in activity recognition tasks.</p></p class="citation"></blockquote><h3 id=5568--97204-mcnet-a-crowd-denstity-estimation-network-based-on-integrating-multiscale-attention-module-qiang-guo-et-al-2024>(55/68 | 97/204) MCNet: A crowd denstity estimation network based on integrating multiscale attention module (Qiang Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qiang Guo, Rubo Zhang, Di Zhao. (2024)<br><strong>MCNet: A crowd denstity estimation network based on integrating multiscale attention module</strong><br><button class=copy-to-clipboard title="MCNet: A crowd denstity estimation network based on integrating multiscale attention module" index=97>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-97 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20173v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20173v1.pdf filename=2403.20173v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Aiming at the metro video surveillance system has not been able to effectively solve the metro crowd density estimation problem, a Metro Crowd density estimation Network (called MCNet) is proposed to automatically classify crowd density level of passengers. Firstly, an Integrating Multi-scale Attention (IMA) module is proposed to enhance the ability of the plain classifiers to extract semantic crowd texture features to accommodate to the characteristics of the crowd texture feature. The innovation of the IMA module is to fuse the dilation <b>convolution,</b> multiscale feature extraction and attention mechanism to obtain multi-scale crowd feature activation from a larger receptive field with lower computational cost, and to strengthen the crowds activation state of <b>convolutional</b> features in top layers. Secondly, a novel lightweight crowd texture feature extraction network is proposed, which can directly process video frames and automatically extract texture features for crowd density estimation, while its faster image processing speed and fewer network parameters make it flexible to be deployed on embedded platforms with limited hardware resources. Finally, this paper integrates IMA module and the lightweight crowd texture feature extraction network to construct the MCNet, and validate the feasibility of this network on image classification dataset: Cifar10 and four crowd density datasets: PETS2009, Mall, QUT and SH_METRO to validate the MCNet whether can be a suitable solution for crowd density estimation in metro video surveillance where there are image processing challenges such as high density, high occlusion, perspective distortion and limited hardware resources.</p></p class="citation"></blockquote><h3 id=5668--98204-segmentation-classification-and-interpretation-of-breast-cancer-medical-images-using-human-in-the-loop-machine-learning-david-vázquez-lema-et-al-2024>(56/68 | 98/204) Segmentation, Classification and Interpretation of Breast Cancer Medical Images using Human-in-the-Loop Machine Learning (David Vázquez-Lema et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>David Vázquez-Lema, Eduardo Mosqueira-Rey, Elena Hernández-Pereira, Carlos Fernández-Lozano, Fernando Seara-Romera, Jorge Pombo-Otero. (2024)<br><strong>Segmentation, Classification and Interpretation of Breast Cancer Medical Images using Human-in-the-Loop Machine Learning</strong><br><button class=copy-to-clipboard title="Segmentation, Classification and Interpretation of Breast Cancer Medical Images using Human-in-the-Loop Machine Learning" index=98>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-98 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: I-2, cs-CV, cs-LG, cs.CV<br>Keyword Score: 10<br>Keywords: human-in-the-loop<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20112v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20112v1.pdf filename=2403.20112v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper explores the application of <b>Human-in-the-Loop</b> (HITL) strategies in training machine learning models in the medical domain. In this case a doctor-in-the-loop approach is proposed to leverage human expertise in dealing with large and complex data. Specifically, the paper deals with the integration of genomic data and Whole Slide Imaging (WSI) analysis of breast cancer. Three different tasks were developed: segmentation of histopathological images, classification of this images regarding the genomic subtype of the cancer and, finally, interpretation of the machine learning results. The involvement of a pathologist helped us to develop a better segmentation model and to enhance the explainatory capabilities of the models, but the classification results were suboptimal, highlighting the limitations of this approach: despite involving human experts, complex domains can still pose challenges, and a HITL approach may not always be effective.</p></p class="citation"></blockquote><h3 id=5768--99204-grounding-and-enhancing-grid-based-models-for-neural-fields-zelin-zhao-et-al-2024>(57/68 | 99/204) Grounding and Enhancing Grid-based Models for Neural Fields (Zelin Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zelin Zhao, Fenglei Fan, Wenlong Liao, Junchi Yan. (2024)<br><strong>Grounding and Enhancing Grid-based Models for Neural Fields</strong><br><button class=copy-to-clipboard title="Grounding and Enhancing Grid-based Models for Neural Fields" index=99>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-99 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Grounding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20002v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20002v1.pdf filename=2403.20002v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Many contemporary studies utilize grid-based models for neural field representation, but a systematic analysis of grid-based models is still missing, hindering the improvement of those models. Therefore, this paper introduces a theoretical framework for grid-based models. This framework points out that these models&rsquo; approximation and generalization behaviors are determined by grid tangent kernels (GTK), which are intrinsic properties of grid-based models. The proposed framework facilitates a consistent and systematic analysis of diverse grid-based models. Furthermore, the introduced framework motivates the development of a novel grid-based model named the Multiplicative Fourier Adaptive Grid (MulFAGrid). The numerical analysis demonstrates that MulFAGrid exhibits a lower generalization bound than its predecessors, indicating its robust generalization performance. Empirical studies reveal that MulFAGrid achieves state-of-the-art performance in various tasks, including 2D image fitting, 3D signed distance field (SDF) reconstruction, and novel view synthesis, demonstrating superior representation ability. The project website is available at <a href=https://sites.google.com/view/cvpr24-2034-submission/home>https://sites.google.com/view/cvpr24-2034-submission/home</a>.</p></p class="citation"></blockquote><h3 id=5868--100204-binarized-low-light-raw-video-enhancement-gengchen-zhang-et-al-2024>(58/68 | 100/204) Binarized Low-light Raw Video Enhancement (Gengchen Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gengchen Zhang, Yulun Zhang, Xin Yuan, Ying Fu. (2024)<br><strong>Binarized Low-light Raw Video Enhancement</strong><br><button class=copy-to-clipboard title="Binarized Low-light Raw Video Enhancement" index=100>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-100 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV<br>Keyword Score: 10<br>Keywords: Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19944v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19944v1.pdf filename=2403.19944v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, deep neural networks have achieved excellent performance on low-light raw video enhancement. However, they often come with high computational complexity and large memory costs, which hinder their applications on resource-limited devices. In this paper, we explore the feasibility of applying the extremely compact binary neural network (BNN) to low-light raw video enhancement. Nevertheless, there are two main issues with binarizing video enhancement models. One is how to fuse the temporal information to improve low-light denoising without complex modules. The other is how to narrow the performance gap between binary <b>convolutions</b> with the full precision ones. To address the first issue, we introduce a spatial-temporal shift operation, which is easy-to-binarize and effective. The temporal shift efficiently aggregates the features of neighbor frames and the spatial shift handles the misalignment caused by the large motion in videos. For the second issue, we present a distribution-aware binary <b>convolution,</b> which captures the distribution characteristics of real-valued input and incorporates them into plain binary <b>convolutions</b> to alleviate the degradation in performance. Extensive quantitative and qualitative experiments have shown our high-efficiency binarized low-light raw video enhancement method can attain a promising performance.</p></p class="citation"></blockquote><h3 id=5968--101204-scenetracker-long-term-scene-flow-estimation-network-bo-wang-et-al-2024>(59/68 | 101/204) SceneTracker: Long-term Scene Flow Estimation Network (Bo Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bo Wang, Jian Li, Yang Yu, Li Liu, Zhenping Sun, Dewen Hu. (2024)<br><strong>SceneTracker: Long-term Scene Flow Estimation Network</strong><br><button class=copy-to-clipboard title="SceneTracker: Long-term Scene Flow Estimation Network" index=101>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-101 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19924v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19924v1.pdf filename=2403.19924v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Considering the complementarity of scene flow estimation in the spatial domain&rsquo;s focusing capability and 3D object tracking in the temporal domain&rsquo;s coherence, this study aims to address a comprehensive new task that can simultaneously capture fine-grained and long-term 3D motion in an online manner: long-term scene flow estimation (LSFE). We introduce SceneTracker, a novel learning-based LSFE network that adopts an iterative approach to approximate the optimal trajectory. Besides, it dynamically indexes and constructs appearance and depth correlation features simultaneously and employs the <b>Transformer</b> to explore and utilize long-range connections within and between trajectories. With detailed experiments, SceneTracker shows superior capabilities in handling 3D spatial occlusion and depth noise interference, highly tailored to the LSFE task&rsquo;s needs. The code for SceneTracker is available at <a href=https://github.com/wwsource/SceneTracker>https://github.com/wwsource/SceneTracker</a>.</p></p class="citation"></blockquote><h3 id=6068--102204-disentangling-racial-phenotypes-fine-grained-control-of-race-related-facial-phenotype-characteristics-seyma-yucer-et-al-2024>(60/68 | 102/204) Disentangling Racial Phenotypes: Fine-Grained Control of Race-related Facial Phenotype Characteristics (Seyma Yucer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Seyma Yucer, Amir Atapour Abarghouei, Noura Al Moubayed, Toby P. Breckon. (2024)<br><strong>Disentangling Racial Phenotypes: Fine-Grained Control of Race-related Facial Phenotype Characteristics</strong><br><button class=copy-to-clipboard title="Disentangling Racial Phenotypes: Fine-Grained Control of Race-related Facial Phenotype Characteristics" index=102>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-102 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 10<br>Keywords: Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19897v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19897v1.pdf filename=2403.19897v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Achieving an effective fine-grained appearance variation over 2D facial images, whilst preserving facial identity, is a challenging task due to the high complexity and entanglement of common 2D facial feature encoding spaces. Despite these challenges, such fine-grained control, by way of disentanglement is a crucial enabler for data-driven racial bias mitigation strategies across multiple automated facial analysis tasks, as it allows to analyse, characterise and synthesise human facial diversity. In this paper, we propose a novel <b>GAN</b> framework to enable fine-grained control over individual race-related phenotype attributes of the facial images. Our framework factors the latent (feature) space into elements that correspond to race-related facial phenotype representations, thereby separating phenotype aspects (e.g. skin, hair colour, nose, eye, mouth shapes), which are notoriously difficult to annotate robustly in real-world facial data. Concurrently, we also introduce a high quality augmented, diverse 2D face image dataset drawn from CelebA-HQ for <b>GAN</b> training. Unlike prior work, our framework only relies upon 2D imagery and related parameters to achieve state-of-the-art individual control over race-related phenotype attributes with improved photo-realistic output.</p></p class="citation"></blockquote><h3 id=6168--103204-ploc-a-new-evaluation-criterion-based-on-physical-location-for-autonomous-driving-datasets-ruining-yang-et-al-2024>(61/68 | 103/204) PLoc: A New Evaluation Criterion Based on Physical Location for Autonomous Driving Datasets (Ruining Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruining Yang, Yuqi Peng. (2024)<br><strong>PLoc: A New Evaluation Criterion Based on Physical Location for Autonomous Driving Datasets</strong><br><button class=copy-to-clipboard title="PLoc: A New Evaluation Criterion Based on Physical Location for Autonomous Driving Datasets" index=103>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-103 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Object Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19893v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19893v1.pdf filename=2403.19893v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Autonomous driving has garnered significant attention as a key research area within artificial intelligence. In the context of autonomous driving scenarios, the varying physical locations of <b>objects</b> <b>correspond</b> to different levels of danger. However, conventional evaluation criteria for automatic driving <b>object</b> <b>detection</b> often overlook the crucial aspect of an <b>object&rsquo;s</b> <b>physical</b> location, leading to evaluation results that may not accurately reflect the genuine threat posed by the <b>object</b> <b>to</b> the autonomous driving vehicle. To enhance the safety of autonomous driving, this paper introduces a novel evaluation criterion based on physical location information, termed PLoc. This criterion transcends the limitations of traditional criteria by acknowledging that the physical location of pedestrians in autonomous driving scenarios can provide valuable safety-related information. Furthermore, this paper presents a newly re-annotated dataset (ApolloScape-R) derived from ApolloScape. ApolloScape-R involves the relabeling of pedestrians based on the significance of their physical location. The dataset is utilized to assess the performance of various <b>object</b> <b>detection</b> models under the proposed PLoc criterion. Experimental results demonstrate that the average accuracy of all <b>object</b> <b>detection</b> models in identifying a person situated in the travel lane of an autonomous vehicle is lower than that for a person on a sidewalk. The dataset is publicly available at <a href=https://github.com/lnyrlyed/ApolloScape-R.git>https://github.com/lnyrlyed/ApolloScape-R.git</a></p></p class="citation"></blockquote><h3 id=6268--104204-talk3d-high-fidelity-talking-portrait-synthesis-via-personalized-3d-generative-prior-jaehoon-ko-et-al-2024>(62/68 | 104/204) Talk3D: High-Fidelity Talking Portrait Synthesis via Personalized 3D Generative Prior (Jaehoon Ko et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jaehoon Ko, Kyusun Cho, Joungbin Lee, Heeji Yoon, Sangmin Lee, Sangjun Ahn, Seungryong Kim. (2024)<br><strong>Talk3D: High-Fidelity Talking Portrait Synthesis via Personalized 3D Generative Prior</strong><br><button class=copy-to-clipboard title="Talk3D: High-Fidelity Talking Portrait Synthesis via Personalized 3D Generative Prior" index=104>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-104 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 8<br>Keywords: Benchmarking, Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20153v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20153v1.pdf filename=2403.20153v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent methods for audio-driven talking head synthesis often optimize neural radiance fields (NeRF) on a monocular talking portrait video, leveraging its capability to render high-fidelity and 3D-consistent novel-view frames. However, they often struggle to reconstruct complete face <b>geometry</b> due to the absence of comprehensive 3D information in the input monocular videos. In this paper, we introduce a novel audio-driven talking head synthesis framework, called Talk3D, that can faithfully reconstruct its plausible facial geometries by effectively adopting the pre-trained 3D-aware generative prior. Given the personalized 3D generative model, we present a novel audio-guided attention U-Net architecture that predicts the dynamic face variations in the NeRF space driven by audio. Furthermore, our model is further modulated by audio-unrelated conditioning tokens which effectively disentangle variations unrelated to audio features. Compared to existing methods, our method excels in generating realistic facial geometries even under extreme head poses. We also conduct extensive experiments showing our approach surpasses state-of-the-art <b>benchmarks</b> in terms of both quantitative and qualitative evaluations.</p></p class="citation"></blockquote><h3 id=6368--105204-multi-level-neural-scene-graphs-for-dynamic-urban-environments-tobias-fischer-et-al-2024>(63/68 | 105/204) Multi-Level Neural Scene Graphs for Dynamic Urban Environments (Tobias Fischer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tobias Fischer, Lorenzo Porzi, Samuel Rota Bulò, Marc Pollefeys, Peter Kontschieder. (2024)<br><strong>Multi-Level Neural Scene Graphs for Dynamic Urban Environments</strong><br><button class=copy-to-clipboard title="Multi-Level Neural Scene Graphs for Dynamic Urban Environments" index=105>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-105 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 6<br>Keywords: Graph, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00168v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00168v1.pdf filename=2404.00168v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We estimate the radiance field of large-scale dynamic areas from multiple vehicle captures under varying environmental conditions. Previous works in this domain are either restricted to static environments, do not scale to more than a single short video, or struggle to separately represent dynamic object instances. To this end, we present a novel, decomposable radiance field approach for dynamic urban environments. We propose a multi-level neural scene <b>graph</b> representation that scales to thousands of images from dozens of sequences with hundreds of fast-moving objects. To enable efficient training and rendering of our representation, we develop a fast composite ray sampling and rendering scheme. To test our approach in urban driving scenarios, we introduce a new, novel view synthesis <b>benchmark.</b> We show that our approach outperforms prior art by a significant margin on both established and our proposed <b>benchmark</b> while being faster in training and rendering.</p></p class="citation"></blockquote><h3 id=6468--106204-fisbe-a-real-world-benchmark-dataset-for-instance-segmentation-of-long-range-thin-filamentous-structures-lisa-mais-et-al-2024>(64/68 | 106/204) FISBe: A real-world benchmark dataset for instance segmentation of long-range thin filamentous structures (Lisa Mais et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lisa Mais, Peter Hirsch, Claire Managan, Ramya Kandarpa, Josef Lorenz Rumberger, Annika Reinke, Lena Maier-Hein, Gudrun Ihrke, Dagmar Kainmueller. (2024)<br><strong>FISBe: A real-world benchmark dataset for instance segmentation of long-range thin filamentous structures</strong><br><button class=copy-to-clipboard title="FISBe: A real-world benchmark dataset for instance segmentation of long-range thin filamentous structures" index=106>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-106 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 6<br>Keywords: Benchmarking, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00130v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00130v1.pdf filename=2404.00130v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Instance segmentation of neurons in volumetric light microscopy images of nervous systems enables groundbreaking research in neuroscience by facilitating joint functional and morphological analyses of neural circuits at cellular resolution. Yet said multi-neuron light microscopy data exhibits extremely challenging properties for the task of instance segmentation: Individual neurons have long-ranging, thin filamentous and widely branching morphologies, multiple neurons are tightly inter-weaved, and partial volume effects, uneven illumination and noise inherent to light microscopy severely impede local disentangling as well as long-range tracing of individual neurons. These properties reflect a current key challenge in machine learning research, namely to effectively capture long-range dependencies in the data. While respective methodological research is buzzing, to date methods are typically <b>benchmarked</b> on synthetic datasets. To address this gap, we release the FlyLight Instance Segmentation <b>Benchmark</b> (FISBe) dataset, the first publicly available multi-neuron light microscopy dataset with pixel-wise annotations. In addition, we define a set of instance segmentation metrics for <b>benchmarking</b> that we designed to be meaningful with regard to downstream analyses. Lastly, we provide three baselines to kick off a competition that we envision to both advance the field of machine learning regarding methodology for capturing long-range data dependencies, and facilitate scientific discovery in basic neuroscience.</p></p class="citation"></blockquote><h3 id=6568--107204-benchmarking-the-robustness-of-temporal-action-detection-models-against-temporal-corruptions-runhao-zeng-et-al-2024>(65/68 | 107/204) Benchmarking the Robustness of Temporal Action Detection Models Against Temporal Corruptions (Runhao Zeng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Runhao Zeng, Xiaoyong Chen, Jiaming Liang, Huisi Wu, Guangzhong Cao, Yong Guo. (2024)<br><strong>Benchmarking the Robustness of Temporal Action Detection Models Against Temporal Corruptions</strong><br><button class=copy-to-clipboard title="Benchmarking the Robustness of Temporal Action Detection Models Against Temporal Corruptions" index=107>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-107 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 6<br>Keywords: Benchmarking, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20254v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20254v1.pdf filename=2403.20254v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Temporal action detection (TAD) aims to locate action positions and recognize action categories in long-term untrimmed videos. Although many methods have achieved promising results, their robustness has not been thoroughly studied. In practice, we observe that temporal information in videos can be occasionally corrupted, such as missing or blurred frames. Interestingly, existing methods often incur a significant performance drop even if only one frame is affected. To formally evaluate the robustness, we establish two temporal corruption robustness <b>benchmarks,</b> namely THUMOS14-C and ActivityNet-v1.3-C. In this paper, we extensively analyze the robustness of seven leading TAD methods and obtain some interesting findings: 1) Existing methods are particularly vulnerable to temporal corruptions, and end-to-end methods are often more susceptible than those with a pre-trained feature extractor; 2) Vulnerability mainly comes from localization error rather than classification error; 3) When corruptions occur in the middle of an action instance, TAD models tend to yield the largest performance drop. Besides building a <b>benchmark,</b> we further develop a simple but effective robust training method to defend against temporal corruptions, through the FrameDrop augmentation and Temporal-Robust Consistency loss. Remarkably, our approach not only improves robustness but also yields promising improvements on clean data. We believe that this study will serve as a <b>benchmark</b> for future research in robust video analysis. Source code and models are available at <a href=https://github.com/Alvin-Zeng/temporal-robustness-benchmark>https://github.com/Alvin-Zeng/temporal-robustness-benchmark</a>.</p></p class="citation"></blockquote><h3 id=6668--108204-snap-it-tap-it-splat-it-tactile-informed-3d-gaussian-splatting-for-reconstructing-challenging-surfaces-mauro-comi-et-al-2024>(66/68 | 108/204) Snap-it, Tap-it, Splat-it: Tactile-Informed 3D Gaussian Splatting for Reconstructing Challenging Surfaces (Mauro Comi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mauro Comi, Alessio Tonioni, Max Yang, Jonathan Tremblay, Valts Blukis, Yijiong Lin, Nathan F. Lepora, Laurence Aitchison. (2024)<br><strong>Snap-it, Tap-it, Splat-it: Tactile-Informed 3D Gaussian Splatting for Reconstructing Challenging Surfaces</strong><br><button class=copy-to-clipboard title="Snap-it, Tap-it, Splat-it: Tactile-Informed 3D Gaussian Splatting for Reconstructing Challenging Surfaces" index=108>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-108 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20275v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20275v1.pdf filename=2403.20275v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Touch and vision go hand in hand, mutually enhancing our ability to understand the world. From a research perspective, the problem of mixing touch and vision is underexplored and presents interesting challenges. To this end, we propose Tactile-Informed 3DGS, a novel approach that incorporates touch data (local depth maps) with multi-view vision data to achieve surface reconstruction and novel view synthesis. Our method optimises 3D Gaussian primitives to accurately model the object&rsquo;s <b>geometry</b> at points of contact. By creating a framework that decreases the transmittance at touch locations, we achieve a refined surface reconstruction, ensuring a uniformly smooth depth map. Touch is particularly useful when considering non-Lambertian objects (e.g. shiny or reflective surfaces) since contemporary methods tend to fail to reconstruct with fidelity specular highlights. By combining vision and tactile sensing, we achieve more accurate <b>geometry</b> reconstructions with fewer images than prior methods. We conduct evaluation on objects with glossy and reflective surfaces and demonstrate the effectiveness of our approach, offering significant improvements in reconstruction quality.</p></p class="citation"></blockquote><h3 id=6768--109204-prototype-based-interpretable-breast-cancer-prediction-models-analysis-and-challenges-shreyasi-pathak-et-al-2024>(67/68 | 109/204) Prototype-based Interpretable Breast Cancer Prediction Models: Analysis and Challenges (Shreyasi Pathak et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shreyasi Pathak, Jörg Schlötterer, Jeroen Veltman, Jeroen Geerdink, Maurice van Keulen, Christin Seifert. (2024)<br><strong>Prototype-based Interpretable Breast Cancer Prediction Models: Analysis and Challenges</strong><br><button class=copy-to-clipboard title="Prototype-based Interpretable Breast Cancer Prediction Models: Analysis and Challenges" index=109>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-109 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 5<br>Keywords: Black Box<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20260v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20260v1.pdf filename=2403.20260v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep learning models have achieved high performance in medical applications, however, their adoption in clinical practice is hindered due to their <b>black-box</b> <b>nature.</b> Self-explainable models, like prototype-based models, can be especially beneficial as they are interpretable by design. However, if the learnt prototypes are of low quality then the prototype-based models are as good as <b>black-box.</b> <b>Having</b> high quality prototypes is a pre-requisite for a truly interpretable model. In this work, we propose a prototype evaluation framework for coherence (PEF-C) for quantitatively evaluating the quality of the prototypes based on domain knowledge. We show the use of PEF-C in the context of breast cancer prediction using mammography. Existing works on prototype-based models on breast cancer prediction using mammography have focused on improving the classification performance of prototype-based models compared to <b>black-box</b> <b>models</b> and have evaluated prototype quality through anecdotal evidence. We are the first to go beyond anecdotal evidence and evaluate the quality of the mammography prototypes systematically using our PEF-C. Specifically, we apply three state-of-the-art prototype-based models, ProtoPNet, BRAIxProtoPNet++ and PIP-Net on mammography images for breast cancer prediction and evaluate these models w.r.t. i) classification performance, and ii) quality of the prototypes, on three public datasets. Our results show that prototype-based models are competitive with <b>black-box</b> <b>models</b> in terms of classification performance, and achieve a higher score in detecting ROIs. However, the quality of the prototypes are not yet sufficient and can be improved in aspects of relevance, purity and learning a variety of prototypes. We call the XAI community to systematically evaluate the quality of the prototypes to check their true usability in high stake decisions and improve such models further.</p></p class="citation"></blockquote><h3 id=6868--110204-fully-geometric-panoramic-localization-junho-kim-et-al-2024>(68/68 | 110/204) Fully Geometric Panoramic Localization (Junho Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junho Kim, Jiwon Jeong, Young Min Kim. (2024)<br><strong>Fully Geometric Panoramic Localization</strong><br><button class=copy-to-clipboard title="Fully Geometric Panoramic Localization" index=110>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-110 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19904v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19904v1.pdf filename=2403.19904v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce a lightweight and accurate localization method that only utilizes the <b>geometry</b> of 2D-3D lines. Given a pre-captured 3D map, our approach localizes a panorama image, taking advantage of the holistic 360 view. The system mitigates potential privacy breaches or domain discrepancies by avoiding trained or hand-crafted visual descriptors. However, as lines alone can be ambiguous, we express distinctive yet compact spatial contexts from relationships between lines, namely the dominant directions of parallel lines and the intersection between non-parallel lines. The resulting representations are efficient in processing time and memory compared to conventional visual descriptor-based methods. Given the groups of dominant line directions and their intersections, we accelerate the search process to test thousands of pose candidates in less than a millisecond without sacrificing accuracy. We empirically show that the proposed 2D-3D matching can localize panoramas for challenging scenes with similar structures, dramatic domain shifts or illumination changes. Our fully geometric approach does not involve extensive parameter tuning or neural network training, making it a practical algorithm that can be readily deployed in the real world. Project page including the code is available through this link: <a href=https://82magnolia.github.io/fgpl/>https://82magnolia.github.io/fgpl/</a>.</p></p class="citation"></blockquote><h2 id=cslg-18>cs.LG (18)</h2><h3 id=118--111204-unleashing-the-potential-of-large-language-models-for-predictive-tabular-tasks-in-data-science-yazheng-yang-et-al-2024>(1/18 | 111/204) Unleashing the Potential of Large Language Models for Predictive Tabular Tasks in Data Science (Yazheng Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yazheng Yang, Yuqi Wang, Sankalok Sen, Lei Li, Qi Liu. (2024)<br><strong>Unleashing the Potential of Large Language Models for Predictive Tabular Tasks in Data Science</strong><br><button class=copy-to-clipboard title="Unleashing the Potential of Large Language Models for Predictive Tabular Tasks in Data Science" index=111>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-111 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 73<br>Keywords: Benchmarking, Few-shot, Zero-shot, LLaMA, In-context Learning, In-context Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20208v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20208v1.pdf filename=2403.20208v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the domain of data science, the predictive tasks of classification, regression, and imputation of missing values are commonly encountered challenges associated with tabular data. This research endeavors to apply <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> towards addressing these predictive tasks. Despite their proficiency in comprehending natural language, <b>LLMs</b> fall short in dealing with structured tabular data. This limitation stems from their lacking exposure to the intricacies of tabular data during their foundational training. Our research aims to mitigate this gap by compiling a comprehensive corpus of tables annotated with instructions and executing <b>large-scale</b> <b>training</b> <b>of</b> <b>Llama-2</b> on this enriched dataset. Furthermore, we investigate the practical application of applying the trained model to <b>zero-shot</b> prediction, <b>few-shot</b> prediction, and <b>in-context</b> <b>learning</b> scenarios. Through extensive experiments, our methodology has shown significant improvements over existing <b>benchmarks.</b> These advancements highlight the efficacy of tailoring <b>LLM</b> training to solve table-related problems in data science, thereby establishing a new <b>benchmark</b> in the utilization of <b>LLMs</b> for enhancing tabular intelligence.</p></p class="citation"></blockquote><h3 id=218--112204-graph-neural-aggregation-diffusion-with-metastability-kaiyuan-cui-et-al-2024>(2/18 | 112/204) Graph Neural Aggregation-diffusion with Metastability (Kaiyuan Cui et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kaiyuan Cui, Xinyan Wang, Zicheng Zhang, Weichen Zhao. (2024)<br><strong>Graph Neural Aggregation-diffusion with Metastability</strong><br><button class=copy-to-clipboard title="Graph Neural Aggregation-diffusion with Metastability" index=112>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-112 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 46<br>Keywords: Message-Passing, Continuous Graph, Graph, Graph Neural Network, Graph Neural Network, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20221v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20221v1.pdf filename=2403.20221v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Continuous</b> <b>graph</b> <b>neural</b> <b>models</b> based on differential equations have expanded the architecture of <b>graph</b> <b>neural</b> <b>networks</b> <b>(GNNs).</b> Due to the connection between <b>graph</b> <b>diffusion</b> <b>and</b> message passing, diffusion-based models have been widely studied. However, diffusion naturally drives the system towards an equilibrium state, leading to issues like over-smoothing. To this end, we propose GRADE inspired by <b>graph</b> <b>aggregation-diffusion</b> <b>equations,</b> which includes the delicate balance between nonlinear diffusion and aggregation induced by interaction potentials. The node representations obtained through aggregation-diffusion equations exhibit metastability, indicating that features can aggregate into multiple clusters. In addition, the dynamics within these clusters can persist for long time periods, offering the potential to alleviate over-smoothing effects. This nonlinear diffusion in our model generalizes existing diffusion-based models and establishes a connection with classical <b>GNNs.</b> We prove that GRADE achieves competitive performance across various <b>benchmarks</b> and alleviates the over-smoothing issue in <b>GNNs</b> evidenced by the enhanced Dirichlet energy.</p></p class="citation"></blockquote><h3 id=318--113204-beyond-the-known-novel-class-discovery-for-open-world-graph-learning-yucheng-jin-et-al-2024>(3/18 | 113/204) Beyond the Known: Novel Class Discovery for Open-world Graph Learning (Yucheng Jin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yucheng Jin, Yun Xiong, Juncheng Fang, Xixi Wu, Dongxiao He, Xing Jia, Bingchen Zhao, Philip Yu. (2024)<br><strong>Beyond the Known: Novel Class Discovery for Open-world Graph Learning</strong><br><button class=copy-to-clipboard title="Beyond the Known: Novel Class Discovery for Open-world Graph Learning" index=113>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-113 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 46<br>Keywords: Message-Passing, Node Classification, Graph, Graph Neural Network, Graph Neural Network, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19907v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19907v1.pdf filename=2403.19907v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Node</b> <b>classification</b> on <b>graphs</b> <b>is</b> <b>of</b> great importance in many applications. Due to the limited labeling capability and evolution in real-world open scenarios, novel classes can emerge on unlabeled testing <b>nodes.</b> <b>However,</b> little attention has been paid to novel class discovery on <b>graphs.</b> <b>Discovering</b> <b>novel</b> classes is challenging as novel and known class <b>nodes</b> <b>are</b> correlated by edges, which makes their representations indistinguishable when applying message passing <b>GNNs.</b> Furthermore, the novel classes lack labeling information to guide the learning process. In this paper, we propose a novel method Open-world <b>gRAph</b> <b>neuraL</b> <b>network</b> (ORAL) to tackle these challenges. ORAL first detects correlations between classes through semi-supervised prototypical learning. Inter-class correlations are subsequently eliminated by the prototypical attention network, leading to distinctive representations for different classes. Furthermore, to fully explore multi-scale <b>graph</b> <b>features</b> <b>for</b> alleviating label deficiencies, ORAL generates pseudo-labels by aligning and ensembling label estimations from multiple stacked prototypical attention networks. Extensive experiments on several <b>benchmark</b> datasets show the effectiveness of our proposed method.</p></p class="citation"></blockquote><h3 id=418--114204-adaptive-decentralized-federated-learning-in-energy-and-latency-constrained-wireless-networks-zhigang-yan-et-al-2024>(4/18 | 114/204) Adaptive Decentralized Federated Learning in Energy and Latency Constrained Wireless Networks (Zhigang Yan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhigang Yan, Dong Li. (2024)<br><strong>Adaptive Decentralized Federated Learning in Energy and Latency Constrained Wireless Networks</strong><br><button class=copy-to-clipboard title="Adaptive Decentralized Federated Learning in Energy and Latency Constrained Wireless Networks" index=114>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-114 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-SY, cs.LG, eess-SY<br>Keyword Score: 33<br>Keywords: Graph, Federated Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20075v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20075v1.pdf filename=2403.20075v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In <b>Federated</b> <b>Learning</b> (FL), with parameter aggregated by a central node, the communication overhead is a substantial concern. To circumvent this limitation and alleviate the single point of failure within the FL framework, recent studies have introduced Decentralized <b>Federated</b> <b>Learning</b> (DFL) as a viable alternative. Considering the device heterogeneity, and energy cost associated with parameter aggregation, in this paper, the problem on how to efficiently leverage the limited resources available to enhance the model performance is investigated. Specifically, we formulate a problem that minimizes the loss function of DFL while considering energy and latency constraints. The proposed solution involves optimizing the number of local training rounds across diverse devices with varying resource budgets. To make this problem tractable, we first analyze the convergence of DFL with edge devices with different rounds of local training. The derived convergence bound reveals the impact of the rounds of local training on the model performance. Then, based on the derived bound, the closed-form solutions of rounds of local training in different devices are obtained. Meanwhile, since the solutions require the energy cost of aggregation as low as possible, we modify different <b>graph-based</b> aggregation schemes to solve this energy consumption minimization problem, which can be applied to different communication scenarios. Finally, a DFL framework which jointly considers the optimized rounds of local training and the energy-saving aggregation scheme is proposed. <b>Simulation</b> results show that, the proposed algorithm achieves a better performance than the conventional schemes with fixed rounds of local training, and consumes less energy than other traditional aggregation schemes.</p></p class="citation"></blockquote><h3 id=518--115204-pikelpn-mitigating-overlooked-inefficiencies-of-low-precision-neural-networks-marina-neseem-et-al-2024>(5/18 | 115/204) PikeLPN: Mitigating Overlooked Inefficiencies of Low-Precision Neural Networks (Marina Neseem et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marina Neseem, Conor McCullough, Randy Hsin, Chas Leichner, Shan Li, In Suk Chong, Andrew G. Howard, Lukasz Lew, Sherief Reda, Ville-Mikko Rautio, Daniele Moro. (2024)<br><strong>PikeLPN: Mitigating Overlooked Inefficiencies of Low-Precision Neural Networks</strong><br><button class=copy-to-clipboard title="PikeLPN: Mitigating Overlooked Inefficiencies of Low-Precision Neural Networks" index=115>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-115 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Convolution, Quantization, Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00103v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00103v1.pdf filename=2404.00103v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Low-precision <b>quantization</b> is recognized for its efficacy in neural network optimization. Our analysis reveals that non-quantized elementwise operations which are prevalent in layers such as parameterized activation functions, batch normalization, and <b>quantization</b> scaling dominate the inference cost of low-precision models. These non-quantized elementwise operations are commonly overlooked in SOTA efficiency metrics such as Arithmetic Computation Effort (ACE). In this paper, we propose ACEv2 - an extended version of ACE which offers a better alignment with the inference cost of <b>quantized</b> models and their energy consumption on ML hardware. Moreover, we introduce PikeLPN, a model that addresses these efficiency issues by applying <b>quantization</b> to both elementwise operations and multiply-accumulate operations. In particular, we present a novel <b>quantization</b> technique for batch normalization layers named QuantNorm which allows for quantizing the batch normalization parameters without compromising the model performance. Additionally, we propose applying Double <b>Quantization</b> where the <b>quantization</b> scaling parameters are <b>quantized.</b> Furthermore, we recognize and resolve the issue of distribution mismatch in Separable <b>Convolution</b> layers by introducing Distribution-Heterogeneous <b>Quantization</b> which enables quantizing them to low-precision. PikeLPN achieves Pareto-optimality in efficiency-accuracy trade-off with up to 3X efficiency improvement compared to SOTA low-precision models.</p></p class="citation"></blockquote><h3 id=618--116204-deepheteroiot-deep-local-and-global-learning-over-heterogeneous-iot-sensor-data-muhammad-sakib-khan-inan-et-al-2024>(6/18 | 116/204) DeepHeteroIoT: Deep Local and Global Learning over Heterogeneous IoT Sensor Data (Muhammad Sakib Khan Inan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Muhammad Sakib Khan Inan, Kewen Liao, Haifeng Shen, Prem Prakash Jayaraman, Dimitrios Georgakopoulos, Ming Jian Tang. (2024)<br><strong>DeepHeteroIoT: Deep Local and Global Learning over Heterogeneous IoT Sensor Data</strong><br><button class=copy-to-clipboard title="DeepHeteroIoT: Deep Local and Global Learning over Heterogeneous IoT Sensor Data" index=116>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-116 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, eess-SP<br>Keyword Score: 30<br>Keywords: Graph Attention Networks, Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19996v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19996v1.pdf filename=2403.19996v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Internet of Things (IoT) sensor data or readings evince variations in timestamp range, sampling frequency, geographical location, unit of measurement, etc. Such presented sequence data heterogeneity makes it difficult for traditional time series classification algorithms to perform well. Therefore, addressing the heterogeneity challenge demands learning not only the sub-patterns (local features) but also the overall pattern (global feature). To address the challenge of classifying heterogeneous IoT sensor data (e.g., categorizing sensor data types like temperature and humidity), we propose a novel deep learning model that incorporates both <b>Convolutional</b> <b>Neural</b> <b>Network</b> and Bi-directional <b>Gated</b> Recurrent Unit to learn local and global features respectively, in an end-to-end manner. Through rigorous experimentation on heterogeneous IoT sensor datasets, we validate the effectiveness of our proposed model, which outperforms recent state-of-the-art classification methods as well as several machine learning and deep learning baselines. In particular, the model achieves an average absolute improvement of 3.37% in Accuracy and 2.85% in F1-Score across datasets</p></p class="citation"></blockquote><h3 id=718--117204-decision-mamba-reinforcement-learning-via-sequence-modeling-with-selective-state-spaces-toshihiro-ota-2024>(7/18 | 117/204) Decision Mamba: Reinforcement Learning via Sequence Modeling with Selective State Spaces (Toshihiro Ota, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Toshihiro Ota. (2024)<br><strong>Decision Mamba: Reinforcement Learning via Sequence Modeling with Selective State Spaces</strong><br><button class=copy-to-clipboard title="Decision Mamba: Reinforcement Learning via Sequence Modeling with Selective State Spaces" index=117>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-117 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19925v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19925v1.pdf filename=2403.19925v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Decision <b>Transformer,</b> a promising approach that applies <b>Transformer</b> architectures to <b>reinforcement</b> <b>learning,</b> relies on causal <b>self-attention</b> to model sequences of states, actions, and rewards. While this method has shown competitive results, this paper investigates the integration of the Mamba framework, known for its advanced capabilities in efficient and effective sequence modeling, into the Decision <b>Transformer</b> architecture, focusing on the potential performance enhancements in sequential decision-making tasks. Our study systematically evaluates this integration by conducting a series of experiments across various decision-making environments, comparing the modified Decision <b>Transformer,</b> Decision Mamba, with its traditional counterpart. This work contributes to the advancement of sequential decision-making models, suggesting that the architecture and training methodology of neural networks can significantly impact their performance in complex tasks, and highlighting the potential of Mamba as a valuable tool for improving the efficacy of <b>Transformer-based</b> models in <b>reinforcement</b> <b>learning</b> scenarios.</p></p class="citation"></blockquote><h3 id=818--118204-mol-air-molecular-reinforcement-learning-with-adaptive-intrinsic-rewards-for-goal-directed-molecular-generation-jinyeong-park-et-al-2024>(8/18 | 118/204) Mol-AIR: Molecular Reinforcement Learning with Adaptive Intrinsic Rewards for Goal-directed Molecular Generation (Jinyeong Park et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinyeong Park, Jaegyoon Ahn, Jonghwan Choi, Jibum Kim. (2024)<br><strong>Mol-AIR: Molecular Reinforcement Learning with Adaptive Intrinsic Rewards for Goal-directed Molecular Generation</strong><br><button class=copy-to-clipboard title="Mol-AIR: Molecular Reinforcement Learning with Adaptive Intrinsic Rewards for Goal-directed Molecular Generation" index=118>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-118 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, q-bio-BM<br>Keyword Score: 23<br>Keywords: Benchmarking, Knowledge Distillation, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20109v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20109v1.pdf filename=2403.20109v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Optimizing techniques for discovering molecular structures with desired properties is crucial in artificial intelligence(AI)-based drug discovery. Combining deep generative models with <b>reinforcement</b> <b>learning</b> has emerged as an effective strategy for generating molecules with specific properties. Despite its potential, this approach is ineffective in exploring the vast chemical space and optimizing particular chemical properties. To overcome these limitations, we present Mol-AIR, a <b>reinforcement</b> <b>learning-based</b> framework using adaptive intrinsic rewards for effective goal-directed molecular generation. Mol-AIR leverages the strengths of both history-based and learning-based intrinsic rewards by exploiting random <b>distillation</b> network and counting-based strategies. In <b>benchmark</b> tests, Mol-AIR demonstrates superior performance over existing approaches in generating molecules with desired properties without any prior knowledge, including penalized LogP, QED, and celecoxib similarity. We believe that Mol-AIR represents a significant advancement in drug discovery, offering a more efficient path to discovering novel therapeutics.</p></p class="citation"></blockquote><h3 id=918--119204-caesar-enhancing-federated-rl-in-heterogeneous-mdps-through-convergence-aware-sampling-with-screening-hei-yi-mak-et-al-2024>(9/18 | 119/204) CAESAR: Enhancing Federated RL in Heterogeneous MDPs through Convergence-Aware Sampling with Screening (Hei Yi Mak et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hei Yi Mak, Flint Xiaofeng Fan, Luca A. Lanzendörfer, Cheston Tan, Wei Tsang Ooi, Roger Wattenhofer. (2024)<br><strong>CAESAR: Enhancing Federated RL in Heterogeneous MDPs through Convergence-Aware Sampling with Screening</strong><br><button class=copy-to-clipboard title="CAESAR: Enhancing Federated RL in Heterogeneous MDPs through Convergence-Aware Sampling with Screening" index=119>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-119 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Markov Decision Process, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20156v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20156v1.pdf filename=2403.20156v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this study, we delve into Federated <b>Reinforcement</b> <b>Learning</b> (FedRL) in the context of value-based agents operating across diverse Markov Decision Processes <b>(MDPs).</b> Existing FedRL methods typically aggregate agents&rsquo; learning by averaging the value functions across them to improve their performance. However, this aggregation strategy is suboptimal in heterogeneous environments where agents converge to diverse optimal value functions. To address this problem, we introduce the Convergence-AwarE SAmpling with scReening (CAESAR) aggregation scheme designed to enhance the learning of individual agents across varied <b>MDPs.</b> CAESAR is an aggregation strategy used by the server that combines convergence-aware sampling with a screening mechanism. By exploiting the fact that agents learning in identical <b>MDPs</b> are converging to the same optimal value function, CAESAR enables the selective assimilation of knowledge from more proficient counterparts, thereby significantly enhancing the overall learning efficiency. We empirically validate our hypothesis and demonstrate the effectiveness of CAESAR in enhancing the learning efficiency of agents, using both a custom-built GridWorld environment and the classical FrozenLake-v1 task, each presenting varying levels of environmental heterogeneity.</p></p class="citation"></blockquote><h3 id=1018--120204-tdanet-a-novel-temporal-denoise-convolutional-neural-network-with-attention-for-fault-diagnosis-zhongzhi-li-et-al-2024>(10/18 | 120/204) TDANet: A Novel Temporal Denoise Convolutional Neural Network With Attention for Fault Diagnosis (Zhongzhi Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhongzhi Li, Rong Fan, Jingqi Tu, Jinyi Ma, Jianliang Ai, Yiqun Dong. (2024)<br><strong>TDANet: A Novel Temporal Denoise Convolutional Neural Network With Attention for Fault Diagnosis</strong><br><button class=copy-to-clipboard title="TDANet: A Novel Temporal Denoise Convolutional Neural Network With Attention for Fault Diagnosis" index=120>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-120 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, eess-SP<br>Keyword Score: 20<br>Keywords: Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19943v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19943v1.pdf filename=2403.19943v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Fault diagnosis plays a crucial role in maintaining the operational integrity of mechanical systems, preventing significant losses due to unexpected failures. As intelligent manufacturing and data-driven approaches evolve, Deep Learning (DL) has emerged as a pivotal technique in fault diagnosis research, recognized for its ability to autonomously extract complex features. However, the practical application of current fault diagnosis methods is challenged by the complexity of industrial environments. This paper proposed the Temporal Denoise <b>Convolutional</b> <b>Neural</b> <b>Network</b> With Attention (TDANet), designed to improve fault diagnosis performance in noise environments. This model transforms one-dimensional signals into two-dimensional tensors based on their periodic properties, employing multi-scale 2D <b>convolution</b> kernels to extract signal information both within and across periods. This method enables effective identification of signal characteristics that vary over multiple time scales. The TDANet incorporates a Temporal Variable Denoise (TVD) module with residual connections and a Multi-head Attention Fusion (MAF) module, enhancing the saliency of information within noisy data and maintaining effective fault diagnosis performance. Evaluation on two datasets, CWRU (single sensor) and Real aircraft sensor fault (multiple sensors), demonstrates that the TDANet model significantly outperforms existing deep learning approaches in terms of diagnostic accuracy under noisy environments.</p></p class="citation"></blockquote><h3 id=1118--121204-mambamixer-efficient-selective-state-space-models-with-dual-token-and-channel-selection-ali-behrouz-et-al-2024>(11/18 | 121/204) MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection (Ali Behrouz et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ali Behrouz, Michele Santacatterina, Ramin Zabih. (2024)<br><strong>MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection</strong><br><button class=copy-to-clipboard title="MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection" index=121>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-121 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Object Detection, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19888v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19888v1.pdf filename=2403.19888v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advances in deep learning have mainly relied on <b>Transformers</b> due to their data dependency and ability to learn at scale. The attention module in these architectures, however, exhibits quadratic time and space in input size, limiting their scalability for long-sequence modeling. Despite recent attempts to design efficient and effective architecture backbone for multi-dimensional data, such as images and multivariate time series, existing models are either data independent, or fail to allow inter- and intra-dimension communication. Recently, State Space Models (SSMs), and more specifically Selective State Space Models, with efficient hardware-aware implementation, have shown promising potential for long sequence modeling. Motivated by the success of SSMs, we present MambaMixer, a new architecture with data-dependent weights that uses a dual selection mechanism across tokens and channels, called Selective Token and Channel Mixer. MambaMixer connects selective mixers using a weighted averaging mechanism, allowing layers to have direct access to early features. As a proof of concept, we design Vision MambaMixer (ViM2) and Time Series MambaMixer (TSM2) architectures based on the MambaMixer block and explore their performance in various vision and time series forecasting tasks. Our results underline the importance of selective mixing across both tokens and channels. In ImageNet classification, <b>object</b> <b>detection,</b> and semantic segmentation tasks, ViM2 achieves competitive performance with well-established vision models and outperforms SSM-based vision models. In time series forecasting, TSM2 achieves outstanding performance compared to state-of-the-art methods while demonstrating significantly improved computational cost. These results show that while <b>Transformers,</b> cross-channel attention, and MLPs are sufficient for good performance in time series forecasting, neither is necessary.</p></p class="citation"></blockquote><h3 id=1218--122204-sparse-multimodal-fusion-with-modal-channel-attention-josiah-bjorgaard-2024>(12/18 | 122/204) Sparse multimodal fusion with modal channel attention (Josiah Bjorgaard, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Josiah Bjorgaard. (2024)<br><strong>Sparse multimodal fusion with modal channel attention</strong><br><button class=copy-to-clipboard title="Sparse multimodal fusion with modal channel attention" index=122>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-122 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 16<br>Keywords: Multi-modal, Multi-modal, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20280v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20280v1.pdf filename=2403.20280v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The ability of masked <b>multimodal</b> <b>transformer</b> architectures to learn a robust embedding space when modality samples are sparsely aligned is studied by measuring the quality of generated embedding spaces as a function of modal sparsity. An extension to the masked <b>multimodal</b> <b>transformer</b> model is proposed which incorporates modal-incomplete channels in the multihead attention mechanism called modal channel attention (MCA). Two datasets with 4 modalities are used, CMU-MOSEI for <b>multimodal</b> sentiment recognition and TCGA for multiomics. Models are shown to learn uniform and aligned embedding spaces with only two out of four modalities in most samples. It was found that, even with no modal sparsity, the proposed MCA mechanism improves the quality of generated embedding spaces, recall metrics, and subsequent performance on downstream tasks.</p></p class="citation"></blockquote><h3 id=1318--123204-localising-the-seizure-onset-zone-from-single-pulse-electrical-stimulation-responses-with-a-transformer-jamie-norris-et-al-2024>(13/18 | 123/204) Localising the Seizure Onset Zone from Single-Pulse Electrical Stimulation Responses with a Transformer (Jamie Norris et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jamie Norris, Aswin Chari, Gerald Cooray, Martin Tisdall, Karl Friston, Richard Rosch. (2024)<br><strong>Localising the Seizure Onset Zone from Single-Pulse Electrical Stimulation Responses with a Transformer</strong><br><button class=copy-to-clipboard title="Localising the Seizure Onset Zone from Single-Pulse Electrical Stimulation Responses with a Transformer" index=123>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-123 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, eess-SP, q-bio-NC<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20324v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20324v1.pdf filename=2403.20324v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Epilepsy is one of the most common neurological disorders, and many patients require surgical intervention when medication fails to control seizures. For effective surgical outcomes, precise localisation of the epileptogenic focus - often approximated through the Seizure Onset Zone (SOZ) - is critical yet remains a challenge. Active probing through electrical stimulation is already standard clinical practice for identifying epileptogenic areas. This paper advances the application of deep learning for SOZ localisation using Single Pulse Electrical Stimulation (SPES) responses. We achieve this by introducing <b>Transformer</b> models that incorporate cross-channel attention. We evaluate these models on held-out patient test sets to assess their generalisability to unseen patients and electrode placements. Our study makes three key contributions: Firstly, we implement an existing deep learning model to compare two SPES analysis paradigms - namely, divergent and convergent. These paradigms evaluate outward and inward effective connections, respectively. Our findings reveal a notable improvement in moving from a divergent (AUROC: 0.574) to a convergent approach (AUROC: 0.666), marking the first application of the latter in this context. Secondly, we demonstrate the efficacy of the <b>Transformer</b> models in handling heterogeneous electrode placements, increasing the AUROC to 0.730. Lastly, by incorporating inter-trial variability, we further refine the <b>Transformer</b> models, with an AUROC of 0.745, yielding more consistent predictions across patients. These advancements provide a deeper insight into SOZ localisation and represent a significant step in modelling patient-specific intracranial EEG electrode placements in SPES. Future work will explore integrating these models into clinical decision-making processes to bridge the gap between deep learning research and practical healthcare applications.</p></p class="citation"></blockquote><h3 id=1418--124204-embracing-unknown-step-by-step-towards-reliable-sparse-training-in-real-world-bowen-lei-et-al-2024>(14/18 | 124/204) Embracing Unknown Step by Step: Towards Reliable Sparse Training in Real World (Bowen Lei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bowen Lei, Dongkuan Xu, Ruqi Zhang, Bani Mallick. (2024)<br><strong>Embracing Unknown Step by Step: Towards Reliable Sparse Training in Real World</strong><br><button class=copy-to-clipboard title="Embracing Unknown Step by Step: Towards Reliable Sparse Training in Real World" index=124>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-124 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20047v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20047v1.pdf filename=2403.20047v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Sparse training has emerged as a promising method for resource-efficient deep neural networks (DNNs) in real-world applications. However, the reliability of sparse models remains a crucial concern, particularly in detecting unknown <b>out-of-distribution</b> (OOD) data. This study addresses the knowledge gap by investigating the reliability of sparse training from an OOD perspective and reveals that sparse training exacerbates OOD unreliability. The lack of unknown information and the sparse constraints hinder the effective exploration of weight space and accurate differentiation between known and unknown knowledge. To tackle these challenges, we propose a new unknown-aware sparse training method, which incorporates a loss modification, auto-tuning strategy, and a voting scheme to guide weight space exploration and mitigate confusion between known and unknown information without incurring significant additional costs or requiring access to additional OOD data. Theoretical insights demonstrate how our method reduces model confidence when faced with OOD samples. Empirical experiments across multiple datasets, model architectures, and sparsity levels validate the effectiveness of our method, with improvements of up to \textbf{8.4%} in AUROC while maintaining comparable or higher accuracy and calibration. This research enhances the understanding and readiness of sparse DNNs for deployment in resource-limited applications. Our code is available on: \url{https://github.com/StevenBoys/MOON}.</p></p class="citation"></blockquote><h3 id=1518--125204-coverage-guaranteed-prediction-sets-for-out-of-distribution-data-xin-zou-et-al-2024>(15/18 | 125/204) Coverage-Guaranteed Prediction Sets for Out-of-Distribution Data (Xin Zou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xin Zou, Weiwei Liu. (2024)<br><strong>Coverage-Guaranteed Prediction Sets for Out-of-Distribution Data</strong><br><button class=copy-to-clipboard title="Coverage-Guaranteed Prediction Sets for Out-of-Distribution Data" index=125>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-125 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19950v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19950v1.pdf filename=2403.19950v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Out-of-distribution</b> (OOD) generalization has attracted increasing research attention in recent years, due to its promising experimental results in real-world applications. In this paper,we study the confidence set prediction problem in the OOD generalization setting. Split conformal prediction (SCP) is an efficient framework for handling the confidence set prediction problem. However, the validity of SCP requires the examples to be exchangeable, which is violated in the OOD setting. Empirically, we show that trivially applying SCP results in a failure to maintain the marginal coverage when the unseen target domain is different from the source domain. To address this issue, we develop a method for forming confident prediction sets in the OOD setting and theoretically prove the validity of our method. Finally, we conduct experiments on simulated data to empirically verify the correctness of our theory and the validity of our proposed method.</p></p class="citation"></blockquote><h3 id=1618--126204-nonlinearity-enhanced-adaptive-activation-function-david-yevick-2024>(16/18 | 126/204) Nonlinearity Enhanced Adaptive Activation Function (David Yevick, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>David Yevick. (2024)<br><strong>Nonlinearity Enhanced Adaptive Activation Function</strong><br><button class=copy-to-clipboard title="Nonlinearity Enhanced Adaptive Activation Function" index=126>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-126 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs-NE, cs.LG<br>Keyword Score: 10<br>Keywords: MNIST<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19896v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19896v1.pdf filename=2403.19896v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A simply implemented activation function with even cubic nonlinearity is introduced that increases the accuracy of neural networks without substantial additional computational resources. This is partially enabled through an apparent tradeoff between convergence and accuracy. The activation function generalizes the standard RELU function by introducing additional degrees of freedom through optimizable parameters that enable the degree of nonlinearity to be adjusted. The associated accuracy enhancement is quantified in the context of the <b>MNIST</b> digit data set through a comparison with standard techniques.</p></p class="citation"></blockquote><h3 id=1718--127204-comparing-hyper-optimized-machine-learning-models-for-predicting-efficiency-degradation-in-organic-solar-cells-david-valientea-et-al-2024>(17/18 | 127/204) Comparing Hyper-optimized Machine Learning Models for Predicting Efficiency Degradation in Organic Solar Cells (David Valientea et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>David Valientea, Fernando Rodríguez-Mas, Juan V. Alegre-Requena, David Dalmau, Juan C. Ferrer. (2024)<br><strong>Comparing Hyper-optimized Machine Learning Models for Predicting Efficiency Degradation in Organic Solar Cells</strong><br><button class=copy-to-clipboard title="Comparing Hyper-optimized Machine Learning Models for Predicting Efficiency Degradation in Organic Solar Cells" index=127>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-127 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 6<br>Keywords: Benchmarking, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00173v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00173v1.pdf filename=2404.00173v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work presents a set of optimal machine learning (ML) models to represent the temporal degradation suffered by the power conversion efficiency (PCE) of polymeric organic solar cells (OSCs) with a multilayer structure ITO/PEDOT:PSS/P3HT:PCBM/Al. To that aim, we generated a database with 996 entries, which includes up to 7 variables regarding both the manufacturing process and environmental conditions for more than 180 days. Then, we relied on a software framework that brings together a conglomeration of automated ML protocols that execute sequentially against our database by simply command-line interface. This easily permits hyper-optimizing and randomizing seeds of the ML models through exhaustive <b>benchmarking</b> so that optimal models are obtained. The accuracy achieved reaches values of the coefficient determination (R2) widely exceeding 0.90, whereas the root mean squared error (RMSE), sum of squared error (SSE), and mean absolute error (MAE)>1% of the target value, the PCE. Additionally, we contribute with validated models able to screen the behavior of OSCs never seen in the database. In that case, R2~0.96-0.97 and RMSE~1%, thus confirming the reliability of the proposal to predict. For comparative purposes, classical Bayesian regression fitting based on non-linear mean squares (LMS) are also presented, which only perform sufficiently for univariate cases of single OSCs. Hence they fail to outperform the breadth of the capabilities shown by the ML models. Finally, thanks to the standardized results offered by the ML framework, we study the dependencies between the variables of the dataset and their implications for the optimal performance and stability of the OSCs. Reproducibility is ensured by a standardized report altogether with the dataset, which are publicly available at Github.</p></p class="citation"></blockquote><h3 id=1818--128204-tfb-towards-comprehensive-and-fair-benchmarking-of-time-series-forecasting-methods-xiangfei-qiu-et-al-2024>(18/18 | 128/204) TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods (Xiangfei Qiu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiangfei Qiu, Jilin Hu, Lekui Zhou, Xingjian Wu, Junyang Du, Buang Zhang, Chenjuan Guo, Aoying Zhou, Christian S. Jensen, Zhenli Sheng, Bin Yang. (2024)<br><strong>TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods</strong><br><button class=copy-to-clipboard title="TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods" index=128>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-128 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CY, cs-LG, cs.LG<br>Keyword Score: 6<br>Keywords: Benchmarking, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20150v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20150v1.pdf filename=2403.20150v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Time series are generated in diverse domains such as economic, traffic, health, and energy, where forecasting of future values has numerous important applications. Not surprisingly, many forecasting methods are being proposed. To ensure progress, it is essential to be able to study and compare such methods empirically in a comprehensive and reliable manner. To achieve this, we propose TFB, an automated <b>benchmark</b> for Time Series Forecasting (TSF) methods. TFB advances the state-of-the-art by addressing shortcomings related to datasets, comparison methods, and evaluation pipelines: 1) insufficient coverage of data domains, 2) stereotype bias against traditional methods, and 3) inconsistent and inflexible pipelines. To achieve better domain coverage, we include datasets from 10 different domains: traffic, electricity, energy, the environment, nature, economic, stock markets, banking, health, and the web. We also provide a time series characterization to ensure that the selected datasets are comprehensive. To remove biases against some methods, we include a diverse range of methods, including statistical learning, machine learning, and deep learning methods, and we also support a variety of evaluation strategies and metrics to ensure a more comprehensive evaluations of different methods. To support the integration of different methods into the <b>benchmark</b> and enable fair comparisons, TFB features a flexible and scalable pipeline that eliminates biases. Next, we employ TFB to perform a thorough evaluation of 21 Univariate Time Series Forecasting (UTSF) methods on 8,068 univariate time series and 14 Multivariate Time Series Forecasting (MTSF) methods on 25 datasets. The <b>benchmark</b> code and data are available at <a href=https://github.com/decisionintelligence/TFB>https://github.com/decisionintelligence/TFB</a>.</p></p class="citation"></blockquote><h2 id=csai-12>cs.AI (12)</h2><h3 id=112--129204-the-impact-of-prompts-on-zero-shot-detection-of-ai-generated-text-kaito-taguchi-et-al-2024>(1/12 | 129/204) The Impact of Prompts on Zero-Shot Detection of AI-Generated Text (Kaito Taguchi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kaito Taguchi, Yujie Gu, Kouichi Sakurai. (2024)<br><strong>The Impact of Prompts on Zero-Shot Detection of AI-Generated Text</strong><br><button class=copy-to-clipboard title="The Impact of Prompts on Zero-Shot Detection of AI-Generated Text" index=129>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-129 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 65<br>Keywords: Black Box, Zero-shot, Text Generation, Fake News Detection, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20127v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20127v1.pdf filename=2403.20127v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, there have been significant advancements in the development of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> While their practical applications are now widespread, their potential for misuse, such as generating <b>fake</b> <b>news</b> and committing plagiarism, has posed significant concerns. To address this issue, detectors have been developed to evaluate whether a given <b>text</b> <b>is</b> human-generated or AI-generated. Among others, <b>zero-shot</b> detectors stand out as effective approaches that do not require additional training data and are often likelihood-based. In chat-based applications, users commonly input <b>prompts</b> and utilize the AI-generated <b>texts.</b> <b>However,</b> <b>zero-shot</b> detectors typically analyze these <b>texts</b> <b>in</b> isolation, neglecting the impact of the original <b>prompts.</b> It is conceivable that this approach may lead to a discrepancy in likelihood assessments between the <b>text</b> <b>generation</b> phase and the detection phase. So far, there remains an unverified gap concerning how the presence or absence of <b>prompts</b> impacts detection accuracy for <b>zero-shot</b> detectors. In this paper, we introduce an evaluative framework to empirically analyze the impact of <b>prompts</b> on the detection accuracy of AI-generated <b>text.</b> <b>We</b> assess various <b>zero-shot</b> detectors using both white-box detection, which leverages the <b>prompt,</b> and <b>black-box</b> <b>detection,</b> which operates without <b>prompt</b> information. Our experiments reveal the significant influence of <b>prompts</b> on detection accuracy. Remarkably, compared with <b>black-box</b> <b>detection</b> without <b>prompts,</b> the white-box methods using <b>prompts</b> demonstrate an increase in AUC of at least $0.1$ across all <b>zero-shot</b> detectors tested. Code is available: \url{https://github.com/kaito25atugich/Detector}.</p></p class="citation"></blockquote><h3 id=212--130204-on-size-and-hardness-generalization-in-unsupervised-learning-for-the-travelling-salesman-problem-yimeng-min-et-al-2024>(2/12 | 130/204) On Size and Hardness Generalization in Unsupervised Learning for the Travelling Salesman Problem (Yimeng Min et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yimeng Min, Carla P. Gomes. (2024)<br><strong>On Size and Hardness Generalization in Unsupervised Learning for the Travelling Salesman Problem</strong><br><button class=copy-to-clipboard title="On Size and Hardness Generalization in Unsupervised Learning for the Travelling Salesman Problem" index=130>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-130 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LG, cs.AI<br>Keyword Score: 43<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Unsupervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20212v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20212v1.pdf filename=2403.20212v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the generalization capability of <b>Unsupervised</b> <b>Learning</b> in solving the Travelling Salesman Problem (TSP). We use a <b>Graph</b> <b>Neural</b> <b>Network</b> <b>(GNN)</b> trained with a surrogate loss function to generate an embedding for each node. We use these embeddings to construct a heat map that indicates the likelihood of each edge being part of the optimal route. We then apply local search to generate our final predictions. Our investigation explores how different training instance sizes, embedding dimensions, and distributions influence the outcomes of <b>Unsupervised</b> <b>Learning</b> methods. Our results show that training with larger instance sizes and increasing embedding dimensions can build a more effective representation, enhancing the model&rsquo;s ability to solve TSP. Furthermore, in evaluating generalization across different distributions, we first determine the hardness of various distributions and explore how different hardnesses affect the final results. Our findings suggest that models trained on harder instances exhibit better generalization capabilities, highlighting the importance of selecting appropriate training instances in solving TSP using <b>Unsupervised</b> <b>Learning.</b></p></p class="citation"></blockquote><h3 id=312--131204-the-future-of-combating-rumors-retrieval-discrimination-and-generation-junhao-xu-et-al-2024>(3/12 | 131/204) The Future of Combating Rumors? Retrieval, Discrimination, and Generation (Junhao Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junhao Xu, Longdi Xian, Zening Liu, Mingliang Chen, Qiuyang Yin, Fenghua Song. (2024)<br><strong>The Future of Combating Rumors? Retrieval, Discrimination, and Generation</strong><br><button class=copy-to-clipboard title="The Future of Combating Rumors? Retrieval, Discrimination, and Generation" index=131>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-131 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: 68T99, cs-AI, cs.AI<br>Keyword Score: 40<br>Keywords: Fine-tuning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20204v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20204v1.pdf filename=2403.20204v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Artificial Intelligence Generated Content (AIGC) technology development has facilitated the creation of rumors with misinformation, impacting societal, economic, and political ecosystems, challenging democracy. Current rumor detection efforts fall short by merely labeling potentially misinformation (classification task), inadequately addressing the issue, and it is unrealistic to have authoritative institutions debunk every piece of information on social media. Our proposed comprehensive debunking process not only detects rumors but also provides explanatory generated content to refute the authenticity of the information. The Expert-Citizen Collective Wisdom (ECCW) module we designed aensures high-precision assessment of the credibility of information and the retrieval module is responsible for retrieving relevant knowledge from a Real-time updated debunking database based on information keywords. By using <b>prompt</b> engineering techniques, we feed results and knowledge into a <b>LLM</b> <b>(Large</b> <b>Language</b> <b>Model),</b> achieving satisfactory discrimination and explanatory effects while eliminating the need for <b>fine-tuning,</b> saving computational costs, and contributing to debunking efforts.</p></p class="citation"></blockquote><h3 id=412--132204-does-faithfulness-conflict-with-plausibility-an-empirical-study-in-explainable-ai-across-nlp-tasks-xiaolei-lu-et-al-2024>(4/12 | 132/204) Does Faithfulness Conflict with Plausibility? An Empirical Study in Explainable AI across NLP Tasks (Xiaolei Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaolei Lu, Jianghong Ma. (2024)<br><strong>Does Faithfulness Conflict with Plausibility? An Empirical Study in Explainable AI across NLP Tasks</strong><br><button class=copy-to-clipboard title="Does Faithfulness Conflict with Plausibility? An Empirical Study in Explainable AI across NLP Tasks" index=132>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-132 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LG, cs.AI<br>Keyword Score: 30<br>Keywords: Explainable AI, Intent Detection, Sentiment Analysis<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00140v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00140v1.pdf filename=2404.00140v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Explainability algorithms aimed at interpreting decision-making AI systems usually consider balancing two critical dimensions: 1) \textit{faithfulness}, where explanations accurately reflect the model&rsquo;s inference process. 2) \textit{plausibility}, where explanations are consistent with domain experts. However, the question arises: do faithfulness and plausibility inherently conflict? In this study, through a comprehensive quantitative comparison between the explanations from the selected explainability methods and expert-level interpretations across three NLP tasks: <b>sentiment</b> <b>analysis,</b> <b>intent</b> <b>detection,</b> and topic labeling, we demonstrate that traditional perturbation-based methods Shapley value and LIME could attain greater faithfulness and plausibility. Our findings suggest that rather than optimizing for one dimension at the expense of the other, we could seek to optimize explainability algorithms with dual objectives to achieve high levels of accuracy and user accessibility in their explanations.</p></p class="citation"></blockquote><h3 id=512--133204-efficient-and-sharp-off-policy-evaluation-in-robust-markov-decision-processes-andrew-bennett-et-al-2024>(5/12 | 133/204) Efficient and Sharp Off-Policy Evaluation in Robust Markov Decision Processes (Andrew Bennett et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andrew Bennett, Nathan Kallus, Miruna Oprescu, Wen Sun, Kaiwen Wang. (2024)<br><strong>Efficient and Sharp Off-Policy Evaluation in Robust Markov Decision Processes</strong><br><button class=copy-to-clipboard title="Efficient and Sharp Off-Policy Evaluation in Robust Markov Decision Processes" index=133>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-133 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI, stat-ML<br>Keyword Score: 30<br>Keywords: Markov Decision Process, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00099v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00099v1.pdf filename=2404.00099v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study evaluating a policy under best- and worst-case perturbations to a <b>Markov</b> <b>decision</b> <b>process</b> (MDP), given transition observations from the original MDP, whether under the same or different policy. This is an important problem when there is the possibility of a shift between historical and future environments, due to e.g. unmeasured confounding, distributional shift, or an adversarial environment. We propose a perturbation model that can modify transition kernel densities up to a given multiplicative factor or its reciprocal, which extends the classic marginal sensitivity model (MSM) for single time step decision making to infinite-horizon RL. We characterize the sharp bounds on policy value under this model, that is, the tightest possible bounds given by the transition observations from the original MDP, and we study the estimation of these bounds from such transition observations. We develop an estimator with several appealing guarantees: it is semiparametrically efficient, and remains so even when certain necessary nuisance functions such as worst-case Q-functions are estimated at slow nonparametric rates. It is also asymptotically normal, enabling easy statistical inference using Wald confidence intervals. In addition, when certain nuisances are estimated inconsistently we still estimate a valid, albeit possibly not sharp bounds on the policy value. We validate these properties in numeric <b>simulations.</b> The combination of accounting for environment shifts from train to test (robustness), being insensitive to nuisance-function estimation (orthogonality), and accounting for having only finite samples to learn from (inference) together leads to credible and reliable policy evaluation.</p></p class="citation"></blockquote><h3 id=612--134204-itcma-a-generative-agent-based-on-a-computational-consciousness-structure-hanzhong-zhang-et-al-2024>(6/12 | 134/204) ITCMA: A Generative Agent Based on a Computational Consciousness Structure (Hanzhong Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hanzhong Zhang, Jibin Yin, Haoyang Wang, Ziwei Xiang. (2024)<br><strong>ITCMA: A Generative Agent Based on a Computational Consciousness Structure</strong><br><button class=copy-to-clipboard title="ITCMA: A Generative Agent Based on a Computational Consciousness Structure" index=134>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-134 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: I-2; J-4, cs-AI, cs-HC, cs.AI, q-bio-NC<br>Keyword Score: 30<br>Keywords: Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20097v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20097v1.pdf filename=2403.20097v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> still face challenges in tasks requiring understanding implicit instructions and applying common-sense knowledge. In such scenarios, <b>LLMs</b> may require multiple attempts to achieve human-level performance, potentially leading to inaccurate responses or inferences in practical environments, affecting their long-term consistency and behavior. This paper introduces the Internal Time-Consciousness Machine (ITCM), a computational consciousness structure. We further propose the ITCM-based Agent (ITCMA), which supports behavior generation and <b>reasoning</b> in open-world settings. ITCMA enhances <b>LLMs&rsquo;</b> ability to understand implicit instructions and apply common-sense knowledge by considering agents&rsquo; interaction and <b>reasoning</b> with the environment. Evaluations in the Alfworld environment show that trained ITCMA outperforms the state-of-the-art (SOTA) by 9% on the seen set. Even untrained ITCMA achieves a 96% task completion rate on the seen set, 5% higher than SOTA, indicating its superiority over traditional intelligent agents in utility and generalization. In real-world tasks with quadruped robots, the untrained ITCMA achieves an 85% task completion rate, which is close to its performance in the unseen set, demonstrating its comparable utility in real-world settings.</p></p class="citation"></blockquote><h3 id=712--135204-towards-greener-llms-bringing-energy-efficiency-to-the-forefront-of-llm-inference-jovan-stojkovic-et-al-2024>(7/12 | 135/204) Towards Greener LLMs: Bringing Energy-Efficiency to the Forefront of LLM Inference (Jovan Stojkovic et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jovan Stojkovic, Esha Choukse, Chaojie Zhang, Inigo Goiri, Josep Torrellas. (2024)<br><strong>Towards Greener LLMs: Bringing Energy-Efficiency to the Forefront of LLM Inference</strong><br><button class=copy-to-clipboard title="Towards Greener LLMs: Bringing Energy-Efficiency to the Forefront of LLM Inference" index=135>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-135 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: C-0; I-2, cs-AI, cs-AR, cs-DC, cs.AI<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20306v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20306v1.pdf filename=2403.20306v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the ubiquitous use of modern <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> across industries, the inference serving for these models is ever expanding. Given the high compute and memory requirements of modern <b>LLMs,</b> more and more top-of-the-line GPUs are being deployed to serve these models. Energy availability has come to the forefront as the biggest challenge for data center expansion to serve these models. In this paper, we present the trade-offs brought up by making energy efficiency the primary goal of <b>LLM</b> serving under performance SLOs. We show that depending on the inputs, the model, and the service-level agreements, there are several knobs available to the <b>LLM</b> inference provider to use for being energy efficient. We characterize the impact of these knobs on the latency, throughput, as well as the energy. By exploring these trade-offs, we offer valuable insights into optimizing energy usage without compromising on performance, thereby paving the way for sustainable and cost-effective <b>LLM</b> deployment in data center environments.</p></p class="citation"></blockquote><h3 id=812--136204-accurate-block-quantization-in-llms-with-outliers-nikita-trukhanov-et-al-2024>(8/12 | 136/204) Accurate Block Quantization in LLMs with Outliers (Nikita Trukhanov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nikita Trukhanov, Ilya Soloveychik. (2024)<br><strong>Accurate Block Quantization in LLMs with Outliers</strong><br><button class=copy-to-clipboard title="Accurate Block Quantization in LLMs with Outliers" index=136>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-136 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-AR, cs-NA, cs.AI, math-NA<br>Keyword Score: 20<br>Keywords: Quantization, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20137v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20137v1.pdf filename=2403.20137v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The demand for inference on extremely large scale <b>LLMs</b> has seen enormous growth in the recent months. It made evident the colossal shortage of dedicated hardware capable of efficient and fast processing of the involved compute and memory movement. The problem is aggravated by the exploding raise in the lengths of the sequences being processed, since those require efficient on-chip storage of the KV-cache of size proportional to the sequence length. To make the required compute feasible and fit the involved data into available memory, numerous <b>quantization</b> techniques have been proposed that allow accurate <b>quantization</b> for both weights and activations. One of the main recent breakthroughs in this direction was introduction of the family of Block Floating Point (BFP) formats characterized by a block of mantissas with a shared scale factor. These enable memory- power-, and compute- efficient hardware support of the tensor operations and provide extremely good <b>quantization</b> accuracy. The main issues preventing widespread application of block formats is caused by the presence of outliers in weights and activations since those affect the accuracy of the other values in the same block. In this paper, we focus on the most critical problem of limited KV-cache storage. We propose a novel approach enabling usage of low precision BFP formats without compromising the resulting model accuracy. We exploit the common channel-wise patterns exhibited by the outliers to rearrange them in such a way, that their <b>quantization</b> quality is significantly improved. The methodology yields 2x savings in the memory footprint without significant degradation of the model&rsquo;s accuracy. Importantly, the rearrangement of channels happens at the compile time and thus has no impact on the inference latency.</p></p class="citation"></blockquote><h3 id=912--137204-development-of-compositionality-and-generalization-through-interactive-learning-of-language-and-action-of-robots-prasanna-vijayaraghavan-et-al-2024>(9/12 | 137/204) Development of Compositionality and Generalization through Interactive Learning of Language and Action of Robots (Prasanna Vijayaraghavan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Prasanna Vijayaraghavan, Jeffrey Frederic Queisser, Sergio Verduzco Flores, Jun Tani. (2024)<br><strong>Development of Compositionality and Generalization through Interactive Learning of Language and Action of Robots</strong><br><button class=copy-to-clipboard title="Development of Compositionality and Generalization through Interactive Learning of Language and Action of Robots" index=137>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-137 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: 68T35, 68T40, I-2-9, cs-AI, cs-CL, cs-RO, cs.AI<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19995v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19995v1.pdf filename=2403.19995v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Humans excel at applying learned behavior to unlearned situations. A crucial component of this generalization behavior is our ability to compose/decompose a whole into reusable parts, an attribute known as compositionality. One of the fundamental questions in robotics concerns this characteristic. &ldquo;How can linguistic compositionality be developed concomitantly with sensorimotor skills through associative learning, particularly when individuals only learn partial linguistic compositions and their corresponding sensorimotor patterns?&rdquo; To address this question, we propose a brain-inspired neural network model that integrates vision, proprioception, and language into a framework of predictive coding and active inference, based on the free-energy principle. The effectiveness and capabilities of this model were assessed through various <b>simulation</b> experiments conducted with a robot arm. Our results show that generalization in learning to unlearned verb-noun compositions, is significantly enhanced when training variations of task composition are increased. We attribute this to self-organized compositional structures in linguistic latent state space being influenced significantly by sensorimotor learning. Ablation studies show that visual attention and working memory are essential to accurately generate visuo-motor sequences to achieve linguistically represented goals. These insights advance our understanding of mechanisms underlying development of compositionality through interactions of linguistic and sensorimotor experience.</p></p class="citation"></blockquote><h3 id=1012--138204-a-learning-based-incentive-mechanism-for-mobile-aigc-service-in-decentralized-internet-of-vehicles-jiani-fan-et-al-2024>(10/12 | 138/204) A Learning-based Incentive Mechanism for Mobile AIGC Service in Decentralized Internet of Vehicles (Jiani Fan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiani Fan, Minrui Xu, Ziyao Liu, Huanyi Ye, Chaojie Gu, Dusit Niyato, Kwok-Yan Lam. (2024)<br><strong>A Learning-based Incentive Mechanism for Mobile AIGC Service in Decentralized Internet of Vehicles</strong><br><button class=copy-to-clipboard title="A Learning-based Incentive Mechanism for Mobile AIGC Service in Decentralized Internet of Vehicles" index=138>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-138 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20151v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20151v1.pdf filename=2403.20151v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Artificial Intelligence-Generated Content (AIGC) refers to the paradigm of automated content generation utilizing AI models. Mobile AIGC services in the Internet of Vehicles (IoV) network have numerous advantages over traditional cloud-based AIGC services, including enhanced network efficiency, better reconfigurability, and stronger data security and privacy. Nonetheless, AIGC service provisioning frequently demands significant resources. Consequently, resource-constrained roadside units (RSUs) face challenges in maintaining a heterogeneous pool of AIGC services and addressing all user service requests without degrading overall performance. Therefore, in this paper, we propose a decentralized incentive mechanism for mobile AIGC service allocation, employing multi-agent deep <b>reinforcement</b> <b>learning</b> to find the balance between the supply of AIGC services on RSUs and user demand for services within the IoV context, optimizing user experience and minimizing transmission latency. Experimental results demonstrate that our approach achieves superior performance compared to other baseline models.</p></p class="citation"></blockquote><h3 id=1112--139204-implications-of-the-ai-act-for-non-discrimination-law-and-algorithmic-fairness-luca-deck-et-al-2024>(11/12 | 139/204) Implications of the AI Act for Non-Discrimination Law and Algorithmic Fairness (Luca Deck et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Luca Deck, Jan-Laurin Müller, Conradin Braun, Domenique Zipperling, Niklas Kühl. (2024)<br><strong>Implications of the AI Act for Non-Discrimination Law and Algorithmic Fairness</strong><br><button class=copy-to-clipboard title="Implications of the AI Act for Non-Discrimination Law and Algorithmic Fairness" index=139>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-139 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20089v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20089v1.pdf filename=2403.20089v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The topic of <b>fairness</b> in AI, as debated in the FATE <b>(Fairness,</b> Accountability, Transparency, and Ethics in AI) communities, has sparked meaningful discussions in the past years. However, from a legal perspective, particularly from European Union law, many open questions remain. Whereas algorithmic <b>fairness</b> aims to mitigate structural inequalities at the design level, European non-discrimination law is tailored to individual cases of discrimination after an AI model has been deployed. The AI Act might present a tremendous step towards bridging these two concepts by shifting non-discrimination responsibilities into the design stage of AI models. Based on an integrative reading of the AI Act, we comment on legal as well as technical enforcement problems and propose practical implications on bias detection and bias correction in order to specify and comply with specific technical requirements.</p></p class="citation"></blockquote><h3 id=1212--140204-diverse-feature-learning-by-self-distillation-and-reset-sejik-park-2024>(12/12 | 140/204) Diverse Feature Learning by Self-distillation and Reset (Sejik Park, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sejik Park. (2024)<br><strong>Diverse Feature Learning by Self-distillation and Reset</strong><br><button class=copy-to-clipboard title="Diverse Feature Learning by Self-distillation and Reset" index=140>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-140 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 10<br>Keywords: Self-Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19941v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19941v1.pdf filename=2403.19941v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Our paper addresses the problem of models struggling to learn diverse features, due to either forgetting previously learned features or failing to learn new ones. To overcome this problem, we introduce Diverse Feature Learning (DFL), a method that combines an important feature preservation algorithm with a new feature learning algorithm. Specifically, for preserving important features, we utilize <b>self-distillation</b> in ensemble models by selecting the meaningful model weights observed during training. For learning new features, we employ reset that involves periodically re-initializing part of the model. As a result, through experiments with various models on the image classification, we have identified the potential for synergistic effects between <b>self-distillation</b> and reset.</p></p class="citation"></blockquote><h2 id=csro-10>cs.RO (10)</h2><h3 id=110--141204-ctrl-sim-reactive-and-controllable-driving-agents-with-offline-reinforcement-learning-luke-rowe-et-al-2024>(1/10 | 141/204) CtRL-Sim: Reactive and Controllable Driving Agents with Offline Reinforcement Learning (Luke Rowe et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Luke Rowe, Roger Girgis, Anthony Gosselin, Bruno Carrez, Florian Golemo, Felix Heide, Liam Paull, Christopher Pal. (2024)<br><strong>CtRL-Sim: Reactive and Controllable Driving Agents with Offline Reinforcement Learning</strong><br><button class=copy-to-clipboard title="CtRL-Sim: Reactive and Controllable Driving Agents with Offline Reinforcement Learning" index=141>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-141 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-LG, cs-RO, cs.RO<br>Keyword Score: 60<br>Keywords: Counter-factual, Fine-tuning, Offline Reinforcement Learning, Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19918v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19918v1.pdf filename=2403.19918v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Evaluating autonomous vehicle stacks (AVs) in <b>simulation</b> typically involves replaying driving logs from real-world recorded traffic. However, agents replayed from <b>offline</b> <b>data</b> <b>do</b> not react to the actions of the AV, and their behaviour cannot be easily controlled to simulate <b>counterfactual</b> scenarios. Existing approaches have attempted to address these shortcomings by proposing methods that rely on heuristics or learned generative models of real-world data but these approaches either lack realism or necessitate costly iterative sampling procedures to control the generated behaviours. In this work, we take an alternative approach and propose CtRL-Sim, a method that leverages return-conditioned <b>offline</b> <b>reinforcement</b> <b>learning</b> within a physics-enhanced Nocturne simulator to efficiently generate reactive and controllable traffic agents. Specifically, we process real-world driving data through the Nocturne simulator to generate a diverse <b>offline</b> <b>reinforcement</b> <b>learning</b> dataset, annotated with various reward terms. With this dataset, we train a return-conditioned multi-agent behaviour model that allows for fine-grained manipulation of agent behaviours by modifying the desired returns for the various reward components. This capability enables the generation of a wide range of driving behaviours beyond the scope of the initial dataset, including those representing adversarial behaviours. We demonstrate that CtRL-Sim can efficiently generate diverse and realistic safety-critical scenarios while providing fine-grained control over agent behaviours. Further, we show that <b>fine-tuning</b> our model on simulated safety-critical scenarios generated by our model enhances this controllability.</p></p class="citation"></blockquote><h3 id=210--142204-an-optimization-based-planner-with-b-spline-parameterized-continuous-time-reference-signals-chuyuan-tao-et-al-2024>(2/10 | 142/204) An Optimization-Based Planner with B-spline Parameterized Continuous-Time Reference Signals (Chuyuan Tao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chuyuan Tao, Sheng Cheng, Yang Zhao, Fanxin Wang, Naira Hovakimyan. (2024)<br><strong>An Optimization-Based Planner with B-spline Parameterized Continuous-Time Reference Signals</strong><br><button class=copy-to-clipboard title="An Optimization-Based Planner with B-spline Parameterized Continuous-Time Reference Signals" index=142>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-142 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 40<br>Keywords: Continuous Time, Continuous Time, Discrete Time, Discrete Time, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00133v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00133v1.pdf filename=2404.00133v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>For the cascaded planning and control modules implemented for robot navigation, the frequency gap between the planner and controller has received limited attention. In this study, we introduce a novel B-spline parameterized optimization-based planner (BSPOP) designed to address the frequency gap challenge with limited onboard computational power in robots. The proposed planner generates <b>continuous-time</b> <b>control</b> inputs for low-level controllers running at arbitrary frequencies to track. Furthermore, when considering the convex control action sets, BSPOP uses the convex hull property to automatically constrain the <b>continuous-time</b> <b>control</b> inputs within the convex set. Consequently, compared with the <b>discrete-time</b> <b>optimization-based</b> planners, BSPOP reduces the number of decision variables and inequality constraints, which improves computational efficiency as a byproduct. <b>Simulation</b> results demonstrate that our approach can achieve a comparable planning performance to the high-frequency baseline optimization-based planners while demanding less computational power. Both <b>simulation</b> and experiment results show that the proposed method performs better in planning compared with baseline planners in the same frequency.</p></p class="citation"></blockquote><h3 id=310--143204-learning-visual-quadrupedal-loco-manipulation-from-demonstrations-zhengmao-he-et-al-2024>(3/10 | 143/204) Learning Visual Quadrupedal Loco-Manipulation from Demonstrations (Zhengmao He et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhengmao He, Kun Lei, Yanjie Ze, Koushil Sreenath, Zhongyu Li, Huazhe Xu. (2024)<br><strong>Learning Visual Quadrupedal Loco-Manipulation from Demonstrations</strong><br><button class=copy-to-clipboard title="Learning Visual Quadrupedal Loco-Manipulation from Demonstrations" index=143>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-143 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-LG, cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20328v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20328v1.pdf filename=2403.20328v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Quadruped robots are progressively being integrated into human environments. Despite the growing locomotion capabilities of quadrupedal robots, their interaction with objects in realistic scenes is still limited. While additional robotic arms on quadrupedal robots enable manipulating objects, they are sometimes redundant given that a quadruped robot is essentially a mobile unit equipped with four limbs, each possessing 3 degrees of freedom (DoFs). Hence, we aim to empower a quadruped robot to execute real-world manipulation tasks using only its legs. We decompose the loco-manipulation process into a low-level <b>reinforcement</b> <b>learning</b> (RL)-based controller and a high-level Behavior Cloning (BC)-based planner. By parameterizing the manipulation trajectory, we synchronize the efforts of the upper and lower layers, thereby leveraging the advantages of both RL and BC. Our approach is validated through <b>simulations</b> and real-world experiments, demonstrating the robot&rsquo;s ability to perform tasks that demand mobility and high precision, such as lifting a basket from the ground while moving, closing a dishwasher, pressing a button, and pushing a door. Project website: <a href=https://zhengmaohe.github.io/leg-manip>https://zhengmaohe.github.io/leg-manip</a></p></p class="citation"></blockquote><h3 id=410--144204-adaptive-energy-regularization-for-autonomous-gait-transition-and-energy-efficient-quadruped-locomotion-boyuan-liang-et-al-2024>(4/10 | 144/204) Adaptive Energy Regularization for Autonomous Gait Transition and Energy-Efficient Quadruped Locomotion (Boyuan Liang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Boyuan Liang, Lingfeng Sun, Xinghao Zhu, Bike Zhang, Ziyin Xiong, Chenran Li, Koushil Sreenath, Masayoshi Tomizuka. (2024)<br><strong>Adaptive Energy Regularization for Autonomous Gait Transition and Energy-Efficient Quadruped Locomotion</strong><br><button class=copy-to-clipboard title="Adaptive Energy Regularization for Autonomous Gait Transition and Energy-Efficient Quadruped Locomotion" index=144>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-144 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20001v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20001v1.pdf filename=2403.20001v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In <b>reinforcement</b> <b>learning</b> for legged robot locomotion, crafting effective reward strategies is crucial. Pre-defined gait patterns and complex reward systems are widely used to stabilize policy training. Drawing from the natural locomotion behaviors of humans and animals, which adapt their gaits to minimize energy consumption, we propose a simplified, energy-centric reward strategy to foster the development of energy-efficient locomotion across various speeds in quadruped robots. By implementing an adaptive energy reward function and adjusting the weights based on velocity, we demonstrate that our approach enables ANYmal-C and Unitree Go1 robots to autonomously select appropriate gaits, such as four-beat walking at lower speeds and trotting at higher speeds, resulting in improved energy efficiency and stable velocity tracking compared to previous methods using complex reward designs and prior gait knowledge. The effectiveness of our policy is validated through <b>simulations</b> in the IsaacGym <b>simulation</b> environment and on real robots, demonstrating its potential to facilitate stable and adaptive locomotion.</p></p class="citation"></blockquote><h3 id=510--145204-encomp-enhanced-covert-maneuver-planning-using-offline-reinforcement-learning-jumman-hossain-et-al-2024>(5/10 | 145/204) EnCoMP: Enhanced Covert Maneuver Planning using Offline Reinforcement Learning (Jumman Hossain et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jumman Hossain, Abu-Zaher Faridee, Nirmalya Roy. (2024)<br><strong>EnCoMP: Enhanced Covert Maneuver Planning using Offline Reinforcement Learning</strong><br><button class=copy-to-clipboard title="EnCoMP: Enhanced Covert Maneuver Planning using Offline Reinforcement Learning" index=145>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-145 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-LG, cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Offline Reinforcement Learning, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20016v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20016v1.pdf filename=2403.20016v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Cover navigation in complex environments is a critical challenge for autonomous robots, requiring the identification and utilization of environmental cover while maintaining efficient navigation. We propose an enhanced navigation system that enables robots to identify and utilize natural and artificial environmental features as cover, thereby minimizing exposure to potential threats. Our perception pipeline leverages LiDAR data to generate high-fidelity cover maps and potential threat maps, providing a comprehensive understanding of the surrounding environment. We train an <b>offline</b> <b>reinforcement</b> <b>learning</b> model using a diverse dataset collected from real-world environments, learning a robust policy that evaluates the quality of candidate actions based on their ability to maximize cover utilization, minimize exposure to threats, and reach the goal efficiently. Extensive real-world experiments demonstrate the superiority of our approach in terms of success rate, cover utilization, exposure minimization, and navigation efficiency compared to state-of-the-art methods.</p></p class="citation"></blockquote><h3 id=610--146204-moma-pos-where-should-mobile-manipulators-stand-in-cluttered-environment-before-task-execution-beichen-shao-et-al-2024>(6/10 | 146/204) MoMa-Pos: Where Should Mobile Manipulators Stand in Cluttered Environment Before Task Execution? (Beichen Shao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Beichen Shao, Yan Ding, Xingchen Wang, Xuefeng Xie, Fuqiang Gu, Jun Luo, Chao Chen. (2024)<br><strong>MoMa-Pos: Where Should Mobile Manipulators Stand in Cluttered Environment Before Task Execution?</strong><br><button class=copy-to-clipboard title="MoMa-Pos: Where Should Mobile Manipulators Stand in Cluttered Environment Before Task Execution?" index=146>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-146 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 13<br>Keywords: Graph, Graph Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19940v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19940v1.pdf filename=2403.19940v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Mobile manipulators always need to determine feasible base positions prior to carrying out navigation-manipulation tasks. Real-world environments are often cluttered with various furniture, obstacles, and dozens of other objects. Efficiently computing base positions poses a challenge. In this work, we introduce a framework named MoMa-Pos to address this issue. MoMa-Pos first learns to predict a small set of objects that, taken together, would be sufficient for finding base positions using a <b>graph</b> <b>embedding</b> architecture. MoMa-Pos then calculates standing positions by considering furniture structures, robot models, and obstacles comprehensively. We have extensively evaluated the proposed MoMa-Pos across different settings (e.g., environment and algorithm parameters) and with various mobile manipulators. Our empirical results show that MoMa-Pos demonstrates remarkable effectiveness and efficiency in its performance, surpassing the methods in the literature. %, but also is adaptable to cluttered environments and different robot models. Supplementary material can be found at \url{https://yding25.com/MoMa-Pos}.</p></p class="citation"></blockquote><h3 id=710--147204-a-sequential-quadratic-programming-approach-to-the-solution-of-open-loop-generalized-nash-equilibria-for-autonomous-racing-edward-l-zhu-et-al-2024>(7/10 | 147/204) A Sequential Quadratic Programming Approach to the Solution of Open-Loop Generalized Nash Equilibria for Autonomous Racing (Edward L. Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Edward L. Zhu, Francesco Borrelli. (2024)<br><strong>A Sequential Quadratic Programming Approach to the Solution of Open-Loop Generalized Nash Equilibria for Autonomous Racing</strong><br><button class=copy-to-clipboard title="A Sequential Quadratic Programming Approach to the Solution of Open-Loop Generalized Nash Equilibria for Autonomous Racing" index=147>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-147 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Karush-Kuhn-Tucker<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00186v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00186v1.pdf filename=2404.00186v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Dynamic games can be an effective approach for modeling interactive behavior between multiple competitive agents in autonomous racing and they provide a theoretical framework for simultaneous prediction and control in such scenarios. In this work, we propose DG-SQP, a numerical method for the solution of local generalized Nash equilibria (GNE) for open-loop general-sum dynamic games for agents with nonlinear dynamics and constraints. In particular, we formulate a sequential quadratic programming (SQP) approach which requires only the solution of a single convex quadratic program at each iteration. The three key elements of the method are a non-monotonic line search for solving the associated <b>KKT</b> equations, a merit function to handle zero sum costs, and a decaying regularization scheme for SQP step selection. We show that our method achieves linear convergence in the neighborhood of local GNE and demonstrate the effectiveness of the approach in the context of head-to-head car racing, where we show significant improvement in solver success rate when comparing against the state-of-the-art PATH solver for dynamic games. An implementation of our solver can be found at <a href=https://github.com/zhu-edward/DGSQP>https://github.com/zhu-edward/DGSQP</a>.</p></p class="citation"></blockquote><h3 id=810--148204-lego-drive-language-enhanced-goal-oriented-closed-loop-end-to-end-autonomous-driving-pranjal-paul-et-al-2024>(8/10 | 148/204) LeGo-Drive: Language-enhanced Goal-oriented Closed-Loop End-to-End Autonomous Driving (Pranjal Paul et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pranjal Paul, Anant Garg, Tushar Choudhary, Arun Kumar Singh, K. Madhava Krishna. (2024)<br><strong>LeGo-Drive: Language-enhanced Goal-oriented Closed-Loop End-to-End Autonomous Driving</strong><br><button class=copy-to-clipboard title="LeGo-Drive: Language-enhanced Goal-oriented Closed-Loop End-to-End Autonomous Driving" index=148>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-148 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20116v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20116v1.pdf filename=2403.20116v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing <b>Vision-Language</b> models (VLMs) estimate either long-term trajectory waypoints or a set of control actions as a reactive solution for closed-loop planning based on their rich scene comprehension. However, these estimations are coarse and are subjective to their &ldquo;world understanding&rdquo; which may generate sub-optimal decisions due to perception errors. In this paper, we introduce LeGo-Drive, which aims to address this issue by estimating a goal location based on the given language command as an intermediate representation in an end-to-end setting. The estimated goal might fall in a non-desirable region, like on top of a car for a parking-like command, leading to inadequate planning. Hence, we propose to train the architecture in an end-to-end manner, resulting in iterative refinement of both the goal and the trajectory collectively. We validate the effectiveness of our method through comprehensive experiments conducted in diverse simulated environments. We report significant improvements in standard autonomous driving metrics, with a goal reaching Success Rate of 81%. We further showcase the versatility of LeGo-Drive across different driving scenarios and linguistic inputs, underscoring its potential for practical deployment in autonomous vehicles and intelligent transportation systems.</p></p class="citation"></blockquote><h3 id=910--149204-a-peg-in-hole-task-strategy-for-holes-in-concrete-andré-yuji-yasutomi-et-al-2024>(9/10 | 149/204) A Peg-in-hole Task Strategy for Holes in Concrete (André Yuji Yasutomi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>André Yuji Yasutomi, Hiroki Mori, Tetsuya Ogata. (2024)<br><strong>A Peg-in-hole Task Strategy for Holes in Concrete</strong><br><button class=copy-to-clipboard title="A Peg-in-hole Task Strategy for Holes in Concrete" index=149>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-149 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19946v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19946v1.pdf filename=2403.19946v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A method that enables an industrial robot to accomplish the peg-in-hole task for holes in concrete is proposed. The proposed method involves slightly detaching the peg from the wall, when moving between search positions, to avoid the negative influence of the concrete&rsquo;s high friction coefficient. It uses a deep neural network (DNN), trained via <b>reinforcement</b> <b>learning,</b> to effectively find holes with variable shape and surface finish (due to the brittle nature of concrete) without analytical modeling or control parameter tuning. The method uses displacement of the peg toward the wall surface, in addition to force and torque, as one of the inputs of the DNN. Since the displacement increases as the peg gets closer to the hole (due to the chamfered shape of holes in concrete), it is a useful parameter for inputting in the DNN. The proposed method was evaluated by training the DNN on a hole 500 times and attempting to find 12 unknown holes. The results of the evaluation show the DNN enabled a robot to find the unknown holes with average success rate of 96.1% and average execution time of 12.5 seconds. Additional evaluations with random initial positions and a different type of peg demonstrate the trained DNN can generalize well to different conditions. Analyses of the influence of the peg displacement input showed the success rate of the DNN is increased by utilizing this parameter. These results validate the proposed method in terms of its effectiveness and applicability to the construction industry.</p></p class="citation"></blockquote><h3 id=1010--150204-fusion-dynamical-systems-with-machine-learning-in-imitation-learning-a-comprehensive-overview-yingbai-hu-et-al-2024>(10/10 | 150/204) Fusion Dynamical Systems with Machine Learning in Imitation Learning: A Comprehensive Overview (Yingbai Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yingbai Hu, Fares J. Abu-Dakka, Fei Chen, Xiao Luo, Zheng Li, Alois Knoll, Weiping Ding. (2024)<br><strong>Fusion Dynamical Systems with Machine Learning in Imitation Learning: A Comprehensive Overview</strong><br><button class=copy-to-clipboard title="Fusion Dynamical Systems with Machine Learning in Imitation Learning: A Comprehensive Overview" index=150>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-150 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19916v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19916v1.pdf filename=2403.19916v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Imitation Learning (IL), also referred to as Learning from Demonstration (LfD), holds significant promise for capturing expert motor skills through efficient imitation, facilitating adept navigation of complex scenarios. A persistent challenge in IL lies in extending generalization from historical demonstrations, enabling the acquisition of new skills without re-teaching. Dynamical system-based IL (DSIL) emerges as a significant subset of IL methodologies, offering the ability to learn trajectories via movement primitives and policy learning based on experiential abstraction. This paper emphasizes the fusion of theoretical paradigms, integrating control theory principles inherent in dynamical systems into IL. This integration notably enhances robustness, adaptability, and convergence in the face of novel scenarios. This survey aims to present a comprehensive overview of DSIL methods, spanning from classical approaches to recent advanced approaches. We categorize DSIL into autonomous dynamical systems and non-autonomous dynamical systems, surveying traditional IL methods with low-dimensional input and advanced deep IL methods with high-dimensional input. Additionally, we present and analyze three main stability methods for IL: Lyapunov stability, contraction theory, and diffeomorphism mapping. Our exploration also extends to popular policy improvement methods for DSIL, encompassing <b>reinforcement</b> <b>learning,</b> deep <b>reinforcement</b> <b>learning,</b> and evolutionary strategies.</p></p class="citation"></blockquote><h2 id=csir-5>cs.IR (5)</h2><h3 id=15--151204-kguf-simple-knowledge-aware-graph-based-recommender-with-user-based-semantic-features-filtering-salvatore-bufi-et-al-2024>(1/5 | 151/204) KGUF: Simple Knowledge-aware Graph-based Recommender with User-based Semantic Features Filtering (Salvatore Bufi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Salvatore Bufi, Alberto Carlo Maria Mancino, Antonio Ferrara, Daniele Malitesta, Tommaso Di Noia, Eugenio Di Sciascio. (2024)<br><strong>KGUF: Simple Knowledge-aware Graph-based Recommender with User-based Semantic Features Filtering</strong><br><button class=copy-to-clipboard title="KGUF: Simple Knowledge-aware Graph-based Recommender with User-based Semantic Features Filtering" index=151>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-151 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 53<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Knowledge Graph, Knowledge Graph, Recommendation, Recommender System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20095v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20095v1.pdf filename=2403.20095v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The recent integration of <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> into <b>recommendation</b> has led to a novel family of Collaborative Filtering (CF) approaches, namely <b>Graph</b> <b>Collaborative</b> <b>Filtering</b> (GCF). Following the same <b>GNNs</b> wave, <b>recommender</b> <b>systems</b> exploiting <b>Knowledge</b> <b>Graphs</b> <b>(KGs)</b> <b>have</b> also been successfully empowered by the GCF rationale to combine the representational power of <b>GNNs</b> with the semantics conveyed by <b>KGs,</b> giving rise to <b>Knowledge-aware</b> <b>Graph</b> <b>Collaborative</b> <b>Filtering</b> (KGCF), which use <b>KGs</b> to mine hidden user intent. Nevertheless, empirical evidence suggests that computing and combining user-level intent might not always be necessary, as simpler approaches can yield comparable or superior results while keeping explicit semantic features. Under this perspective, user historical preferences become essential to refine the <b>KG</b> and retain the most discriminating features, thus leading to concise item representation. Driven by the assumptions above, we propose KGUF, a KGCF model that learns latent representations of semantic features in the <b>KG</b> to better define the item profile. By leveraging user profiles through decision trees, KGUF effectively retains only those features relevant to users. Results on three datasets justify KGUF&rsquo;s rationale, as our approach is able to reach performance comparable or superior to SOTA methods while maintaining a simpler formalization. Link to the repository: <a href=https://github.com/sisinflab/KGUF>https://github.com/sisinflab/KGUF</a>.</p></p class="citation"></blockquote><h3 id=25--152204-shallow-cross-encoders-for-low-latency-retrieval-aleksandr-v-petrov-et-al-2024>(2/5 | 152/204) Shallow Cross-Encoders for Low-Latency Retrieval (Aleksandr V. Petrov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aleksandr V. Petrov, Sean MacAvaney, Craig Macdonald. (2024)<br><strong>Shallow Cross-Encoders for Low-Latency Retrieval</strong><br><button class=copy-to-clipboard title="Shallow Cross-Encoders for Low-Latency Retrieval" index=152>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-152 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-CL, cs-IR, cs.IR<br>Keyword Score: 40<br>Keywords: Recommendation, BERT, T5, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20222v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20222v1.pdf filename=2403.20222v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Transformer-based</b> Cross-Encoders achieve state-of-the-art effectiveness in text retrieval. However, Cross-Encoders based on large <b>transformer</b> models (such as <b>BERT</b> or <b>T5)</b> are computationally expensive and allow for scoring only a small number of documents within a reasonably small latency window. However, keeping search latencies low is important for user satisfaction and energy usage. In this paper, we show that weaker shallow <b>transformer</b> models (i.e., <b>transformers</b> with a limited number of layers) actually perform better than full-scale models when constrained to these practical low-latency settings since they can estimate the relevance of more documents in the same time budget. We further show that shallow <b>transformers</b> may benefit from the generalized Binary Cross-Entropy (gBCE) training scheme, which has recently demonstrated success for <b>recommendation</b> tasks. Our experiments with TREC Deep Learning passage ranking query sets demonstrate significant improvements in shallow and full-scale models in low-latency scenarios. For example, when the latency limit is 25ms per query, MonoBERT-Large (a cross-encoder based on a full-scale <b>BERT</b> model) is only able to achieve NDCG@10 of 0.431 on TREC DL 2019, while TinyBERT-gBCE (a cross-encoder based on TinyBERT trained with gBCE) reaches NDCG@10 of 0.652, a +51% gain over MonoBERT-Large. We also show that shallow Cross-Encoders are effective even when used without a GPU (e.g., with CPU inference, NDCG@10 decreases only by 3% compared to GPU inference with 50ms latency), which makes Cross-Encoders practical to run even without specialized hardware acceleration.</p></p class="citation"></blockquote><h3 id=35--153204-robust-federated-contrastive-recommender-system-against-model-poisoning-attack-wei-yuan-et-al-2024>(3/5 | 153/204) Robust Federated Contrastive Recommender System against Model Poisoning Attack (Wei Yuan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wei Yuan, Chaoqun Yang, Liang Qu, Guanhua Ye, Quoc Viet Hung Nguyen, Hongzhi Yin. (2024)<br><strong>Robust Federated Contrastive Recommender System against Model Poisoning Attack</strong><br><button class=copy-to-clipboard title="Robust Federated Contrastive Recommender System against Model Poisoning Attack" index=153>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-153 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 30<br>Keywords: Contrastive Learning, Recommendation, Recommender System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20107v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20107v1.pdf filename=2403.20107v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Federated <b>Recommender</b> <b>Systems</b> (FedRecs) have garnered increasing attention recently, thanks to their privacy-preserving benefits. However, the decentralized and open characteristics of current FedRecs present two dilemmas. First, the performance of FedRecs is compromised due to highly sparse on-device data for each client. Second, the system&rsquo;s robustness is undermined by the vulnerability to model poisoning attacks launched by malicious users. In this paper, we introduce a novel <b>contrastive</b> <b>learning</b> framework designed to fully leverage the client&rsquo;s sparse data through embedding augmentation, referred to as CL4FedRec. Unlike previous <b>contrastive</b> <b>learning</b> approaches in FedRecs that necessitate clients to share their private parameters, our CL4FedRec aligns with the basic FedRec learning protocol, ensuring compatibility with most existing FedRec implementations. We then evaluate the robustness of FedRecs equipped with CL4FedRec by subjecting it to several state-of-the-art model poisoning attacks. Surprisingly, our observations reveal that <b>contrastive</b> <b>learning</b> tends to exacerbate the vulnerability of FedRecs to these attacks. This is attributed to the enhanced embedding uniformity, making the polluted target item embedding easily proximate to popular items. Based on this insight, we propose an enhanced and robust version of CL4FedRec (rCL4FedRec) by introducing a regularizer to maintain the distance among item embeddings with different popularity levels. Extensive experiments conducted on four commonly used <b>recommendation</b> datasets demonstrate that CL4FedRec significantly enhances both the model&rsquo;s performance and the robustness of FedRecs.</p></p class="citation"></blockquote><h3 id=45--154204-review-based-cross-domain-recommendation-via-hyperbolic-embedding-and-hierarchy-aware-domain-disentanglement-yoonhyuk-choi-2024>(4/5 | 154/204) Review-Based Cross-Domain Recommendation via Hyperbolic Embedding and Hierarchy-Aware Domain Disentanglement (Yoonhyuk Choi, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yoonhyuk Choi. (2024)<br><strong>Review-Based Cross-Domain Recommendation via Hyperbolic Embedding and Hierarchy-Aware Domain Disentanglement</strong><br><button class=copy-to-clipboard title="Review-Based Cross-Domain Recommendation via Hyperbolic Embedding and Hierarchy-Aware Domain Disentanglement" index=154>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-154 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs-LG, cs.IR<br>Keyword Score: 25<br>Keywords: Geometry, Recommendation, Recommender System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20298v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20298v1.pdf filename=2403.20298v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The issue of data sparsity poses a significant challenge to <b>recommender</b> <b>systems.</b> In response to this, algorithms that leverage side information such as review texts have been proposed. Furthermore, Cross-Domain <b>Recommendation</b> (CDR), which captures domain-shareable knowledge and transfers it from a richer domain (source) to a sparser one (target), has received notable attention. Nevertheless, the majority of existing methodologies assume a Euclidean embedding space, encountering difficulties in accurately representing richer text information and managing complex interactions between users and items. This paper advocates a hyperbolic CDR approach based on review texts for modeling user-item relationships. We first emphasize that conventional distance-based domain alignment techniques may cause problems because small modifications in hyperbolic <b>geometry</b> result in magnified perturbations, ultimately leading to the collapse of hierarchical structures. To address this challenge, we propose hierarchy-aware embedding and domain alignment schemes that adjust the scale to extract domain-shareable information without disrupting structural forms. The process involves the initial embedding of review texts in hyperbolic space, followed by feature extraction incorporating degree-based normalization and structure alignment. We conducted extensive experiments to substantiate the efficiency, robustness, and scalability of our proposed model in comparison to state-of-the-art baselines.</p></p class="citation"></blockquote><h3 id=55--155204-aiming-at-the-target-filter-collaborative-information-for-cross-domain-recommendation-hanyu-li-et-al-2024>(5/5 | 155/204) Aiming at the Target: Filter Collaborative Information for Cross-Domain Recommendation (Hanyu Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hanyu Li, Weizhi Ma, Peijie Sun, Jiayu Li, Cunxiang Yin, Yancheng He, Guoqiang Xu, Min Zhang, Shaoping Ma. (2024)<br><strong>Aiming at the Target: Filter Collaborative Information for Cross-Domain Recommendation</strong><br><button class=copy-to-clipboard title="Aiming at the Target: Filter Collaborative Information for Cross-Domain Recommendation" index=155>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-155 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20296v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20296v1.pdf filename=2403.20296v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Cross-domain recommender (CDR) systems aim to enhance the performance of the target domain by utilizing data from other related domains. However, irrelevant information from the source domain may instead degrade target domain performance, which is known as the negative transfer problem. There have been some attempts to address this problem, mostly by designing adaptive representations for overlapped users. Whereas, representation adaptions solely rely on the expressive capacity of the CDR model, lacking explicit constraint to filter the irrelevant source-domain collaborative information for the target domain. In this paper, we propose a novel Collaborative information regularized User Transformation (CUT) framework to tackle the negative transfer problem by directly filtering users&rsquo; collaborative information. In CUT, user similarity in the target domain is adopted as a constraint for user transformation learning to filter the user collaborative information from the source domain. CUT first learns user similarity relationships from the target domain. Then, source-target information transfer is guided by the user similarity, where we design a user transformation layer to learn target-domain user representations and a contrastive loss to supervise the user collaborative information transferred. The results show significant performance improvement of CUT compared with SOTA single and cross-domain methods. Further analysis of the target-domain results illustrates that CUT can effectively alleviate the negative transfer problem.</p></p class="citation"></blockquote><h2 id=csmm-1>cs.MM (1)</h2><h3 id=11--156204-convbench-a-multi-turn-conversation-evaluation-benchmark-with-hierarchical-capability-for-large-vision-language-models-shuo-liu-et-al-2024>(1/1 | 156/204) ConvBench: A Multi-Turn Conversation Evaluation Benchmark with Hierarchical Capability for Large Vision-Language Models (Shuo Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shuo Liu, Kaining Ying, Hao Zhang, Yue Yang, Yuqi Lin, Tianle Zhang, Chuanhao Li, Yu Qiao, Ping Luo, Wenqi Shao, Kaipeng Zhang. (2024)<br><strong>ConvBench: A Multi-Turn Conversation Evaluation Benchmark with Hierarchical Capability for Large Vision-Language Models</strong><br><button class=copy-to-clipboard title="ConvBench: A Multi-Turn Conversation Evaluation Benchmark with Hierarchical Capability for Large Vision-Language Models" index=156>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-156 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MM<br>Categories: cs-MM, cs.MM<br>Keyword Score: 49<br>Keywords: Automatic Evaluation, Benchmarking, Multi-modal, Multi-modal, GPT-4, Reasoning, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20194v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20194v1.pdf filename=2403.20194v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents ConvBench, a novel multi-turn conversation evaluation <b>benchmark</b> tailored for Large <b>Vision-Language</b> Models (LVLMs). Unlike existing <b>benchmarks</b> that assess individual capabilities in single-turn dialogues, ConvBench adopts a three-level <b>multimodal</b> capability hierarchy, mimicking human cognitive processes by stacking up perception, <b>reasoning,</b> and creativity. Each level focuses on a distinct capability, mirroring the cognitive progression from basic perception to logical <b>reasoning</b> and ultimately to advanced creativity. ConvBench comprises 577 meticulously curated multi-turn conversations encompassing 215 tasks reflective of real-world demands. <b>Automatic</b> <b>evaluations</b> quantify response performance at each turn and overall conversation level. Leveraging the capability hierarchy, ConvBench enables precise attribution of conversation mistakes to specific levels. Experimental results reveal a performance gap between <b>multi-modal</b> models, including <b>GPT4-V,</b> and human performance in multi-turn conversations. Additionally, weak fine-grained perception in <b>multi-modal</b> models contributes to <b>reasoning</b> and creation failures. ConvBench serves as a catalyst for further research aimed at enhancing visual dialogues.</p></p class="citation"></blockquote><h2 id=eessiv-7>eess.IV (7)</h2><h3 id=17--157204-unsupervised-tumor-aware-distillation-for-multi-modal-brain-image-translation-chuan-huang-et-al-2024>(1/7 | 157/204) Unsupervised Tumor-Aware Distillation for Multi-Modal Brain Image Translation (Chuan Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chuan Huang, Jia Wei, Rui Li. (2024)<br><strong>Unsupervised Tumor-Aware Distillation for Multi-Modal Brain Image Translation</strong><br><button class=copy-to-clipboard title="Unsupervised Tumor-Aware Distillation for Multi-Modal Brain Image Translation" index=157>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-157 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 43<br>Keywords: Knowledge Distillation, Knowledge Distillation, Multi-modal, Unsupervised Learning, Image2text<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20168v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20168v1.pdf filename=2403.20168v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multi-modal</b> brain <b>images</b> <b>from</b> MRI scans are widely used in clinical diagnosis to provide complementary information from different modalities. However, obtaining fully paired <b>multi-modal</b> <b>images</b> <b>in</b> practice is challenging due to various factors, such as time, cost, and artifacts, resulting in modality-missing brain <b>images.</b> <b>To</b> address this problem, <b>unsupervised</b> <b>multi-modal</b> brain <b>image</b> <b>translation</b> has been extensively studied. Existing methods suffer from the problem of brain tumor deformation during translation, as they fail to focus on the tumor areas when translating the whole <b>images.</b> <b>In</b> this paper, we propose an <b>unsupervised</b> tumor-aware <b>distillation</b> teacher-student network called UTAD-Net, which is capable of perceiving and translating tumor areas precisely. Specifically, our model consists of two parts: a teacher network and a student network. The teacher network learns an end-to-end mapping from source to target modality using unpaired <b>images</b> <b>and</b> corresponding tumor masks first. Then, the translation knowledge is <b>distilled</b> into the student network, enabling it to generate more realistic tumor areas and whole <b>images</b> <b>without</b> masks. Experiments show that our model achieves competitive performance on both quantitative and qualitative evaluations of <b>image</b> <b>quality</b> compared with state-of-the-art methods. Furthermore, we demonstrate the effectiveness of the generated <b>images</b> <b>on</b> downstream segmentation tasks. Our code is available at <a href=https://github.com/scut-HC/UTAD-Net>https://github.com/scut-HC/UTAD-Net</a>.</p></p class="citation"></blockquote><h3 id=27--158204-ultralight-vm-unet-parallel-vision-mamba-significantly-reduces-parameters-for-skin-lesion-segmentation-renkai-wu-et-al-2024>(2/7 | 158/204) UltraLight VM-UNet: Parallel Vision Mamba Significantly Reduces Parameters for Skin Lesion Segmentation (Renkai Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Renkai Wu, Yinghao Liu, Pengchen Liang, Qing Chang. (2024)<br><strong>UltraLight VM-UNet: Parallel Vision Mamba Significantly Reduces Parameters for Skin Lesion Segmentation</strong><br><button class=copy-to-clipboard title="UltraLight VM-UNet: Parallel Vision Mamba Significantly Reduces Parameters for Skin Lesion Segmentation" index=158>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-158 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 20<br>Keywords: Convolutional Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20035v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20035v1.pdf filename=2403.20035v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Traditionally for improving the segmentation performance of models, most approaches prefer to use adding more complex modules. And this is not suitable for the medical field, especially for mobile medical devices, where computationally loaded models are not suitable for real clinical environments due to computational resource constraints. Recently, state-space models (SSMs), represented by Mamba, have become a strong competitor to traditional <b>CNNs</b> and <b>Transformers.</b> In this paper, we deeply explore the key elements of parameter influence in Mamba and propose an UltraLight Vision Mamba UNet (UltraLight VM-UNet) based on this. Specifically, we propose a method for processing features in parallel Vision Mamba, named PVM Layer, which achieves excellent performance with the lowest computational load while keeping the overall number of processing channels constant. We conducted comparisons and ablation experiments with several state-of-the-art lightweight models on three skin lesion public datasets and demonstrated that the UltraLight VM-UNet exhibits the same strong performance competitiveness with parameters of only 0.049M and GFLOPs of 0.060. In addition, this study deeply explores the key elements of parameter influence in Mamba, which will lay a theoretical foundation for Mamba to possibly become a new mainstream module for lightweighting in the future. The code is available from <a href=https://github.com/wurenkai/UltraLight-VM-UNet>https://github.com/wurenkai/UltraLight-VM-UNet</a> .</p></p class="citation"></blockquote><h3 id=37--159204-an-interpretable-cross-attentive-multi-modal-mri-fusion-framework-for-schizophrenia-diagnosis-ziyu-zhou-et-al-2024>(3/7 | 159/204) An Interpretable Cross-Attentive Multi-modal MRI Fusion Framework for Schizophrenia Diagnosis (Ziyu Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ziyu Zhou, Anton Orlichenko, Gang Qu, Zening Fu, Vince D Calhoun, Zhengming Ding, Yu-Ping Wang. (2024)<br><strong>An Interpretable Cross-Attentive Multi-modal MRI Fusion Framework for Schizophrenia Diagnosis</strong><br><button class=copy-to-clipboard title="An Interpretable Cross-Attentive Multi-modal MRI Fusion Framework for Schizophrenia Diagnosis" index=159>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-159 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 13<br>Keywords: Multi-modal, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00144v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00144v1.pdf filename=2404.00144v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Both functional and structural magnetic resonance imaging (fMRI and sMRI) are widely used for the diagnosis of mental disorder. However, combining complementary information from these two modalities is challenging due to their heterogeneity. Many existing methods fall short of capturing the interaction between these modalities, frequently defaulting to a simple combination of latent features. In this paper, we propose a novel Cross-Attentive <b>Multi-modal</b> Fusion framework (CAMF), which aims to capture both intra-modal and inter-modal relationships between fMRI and sMRI, enhancing <b>multi-modal</b> data representation. Specifically, our CAMF framework employs <b>self-attention</b> modules to identify interactions within each modality while cross-attention modules identify interactions between modalities. Subsequently, our approach optimizes the integration of latent features from both modalities. This approach significantly improves classification accuracy, as demonstrated by our evaluations on two extensive <b>multi-modal</b> brain imaging datasets, where CAMF consistently outperforms existing methods. Furthermore, the gradient-guided Score-CAM is applied to interpret critical functional networks and brain regions involved in schizophrenia. The bio-markers identified by CAMF align with established research, potentially offering new insights into the diagnosis and pathological endophenotypes of schizophrenia.</p></p class="citation"></blockquote><h3 id=47--160204-fetaldiffusion-pose-controllable-3d-fetal-mri-synthesis-with-conditional-diffusion-model-molin-zhang-et-al-2024>(4/7 | 160/204) FetalDiffusion: Pose-Controllable 3D Fetal MRI Synthesis with Conditional Diffusion Model (Molin Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Molin Zhang, Polina Golland, Patricia Ellen Grant, Elfar Adalsteinsson. (2024)<br><strong>FetalDiffusion: Pose-Controllable 3D Fetal MRI Synthesis with Conditional Diffusion Model</strong><br><button class=copy-to-clipboard title="FetalDiffusion: Pose-Controllable 3D Fetal MRI Synthesis with Conditional Diffusion Model" index=160>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-160 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00132v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00132v1.pdf filename=2404.00132v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The quality of fetal MRI is significantly affected by unpredictable and substantial fetal motion, leading to the introduction of artifacts even when fast acquisition sequences are employed. The development of 3D real-time fetal pose estimation approaches on volumetric EPI fetal MRI opens up a promising avenue for fetal motion monitoring and prediction. Challenges arise in fetal pose estimation due to limited number of real scanned fetal MR training images, hindering model generalization when the acquired fetal MRI lacks adequate pose. In this study, we introduce FetalDiffusion, a novel approach utilizing a conditional <b>diffusion</b> <b>model</b> to generate 3D synthetic fetal MRI with controllable pose. Additionally, an auxiliary pose-level loss is adopted to enhance model performance. Our work demonstrates the success of this proposed model by producing high-quality synthetic fetal MRI images with accurate and recognizable fetal poses, comparing favorably with in-vivo real fetal MRI. Furthermore, we show that the integration of synthetic fetal MR images enhances the fetal pose estimation model&rsquo;s performance, particularly when the number of available real scanned data is limited resulting in 15.4% increase in PCK and 50.2% reduced in mean error. All experiments are done on a single 32GB V100 GPU. Our method holds promise for improving real-time tracking models, thereby addressing fetal motion issues more effectively.</p></p class="citation"></blockquote><h3 id=57--161204-a-multi-stage-semi-supervised-learning-for-ankle-fracture-classification-on-ct-images-hongzhi-liu-et-al-2024>(5/7 | 161/204) A multi-stage semi-supervised learning for ankle fracture classification on CT images (Hongzhi Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hongzhi Liu, Guicheng Li, Jiacheng Nie, Hui Tang, Chunfeng Yang, Qianjin Feng, Hailin Xu, Yang Chen. (2024)<br><strong>A multi-stage semi-supervised learning for ankle fracture classification on CT images</strong><br><button class=copy-to-clipboard title="A multi-stage semi-supervised learning for ankle fracture classification on CT images" index=161>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-161 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Semi-Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19983v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19983v1.pdf filename=2403.19983v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Because of the complicated mechanism of ankle injury, it is very difficult to diagnose ankle fracture in clinic. In order to simplify the process of fracture diagnosis, an automatic diagnosis model of ankle fracture was proposed. Firstly, a tibia-fibula segmentation network is proposed for the joint tibiofibular region of the ankle joint, and the corresponding segmentation dataset is established on the basis of fracture data. Secondly, the image registration method is used to register the bone segmentation mask with the normal bone mask. Finally, a <b>semi-supervised</b> <b>classifier</b> is constructed to make full use of a large number of unlabeled data to classify ankle fractures. Experiments show that the proposed method can segment fractures with fracture lines accurately and has better performance than the general method. At the same time, this method is superior to classification network in several indexes.</p></p class="citation"></blockquote><h3 id=67--162204-multi-task-magnetic-resonance-imaging-reconstruction-using-meta-learning-wanyu-bian-et-al-2024>(6/7 | 162/204) Multi-task Magnetic Resonance Imaging Reconstruction using Meta-learning (Wanyu Bian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wanyu Bian, Albert Jang, Fang Liu. (2024)<br><strong>Multi-task Magnetic Resonance Imaging Reconstruction using Meta-learning</strong><br><button class=copy-to-clipboard title="Multi-task Magnetic Resonance Imaging Reconstruction using Meta-learning" index=162>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-162 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV, math-OC<br>Keyword Score: 10<br>Keywords: Meta Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19966v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19966v1.pdf filename=2403.19966v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Using single-task deep learning methods to reconstruct Magnetic Resonance Imaging (MRI) data acquired with different imaging sequences is inherently challenging. The trained deep learning model typically lacks generalizability, and the dissimilarity among image datasets with different types of contrast leads to suboptimal learning performance. This paper proposes a <b>meta-learning</b> <b>approach</b> to efficiently learn image features from multiple MR image datasets. Our algorithm can perform multi-task learning to simultaneously reconstruct MR images acquired using different imaging sequences with different image contrasts. The experiment results demonstrate the ability of our new <b>meta-learning</b> <b>reconstruction</b> method to successfully reconstruct highly-undersampled k-space data from multiple MRI datasets simultaneously, outperforming other compelling reconstruction methods previously developed for single-task learning.</p></p class="citation"></blockquote><h3 id=77--163204-revolutionizing-disease-diagnosis-with-simultaneous-functional-petmr-and-deeply-integrated-brain-metabolic-hemodynamic-and-perfusion-networks-luoyu-wang-et-al-2024>(7/7 | 163/204) Revolutionizing Disease Diagnosis with simultaneous functional PET/MR and Deeply Integrated Brain Metabolic, Hemodynamic, and Perfusion Networks (Luoyu Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Luoyu Wang, Yitian Tao, Qing Yang, Yan Liang, Siwei Liu, Hongcheng Shi, Dinggang Shen, Han Zhang. (2024)<br><strong>Revolutionizing Disease Diagnosis with simultaneous functional PET/MR and Deeply Integrated Brain Metabolic, Hemodynamic, and Perfusion Networks</strong><br><button class=copy-to-clipboard title="Revolutionizing Disease Diagnosis with simultaneous functional PET/MR and Deeply Integrated Brain Metabolic, Hemodynamic, and Perfusion Networks" index=163>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-163 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-AI, cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 6<br>Keywords: Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20058v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20058v1.pdf filename=2403.20058v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Simultaneous functional PET/MR (sf-PET/MR) presents a cutting-edge <b>multimodal</b> neuroimaging technique. It provides an unprecedented opportunity for concurrently monitoring and integrating multifaceted brain networks built by spatiotemporally covaried metabolic activity, neural activity, and cerebral blood flow (perfusion). Albeit high scientific/clinical values, short in hardware accessibility of PET/MR hinders its applications, let alone modern AI-based PET/MR fusion models. Our objective is to develop a clinically feasible AI-based disease diagnosis model trained on comprehensive sf-PET/MR data with the power of, during inferencing, allowing single modality input (e.g., PET only) as well as enforcing <b>multimodal-based</b> accuracy. To this end, we propose MX-ARM, a <b>multimodal</b> MiXture-of-experts Alignment and Reconstruction Model. It is modality detachable and exchangeable, allocating different multi-layer perceptrons dynamically (&ldquo;mixture of experts&rdquo;) through learnable weights to learn respective representations from different modalities. Such design will not sacrifice model performance in uni-modal situation. To fully exploit the inherent complex and nonlinear relation among modalities while producing fine-grained representations for uni-modal inference, we subsequently add a modal alignment module to line up a dominant modality (e.g., PET) with representations of auxiliary modalities (MR). We further adopt <b>multimodal</b> reconstruction to promote the quality of learned features. Experiments on precious <b>multimodal</b> sf-PET/MR data for Mild Cognitive Impairment diagnosis showcase the efficacy of our model toward clinically feasible precision medicine.</p></p class="citation"></blockquote><h2 id=csdb-3>cs.DB (3)</h2><h3 id=13--164204-purple-making-a-large-language-model-a-better-sql-writer-tonghui-ren-et-al-2024>(1/3 | 164/204) PURPLE: Making a Large Language Model a Better SQL Writer (Tonghui Ren et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tonghui Ren, Yuankai Fan, Zhenying He, Ren Huang, Jiaqi Dai, Can Huang, Yinan Jing, Kai Zhang, Yifan Yang, X. Sean Wang. (2024)<br><strong>PURPLE: Making a Large Language Model a Better SQL Writer</strong><br><button class=copy-to-clipboard title="PURPLE: Making a Large Language Model a Better SQL Writer" index=164>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-164 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DB<br>Categories: cs-AI, cs-CL, cs-DB, cs.DB<br>Keyword Score: 43<br>Keywords: Benchmarking, Natural Language Understanding, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20014v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20014v1.pdf filename=2403.20014v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Model</b> <b>(LLM)</b> techniques play an increasingly important role in <b>Natural</b> <b>Language</b> <b>to</b> SQL (NL2SQL) translation. <b>LLMs</b> trained by extensive corpora have strong <b>natural</b> <b>language</b> <b>understanding</b> and basic SQL generation abilities without additional tuning specific to NL2SQL tasks. Existing <b>LLMs-based</b> NL2SQL approaches try to improve the translation by enhancing the <b>LLMs</b> with an emphasis on user intention understanding. However, <b>LLMs</b> sometimes fail to generate appropriate SQL due to their lack of knowledge in organizing complex logical operator composition. A promising method is to input the <b>LLMs</b> with demonstrations, which include known NL2SQL translations from various databases. <b>LLMs</b> can learn to organize operator compositions from the input demonstrations for the given task. In this paper, we propose PURPLE (Pre-trained models Utilized to Retrieve <b>Prompts</b> for Logical Enhancement), which improves accuracy by retrieving demonstrations containing the requisite logical operator composition for the NL2SQL task on hand, thereby guiding <b>LLMs</b> to produce better SQL translation. PURPLE achieves a new state-of-the-art performance of 80.5% exact-set match accuracy and 87.8% execution match accuracy on the validation set of the popular NL2SQL <b>benchmark</b> Spider. PURPLE maintains high accuracy across diverse <b>benchmarks,</b> budgetary constraints, and various <b>LLMs,</b> showing robustness and cost-effectiveness.</p></p class="citation"></blockquote><h3 id=23--165204-budget-aware-query-tuning-an-automl-perspective-wentao-wu-et-al-2024>(2/3 | 165/204) Budget-aware Query Tuning: An AutoML Perspective (Wentao Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wentao Wu, Chi Wang. (2024)<br><strong>Budget-aware Query Tuning: An AutoML Perspective</strong><br><button class=copy-to-clipboard title="Budget-aware Query Tuning: An AutoML Perspective" index=165>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-165 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DB<br>Categories: cs-AI, cs-DB, cs-LG, cs.DB<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00137v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00137v1.pdf filename=2404.00137v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Modern database systems rely on cost-based query optimizers to come up with good execution plans for input queries. Such query optimizers rely on cost models to estimate the costs of candidate query execution plans. A cost model represents a function from a set of cost units to query execution cost, where each cost unit specifies the unit cost of executing a certain type of query processing operation (such as table scan or join). These cost units are traditionally viewed as constants, whose values only depend on the platform configuration where the database system runs on top of but are invariant for queries processed by the database system. In this paper, we challenge this classic view by thinking of these cost units as variables instead. We show that, by varying the cost-unit values one can obtain query plans that significantly outperform the default query plans returned by the query optimizer when viewing the cost units as constants. We term this cost-unit tuning process &ldquo;query tuning&rdquo; (QT) and show that it is similar to the well-known hyper-parameter optimization (HPO) problem in AutoML. As a result, any state-of-the-art HPO technologies can be applied to QT. We study the QT problem in the context of anytime tuning, which is desirable in practice by constraining the total time spent on QT within a given budget &ndash; we call this problem budget-aware query tuning. We further extend our study from tuning a single query to tuning a workload with multiple queries, and we call this generalized problem budget-aware workload tuning (WT), which aims for minimizing the execution time of the entire workload. WT is more challenging as one needs to further prioritize individual query tuning within the given time budget. We propose solutions to both QT and WT and experimental evaluation using both <b>benchmark</b> and real workloads demonstrates the efficacy of our proposed solutions.</p></p class="citation"></blockquote><h3 id=33--166204-multi-objective-genetic-algorithm-for-materialized-view-optimization-in-data-warehouses-mahdi-manavi-2024>(3/3 | 166/204) Multi-Objective Genetic Algorithm for Materialized View Optimization in Data Warehouses (Mahdi Manavi, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mahdi Manavi. (2024)<br><strong>Multi-Objective Genetic Algorithm for Materialized View Optimization in Data Warehouses</strong><br><button class=copy-to-clipboard title="Multi-Objective Genetic Algorithm for Materialized View Optimization in Data Warehouses" index=166>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-166 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DB<br>Categories: cs-DB, cs.DB<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19906v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19906v1.pdf filename=2403.19906v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Materialized views can significantly improve database query performance but identifying the optimal set of views to materialize is challenging. Prior work on automating and optimizing materialized view selection has limitations in execution time and total cost. In this paper, we present a novel genetic algorithm based approach to materialized view selection that aims to minimize execution time and total cost. Our technique encodes materialized view configurations as chromosomes and evolves the population over generations to discover high quality solutions. We employ an adaptive mutation rate, multi-objective fitness function, and lexicase selection to enhance genetic search. Comprehensive experiments on the TPC-H <b>benchmark</b> demonstrate the effectiveness of our algorithm. Compared to stateof-the-art methods, our approach improves average execution time by 11% and reduces total materialized view costs by an average of 16 million. These gains highlight the benefits of a datadriven evolutionary approach. Our genetic algorithm framework significantly outperforms current materialized view selection techniques in both efficiency and total cost reduction. This work represents an important advance in enabling performant and cost-effective utilization of materialized views in enterprise systems.</p></p class="citation"></blockquote><h2 id=csar-3>cs.AR (3)</h2><h3 id=13--167204-an-fpga-based-reconfigurable-accelerator-for-convolution-transformer-hybrid-efficientvit-haikuo-shao-et-al-2024>(1/3 | 167/204) An FPGA-Based Reconfigurable Accelerator for Convolution-Transformer Hybrid EfficientViT (Haikuo Shao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haikuo Shao, Huihong Shi, Wendong Mao, Zhongfeng Wang. (2024)<br><strong>An FPGA-Based Reconfigurable Accelerator for Convolution-Transformer Hybrid EfficientViT</strong><br><button class=copy-to-clipboard title="An FPGA-Based Reconfigurable Accelerator for Convolution-Transformer Hybrid EfficientViT" index=167>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-167 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AR<br>Categories: cs-AR, cs-LG, cs.AR<br>Keyword Score: 40<br>Keywords: Vision Transformer, Convolution, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20230v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20230v1.pdf filename=2403.20230v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Vision</b> <b>Transformers</b> (ViTs) have achieved significant success in computer <b>vision.</b> <b>However,</b> their intensive computations and massive memory footprint challenge ViTs&rsquo; deployment on embedded devices, calling for efficient ViTs. Among them, EfficientViT, the state-of-the-art one, features a <b>Convolution-Transformer</b> hybrid architecture, enhancing both accuracy and hardware efficiency. Unfortunately, existing accelerators cannot fully exploit the hardware benefits of EfficientViT due to its unique architecture. In this paper, we propose an FPGA-based accelerator for EfficientViT to advance the hardware efficiency frontier of ViTs. Specifically, we design a reconfigurable architecture to efficiently support various operation types, including lightweight <b>convolutions</b> and attention, boosting hardware utilization. Additionally, we present a time-multiplexed and pipelined dataflow to facilitate both intra- and inter-layer fusions, reducing off-chip data access costs. Experimental results show that our accelerator achieves up to 780.2 GOPS in throughput and 105.1 GOPS/W in energy efficiency at 200MHz on the Xilinx ZCU102 FPGA, which significantly outperforms prior works.</p></p class="citation"></blockquote><h3 id=23--168204-hot-lego-architect-microfluidic-cooling-equipped-3dics-with-pre-rtl-thermal-simulation-runxi-wang-et-al-2024>(2/3 | 168/204) Hot-LEGO: Architect Microfluidic Cooling Equipped 3DICs with Pre-RTL Thermal Simulation (Runxi Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Runxi Wang, Jun-Han Han, Mircea Stan, Xinfei Guo. (2024)<br><strong>Hot-LEGO: Architect Microfluidic Cooling Equipped 3DICs with Pre-RTL Thermal Simulation</strong><br><button class=copy-to-clipboard title="Hot-LEGO: Architect Microfluidic Cooling Equipped 3DICs with Pre-RTL Thermal Simulation" index=168>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-168 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AR<br>Categories: cs-AR, cs.AR<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20050v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20050v1.pdf filename=2403.20050v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Microfluidic cooling has been recognized as one of the most promising solutions to achieve efficient thermal management for three-dimensional integrated circuits (3DICs). It enables more opportunities to architect 3DICs with different die configurations. It becomes increasingly important to perform thermal analysis in the early design phases to validate the architectural design decisions. This is even more critical for microfluidic cooling equipped 3DICs as the embedded cooling structures greatly influence the performance, power, and reliability of the stacked system. We exploited the existing architectural simulators and developed a Pre-register-transfer-level (Pre-RTL) thermal <b>simulation</b> methodology named Hot-LEGO that integrates these tools with their latest features such as support for microfluidic cooling and 3DIC stacking configurations. This methodology differs from existing ones by looking into the design granularity at a much finer level which enables the exploration of unique architecture combinations across the vertical stack. Though architectural-level simulators are not designed for signoff-calibre, it offers speed and agility which are imperative for early design space exploration. We claim that this ongoing work will speed up the co-design cycle of microfluidic cooling and offer a portable methodology for architects to perform exhaustive search for the optimal microarchitecture solutions in 3DICs.</p></p class="citation"></blockquote><h3 id=33--169204-balanced-data-placement-for-gemv-acceleration-with-processing-in-memory-mohamed-assem-ibrahim-et-al-2024>(3/3 | 169/204) Balanced Data Placement for GEMV Acceleration with Processing-In-Memory (Mohamed Assem Ibrahim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohamed Assem Ibrahim, Mahzabeen Islam, Shaizeen Aga. (2024)<br><strong>Balanced Data Placement for GEMV Acceleration with Processing-In-Memory</strong><br><button class=copy-to-clipboard title="Balanced Data Placement for GEMV Acceleration with Processing-In-Memory" index=169>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-169 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AR<br>Categories: cs-AR, cs-DC, cs.AR<br>Keyword Score: 10<br>Keywords: Generative AI<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20297v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20297v2.pdf filename=2403.20297v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With unprecedented demand for <b>generative</b> <b>AI</b> (GenAI) inference, acceleration of primitives that dominate GenAI such as general matrix-vector multiplication (GEMV) is receiving considerable attention. A challenge with GEMVs is the high memory bandwidth this primitive demands. Multiple memory vendors have proposed commercially viable processing-in-memory (PIM) prototypes that attain bandwidth boost over processor via augmenting memory banks with compute capabilities and broadcasting same command to all banks. While proposed PIM designs stand to accelerate GEMV, we observe in this work that a key impediment to truly harness PIM acceleration is deducing optimal data-placement to place the matrix in memory banks. To this end, we tease out several factors that impact data-placement and propose PIMnast methodology which, like a gymnast, balances these factors to identify data-placements that deliver GEMV acceleration. Across a spectrum of GenAI models, our proposed PIMnast methodology along with additional orchestration knobs we identify delivers up to 6.86$\times$ speedup for GEMVs (of the available 7$\times$ roofline speedup) leading to up to 5$\times$ speedup for per-token latencies.</p></p class="citation"></blockquote><h2 id=csit-6>cs.IT (6)</h2><h3 id=16--170204-cooperative-sensing-and-communication-for-isac-networks-performance-analysis-and-optimization-kaitao-meng-et-al-2024>(1/6 | 170/204) Cooperative Sensing and Communication for ISAC Networks: Performance Analysis and Optimization (Kaitao Meng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kaitao Meng, Christos Masouros. (2024)<br><strong>Cooperative Sensing and Communication for ISAC Networks: Performance Analysis and Optimization</strong><br><button class=copy-to-clipboard title="Cooperative Sensing and Communication for ISAC Networks: Performance Analysis and Optimization" index=170>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-170 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 35<br>Keywords: Geometry, Simulation, Simulator, Scaling Law<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20228v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20228v1.pdf filename=2403.20228v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we study integrated sensing and communication (ISAC) networks intending to effectively balance sensing and communication (S&amp;C) performance at the network level. Through the simultaneous utilization of multi-point (CoMP) coordinated joint transmission and distributed multiple-input multiple-output (MIMO) radar techniques, we propose a cooperative networked ISAC scheme to enhance both S&amp;C services. Then, the tool of stochastic <b>geometry</b> is exploited to capture the S&amp;C performance, which allows us to illuminate key cooperative dependencies in the ISAC network. Remarkably, the derived expression of the Cramer-Rao lower bound (CRLB) of the localization accuracy unveils a significant finding: Deploying $N$ ISAC transceivers yields an enhanced sensing performance across the entire network, in accordance with the $\ln^2N$ <b>scaling</b> <b>law.</b> <b>Simulation</b> results demonstrate that compared to the time-sharing scheme, the proposed cooperative ISAC scheme can effectively improve the average data rate and reduce the CRLB.</p></p class="citation"></blockquote><h3 id=26--171204-a-signature-based-approach-towards-global-channel-charting-with-ultra-low-complexity-longhai-zhao-et-al-2024>(2/6 | 171/204) A Signature Based Approach Towards Global Channel Charting with Ultra Low Complexity (Longhai Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Longhai Zhao, Yunchuan Yang, Qi Xiong, He Wang, Bin Yu, Feifei Sun, Chengjun Sun. (2024)<br><strong>A Signature Based Approach Towards Global Channel Charting with Ultra Low Complexity</strong><br><button class=copy-to-clipboard title="A Signature Based Approach Towards Global Channel Charting with Ultra Low Complexity" index=171>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-171 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 25<br>Keywords: Geometry, Unsupervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20091v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20091v1.pdf filename=2403.20091v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Channel charting, an <b>unsupervised</b> <b>learning</b> method that learns a low-dimensional representation from channel information to preserve geometrical property of physical space of user equipments (UEs), has drawn many attentions from both academic and industrial communities, because it can facilitate many downstream tasks, such as indoor localization, UE handover, beam management, and so on. However, many previous works mainly focus on charting that only preserves local <b>geometry</b> and use raw channel information to learn the chart, which do not consider the global <b>geometry</b> and are often computationally intensive and very time-consuming. Therefore, in this paper, a novel signature based approach for global channel charting with ultra low complexity is proposed. By using an iterated-integral based method called signature transform, a compact feature map and a novel distance metric are proposed, which enable channel charting with ultra low complexity and preserving both local and global <b>geometry.</b> We demonstrate the efficacy of our method using synthetic and open-source real-field datasets.</p></p class="citation"></blockquote><h3 id=36--172204-secure-full-duplex-communication-via-movable-antennas-jingze-ding-et-al-2024>(3/6 | 172/204) Secure Full-Duplex Communication via Movable Antennas (Jingze Ding et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jingze Ding, Zijian Zhou, Chenbo Wang, Wenyao Li, Lifeng Lin, Bingli Jiao. (2024)<br><strong>Secure Full-Duplex Communication via Movable Antennas</strong><br><button class=copy-to-clipboard title="Secure Full-Duplex Communication via Movable Antennas" index=172>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-172 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 23<br>Keywords: Benchmarking, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20025v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20025v1.pdf filename=2403.20025v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper investigates physical layer security (PLS) for a movable antenna (MA)-assisted full-duplex (FD) system. In this system, an FD base station (BS) with multiple MAs for transmission and reception provides services for an uplink (UL) user and a downlink (DL) user. Each user operates in half-duplex (HD) mode and is equipped with a single fixed-position antenna (FPA), in the presence of a single-FPA eavesdropper (Eve). To ensure secure communication, artificial noise (AN) is transmitted to obstruct the interception of Eve. The objective of this paper is to maximize the sum secrecy rate (SSR) of the UL and DL users by jointly optimizing the beamformers of the BS and the positions of MAs. This paper also proposes an alternating optimization (AO) method to address the non-convex problem, which decomposes the optimization problem into three subproblems and solves them iteratively. <b>Simulation</b> results demonstrate a significant performance gain in the SSR achieved by the proposed scheme compared to the <b>benchmark</b> schemes.</p></p class="citation"></blockquote><h3 id=46--173204-minimizing-end-to-end-latency-for-joint-source-channel-coding-systems-kaiyi-chi-et-al-2024>(4/6 | 173/204) Minimizing End-to-End Latency for Joint Source-Channel Coding Systems (Kaiyi Chi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kaiyi Chi, Qianqian Yang, Yuanchao Shu, Zhaohui Yang, Zhiguo Shi. (2024)<br><strong>Minimizing End-to-End Latency for Joint Source-Channel Coding Systems</strong><br><button class=copy-to-clipboard title="Minimizing End-to-End Latency for Joint Source-Channel Coding Systems" index=173>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-173 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs-SY, cs.IT, eess-SY, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20198v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20198v1.pdf filename=2403.20198v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While existing studies have highlighted the advantages of deep learning (DL)-based joint source-channel coding (JSCC) schemes in enhancing transmission efficiency, they often overlook the crucial aspect of resource management during the deployment phase. In this paper, we propose an approach to minimize the transmission latency in an uplink JSCC-based system. We first analyze the correlation between end-to-end latency and task performance, based on which the end-to-end delay model for each device is established. Then, we formulate a non-convex optimization problem aiming at minimizing the maximum end-to-end latency across all devices, which is proved to be NP-hard. We then transform the original problem into a more tractable one, from which we derive the closed form solution on the optimal compression ratio, truncation threshold selection policy, and resource allocation strategy. We further introduce a heuristic algorithm with low complexity, leveraging insights from the structure of the optimal solution. <b>Simulation</b> results demonstrate that both the proposed optimal algorithm and the heuristic algorithm significantly reduce end-to-end latency. Notably, the proposed heuristic algorithm achieves nearly the same performance to the optimal solution but with considerably lower computational complexity.</p></p class="citation"></blockquote><h3 id=56--174204-joint-training-and-reflection-pattern-optimization-for-non-ideal-ris-aided-multiuser-systems-zhenyao-he-et-al-2024>(5/6 | 174/204) Joint Training and Reflection Pattern Optimization for Non-Ideal RIS-Aided Multiuser Systems (Zhenyao He et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhenyao He, Jindan Xu, Hong Shen, Wei Xu, Chau Yuen, Marco Di Renzo. (2024)<br><strong>Joint Training and Reflection Pattern Optimization for Non-Ideal RIS-Aided Multiuser Systems</strong><br><button class=copy-to-clipboard title="Joint Training and Reflection Pattern Optimization for Non-Ideal RIS-Aided Multiuser Systems" index=174>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-174 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19955v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19955v1.pdf filename=2403.19955v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Reconfigurable intelligent surface (RIS) is a promising technique to improve the performance of future wireless communication systems at low energy consumption. To reap the potential benefits of RIS-aided beamforming, it is vital to enhance the accuracy of channel estimation. In this paper, we consider an RIS-aided multiuser system with non-ideal reflecting elements, each of which has a phase-dependent reflecting amplitude, and we aim to minimize the mean-squared error (MSE) of the channel estimation by jointly optimizing the training signals at the user equipments (UEs) and the reflection pattern at the RIS. As examples the least squares (LS) and linear minimum MSE (LMMSE) estimators are considered. The considered problems do not admit simple solution mainly due to the complicated constraints pertaining to the non-ideal RIS reflecting elements. As far as the LS criterion is concerned, we tackle this difficulty by first proving the optimality of orthogonal training symbols and then propose a majorization-minimization (MM)-based iterative method to design the reflection pattern, where a semi-closed form solution is obtained in each iteration. As for the LMMSE criterion, we address the joint training and reflection pattern optimization problem with an MM-based alternating algorithm, where a closed-form solution to the training symbols and a semi-closed form solution to the RIS reflecting coefficients are derived, respectively. Furthermore, an acceleration scheme is proposed to improve the convergence rate of the proposed MM algorithms. Finally, <b>simulation</b> results demonstrate the performance advantages of our proposed joint training and reflection pattern designs.</p></p class="citation"></blockquote><h3 id=66--175204-an-information-theoretic-framework-for-out-of-distribution-generalization-wenliang-liu-et-al-2024>(6/6 | 175/204) An Information-Theoretic Framework for Out-of-Distribution Generalization (Wenliang Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenliang Liu, Guanding Yu, Lele Wang, Renjie Liao. (2024)<br><strong>An Information-Theoretic Framework for Out-of-Distribution Generalization</strong><br><button class=copy-to-clipboard title="An Information-Theoretic Framework for Out-of-Distribution Generalization" index=175>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-175 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs-LG, cs.IT, math-IT<br>Keyword Score: 10<br>Keywords: Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19895v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19895v1.pdf filename=2403.19895v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the <b>Out-of-Distribution</b> (OOD) generalization in machine learning and propose a general framework that provides information-theoretic generalization bounds. Our framework interpolates freely between Integral Probability Metric (IPM) and $f$-divergence, which naturally recovers some known results (including Wasserstein- and KL-bounds), as well as yields new generalization bounds. Moreover, we show that our framework admits an optimal transport interpretation. When evaluated in two concrete examples, the proposed bounds either strictly improve upon existing bounds in some cases or recover the best among existing OOD generalization bounds.</p></p class="citation"></blockquote><h2 id=cscy-1>cs.CY (1)</h2><h3 id=11--176204-distributed-agency-in-second-language-learning-and-teaching-through-generative-ai-robert-godwin-jones-2024>(1/1 | 176/204) Distributed agency in second language learning and teaching through generative AI (Robert Godwin-Jones, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Robert Godwin-Jones. (2024)<br><strong>Distributed agency in second language learning and teaching through generative AI</strong><br><button class=copy-to-clipboard title="Distributed agency in second language learning and teaching through generative AI" index=176>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-176 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-AI, cs-CY, cs.CY<br>Keyword Score: 30<br>Keywords: Generative AI, ChatGPT, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20216v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20216v1.pdf filename=2403.20216v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Generative</b> <b>AI</b> offers significant opportunities for language learning. Tools like <b>ChatGPT</b> can provide informal second language practice through chats in written or voice forms, with the learner specifying through <b>prompts</b> conversational parameters such as proficiency level, language register, and discussion topics. AI can be instructed to give corrective feedback, create practice exercises, or develop an extended study plan. Instructors can use AI to build learning and assessment materials in a variety of media. AI is likely to make immersive technologies more powerful and versatile, moving away from scripted interactions. For both learners and teachers, it is important to understand the limitations of AI systems that arise from their purely statistical model of human language, which limits their ability to deal with nuanced social and cultural aspects of language use. Additionally, there are ethical concerns over how AI systems are created as well as practical constraints in their use, especially for less privileged populations. The power and versatility of AI tools are likely to turn them into valuable and constant companions in many peoples lives (akin to smartphones), creating a close connection that goes beyond simple tool use. Ecological theories such as sociomaterialism are helpful in examining the shared agency that develops through close user-AI interactions, as are the perspectives on human-object relations from Indigenous cultures.</p></p class="citation"></blockquote><h2 id=cscr-3>cs.CR (3)</h2><h3 id=13--177204-homomorphic-wisards-efficient-weightless-neural-network-training-over-encrypted-data-leonardo-neumann-et-al-2024>(1/3 | 177/204) Homomorphic WiSARDs: Efficient Weightless Neural Network training over encrypted data (Leonardo Neumann et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Leonardo Neumann, Antonio Guimarães, Diego F. Aranha, Edson Borin. (2024)<br><strong>Homomorphic WiSARDs: Efficient Weightless Neural Network training over encrypted data</strong><br><button class=copy-to-clipboard title="Homomorphic WiSARDs: Efficient Weightless Neural Network training over encrypted data" index=177>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-177 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-LG, cs.CR<br>Keyword Score: 30<br>Keywords: MNIST, Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20190v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20190v1.pdf filename=2403.20190v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The widespread application of machine learning algorithms is a matter of increasing concern for the data privacy research community, and many have sought to develop privacy-preserving techniques for it. Among existing approaches, the homomorphic evaluation of ML algorithms stands out by performing operations directly over encrypted data, enabling strong guarantees of confidentiality. The homomorphic evaluation of inference algorithms is practical even for relatively deep <b>Convolution</b> Neural Networks <b>(CNNs).</b> However, training is still a major challenge, with current solutions often resorting to lightweight algorithms that can be unfit for solving more complex problems, such as image recognition. This work introduces the homomorphic evaluation of Wilkie, Stonham, and Aleksander&rsquo;s Recognition Device (WiSARD) and subsequent Weightless Neural Networks (WNNs) for training and inference on encrypted data. Compared to <b>CNNs,</b> WNNs offer better performance with a relatively small accuracy drop. We develop a complete framework for it, including several building blocks that can be of independent interest. Our framework achieves 91.7% accuracy on the <b>MNIST</b> dataset after only 3.5 minutes of encrypted training (multi-threaded), going up to 93.8% in 3.5 hours. For the HAM10000 dataset, we achieve 67.9% accuracy in just 1.5 minutes, going up to 69.9% after 1 hour. Compared to the state of the art on the HE evaluation of <b>CNN</b> training, Glyph (Lou et al., NeurIPS 2020), these results represent a speedup of up to 1200 times with an accuracy loss of at most 5.4%. For HAM10000, we even achieved a 0.65% accuracy improvement while being 60 times faster than Glyph. We also provide solutions for small-scale encrypted training. In a single thread on a desktop machine using less than 200MB of memory, we train over 1000 <b>MNIST</b> images in 12 minutes or over the entire Wisconsin Breast Cancer dataset in just 11 seconds.</p></p class="citation"></blockquote><h3 id=23--178204-security-risks-concerns-of-generative-ai-in-the-iot-honghui-xu-et-al-2024>(2/3 | 178/204) Security Risks Concerns of Generative AI in the IoT (Honghui Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Honghui Xu, Yingshu Li, Olusesi Balogun, Shaoen Wu, Yue Wang, Zhipeng Cai. (2024)<br><strong>Security Risks Concerns of Generative AI in the IoT</strong><br><button class=copy-to-clipboard title="Security Risks Concerns of Generative AI in the IoT" index=178>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-178 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs.CR<br>Keyword Score: 10<br>Keywords: Generative AI<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00139v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00139v1.pdf filename=2404.00139v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In an era where the Internet of Things (IoT) intersects increasingly with <b>generative</b> <b>Artificial</b> Intelligence (AI), this article scrutinizes the emergent security risks inherent in this integration. We explore how <b>generative</b> <b>AI</b> drives innovation in IoT and we analyze the potential for data breaches when using <b>generative</b> <b>AI</b> and the misuse of <b>generative</b> <b>AI</b> technologies in IoT ecosystems. These risks not only threaten the privacy and efficiency of IoT systems but also pose broader implications for trust and safety in AI-driven environments. The discussion in this article extends to strategic approaches for mitigating these risks, including the development of robust security protocols, the multi-layered security approaches, and the adoption of AI technological solutions. Through a comprehensive analysis, this article aims to shed light on the critical balance between embracing AI advancements and ensuring stringent security in IoT, providing insights into the future direction of these intertwined technologies.</p></p class="citation"></blockquote><h3 id=33--179204-decentralized-multimedia-data-sharing-in-iov-a-learning-based-equilibrium-of-supply-and-demand-jiani-fan-et-al-2024>(3/3 | 179/204) Decentralized Multimedia Data Sharing in IoV: A Learning-based Equilibrium of Supply and Demand (Jiani Fan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiani Fan, Minrui Xu, Jiale Guo, Lwin Khin Shar, Jiawen Kang, Dusit Niyato, Kwok-Yan Lam. (2024)<br><strong>Decentralized Multimedia Data Sharing in IoV: A Learning-based Equilibrium of Supply and Demand</strong><br><button class=copy-to-clipboard title="Decentralized Multimedia Data Sharing in IoV: A Learning-based Equilibrium of Supply and Demand" index=179>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-179 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20218v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20218v1.pdf filename=2403.20218v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Internet of Vehicles (IoV) has great potential to transform transportation systems by enhancing road safety, reducing traffic congestion, and improving user experience through onboard infotainment applications. Decentralized data sharing can improve security, privacy, reliability, and facilitate infotainment data sharing in IoVs. However, decentralized data sharing may not achieve the expected efficiency if there are IoV users who only want to consume the shared data but are not willing to contribute their own data to the community, resulting in incomplete information observed by other vehicles and infrastructure, which can introduce additional transmission latency. Therefore, in this article, by modeling the data sharing ecosystem as a data trading market, we propose a decentralized data-sharing incentive mechanism based on multi-intelligent <b>reinforcement</b> <b>learning</b> to learn the supply-demand balance in markets and minimize transmission latency. Our proposed mechanism takes into account the dynamic nature of IoV markets, which can experience frequent fluctuations in supply and demand. We propose a time-sensitive Key-Policy Attribute-Based Encryption (KP-ABE) mechanism coupled with Named Data Networking (NDN) to protect data in IoVs, which adds a layer of security to our proposed solution. Additionally, we design a decentralized market for efficient data sharing in IoVs, where continuous double auctions are adopted. The proposed mechanism based on multi-agent deep <b>reinforcement</b> <b>learning</b> can learn the supply-demand equilibrium in markets, thus improving the efficiency and sustainability of markets. Theoretical analysis and experimental results show that our proposed learning-based incentive mechanism outperforms baselines by 10% in determining the equilibrium of supply and demand while reducing transmission latency by 20%.</p></p class="citation"></blockquote><h2 id=csne-1>cs.NE (1)</h2><h3 id=11--180204-biologically-plausible-topology-improved-spiking-actor-network-for-efficient-deep-reinforcement-learning-duzhen-zhang-et-al-2024>(1/1 | 180/204) Biologically-Plausible Topology Improved Spiking Actor Network for Efficient Deep Reinforcement Learning (Duzhen Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Duzhen Zhang, Qingyu Wang, Tielin Zhang, Bo Xu. (2024)<br><strong>Biologically-Plausible Topology Improved Spiking Actor Network for Efficient Deep Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Biologically-Plausible Topology Improved Spiking Actor Network for Efficient Deep Reinforcement Learning" index=180>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-180 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-NE, cs.NE, q-bio-NC<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20163v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20163v1.pdf filename=2403.20163v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The success of Deep <b>Reinforcement</b> <b>Learning</b> (DRL) is largely attributed to utilizing Artificial Neural Networks (ANNs) as function approximators. Recent advances in neuroscience have unveiled that the human brain achieves efficient reward-based learning, at least by integrating spiking neurons with spatial-temporal dynamics and network topologies with biologically-plausible connectivity patterns. This integration process allows spiking neurons to efficiently combine information across and within layers via nonlinear dendritic trees and lateral interactions. The fusion of these two topologies enhances the network&rsquo;s information-processing ability, crucial for grasping intricate perceptions and guiding decision-making procedures. However, ANNs and brain networks differ significantly. ANNs lack intricate dynamical neurons and only feature inter-layer connections, typically achieved by direct linear summation, without intra-layer connections. This limitation leads to constrained network expressivity. To address this, we propose a novel alternative for function approximator, the Biologically-Plausible Topology improved Spiking Actor Network (BPT-SAN), tailored for efficient decision-making in DRL. The BPT-SAN incorporates spiking neurons with intricate spatial-temporal dynamics and introduces intra-layer connections, enhancing spatial-temporal state representation and facilitating more precise biological <b>simulations.</b> Diverging from the conventional direct linear weighted sum, the BPT-SAN models the local nonlinearities of dendritic trees within the inter-layer connections. For the intra-layer connections, the BPT-SAN introduces lateral interactions between adjacent neurons, integrating them into the membrane potential formula to ensure accurate spike firing.</p></p class="citation"></blockquote><h2 id=q-biobm-2>q-bio.BM (2)</h2><h3 id=12--181204-molecular-generative-adversarial-network-with-multi-property-optimization-huidong-tang-et-al-2024>(1/2 | 181/204) Molecular Generative Adversarial Network with Multi-Property Optimization (Huidong Tang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Huidong Tang, Chen Li, Sayaka Kamei, Yoshihiro Yamanishi, Yasuhiko Morimoto. (2024)<br><strong>Molecular Generative Adversarial Network with Multi-Property Optimization</strong><br><button class=copy-to-clipboard title="Molecular Generative Adversarial Network with Multi-Property Optimization" index=181>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-181 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.BM<br>Categories: cs-AI, cs-LG, q-bio-BM, q-bio.BM<br>Keyword Score: 30<br>Keywords: Generative Adversarial Network, Generative Adversarial Network, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00081v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00081v1.pdf filename=2404.00081v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep <b>generative</b> <b>models,</b> <b>such</b> as <b>generative</b> <b>adversarial</b> <b>networks</b> <b>(GANs),</b> have been employed for $de~novo$ molecular generation in drug discovery. Most prior studies have utilized <b>reinforcement</b> <b>learning</b> (RL) algorithms, particularly Monte Carlo tree search (MCTS), to handle the discrete nature of molecular representations in <b>GANs.</b> However, due to the inherent instability in training <b>GANs</b> and RL models, along with the high computational cost associated with MCTS sampling, MCTS RL-based <b>GANs</b> struggle to scale to large chemical databases. To tackle these challenges, this study introduces a novel <b>GAN</b> based on actor-critic RL with instant and global rewards, called InstGAN, to generate molecules at the token-level with multi-property optimization. Furthermore, maximized information entropy is leveraged to alleviate the mode collapse. The experimental results demonstrate that InstGAN outperforms other baselines, achieves comparable performance to state-of-the-art models, and efficiently generates molecules with multi-property optimization. The source code will be released upon acceptance of the paper.</p></p class="citation"></blockquote><h3 id=22--182204-fabind-enhancing-molecular-docking-through-improved-pocket-prediction-and-pose-generation-kaiyuan-gao-et-al-2024>(2/2 | 182/204) FABind+: Enhancing Molecular Docking through Improved Pocket Prediction and Pose Generation (Kaiyuan Gao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kaiyuan Gao, Qizhi Pei, Jinhua Zhu, Tao Qin, Kun He, Lijun Wu. (2024)<br><strong>FABind+: Enhancing Molecular Docking through Improved Pocket Prediction and Pose Generation</strong><br><button class=copy-to-clipboard title="FABind+: Enhancing Molecular Docking through Improved Pocket Prediction and Pose Generation" index=182>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-182 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.BM<br>Categories: cs-AI, cs-LG, q-bio-BM, q-bio.BM<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20261v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20261v2.pdf filename=2403.20261v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Molecular docking is a pivotal process in drug discovery. While traditional techniques rely on extensive sampling and <b>simulation</b> governed by physical principles, these methods are often slow and costly. The advent of deep learning-based approaches has shown significant promise, offering increases in both accuracy and efficiency. Building upon the foundational work of FABind, a model designed with a focus on speed and accuracy, we present FABind+, an enhanced iteration that largely boosts the performance of its predecessor. We identify pocket prediction as a critical bottleneck in molecular docking and propose a novel methodology that significantly refines pocket prediction, thereby streamlining the docking process. Furthermore, we introduce modifications to the docking module to enhance its pose generation capabilities. In an effort to bridge the gap with conventional sampling/generative methods, we incorporate a simple yet effective sampling technique coupled with a confidence model, requiring only minor adjustments to the regression framework of FABind. Experimental results and analysis reveal that FABind+ remarkably outperforms the original FABind, achieves competitive state-of-the-art performance, and delivers insightful modeling strategies. This demonstrates FABind+ represents a substantial step forward in molecular docking and drug discovery. Our code is in <a href=https://github.com/QizhiPei/FABind>https://github.com/QizhiPei/FABind</a>.</p></p class="citation"></blockquote><h2 id=csni-4>cs.NI (4)</h2><h3 id=14--183204-fairtt-an-empirical-approach-for-enhanced-rtt-fairness-and-bottleneck-throughput-in-bbr-akshita-abrol-et-al-2024>(1/4 | 183/204) FaiRTT: An Empirical Approach for Enhanced RTT Fairness and Bottleneck Throughput in BBR (Akshita Abrol et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Akshita Abrol, Purnima Murali Mohan, Tram Truong-Huu. (2024)<br><strong>FaiRTT: An Empirical Approach for Enhanced RTT Fairness and Bottleneck Throughput in BBR</strong><br><button class=copy-to-clipboard title="FaiRTT: An Empirical Approach for Enhanced RTT Fairness and Bottleneck Throughput in BBR" index=183>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-183 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI<br>Keyword Score: 30<br>Keywords: Fairness, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19973v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19973v1.pdf filename=2403.19973v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In next-generation networks, achieving Round-trip Time (RTT) <b>fairness</b> is essential for ensuring fair bandwidth distribution among diverse flow types, enhancing overall network utilization. The TCP congestion control algorithm &ndash; BBR, was proposed by Google to dynamically adjust sending rates in response to changing network conditions. While BBRv2 was implemented to overcome the unfairness limitation of BBRv1, it still faces intra-protocol <b>fairness</b> challenges in balancing the demands of high-bandwidth, long-RTT elephant flows and more frequent short-RTT mice flows. These issues lead to throughput imbalances and queue buildup, resulting in elephant flow dominance and mice flow starvation. In this paper, we first investigate the limitations of Google&rsquo;s BBR algorithm, specifically in the context of intra-protocol RTT <b>fairness</b> in beyond 5G (B5G) networks. While existing works address this limitation by adjusting the pacing rate, it eventually leads to low throughput. We hence develop the FaiRTT algorithm to resolve the problem by dynamically estimating the Bandwidth Delay Product (BDP) sending rate based on RTT measurements, focusing on equitable bandwidth allocation. By modeling the Inf light dependency on the BDP, bottleneck bandwidth, and packet departure time after every ACK, we can resolve the intra-protocol <b>fairness</b> while not compromising the throughput on the bottleneck link. Through extensive <b>simulations</b> on NS-3 and comprehensive performance evaluations, FaiRTT is shown to significantly improve the <b>fairness</b> index and network throughput, significantly outperforming BBRv2, for diverse flow types. FaiRTT achieves an average throughput ratio of 1.08 between elephant and mice flows, an average <b>fairness</b> index of 0.98, and an average utilization of the bottleneck link of 98.78%.</p></p class="citation"></blockquote><h3 id=24--184204-a-comprehensive-evaluation-of-the-impact-of-atm-qos-mechanisms-on-network-performance-for-multimedia-and-data-applications-mahdi-manavi-2024>(2/4 | 184/204) A Comprehensive Evaluation of the Impact of ATM QoS Mechanisms on Network Performance for Multimedia and Data Applications (Mahdi Manavi, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mahdi Manavi. (2024)<br><strong>A Comprehensive Evaluation of the Impact of ATM QoS Mechanisms on Network Performance for Multimedia and Data Applications</strong><br><button class=copy-to-clipboard title="A Comprehensive Evaluation of the Impact of ATM QoS Mechanisms on Network Performance for Multimedia and Data Applications" index=184>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-184 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19914v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19914v1.pdf filename=2403.19914v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Asynchronous Transfer Mode (ATM) network is crucial due to its ability to efficiently transmit data, provide reliable connections, and support various service classes with specific Quality of Service (QoS) requirements. In this paper, we utilize the OPNET network <b>simulation</b> software to model an ATM network and analyze the impact of QoS classification on network performance. We investigate the effects of Constant Bit Rate (CBR), Variable Bit Rate (VBR), Available Bit Rate (ABR) and Unspecified Bit Rate (UBR) models on various network traffic types such as voice, video and data. For voice traffic, we examine key QoS parameters including Jitter, Packet Delay Variation and End-to-End Delay. For video traffic, we evaluate Packet Delay Variation and End-to-End Delay. Additionally, we analyze Download Response Time for data traffic to assess the influence of QoS on the ATM network. Our results demonstrate that CBR and VBR are preferred for real-time traffic like voice and video, providing low delay and jitter. The <b>simulation</b> approach enables us to test various configurations and gain insights not possible in hardware tests. Our findings can help network operators determine the optimal QoS settings and tradeoffs when deploying ATM for modern multi-service networks.</p></p class="citation"></blockquote><h3 id=34--185204-neuralunadtnet-feedforward-neural-network-based-routing-protocol-for-delay-tolerant-lunar-communication-networks-parth-patel-et-al-2024>(3/4 | 185/204) NeuraLunaDTNet: Feedforward Neural Network-Based Routing Protocol for Delay-Tolerant Lunar Communication Networks (Parth Patel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Parth Patel, Milena Radenkovic. (2024)<br><strong>NeuraLunaDTNet: Feedforward Neural Network-Based Routing Protocol for Delay-Tolerant Lunar Communication Networks</strong><br><button class=copy-to-clipboard title="NeuraLunaDTNet: Feedforward Neural Network-Based Routing Protocol for Delay-Tolerant Lunar Communication Networks" index=185>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-185 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-AI, cs-NI, cs.NI<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20199v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20199v1.pdf filename=2403.20199v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Space Communication poses challenges such as severe delays, hard-to-predict routes and communication disruptions. The Delay Tolerant Network architecture, having been specifically designed keeping such scenarios in mind, is suitable to address some challenges. The traditional DTN routing protocols fall short of delivering optimal performance, due to the inherent complexities of space communication. Researchers have aimed at using recent advancements in AI to mitigate some routing challenges [9]. We propose utilising a feedforward neural network to develop a novel protocol NeuraLunaDTNet, which enhances the efficiency of the PRoPHET routing protocol for lunar communication, by learning contact plans in dynamically changing spatio-temporal <b>graph.</b></p></p class="citation"></blockquote><h3 id=44--186204-dhnet-a-distributed-network-architecture-for-smart-home-chaoqi-zhou-et-al-2024>(4/4 | 186/204) DHNet: A Distributed Network Architecture for Smart Home (Chaoqi Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chaoqi Zhou, Jingpu Duan, YuPeng Xiao, Qing Li, Dingding Chen, Ruobin Zheng, Shaoteng Liu. (2024)<br><strong>DHNet: A Distributed Network Architecture for Smart Home</strong><br><button class=copy-to-clipboard title="DHNet: A Distributed Network Architecture for Smart Home" index=186>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-186 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI<br>Keyword Score: 3<br>Keywords: Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19931v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19931v1.pdf filename=2403.19931v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the increasing popularity of smart homes, more and more devices need to connect to home networks. Traditional home networks mainly rely on centralized networking, where an excessive number of devices in the centralized topology can increase the pressure on the central router, potentially leading to decreased network performance metrics such as communication latency. To address the latency performance issues brought about by centralized networks, this paper proposes a new network system called DHNet, and designs an algorithm for <b>clustering</b> networking and communication based on vector routing. Communication within clusters in a simulated virtual environment achieves a latency of approximately 0.7 milliseconds. Furthermore, by directly using the first non-&ldquo;lo&rdquo; network card address of a device as the protocol&rsquo;s network layer address, the protocol avoids the several tens of milliseconds of access latency caused by DHCP. The integration of service discovery functionality into the network layer protocol is achieved through a combination of &ldquo;server-initiated service push&rdquo; and &ldquo;client request + server reply&rdquo; methods. Compared to traditional application-layer DNS passive service discovery, the average latency is reduced by over 50%. The PVH protocol is implemented in the user space using the Go programming language, with implementation details drawn from Google&rsquo;s gVisor project. The code has been ported from x86_64 Linux computers to devices such as OpenWrt routers and Android smartphones. The PVH protocol can communicate through &ldquo;tunnels&rdquo; to provide IP compatibility, allowing existing applications based on TCP/IP to communicate using the PVH protocol without requiring modifications to their code.</p></p class="citation"></blockquote><h2 id=mathoc-3>math.OC (3)</h2><h3 id=13--187204-beyond-suspension-a-two-phase-methodology-for-concluding-sports-leagues-ali-hassanzadeh-et-al-2024>(1/3 | 187/204) Beyond Suspension: A Two-phase Methodology for Concluding Sports Leagues (Ali Hassanzadeh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ali Hassanzadeh, Mojtaba Hosseini, John G. Turner. (2024)<br><strong>Beyond Suspension: A Two-phase Methodology for Concluding Sports Leagues</strong><br><button class=copy-to-clipboard title="Beyond Suspension: A Two-phase Methodology for Concluding Sports Leagues" index=187>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-187 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: 90B50 (Primary) 90C06, 90C11, 90C90 (Secondary), cs-AI, cs-LG, math-OC, math.OC<br>Keyword Score: 23<br>Keywords: Benchmarking, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00178v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00178v1.pdf filename=2404.00178v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Problem definition: Professional sports leagues may be suspended due to various reasons such as the recent COVID-19 pandemic. A critical question the league must address when re-opening is how to appropriately select a subset of the remaining games to conclude the season in a shortened time frame. Academic/practical relevance: Despite the rich literature on scheduling an entire season starting from a blank slate, concluding an existing season is quite different. Our approach attempts to achieve team rankings similar to that which would have resulted had the season been played out in full. Methodology: We propose a data-driven model which exploits predictive and prescriptive analytics to produce a schedule for the remainder of the season comprised of a subset of originally-scheduled games. Our model introduces novel rankings-based objectives within a stochastic optimization model, whose parameters are first estimated using a predictive model. We introduce a deterministic equivalent reformulation along with a tailored Frank-Wolfe algorithm to efficiently solve our problem, as well as a robust counterpart based on min-max regret. Results: We present <b>simulation-based</b> numerical experiments from previous National Basketball Association (NBA) seasons 2004&ndash;2019, and show that our models are computationally efficient, outperform a greedy <b>benchmark</b> that approximates a non-rankings-based scheduling policy, and produce interpretable results. Managerial implications: Our data-driven decision-making framework may be used to produce a shortened season with 25-50% fewer games while still producing an end-of-season ranking similar to that of the full season, had it been played.</p></p class="citation"></blockquote><h3 id=23--188204-an-ordinary-differential-equation-for-entropic-optimal-transport-and-its-linearly-constrained-variants-joshua-zoen-git-hiew-et-al-2024>(2/3 | 188/204) An ordinary differential equation for entropic optimal transport and its linearly constrained variants (Joshua Zoen-Git Hiew et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Joshua Zoen-Git Hiew, Luca Nenna, Brendan Pass. (2024)<br><strong>An ordinary differential equation for entropic optimal transport and its linearly constrained variants</strong><br><button class=copy-to-clipboard title="An ordinary differential equation for entropic optimal transport and its linearly constrained variants" index=188>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-188 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: 49Q22 (Primary) 49N15, 94A17, 49K40 (Secondary), cs-NA, math-AP, math-NA, math-OC, math-PR, math.OC<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20238v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20238v1.pdf filename=2403.20238v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We characterize the solution to the entropically regularized optimal transport problem by a well-posed ordinary differential equation (ODE). Our approach works for discrete marginals and general cost functions, and in addition to two marginal problems, applies to multi-marginal problems and those with additional linear constraints. Solving the ODE gives a new numerical method to solve the optimal transport problem, which has the advantage of yielding the solution for all intermediate values of the ODE parameter (which is equivalent to the usual regularization parameter). We illustrate this method with several numerical <b>simulations.</b> The formulation of the ODE also allows one to compute derivatives of the optimal cost when the ODE parameter is $0$, corresponding to the fully regularized limit problem in which only the entropy is minimized.</p></p class="citation"></blockquote><h3 id=33--189204-invertibility-of-discrete-time-linear-systems-with-sparse-inputs-kyle-poe-et-al-2024>(3/3 | 189/204) Invertibility of Discrete-Time Linear Systems with Sparse Inputs (Kyle Poe et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kyle Poe, Enrique Mallada, Rene Vidal. (2024)<br><strong>Invertibility of Discrete-Time Linear Systems with Sparse Inputs</strong><br><button class=copy-to-clipboard title="Invertibility of Discrete-Time Linear Systems with Sparse Inputs" index=189>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-189 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-SY, eess-SP, eess-SY, math-OC, math.OC<br>Keyword Score: 15<br>Keywords: Discrete Time, Discrete Time, Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20294v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20294v1.pdf filename=2403.20294v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>One of the fundamental problems of interest for <b>discrete-time</b> <b>linear</b> systems is whether its input sequence may be recovered given its output sequence, a.k.a. the left inversion problem. Many conditions on the state space <b>geometry,</b> dynamics, and spectral structure of a system have been used to characterize the well-posedness of this problem, without assumptions on the inputs. However, certain structural assumptions, such as input sparsity, have been shown to translate to practical gains in the performance of inversion algorithms, surpassing classical guarantees. Establishing necessary and sufficient conditions for left invertibility of systems with sparse inputs is therefore a crucial step toward understanding the performance limits of system inversion under structured input assumptions. In this work, we provide the first necessary and sufficient characterizations of left invertibility for linear systems with sparse inputs, echoing classic characterizations for standard linear systems. The key insight in deriving these results is in establishing the existence of two novel geometric invariants unique to the sparse-input setting, the weakly unobservable and strongly reachable subspace arrangements. By means of a concrete example, we demonstrate the utility of these characterizations. We conclude by discussing extensions and applications of this framework to several related problems in sparse control.</p></p class="citation"></blockquote><h2 id=csce-1>cs.CE (1)</h2><h3 id=11--190204-parallel-performance-of-shared-memory-parallel-spectral-deferred-corrections-philip-freese-et-al-2024>(1/1 | 190/204) Parallel performance of shared memory parallel spectral deferred corrections (Philip Freese et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Philip Freese, Sebastian Götschel, Thibaut Lunet, Daniel Ruprecht, Martin Schreiber. (2024)<br><strong>Parallel performance of shared memory parallel spectral deferred corrections</strong><br><button class=copy-to-clipboard title="Parallel performance of shared memory parallel spectral deferred corrections" index=190>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-190 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CE<br>Categories: 65Y05, 65M08, 65M70, cs-CE, cs-DC, cs-NA, cs.CE, math-NA<br>Keyword Score: 23<br>Keywords: Benchmarking, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20135v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20135v1.pdf filename=2403.20135v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We investigate parallel performance of parallel spectral deferred corrections, a numerical approach that provides small-scale parallelism for the numerical solution of initial value problems. The scheme is applied to the shallow water equation and uses an IMEX splitting that integrates fast modes implicitly and slow modes explicitly in order to be efficient. We describe parallel $\texttt{OpenMP}$-based implementations of parallel SDC in two well established <b>simulation</b> codes: the finite volume based operational ocean model $\texttt{ICON-O}$ and the spherical harmonics based research code $\texttt{SWEET}$. The implementations are <b>benchmarked</b> on a single node of the JUSUF ($\texttt{SWEET}$) and JUWELS ($\texttt{ICON-O}$) system at J"ulich Supercomputing Centre. We demonstrate a reduction of time-to-solution across a range of accuracies. For $\texttt{ICON-O}$, we show speedup over the currently used Adams&ndash;Bashforth-2 integrator with $\texttt{OpenMP}$ loop parallelization. For $\texttt{SWEET}$, we show speedup over serial spectral deferred corrections and a second order implicit-explicit integrator.</p></p class="citation"></blockquote><h2 id=eesssy-2>eess.SY (2)</h2><h3 id=12--191204-keeping-up-with-the-winner-targeted-advertisement-to-communities-in-social-networks-shailaja-mallick-et-al-2024>(1/2 | 191/204) Keeping Up With the Winner! Targeted Advertisement to Communities in Social Networks (Shailaja Mallick et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shailaja Mallick, Vishwaraj Doshi, Do Young Eun. (2024)<br><strong>Keeping Up With the Winner! Targeted Advertisement to Communities in Social Networks</strong><br><button class=copy-to-clipboard title="Keeping Up With the Winner! Targeted Advertisement to Communities in Social Networks" index=191>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-191 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SI, cs-SY, eess-SY, eess.SY<br>Keyword Score: 23<br>Keywords: Graph, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19903v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19903v1.pdf filename=2403.19903v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>When a new product enters a market already dominated by an existing product, will it survive along with this dominant product? Most of the existing works have shown the coexistence of two competing products spreading/being adopted on overlaid <b>graphs</b> with same set of users. However, when it comes to the survival of a weaker product on the same <b>graph,</b> it has been established that the stronger one dominates the market and wipes out the other. This paper makes a step towards narrowing this gap so that a new/weaker product can also survive along with its competitor with a positive market share. Specifically, we identify a locally optimal set of users to induce a community that is targeted with advertisement by the product launching company under a given budget constraint. To this end, we model the system as competing Susceptible-Infected-Susceptible (SIS) epidemics and employ perturbation techniques to quantify and attain a positive market share in a cost-efficient manner. Our extensive <b>simulation</b> results with real-world <b>graph</b> dataset show that with our choice of target users, a new product can establish itself with positive market share, which otherwise would be dominated and eventually wiped out of the competitive market under the same budget constraint.</p></p class="citation"></blockquote><h3 id=22--192204-low-cost-adaptive-obstacle-avoidance-trajectory-control-for-express-delivery-drone-yanhui-zhang-et-al-2024>(2/2 | 192/204) Low-cost adaptive obstacle avoidance trajectory control for express delivery drone (Yanhui Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yanhui Zhang, Caisheng Wei, Yifan Zhang, Congcong Tian, Weifang Chen. (2024)<br><strong>Low-cost adaptive obstacle avoidance trajectory control for express delivery drone</strong><br><button class=copy-to-clipboard title="Low-cost adaptive obstacle avoidance trajectory control for express delivery drone" index=192>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-192 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19956v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19956v1.pdf filename=2403.19956v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper studies quadcopters obstacle avoidance trajectory control (OATC) problem for express delivery. A new nonlinear adaptive learning controller that is low-cost and portable to different wheelbase sizes is proposed to adapt to large-angle maneuvers and load changes in UAV delivery missions. The controller consists of a nonlinear variable gain (NLVG) function and an extreme value search (ES) algorithm to reduce overshoot and settling time. Finally, <b>simulations</b> were conducted on a quadcopter to verify the effectiveness of the proposed control scheme under two typical collision-free trajectories.</p></p class="citation"></blockquote><h2 id=csdc-1>cs.DC (1)</h2><h3 id=11--193204-experiências-resultados-e-reflexões-a-partir-do-gerenciamento-de-experimentos-no-mundo-real-com-fanets-e-vants----versão-estendida-bruno-josé-olivieri-de-souza-et-al-2024>(1/1 | 193/204) Experiências, Resultados e Reflexões a partir do Gerenciamento de experimentos no Mundo Real com FANETs e VANTs &ndash; Versão Estendida (Bruno José Olivieri de Souza et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bruno José Olivieri de Souza, markus Endler. (2024)<br><strong>Experiências, Resultados e Reflexões a partir do Gerenciamento de experimentos no Mundo Real com FANETs e VANTs &ndash; Versão Estendida</strong><br><button class=copy-to-clipboard title="Experiências, Resultados e Reflexões a partir do Gerenciamento de experimentos no Mundo Real com FANETs e VANTs -- Versão Estendida" index=193>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-193 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs.DC<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00113v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00113v1.pdf filename=2404.00113v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the research on FANETs (Flying Ad-Hoc Networks) and distributed coordination of UAVs (Unmanned Aerial Vehicles), also known as drones, there are many studies that validate their proposals through <b>simulations.</b> <b>Simulations</b> are important, but beyond them, there is also a need for real-world tests to validate the proposals and enhance results. However, field experiments involving drones and FANETs are not trivial, and this work aims to share experiences and results obtained during the construction of a testbed actively used in comparing <b>simulations</b> and field tests.</p></p class="citation"></blockquote><h2 id=mathna-2>math.NA (2)</h2><h3 id=12--194204-dual-simplex-volume-maximization-for-simplex-structured-matrix-factorization-maryam-abdolali-et-al-2024>(1/2 | 194/204) Dual Simplex Volume Maximization for Simplex-Structured Matrix Factorization (Maryam Abdolali et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Maryam Abdolali, Giovanni Barbarino, Nicolas Gillis. (2024)<br><strong>Dual Simplex Volume Maximization for Simplex-Structured Matrix Factorization</strong><br><button class=copy-to-clipboard title="Dual Simplex Volume Maximization for Simplex-Structured Matrix Factorization" index=194>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-194 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-IR, cs-LG, cs-NA, eess-SP, math-NA, math.NA, stat-ML<br>Keyword Score: 20<br>Keywords: Topic Model, Topic Modeling<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20197v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20197v1.pdf filename=2403.20197v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Simplex-structured matrix factorization (SSMF) is a generalization of nonnegative matrix factorization, a fundamental interpretable data analysis model, and has applications in hyperspectral unmixing and <b>topic</b> <b>modeling.</b> To obtain identifiable solutions, a standard approach is to find minimum-volume solutions. By taking advantage of the duality/polarity concept for polytopes, we convert minimum-volume SSMF in the primal space to a maximum-volume problem in the dual space. We first prove the identifiability of this maximum-volume dual problem. Then, we use this dual formulation to provide a novel optimization approach which bridges the gap between two existing families of algorithms for SSMF, namely volume minimization and facet identification. Numerical experiments show that the proposed approach performs favorably compared to the state-of-the-art SSMF algorithms.</p></p class="citation"></blockquote><h3 id=22--195204-sampling-error-mitigation-through-spectrum-smoothing-in-ensemble-data-assimilation-bosu-choi-et-al-2024>(2/2 | 195/204) Sampling error mitigation through spectrum smoothing in ensemble data assimilation (Bosu Choi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bosu Choi, Yoonsang Lee. (2024)<br><strong>Sampling error mitigation through spectrum smoothing in ensemble data assimilation</strong><br><button class=copy-to-clipboard title="Sampling error mitigation through spectrum smoothing in ensemble data assimilation" index=195>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-195 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA<br>Keyword Score: 10<br>Keywords: Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.00154v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.00154v1.pdf filename=2404.00154v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In data assimilation, an ensemble provides a nonintrusive way to evolve a probability density described by a nonlinear prediction model. Although a large ensemble size is required for statistical accuracy, the ensemble size is typically limited to a small number due to the computational cost of running the prediction model, which leads to a sampling error. Several methods, such as localization, exist to mitigate the sampling error, often requiring problem-dependent <b>fine-tuning</b> and design. This work introduces another sampling error mitigation method using a smoothness constraint in the Fourier space. In particular, this work smoothes out the spectrum of the system to increase the stability and accuracy even under a small ensemble size. The efficacy of the new idea is validated through a suite of stringent test problems, including Lorenz 96 and Kuramoto-Sivashinsky turbulence models.</p></p class="citation"></blockquote><h2 id=statco-1>stat.CO (1)</h2><h3 id=11--196204-investigating-the-combinatorial-potential-and-applicability-of-random-equation-systems-with-mixture-models-in-a-bayesian-framework-wolfgang-hoegele-2024>(1/1 | 196/204) Investigating the Combinatorial Potential and Applicability of Random Equation Systems with Mixture Models in a Bayesian Framework (Wolfgang Hoegele, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wolfgang Hoegele. (2024)<br><strong>Investigating the Combinatorial Potential and Applicability of Random Equation Systems with Mixture Models in a Bayesian Framework</strong><br><button class=copy-to-clipboard title="Investigating the Combinatorial Potential and Applicability of Random Equation Systems with Mixture Models in a Bayesian Framework" index=196>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-196 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.CO<br>Categories: cs-NA, math-NA, stat-CO, stat.CO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20152v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20152v1.pdf filename=2403.20152v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Investigating solutions of nonlinear equation systems is challenging in a general framework, especially if the equations contain uncertainties about parameters modeled by probability densities. Such random equations, understood as stationary (non-dynamical) equations with parameters as random variables, have a long history and a broad range of applications. In this work, we study nonlinear random equations by combining them with mixture model parameter random variables in order to investigate the combinatorial complexity of such equations and how this can be utilized practically. We derive a general likelihood function and posterior density of approximate best fit solutions while avoiding significant restrictions about the type of nonlinearity or mixture models, and demonstrate their numerically efficient application for the applied researcher. In the results section we are specifically focusing on example <b>simulations</b> of approximate likelihood/posterior solutions for random linear equation systems, nonlinear systems of random conic section equations, as well as applications to portfolio optimization, stochastic control and random matrix theory in order to show the wide applicability of the presented methodology.</p></p class="citation"></blockquote><h2 id=csds-2>cs.DS (2)</h2><h3 id=12--197204-shaving-logs-via-large-sieve-inequality-faster-algorithms-for-sparse-convolution-and-more-ce-jin-et-al-2024>(1/2 | 197/204) Shaving Logs via Large Sieve Inequality: Faster Algorithms for Sparse Convolution and More (Ce Jin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ce Jin, Yinzhan Xu. (2024)<br><strong>Shaving Logs via Large Sieve Inequality: Faster Algorithms for Sparse Convolution and More</strong><br><button class=copy-to-clipboard title="Shaving Logs via Large Sieve Inequality: Faster Algorithms for Sparse Convolution and More" index=197>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-197 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs.DS<br>Keyword Score: 10<br>Keywords: Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20326v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20326v1.pdf filename=2403.20326v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In sparse <b>convolution-type</b> problems, a common technique is to hash the input integers modulo a random prime $p\in [Q/2,Q]$ for some parameter $Q$, which reduces the range of the input integers while preserving their additive structure. However, this hash family suffers from two drawbacks, which led to bottlenecks in many state-of-the-art algorithms: (1) The collision probability of two elements from $[N]$ is $O(\frac{\log N}{Q})$ rather than $O(\frac{1}{Q})$; (2) It is difficult to derandomize the choice of $p$; known derandomization techniques lead to super-logarithmic overhead [Chan, Lewenstein STOC'15]. In this paper, we partially overcome these drawbacks in certain scenarios, via novel applications of the large sieve inequality from analytic number theory. Consequently, we obtain the following improved algorithms for various problems (in the standard word RAM model): Sparse Nonnegative <b>Convolution:</b> We obtain an $O(t\log t)$-time Las Vegas algorithm that computes the <b>convolution</b> $A\star B$ of two nonnegative integer vectors $A,B$, where $t$ is the output sparsity $|A\star B|_0$. Moreover, our algorithm terminates in $O(t\log t)$ time with $1-1/\mathrm{poly}(t)$ probability. Text-to-Pattern Hamming Distances: Given a length-$m$ pattern $P$ and a length-$n$ text $T$, we obtain a deterministic $O(n\sqrt{m\log \log m})$-time algorithm that exactly computes the Hamming distance between $P$ and every length-$m$ substring of $T$. Sparse General <b>Convolution:</b> We also give a Monte Carlo $O(t\log t)$ time algorithm for sparse <b>convolution</b> with possibly negative input in the restricted case where the length $N$ of the input vectors satisfies $N\le t^{1.99}$.</p></p class="citation"></blockquote><h3 id=22--198204-optimal-communication-for-classic-functions-in-the-coordinator-model-and-beyond-hossein-esfandiari-et-al-2024>(2/2 | 198/204) Optimal Communication for Classic Functions in the Coordinator Model and Beyond (Hossein Esfandiari et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hossein Esfandiari, Praneeth Kacham, Vahab Mirrokni, David P. Woodruff, Peilin Zhong. (2024)<br><strong>Optimal Communication for Classic Functions in the Coordinator Model and Beyond</strong><br><button class=copy-to-clipboard title="Optimal Communication for Classic Functions in the Coordinator Model and Beyond" index=198>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-198 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs.DS<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20307v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20307v1.pdf filename=2403.20307v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the coordinator model of communication with $s$ servers, given an arbitrary non-negative function $f$, we study the problem of approximating the sum $\sum_{i \in [n]}f(x_i)$ up to a $1 \pm \varepsilon$ factor. Here the vector $x \in R^n$ is defined to be $x = x(1) + \cdots + x(s)$, where $x(j) \ge 0$ denotes the non-negative vector held by the $j$-th server. A special case of the problem is when $f(x) = x^k$ which corresponds to the well-studied problem of $F_k$ moment estimation in the distributed communication model. We introduce a new parameter $c_f[s]$ which captures the communication complexity of approximating $\sum_{i\in [n]} f(x_i)$ and for a broad class of functions $f$ which includes $f(x) = x^k$ for $k \ge 2$ and other robust functions such as the Huber loss function, we give a two round protocol that uses total communication $c_f[s]/\varepsilon^2$ bits, up to polylogarithmic factors. For this broad class of functions, our result improves upon the communication bounds achieved by Kannan, Vempala, and Woodruff (COLT 2014) and Woodruff and Zhang (STOC 2012), obtaining the optimal communication up to polylogarithmic factors in the minimum number of rounds. We show that our protocol can also be used for approximating higher-order correlations. Apart from the coordinator model, algorithms for other <b>graph</b> topologies in which each node is a server have been extensively studied. We argue that directly lifting protocols leads to inefficient algorithms. Hence, a natural question is the problems that can be efficiently solved in general <b>graph</b> topologies. We give communication efficient protocols in the so-called personalized CONGEST model for solving linear regression and low rank approximation by designing composable sketches. Our sketch construction may be of independent interest and can implement any importance sampling procedure that has a monotonicity property.</p></p class="citation"></blockquote><h2 id=statml-1>stat.ML (1)</h2><h3 id=11--199204-functional-bilevel-optimization-for-machine-learning-ieva-petrulionyte-et-al-2024>(1/1 | 199/204) Functional Bilevel Optimization for Machine Learning (Ieva Petrulionyte et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ieva Petrulionyte, Julien Mairal, Michael Arbel. (2024)<br><strong>Functional Bilevel Optimization for Machine Learning</strong><br><button class=copy-to-clipboard title="Functional Bilevel Optimization for Machine Learning" index=199>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-199 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20233v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20233v1.pdf filename=2403.20233v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we introduce a new functional point of view on bilevel optimization problems for machine learning, where the inner objective is minimized over a function space. These types of problems are most often solved by using methods developed in the parametric setting, where the inner objective is strongly convex with respect to the parameters of the prediction function. The functional point of view does not rely on this assumption and notably allows using over-parameterized neural networks as the inner prediction function. We propose scalable and efficient algorithms for the functional bilevel optimization problem and illustrate the benefits of our approach on instrumental regression and <b>reinforcement</b> <b>learning</b> tasks, which admit natural functional bilevel structures.</p></p class="citation"></blockquote><h2 id=cssd-1>cs.SD (1)</h2><h3 id=11--200204-voice-signal-processing-for-machine-learning-the-case-of-speaker-isolation-radan-ganchev-2024>(1/1 | 200/204) Voice Signal Processing for Machine Learning. The Case of Speaker Isolation (Radan Ganchev, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Radan Ganchev. (2024)<br><strong>Voice Signal Processing for Machine Learning. The Case of Speaker Isolation</strong><br><button class=copy-to-clipboard title="Voice Signal Processing for Machine Learning. The Case of Speaker Isolation" index=200>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-200 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-LG, cs-SD, cs.SD, eess-AS<br>Keyword Score: 10<br>Keywords: Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20202v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20202v1.pdf filename=2403.20202v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The widespread use of automated voice assistants along with other recent technological developments have increased the demand for applications that process audio signals and human voice in particular. Voice recognition tasks are typically performed using artificial intelligence and machine learning models. Even though end-to-end models exist, properly pre-processing the signal can greatly reduce the complexity of the task and allow it to be solved with a simpler ML model and fewer computational resources. However, ML engineers who work on such tasks might not have a background in signal processing which is an entirely different area of expertise. The objective of this work is to provide a concise comparative analysis of Fourier and Wavelet transforms that are most commonly used as signal decomposition methods for audio processing tasks. Metrics for evaluating speech intelligibility are also discussed, namely Scale-Invariant Signal-to-Distortion Ratio (SI-SDR), Perceptual Evaluation of Speech Quality (PESQ), and Short-Time Objective Intelligibility (STOI). The level of detail in the exposition is meant to be sufficient for an ML engineer to make informed decisions when choosing, <b>fine-tuning,</b> and evaluating a decomposition method for a specific ML model. The exposition contains mathematical definitions of the relevant concepts accompanied with intuitive non-mathematical explanations in order to make the text more accessible to engineers without deep expertise in signal processing. Formal mathematical definitions and proofs of theorems are intentionally omitted in order to keep the text concise.</p></p class="citation"></blockquote><h2 id=eessas-1>eess.AS (1)</h2><h3 id=11--201204-exploring-pathological-speech-quality-assessment-with-asr-powered-wav2vec2-in-data-scarce-context-tuan-nguyen-et-al-2024>(1/1 | 201/204) Exploring Pathological Speech Quality Assessment with ASR-Powered Wav2Vec2 in Data-Scarce Context (Tuan Nguyen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tuan Nguyen, Corinne Fredouille, Alain Ghio, Mathieu Balaguer, Virginie Woisard. (2024)<br><strong>Exploring Pathological Speech Quality Assessment with ASR-Powered Wav2Vec2 in Data-Scarce Context</strong><br><button class=copy-to-clipboard title="Exploring Pathological Speech Quality Assessment with ASR-Powered Wav2Vec2 in Data-Scarce Context" index=201>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-201 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-CL, cs-LG, cs-SD, eess-AS, eess.AS<br>Keyword Score: 10<br>Keywords: Automatic Speech Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20184v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20184v1.pdf filename=2403.20184v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Automatic speech quality assessment has raised more attention as an alternative or support to traditional perceptual clinical evaluation. However, most research so far only gains good results on simple tasks such as binary classification, largely due to data scarcity. To deal with this challenge, current works tend to segment patients&rsquo; audio files into many samples to augment the datasets. Nevertheless, this approach has limitations, as it indirectly relates overall audio scores to individual segments. This paper introduces a novel approach where the system learns at the audio level instead of segments despite data scarcity. This paper proposes to use the pre-trained Wav2Vec2 architecture for both SSL, and <b>ASR</b> as feature extractor in speech assessment. Carried out on the HNC dataset, our <b>ASR-driven</b> approach established a new baseline compared with other approaches, obtaining average $MSE=0.73$ and $MSE=1.15$ for the prediction of intelligibility and severity scores respectively, using only 95 training samples. It shows that the <b>ASR</b> based Wav2Vec2 model brings the best results and may indicate a strong correlation between <b>ASR</b> and speech quality assessment. We also measure its ability on variable segment durations and speech content, exploring factors influencing its decision.</p></p class="citation"></blockquote><h2 id=eesssp-1>eess.SP (1)</h2><h3 id=11--202204-nonparametric-bellman-mappings-for-reinforcement-learning-application-to-robust-adaptive-filtering-yuki-akiyama-et-al-2024>(1/1 | 202/204) Nonparametric Bellman Mappings for Reinforcement Learning: Application to Robust Adaptive Filtering (Yuki Akiyama et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuki Akiyama, Minh Vu, Konstantinos Slavakis. (2024)<br><strong>Nonparametric Bellman Mappings for Reinforcement Learning: Application to Robust Adaptive Filtering</strong><br><button class=copy-to-clipboard title="Nonparametric Bellman Mappings for Reinforcement Learning: Application to Robust Adaptive Filtering" index=202>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-202 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SP<br>Categories: cs-LG, eess-SP, eess.SP<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20020v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20020v1.pdf filename=2403.20020v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper designs novel nonparametric Bellman mappings in reproducing kernel Hilbert spaces (RKHSs) for <b>reinforcement</b> <b>learning</b> (RL). The proposed mappings benefit from the rich approximating properties of RKHSs, adopt no assumptions on the statistics of the data owing to their nonparametric nature, require no knowledge on transition probabilities of Markov decision processes, and may operate without any training data. Moreover, they allow for sampling on-the-fly via the design of trajectory samples, re-use past test data via experience replay, effect dimensionality reduction by random Fourier features, and enable computationally lightweight operations to fit into efficient online or time-adaptive learning. The paper offers also a variational framework to design the free parameters of the proposed Bellman mappings, and shows that appropriate choices of those parameters yield several popular Bellman-mapping designs. As an application, the proposed mappings are employed to offer a novel solution to the problem of countering outliers in adaptive filtering. More specifically, with no prior information on the statistics of the outliers and no training data, a policy-iteration algorithm is introduced to select online, per time instance, the ``optimal&rsquo;&rsquo; coefficient p in the least-mean-p-power-error method. Numerical tests on synthetic data showcase, in most of the cases, the superior performance of the proposed solution over several RL and non-RL schemes.</p></p class="citation"></blockquote><h2 id=csse-1>cs.SE (1)</h2><h3 id=11--203204-towards-a-fault-injection-benchmarking-suite-tianhao-wang-et-al-2024>(1/1 | 203/204) Towards a Fault-Injection Benchmarking Suite (Tianhao Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianhao Wang, Robin Thunig, Horst Schirmeier. (2024)<br><strong>Towards a Fault-Injection Benchmarking Suite</strong><br><button class=copy-to-clipboard title="Towards a Fault-Injection Benchmarking Suite" index=203>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-203 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 6<br>Keywords: Benchmarking, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20319v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20319v1.pdf filename=2403.20319v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Soft errors in memories and logic circuits are known to disturb program execution. In this context, the research community has been proposing a plethora of fault-tolerance (FT) solutions over the last decades, as well as fault-injection (FI) approaches to test, measure and compare them. However, there is no agreed-upon <b>benchmarking</b> suite for demonstrating FT or FI approaches. As a replacement, authors pick <b>benchmarks</b> from other domains, e.g. embedded systems. This leads to little comparability across publications, and causes behavioral overlap within <b>benchmarks</b> that were not selected for orthogonality in the FT/FI domain. In this paper, we want to initiate a discussion on what a <b>benchmarking</b> suite for the FT/FI domain should look like, and propose criteria for <b>benchmark</b> selection.</p></p class="citation"></blockquote><h2 id=math-ph-1>math-ph (1)</h2><h3 id=11--204204-designing-poisson-integrators-through-machine-learning-miguel-vaquero-et-al-2024>(1/1 | 204/204) Designing Poisson Integrators Through Machine Learning (Miguel Vaquero et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Miguel Vaquero, David Martín de Diego, Jorge Cortés. (2024)<br><strong>Designing Poisson Integrators Through Machine Learning</strong><br><button class=copy-to-clipboard title="Designing Poisson Integrators Through Machine Learning" index=204>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-204 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math-ph<br>Categories: 37J06, 70H15, 70H20, 70G45, 65L05, 68T07, G-1-8; J-2, cs-LG, cs-NA, math-DG, math-DS, math-MP, math-NA, math-ph, math-ph<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.20139v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.20139v1.pdf filename=2403.20139v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a general method to construct Poisson integrators, i.e., integrators that preserve the underlying Poisson <b>geometry.</b> We assume the Poisson manifold is integrable, meaning there is a known local symplectic groupoid for which the Poisson manifold serves as the set of units. Our constructions build upon the correspondence between Poisson diffeomorphisms and Lagrangian bisections, which allows us to reformulate the design of Poisson integrators as solutions to a certain PDE (Hamilton-Jacobi). The main novelty of this work is to understand the Hamilton-Jacobi PDE as an optimization problem, whose solution can be easily approximated using machine learning related techniques. This research direction aligns with the current trend in the PDE and machine learning communities, as initiated by Physics- Informed Neural Networks, advocating for designs that combine both physical modeling (the Hamilton-Jacobi PDE) and data.</p></p class="citation"></blockquote></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/akitenkrad-blog/posts/arxiv/202403/20240330000000/ title="arXiv @ 2024.03.30" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Prev</div><div class=next-prev-text>arXiv @ 2024.03.30</div></a></div><div class="col-md-6 next-article"><a href=/akitenkrad-blog/posts/arxiv/202404/20240401000000/ title="arXiv @ 2024.04.01" class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>arXiv @ 2024.04.01</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#primary-categories>Primary Categories</a></li><li><a href=#keywords>Keywords</a></li><li><a href=#cscl-42>cs.CL (42)</a><ul><li><a href=#142--1204-chatgpt-vs-media-bias-a-comparative-study-of-gpt-35-and-fine-tuned-language-models-zehao-wen-et-al-2024>(1/42 | 1/204) ChatGPT v.s. Media Bias: A Comparative Study of GPT-3.5 and Fine-tuned Language Models (Zehao Wen et al., 2024)</a></li><li><a href=#242--2204-enhancing-the-general-agent-capabilities-of-low-parameter-llms-through-tuning-and-multi-branch-reasoning-qinhao-zhou-et-al-2024>(2/42 | 2/204) Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning (Qinhao Zhou et al., 2024)</a></li><li><a href=#342--3204-dataagent-evaluating-large-language-models-ability-to-answer-zero-shot-natural-language-queries-manit-mishra-et-al-2024>(3/42 | 3/204) DataAgent: Evaluating Large Language Models&rsquo; Ability to Answer Zero-Shot, Natural Language Queries (Manit Mishra et al., 2024)</a></li><li><a href=#442--4204-latxa-an-open-language-model-and-evaluation-suite-for-basque-julen-etxaniz-et-al-2024>(4/42 | 4/204) Latxa: An Open Language Model and Evaluation Suite for Basque (Julen Etxaniz et al., 2024)</a></li><li><a href=#542--5204-on-the-fly-definition-augmentation-of-llms-for-biomedical-ner-monica-munnangi-et-al-2024>(5/42 | 5/204) On-the-fly Definition Augmentation of LLMs for Biomedical NER (Monica Munnangi et al., 2024)</a></li><li><a href=#642--6204-are-llms-effective-backbones-for-fine-tuning-an-experimental-investigation-of-supervised-llms-on-chinese-short-text-matching-shulin-liu-et-al-2024>(6/42 | 6/204) Are LLMs Effective Backbones for Fine-tuning? An Experimental Investigation of Supervised LLMs on Chinese Short Text Matching (Shulin Liu et al., 2024)</a></li><li><a href=#742--7204-towards-a-robust-retrieval-based-summarization-system-shengjie-liu-et-al-2024>(7/42 | 7/204) Towards a Robust Retrieval-Based Summarization System (Shengjie Liu et al., 2024)</a></li><li><a href=#842--8204-elitr-bench-a-meeting-assistant-benchmark-for-long-context-language-models-thibaut-thonet-et-al-2024>(8/42 | 8/204) ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models (Thibaut Thonet et al., 2024)</a></li><li><a href=#942--9204-luq-long-text-uncertainty-quantification-for-llms-caiqi-zhang-et-al-2024>(9/42 | 9/204) LUQ: Long-text Uncertainty Quantification for LLMs (Caiqi Zhang et al., 2024)</a></li><li><a href=#1042--10204-fine-tuning-large-language-models-for-automated-diagnostic-screening-summaries-manjeet-yadav-et-al-2024>(10/42 | 10/204) Fine-tuning Large Language Models for Automated Diagnostic Screening Summaries (Manjeet Yadav et al., 2024)</a></li><li><a href=#1142--11204-transformer-lite-high-efficiency-deployment-of-large-language-models-on-mobile-phone-gpus-luchang-li-et-al-2024>(11/42 | 11/204) Transformer-Lite: High-efficiency Deployment of Large Language Models on Mobile Phone GPUs (Luchang Li et al., 2024)</a></li><li><a href=#1242--12204-realm-reference-resolution-as-language-modeling-joel-ruben-antony-moniz-et-al-2024>(12/42 | 12/204) ReALM: Reference Resolution As Language Modeling (Joel Ruben Antony Moniz et al., 2024)</a></li><li><a href=#1342--13204-gecko-versatile-text-embeddings-distilled-from-large-language-models-jinhyuk-lee-et-al-2024>(13/42 | 13/204) Gecko: Versatile Text Embeddings Distilled from Large Language Models (Jinhyuk Lee et al., 2024)</a></li><li><a href=#1442--14204-layernorm-a-key-component-in-parameter-efficient-fine-tuning-taha-valizadehaslani-et-al-2024>(14/42 | 14/204) LayerNorm: A key component in parameter-efficient fine-tuning (Taha ValizadehAslani et al., 2024)</a></li><li><a href=#1542--15204-can-llms-learn-from-previous-mistakes-investigating-llms-errors-to-boost-for-reasoning-yongqi-tong-et-al-2024>(15/42 | 15/204) Can LLMs Learn from Previous Mistakes? Investigating LLMs&rsquo; Errors to Boost for Reasoning (Yongqi Tong et al., 2024)</a></li><li><a href=#1642--16204-mango-a-benchmark-for-evaluating-mapping-and-navigation-abilities-of-large-language-models-peng-ding-et-al-2024>(16/42 | 16/204) MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models (Peng Ding et al., 2024)</a></li><li><a href=#1742--17204-wait-its-all-token-noise-always-has-been-interpreting-llm-behavior-using-shapley-value-behnam-mohammadi-2024>(17/42 | 17/204) Wait, It&rsquo;s All Token Noise? Always Has Been: Interpreting LLM Behavior Using Shapley Value (Behnam Mohammadi, 2024)</a></li><li><a href=#1842--18204-classifying-conspiratorial-narratives-at-scale-false-alarms-and-erroneous-connections-ahmad-diab-et-al-2024>(18/42 | 18/204) Classifying Conspiratorial Narratives At Scale: False Alarms and Erroneous Connections (Ahmad Diab et al., 2024)</a></li><li><a href=#1942--19204-large-language-model-based-situational-dialogues-for-second-language-learning-shuyao-xu-et-al-2024>(19/42 | 19/204) Large Language Model based Situational Dialogues for Second Language Learning (Shuyao Xu et al., 2024)</a></li><li><a href=#2042--20204-indibias-a-benchmark-dataset-to-measure-social-biases-in-language-models-for-indian-context-nihar-ranjan-sahoo-et-al-2024>(20/42 | 20/204) IndiBias: A Benchmark Dataset to Measure Social Biases in Language Models for Indian Context (Nihar Ranjan Sahoo et al., 2024)</a></li><li><a href=#2142--21204-gpta-generative-prompt-tuning-assistant-for-synergistic-downstream-neural-network-enhancement-with-llms-xiao-liu-et-al-2024>(21/42 | 21/204) GPTA: Generative Prompt Tuning Assistant for Synergistic Downstream Neural Network Enhancement with LLMs (Xiao Liu et al., 2024)</a></li><li><a href=#2242--22204-measuring-taiwanese-mandarin-language-understanding-po-heng-chen-et-al-2024>(22/42 | 22/204) Measuring Taiwanese Mandarin Language Understanding (Po-Heng Chen et al., 2024)</a></li><li><a href=#2342--23204-can-llms-correct-physicians-yet-investigating-effective-interaction-methods-in-the-medical-domain-burcu-sayin-et-al-2024>(23/42 | 23/204) Can LLMs Correct Physicians, Yet? Investigating Effective Interaction Methods in the Medical Domain (Burcu Sayin et al., 2024)</a></li><li><a href=#2442--24204-entertainment-chatbot-for-the-digital-inclusion-of-elderly-people-without-abstraction-capabilities-silvia-garcía-méndez-et-al-2024>(24/42 | 24/204) Entertainment chatbot for the digital inclusion of elderly people without abstraction capabilities (Silvia García-Méndez et al., 2024)</a></li><li><a href=#2542--25204-cross-lingual-transfer-robustness-to-lower-resource-languages-on-adversarial-datasets-shadi-manafi-et-al-2024>(25/42 | 25/204) Cross-Lingual Transfer Robustness to Lower-Resource Languages on Adversarial Datasets (Shadi Manafi et al., 2024)</a></li><li><a href=#2642--26204-llava-gemma-accelerating-multimodal-foundation-models-with-a-compact-language-model-musashi-hinck-et-al-2024>(26/42 | 26/204) LLaVA-Gemma: Accelerating Multimodal Foundation Models with a Compact Language Model (Musashi Hinck et al., 2024)</a></li><li><a href=#2742--27204-using-llms-to-model-the-beliefs-and-preferences-of-targeted-populations-keiichi-namikoshi-et-al-2024>(27/42 | 27/204) Using LLMs to Model the Beliefs and Preferences of Targeted Populations (Keiichi Namikoshi et al., 2024)</a></li><li><a href=#2842--28204-a-systematic-analysis-of-subwords-and-cross-lingual-transfer-in-multilingual-translation-francois-meyer-et-al-2024>(28/42 | 28/204) A Systematic Analysis of Subwords and Cross-Lingual Transfer in Multilingual Translation (Francois Meyer et al., 2024)</a></li><li><a href=#2942--29204-adverb-is-the-key-simple-text-data-augmentation-with-adverb-deletion-juhwan-choi-et-al-2024>(29/42 | 29/204) Adverb Is the Key: Simple Text Data Augmentation with Adverb Deletion (Juhwan Choi et al., 2024)</a></li><li><a href=#3042--30204-realkie-five-novel-datasets-for-enterprise-key-information-extraction-benjamin-townsend-et-al-2024>(30/42 | 30/204) RealKIE: Five Novel Datasets for Enterprise Key Information Extraction (Benjamin Townsend et al., 2024)</a></li><li><a href=#3142--31204-dijiang-efficient-large-language-models-through-compact-kernelization-hanting-chen-et-al-2024>(31/42 | 31/204) DiJiang: Efficient Large Language Models through Compact Kernelization (Hanting Chen et al., 2024)</a></li><li><a href=#3242--32204-where-are-you-from-let-me-guess-subdialect-recognition-of-speeches-in-sorani-kurdish-sana-isam-et-al-2024>(32/42 | 32/204) Where Are You From? Let Me Guess! Subdialect Recognition of Speeches in Sorani Kurdish (Sana Isam et al., 2024)</a></li><li><a href=#3342--33204-emotion-anchored-contrastive-learning-framework-for-emotion-recognition-in-conversation-fangxu-yu-et-al-2024>(33/42 | 33/204) Emotion-Anchored Contrastive Learning Framework for Emotion Recognition in Conversation (Fangxu Yu et al., 2024)</a></li><li><a href=#3442--34204-an-efficient-approach-for-studying-cross-lingual-transfer-in-multilingual-language-models-fahim-faisal-et-al-2024>(34/42 | 34/204) An Efficient Approach for Studying Cross-Lingual Transfer in Multilingual Language Models (Fahim Faisal et al., 2024)</a></li><li><a href=#3542--35204-on-large-language-models-hallucination-with-regard-to-known-facts-che-jiang-et-al-2024>(35/42 | 35/204) On Large Language Models&rsquo; Hallucination with Regard to Known Facts (Che Jiang et al., 2024)</a></li><li><a href=#3642--36204-individual-text-corpora-predict-openness-interests-knowledge-and-level-of-education-markus-j-hofmann-et-al-2024>(36/42 | 36/204) Individual Text Corpora Predict Openness, Interests, Knowledge and Level of Education (Markus J. Hofmann et al., 2024)</a></li><li><a href=#3742--37204-towards-a-framework-for-evaluating-explanations-in-automated-fact-verification-neema-kotonya-et-al-2024>(37/42 | 37/204) Towards a Framework for Evaluating Explanations in Automated Fact Verification (Neema Kotonya et al., 2024)</a></li><li><a href=#3842--38204-automatic-alignment-of-discourse-relations-of-different-discourse-annotation-frameworks-yingxue-fu-2024>(38/42 | 38/204) Automatic Alignment of Discourse Relations of Different Discourse Annotation Frameworks (Yingxue Fu, 2024)</a></li><li><a href=#3942--39204-user-modeling-challenges-in-interactive-ai-assistant-systems-megan-su-et-al-2024>(39/42 | 39/204) User Modeling Challenges in Interactive AI Assistant Systems (Megan Su et al., 2024)</a></li><li><a href=#4042--40204-the-lscd-benchmark-a-testbed-for-diachronic-word-meaning-tasks-dominik-schlechtweg-et-al-2024>(40/42 | 40/204) The LSCD Benchmark: a Testbed for Diachronic Word Meaning Tasks (Dominik Schlechtweg et al., 2024)</a></li><li><a href=#4142--41204-ipa-transcription-of-bengali-texts-kanij-fatema-et-al-2024>(41/42 | 41/204) IPA Transcription of Bengali Texts (Kanij Fatema et al., 2024)</a></li><li><a href=#4242--42204-slfnet-generating-semantic-logic-forms-from-natural-language-using-semantic-probability-graphs-hao-wu-et-al-2024>(42/42 | 42/204) SLFNet: Generating Semantic Logic Forms from Natural Language Using Semantic Probability Graphs (Hao Wu et al., 2024)</a></li></ul></li><li><a href=#cscv-68>cs.CV (68)</a><ul><li><a href=#168--43204-uncovering-bias-in-large-vision-language-models-with-counterfactuals-phillip-howard-et-al-2024>(1/68 | 43/204) Uncovering Bias in Large Vision-Language Models with Counterfactuals (Phillip Howard et al., 2024)</a></li><li><a href=#268--44204-relation-rectification-in-diffusion-model-yinwei-wu-et-al-2024>(2/68 | 44/204) Relation Rectification in Diffusion Model (Yinwei Wu et al., 2024)</a></li><li><a href=#368--45204-convolutional-prompting-meets-language-models-for-continual-learning-anurag-roy-et-al-2024>(3/68 | 45/204) Convolutional Prompting meets Language Models for Continual Learning (Anurag Roy et al., 2024)</a></li><li><a href=#468--46204-draw-and-understand-leveraging-visual-prompts-to-enable-mllms-to-comprehend-what-you-want-weifeng-lin-et-al-2024>(4/68 | 46/204) Draw-and-Understand: Leveraging Visual Prompts to Enable MLLMs to Comprehend What You Want (Weifeng Lin et al., 2024)</a></li><li><a href=#568--47204-medclip-sam-bridging-text-and-image-towards-universal-medical-image-segmentation-taha-koleilat-et-al-2024>(5/68 | 47/204) MedCLIP-SAM: Bridging Text and Image Towards Universal Medical Image Segmentation (Taha Koleilat et al., 2024)</a></li><li><a href=#668--48204-fairrag-fair-human-generation-via-fair-retrieval-augmentation-robik-shrestha-et-al-2024>(6/68 | 48/204) FairRAG: Fair Human Generation via Fair Retrieval Augmentation (Robik Shrestha et al., 2024)</a></li><li><a href=#768--49204-unsolvable-problem-detection-evaluating-trustworthiness-of-vision-language-models-atsuyuki-miyai-et-al-2024>(7/68 | 49/204) Unsolvable Problem Detection: Evaluating Trustworthiness of Vision Language Models (Atsuyuki Miyai et al., 2024)</a></li><li><a href=#868--50204-learn-no-to-say-yes-better-improving-vision-language-models-via-negations-jaisidh-singh-et-al-2024>(8/68 | 50/204) Learn &lsquo;No&rsquo; to Say &lsquo;Yes&rsquo; Better: Improving Vision-Language Models via Negations (Jaisidh Singh et al., 2024)</a></li><li><a href=#968--51204-eclipse-efficient-continual-learning-in-panoptic-segmentation-with-visual-prompt-tuning-beomyoung-kim-et-al-2024>(9/68 | 51/204) ECLIPSE: Efficient Continual Learning in Panoptic Segmentation with Visual Prompt Tuning (Beomyoung Kim et al., 2024)</a></li><li><a href=#1068--52204-sgd-street-view-synthesis-with-gaussian-splatting-and-diffusion-prior-zhongrui-yu-et-al-2024>(10/68 | 52/204) SGD: Street View Synthesis with Gaussian Splatting and Diffusion Prior (Zhongrui Yu et al., 2024)</a></li><li><a href=#1168--53204-agileformer-spatially-agile-transformer-unet-for-medical-image-segmentation-peijie-qiu-et-al-2024>(11/68 | 53/204) AgileFormer: Spatially Agile Transformer UNet for Medical Image Segmentation (Peijie Qiu et al., 2024)</a></li><li><a href=#1268--54204-deepfake-sentry-harnessing-ensemble-intelligence-for-resilient-detection-and-generalisation-liviu-daniel-ştefan-et-al-2024>(12/68 | 54/204) Deepfake Sentry: Harnessing Ensemble Intelligence for Resilient Detection and Generalisation (Liviu-Daniel Ştefan et al., 2024)</a></li><li><a href=#1368--55204-long-tailed-anomaly-detection-with-learnable-class-names-chih-hui-ho-et-al-2024>(13/68 | 55/204) Long-Tailed Anomaly Detection with Learnable Class Names (Chih-Hui Ho et al., 2024)</a></li><li><a href=#1468--56204-freeseg-diff-training-free-open-vocabulary-segmentation-with-diffusion-models-barbara-toniella-corradini-et-al-2024>(14/68 | 56/204) FreeSeg-Diff: Training-Free Open-Vocabulary Segmentation with Diffusion Models (Barbara Toniella Corradini et al., 2024)</a></li><li><a href=#1568--57204-on-inherent-adversarial-robustness-of-active-vision-systems-amitangshu-mukherjee-et-al-2024>(15/68 | 57/204) On Inherent Adversarial Robustness of Active Vision Systems (Amitangshu Mukherjee et al., 2024)</a></li><li><a href=#1668--58204-heterogeneous-network-based-contrastive-learning-method-for-polsar-land-cover-classification-jianfeng-cai-et-al-2024>(16/68 | 58/204) Heterogeneous Network Based Contrastive Learning Method for PolSAR Land Cover Classification (Jianfeng Cai et al., 2024)</a></li><li><a href=#1768--59204-ct-respiratory-motion-synthesis-using-joint-supervised-and-adversarial-learning-yi-heng-cao-et-al-2024>(17/68 | 59/204) CT respiratory motion synthesis using joint supervised and adversarial learning (Yi-Heng Cao et al., 2024)</a></li><li><a href=#1868--60204-vsrd-instance-aware-volumetric-silhouette-rendering-for-weakly-supervised-3d-object-detection-zihua-liu-et-al-2024>(18/68 | 60/204) VSRD: Instance-Aware Volumetric Silhouette Rendering for Weakly Supervised 3D Object Detection (Zihua Liu et al., 2024)</a></li><li><a href=#1968--61204-mixed-precision-supernet-training-from-vision-foundation-models-using-low-rank-adapter-yuiko-sakuma-et-al-2024>(19/68 | 61/204) Mixed-precision Supernet Training from Vision Foundation Models using Low Rank Adapter (Yuiko Sakuma et al., 2024)</a></li><li><a href=#2068--62204-classification-of-diabetic-retinopathy-using-pre-trained-deep-learning-models-inas-al-kamachy-et-al-2024>(20/68 | 62/204) Classification of Diabetic Retinopathy using Pre-Trained Deep Learning Models (Inas Al-Kamachy et al., 2024)</a></li><li><a href=#2168--63204-gda-generalized-diffusion-for-robust-test-time-adaptation-yun-yun-tsai-et-al-2024>(21/68 | 63/204) GDA: Generalized Diffusion for Robust Test-time Adaptation (Yun-Yun Tsai et al., 2024)</a></li><li><a href=#2268--64204-context-aware-integration-of-language-and-visual-references-for-natural-language-tracking-yanyan-shao-et-al-2024>(22/68 | 64/204) Context-Aware Integration of Language and Visual References for Natural Language Tracking (Yanyan Shao et al., 2024)</a></li><li><a href=#2368--65204-robust-ensemble-person-re-identification-via-orthogonal-fusion-with-occlusion-handling-syeda-nyma-ferdous-et-al-2024>(23/68 | 65/204) Robust Ensemble Person Re-Identification via Orthogonal Fusion with Occlusion Handling (Syeda Nyma Ferdous et al., 2024)</a></li><li><a href=#2468--66204-holo-vqvae-vq-vae-for-phase-only-holograms-joohyun-park-et-al-2024>(24/68 | 66/204) Holo-VQVAE: VQ-VAE for phase-only holograms (Joohyun Park et al., 2024)</a></li><li><a href=#2568--67204-h2rsvlm-towards-helpful-and-honest-remote-sensing-large-vision-language-model-chao-pang-et-al-2024>(25/68 | 67/204) H2RSVLM: Towards Helpful and Honest Remote Sensing Large Vision Language Model (Chao Pang et al., 2024)</a></li><li><a href=#2668--68204-motion-inversion-for-video-customization-luozhou-wang-et-al-2024>(26/68 | 68/204) Motion Inversion for Video Customization (Luozhou Wang et al., 2024)</a></li><li><a href=#2768--69204-fairclip-harnessing-fairness-in-vision-language-learning-yan-luo-et-al-2024>(27/68 | 69/204) FairCLIP: Harnessing Fairness in Vision-Language Learning (Yan Luo et al., 2024)</a></li><li><a href=#2868--70204-are-we-on-the-right-way-for-evaluating-large-vision-language-models-lin-chen-et-al-2024>(28/68 | 70/204) Are We on the Right Way for Evaluating Large Vision-Language Models? (Lin Chen et al., 2024)</a></li><li><a href=#2968--71204-negative-label-guided-ood-detection-with-pretrained-vision-language-models-xue-jiang-et-al-2024>(29/68 | 71/204) Negative Label Guided OOD Detection with Pretrained Vision-Language Models (Xue Jiang et al., 2024)</a></li><li><a href=#3068--72204-dvis-daq-improving-video-segmentation-via-dynamic-anchor-queries-yikang-zhou-et-al-2024>(30/68 | 72/204) DVIS-DAQ: Improving Video Segmentation via Dynamic Anchor Queries (Yikang Zhou et al., 2024)</a></li><li><a href=#3168--73204-aggregating-local-and-global-features-via-selective-state-spaces-model-for-efficient-image-deblurring-hu-gao-et-al-2024>(31/68 | 73/204) Aggregating Local and Global Features via Selective State Spaces Model for Efficient Image Deblurring (Hu Gao et al., 2024)</a></li><li><a href=#3268--74204-fsmr-a-feature-swapping-multi-modal-reasoning-approach-with-joint-textual-and-visual-clues-shuang-li-et-al-2024>(32/68 | 74/204) FSMR: A Feature Swapping Multi-modal Reasoning Approach with Joint Textual and Visual Clues (Shuang Li et al., 2024)</a></li><li><a href=#3368--75204-semantically-shifted-incremental-adapter-tuning-is-a-continual-vitransformer-yuwen-tan-et-al-2024>(33/68 | 75/204) Semantically-Shifted Incremental Adapter-Tuning is A Continual ViTransformer (Yuwen Tan et al., 2024)</a></li><li><a href=#3468--76204-efficient-modulation-for-vision-networks-xu-ma-et-al-2024>(34/68 | 76/204) Efficient Modulation for Vision Networks (Xu Ma et al., 2024)</a></li><li><a href=#3568--77204-catsnet-a-context-aware-network-for-height-estimation-in-a-forested-area-based-on-pol-tomosar-data-wenyu-yang-et-al-2024>(35/68 | 77/204) CATSNet: a context-aware network for Height Estimation in a Forested Area based on Pol-TomoSAR data (Wenyu Yang et al., 2024)</a></li><li><a href=#3668--78204-stegogan-leveraging-steganography-for-non-bijective-image-to-image-translation-sidi-wu-et-al-2024>(36/68 | 78/204) StegoGAN: Leveraging Steganography for Non-Bijective Image-to-Image Translation (Sidi Wu et al., 2024)</a></li><li><a href=#3768--79204-selective-attention-based-modulation-for-continual-learning-giovanni-bellitto-et-al-2024>(37/68 | 79/204) Selective Attention-based Modulation for Continual Learning (Giovanni Bellitto et al., 2024)</a></li><li><a href=#3868--80204-colorful-cutout-enhancing-image-data-augmentation-with-curriculum-learning-juhwan-choi-et-al-2024>(38/68 | 80/204) Colorful Cutout: Enhancing Image Data Augmentation with Curriculum Learning (Juhwan Choi et al., 2024)</a></li><li><a href=#3968--81204-a-parallel-attention-network-for-cattle-face-recognition-jiayu-li-et-al-2024>(39/68 | 81/204) A Parallel Attention Network for Cattle Face Recognition (Jiayu Li et al., 2024)</a></li><li><a href=#4068--82204-separate-dynamic-and-differentiable-smart-pruner-for-blockoutput-channel-pruning-on-computer-vision-tasks-guanhua-ding-et-al-2024>(40/68 | 82/204) Separate, Dynamic and Differentiable (SMART) Pruner for Block/Output Channel Pruning on Computer Vision Tasks (Guanhua Ding et al., 2024)</a></li><li><a href=#4168--83204-structure-matters-tackling-the-semantic-discrepancy-in-diffusion-models-for-image-inpainting-haipeng-liu-et-al-2024>(41/68 | 83/204) Structure Matters: Tackling the Semantic Discrepancy in Diffusion Models for Image Inpainting (Haipeng Liu et al., 2024)</a></li><li><a href=#4268--84204-benchmarking-counterfactual-image-generation-thomas-melistas-et-al-2024>(42/68 | 84/204) Benchmarking Counterfactual Image Generation (Thomas Melistas et al., 2024)</a></li><li><a href=#4368--85204-latent-embedding-clustering-for-occlusion-robust-head-pose-estimation-josé-celestino-et-al-2024>(43/68 | 85/204) Latent Embedding Clustering for Occlusion Robust Head Pose Estimation (José Celestino et al., 2024)</a></li><li><a href=#4468--86204-mtmmc-a-large-scale-real-world-multi-modal-camera-tracking-benchmark-sanghyun-woo-et-al-2024>(44/68 | 86/204) MTMMC: A Large-Scale Real-World Multi-Modal Camera Tracking Benchmark (Sanghyun Woo et al., 2024)</a></li><li><a href=#4568--87204-neslam-neural-implicit-mapping-and-self-supervised-feature-tracking-with-depth-completion-and-denoising-tianchen-deng-et-al-2024>(45/68 | 87/204) NeSLAM: Neural Implicit Mapping and Self-Supervised Feature Tracking With Depth Completion and Denoising (Tianchen Deng et al., 2024)</a></li><li><a href=#4668--88204-stable-surface-regularization-for-fast-few-shot-nerf-byeongin-joung-et-al-2024>(46/68 | 88/204) Stable Surface Regularization for Fast Few-Shot NeRF (Byeongin Joung et al., 2024)</a></li><li><a href=#4768--89204-multi-region-transfer-learning-for-segmentation-of-crop-field-boundaries-in-satellite-images-with-limited-labels-hannah-kerner-et-al-2024>(47/68 | 89/204) Multi-Region Transfer Learning for Segmentation of Crop Field Boundaries in Satellite Images with Limited Labels (Hannah Kerner et al., 2024)</a></li><li><a href=#4868--90204-modeling-weather-uncertainty-for-multi-weather-co-presence-estimation-qi-bi-et-al-2024>(48/68 | 90/204) Modeling Weather Uncertainty for Multi-weather Co-Presence Estimation (Qi Bi et al., 2024)</a></li><li><a href=#4968--91204-optimal-blackjack-strategy-recommender-a-comprehensive-study-on-computer-vision-integration-for-enhanced-gameplay-krishnanshu-gupta-et-al-2024>(49/68 | 91/204) Optimal Blackjack Strategy Recommender: A Comprehensive Study on Computer Vision Integration for Enhanced Gameplay (Krishnanshu Gupta et al., 2024)</a></li><li><a href=#5068--92204-universal-bovine-identification-via-depth-data-and-deep-metric-learning-asheesh-sharma-et-al-2024>(50/68 | 92/204) Universal Bovine Identification via Depth Data and Deep Metric Learning (Asheesh Sharma et al., 2024)</a></li><li><a href=#5168--93204-mtlora-a-low-rank-adaptation-approach-for-efficient-multi-task-learning-ahmed-agiza-et-al-2024>(51/68 | 93/204) MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning (Ahmed Agiza et al., 2024)</a></li><li><a href=#5268--94204-u-vap-user-specified-visual-appearance-personalization-via-decoupled-self-augmentation-you-wu-et-al-2024>(52/68 | 94/204) U-VAP: User-specified Visual Appearance Personalization via Decoupled Self Augmentation (You Wu et al., 2024)</a></li><li><a href=#5368--95204-sketch-to-architecture-generative-ai-aided-architectural-design-pengzhi-li-et-al-2024>(53/68 | 95/204) Sketch-to-Architecture: Generative AI-aided Architectural Design (Pengzhi Li et al., 2024)</a></li><li><a href=#5468--96204-harmamba-efficient-wearable-sensor-human-activity-recognition-based-on-bidirectional-selective-ssm-shuangjian-li-et-al-2024>(54/68 | 96/204) HARMamba: Efficient Wearable Sensor Human Activity Recognition Based on Bidirectional Selective SSM (Shuangjian Li et al., 2024)</a></li><li><a href=#5568--97204-mcnet-a-crowd-denstity-estimation-network-based-on-integrating-multiscale-attention-module-qiang-guo-et-al-2024>(55/68 | 97/204) MCNet: A crowd denstity estimation network based on integrating multiscale attention module (Qiang Guo et al., 2024)</a></li><li><a href=#5668--98204-segmentation-classification-and-interpretation-of-breast-cancer-medical-images-using-human-in-the-loop-machine-learning-david-vázquez-lema-et-al-2024>(56/68 | 98/204) Segmentation, Classification and Interpretation of Breast Cancer Medical Images using Human-in-the-Loop Machine Learning (David Vázquez-Lema et al., 2024)</a></li><li><a href=#5768--99204-grounding-and-enhancing-grid-based-models-for-neural-fields-zelin-zhao-et-al-2024>(57/68 | 99/204) Grounding and Enhancing Grid-based Models for Neural Fields (Zelin Zhao et al., 2024)</a></li><li><a href=#5868--100204-binarized-low-light-raw-video-enhancement-gengchen-zhang-et-al-2024>(58/68 | 100/204) Binarized Low-light Raw Video Enhancement (Gengchen Zhang et al., 2024)</a></li><li><a href=#5968--101204-scenetracker-long-term-scene-flow-estimation-network-bo-wang-et-al-2024>(59/68 | 101/204) SceneTracker: Long-term Scene Flow Estimation Network (Bo Wang et al., 2024)</a></li><li><a href=#6068--102204-disentangling-racial-phenotypes-fine-grained-control-of-race-related-facial-phenotype-characteristics-seyma-yucer-et-al-2024>(60/68 | 102/204) Disentangling Racial Phenotypes: Fine-Grained Control of Race-related Facial Phenotype Characteristics (Seyma Yucer et al., 2024)</a></li><li><a href=#6168--103204-ploc-a-new-evaluation-criterion-based-on-physical-location-for-autonomous-driving-datasets-ruining-yang-et-al-2024>(61/68 | 103/204) PLoc: A New Evaluation Criterion Based on Physical Location for Autonomous Driving Datasets (Ruining Yang et al., 2024)</a></li><li><a href=#6268--104204-talk3d-high-fidelity-talking-portrait-synthesis-via-personalized-3d-generative-prior-jaehoon-ko-et-al-2024>(62/68 | 104/204) Talk3D: High-Fidelity Talking Portrait Synthesis via Personalized 3D Generative Prior (Jaehoon Ko et al., 2024)</a></li><li><a href=#6368--105204-multi-level-neural-scene-graphs-for-dynamic-urban-environments-tobias-fischer-et-al-2024>(63/68 | 105/204) Multi-Level Neural Scene Graphs for Dynamic Urban Environments (Tobias Fischer et al., 2024)</a></li><li><a href=#6468--106204-fisbe-a-real-world-benchmark-dataset-for-instance-segmentation-of-long-range-thin-filamentous-structures-lisa-mais-et-al-2024>(64/68 | 106/204) FISBe: A real-world benchmark dataset for instance segmentation of long-range thin filamentous structures (Lisa Mais et al., 2024)</a></li><li><a href=#6568--107204-benchmarking-the-robustness-of-temporal-action-detection-models-against-temporal-corruptions-runhao-zeng-et-al-2024>(65/68 | 107/204) Benchmarking the Robustness of Temporal Action Detection Models Against Temporal Corruptions (Runhao Zeng et al., 2024)</a></li><li><a href=#6668--108204-snap-it-tap-it-splat-it-tactile-informed-3d-gaussian-splatting-for-reconstructing-challenging-surfaces-mauro-comi-et-al-2024>(66/68 | 108/204) Snap-it, Tap-it, Splat-it: Tactile-Informed 3D Gaussian Splatting for Reconstructing Challenging Surfaces (Mauro Comi et al., 2024)</a></li><li><a href=#6768--109204-prototype-based-interpretable-breast-cancer-prediction-models-analysis-and-challenges-shreyasi-pathak-et-al-2024>(67/68 | 109/204) Prototype-based Interpretable Breast Cancer Prediction Models: Analysis and Challenges (Shreyasi Pathak et al., 2024)</a></li><li><a href=#6868--110204-fully-geometric-panoramic-localization-junho-kim-et-al-2024>(68/68 | 110/204) Fully Geometric Panoramic Localization (Junho Kim et al., 2024)</a></li></ul></li><li><a href=#cslg-18>cs.LG (18)</a><ul><li><a href=#118--111204-unleashing-the-potential-of-large-language-models-for-predictive-tabular-tasks-in-data-science-yazheng-yang-et-al-2024>(1/18 | 111/204) Unleashing the Potential of Large Language Models for Predictive Tabular Tasks in Data Science (Yazheng Yang et al., 2024)</a></li><li><a href=#218--112204-graph-neural-aggregation-diffusion-with-metastability-kaiyuan-cui-et-al-2024>(2/18 | 112/204) Graph Neural Aggregation-diffusion with Metastability (Kaiyuan Cui et al., 2024)</a></li><li><a href=#318--113204-beyond-the-known-novel-class-discovery-for-open-world-graph-learning-yucheng-jin-et-al-2024>(3/18 | 113/204) Beyond the Known: Novel Class Discovery for Open-world Graph Learning (Yucheng Jin et al., 2024)</a></li><li><a href=#418--114204-adaptive-decentralized-federated-learning-in-energy-and-latency-constrained-wireless-networks-zhigang-yan-et-al-2024>(4/18 | 114/204) Adaptive Decentralized Federated Learning in Energy and Latency Constrained Wireless Networks (Zhigang Yan et al., 2024)</a></li><li><a href=#518--115204-pikelpn-mitigating-overlooked-inefficiencies-of-low-precision-neural-networks-marina-neseem-et-al-2024>(5/18 | 115/204) PikeLPN: Mitigating Overlooked Inefficiencies of Low-Precision Neural Networks (Marina Neseem et al., 2024)</a></li><li><a href=#618--116204-deepheteroiot-deep-local-and-global-learning-over-heterogeneous-iot-sensor-data-muhammad-sakib-khan-inan-et-al-2024>(6/18 | 116/204) DeepHeteroIoT: Deep Local and Global Learning over Heterogeneous IoT Sensor Data (Muhammad Sakib Khan Inan et al., 2024)</a></li><li><a href=#718--117204-decision-mamba-reinforcement-learning-via-sequence-modeling-with-selective-state-spaces-toshihiro-ota-2024>(7/18 | 117/204) Decision Mamba: Reinforcement Learning via Sequence Modeling with Selective State Spaces (Toshihiro Ota, 2024)</a></li><li><a href=#818--118204-mol-air-molecular-reinforcement-learning-with-adaptive-intrinsic-rewards-for-goal-directed-molecular-generation-jinyeong-park-et-al-2024>(8/18 | 118/204) Mol-AIR: Molecular Reinforcement Learning with Adaptive Intrinsic Rewards for Goal-directed Molecular Generation (Jinyeong Park et al., 2024)</a></li><li><a href=#918--119204-caesar-enhancing-federated-rl-in-heterogeneous-mdps-through-convergence-aware-sampling-with-screening-hei-yi-mak-et-al-2024>(9/18 | 119/204) CAESAR: Enhancing Federated RL in Heterogeneous MDPs through Convergence-Aware Sampling with Screening (Hei Yi Mak et al., 2024)</a></li><li><a href=#1018--120204-tdanet-a-novel-temporal-denoise-convolutional-neural-network-with-attention-for-fault-diagnosis-zhongzhi-li-et-al-2024>(10/18 | 120/204) TDANet: A Novel Temporal Denoise Convolutional Neural Network With Attention for Fault Diagnosis (Zhongzhi Li et al., 2024)</a></li><li><a href=#1118--121204-mambamixer-efficient-selective-state-space-models-with-dual-token-and-channel-selection-ali-behrouz-et-al-2024>(11/18 | 121/204) MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection (Ali Behrouz et al., 2024)</a></li><li><a href=#1218--122204-sparse-multimodal-fusion-with-modal-channel-attention-josiah-bjorgaard-2024>(12/18 | 122/204) Sparse multimodal fusion with modal channel attention (Josiah Bjorgaard, 2024)</a></li><li><a href=#1318--123204-localising-the-seizure-onset-zone-from-single-pulse-electrical-stimulation-responses-with-a-transformer-jamie-norris-et-al-2024>(13/18 | 123/204) Localising the Seizure Onset Zone from Single-Pulse Electrical Stimulation Responses with a Transformer (Jamie Norris et al., 2024)</a></li><li><a href=#1418--124204-embracing-unknown-step-by-step-towards-reliable-sparse-training-in-real-world-bowen-lei-et-al-2024>(14/18 | 124/204) Embracing Unknown Step by Step: Towards Reliable Sparse Training in Real World (Bowen Lei et al., 2024)</a></li><li><a href=#1518--125204-coverage-guaranteed-prediction-sets-for-out-of-distribution-data-xin-zou-et-al-2024>(15/18 | 125/204) Coverage-Guaranteed Prediction Sets for Out-of-Distribution Data (Xin Zou et al., 2024)</a></li><li><a href=#1618--126204-nonlinearity-enhanced-adaptive-activation-function-david-yevick-2024>(16/18 | 126/204) Nonlinearity Enhanced Adaptive Activation Function (David Yevick, 2024)</a></li><li><a href=#1718--127204-comparing-hyper-optimized-machine-learning-models-for-predicting-efficiency-degradation-in-organic-solar-cells-david-valientea-et-al-2024>(17/18 | 127/204) Comparing Hyper-optimized Machine Learning Models for Predicting Efficiency Degradation in Organic Solar Cells (David Valientea et al., 2024)</a></li><li><a href=#1818--128204-tfb-towards-comprehensive-and-fair-benchmarking-of-time-series-forecasting-methods-xiangfei-qiu-et-al-2024>(18/18 | 128/204) TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods (Xiangfei Qiu et al., 2024)</a></li></ul></li><li><a href=#csai-12>cs.AI (12)</a><ul><li><a href=#112--129204-the-impact-of-prompts-on-zero-shot-detection-of-ai-generated-text-kaito-taguchi-et-al-2024>(1/12 | 129/204) The Impact of Prompts on Zero-Shot Detection of AI-Generated Text (Kaito Taguchi et al., 2024)</a></li><li><a href=#212--130204-on-size-and-hardness-generalization-in-unsupervised-learning-for-the-travelling-salesman-problem-yimeng-min-et-al-2024>(2/12 | 130/204) On Size and Hardness Generalization in Unsupervised Learning for the Travelling Salesman Problem (Yimeng Min et al., 2024)</a></li><li><a href=#312--131204-the-future-of-combating-rumors-retrieval-discrimination-and-generation-junhao-xu-et-al-2024>(3/12 | 131/204) The Future of Combating Rumors? Retrieval, Discrimination, and Generation (Junhao Xu et al., 2024)</a></li><li><a href=#412--132204-does-faithfulness-conflict-with-plausibility-an-empirical-study-in-explainable-ai-across-nlp-tasks-xiaolei-lu-et-al-2024>(4/12 | 132/204) Does Faithfulness Conflict with Plausibility? An Empirical Study in Explainable AI across NLP Tasks (Xiaolei Lu et al., 2024)</a></li><li><a href=#512--133204-efficient-and-sharp-off-policy-evaluation-in-robust-markov-decision-processes-andrew-bennett-et-al-2024>(5/12 | 133/204) Efficient and Sharp Off-Policy Evaluation in Robust Markov Decision Processes (Andrew Bennett et al., 2024)</a></li><li><a href=#612--134204-itcma-a-generative-agent-based-on-a-computational-consciousness-structure-hanzhong-zhang-et-al-2024>(6/12 | 134/204) ITCMA: A Generative Agent Based on a Computational Consciousness Structure (Hanzhong Zhang et al., 2024)</a></li><li><a href=#712--135204-towards-greener-llms-bringing-energy-efficiency-to-the-forefront-of-llm-inference-jovan-stojkovic-et-al-2024>(7/12 | 135/204) Towards Greener LLMs: Bringing Energy-Efficiency to the Forefront of LLM Inference (Jovan Stojkovic et al., 2024)</a></li><li><a href=#812--136204-accurate-block-quantization-in-llms-with-outliers-nikita-trukhanov-et-al-2024>(8/12 | 136/204) Accurate Block Quantization in LLMs with Outliers (Nikita Trukhanov et al., 2024)</a></li><li><a href=#912--137204-development-of-compositionality-and-generalization-through-interactive-learning-of-language-and-action-of-robots-prasanna-vijayaraghavan-et-al-2024>(9/12 | 137/204) Development of Compositionality and Generalization through Interactive Learning of Language and Action of Robots (Prasanna Vijayaraghavan et al., 2024)</a></li><li><a href=#1012--138204-a-learning-based-incentive-mechanism-for-mobile-aigc-service-in-decentralized-internet-of-vehicles-jiani-fan-et-al-2024>(10/12 | 138/204) A Learning-based Incentive Mechanism for Mobile AIGC Service in Decentralized Internet of Vehicles (Jiani Fan et al., 2024)</a></li><li><a href=#1112--139204-implications-of-the-ai-act-for-non-discrimination-law-and-algorithmic-fairness-luca-deck-et-al-2024>(11/12 | 139/204) Implications of the AI Act for Non-Discrimination Law and Algorithmic Fairness (Luca Deck et al., 2024)</a></li><li><a href=#1212--140204-diverse-feature-learning-by-self-distillation-and-reset-sejik-park-2024>(12/12 | 140/204) Diverse Feature Learning by Self-distillation and Reset (Sejik Park, 2024)</a></li></ul></li><li><a href=#csro-10>cs.RO (10)</a><ul><li><a href=#110--141204-ctrl-sim-reactive-and-controllable-driving-agents-with-offline-reinforcement-learning-luke-rowe-et-al-2024>(1/10 | 141/204) CtRL-Sim: Reactive and Controllable Driving Agents with Offline Reinforcement Learning (Luke Rowe et al., 2024)</a></li><li><a href=#210--142204-an-optimization-based-planner-with-b-spline-parameterized-continuous-time-reference-signals-chuyuan-tao-et-al-2024>(2/10 | 142/204) An Optimization-Based Planner with B-spline Parameterized Continuous-Time Reference Signals (Chuyuan Tao et al., 2024)</a></li><li><a href=#310--143204-learning-visual-quadrupedal-loco-manipulation-from-demonstrations-zhengmao-he-et-al-2024>(3/10 | 143/204) Learning Visual Quadrupedal Loco-Manipulation from Demonstrations (Zhengmao He et al., 2024)</a></li><li><a href=#410--144204-adaptive-energy-regularization-for-autonomous-gait-transition-and-energy-efficient-quadruped-locomotion-boyuan-liang-et-al-2024>(4/10 | 144/204) Adaptive Energy Regularization for Autonomous Gait Transition and Energy-Efficient Quadruped Locomotion (Boyuan Liang et al., 2024)</a></li><li><a href=#510--145204-encomp-enhanced-covert-maneuver-planning-using-offline-reinforcement-learning-jumman-hossain-et-al-2024>(5/10 | 145/204) EnCoMP: Enhanced Covert Maneuver Planning using Offline Reinforcement Learning (Jumman Hossain et al., 2024)</a></li><li><a href=#610--146204-moma-pos-where-should-mobile-manipulators-stand-in-cluttered-environment-before-task-execution-beichen-shao-et-al-2024>(6/10 | 146/204) MoMa-Pos: Where Should Mobile Manipulators Stand in Cluttered Environment Before Task Execution? (Beichen Shao et al., 2024)</a></li><li><a href=#710--147204-a-sequential-quadratic-programming-approach-to-the-solution-of-open-loop-generalized-nash-equilibria-for-autonomous-racing-edward-l-zhu-et-al-2024>(7/10 | 147/204) A Sequential Quadratic Programming Approach to the Solution of Open-Loop Generalized Nash Equilibria for Autonomous Racing (Edward L. Zhu et al., 2024)</a></li><li><a href=#810--148204-lego-drive-language-enhanced-goal-oriented-closed-loop-end-to-end-autonomous-driving-pranjal-paul-et-al-2024>(8/10 | 148/204) LeGo-Drive: Language-enhanced Goal-oriented Closed-Loop End-to-End Autonomous Driving (Pranjal Paul et al., 2024)</a></li><li><a href=#910--149204-a-peg-in-hole-task-strategy-for-holes-in-concrete-andré-yuji-yasutomi-et-al-2024>(9/10 | 149/204) A Peg-in-hole Task Strategy for Holes in Concrete (André Yuji Yasutomi et al., 2024)</a></li><li><a href=#1010--150204-fusion-dynamical-systems-with-machine-learning-in-imitation-learning-a-comprehensive-overview-yingbai-hu-et-al-2024>(10/10 | 150/204) Fusion Dynamical Systems with Machine Learning in Imitation Learning: A Comprehensive Overview (Yingbai Hu et al., 2024)</a></li></ul></li><li><a href=#csir-5>cs.IR (5)</a><ul><li><a href=#15--151204-kguf-simple-knowledge-aware-graph-based-recommender-with-user-based-semantic-features-filtering-salvatore-bufi-et-al-2024>(1/5 | 151/204) KGUF: Simple Knowledge-aware Graph-based Recommender with User-based Semantic Features Filtering (Salvatore Bufi et al., 2024)</a></li><li><a href=#25--152204-shallow-cross-encoders-for-low-latency-retrieval-aleksandr-v-petrov-et-al-2024>(2/5 | 152/204) Shallow Cross-Encoders for Low-Latency Retrieval (Aleksandr V. Petrov et al., 2024)</a></li><li><a href=#35--153204-robust-federated-contrastive-recommender-system-against-model-poisoning-attack-wei-yuan-et-al-2024>(3/5 | 153/204) Robust Federated Contrastive Recommender System against Model Poisoning Attack (Wei Yuan et al., 2024)</a></li><li><a href=#45--154204-review-based-cross-domain-recommendation-via-hyperbolic-embedding-and-hierarchy-aware-domain-disentanglement-yoonhyuk-choi-2024>(4/5 | 154/204) Review-Based Cross-Domain Recommendation via Hyperbolic Embedding and Hierarchy-Aware Domain Disentanglement (Yoonhyuk Choi, 2024)</a></li><li><a href=#55--155204-aiming-at-the-target-filter-collaborative-information-for-cross-domain-recommendation-hanyu-li-et-al-2024>(5/5 | 155/204) Aiming at the Target: Filter Collaborative Information for Cross-Domain Recommendation (Hanyu Li et al., 2024)</a></li></ul></li><li><a href=#csmm-1>cs.MM (1)</a><ul><li><a href=#11--156204-convbench-a-multi-turn-conversation-evaluation-benchmark-with-hierarchical-capability-for-large-vision-language-models-shuo-liu-et-al-2024>(1/1 | 156/204) ConvBench: A Multi-Turn Conversation Evaluation Benchmark with Hierarchical Capability for Large Vision-Language Models (Shuo Liu et al., 2024)</a></li></ul></li><li><a href=#eessiv-7>eess.IV (7)</a><ul><li><a href=#17--157204-unsupervised-tumor-aware-distillation-for-multi-modal-brain-image-translation-chuan-huang-et-al-2024>(1/7 | 157/204) Unsupervised Tumor-Aware Distillation for Multi-Modal Brain Image Translation (Chuan Huang et al., 2024)</a></li><li><a href=#27--158204-ultralight-vm-unet-parallel-vision-mamba-significantly-reduces-parameters-for-skin-lesion-segmentation-renkai-wu-et-al-2024>(2/7 | 158/204) UltraLight VM-UNet: Parallel Vision Mamba Significantly Reduces Parameters for Skin Lesion Segmentation (Renkai Wu et al., 2024)</a></li><li><a href=#37--159204-an-interpretable-cross-attentive-multi-modal-mri-fusion-framework-for-schizophrenia-diagnosis-ziyu-zhou-et-al-2024>(3/7 | 159/204) An Interpretable Cross-Attentive Multi-modal MRI Fusion Framework for Schizophrenia Diagnosis (Ziyu Zhou et al., 2024)</a></li><li><a href=#47--160204-fetaldiffusion-pose-controllable-3d-fetal-mri-synthesis-with-conditional-diffusion-model-molin-zhang-et-al-2024>(4/7 | 160/204) FetalDiffusion: Pose-Controllable 3D Fetal MRI Synthesis with Conditional Diffusion Model (Molin Zhang et al., 2024)</a></li><li><a href=#57--161204-a-multi-stage-semi-supervised-learning-for-ankle-fracture-classification-on-ct-images-hongzhi-liu-et-al-2024>(5/7 | 161/204) A multi-stage semi-supervised learning for ankle fracture classification on CT images (Hongzhi Liu et al., 2024)</a></li><li><a href=#67--162204-multi-task-magnetic-resonance-imaging-reconstruction-using-meta-learning-wanyu-bian-et-al-2024>(6/7 | 162/204) Multi-task Magnetic Resonance Imaging Reconstruction using Meta-learning (Wanyu Bian et al., 2024)</a></li><li><a href=#77--163204-revolutionizing-disease-diagnosis-with-simultaneous-functional-petmr-and-deeply-integrated-brain-metabolic-hemodynamic-and-perfusion-networks-luoyu-wang-et-al-2024>(7/7 | 163/204) Revolutionizing Disease Diagnosis with simultaneous functional PET/MR and Deeply Integrated Brain Metabolic, Hemodynamic, and Perfusion Networks (Luoyu Wang et al., 2024)</a></li></ul></li><li><a href=#csdb-3>cs.DB (3)</a><ul><li><a href=#13--164204-purple-making-a-large-language-model-a-better-sql-writer-tonghui-ren-et-al-2024>(1/3 | 164/204) PURPLE: Making a Large Language Model a Better SQL Writer (Tonghui Ren et al., 2024)</a></li><li><a href=#23--165204-budget-aware-query-tuning-an-automl-perspective-wentao-wu-et-al-2024>(2/3 | 165/204) Budget-aware Query Tuning: An AutoML Perspective (Wentao Wu et al., 2024)</a></li><li><a href=#33--166204-multi-objective-genetic-algorithm-for-materialized-view-optimization-in-data-warehouses-mahdi-manavi-2024>(3/3 | 166/204) Multi-Objective Genetic Algorithm for Materialized View Optimization in Data Warehouses (Mahdi Manavi, 2024)</a></li></ul></li><li><a href=#csar-3>cs.AR (3)</a><ul><li><a href=#13--167204-an-fpga-based-reconfigurable-accelerator-for-convolution-transformer-hybrid-efficientvit-haikuo-shao-et-al-2024>(1/3 | 167/204) An FPGA-Based Reconfigurable Accelerator for Convolution-Transformer Hybrid EfficientViT (Haikuo Shao et al., 2024)</a></li><li><a href=#23--168204-hot-lego-architect-microfluidic-cooling-equipped-3dics-with-pre-rtl-thermal-simulation-runxi-wang-et-al-2024>(2/3 | 168/204) Hot-LEGO: Architect Microfluidic Cooling Equipped 3DICs with Pre-RTL Thermal Simulation (Runxi Wang et al., 2024)</a></li><li><a href=#33--169204-balanced-data-placement-for-gemv-acceleration-with-processing-in-memory-mohamed-assem-ibrahim-et-al-2024>(3/3 | 169/204) Balanced Data Placement for GEMV Acceleration with Processing-In-Memory (Mohamed Assem Ibrahim et al., 2024)</a></li></ul></li><li><a href=#csit-6>cs.IT (6)</a><ul><li><a href=#16--170204-cooperative-sensing-and-communication-for-isac-networks-performance-analysis-and-optimization-kaitao-meng-et-al-2024>(1/6 | 170/204) Cooperative Sensing and Communication for ISAC Networks: Performance Analysis and Optimization (Kaitao Meng et al., 2024)</a></li><li><a href=#26--171204-a-signature-based-approach-towards-global-channel-charting-with-ultra-low-complexity-longhai-zhao-et-al-2024>(2/6 | 171/204) A Signature Based Approach Towards Global Channel Charting with Ultra Low Complexity (Longhai Zhao et al., 2024)</a></li><li><a href=#36--172204-secure-full-duplex-communication-via-movable-antennas-jingze-ding-et-al-2024>(3/6 | 172/204) Secure Full-Duplex Communication via Movable Antennas (Jingze Ding et al., 2024)</a></li><li><a href=#46--173204-minimizing-end-to-end-latency-for-joint-source-channel-coding-systems-kaiyi-chi-et-al-2024>(4/6 | 173/204) Minimizing End-to-End Latency for Joint Source-Channel Coding Systems (Kaiyi Chi et al., 2024)</a></li><li><a href=#56--174204-joint-training-and-reflection-pattern-optimization-for-non-ideal-ris-aided-multiuser-systems-zhenyao-he-et-al-2024>(5/6 | 174/204) Joint Training and Reflection Pattern Optimization for Non-Ideal RIS-Aided Multiuser Systems (Zhenyao He et al., 2024)</a></li><li><a href=#66--175204-an-information-theoretic-framework-for-out-of-distribution-generalization-wenliang-liu-et-al-2024>(6/6 | 175/204) An Information-Theoretic Framework for Out-of-Distribution Generalization (Wenliang Liu et al., 2024)</a></li></ul></li><li><a href=#cscy-1>cs.CY (1)</a><ul><li><a href=#11--176204-distributed-agency-in-second-language-learning-and-teaching-through-generative-ai-robert-godwin-jones-2024>(1/1 | 176/204) Distributed agency in second language learning and teaching through generative AI (Robert Godwin-Jones, 2024)</a></li></ul></li><li><a href=#cscr-3>cs.CR (3)</a><ul><li><a href=#13--177204-homomorphic-wisards-efficient-weightless-neural-network-training-over-encrypted-data-leonardo-neumann-et-al-2024>(1/3 | 177/204) Homomorphic WiSARDs: Efficient Weightless Neural Network training over encrypted data (Leonardo Neumann et al., 2024)</a></li><li><a href=#23--178204-security-risks-concerns-of-generative-ai-in-the-iot-honghui-xu-et-al-2024>(2/3 | 178/204) Security Risks Concerns of Generative AI in the IoT (Honghui Xu et al., 2024)</a></li><li><a href=#33--179204-decentralized-multimedia-data-sharing-in-iov-a-learning-based-equilibrium-of-supply-and-demand-jiani-fan-et-al-2024>(3/3 | 179/204) Decentralized Multimedia Data Sharing in IoV: A Learning-based Equilibrium of Supply and Demand (Jiani Fan et al., 2024)</a></li></ul></li><li><a href=#csne-1>cs.NE (1)</a><ul><li><a href=#11--180204-biologically-plausible-topology-improved-spiking-actor-network-for-efficient-deep-reinforcement-learning-duzhen-zhang-et-al-2024>(1/1 | 180/204) Biologically-Plausible Topology Improved Spiking Actor Network for Efficient Deep Reinforcement Learning (Duzhen Zhang et al., 2024)</a></li></ul></li><li><a href=#q-biobm-2>q-bio.BM (2)</a><ul><li><a href=#12--181204-molecular-generative-adversarial-network-with-multi-property-optimization-huidong-tang-et-al-2024>(1/2 | 181/204) Molecular Generative Adversarial Network with Multi-Property Optimization (Huidong Tang et al., 2024)</a></li><li><a href=#22--182204-fabind-enhancing-molecular-docking-through-improved-pocket-prediction-and-pose-generation-kaiyuan-gao-et-al-2024>(2/2 | 182/204) FABind+: Enhancing Molecular Docking through Improved Pocket Prediction and Pose Generation (Kaiyuan Gao et al., 2024)</a></li></ul></li><li><a href=#csni-4>cs.NI (4)</a><ul><li><a href=#14--183204-fairtt-an-empirical-approach-for-enhanced-rtt-fairness-and-bottleneck-throughput-in-bbr-akshita-abrol-et-al-2024>(1/4 | 183/204) FaiRTT: An Empirical Approach for Enhanced RTT Fairness and Bottleneck Throughput in BBR (Akshita Abrol et al., 2024)</a></li><li><a href=#24--184204-a-comprehensive-evaluation-of-the-impact-of-atm-qos-mechanisms-on-network-performance-for-multimedia-and-data-applications-mahdi-manavi-2024>(2/4 | 184/204) A Comprehensive Evaluation of the Impact of ATM QoS Mechanisms on Network Performance for Multimedia and Data Applications (Mahdi Manavi, 2024)</a></li><li><a href=#34--185204-neuralunadtnet-feedforward-neural-network-based-routing-protocol-for-delay-tolerant-lunar-communication-networks-parth-patel-et-al-2024>(3/4 | 185/204) NeuraLunaDTNet: Feedforward Neural Network-Based Routing Protocol for Delay-Tolerant Lunar Communication Networks (Parth Patel et al., 2024)</a></li><li><a href=#44--186204-dhnet-a-distributed-network-architecture-for-smart-home-chaoqi-zhou-et-al-2024>(4/4 | 186/204) DHNet: A Distributed Network Architecture for Smart Home (Chaoqi Zhou et al., 2024)</a></li></ul></li><li><a href=#mathoc-3>math.OC (3)</a><ul><li><a href=#13--187204-beyond-suspension-a-two-phase-methodology-for-concluding-sports-leagues-ali-hassanzadeh-et-al-2024>(1/3 | 187/204) Beyond Suspension: A Two-phase Methodology for Concluding Sports Leagues (Ali Hassanzadeh et al., 2024)</a></li><li><a href=#23--188204-an-ordinary-differential-equation-for-entropic-optimal-transport-and-its-linearly-constrained-variants-joshua-zoen-git-hiew-et-al-2024>(2/3 | 188/204) An ordinary differential equation for entropic optimal transport and its linearly constrained variants (Joshua Zoen-Git Hiew et al., 2024)</a></li><li><a href=#33--189204-invertibility-of-discrete-time-linear-systems-with-sparse-inputs-kyle-poe-et-al-2024>(3/3 | 189/204) Invertibility of Discrete-Time Linear Systems with Sparse Inputs (Kyle Poe et al., 2024)</a></li></ul></li><li><a href=#csce-1>cs.CE (1)</a><ul><li><a href=#11--190204-parallel-performance-of-shared-memory-parallel-spectral-deferred-corrections-philip-freese-et-al-2024>(1/1 | 190/204) Parallel performance of shared memory parallel spectral deferred corrections (Philip Freese et al., 2024)</a></li></ul></li><li><a href=#eesssy-2>eess.SY (2)</a><ul><li><a href=#12--191204-keeping-up-with-the-winner-targeted-advertisement-to-communities-in-social-networks-shailaja-mallick-et-al-2024>(1/2 | 191/204) Keeping Up With the Winner! Targeted Advertisement to Communities in Social Networks (Shailaja Mallick et al., 2024)</a></li><li><a href=#22--192204-low-cost-adaptive-obstacle-avoidance-trajectory-control-for-express-delivery-drone-yanhui-zhang-et-al-2024>(2/2 | 192/204) Low-cost adaptive obstacle avoidance trajectory control for express delivery drone (Yanhui Zhang et al., 2024)</a></li></ul></li><li><a href=#csdc-1>cs.DC (1)</a><ul><li><a href=#11--193204-experiências-resultados-e-reflexões-a-partir-do-gerenciamento-de-experimentos-no-mundo-real-com-fanets-e-vants----versão-estendida-bruno-josé-olivieri-de-souza-et-al-2024>(1/1 | 193/204) Experiências, Resultados e Reflexões a partir do Gerenciamento de experimentos no Mundo Real com FANETs e VANTs &ndash; Versão Estendida (Bruno José Olivieri de Souza et al., 2024)</a></li></ul></li><li><a href=#mathna-2>math.NA (2)</a><ul><li><a href=#12--194204-dual-simplex-volume-maximization-for-simplex-structured-matrix-factorization-maryam-abdolali-et-al-2024>(1/2 | 194/204) Dual Simplex Volume Maximization for Simplex-Structured Matrix Factorization (Maryam Abdolali et al., 2024)</a></li><li><a href=#22--195204-sampling-error-mitigation-through-spectrum-smoothing-in-ensemble-data-assimilation-bosu-choi-et-al-2024>(2/2 | 195/204) Sampling error mitigation through spectrum smoothing in ensemble data assimilation (Bosu Choi et al., 2024)</a></li></ul></li><li><a href=#statco-1>stat.CO (1)</a><ul><li><a href=#11--196204-investigating-the-combinatorial-potential-and-applicability-of-random-equation-systems-with-mixture-models-in-a-bayesian-framework-wolfgang-hoegele-2024>(1/1 | 196/204) Investigating the Combinatorial Potential and Applicability of Random Equation Systems with Mixture Models in a Bayesian Framework (Wolfgang Hoegele, 2024)</a></li></ul></li><li><a href=#csds-2>cs.DS (2)</a><ul><li><a href=#12--197204-shaving-logs-via-large-sieve-inequality-faster-algorithms-for-sparse-convolution-and-more-ce-jin-et-al-2024>(1/2 | 197/204) Shaving Logs via Large Sieve Inequality: Faster Algorithms for Sparse Convolution and More (Ce Jin et al., 2024)</a></li><li><a href=#22--198204-optimal-communication-for-classic-functions-in-the-coordinator-model-and-beyond-hossein-esfandiari-et-al-2024>(2/2 | 198/204) Optimal Communication for Classic Functions in the Coordinator Model and Beyond (Hossein Esfandiari et al., 2024)</a></li></ul></li><li><a href=#statml-1>stat.ML (1)</a><ul><li><a href=#11--199204-functional-bilevel-optimization-for-machine-learning-ieva-petrulionyte-et-al-2024>(1/1 | 199/204) Functional Bilevel Optimization for Machine Learning (Ieva Petrulionyte et al., 2024)</a></li></ul></li><li><a href=#cssd-1>cs.SD (1)</a><ul><li><a href=#11--200204-voice-signal-processing-for-machine-learning-the-case-of-speaker-isolation-radan-ganchev-2024>(1/1 | 200/204) Voice Signal Processing for Machine Learning. The Case of Speaker Isolation (Radan Ganchev, 2024)</a></li></ul></li><li><a href=#eessas-1>eess.AS (1)</a><ul><li><a href=#11--201204-exploring-pathological-speech-quality-assessment-with-asr-powered-wav2vec2-in-data-scarce-context-tuan-nguyen-et-al-2024>(1/1 | 201/204) Exploring Pathological Speech Quality Assessment with ASR-Powered Wav2Vec2 in Data-Scarce Context (Tuan Nguyen et al., 2024)</a></li></ul></li><li><a href=#eesssp-1>eess.SP (1)</a><ul><li><a href=#11--202204-nonparametric-bellman-mappings-for-reinforcement-learning-application-to-robust-adaptive-filtering-yuki-akiyama-et-al-2024>(1/1 | 202/204) Nonparametric Bellman Mappings for Reinforcement Learning: Application to Robust Adaptive Filtering (Yuki Akiyama et al., 2024)</a></li></ul></li><li><a href=#csse-1>cs.SE (1)</a><ul><li><a href=#11--203204-towards-a-fault-injection-benchmarking-suite-tianhao-wang-et-al-2024>(1/1 | 203/204) Towards a Fault-Injection Benchmarking Suite (Tianhao Wang et al., 2024)</a></li></ul></li><li><a href=#math-ph-1>math-ph (1)</a><ul><li><a href=#11--204204-designing-poisson-integrators-through-machine-learning-miguel-vaquero-et-al-2024>(1/1 | 204/204) Designing Poisson Integrators Through Machine Learning (Miguel Vaquero et al., 2024)</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:contact.to.akitenkrad@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>contact.to.akitenkrad@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/akitenkrad-blog/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2020 Akitenkrad.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/akitenkrad-blog/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/jquery-3.4.1.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/popper.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/bootstrap.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/navbar.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/plyr.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=https://akitenkrad.github.io/akitenkrad-blog/js/single.js></script><script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.css><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.js></script><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!0})})</script></body></html>