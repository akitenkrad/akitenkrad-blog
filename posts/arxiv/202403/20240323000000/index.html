<!doctype html><html><head><title>arXiv @ 2024.03.23</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/bootstrap.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/main.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/navbar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/plyr.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/flag-icon.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/custom/style.css><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM crossorigin=anonymous><script async src=https://cdn.jsdelivr.net/npm/es-module-shims@1/dist/es-module-shims.min.js crossorigin=anonymous></script><script type=importmap>
{
  "imports": {
    "@popperjs/core": "https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.8/dist/esm/popper.min.js",
    "bootstrap": "https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.esm.min.js"
  }
}
</script><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js integrity=sha384-geWF76RCwLtnZ8qwWowPQNguL3RmwHVBC9FhGdlKrxdiJJigb/j/68SIy3Te4Bkz crossorigin=anonymous></script><script type=module src=https://akitenkrad.github.io/akitenkrad-blog/js/custom/custom.js></script><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/fontawesome/css/all.min.css><link rel=icon type=image/png href=https://akitenkrad.github.io/akitenkrad-blog/images/favicons/favicon-96x96_huf1ee13f0caf27d1547f91fb46207d708_13005_42x0_resize_box_3.png><meta property="og:title" content="arXiv @ 2024.03.23"><meta property="og:description" content="Primary Categories astro-ph.IM (1) cond-mat.dis-nn (1) cs.AI (5) cs.AR (1) cs.CE (2) cs.CL (52) cs.CR (5) cs.CV (104) cs.CY (6) cs.DB (1) cs.DC (1) cs.DS (2) cs.HC (4) cs.IR (3) cs.IT (1) cs.LG (35) cs.NE (8) cs.NI (1) cs.RO (20) cs.SD (5) cs.SE (3) cs.SI (4) eess.AS (3) eess.IV (7) eess.SP (1) eess.SY (14) hep-ex (1) math.CO (1) math.NA (6) math.OC (1) physics.med-ph (1) q-bio.BM (1) q-bio.QM (1) quant-ph (4) stat."><meta property="og:type" content="article"><meta property="og:url" content="https://akitenkrad.github.io/akitenkrad-blog/posts/arxiv/202403/20240323000000/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-03-23T00:00:00+00:00"><meta property="article:modified_time" content="2024-03-23T00:00:00+00:00"><meta name=description content="arXiv @ 2024.03.23"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/single.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/sidebar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/style.css><script type=text/javascript src=//ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js></script><script language=JavaScript>$(document).ready(function(){$("a[href^='http']:not([href*='"+location.hostname+"'])").attr("target","_blank")})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-1MYYZQG0WE"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-1MYYZQG0WE",{anonymize_ip:!1})}</script></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow" style=position:fixed><div class=container><button class="navbar-toggler navbar-light navbar-for-sidebar" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span>
</button>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav mr-auto"><li class=nav-item><a class=navbar-brand href=/akitenkrad-blog><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png alt=Logo>
Akitenkrad's Blog</a></li></ul><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=/akitenkrad-blog#home>Home</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#about>About</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#recent-posts>Recent Posts</a></li><li class=nav-item><a class=nav-link id=blog-link href=https://akitenkrad.github.io/akitenkrad-blog/posts>Posts</a></li><li class=nav-item><a class=nav-link id=tag-link href=https://akitenkrad.github.io/akitenkrad-blog/tags>Tags</a></li></ul></div></div><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/akitenkrad-blog/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/>Papers</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202205/>2022.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202205/20220518224923/ title="A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks">A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220508162318/ title="A Deep Cascade Model for Multi-Document Reading Comprehension">A Deep Cascade Model for Multi-Document Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220506021208/ title="A Primer in BERTology: What We Know About How BERT Works">A Primer in BERTology: What We Know About How BERT Works</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220529131339/ title="Attention Is All You Need">Attention Is All You Need</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220503010000/ title="DeBERTa: Decoding-Enhanced BERT with Disentangled Attention">DeBERTa: Decoding-Enhanced BERT with Disentangled Attention</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220505222900/ title="Dense Passage Retrieval for Open-Domain Question Answering">Dense Passage Retrieval for Open-Domain Question Answering</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220511010217/ title="Multi-Style Generative Reading Comprehension">Multi-Style Generative Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220530102936/ title="Neural Machine Translation of Rare Words with Subword Units">Neural Machine Translation of Rare Words with Subword Units</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220523223206/ title="RoBERTa: A Robustly Optimized BERT Pretraining Approach">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220520124748/ title="Semi-Supervised Classification with Graph Convolutional Networks">Semi-Supervised Classification with Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220509110738/ title="Survey on graph embeddings and their applications to machine learning problems on graphs">Survey on graph embeddings and their applications to machine learning problems on graphs</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220514151839/ title="UnitedQA: A Hybrid Approach for Open Domain Question Answering">UnitedQA: A Hybrid Approach for Open Domain Question Answering</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202206/>2022.06</a><ul><li><a href=/akitenkrad-blog/posts/papers/202206/20220612105422/ title="Attributed Network Embedding for Learning in a Dynamic Environment">Attributed Network Embedding for Learning in a Dynamic Environment</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220608085622/ title="CodeBERT: A Pre-Trained Model for Programming and Natural Languages">CodeBERT: A Pre-Trained Model for Programming and Natural Languages</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220618223844/ title="High-order Proximity Preserved Embedding for Dynamic Networks">High-order Proximity Preserved Embedding for Dynamic Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220602171700/ title="S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension">S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202207/>2022.07</a><ul><li><a href=/akitenkrad-blog/posts/papers/202207/20220727145036/ title="Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions">Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions</a></li><li><a href=/akitenkrad-blog/posts/papers/202207/20220726163444/ title="Dynamic Network Embedding Survey">Dynamic Network Embedding Survey</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/papers/202208/20220802103319/ title="Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN">Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/papers/202209/20220909180827/ title="Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting">Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202305/>2023.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202305/20230513094804/ title="Improving Language Understanding by Generative Pre-Training">Improving Language Understanding by Generative Pre-Training</a></li><li><a href=/akitenkrad-blog/posts/papers/202305/20230514200241/ title="Language Models are Unsupervised Multitask Learners">Language Models are Unsupervised Multitask Learners</a></li></ul></li><li><a href=/akitenkrad-blog/posts/papers/202306/ title=2023.06>2023.06</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/>arXiv</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202307/>2023.07</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230701000000/ title="arXiv @ 2023.07.01">arXiv @ 2023.07.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230702000000/ title="arXiv @ 2023.07.02">arXiv @ 2023.07.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230703000000/ title="arXiv @ 2023.07.03">arXiv @ 2023.07.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230704000000/ title="arXiv @ 2023.07.04">arXiv @ 2023.07.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230705000000/ title="arXiv @ 2023.07.05">arXiv @ 2023.07.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230706000000/ title="arXiv @ 2023.07.06">arXiv @ 2023.07.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230707000000/ title="arXiv @ 2023.07.07">arXiv @ 2023.07.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230708000000/ title="arXiv @ 2023.07.08">arXiv @ 2023.07.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230709000000/ title="arXiv @ 2023.07.09">arXiv @ 2023.07.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230710000000/ title="arXiv @ 2023.07.10">arXiv @ 2023.07.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230711000000/ title="arXiv @ 2023.07.11">arXiv @ 2023.07.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230712000000/ title="arXiv @ 2023.07.12">arXiv @ 2023.07.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230713000000/ title="arXiv @ 2023.07.13">arXiv @ 2023.07.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230714000000/ title="arXiv @ 2023.07.14">arXiv @ 2023.07.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230715000000/ title="arXiv @ 2023.07.15">arXiv @ 2023.07.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230716000000/ title="arXiv @ 2023.07.16">arXiv @ 2023.07.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230717000000/ title="arXiv @ 2023.07.17">arXiv @ 2023.07.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230718000000/ title="arXiv @ 2023.07.18">arXiv @ 2023.07.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230719000000/ title="arXiv @ 2023.07.19">arXiv @ 2023.07.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230720000000/ title="arXiv @ 2023.07.20">arXiv @ 2023.07.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230721000000/ title="arXiv @ 2023.07.21">arXiv @ 2023.07.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230722000000/ title="arXiv @ 2023.07.22">arXiv @ 2023.07.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230723000000/ title="arXiv @ 2023.07.23">arXiv @ 2023.07.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230724000000/ title="arXiv @ 2023.07.24">arXiv @ 2023.07.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230725000000/ title="arXiv @ 2023.07.25">arXiv @ 2023.07.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230726000000/ title="arXiv @ 2023.07.26">arXiv @ 2023.07.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230727000000/ title="arXiv @ 2023.07.27">arXiv @ 2023.07.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230728000000/ title="arXiv @ 2023.07.28">arXiv @ 2023.07.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230729000000/ title="arXiv @ 2023.07.29">arXiv @ 2023.07.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230730000000/ title="arXiv @ 2023.07.30">arXiv @ 2023.07.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230731000000/ title="arXiv @ 2023.07.31">arXiv @ 2023.07.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202308/>2023.08</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230801000000/ title="arXiv @ 2023.08.01">arXiv @ 2023.08.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230802000000/ title="arXiv @ 2023.08.02">arXiv @ 2023.08.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230803000000/ title="arXiv @ 2023.08.03">arXiv @ 2023.08.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230804000000/ title="arXiv @ 2023.08.04">arXiv @ 2023.08.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230805000000/ title="arXiv @ 2023.08.05">arXiv @ 2023.08.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230806000000/ title="arXiv @ 2023.08.06">arXiv @ 2023.08.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230807000000/ title="arXiv @ 2023.08.07">arXiv @ 2023.08.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230808000000/ title="arXiv @ 2023.08.08">arXiv @ 2023.08.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230809000000/ title="arXiv @ 2023.08.09">arXiv @ 2023.08.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230810000000/ title="arXiv @ 2023.08.10">arXiv @ 2023.08.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230811000000/ title="arXiv @ 2023.08.11">arXiv @ 2023.08.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230812000000/ title="arXiv @ 2023.08.12">arXiv @ 2023.08.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230813000000/ title="arXiv @ 2023.08.13">arXiv @ 2023.08.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230814000000/ title="arXiv @ 2023.08.14">arXiv @ 2023.08.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230815000000/ title="arXiv @ 2023.08.15">arXiv @ 2023.08.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230816000000/ title="arXiv @ 2023.08.16">arXiv @ 2023.08.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230817000000/ title="arXiv @ 2023.08.17">arXiv @ 2023.08.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230818000000/ title="arXiv @ 2023.08.18">arXiv @ 2023.08.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230819000000/ title="arXiv @ 2023.08.19">arXiv @ 2023.08.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230820000000/ title="arXiv @ 2023.08.20">arXiv @ 2023.08.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230821000000/ title="arXiv @ 2023.08.21">arXiv @ 2023.08.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230822000000/ title="arXiv @ 2023.08.22">arXiv @ 2023.08.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230823000000/ title="arXiv @ 2023.08.23">arXiv @ 2023.08.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230824000000/ title="arXiv @ 2023.08.24">arXiv @ 2023.08.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230825000000/ title="arXiv @ 2023.08.25">arXiv @ 2023.08.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230826000000/ title="arXiv @ 2023.08.26">arXiv @ 2023.08.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230827000000/ title="arXiv @ 2023.08.27">arXiv @ 2023.08.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230828000000/ title="arXiv @ 2023.08.28">arXiv @ 2023.08.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230829000000/ title="arXiv @ 2023.08.29">arXiv @ 2023.08.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230830000000/ title="arXiv @ 2023.08.30">arXiv @ 2023.08.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230831000000/ title="arXiv @ 2023.08.31">arXiv @ 2023.08.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202309/>2023.09</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230901000000/ title="arXiv @ 2023.09.01">arXiv @ 2023.09.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230902000000/ title="arXiv @ 2023.09.02">arXiv @ 2023.09.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230903000000/ title="arXiv @ 2023.09.03">arXiv @ 2023.09.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230904000000/ title="arXiv @ 2023.09.04">arXiv @ 2023.09.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230905000000/ title="arXiv @ 2023.09.05">arXiv @ 2023.09.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230906000000/ title="arXiv @ 2023.09.06">arXiv @ 2023.09.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230907000000/ title="arXiv @ 2023.09.07">arXiv @ 2023.09.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230908000000/ title="arXiv @ 2023.09.08">arXiv @ 2023.09.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230909000000/ title="arXiv @ 2023.09.09">arXiv @ 2023.09.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230910000000/ title="arXiv @ 2023.09.10">arXiv @ 2023.09.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230911000000/ title="arXiv @ 2023.09.11">arXiv @ 2023.09.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230912000000/ title="arXiv @ 2023.09.12">arXiv @ 2023.09.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230913000000/ title="arXiv @ 2023.09.13">arXiv @ 2023.09.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230914000000/ title="arXiv @ 2023.09.14">arXiv @ 2023.09.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230915000000/ title="arXiv @ 2023.09.15">arXiv @ 2023.09.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230916000000/ title="arXiv @ 2023.09.16">arXiv @ 2023.09.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230917000000/ title="arXiv @ 2023.09.17">arXiv @ 2023.09.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230918000000/ title="arXiv @ 2023.09.18">arXiv @ 2023.09.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230919000000/ title="arXiv @ 2023.09.19">arXiv @ 2023.09.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230920000000/ title="arXiv @ 2023.09.20">arXiv @ 2023.09.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230921000000/ title="arXiv @ 2023.09.21">arXiv @ 2023.09.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230922000000/ title="arXiv @ 2023.09.22">arXiv @ 2023.09.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230923000000/ title="arXiv @ 2023.09.23">arXiv @ 2023.09.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230924000000/ title="arXiv @ 2023.09.24">arXiv @ 2023.09.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230925000000/ title="arXiv @ 2023.09.25">arXiv @ 2023.09.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230926000000/ title="arXiv @ 2023.09.26">arXiv @ 2023.09.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230927000000/ title="arXiv @ 2023.09.27">arXiv @ 2023.09.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230928000000/ title="arXiv @ 2023.09.28">arXiv @ 2023.09.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230929000000/ title="arXiv @ 2023.09.29">arXiv @ 2023.09.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230930000000/ title="arXiv @ 2023.09.30">arXiv @ 2023.09.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202310/>2023.1</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231001000000/ title="arXiv @ 2023.10.01">arXiv @ 2023.10.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231002000000/ title="arXiv @ 2023.10.02">arXiv @ 2023.10.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231003000000/ title="arXiv @ 2023.10.03">arXiv @ 2023.10.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231004000000/ title="arXiv @ 2023.10.04">arXiv @ 2023.10.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231005000000/ title="arXiv @ 2023.10.05">arXiv @ 2023.10.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231006000000/ title="arXiv @ 2023.10.06">arXiv @ 2023.10.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231007000000/ title="arXiv @ 2023.10.07">arXiv @ 2023.10.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231008000000/ title="arXiv @ 2023.10.08">arXiv @ 2023.10.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231009000000/ title="arXiv @ 2023.10.09">arXiv @ 2023.10.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231010000000/ title="arXiv @ 2023.10.10">arXiv @ 2023.10.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231011000000/ title="arXiv @ 2023.10.11">arXiv @ 2023.10.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231012000000/ title="arXiv @ 2023.10.12">arXiv @ 2023.10.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231013000000/ title="arXiv @ 2023.10.13">arXiv @ 2023.10.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231014000000/ title="arXiv @ 2023.10.14">arXiv @ 2023.10.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231015000000/ title="arXiv @ 2023.10.15">arXiv @ 2023.10.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231016000000/ title="arXiv @ 2023.10.16">arXiv @ 2023.10.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231017000000/ title="arXiv @ 2023.10.17">arXiv @ 2023.10.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231018000000/ title="arXiv @ 2023.10.18">arXiv @ 2023.10.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231019000000/ title="arXiv @ 2023.10.19">arXiv @ 2023.10.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231020000000/ title="arXiv @ 2023.10.20">arXiv @ 2023.10.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231021000000/ title="arXiv @ 2023.10.21">arXiv @ 2023.10.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231022000000/ title="arXiv @ 2023.10.22">arXiv @ 2023.10.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231023000000/ title="arXiv @ 2023.10.23">arXiv @ 2023.10.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231024000000/ title="arXiv @ 2023.10.24">arXiv @ 2023.10.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231025000000/ title="arXiv @ 2023.10.25">arXiv @ 2023.10.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231026000000/ title="arXiv @ 2023.10.26">arXiv @ 2023.10.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231027000000/ title="arXiv @ 2023.10.27">arXiv @ 2023.10.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231028000000/ title="arXiv @ 2023.10.28">arXiv @ 2023.10.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231029000000/ title="arXiv @ 2023.10.29">arXiv @ 2023.10.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231030000000/ title="arXiv @ 2023.10.30">arXiv @ 2023.10.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231031000000/ title="arXiv @ 2023.10.31">arXiv @ 2023.10.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202311/>2023.11</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231101000000/ title="arXiv @ 2023.11.01">arXiv @ 2023.11.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231102000000/ title="arXiv @ 2023.11.02">arXiv @ 2023.11.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231103000000/ title="arXiv @ 2023.11.03">arXiv @ 2023.11.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231104000000/ title="arXiv @ 2023.11.04">arXiv @ 2023.11.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231105000000/ title="arXiv @ 2023.11.05">arXiv @ 2023.11.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231106000000/ title="arXiv @ 2023.11.06">arXiv @ 2023.11.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231107000000/ title="arXiv @ 2023.11.07">arXiv @ 2023.11.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231108000000/ title="arXiv @ 2023.11.08">arXiv @ 2023.11.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231109000000/ title="arXiv @ 2023.11.09">arXiv @ 2023.11.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231110000000/ title="arXiv @ 2023.11.10">arXiv @ 2023.11.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231111000000/ title="arXiv @ 2023.11.11">arXiv @ 2023.11.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231112000000/ title="arXiv @ 2023.11.12">arXiv @ 2023.11.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231113000000/ title="arXiv @ 2023.11.13">arXiv @ 2023.11.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231114000000/ title="arXiv @ 2023.11.14">arXiv @ 2023.11.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231115000000/ title="arXiv @ 2023.11.15">arXiv @ 2023.11.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231116000000/ title="arXiv @ 2023.11.16">arXiv @ 2023.11.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231117000000/ title="arXiv @ 2023.11.17">arXiv @ 2023.11.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231118000000/ title="arXiv @ 2023.11.18">arXiv @ 2023.11.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231119000000/ title="arXiv @ 2023.11.19">arXiv @ 2023.11.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231120000000/ title="arXiv @ 2023.11.20">arXiv @ 2023.11.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231121000000/ title="arXiv @ 2023.11.21">arXiv @ 2023.11.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231122000000/ title="arXiv @ 2023.11.22">arXiv @ 2023.11.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231123000000/ title="arXiv @ 2023.11.23">arXiv @ 2023.11.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231124000000/ title="arXiv @ 2023.11.24">arXiv @ 2023.11.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231125000000/ title="arXiv @ 2023.11.25">arXiv @ 2023.11.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231126000000/ title="arXiv @ 2023.11.26">arXiv @ 2023.11.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231127000000/ title="arXiv @ 2023.11.27">arXiv @ 2023.11.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231128000000/ title="arXiv @ 2023.11.28">arXiv @ 2023.11.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231129000000/ title="arXiv @ 2023.11.29">arXiv @ 2023.11.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231130000000/ title="arXiv @ 2023.11.30">arXiv @ 2023.11.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202312/>2023.12</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231201000000/ title="arXiv @ 2023.12.01">arXiv @ 2023.12.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231202000000/ title="arXiv @ 2023.12.02">arXiv @ 2023.12.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231203000000/ title="arXiv @ 2023.12.03">arXiv @ 2023.12.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231204000000/ title="arXiv @ 2023.12.04">arXiv @ 2023.12.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231205000000/ title="arXiv @ 2023.12.05">arXiv @ 2023.12.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231206000000/ title="arXiv @ 2023.12.06">arXiv @ 2023.12.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231207000000/ title="arXiv @ 2023.12.07">arXiv @ 2023.12.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231208000000/ title="arXiv @ 2023.12.08">arXiv @ 2023.12.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231209000000/ title="arXiv @ 2023.12.09">arXiv @ 2023.12.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231210000000/ title="arXiv @ 2023.12.10">arXiv @ 2023.12.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231211000000/ title="arXiv @ 2023.12.11">arXiv @ 2023.12.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231212000000/ title="arXiv @ 2023.12.12">arXiv @ 2023.12.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231213000000/ title="arXiv @ 2023.12.13">arXiv @ 2023.12.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231214000000/ title="arXiv @ 2023.12.14">arXiv @ 2023.12.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231215000000/ title="arXiv @ 2023.12.15">arXiv @ 2023.12.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231216000000/ title="arXiv @ 2023.12.16">arXiv @ 2023.12.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231217000000/ title="arXiv @ 2023.12.17">arXiv @ 2023.12.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231218000000/ title="arXiv @ 2023.12.18">arXiv @ 2023.12.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231219000000/ title="arXiv @ 2023.12.19">arXiv @ 2023.12.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231220000000/ title="arXiv @ 2023.12.20">arXiv @ 2023.12.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231221000000/ title="arXiv @ 2023.12.21">arXiv @ 2023.12.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231222000000/ title="arXiv @ 2023.12.22">arXiv @ 2023.12.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231223000000/ title="arXiv @ 2023.12.23">arXiv @ 2023.12.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231224000000/ title="arXiv @ 2023.12.24">arXiv @ 2023.12.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231225000000/ title="arXiv @ 2023.12.25">arXiv @ 2023.12.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231226000000/ title="arXiv @ 2023.12.26">arXiv @ 2023.12.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231227000000/ title="arXiv @ 2023.12.27">arXiv @ 2023.12.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231228000000/ title="arXiv @ 2023.12.28">arXiv @ 2023.12.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231229000000/ title="arXiv @ 2023.12.29">arXiv @ 2023.12.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231230000000/ title="arXiv @ 2023.12.30">arXiv @ 2023.12.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231231000000/ title="arXiv @ 2023.12.31">arXiv @ 2023.12.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202401/>2024.01</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240101000000/ title="arXiv @ 2024.01.01">arXiv @ 2024.01.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240102000000/ title="arXiv @ 2024.01.02">arXiv @ 2024.01.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240103000000/ title="arXiv @ 2024.01.03">arXiv @ 2024.01.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240104000000/ title="arXiv @ 2024.01.04">arXiv @ 2024.01.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240105000000/ title="arXiv @ 2024.01.05">arXiv @ 2024.01.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240106000000/ title="arXiv @ 2024.01.06">arXiv @ 2024.01.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240107000000/ title="arXiv @ 2024.01.07">arXiv @ 2024.01.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240108000000/ title="arXiv @ 2024.01.08">arXiv @ 2024.01.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240109000000/ title="arXiv @ 2024.01.09">arXiv @ 2024.01.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240110000000/ title="arXiv @ 2024.01.10">arXiv @ 2024.01.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240111000000/ title="arXiv @ 2024.01.11">arXiv @ 2024.01.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240112000000/ title="arXiv @ 2024.01.12">arXiv @ 2024.01.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240113000000/ title="arXiv @ 2024.01.13">arXiv @ 2024.01.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240114000000/ title="arXiv @ 2024.01.14">arXiv @ 2024.01.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240115000000/ title="arXiv @ 2024.01.15">arXiv @ 2024.01.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240116000000/ title="arXiv @ 2024.01.16">arXiv @ 2024.01.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240117000000/ title="arXiv @ 2024.01.17">arXiv @ 2024.01.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240118000000/ title="arXiv @ 2024.01.18">arXiv @ 2024.01.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240119000000/ title="arXiv @ 2024.01.19">arXiv @ 2024.01.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240120000000/ title="arXiv @ 2024.01.20">arXiv @ 2024.01.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240121000000/ title="arXiv @ 2024.01.21">arXiv @ 2024.01.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240122000000/ title="arXiv @ 2024.01.22">arXiv @ 2024.01.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240123000000/ title="arXiv @ 2024.01.23">arXiv @ 2024.01.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240124000000/ title="arXiv @ 2024.01.24">arXiv @ 2024.01.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240125000000/ title="arXiv @ 2024.01.25">arXiv @ 2024.01.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240126000000/ title="arXiv @ 2024.01.26">arXiv @ 2024.01.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240127000000/ title="arXiv @ 2024.01.27">arXiv @ 2024.01.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240128000000/ title="arXiv @ 2024.01.28">arXiv @ 2024.01.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240129000000/ title="arXiv @ 2024.01.29">arXiv @ 2024.01.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240130000000/ title="arXiv @ 2024.01.30">arXiv @ 2024.01.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240131000000/ title="arXiv @ 2024.01.31">arXiv @ 2024.01.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202402/>2024.02</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240201000000/ title="arXiv @ 2024.02.01">arXiv @ 2024.02.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240202000000/ title="arXiv @ 2024.02.02">arXiv @ 2024.02.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240203000000/ title="arXiv @ 2024.02.03">arXiv @ 2024.02.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240204000000/ title="arXiv @ 2024.02.04">arXiv @ 2024.02.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240205000000/ title="arXiv @ 2024.02.05">arXiv @ 2024.02.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240206000000/ title="arXiv @ 2024.02.06">arXiv @ 2024.02.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/ title="arXiv @ 2024.02.07">arXiv @ 2024.02.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/ title="arXiv @ 2024.02.08">arXiv @ 2024.02.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/ title="arXiv @ 2024.02.09">arXiv @ 2024.02.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/ title="arXiv @ 2024.02.10">arXiv @ 2024.02.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240211000000/ title="arXiv @ 2024.02.11">arXiv @ 2024.02.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/ title="arXiv @ 2024.02.12">arXiv @ 2024.02.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/ title="arXiv @ 2024.02.13">arXiv @ 2024.02.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/ title="arXiv @ 2024.02.14">arXiv @ 2024.02.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/ title="arXiv @ 2024.02.15">arXiv @ 2024.02.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240216000000/ title="arXiv @ 2024.02.16">arXiv @ 2024.02.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/ title="arXiv @ 2024.02.17">arXiv @ 2024.02.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240218000000/ title="arXiv @ 2024.02.18">arXiv @ 2024.02.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/ title="arXiv @ 2024.02.19">arXiv @ 2024.02.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/ title="arXiv @ 2024.02.20">arXiv @ 2024.02.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/ title="arXiv @ 2024.02.21">arXiv @ 2024.02.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/ title="arXiv @ 2024.02.22">arXiv @ 2024.02.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/ title="arXiv @ 2024.02.23">arXiv @ 2024.02.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/ title="arXiv @ 2024.02.24">arXiv @ 2024.02.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/ title="arXiv @ 2024.02.25">arXiv @ 2024.02.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/ title="arXiv @ 2024.02.26">arXiv @ 2024.02.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/ title="arXiv @ 2024.02.27">arXiv @ 2024.02.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/ title="arXiv @ 2024.02.28">arXiv @ 2024.02.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/ title="arXiv @ 2024.02.29">arXiv @ 2024.02.29</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/202403/>2024.03</a><ul class=active><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/ title="arXiv @ 2024.03.01">arXiv @ 2024.03.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/ title="arXiv @ 2024.03.02">arXiv @ 2024.03.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/ title="arXiv @ 2024.03.03">arXiv @ 2024.03.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240304000000/ title="arXiv @ 2024.03.04">arXiv @ 2024.03.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/ title="arXiv @ 2024.03.05">arXiv @ 2024.03.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/ title="arXiv @ 2024.03.06">arXiv @ 2024.03.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/ title="arXiv @ 2024.03.07">arXiv @ 2024.03.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/ title="arXiv @ 2024.03.08">arXiv @ 2024.03.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/ title="arXiv @ 2024.03.09">arXiv @ 2024.03.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/ title="arXiv @ 2024.03.10">arXiv @ 2024.03.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/ title="arXiv @ 2024.03.11">arXiv @ 2024.03.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/ title="arXiv @ 2024.03.12">arXiv @ 2024.03.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/ title="arXiv @ 2024.03.13">arXiv @ 2024.03.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/ title="arXiv @ 2024.03.14">arXiv @ 2024.03.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/ title="arXiv @ 2024.03.15">arXiv @ 2024.03.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/ title="arXiv @ 2024.03.16">arXiv @ 2024.03.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/ title="arXiv @ 2024.03.17">arXiv @ 2024.03.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/ title="arXiv @ 2024.03.18">arXiv @ 2024.03.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240319000000/ title="arXiv @ 2024.03.19">arXiv @ 2024.03.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/ title="arXiv @ 2024.03.20">arXiv @ 2024.03.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/ title="arXiv @ 2024.03.21">arXiv @ 2024.03.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/ title="arXiv @ 2024.03.22">arXiv @ 2024.03.22</a></li><li><a class=active href=/akitenkrad-blog/posts/arxiv/202403/20240323000000/ title="arXiv @ 2024.03.23">arXiv @ 2024.03.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/ title="arXiv @ 2024.03.24">arXiv @ 2024.03.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240325000000/ title="arXiv @ 2024.03.25">arXiv @ 2024.03.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240326000000/ title="arXiv @ 2024.03.26">arXiv @ 2024.03.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240327000000/ title="arXiv @ 2024.03.27">arXiv @ 2024.03.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240328000000/ title="arXiv @ 2024.03.28">arXiv @ 2024.03.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240329000000/ title="arXiv @ 2024.03.29">arXiv @ 2024.03.29</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/algorithms/>Algorithms</a><ul><li><a href=/akitenkrad-blog/posts/algorithms/bandit-algorithm-basic/ title="Bandit Algorithm Basic">Bandit Algorithm Basic</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/conference/>Conference</a><ul><li><a href=/akitenkrad-blog/posts/conference/acl/ title="Annual Meeting of the Association for Computational Linguistics">Annual Meeting of the Association for Computational Linguistics</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/>Figures</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/figures/202208/20220822092748/ title=1ヶ月当たりの実労働時間の推移>1ヶ月当たりの実労働時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220813115200/ title=コンビニエンスストアの店舗数の推移>コンビニエンスストアの店舗数の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220821113322/ title=地域別1世帯当たり1ヶ月間の支出額の推移>地域別1世帯当たり1ヶ月間の支出額の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220828111836/ title="夫婦別子供の有無別生活時間 (2016)">夫婦別子供の有無別生活時間 (2016)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220827103615/ title=家族類型の変遷>家族類型の変遷</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220815131016/ title="年齢別大学院入学者数 (2021)">年齢別大学院入学者数 (2021)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220818122628/ title=情報サービス業における企業特殊的人的資本（名目）の推移>情報サービス業における企業特殊的人的資本（名目）の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220824095127/ title=有業者の社会生活における活動の変化>有業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220814175022/ title=東京23区の乗用車保有台数>東京23区の乗用車保有台数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220817121117/ title="業種別企業特殊的人的資本 (2018)">業種別企業特殊的人的資本 (2018)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220825094111/ title=無業者の社会生活における活動の変化>無業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220823104100/ title=睡眠時間の推移>睡眠時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220826102652/ title=総人口の推移>総人口の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220820225528/ title=都道府県別国公立別学校数・学生数>都道府県別国公立別学校数・学生数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220816120845/ title="都道府県別消費者物価指数/教育（全国平均=100）">都道府県別消費者物価指数/教育（全国平均=100）</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220812173814/ title=金沢市の夏の月平均気温の遷移>金沢市の夏の月平均気温の遷移</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/figures/202209/20220907122906/ title="ACL-2022 WordCloud">ACL-2022 WordCloud</a></li></ul></li></ul></li><li><a href=/akitenkrad-blog/posts/latex/ title="Latex Mathematics Syntax Guide">Latex Mathematics Syntax Guide</a></li><li><a href=/akitenkrad-blog/posts/markdown/ title="Markdown Sample">Markdown Sample</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/akitenkrad-blog/posts/arxiv/202403/20240323000000/hero.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_120x120_fit_box_3.png alt="Author Image"><h5 class=author-name></h5><p>Saturday, Mar 23, 2024</p></div><div class=title><h1>arXiv @ 2024.03.23</h1></div><div class=taxonomy-terms><ul><li class=rounded><a href=/akitenkrad-blog/tags/arxiv class="btn, btn-sm">arXiv</a></li><li class=rounded><a href=/akitenkrad-blog/tags/published2024 class="btn, btn-sm">2024</a></li></ul></div><div class=post-content id=post-content><figure style=border:none;width:100%;display:flex;justify-content:center><iframe src=pie.html width=900 height=620 style=border:none></iframe></figure><h2 id=primary-categories>Primary Categories</h2><ul><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240323000000/#astro-phim-1>astro-ph.IM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240323000000/#cond-matdis-nn-1>cond-mat.dis-nn (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240323000000/#csai-5>cs.AI (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240323000000/#csar-1>cs.AR (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240323000000/#csce-2>cs.CE (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240323000000/#cscl-52>cs.CL (52)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240323000000/#cscr-5>cs.CR (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240323000000/#cscv-104>cs.CV (104)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240323000000/#cscy-6>cs.CY (6)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240323000000/#csdb-1>cs.DB (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240323000000/#csdc-1>cs.DC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240323000000/#csds-2>cs.DS (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240323000000/#cshc-4>cs.HC (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240323000000/#csir-3>cs.IR (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240323000000/#csit-1>cs.IT (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240323000000/#cslg-35>cs.LG (35)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240323000000/#csne-8>cs.NE (8)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240323000000/#csni-1>cs.NI (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240323000000/#csro-20>cs.RO (20)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240323000000/#cssd-5>cs.SD (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240323000000/#csse-3>cs.SE (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240323000000/#cssi-4>cs.SI (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240323000000/#eessas-3>eess.AS (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240323000000/#eessiv-7>eess.IV (7)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240323000000/#eesssp-1>eess.SP (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240323000000/#eesssy-14>eess.SY (14)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240323000000/#hep-ex-1>hep-ex (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240323000000/#mathco-1>math.CO (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240323000000/#mathna-6>math.NA (6)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240323000000/#mathoc-1>math.OC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240323000000/#physicsmed-ph-1>physics.med-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240323000000/#q-biobm-1>q-bio.BM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240323000000/#q-bioqm-1>q-bio.QM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240323000000/#quant-ph-4>quant-ph (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240323000000/#statml-6>stat.ML (6)</a></li></ul><h2 id=keywords>Keywords</h2><table border=1 class=dataframe><thead><tr style=text-align:right><th>keyword</th><th>cs.CL</th><th>cs.CV</th><th>cs.LG</th><th>cs.RO</th><th>eess.SY</th></tr></thead><tbody><tr><td>Active Learning</td><td>1</td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>Adversarial Attack</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Adversarial Learning</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Anomaly Detection</td><td></td><td>3</td><td>2</td><td></td><td></td></tr><tr><td>Autoencoder</td><td></td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>Automatic Speech Recognition</td><td>4</td><td></td><td></td><td></td><td></td></tr><tr><td>BART</td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>BERT</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>BERTScore</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Benchmarking</td><td>12</td><td>38</td><td>3</td><td>1</td><td>2</td></tr><tr><td>Black Box</td><td>1</td><td>2</td><td>1</td><td>1</td><td></td></tr><tr><td>Chain-of-thought Prompt</td><td>2</td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>ChatGPT</td><td>3</td><td></td><td></td><td></td><td></td></tr><tr><td>Chatbot</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Clustering</td><td></td><td>3</td><td>1</td><td></td><td></td></tr><tr><td>Common-sense Reasoning</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Continual Learning</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Continuous Time</td><td></td><td></td><td></td><td>2</td><td>2</td></tr><tr><td>Contrastive Learning</td><td></td><td>4</td><td></td><td></td><td></td></tr><tr><td>ControlNet</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Convolution</td><td></td><td>4</td><td>1</td><td></td><td></td></tr><tr><td>Convolutional Neural Network</td><td></td><td>8</td><td>1</td><td></td><td></td></tr><tr><td>Counter-factual</td><td></td><td></td><td>1</td><td></td><td>1</td></tr><tr><td>Data Augmentation</td><td>1</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Dense Retrieval</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Diffusion Model</td><td></td><td>12</td><td>3</td><td></td><td></td></tr><tr><td>Discrete Time</td><td></td><td></td><td></td><td></td><td>2</td></tr><tr><td>Distribution Shift</td><td></td><td>2</td><td>2</td><td>2</td><td></td></tr><tr><td>Domain Adaptation</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Face Recognition</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Fairness</td><td>1</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Federated Learning</td><td></td><td>1</td><td>3</td><td></td><td></td></tr><tr><td>Few-shot</td><td>3</td><td>3</td><td>1</td><td></td><td></td></tr><tr><td>Fine-tuning</td><td>15</td><td>10</td><td>4</td><td></td><td></td></tr><tr><td>Foundation Model</td><td>1</td><td>7</td><td>1</td><td></td><td></td></tr><tr><td>GPT</td><td>3</td><td>3</td><td>2</td><td></td><td></td></tr><tr><td>GPT-3</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>GPT-3.5</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>GPT-4</td><td>3</td><td></td><td>1</td><td></td><td></td></tr><tr><td>GPT-4 turbo</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Generative AI</td><td>1</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Generative Adversarial Network</td><td></td><td>2</td><td>1</td><td></td><td></td></tr><tr><td>Geometry</td><td></td><td>7</td><td></td><td>1</td><td></td></tr><tr><td>Graph</td><td>2</td><td>5</td><td>7</td><td>3</td><td>1</td></tr><tr><td>Graph Attention Networks</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Graph Neural Network</td><td></td><td>1</td><td>2</td><td></td><td></td></tr><tr><td>GraphSAGE</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Grounding</td><td>1</td><td>2</td><td></td><td>1</td><td></td></tr><tr><td>High-Resource</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Image2text</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>In-context Learning</td><td>5</td><td>1</td><td></td><td></td><td>2</td></tr><tr><td>Information Retrieval</td><td>2</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Instruction Following</td><td>3</td><td></td><td></td><td></td><td></td></tr><tr><td>Instruction Tuning</td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>Knowledge Distillation</td><td>5</td><td>5</td><td></td><td>2</td><td></td></tr><tr><td>Knowledge Graph</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Knowledge Transfer</td><td></td><td>3</td><td>1</td><td></td><td></td></tr><tr><td>LLaMA</td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>Large Language Model</td><td>58</td><td>12</td><td>8</td><td>1</td><td>1</td></tr><tr><td>Low-Resource</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Machine Unlearning</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Markov Decision Process</td><td></td><td></td><td>2</td><td></td><td></td></tr><tr><td>Mathematical Reasoning</td><td>1</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Message-Passing</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Meta Learning</td><td></td><td></td><td></td><td></td><td>1</td></tr><tr><td>Model Compression</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Multi-modal</td><td>13</td><td>14</td><td></td><td>4</td><td></td></tr><tr><td>Multiple Instance Learning</td><td></td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>Mutual Information</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Named Entity Recognition</td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>Natural Language Generation</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Natural Language Inference</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Neural Machine Translation</td><td>5</td><td></td><td></td><td></td><td></td></tr><tr><td>Node Classification</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Object Detection</td><td></td><td>4</td><td>1</td><td>1</td><td></td></tr><tr><td>Open-Domain Question Answering</td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>Out-of-distribution</td><td></td><td>2</td><td>2</td><td></td><td></td></tr><tr><td>Perplexity</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Pre-trained Language Model</td><td>1</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Probabilistic Model</td><td></td><td></td><td>2</td><td>1</td><td></td></tr><tr><td>Prompt</td><td>11</td><td>12</td><td>4</td><td></td><td></td></tr><tr><td>Prompt Learning</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Pruning</td><td></td><td>2</td><td>2</td><td></td><td></td></tr><tr><td>Quantization</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Question Answering</td><td>9</td><td>5</td><td></td><td></td><td></td></tr><tr><td>Reasoning</td><td>8</td><td>6</td><td></td><td>1</td><td></td></tr><tr><td>Recommendation</td><td></td><td></td><td>2</td><td></td><td></td></tr><tr><td>Reinforcement Learning</td><td>3</td><td></td><td>6</td><td>4</td><td>2</td></tr><tr><td>Reinforcement Learning from Human Feedback</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Relation Extraction</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Representation Learning</td><td></td><td>2</td><td>2</td><td></td><td></td></tr><tr><td>Retrieval-Augmented Generation</td><td>5</td><td></td><td>2</td><td></td><td></td></tr><tr><td>Scaling Law</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Self-Attention</td><td></td><td>4</td><td>1</td><td></td><td></td></tr><tr><td>Self-supervised Learning</td><td></td><td>8</td><td>1</td><td></td><td></td></tr><tr><td>Semi-Supervised Learning</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Sentiment Analysis</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Simulation</td><td></td><td>4</td><td>2</td><td>7</td><td>6</td></tr><tr><td>Simulator</td><td></td><td>4</td><td>2</td><td>7</td><td>6</td></tr><tr><td>Stemming</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Stochastic Gradient Descent</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Style Transfer</td><td></td><td>3</td><td></td><td></td><td></td></tr><tr><td>Summarization</td><td>4</td><td></td><td>1</td><td>1</td><td></td></tr><tr><td>Supervised Learning</td><td>4</td><td>9</td><td>3</td><td></td><td></td></tr><tr><td>Text Classification</td><td>2</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Text Embedding</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Text2image</td><td></td><td>3</td><td>1</td><td>1</td><td></td></tr><tr><td>Transfer Learning</td><td></td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>Transformer</td><td>5</td><td>16</td><td>2</td><td>1</td><td></td></tr><tr><td>Unsupervised Learning</td><td>2</td><td>9</td><td>2</td><td></td><td></td></tr><tr><td>Vision Transformer</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Vision-and-Language</td><td>1</td><td>7</td><td></td><td>1</td><td></td></tr><tr><td>Visual Question Answering</td><td></td><td>4</td><td></td><td></td><td></td></tr><tr><td>Weakly Supervised Learning</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Weakly-supervised Learning</td><td>1</td><td>2</td><td>1</td><td></td><td></td></tr><tr><td>Word Embedding</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Zero-shot</td><td>3</td><td>14</td><td></td><td>2</td><td></td></tr><tr><td>Zero-shot Learning</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>human-in-the-loop</td><td>1</td><td></td><td></td><td>2</td><td></td></tr></tbody></table><script>$(function(){$("table").addClass("keyword-table table-bordered border-success"),$("table thead").addClass("sticky-top"),$("table tbody td").css("text-align","")})</script><h2 id=cscl-52>cs.CL (52)</h2><h3 id=152--1312-building-accurate-translation-tailored-llms-with-language-aware-instruction-tuning-changtong-zan-et-al-2024>(1/52 | 1/312) Building Accurate Translation-Tailored LLMs with Language Aware Instruction Tuning (Changtong Zan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Changtong Zan, Liang Ding, Li Shen, Yibing Zhen, Weifeng Liu, Dacheng Tao. (2024)<br><strong>Building Accurate Translation-Tailored LLMs with Language Aware Instruction Tuning</strong><br><button class=copy-to-clipboard title="Building Accurate Translation-Tailored LLMs with Language Aware Instruction Tuning" index=1>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-1 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 133<br>Keywords: Benchmarking, Few-shot, Fine-tuning, Low-Resource, Supervised Learning, Zero-shot, LLaMA, Instruction Following, In-context Learning, In-context Learning, Instruction Tuning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14399v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14399v1.pdf filename=2403.14399v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Translation-tailored <b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> exhibit remarkable translation capabilities, even competing with <b>supervised-trained</b> commercial translation systems. However, off-target translation remains an unsolved problem, especially for <b>low-resource</b> languages, hindering us from developing accurate <b>LLMs-based</b> translation models. To mitigate the off-target translation problem and enhance the performance of <b>LLMs</b> on translation, recent works have either designed advanced <b>prompting</b> strategies to highlight the functionality of translation <b>instructions</b> <b>or</b> exploited the <b>in-context</b> <b>learning</b> ability of <b>LLMs</b> by feeding <b>few-shot</b> demonstrations. However, these methods essentially do not improve <b>LLM&rsquo;s</b> ability to follow translation <b>instructions,</b> <b>especially</b> the language direction information. In this work, we design a two-stage <b>fine-tuning</b> algorithm to improve the <b>instruction-following</b> <b>ability</b> (especially the translation direction) of <b>LLMs.</b> Specifically, we first tune <b>LLMs</b> with the maximum likelihood estimation loss on the translation dataset to elicit the basic translation capabilities. In the second stage, we construct <b>instruction-conflicting</b> <b>samples</b> by randomly replacing the translation directions with a wrong one within the <b>instruction,</b> <b>and</b> then introduce an extra unlikelihood loss to learn those samples. Experiments on IWSLT and WMT <b>benchmarks</b> upon the <b>LLaMA</b> model spanning 16 <b>zero-shot</b> directions show that, compared to the competitive baseline &ndash; translation-finetuned <b>LLama,</b> our method could effectively reduce the off-target translation ratio (averagely -53.3%), thus improving translation quality with average +5.7 SacreBLEU and +16.4 BLEURT. Analysis shows that our method could preserve the model&rsquo;s general task performance on AlpacaEval. Code and models will be released at \url{https://github.com/alphadl/LanguageAware_Tuning}.</p></p class="citation"></blockquote><h3 id=252--2312-llm-based-extraction-of-contradictions-from-patents-stefan-trapp-et-al-2024>(2/52 | 2/312) LLM-based Extraction of Contradictions from Patents (Stefan Trapp et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Stefan Trapp, Joachim Warschat. (2024)<br><strong>LLM-based Extraction of Contradictions from Patents</strong><br><button class=copy-to-clipboard title="LLM-based Extraction of Contradictions from Patents" index=2>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-2 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: I-2-7; H-3-1, cs-CL, cs.CL<br>Keyword Score: 110<br>Keywords: Dense Retrieval, Fine-tuning, BERT, GPT, GPT-4, Transformer, Question Answering, Large Language Model, Large Language Model, Prompt, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14258v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14258v1.pdf filename=2403.14258v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Already since the 1950s TRIZ shows that patents and the technical contradictions they solve are an important source of inspiration for the development of innovative products. However, TRIZ is a heuristic based on a historic patent analysis and does not make use of the ever-increasing number of latest technological solutions in current patents. Because of the huge number of patents, their length, and, last but not least, their complexity there is a need for modern patent retrieval and patent analysis to go beyond keyword-oriented methods. Recent advances in patent retrieval and analysis mainly focus on <b>dense</b> <b>vectors</b> based on neural AI <b>Transformer</b> language models like Google <b>BERT.</b> They are, for example, used for <b>dense</b> <b>retrieval,</b> <b>question</b> <b>answering</b> or <b>summarization</b> and key concept extraction. A research focus within the methods for patent <b>summarization</b> and key concept extraction are generic inventive concepts respectively TRIZ concepts like problems, solutions, advantage of invention, parameters, and contradictions. Succeeding rule-based approaches, <b>finetuned</b> <b>BERT-like</b> language models for sentence-wise classification represent the state-of-the-art of inventive concept extraction. While they work comparatively well for basic concepts like problems or solutions, contradictions - as a more complex abstraction - remain a challenge for these models. This paper goes one step further, as it presents a method to extract TRIZ contradictions from patent texts based on <b>Prompt</b> Engineering using a generative <b>Large</b> <b>Language</b> <b>Model</b> <b>(LLM),</b> namely OpenAI&rsquo;s <b>GPT-4.</b> Contradiction detection, sentence extraction, contradiction <b>summarization,</b> parameter extraction and assignment to the 39 abstract TRIZ engineering parameters are all performed in a single <b>prompt</b> using the LangChain framework. Our results show that &ldquo;off-the-shelf&rdquo; <b>GPT-4</b> is a serious alternative to existing approaches.</p></p class="citation"></blockquote><h3 id=352--3312-a-chain-of-thought-prompting-approach-with-llms-for-evaluating-students-formative-assessment-responses-in-science-clayton-cohn-et-al-2024>(3/52 | 3/312) A Chain-of-Thought Prompting Approach with LLMs for Evaluating Students&rsquo; Formative Assessment Responses in Science (Clayton Cohn et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Clayton Cohn, Nicole Hutchins, Tuan Le, Gautam Biswas. (2024)<br><strong>A Chain-of-Thought Prompting Approach with LLMs for Evaluating Students&rsquo; Formative Assessment Responses in Science</strong><br><button class=copy-to-clipboard title="A Chain-of-Thought Prompting Approach with LLMs for Evaluating Students' Formative Assessment Responses in Science" index=3>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-3 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 100<br>Keywords: Active Learning, Few-shot, human-in-the-loop, GPT, GPT-4, Reasoning, Chain-of-thought Prompt, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14565v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14565v1.pdf filename=2403.14565v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper explores the use of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to score and explain short-answer assessments in K-12 science. While existing methods can score more structured math and computer science assessments, they often do not provide explanations for the scores. Our study focuses on employing <b>GPT-4</b> for automated assessment in middle school Earth Science, combining <b>few-shot</b> and <b>active</b> <b>learning</b> with <b>chain-of-thought</b> <b>reasoning.</b> Using a <b>human-in-the-loop</b> approach, we successfully score and provide meaningful explanations for formative assessment responses. A systematic analysis of our method&rsquo;s pros and cons sheds light on the potential for <b>human-in-the-loop</b> techniques to enhance automated grading for open-ended science assessments.</p></p class="citation"></blockquote><h3 id=452--4312-fit-rag-black-box-rag-with-factual-information-and-token-reduction-yuren-mao-et-al-2024>(4/52 | 4/312) FIT-RAG: Black-Box RAG with Factual Information and Token Reduction (Yuren Mao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuren Mao, Xuemei Dong, Wenyi Xu, Yunjun Gao, Bin Wei, Ying Zhang. (2024)<br><strong>FIT-RAG: Black-Box RAG with Factual Information and Token Reduction</strong><br><button class=copy-to-clipboard title="FIT-RAG: Black-Box RAG with Factual Information and Token Reduction" index=4>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-4 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-IR, cs.CL<br>Keyword Score: 95<br>Keywords: Black Box, Fine-tuning, Fine-tuning, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Open-Domain Question Answering, Question Answering, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14374v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14374v1.pdf filename=2403.14374v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Due to the extraordinarily <b>large</b> <b>number</b> <b>of</b> parameters, <b>fine-tuning</b> <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> to update long-tail or out-of-date knowledge is impractical in lots of applications. To avoid <b>fine-tuning,</b> we can alternatively treat a <b>LLM</b> as a <b>black-box</b> <b>(i.e.,</b> freeze the parameters of the <b>LLM)</b> and augment it with a <b>Retrieval-Augmented</b> <b>Generation</b> <b>(RAG)</b> system, namely <b>black-box</b> <b>RAG.</b> Recently, <b>black-box</b> <b>RAG</b> has achieved success in knowledge-intensive tasks and has gained much attention. Existing <b>black-box</b> <b>RAG</b> methods typically <b>fine-tune</b> the retriever to cater to <b>LLMs&rsquo;</b> preferences and concatenate all the retrieved documents as the input, which suffers from two issues: (1) Ignorance of Factual Information. The <b>LLM</b> preferred documents may not contain the factual information for the given <b>question,</b> <b>which</b> can mislead the retriever and hurt the effectiveness of <b>black-box</b> <b>RAG;</b> (2) Waste of Tokens. Simply concatenating all the retrieved documents brings <b>large</b> <b>amounts</b> <b>of</b> unnecessary tokens for <b>LLMs,</b> which degenerates the efficiency of <b>black-box</b> <b>RAG.</b> To address these issues, this paper proposes a novel <b>black-box</b> <b>RAG</b> framework which utilizes the factual information in the <b>retrieval</b> <b>and</b> <b>reduces</b> the number of tokens for augmentation, dubbed FIT-RAG. FIT-RAG utilizes the factual information by constructing a bi-label document scorer. Besides, it reduces the tokens by introducing a self-knowledge recognizer and a sub-document-level token reducer. FIT-RAG achieves both superior effectiveness and efficiency, which is validated by extensive experiments across three <b>open-domain</b> <b>question-answering</b> <b>datasets:</b> TriviaQA, NQ and PopQA. FIT-RAG can improve the answering accuracy of Llama2-13B-Chat by 14.3% on TriviaQA, 19.9% on NQ and 27.5% on PopQA, respectively. Furthermore, it can save approximately half of the tokens on average across the three datasets.</p></p class="citation"></blockquote><h3 id=552--5312-mmidr-teaching-large-language-model-to-interpret-multimodal-misinformation-via-knowledge-distillation-longzheng-wang-et-al-2024>(5/52 | 5/312) MMIDR: Teaching Large Language Model to Interpret Multimodal Misinformation via Knowledge Distillation (Longzheng Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Longzheng Wang, Xiaohan Xu, Lei Zhang, Jiarui Lu, Yongxiu Xu, Hongbo Xu, Chuang Zhang. (2024)<br><strong>MMIDR: Teaching Large Language Model to Interpret Multimodal Misinformation via Knowledge Distillation</strong><br><button class=copy-to-clipboard title="MMIDR: Teaching Large Language Model to Interpret Multimodal Misinformation via Knowledge Distillation" index=5>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-5 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 86<br>Keywords: Data Augmentation, Knowledge Distillation, Knowledge Distillation, Knowledge Distillation, Multi-modal, Multi-modal, Instruction Following, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14171v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14171v1.pdf filename=2403.14171v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Automatic detection of <b>multimodal</b> misinformation has gained a widespread attention recently. However, the potential of powerful <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> for <b>multimodal</b> misinformation detection remains underexplored. Besides, how to teach <b>LLMs</b> to interpret <b>multimodal</b> misinformation in cost-effective and accessible way is still an open question. To address that, we propose MMIDR, a framework designed to teach <b>LLMs</b> in providing fluent and high-quality textual explanations for their decision-making process of <b>multimodal</b> misinformation. To convert <b>multimodal</b> misinformation into an appropriate <b>instruction-following</b> <b>format,</b> we present a <b>data</b> <b>augmentation</b> perspective and pipeline. This pipeline consists of a visual information processing module and an evidence retrieval module. Subsequently, we <b>prompt</b> the proprietary <b>LLMs</b> with processed contents to extract rationales for interpreting the authenticity of <b>multimodal</b> misinformation. Furthermore, we design an efficient <b>knowledge</b> <b>distillation</b> approach to <b>distill</b> the capability of proprietary <b>LLMs</b> in explaining <b>multimodal</b> misinformation into open-source <b>LLMs.</b> To explore several research questions regarding the performance of <b>LLMs</b> in <b>multimodal</b> misinformation detection tasks, we construct an <b>instruction-following</b> <b>multimodal</b> misinformation dataset and conduct comprehensive experiments. The experimental findings reveal that our MMIDR exhibits sufficient detection performance and possesses the capacity to provide compelling rationales to support its assessments.</p></p class="citation"></blockquote><h3 id=652--6312-gtbls-generating-tables-from-text-by-conditional-question-answering-anirudh-sundar-et-al-2024>(6/52 | 6/312) gTBLS: Generating Tables from Text by Conditional Question Answering (Anirudh Sundar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anirudh Sundar, Christopher Richardson, Larry Heck. (2024)<br><strong>gTBLS: Generating Tables from Text by Conditional Question Answering</strong><br><button class=copy-to-clipboard title="gTBLS: Generating Tables from Text by Conditional Question Answering" index=6>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-6 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-IR, cs-LG, cs.CL<br>Keyword Score: 80<br>Keywords: Fine-tuning, Fine-tuning, Knowledge Distillation, Zero-shot, Transformer, Question Answering, BERTScore, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14457v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14457v1.pdf filename=2403.14457v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Distilling</b> <b>large,</b> <b>unstructured</b> <b>text</b> into a structured, condensed form such as tables is an open research problem. One of the primary challenges in automatically generating tables is ensuring their syntactic validity. Prior approaches address this challenge by including additional parameters in the <b>Transformer&rsquo;s</b> attention mechanism to attend to specific rows and column headers. In contrast to this single-stage method, this paper presents a two-stage approach called Generative Tables (gTBLS). The first stage infers table structure (row and column headers) from the text. The second stage formulates <b>questions</b> <b>using</b> these headers and <b>fine-tunes</b> a causal language model to answer them. Furthermore, the gTBLS approach is amenable to the utilization of pre-trained <b>Large</b> <b>Language</b> <b>Models</b> in a <b>zero-shot</b> configuration, presenting a solution for table generation in situations where <b>fine-tuning</b> is not feasible. gTBLS improves prior approaches by up to 10% in <b>BERTScore</b> on the table construction task and up to 20% on the table content generation task of the E2E, WikiTableText, WikiBio, and RotoWire datasets.</p></p class="citation"></blockquote><h3 id=752--7312-k-act2emo-korean-commonsense-knowledge-graph-for-indirect-emotional-expression-kyuhee-kim-et-al-2024>(7/52 | 7/312) K-Act2Emo: Korean Commonsense Knowledge Graph for Indirect Emotional Expression (Kyuhee Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kyuhee Kim, Surin Lee, Sangah Lee. (2024)<br><strong>K-Act2Emo: Korean Commonsense Knowledge Graph for Indirect Emotional Expression</strong><br><button class=copy-to-clipboard title="K-Act2Emo: Korean Commonsense Knowledge Graph for Indirect Emotional Expression" index=7>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-7 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 78<br>Keywords: Graph, Fine-tuning, Knowledge Graph, BART, GPT, GPT-4, GPT-4 turbo, Reasoning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14253v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14253v2.pdf filename=2403.14253v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In many literary texts, emotions are indirectly conveyed through descriptions of actions, facial expressions, and appearances, necessitating emotion inference for narrative understanding. In this paper, we introduce K-Act2Emo, a Korean commonsense <b>knowledge</b> <b>graph</b> (CSKG) comprising 1,900 indirect emotional expressions and the emotions inferable from them. We categorize <b>reasoning</b> types into inferences in positive situations, inferences in negative situations, and inferences when expressions do not serve as emotional cues. Unlike existing CSKGs, K-Act2Emo specializes in emotional contexts, and experimental results validate its effectiveness for training emotion inference models. Significantly, the <b>BART-based</b> <b>knowledge</b> <b>model</b> <b>fine-tuned</b> with K-Act2Emo outperforms various existing Korean <b>large</b> <b>language</b> <b>models,</b> achieving performance levels comparable to <b>GPT-4</b> <b>Turbo.</b></p></p class="citation"></blockquote><h3 id=852--8312-chainlm-empowering-large-language-models-with-improved-chain-of-thought-prompting-xiaoxue-cheng-et-al-2024>(8/52 | 8/312) ChainLM: Empowering Large Language Models with Improved Chain-of-Thought Prompting (Xiaoxue Cheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaoxue Cheng, Junyi Li, Wayne Xin Zhao, Ji-Rong Wen. (2024)<br><strong>ChainLM: Empowering Large Language Models with Improved Chain-of-Thought Prompting</strong><br><button class=copy-to-clipboard title="ChainLM: Empowering Large Language Models with Improved Chain-of-Thought Prompting" index=8>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-8 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: Fine-tuning, LLaMA, Reasoning, Chain-of-thought Prompt, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14312v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14312v1.pdf filename=2403.14312v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Chain-of-Thought</b> <b>(CoT)</b> <b>prompting</b> can enhance the <b>reasoning</b> capabilities of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> establishing itself as a primary approach to solving complex <b>reasoning</b> tasks. Existing CoT synthesis approaches usually focus on simpler <b>reasoning</b> tasks and thus result in low-quality and inconsistent CoT <b>prompts.</b> In response to this challenge, we present an empirical investigation of CoT <b>prompting</b> and introduce CoTGenius, a novel framework designed for the automatic generation of superior CoT <b>prompts.</b> CoTGenius is developed based on three major evolution strategies, i.e., complicate, diversify, and specify-alongside two filtering mechanisms: evolutionary success judgement and correctness verification. We further employ CoTGenius to create an extensive CoT dataset, and subsequently <b>fine-tune</b> the <b>Llama</b> 2-Chat 7B and 13B models on this dataset. We call the resulting model ChainLM. To deal with the cumulative error issue in <b>reasoning</b> steps, we propose a step-level debating method, wherein multiple debaters discuss each <b>reasoning</b> step to arrive at the correct answer. Extensive experiments demonstrate that our ChainLM models exhibit enhanced proficiency in addressing a spectrum of complex <b>reasoning</b> problems compared to existing models. In addition, we conduct an in-depth analysis of the impact of data categories within CoTGenius on the model performance. We release our dataset and code at <a href=https://github.com/RUCAIBox/ChainLM>https://github.com/RUCAIBox/ChainLM</a>.</p></p class="citation"></blockquote><h3 id=952--9312-layoutllm-large-language-model-instruction-tuning-for-visually-rich-document-understanding-masato-fujitake-2024>(9/52 | 9/312) LayoutLLM: Large Language Model Instruction Tuning for Visually Rich Document Understanding (Masato Fujitake, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Masato Fujitake. (2024)<br><strong>LayoutLLM: Large Language Model Instruction Tuning for Visually Rich Document Understanding</strong><br><button class=copy-to-clipboard title="LayoutLLM: Large Language Model Instruction Tuning for Visually Rich Document Understanding" index=9>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-9 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-CV, cs-LG, cs.CL<br>Keyword Score: 66<br>Keywords: Fine-tuning, Fine-tuning, Multi-modal, Multi-modal, Information Retrieval, Instruction Tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14252v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14252v1.pdf filename=2403.14252v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper proposes LayoutLLM, a more flexible document analysis method for understanding imaged documents. Visually Rich Document Understanding tasks, such as document image classification and <b>information</b> <b>extraction,</b> have gained significant attention due to their importance. Existing methods have been developed to enhance document comprehension by incorporating pre-training awareness of images, text, and layout structure. However, these methods require <b>fine-tuning</b> for each task and dataset, and the models are expensive to train and operate. To overcome this limitation, we propose a new LayoutLLM that integrates these with <b>large-scale</b> <b>language</b> <b>models</b> <b>(LLMs).</b> By leveraging the strengths of existing research in document image understanding and <b>LLMs&rsquo;</b> superior language understanding capabilities, the proposed model, <b>fine-tuned</b> with <b>multimodal</b> <b>instruction</b> <b>datasets,</b> performs an understanding of document images in a single model. Our experiments demonstrate improvement over the baseline model in various document analysis tasks.</p></p class="citation"></blockquote><h3 id=1052--10312-autore-document-level-relation-extraction-with-large-language-models-xue-lilong-et-al-2024>(10/52 | 10/312) AutoRE: Document-Level Relation Extraction with Large Language Models (Xue Lilong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xue Lilong, Zhang Dan, Dong Yuxiao, Tang Jie. (2024)<br><strong>AutoRE: Document-Level Relation Extraction with Large Language Models</strong><br><button class=copy-to-clipboard title="AutoRE: Document-Level Relation Extraction with Large Language Models" index=10>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-10 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Fine-tuning, Information Retrieval, Relation Extraction, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14888v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14888v1.pdf filename=2403.14888v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have demonstrated exceptional abilities in comprehending and generating text, motivating numerous researchers to utilize them for <b>Information</b> <b>Extraction</b> (IE) purposes, including <b>Relation</b> <b>Extraction</b> (RE). Nonetheless, most existing methods are predominantly designed for Sentence-level <b>Relation</b> <b>Extraction</b> (SentRE) tasks, which typically encompass a restricted set of <b>relations</b> <b>and</b> triplet facts within a single sentence. Furthermore, certain approaches resort to treating <b>relations</b> <b>as</b> candidate choices integrated into <b>prompt</b> templates, leading to inefficient processing and suboptimal performance when tackling Document-Level <b>Relation</b> <b>Extraction</b> (DocRE) tasks, which entail handling multiple <b>relations</b> <b>and</b> triplet facts distributed across a given document, posing distinct challenges. To overcome these limitations, we introduce AutoRE, an end-to-end DocRE model that adopts a novel RE extraction paradigm named RHF <b>(Relation-Head-Facts).</b> <b>Unlike</b> existing approaches, AutoRE does not rely on the assumption of known <b>relation</b> <b>options,</b> making it more reflective of real-world scenarios. Additionally, we have developed an easily extensible RE framework using a Parameters Efficient Fine Tuning (PEFT) algorithm (QLoRA). Our experiments on the RE-DocRED dataset showcase AutoRE&rsquo;s best performance, achieving state-of-the-art results, surpassing TAG by 10.03% and 9.03% respectively on the dev and test set.</p></p class="citation"></blockquote><h3 id=1152--11312-from-large-to-tiny-distilling-and-refining-mathematical-expertise-for-math-word-problems-with-weakly-supervision-qingwen-lin-et-al-2024>(11/52 | 11/312) From Large to Tiny: Distilling and Refining Mathematical Expertise for Math Word Problems with Weakly Supervision (Qingwen Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qingwen Lin, Boyan Xu, Zhengting Huang, Ruichu Cai. (2024)<br><strong>From Large to Tiny: Distilling and Refining Mathematical Expertise for Math Word Problems with Weakly Supervision</strong><br><button class=copy-to-clipboard title="From Large to Tiny: Distilling and Refining Mathematical Expertise for Math Word Problems with Weakly Supervision" index=11>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-11 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Knowledge Distillation, Supervised Learning, Weakly-supervised Learning, ChatGPT, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14390v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14390v1.pdf filename=2403.14390v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Addressing the challenge of high annotation costs in solving Math Word Problems (MWPs) through full supervision with intermediate equations, recent works have proposed weakly <b>supervised</b> task settings that rely solely on the final answer as a <b>supervised</b> signal. Existing leading approaches typically employ various search techniques to infer intermediate equations, but cannot ensure their semantic consistency with natural language descriptions. The rise of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> like <b>ChatGPT</b> has opened up new possibilities for addressing MWPs directly. However, the computational demands of <b>LLMs</b> make them less than ideal for use in settings where resources are tight. In light of these challenges, we introduce an innovative two-stage framework that adeptly transfers mathematical Expertise from <b>large</b> <b>to</b> <b>tiny</b> language models. In \emph{Distillation Stage}, we propose a series of extraction processes that satisfy the properties of MWPs to <b>distill</b> mathematical knowledge from <b>LLMs</b> to construct problem-equation pairs required for <b>supervised</b> training. In \emph{Refinement Stage}, Due to Knowledge <b>distilling</b> method cannot guarantee the full utilization of all data, we further utilize the unsuccessfully searched data effectively by Knowledge Refine method. Finally, We train a small model using <b>distilled</b> data generated through two-stage methods. As our method fully leverages the semantic understanding capabilities during the searching &lsquo;problem-equation&rsquo; pair, it demonstrates significantly improved performance on the Math23K and Weak12K datasets compared to existing small model methods, while maintaining a much lower computational cost than <b>ChatGPT.</b></p></p class="citation"></blockquote><h3 id=1252--12312-reinforcement-learning-from-reflective-feedback-rlrf-aligning-and-improving-llms-via-fine-grained-self-reflection-kyungjae-lee-et-al-2024>(12/52 | 12/312) Reinforcement Learning from Reflective Feedback (RLRF): Aligning and Improving LLMs via Fine-Grained Self-Reflection (Kyungjae Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kyungjae Lee, Dasol Hwang, Sunghyun Park, Youngsoo Jang, Moontae Lee. (2024)<br><strong>Reinforcement Learning from Reflective Feedback (RLRF): Aligning and Improving LLMs via Fine-Grained Self-Reflection</strong><br><button class=copy-to-clipboard title="Reinforcement Learning from Reflective Feedback (RLRF): Aligning and Improving LLMs via Fine-Grained Self-Reflection" index=12>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-12 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Fine-tuning, Reinforcement Learning, Reinforcement Learning from Human Feedback, Mathematical Reasoning, Reasoning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14238v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14238v1.pdf filename=2403.14238v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite the promise of <b>RLHF</b> in aligning <b>LLMs</b> with human preferences, it often leads to superficial alignment, prioritizing stylistic changes over improving downstream performance of <b>LLMs.</b> Underspecified preferences could obscure directions to align the models. Lacking exploration restricts identification of desirable outputs to improve the models. To overcome these challenges, we propose a novel framework: <b>Reinforcement</b> <b>Learning</b> from Reflective Feedback (RLRF), which leverages fine-grained feedback based on detailed criteria to improve the core capabilities of <b>LLMs.</b> RLRF employs a self-reflection mechanism to systematically explore and refine <b>LLM</b> responses, then <b>fine-tuning</b> the models via a RL algorithm along with promising responses. Our experiments across Just-Eval, Factuality, and <b>Mathematical</b> <b>Reasoning</b> demonstrate the efficacy and transformative potential of RLRF beyond superficial surface-level adjustment.</p></p class="citation"></blockquote><h3 id=1352--13312-chatgpt-alternative-solutions-large-language-models-survey-hanieh-alipour-et-al-2024>(13/52 | 13/312) ChatGPT Alternative Solutions: Large Language Models Survey (Hanieh Alipour et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hanieh Alipour, Nick Pendar, Kohinoor Roy. (2024)<br><strong>ChatGPT Alternative Solutions: Large Language Models Survey</strong><br><button class=copy-to-clipboard title="ChatGPT Alternative Solutions: Large Language Models Survey" index=13>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-13 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 56<br>Keywords: Benchmarking, Benchmarking, Generative AI, ChatGPT, Chatbot, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14469v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14469v1.pdf filename=2403.14469v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent times, the grandeur of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> has not only shone in the realm of natural language processing but has also cast its brilliance across a vast array of applications. This remarkable display of <b>LLM</b> capabilities has ignited a surge in research contributions within this domain, spanning a diverse spectrum of topics. These contributions encompass advancements in neural network architecture, context length enhancements, model alignment, training datasets, <b>benchmarking,</b> efficiency improvements, and more. Recent years have witnessed a dynamic synergy between academia and industry, propelling the field of <b>LLM</b> research to new heights. A notable milestone in this journey is the introduction of <b>ChatGPT,</b> a powerful AI <b>chatbot</b> grounded in <b>LLMs,</b> which has garnered widespread societal attention. The evolving technology of <b>LLMs</b> has begun to reshape the landscape of the entire AI community, promising a revolutionary shift in the way we create and employ AI algorithms. Given this swift-paced technical evolution, our survey embarks on a journey to encapsulate the recent strides made in the world of <b>LLMs.</b> Through an exploration of the background, key discoveries, and prevailing methodologies, we offer an up-to-the-minute review of the literature. By examining multiple <b>LLM</b> models, our paper not only presents a comprehensive overview but also charts a course that identifies existing challenges and points toward potential future research trajectories. This survey furnishes a well-rounded perspective on the current state of <b>generative</b> <b>AI,</b> shedding light on opportunities for further exploration, enhancement, and innovation.</p></p class="citation"></blockquote><h3 id=1452--14312-a-multimodal-approach-to-device-directed-speech-detection-with-large-language-models-dominik-wagner-et-al-2024>(14/52 | 14/312) A Multimodal Approach to Device-Directed Speech Detection with Large Language Models (Dominik Wagner et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dominik Wagner, Alexander Churchill, Siddharth Sigtia, Panayiotis Georgiou, Matt Mirsamadi, Aarshee Mishra, Erik Marchi. (2024)<br><strong>A Multimodal Approach to Device-Directed Speech Detection with Large Language Models</strong><br><button class=copy-to-clipboard title="A Multimodal Approach to Device-Directed Speech Detection with Large Language Models" index=14>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-14 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL, eess-AS<br>Keyword Score: 56<br>Keywords: Multi-modal, Multi-modal, Automatic Speech Recognition, Automatic Speech Recognition, Automatic Speech Recognition, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14438v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14438v2.pdf filename=2403.14438v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Interactions with virtual assistants typically start with a predefined trigger phrase followed by the user command. To make interactions with the assistant more intuitive, we explore whether it is feasible to drop the requirement that users must begin each command with a trigger phrase. We explore this task in three ways: First, we train classifiers using only acoustic information obtained from the audio waveform. Second, we take the decoder outputs of an <b>automatic</b> <b>speech</b> <b>recognition</b> <b>(ASR)</b> system, such as 1-best hypotheses, as input features to a <b>large</b> <b>language</b> <b>model</b> <b>(LLM).</b> Finally, we explore a <b>multimodal</b> system that combines acoustic and lexical features, as well as <b>ASR</b> decoder signals in an <b>LLM.</b> Using <b>multimodal</b> information yields relative equal-error-rate improvements over text-only and audio-only models of up to 39% and 61%. Increasing the size of the <b>LLM</b> and training with low-rank adaption leads to further relative EER reductions of up to 18% on our dataset.</p></p class="citation"></blockquote><h3 id=1552--15312-benchmarking-chinese-commonsense-reasoning-of-llms-from-chinese-specifics-to-reasoning-memorization-correlations-jiaxing-sun-et-al-2024>(15/52 | 15/312) Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations (Jiaxing Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiaxing Sun, Weiquan Huang, Jiang Wu, Chenya Gu, Wei Li, Songyang Zhang, Hang Yan, Conghui He. (2024)<br><strong>Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations</strong><br><button class=copy-to-clipboard title="Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations" index=15>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-15 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 56<br>Keywords: Benchmarking, Benchmarking, Common-sense Reasoning, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14112v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14112v1.pdf filename=2403.14112v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce CHARM, the first <b>benchmark</b> for comprehensively and in-depth evaluating the <b>commonsense</b> <b>reasoning</b> ability of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> in Chinese, which covers both globally known and Chinese-specific <b>commonsense.</b> <b>We</b> evaluated 7 English and 12 Chinese-oriented <b>LLMs</b> on CHARM, employing 5 representative <b>prompt</b> strategies for improving <b>LLMs&rsquo;</b> <b>reasoning</b> ability, such as Chain-of-Thought. Our findings indicate that the <b>LLM&rsquo;s</b> language orientation and the task&rsquo;s domain influence the effectiveness of the <b>prompt</b> strategy, which enriches previous research findings. We built closely-interconnected <b>reasoning</b> and memorization tasks, and found that some <b>LLMs</b> struggle with memorizing Chinese <b>commonsense,</b> <b>affecting</b> their <b>reasoning</b> ability, while others show differences in <b>reasoning</b> despite similar memorization performance. We also evaluated the <b>LLMs&rsquo;</b> memorization-independent <b>reasoning</b> abilities and analyzed the typical errors. Our study precisely identified the <b>LLMs&rsquo;</b> strengths and weaknesses, providing the clear direction for optimization. It can also serve as a reference for studies in other fields. We will release CHARM at <a href=https://github.com/opendatalab/CHARM>https://github.com/opendatalab/CHARM</a> .</p></p class="citation"></blockquote><h3 id=1652--16312-improving-the-robustness-of-large-language-models-via-consistency-alignment-yukun-zhao-et-al-2024>(16/52 | 16/312) Improving the Robustness of Large Language Models via Consistency Alignment (Yukun Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yukun Zhao, Lingyong Yan, Weiwei Sun, Guoliang Xing, Shuaiqiang Wang, Chong Meng, Zhicong Cheng, Zhaochun Ren, Dawei Yin. (2024)<br><strong>Improving the Robustness of Large Language Models via Consistency Alignment</strong><br><button class=copy-to-clipboard title="Improving the Robustness of Large Language Models via Consistency Alignment" index=16>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-16 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Fine-tuning, Supervised Learning, Instruction Following, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14221v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14221v2.pdf filename=2403.14221v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have shown tremendous success in following user <b>instructions</b> <b>and</b> generating helpful responses. Nevertheless, their robustness is still far from optimal, as they may generate significantly inconsistent responses due to minor changes in the verbalized <b>instructions.</b> <b>Recent</b> literature has explored this inconsistency issue, highlighting the importance of continued improvement in the robustness of response generation. However, systematic analysis and solutions are still lacking. In this paper, we quantitatively define the inconsistency problem and propose a two-stage training framework consisting of <b>instruction-augmented</b> <b>supervised</b> <b>fine-tuning</b> and consistency alignment training. The first stage helps a model generalize on following <b>instructions</b> <b>via</b> similar <b>instruction</b> <b>augmentations.</b> In the second stage, we improve the diversity and help the model understand which responses are more aligned with human expectations by differentiating subtle differences in similar responses. The training process is accomplished by self-rewards inferred from the trained model at the first stage without referring to external human preference resources. We conduct extensive experiments on recent publicly available <b>LLMs</b> on <b>instruction-following</b> <b>tasks</b> and demonstrate the effectiveness of our training framework.</p></p class="citation"></blockquote><h3 id=1752--17312-context-quality-matters-in-training-fusion-in-decoder-for-extractive-open-domain-question-answering-kosuke-akimoto-et-al-2024>(17/52 | 17/312) Context Quality Matters in Training Fusion-in-Decoder for Extractive Open-Domain Question Answering (Kosuke Akimoto et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kosuke Akimoto, Kunihiro Takeoka, Masafumi Oyamada. (2024)<br><strong>Context Quality Matters in Training Fusion-in-Decoder for Extractive Open-Domain Question Answering</strong><br><button class=copy-to-clipboard title="Context Quality Matters in Training Fusion-in-Decoder for Extractive Open-Domain Question Answering" index=17>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-17 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Retrieval-Augmented Generation, Retrieval-Augmented Generation, Open-Domain Question Answering, Question Answering, In-context Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14197v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14197v1.pdf filename=2403.14197v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Retrieval-augmented</b> <b>generation</b> <b>models</b> augment knowledge encoded in a language model by providing additional relevant external knowledge (context) during generation. Although it has been shown that the quantity and quality of context impact the performance of <b>retrieval-augmented</b> <b>generation</b> <b>models</b> during inference, limited research explores how these characteristics affect model training. This paper explores how context quantity and quality during model training affect the performance of Fusion-in-Decoder (FiD), the state-of-the-art <b>retrieval-augmented</b> <b>generation</b> <b>model,</b> in extractive <b>open-domain</b> <b>question</b> <b>answering</b> tasks. Experimental results suggest that FiD models overfit to context quality during training and show suboptimal performance when evaluated on different context quality. Through the experimental results, we also reveal FiD models trained with different context quality have different cross-attention distribution patterns. Specifically, as context quality during training increases, FiD models tend to attend more uniformly to each passage in context. Finally, based on these observations, we propose a method to mitigate overfitting to specific context quality by introducing bias to the cross-attention distribution, which we demonstrate to be effective in improving the performance of FiD models on different context quality.</p></p class="citation"></blockquote><h3 id=1852--18312-from-handcrafted-features-to-llms-a-brief-survey-for-machine-translation-quality-estimation-haofei-zhao-et-al-2024>(18/52 | 18/312) From Handcrafted Features to LLMs: A Brief Survey for Machine Translation Quality Estimation (Haofei Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haofei Zhao, Yilun Liu, Shimin Tao, Weibin Meng, Yimeng Chen, Xiang Geng, Chang Su, Min Zhang, Hao Yang. (2024)<br><strong>From Handcrafted Features to LLMs: A Brief Survey for Machine Translation Quality Estimation</strong><br><button class=copy-to-clipboard title="From Handcrafted Features to LLMs: A Brief Survey for Machine Translation Quality Estimation" index=18>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-18 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Neural Machine Translation, Neural Machine Translation, Large Language Model, Large Language Model, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14118v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14118v1.pdf filename=2403.14118v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Machine</b> <b>Translation</b> Quality Estimation (MTQE) is the task of estimating the quality of <b>machine-translated</b> <b>text</b> in real time without the need for reference translations, which is of great importance for the development of <b>MT.</b> After two decades of evolution, QE has yielded a wealth of results. This article provides a comprehensive overview of QE datasets, annotation methods, shared tasks, methodologies, challenges, and future research directions. It begins with an introduction to the background and significance of QE, followed by an explanation of the concepts and evaluation metrics for word-level QE, sentence-level QE, document-level QE, and explainable QE. The paper categorizes the methods developed throughout the history of QE into those based on handcrafted features, deep learning, and <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> with a further division of deep learning-based methods into classic deep learning and those incorporating <b>pre-trained</b> <b>language</b> <b>models</b> (LMs). Additionally, the article details the advantages and limitations of each method and offers a straightforward comparison of different approaches. Finally, the paper discusses the current challenges in QE research and provides an outlook on future research directions.</p></p class="citation"></blockquote><h3 id=1952--19312-comparing-plausibility-estimates-in-base-and-instruction-tuned-large-language-models-carina-kauf-et-al-2024>(19/52 | 19/312) Comparing Plausibility Estimates in Base and Instruction-Tuned Large Language Models (Carina Kauf et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Carina Kauf, Emmanuele Chersoni, Alessandro Lenci, Evelina Fedorenko, Anna A. Ivanova. (2024)<br><strong>Comparing Plausibility Estimates in Base and Instruction-Tuned Large Language Models</strong><br><button class=copy-to-clipboard title="Comparing Plausibility Estimates in Base and Instruction-Tuned Large Language Models" index=19>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-19 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Zero-shot, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14859v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14859v1.pdf filename=2403.14859v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Instruction-tuned <b>LLMs</b> can respond to explicit queries formulated as <b>prompts,</b> which greatly facilitates interaction with human users. However, <b>prompt-based</b> approaches might not always be able to tap into the wealth of implicit knowledge acquired by <b>LLMs</b> during pre-training. This paper presents a comprehensive study of ways to evaluate semantic plausibility in <b>LLMs.</b> We compare base and instruction-tuned <b>LLM</b> performance on an English sentence plausibility task via (a) explicit <b>prompting</b> and (b) implicit estimation via direct readout of the probabilities models assign to strings. Experiment 1 shows that, across model architectures and plausibility datasets, (i) log likelihood ($\textit{LL}$) scores are the most reliable indicator of sentence plausibility, with <b>zero-shot</b> <b>prompting</b> yielding inconsistent and typically poor results; (ii) $\textit{LL}$-based performance is still inferior to human performance; (iii) instruction-tuned models have worse $\textit{LL}$-based performance than base models. In Experiment 2, we show that $\textit{LL}$ scores across models are modulated by context in the expected way, showing high performance on three metrics of context-sensitive plausibility and providing a direct match to explicit human plausibility judgments. Overall, $\textit{LL}$ estimates remain a more reliable measure of plausibility in <b>LLMs</b> than direct <b>prompting.</b></p></p class="citation"></blockquote><h3 id=2052--20312-the-opportunities-and-risks-of-large-language-models-in-mental-health-hannah-r-lawrence-et-al-2024>(20/52 | 20/312) The opportunities and risks of large language models in mental health (Hannah R. Lawrence et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hannah R. Lawrence, Renee A. Schneider, Susan B. Rubin, Maja J. Mataric, Daniel J. McDuff, Megan Jones Bell. (2024)<br><strong>The opportunities and risks of large language models in mental health</strong><br><button class=copy-to-clipboard title="The opportunities and risks of large language models in mental health" index=20>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-20 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-CY, cs-HC, cs-LG, cs.CL<br>Keyword Score: 40<br>Keywords: Fine-tuning, Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14814v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14814v2.pdf filename=2403.14814v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Global rates of mental health concerns are rising and there is increasing realization that existing models of mental healthcare will not adequately expand to meet the demand. With the emergence of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> has come great optimism regarding their promise to create novel, <b>large-scale</b> <b>solutions</b> <b>to</b> support mental health. Despite their nascence, <b>LLMs</b> have already been applied to mental health-related tasks. In this review, we <b>summarize</b> the extant literature on efforts to use <b>LLMs</b> to provide mental health education, assessment, and intervention and highlight key opportunities for positive impact in each area. We then highlight risks associated with <b>LLMs</b> application to mental health and encourage adoption of strategies to mitigate these risks. The urgent need for mental health support must be balanced with responsible development, testing, and deployment of mental health <b>LLMs.</b> Especially critical is ensuring that mental health <b>LLMs</b> are <b>fine-tuned</b> for mental health, enhance mental health equity, adhere to ethical standards, and that people, including those with lived experience with mental health concerns, are involved in all stages from development through deployment. Prioritizing these efforts will minimize potential harms to mental health and maximize the likelihood that <b>LLMs</b> will positively impact mental health globally.</p></p class="citation"></blockquote><h3 id=2152--21312-adaptive-rag-learning-to-adapt-retrieval-augmented-large-language-models-through-question-complexity-soyeong-jeong-et-al-2024>(21/52 | 21/312) Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity (Soyeong Jeong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Soyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Ju Hwang, Jong C. Park. (2024)<br><strong>Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity</strong><br><button class=copy-to-clipboard title="Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity" index=21>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-21 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Question Answering, Question Answering, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14403v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14403v1.pdf filename=2403.14403v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Retrieval-Augmented <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> which incorporate the non-parametric knowledge from external knowledge bases into <b>LLMs,</b> have emerged as a promising approach to enhancing response accuracy in several tasks, such as <b>Question-Answering</b> <b>(QA).</b> However, even though there are various approaches dealing with queries of different complexities, they either handle simple queries with unnecessary computational overhead or fail to adequately address complex multi-step queries; yet, not all user requests fall into only one of the simple or complex categories. In this work, we propose a novel adaptive <b>QA</b> framework, that can dynamically select the most suitable strategy for (retrieval-augmented) <b>LLMs</b> from the simplest to the most sophisticated ones based on the query complexity. Also, this selection process is operationalized with a classifier, which is a smaller LM trained to predict the complexity level of incoming queries with automatically collected labels, obtained from actual predicted outcomes of models and inherent inductive biases in datasets. This approach offers a balanced strategy, seamlessly adapting between the iterative and single-step retrieval-augmented <b>LLMs,</b> as well as the no-retrieval methods, in response to a range of query complexities. We validate our model on a set of open-domain <b>QA</b> datasets, covering multiple query complexities, and show that ours enhances the overall efficiency and accuracy of <b>QA</b> systems, compared to relevant baselines including the adaptive retrieval approaches. Code is available at: <a href=https://github.com/starsuzi/Adaptive-RAG>https://github.com/starsuzi/Adaptive-RAG</a>.</p></p class="citation"></blockquote><h3 id=2252--22312-sequence-to-sequence-language-models-for-character-and-emotion-detection-in-dream-narratives-gustave-cortal-2024>(22/52 | 22/312) Sequence-to-Sequence Language Models for Character and Emotion Detection in Dream Narratives (Gustave Cortal, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gustave Cortal. (2024)<br><strong>Sequence-to-Sequence Language Models for Character and Emotion Detection in Dream Narratives</strong><br><button class=copy-to-clipboard title="Sequence-to-Sequence Language Models for Character and Emotion Detection in Dream Narratives" index=22>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-22 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Supervised Learning, In-context Learning, In-context Learning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15486v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15486v1.pdf filename=2403.15486v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The study of dreams has been central to understanding human (un)consciousness, cognition, and culture for centuries. Analyzing dreams quantitatively depends on labor-intensive, manual annotation of dream narratives. We automate this process through a natural language sequence-to-sequence generation framework. This paper presents the first study on character and emotion detection in the English portion of the open DreamBank corpus of dream narratives. Our results show that language models can effectively address this complex task. To get insight into prediction performance, we evaluate the impact of model size, prediction order of characters, and the consideration of proper names and character traits. We compare our approach with a <b>large</b> <b>language</b> <b>model</b> using <b>in-context</b> <b>learning.</b> Our <b>supervised</b> models perform better while having 28 times fewer parameters. Our model and its generated annotations are made publicly available.</p></p class="citation"></blockquote><h3 id=2352--23312-extracting-emotion-phrases-from-tweets-using-bart-mahdi-rezapour-2024>(23/52 | 23/312) Extracting Emotion Phrases from Tweets using BART (Mahdi Rezapour, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mahdi Rezapour. (2024)<br><strong>Extracting Emotion Phrases from Tweets using BART</strong><br><button class=copy-to-clipboard title="Extracting Emotion Phrases from Tweets using BART" index=23>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-23 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL, stat-AP<br>Keyword Score: 40<br>Keywords: BART, Transformer, Question Answering, Sentiment Analysis<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14050v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14050v2.pdf filename=2403.14050v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Sentiment</b> <b>analysis</b> is a natural language processing task that aims to identify and extract the emotional aspects of a text. However, many existing <b>sentiment</b> <b>analysis</b> methods primarily classify the overall polarity of a text, overlooking the specific phrases that convey <b>sentiment.</b> <b>In</b> this paper, we applied an approach to <b>sentiment</b> <b>analysis</b> based on a <b>question-answering</b> <b>framework.</b> Our approach leverages the power of Bidirectional Autoregressive <b>Transformer</b> <b>(BART),</b> a pre-trained sequence-to-sequence model, to extract a phrase from a given text that amplifies a given <b>sentiment</b> <b>polarity.</b> We create a natural language <b>question</b> <b>that</b> identifies the specific emotion to extract and then guide <b>BART</b> to pay attention to the relevant emotional cues in the text. We use a classifier within <b>BART</b> to predict the start and end positions of the answer span within the text, which helps to identify the precise boundaries of the extracted emotion phrase. Our approach offers several advantages over most <b>sentiment</b> <b>analysis</b> studies, including capturing the complete context and meaning of the text and extracting precise token spans that highlight the intended <b>sentiment.</b> <b>We</b> achieved an end loss of 87% and Jaccard score of 0.61.</p></p class="citation"></blockquote><h3 id=2452--24312-lexicon-level-contrastive-visual-grounding-improves-language-modeling-chengxu-zhuang-et-al-2024>(24/52 | 24/312) Lexicon-Level Contrastive Visual-Grounding Improves Language Modeling (Chengxu Zhuang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chengxu Zhuang, Evelina Fedorenko, Jacob Andreas. (2024)<br><strong>Lexicon-Level Contrastive Visual-Grounding Improves Language Modeling</strong><br><button class=copy-to-clipboard title="Lexicon-Level Contrastive Visual-Grounding Improves Language Modeling" index=24>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-24 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 39<br>Keywords: Benchmarking, Multi-modal, Multi-modal, Grounding, Perplexity, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14551v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14551v1.pdf filename=2403.14551v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Today&rsquo;s most accurate language models are trained on orders of magnitude more language data than human language learners receive - but with no supervision from other sensory modalities that play a crucial role in human learning. Can we make LMs&rsquo; representations and predictions more accurate (and more human-like) with more ecologically plausible supervision? This paper describes LexiContrastive <b>Grounding</b> (LCG), a grounded language learning procedure that leverages visual supervision to improve textual representations. LexiContrastive <b>Grounding</b> combines a next token prediction strategy with a contrastive visual <b>grounding</b> objective, focusing on early-layer representations that encode lexical information. Across multiple word-learning and sentence-understanding <b>benchmarks,</b> LexiContrastive <b>Grounding</b> not only outperforms standard language-only models in learning efficiency, but also improves upon <b>vision-and-language</b> learning procedures including CLIP, GIT, Flamingo, and Vokenization. Moreover, LexiContrastive <b>Grounding</b> improves <b>perplexity</b> by around 5% on multiple language modeling tasks. This work underscores the potential of incorporating visual <b>grounding</b> into language models, aligning more closely with the <b>multimodal</b> nature of human language acquisition.</p></p class="citation"></blockquote><h3 id=2552--25312-open-source-conversational-llms-do-not-know-most-spanish-words-javier-conde-et-al-2024>(25/52 | 25/312) Open Source Conversational LLMs do not know most Spanish words (Javier Conde et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Javier Conde, Miguel González, Nina Melero, Raquel Ferrando, Gonzalo Martínez, Elena Merino-Gómez, José Alberto Hernández, Pedro Reviriego. (2024)<br><strong>Open Source Conversational LLMs do not know most Spanish words</strong><br><button class=copy-to-clipboard title="Open Source Conversational LLMs do not know most Spanish words" index=25>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-25 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, Fairness, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15491v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15491v1.pdf filename=2403.15491v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The growing interest in <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> and in particular in conversational models with which users can interact has led to the development of a <b>large</b> <b>number</b> <b>of</b> open-source chat <b>LLMs.</b> These models are evaluated on a wide range of <b>benchmarks</b> to assess their capabilities in answering questions or solving problems on almost any possible topic or to test their ability to reason or interpret texts. Instead, the evaluation of the knowledge that these models have of the languages has received much less attention. For example, the words that they can recognize and use in different languages. In this paper, we evaluate the knowledge that open-source chat <b>LLMs</b> have of Spanish words by testing a sample of words in a reference dictionary. The results show that open-source chat <b>LLMs</b> produce incorrect meanings for an important fraction of the words and are not able to use most of the words correctly to write sentences with context. These results show how Spanish is left behind in the open-source <b>LLM</b> race and highlight the need to push for linguistic <b>fairness</b> in conversational <b>LLMs</b> ensuring that they provide similar performance across languages.</p></p class="citation"></blockquote><h3 id=2652--26312-detoxifying-large-language-models-via-knowledge-editing-mengru-wang-et-al-2024>(26/52 | 26/312) Detoxifying Large Language Models via Knowledge Editing (Mengru Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mengru Wang, Ningyu Zhang, Ziwen Xu, Zekun Xi, Shumin Deng, Yunzhi Yao, Qishen Zhang, Linyi Yang, Jindong Wang, Huajun Chen. (2024)<br><strong>Detoxifying Large Language Models via Knowledge Editing</strong><br><button class=copy-to-clipboard title="Detoxifying Large Language Models via Knowledge Editing" index=26>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-26 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-CV, cs-HC, cs-LG, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14472v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14472v1.pdf filename=2403.14472v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper investigates using knowledge editing techniques to detoxify <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> We construct a <b>benchmark,</b> SafeEdit, which covers nine unsafe categories with various powerful attack <b>prompts</b> and equips comprehensive metrics for systematic evaluation. We conduct experiments to compare knowledge editing approaches with previous baselines, indicating that knowledge editing has the potential to efficiently detoxify <b>LLMs</b> with limited impact on general performance. Then, we propose a simple yet effective baseline, dubbed Detoxifying with Intraoperative Neural Monitoring (DINM), to diminish the toxicity of <b>LLMs</b> within a few tuning steps via only one instance. We further provide an in-depth analysis of the internal mechanism for various detoxify approaches, demonstrating that previous methods like SFT and DPO may merely suppress the activations of toxic parameters, while DINM mitigates the toxicity of the toxic parameters to a certain extent, making permanent adjustments. We hope that these insights could shed light on future work of developing detoxifying approaches and the underlying knowledge mechanisms of <b>LLMs.</b> Code and <b>benchmark</b> are available at <a href=https://github.com/zjunlp/EasyEdit>https://github.com/zjunlp/EasyEdit</a>.</p></p class="citation"></blockquote><h3 id=2752--27312-dermacen-analytica-a-novel-methodology-integrating-multi-modal-large-language-models-with-machine-learning-in-tele-dermatology-dimitrios-p-panagoulias-et-al-2024>(27/52 | 27/312) Dermacen Analytica: A Novel Methodology Integrating Multi-Modal Large Language Models with Machine Learning in tele-dermatology (Dimitrios P. Panagoulias et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dimitrios P. Panagoulias, Evridiki Tsoureli-Nikita, Maria Virvou, George A. Tsihrintzis. (2024)<br><strong>Dermacen Analytica: A Novel Methodology Integrating Multi-Modal Large Language Models with Machine Learning in tele-dermatology</strong><br><button class=copy-to-clipboard title="Dermacen Analytica: A Novel Methodology Integrating Multi-Modal Large Language Models with Machine Learning in tele-dermatology" index=27>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-27 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-CV, cs.CL<br>Keyword Score: 33<br>Keywords: Multi-modal, Transformer, Natural Language Inference, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14243v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14243v1.pdf filename=2403.14243v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rise of Artificial Intelligence creates great promise in the field of medical discovery, diagnostics and patient management. However, the vast complexity of all medical domains require a more complex approach that combines machine learning algorithms, classifiers, segmentation algorithms and, lately, <b>large</b> <b>language</b> <b>models.</b> In this paper, we describe, implement and assess an Artificial Intelligence-empowered system and methodology aimed at assisting the diagnosis process of skin lesions and other skin conditions within the field of dermatology that aims to holistically address the diagnostic process in this domain. The workflow integrates <b>large</b> <b>language,</b> <b>transformer-based</b> vision models and sophisticated machine learning tools. This holistic approach achieves a nuanced interpretation of dermatological conditions that simulates and facilitates a dermatologist&rsquo;s workflow. We assess our proposed methodology through a thorough cross-model validation technique embedded in an evaluation pipeline that utilizes publicly available medical case studies of skin conditions and relevant images. To quantitatively score the system performance, advanced machine learning and <b>natural</b> <b>language</b> <b>processing</b> tools are employed which focus on similarity comparison and <b>natural</b> <b>language</b> <b>inference.</b> Additionally, we incorporate a human expert evaluation process based on a structured checklist to further validate our results. We implemented the proposed methodology in a system which achieved approximate (weighted) scores of 0.87 for both contextual understanding and diagnostic accuracy, demonstrating the efficacy of our approach in enhancing dermatological analysis. The proposed methodology is expected to prove useful in the development of next-generation tele-dermatology applications, enhancing remote consultation capabilities and access to care, especially in underserved areas.</p></p class="citation"></blockquote><h3 id=2852--28312-large-scale-label-interpretation-learning-for-few-shot-named-entity-recognition-jonas-golde-et-al-2024>(28/52 | 28/312) Large-Scale Label Interpretation Learning for Few-Shot Named Entity Recognition (Jonas Golde et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jonas Golde, Felix Hamborg, Alan Akbik. (2024)<br><strong>Large-Scale Label Interpretation Learning for Few-Shot Named Entity Recognition</strong><br><button class=copy-to-clipboard title="Large-Scale Label Interpretation Learning for Few-Shot Named Entity Recognition" index=28>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-28 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, Few-shot, Named Entity Recognition, Named Entity Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14222v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14222v1.pdf filename=2403.14222v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Few-shot</b> <b>named</b> <b>entity</b> <b>recognition</b> <b>(NER)</b> detects <b>named</b> <b>entities</b> <b>within</b> text using only a few annotated examples. One promising line of research is to leverage natural language descriptions of each entity type: the common label PER might, for example, be verbalized as &lsquo;&lsquo;person entity.&rsquo;&rsquo; In an initial label interpretation learning phase, the model learns to interpret such verbalized descriptions of entity types. In a subsequent <b>few-shot</b> tagset extension phase, this model is then given a description of a previously unseen entity type (such as &lsquo;&lsquo;music album&rsquo;&rsquo;) and optionally a few training examples to perform <b>few-shot</b> <b>NER</b> for this type. In this paper, we systematically explore the impact of a strong semantic prior to interpret verbalizations of new entity types by massively scaling up the number and granularity of entity types used for label interpretation learning. To this end, we leverage an entity linking <b>benchmark</b> to create a dataset with orders of magnitude of more distinct entity types and descriptions as currently used datasets. We find that this increased signal yields strong results in zero- and <b>few-shot</b> <b>NER</b> in in-domain, cross-domain, and even cross-lingual settings. Our findings indicate significant potential for improving <b>few-shot</b> <b>NER</b> through heuristical data-based optimization.</p></p class="citation"></blockquote><h3 id=2952--29312-enhancing-medical-support-in-the-arabic-language-through-personalized-chatgpt-assistance-mohamed-issa-et-al-2024>(29/52 | 29/312) Enhancing Medical Support in the Arabic Language Through Personalized ChatGPT Assistance (Mohamed Issa et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohamed Issa, Ahmed Abdelwahed. (2024)<br><strong>Enhancing Medical Support in the Arabic Language Through Personalized ChatGPT Assistance</strong><br><button class=copy-to-clipboard title="Enhancing Medical Support in the Arabic Language Through Personalized ChatGPT Assistance" index=29>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-29 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-IR, cs.CL<br>Keyword Score: 30<br>Keywords: ChatGPT, Prompt, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15501v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15501v1.pdf filename=2403.15501v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This Paper discusses the growing popularity of online medical diagnosis as an alternative to traditional doctor visits. It highlights the limitations of existing tools and emphasizes the advantages of using <b>ChatGPT,</b> which provides real-time, personalized medical diagnosis at no cost. The paragraph <b>summarizes</b> a research study that evaluated the performance of <b>ChatGPT</b> in Arabic medical diagnosis. The study involved compiling a dataset of disease information and generating multiple messages for each disease using different <b>prompting</b> techniques. <b>ChatGPT&rsquo;s</b> performance was assessed by measuring the similarity between its responses and the actual diseases. The results showed promising performance, with average scores of around 76% for similarity measures. Various <b>prompting</b> techniques were used, and chain <b>prompting</b> demonstrated a relative advantage. The study also recorded an average response time of 6.12 seconds for the <b>ChatGPT</b> API, which is considered acceptable but has room for improvement. While <b>ChatGPT</b> cannot replace human doctors entirely, the findings suggest its potential in emergency cases and addressing general medical inquiries. Overall, the study highlights <b>ChatGPT&rsquo;s</b> viability as a valuable tool in the medical field.</p></p class="citation"></blockquote><h3 id=3052--30312-large-language-models-for-multi-choice-question-classification-of-medical-subjects-víctor-ponce-lópez-2024>(30/52 | 30/312) Large Language Models for Multi-Choice Question Classification of Medical Subjects (Víctor Ponce-López, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Víctor Ponce-López. (2024)<br><strong>Large Language Models for Multi-Choice Question Classification of Medical Subjects</strong><br><button class=copy-to-clipboard title="Large Language Models for Multi-Choice Question Classification of Medical Subjects" index=30>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-30 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Question Answering, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14582v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14582v1.pdf filename=2403.14582v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The aim of this paper is to evaluate whether <b>large</b> <b>language</b> <b>models</b> trained on multi-choice <b>question</b> <b>data</b> can be used to discriminate between medical subjects. This is an important and challenging task for automatic <b>question</b> <b>answering.</b> To achieve this goal, we train deep neural networks for multi-class classification of <b>questions</b> <b>into</b> the inferred medical subjects. Using our Multi-Question (MQ) Sequence-BERT method, we outperform the state-of-the-art results on the MedMCQA dataset with an accuracy of 0.68 and 0.60 on their development and test sets, respectively. In this sense, we show the capability of AI and <b>LLMs</b> in particular for multi-classification tasks in the Healthcare domain.</p></p class="citation"></blockquote><h3 id=3152--31312-multi-level-explanations-for-generative-language-models-lucas-monteiro-paes-et-al-2024>(31/52 | 31/312) Multi-Level Explanations for Generative Language Models (Lucas Monteiro Paes et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lucas Monteiro Paes, Dennis Wei, Hyo Jin Do, Hendrik Strobelt, Ronny Luss, Amit Dhurandhar, Manish Nagireddy, Karthikeyan Natesan Ramamurthy, Prasanna Sattigeri, Werner Geyer, Soumya Ghosh. (2024)<br><strong>Multi-Level Explanations for Generative Language Models</strong><br><button class=copy-to-clipboard title="Multi-Level Explanations for Generative Language Models" index=31>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-31 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Question Answering, Text Classification, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14459v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14459v1.pdf filename=2403.14459v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Perturbation-based explanation methods such as LIME and SHAP are commonly applied to <b>text</b> <b>classification.</b> This work focuses on their extension to generative language models. To address the challenges of <b>text</b> <b>as</b> output and long <b>text</b> <b>inputs,</b> we propose a general framework called MExGen that can be instantiated with different attribution algorithms. To handle <b>text</b> <b>output,</b> we introduce the notion of scalarizers for mapping <b>text</b> <b>to</b> real numbers and investigate multiple possibilities. To handle long inputs, we take a multi-level approach, proceeding from coarser levels of granularity to finer ones, and focus on algorithms with linear scaling in model queries. We conduct a systematic evaluation, both automated and human, of perturbation-based attribution methods for <b>summarization</b> and context-grounded <b>question</b> <b>answering.</b> The results show that our framework can provide more locally faithful explanations of generated outputs.</p></p class="citation"></blockquote><h3 id=3252--32312-erd-a-framework-for-improving-llm-reasoning-for-cognitive-distortion-classification-sehee-lim-et-al-2024>(32/52 | 32/312) ERD: A Framework for Improving LLM Reasoning for Cognitive Distortion Classification (Sehee Lim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sehee Lim, Yejin Kim, Chi-Hyun Choi, Jy-yong Sohn, Byung-Hoon Kim. (2024)<br><strong>ERD: A Framework for Improving LLM Reasoning for Cognitive Distortion Classification</strong><br><button class=copy-to-clipboard title="ERD: A Framework for Improving LLM Reasoning for Cognitive Distortion Classification" index=32>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-32 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 30<br>Keywords: Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14255v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14255v1.pdf filename=2403.14255v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Improving the accessibility of psychotherapy with the aid of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> is garnering a significant attention in recent years. Recognizing cognitive distortions from the interviewee&rsquo;s utterances can be an essential part of psychotherapy, especially for cognitive behavioral therapy. In this paper, we propose ERD, which improves <b>LLM-based</b> cognitive distortion classification performance with the aid of additional modules of (1) extracting the parts related to cognitive distortion, and (2) debating the <b>reasoning</b> steps by multiple agents. Our experimental results on a public dataset show that ERD improves the multi-class F1 score as well as binary specificity score. Regarding the latter score, it turns out that our method is effective in debiasing the baseline method which has high false positive rate, especially when the summary of multi-agent debate is provided to <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=3352--33312-edt-improving-large-language-models-generation-by-entropy-based-dynamic-temperature-sampling-shimao-zhang-et-al-2024>(33/52 | 33/312) EDT: Improving Large Language Models&rsquo; Generation by Entropy-based Dynamic Temperature Sampling (Shimao Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shimao Zhang, Yu Bao, Shujian Huang. (2024)<br><strong>EDT: Improving Large Language Models&rsquo; Generation by Entropy-based Dynamic Temperature Sampling</strong><br><button class=copy-to-clipboard title="EDT: Improving Large Language Models' Generation by Entropy-based Dynamic Temperature Sampling" index=33>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-33 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 23<br>Keywords: Benchmarking, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14541v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14541v1.pdf filename=2403.14541v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have demonstrated outstanding performance across a wide range of downstream language tasks. Temperature sampling is a commonly used decoding strategy for <b>LLMs&rsquo;</b> generation process. However, a fixed temperature parameter is used in most cases, which may not always be an optimal choice for balancing generation quality and diversity. In this paper, we propose an effective Entropy-based Dynamic Temperature (EDT) Sampling method, to achieve a more balanced performance in terms of both generation quality and diversity by dynamically selecting the temperature parameter. Additionally, we also show model performance and comprehensive analyses for 4 different generation <b>benchmarks.</b> Our experiments show that EDT significantly outperforms the existing strategies across different tasks.</p></p class="citation"></blockquote><h3 id=3452--34312-rakutenai-7b-extending-large-language-models-for-japanese-rakuten-group-et-al-2024>(34/52 | 34/312) RakutenAI-7B: Extending Large Language Models for Japanese (Rakuten Group et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rakuten Group, Aaron Levine, Connie Huang, Chenguang Wang, Eduardo Batista, Ewa Szymanska, Hongyi Ding, Hou Wei Chou, Jean-François Pessiot, Johanes Effendi, Justin Chiu, Kai Torben Ohlhus, Karan Chopra, Keiji Shinzato, Koji Murakami, Lee Xiong, Lei Chen, Maki Kubota, Maksim Tkachenko, Miroku Lee, Naoki Takahashi, Prathyusha Jwalapuram, Ryutaro Tatsushima, Saurabh Jain, Sunil Kumar Yadav, Ting Cai, Wei-Te Chen, Yandi Xia, Yuki Nakayama, Yutaka Higashiyama. (2024)<br><strong>RakutenAI-7B: Extending Large Language Models for Japanese</strong><br><button class=copy-to-clipboard title="RakutenAI-7B: Extending Large Language Models for Japanese" index=34>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-34 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 23<br>Keywords: Benchmarking, Foundation Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15484v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15484v1.pdf filename=2403.15484v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce RakutenAI-7B, a suite of Japanese-oriented <b>large</b> <b>language</b> <b>models</b> that achieve the best performance on the Japanese LM Harness <b>benchmarks</b> among the open 7B models. Along with the <b>foundation</b> <b>model,</b> we release instruction- and chat-tuned models, RakutenAI-7B-instruct and RakutenAI-7B-chat respectively, under the Apache 2.0 license.</p></p class="citation"></blockquote><h3 id=3552--35312-sequential-decision-making-for-inline-text-autocomplete-rohan-chitnis-et-al-2024>(35/52 | 35/312) Sequential Decision-Making for Inline Text Autocomplete (Rohan Chitnis et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rohan Chitnis, Shentao Yang, Alborz Geramifard. (2024)<br><strong>Sequential Decision-Making for Inline Text Autocomplete</strong><br><button class=copy-to-clipboard title="Sequential Decision-Making for Inline Text Autocomplete" index=35>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-35 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-HC, cs-LG, cs.CL<br>Keyword Score: 20<br>Keywords: Reinforcement Learning, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15502v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15502v1.pdf filename=2403.15502v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Autocomplete suggestions are fundamental to modern text entry systems, with applications in domains such as messaging and email composition. Typically, autocomplete suggestions are generated from a language model with a confidence threshold. However, this threshold does not directly take into account the cognitive load imposed on the user by surfacing suggestions, such as the effort to switch contexts from typing to reading the suggestion, and the time to decide whether to accept the suggestion. In this paper, we study the problem of improving inline autocomplete suggestions in text entry systems via a sequential decision-making formulation, and use <b>reinforcement</b> <b>learning</b> to learn suggestion policies through repeated interactions with a target user over time. This formulation allows us to factor cognitive load into the objective of training an autocomplete model, through a reward function based on text entry speed. We acquired theoretical and experimental evidence that, under certain objectives, the sequential decision-making formulation of the autocomplete problem provides a better suggestion policy than myopic single-step <b>reasoning.</b> However, aligning these objectives with real users requires further exploration. In particular, we hypothesize that the objectives under which sequential decision-making can improve autocomplete systems are not tailored solely to text entry speed, but more broadly to metrics such as user satisfaction and convenience.</p></p class="citation"></blockquote><h3 id=3652--36312-visual-analytics-for-fine-grained-text-classification-models-and-datasets-munkhtulga-battogtokh-et-al-2024>(36/52 | 36/312) Visual Analytics for Fine-grained Text Classification Models and Datasets (Munkhtulga Battogtokh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Munkhtulga Battogtokh, Yiwen Xing, Cosmin Davidescu, Alfie Abdul-Rahman, Michael Luck, Rita Borgo. (2024)<br><strong>Visual Analytics for Fine-grained Text Classification Models and Datasets</strong><br><button class=copy-to-clipboard title="Visual Analytics for Fine-grained Text Classification Models and Datasets" index=36>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-36 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Reasoning, Text Classification<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15492v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15492v1.pdf filename=2403.15492v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In natural language processing (NLP), <b>text</b> <b>classification</b> tasks are increasingly fine-grained, as datasets are fragmented into a larger number of classes that are more difficult to differentiate from one another. As a consequence, the semantic structures of datasets have become more complex, and model decisions more difficult to explain. Existing tools, suited for coarse-grained classification, falter under these additional challenges. In response to this gap, we worked closely with NLP domain experts in an iterative design-and-evaluation process to characterize and tackle the growing requirements in their workflow of developing fine-grained <b>text</b> <b>classification</b> models. The result of this collaboration is the development of SemLa, a novel visual analytics system tailored for 1) dissecting complex semantic structures in a dataset when it is spatialized in model embedding space, and 2) visualizing fine-grained nuances in the meaning of <b>text</b> <b>samples</b> to faithfully explain model <b>reasoning.</b> This paper details the iterative design study and the resulting innovations featured in SemLa. The final design allows contrastive analysis at different levels by unearthing lexical and conceptual patterns including biases and artifacts in data. Expert feedback on our final design and case studies confirm that SemLa is a useful tool for supporting model validation and debugging as well as data annotation.</p></p class="citation"></blockquote><h3 id=3752--37312-prediction-of-translation-techniques-for-the-translation-process-fan-zhou-et-al-2024>(37/52 | 37/312) Prediction of Translation Techniques for the Translation Process (Fan Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fan Zhou, Vincent Vandeghinste. (2024)<br><strong>Prediction of Translation Techniques for the Translation Process</strong><br><button class=copy-to-clipboard title="Prediction of Translation Techniques for the Translation Process" index=37>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-37 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Neural Machine Translation, Neural Machine Translation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14454v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14454v1.pdf filename=2403.14454v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Machine</b> <b>translation</b> <b>(MT)</b> encompasses a variety of methodologies aimed at enhancing the accuracy of translations. In contrast, the process of human-generated translation relies on a wide range of translation techniques, which are crucial for ensuring linguistic adequacy and fluency. This study suggests that these translation techniques could further optimize <b>machine</b> <b>translation</b> if they are automatically identified before being applied to guide the translation process effectively. The study differentiates between two scenarios of the translation process: from-scratch translation and post-editing. For each scenario, a specific set of experiments has been designed to forecast the most appropriate translation techniques. The findings indicate that the predictive accuracy for from-scratch translation reaches 82%, while the post-editing process exhibits even greater potential, achieving an accuracy rate of 93%.</p></p class="citation"></blockquote><h3 id=3852--38312-more-than-just-statistical-recurrence-human-and-machine-unsupervised-learning-of-māori-word-segmentation-across-morphological-processes-ashvini-varatharaj-et-al-2024>(38/52 | 38/312) More than Just Statistical Recurrence: Human and Machine Unsupervised Learning of Māori Word Segmentation across Morphological Processes (Ashvini Varatharaj et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ashvini Varatharaj, Simon Todd. (2024)<br><strong>More than Just Statistical Recurrence: Human and Machine Unsupervised Learning of Māori Word Segmentation across Morphological Processes</strong><br><button class=copy-to-clipboard title="More than Just Statistical Recurrence: Human and Machine Unsupervised Learning of Māori Word Segmentation across Morphological Processes" index=38>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-38 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Unsupervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14444v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14444v1.pdf filename=2403.14444v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Non-M=aori-speaking New Zealanders (NMS)are able to segment M=aori words in a highlysimilar way to fluent speakers (Panther et al.,2024). This ability is assumed to derive through the identification and extraction of statistically recurrent forms. We examine this assumption by asking how NMS segmentations compare to those produced by Morfessor, an <b>unsupervised</b> <b>machine</b> learning model that operates based on statistical recurrence, across words formed by a variety of morphological processes. Both NMS and Morfessor succeed in segmenting words formed by concatenative processes (compounding and affixation without allomorphy), but NMS also succeed for words that invoke templates (reduplication and allomorphy) and other cues to morphological structure, implying that their learning process is sensitive to more than just statistical recurrence.</p></p class="citation"></blockquote><h3 id=3952--39312-locating-and-mitigating-gender-bias-in-large-language-models-yuchen-cai-et-al-2024>(39/52 | 39/312) Locating and Mitigating Gender Bias in Large Language Models (Yuchen Cai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuchen Cai, Ding Cao, Rongxi Guo, Yaqin Wen, Guiquan Liu, Enhong Chen. (2024)<br><strong>Locating and Mitigating Gender Bias in Large Language Models</strong><br><button class=copy-to-clipboard title="Locating and Mitigating Gender Bias in Large Language Models" index=39>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-39 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14409v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14409v1.pdf filename=2403.14409v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models(LLM)</b> are pre-trained on extensive corpora to learn facts and human cognition which contain human preferences. However, this process can inadvertently lead to these models acquiring biases and stereotypes prevalent in society. Prior research has typically tackled the issue of bias through a one-dimensional perspective, concentrating either on locating or mitigating it. This limited perspective has created obstacles in facilitating research on bias to synergistically complement and progressively build upon one another. In this study, we integrate the processes of locating and mitigating bias within a unified framework. Initially, we use causal mediation analysis to trace the causal effects of different components&rsquo; activation within a <b>large</b> <b>language</b> <b>model.</b> Building on this, we propose the LSDM (Least Square Debias Method), a knowledge-editing based method for mitigating gender bias in occupational pronouns, and compare it against two baselines on three gender bias datasets and seven knowledge competency test datasets. The experimental results indicate that the primary contributors to gender bias are the bottom MLP modules acting on the last token of occupational pronouns and the top attention module acting on the final word in the sentence. Furthermore, LSDM mitigates gender bias in the model more effectively than the other baselines, while fully preserving the model&rsquo;s capabilities in all other aspects.</p></p class="citation"></blockquote><h3 id=4052--40312-wikifactdiff-a-large-realistic-and-temporally-adaptable-dataset-for-atomic-factual-knowledge-update-in-causal-language-models-hichem-ammar-khodja-et-al-2024>(40/52 | 40/312) WikiFactDiff: A Large, Realistic, and Temporally Adaptable Dataset for Atomic Factual Knowledge Update in Causal Language Models (Hichem Ammar Khodja et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hichem Ammar Khodja, Frédéric Béchet, Quentin Brabant, Alexis Nasr, Gwénolé Lecorvé. (2024)<br><strong>WikiFactDiff: A Large, Realistic, and Temporally Adaptable Dataset for Atomic Factual Knowledge Update in Causal Language Models</strong><br><button class=copy-to-clipboard title="WikiFactDiff: A Large, Realistic, and Temporally Adaptable Dataset for Atomic Factual Knowledge Update in Causal Language Models" index=40>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-40 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14364v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14364v1.pdf filename=2403.14364v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The factuality of <b>large</b> <b>language</b> <b>model</b> <b>(LLMs)</b> tends to decay over time since events posterior to their training are &ldquo;unknown&rdquo; to them. One way to keep models up-to-date could be factual update: the task of inserting, replacing, or removing certain simple (atomic) facts within the model. To study this task, we present WikiFactDiff, a dataset that describes the evolution of factual knowledge between two dates as a collection of simple facts divided into three categories: new, obsolete, and static. We describe several update scenarios arising from various combinations of these three types of basic update. The facts are represented by subject-relation-object triples; indeed, WikiFactDiff was constructed by comparing the state of the Wikidata knowledge base at 4 January 2021 and 27 February 2023. Those fact are accompanied by verbalization templates and cloze tests that enable running update algorithms and their evaluation metrics. Contrary to other datasets, such as zsRE and CounterFact, WikiFactDiff constitutes a realistic update setting that involves various update scenarios, including replacements, archival, and new entity insertions. We also present an evaluation of existing update algorithms on WikiFactDiff.</p></p class="citation"></blockquote><h3 id=4152--41312-automatic-annotation-of-grammaticality-in-child-caregiver-conversations-mitja-nikolaus-et-al-2024>(41/52 | 41/312) Automatic Annotation of Grammaticality in Child-Caregiver Conversations (Mitja Nikolaus et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mitja Nikolaus, Abhishek Agrawal, Petros Kaklamanis, Alex Warstadt, Abdellah Fourtassi. (2024)<br><strong>Automatic Annotation of Grammaticality in Child-Caregiver Conversations</strong><br><button class=copy-to-clipboard title="Automatic Annotation of Grammaticality in Child-Caregiver Conversations" index=41>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-41 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Fine-tuning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14208v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14208v1.pdf filename=2403.14208v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The acquisition of grammar has been a central question to adjudicate between theories of language acquisition. In order to conduct faster, more reproducible, and larger-scale corpus studies on grammaticality in child-caregiver conversations, tools for automatic annotation can offer an effective alternative to tedious manual annotation. We propose a coding scheme for context-dependent grammaticality in child-caregiver conversations and annotate more than 4,000 utterances from a large corpus of transcribed conversations. Based on these annotations, we train and evaluate a range of NLP models. Our results show that <b>fine-tuned</b> <b>Transformer-based</b> models perform best, achieving human inter-annotation agreement levels.As a first application and sanity check of this tool, we use the trained models to annotate a corpus almost two orders of magnitude larger than the manually annotated data and verify that children&rsquo;s grammaticality shows a steady increase with age.This work contributes to the growing literature on applying state-of-the-art NLP methods to help study child language acquisition at scale.</p></p class="citation"></blockquote><h3 id=4252--42312-m3av-a-multimodal-multigenre-and-multipurpose-audio-visual-academic-lecture-dataset-zhe-chen-et-al-2024>(42/52 | 42/312) M$^3$AV: A Multimodal, Multigenre, and Multipurpose Audio-Visual Academic Lecture Dataset (Zhe Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhe Chen, Heyang Liu, Wenyi Yu, Guangzhi Sun, Hongcheng Liu, Ji Wu, Chao Zhang, Yu Wang, Yanfeng Wang. (2024)<br><strong>M$^3$AV: A Multimodal, Multigenre, and Multipurpose Audio-Visual Academic Lecture Dataset</strong><br><button class=copy-to-clipboard title="M$^3$AV: A Multimodal, Multigenre, and Multipurpose Audio-Visual Academic Lecture Dataset" index=42>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-42 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 16<br>Keywords: Multi-modal, Multi-modal, Automatic Speech Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14168v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14168v1.pdf filename=2403.14168v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Publishing open-source academic video recordings is an emergent and prevalent approach to sharing knowledge online. Such videos carry rich <b>multimodal</b> information including <b>speech,</b> <b>the</b> facial and body movements of the speakers, as well as the texts and pictures in the slides and possibly even the papers. Although multiple academic video datasets have been constructed and released, few of them support both <b>multimodal</b> content recognition and understanding tasks, which is partially due to the lack of high-quality human annotations. In this paper, we propose a novel <b>multimodal,</b> multigenre, and multipurpose audio-visual academic lecture dataset (M$^3$AV), which has almost 367 hours of videos from five sources covering computer science, mathematics, and medical and biology topics. With high-quality human annotations of the spoken and written words, in particular high-valued name entities, the dataset can be used for multiple audio-visual recognition and understanding tasks. Evaluations performed on contextual <b>speech</b> <b>recognition,</b> <b>speech</b> <b>synthesis,</b> and slide and script generation tasks demonstrate that the diversity of M$^3$AV makes it a challenging dataset.</p></p class="citation"></blockquote><h3 id=4352--43312-mogam-a-multimodal-object-oriented-graph-attention-model-for-depression-detection-junyeop-cha-et-al-2024>(43/52 | 43/312) MOGAM: A Multimodal Object-oriented Graph Attention Model for Depression Detection (Junyeop Cha et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junyeop Cha, Seoyun Kim, Dongjae Kim, Eunil Park. (2024)<br><strong>MOGAM: A Multimodal Object-oriented Graph Attention Model for Depression Detection</strong><br><button class=copy-to-clipboard title="MOGAM: A Multimodal Object-oriented Graph Attention Model for Depression Detection" index=43>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-43 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 12<br>Keywords: Graph, Benchmarking, Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15485v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15485v1.pdf filename=2403.15485v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Early detection plays a crucial role in the treatment of depression. Therefore, numerous studies have focused on social media platforms, where individuals express their emotions, aiming to achieve early detection of depression. However, the majority of existing approaches often rely on specific features, leading to limited scalability across different types of social media datasets, such as text, images, or videos. To overcome this limitation, we introduce a <b>Multimodal</b> Object-Oriented <b>Graph</b> Attention Model (MOGAM), which can be applied to diverse types of data, offering a more scalable and versatile solution. Furthermore, to ensure that our model can capture authentic symptoms of depression, we only include vlogs from users with a clinical diagnosis. To leverage the diverse features of vlogs, we adopt a <b>multimodal</b> approach and collect additional metadata such as the title, description, and duration of the vlogs. To effectively aggregate these <b>multimodal</b> features, we employed a cross-attention mechanism. MOGAM achieved an accuracy of 0.871 and an F1-score of 0.888. Moreover, to validate the scalability of MOGAM, we evaluated its performance with a <b>benchmark</b> dataset and achieved comparable results with prior studies (0.61 F1-score). In conclusion, we believe that the proposed model, MOGAM, is an effective solution for detecting depression in social media, offering potential benefits in the early detection and treatment of this mental health condition.</p></p class="citation"></blockquote><h3 id=4452--44312-evaluating-the-performance-of-llms-on-technical-language-processing-tasks-andrew-kernycky-et-al-2024>(44/52 | 44/312) Evaluating the Performance of LLMs on Technical Language Processing tasks (Andrew Kernycky et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andrew Kernycky, David Coleman, Christopher Spence, Udayan Das. (2024)<br><strong>Evaluating the Performance of LLMs on Technical Language Processing tasks</strong><br><button class=copy-to-clipboard title="Evaluating the Performance of LLMs on Technical Language Processing tasks" index=44>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-44 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-IR, cs.CL<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15503v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15503v1.pdf filename=2403.15503v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper we present the results of an evaluation study of the perfor-mance of <b>LLMs</b> on Technical Language Processing tasks. Humans are often confronted with tasks in which they have to gather information from dispar-ate sources and require making sense of large bodies of text. These tasks can be significantly complex for humans and often require deep study including rereading portions of a text. Towards simplifying the task of gathering in-formation we evaluated <b>LLMs</b> with chat interfaces for their ability to provide answers to standard questions that a human can be expected to answer based on their reading of a body of text. The body of text under study is Title 47 of the United States Code of Federal Regulations (CFR) which describes regula-tions for commercial telecommunications as governed by the Federal Com-munications Commission (FCC). This has been a body of text of interest be-cause our larger research concerns the issue of making sense of information related to Wireless Spectrum Governance and usage in an automated manner to support Dynamic Spectrum Access. The information concerning this wireless spectrum domain is found in many disparate sources, with Title 47 of the CFR being just one of many. Using a range of <b>LLMs</b> and providing the required CFR text as context we were able to quantify the performance of those <b>LLMs</b> on the specific task of answering the questions below.</p></p class="citation"></blockquote><h3 id=4552--45312-tams-translation-assisted-morphological-segmentation-enora-rice-et-al-2024>(45/52 | 45/312) TAMS: Translation-Assisted Morphological Segmentation (Enora Rice et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Enora Rice, Ali Marashian, Luke Gessler, Alexis Palmer, Katharina von der Wense. (2024)<br><strong>TAMS: Translation-Assisted Morphological Segmentation</strong><br><button class=copy-to-clipboard title="TAMS: Translation-Assisted Morphological Segmentation" index=45>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-45 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: High-Resource<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14840v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14840v1.pdf filename=2403.14840v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Canonical morphological segmentation is the process of analyzing words into the standard (aka underlying) forms of their constituent morphemes. This is a core task in language documentation, and NLP systems have the potential to dramatically speed up this process. But in typical language documentation settings, training data for canonical morpheme segmentation is scarce, making it difficult to train high quality models. However, translation data is often much more abundant, and, in this work, we present a method that attempts to leverage this data in the canonical segmentation task. We propose a character-level sequence-to-sequence model that incorporates representations of translations obtained from pretrained <b>high-resource</b> monolingual language models as an additional signal. Our model outperforms the baseline in a super-low resource setting but yields mixed results on training splits with more data. While further work is needed to make translations useful in higher-resource settings, our model shows promise in severely resource-constrained settings.</p></p class="citation"></blockquote><h3 id=4652--46312-a-collection-of-pragmatic-similarity-judgments-over-spoken-dialog-utterances-nigel-g-ward-et-al-2024>(46/52 | 46/312) A Collection of Pragmatic-Similarity Judgments over Spoken Dialog Utterances (Nigel G. Ward et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nigel G. Ward, Divette Marco. (2024)<br><strong>A Collection of Pragmatic-Similarity Judgments over Spoken Dialog Utterances</strong><br><button class=copy-to-clipboard title="A Collection of Pragmatic-Similarity Judgments over Spoken Dialog Utterances" index=46>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-46 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Neural Machine Translation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14808v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14808v1.pdf filename=2403.14808v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Automatic measures of similarity between utterances are invaluable for training speech synthesizers, evaluating <b>machine</b> <b>translation,</b> and assessing learner productions. While there exist measures for semantic similarity and prosodic similarity, there are as yet none for pragmatic similarity. To enable the training of such measures, we developed the first collection of human judgments of pragmatic similarity between utterance pairs. Each pair consisting of an utterance extracted from a recorded dialog and a re-enactment of that utterance. Re-enactments were done under various conditions designed to create a variety of degrees of similarity. Each pair was rated on a continuous scale by 6 to 9 judges. The average inter-judge correlation was as high as 0.72 for English and 0.66 for Spanish. We make this data available at <a href=https://github.com/divettemarco/PragSim>https://github.com/divettemarco/PragSim</a> .</p></p class="citation"></blockquote><h3 id=4752--47312-the-era-of-semantic-decoding-maxime-peyrard-et-al-2024>(47/52 | 47/312) The Era of Semantic Decoding (Maxime Peyrard et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Maxime Peyrard, Martin Josifoski, Robert West. (2024)<br><strong>The Era of Semantic Decoding</strong><br><button class=copy-to-clipboard title="The Era of Semantic Decoding" index=47>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-47 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-HC, cs-MA, cs.CL<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14562v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14562v1.pdf filename=2403.14562v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent work demonstrated great promise in the idea of orchestrating collaborations between <b>LLMs,</b> human input, and various tools to address the inherent limitations of <b>LLMs.</b> We propose a novel perspective called semantic decoding, which frames these collaborative processes as optimization procedures in semantic space. Specifically, we conceptualize <b>LLMs</b> as semantic processors that manipulate meaningful pieces of information that we call semantic tokens (known thoughts). <b>LLMs</b> are among a large pool of other semantic processors, including humans and tools, such as search engines or code executors. Collectively, semantic processors engage in dynamic exchanges of semantic tokens to progressively construct high-utility outputs. We refer to these orchestrated interactions among semantic processors, optimizing and searching in semantic space, as semantic decoding algorithms. This concept draws a direct parallel to the well-studied problem of syntactic decoding, which involves crafting algorithms to best exploit auto-regressive language models for extracting high-utility sequences of syntactic tokens. By focusing on the semantic level and disregarding syntactic details, we gain a fresh perspective on the engineering of AI systems, enabling us to imagine systems with much greater complexity and capabilities. In this position paper, we formalize the transition from syntactic to semantic tokens as well as the analogy between syntactic and semantic decoding. Subsequently, we explore the possibilities of optimizing within the space of semantic tokens via semantic decoding algorithms. We conclude with a list of research opportunities and questions arising from this fresh perspective. The semantic decoding perspective offers a powerful abstraction for search and optimization directly in the space of meaningful concepts, with semantic tokens as the fundamental units of a new type of computation.</p></p class="citation"></blockquote><h3 id=4852--48312-emergent-communication-and-learning-pressures-in-language-models-a-language-evolution-perspective-lukas-galke-et-al-2024>(48/52 | 48/312) Emergent communication and learning pressures in language models: a language evolution perspective (Lukas Galke et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lukas Galke, Limor Raviv. (2024)<br><strong>Emergent communication and learning pressures in language models: a language evolution perspective</strong><br><button class=copy-to-clipboard title="Emergent communication and learning pressures in language models: a language evolution perspective" index=48>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-48 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: I-2-7, cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14427v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14427v1.pdf filename=2403.14427v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Language models and humans are two types of learning systems. Finding or facilitating commonalities could enable major breakthroughs in our understanding of the acquisition and evolution of language. Many theories of language evolution rely heavily on learning biases and learning pressures. Yet due to substantial differences in learning pressures, it is questionable whether the similarity between humans and machines is sufficient for insights to carry over and to be worth testing with human participants. Here, we review the emergent communication literature, a subfield of multi-agent <b>reinforcement</b> <b>learning,</b> from a language evolution perspective. We find that the emergent communication literature excels at designing and adapting models to recover initially absent linguistic phenomena of natural languages. Based on a short literature review, we identify key pressures that have recovered initially absent human patterns in emergent communication models: communicative success, efficiency, learnability, and other psycho-/sociolinguistic factors. We argue that this may serve as inspiration for how to design language models for language acquisition and language evolution research.</p></p class="citation"></blockquote><h3 id=4952--49312-editing-knowledge-representation-of-language-lodel-via-rephrased-prefix-prompts-yuchen-cai-et-al-2024>(49/52 | 49/312) Editing Knowledge Representation of Language Lodel via Rephrased Prefix Prompts (Yuchen Cai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuchen Cai, Ding Cao, Rongxi Guo, Yaqin Wen, Guiquan Liu, Enhong Chen. (2024)<br><strong>Editing Knowledge Representation of Language Lodel via Rephrased Prefix Prompts</strong><br><button class=copy-to-clipboard title="Editing Knowledge Representation of Language Lodel via Rephrased Prefix Prompts" index=49>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-49 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14381v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14381v1.pdf filename=2403.14381v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neural language models (LMs) have been extensively trained on vast corpora to store factual knowledge about various aspects of the world described in texts. Current technologies typically employ knowledge editing methods or specific <b>prompts</b> to modify LM outputs. However, existing knowledge editing methods are costly and inefficient, struggling to produce appropriate text. Additionally, <b>prompt</b> engineering is opaque and requires significant effort to find suitable <b>prompts.</b> To address these issues, we introduce a new method called PSPEM (Prefix Soft <b>Prompt</b> Editing Method), that can be used for a lifetime with just one training. It resolves the inefficiencies and generalizability issues in knowledge editing methods and overcomes the opacity of <b>prompt</b> engineering by automatically seeking optimal soft <b>prompts.</b> Specifically, PSPEM utilizes a <b>prompt</b> encoder and an encoding converter to refine key information in <b>prompts</b> and uses <b>prompt</b> alignment techniques to guide model generation, ensuring text consistency and adherence to the intended structure and content, thereby maintaining an optimal balance between efficiency and accuracy. We have validated the effectiveness of PSPEM through knowledge editing and attribute inserting. On the COUNTERFACT dataset, PSPEM achieved nearly 100% editing accuracy and demonstrated the highest level of fluency. We further analyzed the similarities between PSPEM and original <b>prompts</b> and their impact on the model&rsquo;s internals. The results indicate that PSPEM can serve as an alternative to original <b>prompts,</b> supporting the model in effective editing.</p></p class="citation"></blockquote><h3 id=5052--50312-beyond-surface-similarity-detecting-subtle-semantic-shifts-in-financial-narratives-jiaxin-liu-et-al-2024>(50/52 | 50/312) Beyond Surface Similarity: Detecting Subtle Semantic Shifts in Financial Narratives (Jiaxin Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiaxin Liu, Yi Yang, Kar Yan Tam. (2024)<br><strong>Beyond Surface Similarity: Detecting Subtle Semantic Shifts in Financial Narratives</strong><br><button class=copy-to-clipboard title="Beyond Surface Similarity: Detecting Subtle Semantic Shifts in Financial Narratives" index=50>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-50 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14341v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14341v1.pdf filename=2403.14341v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we introduce the Financial-STS task, a financial domain-specific NLP task designed to measure the nuanced semantic similarity between pairs of financial narratives. These narratives originate from the financial statements of the same company but correspond to different periods, such as year-over-year comparisons. Measuring the subtle semantic differences between these paired narratives enables market stakeholders to gauge changes over time in the company&rsquo;s financial and operational situations, which is critical for financial decision-making. We find that existing pretrained embedding models and <b>LLM</b> embeddings fall short in discerning these subtle financial narrative shifts. To address this gap, we propose an <b>LLM-augmented</b> pipeline specifically designed for the Financial-STS task. Evaluation on a human-annotated dataset demonstrates that our proposed method outperforms existing methods trained on classic STS tasks and generic <b>LLM</b> embeddings.</p></p class="citation"></blockquote><h3 id=5152--51312-is-reference-necessary-in-the-evaluation-of-nlg-systems-when-and-where-shuqian-sheng-et-al-2024>(51/52 | 51/312) Is Reference Necessary in the Evaluation of NLG Systems? When and Where? (Shuqian Sheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shuqian Sheng, Yi Xu, Luoyi Fu, Jiaxin Ding, Lei Zhou, Xinbing Wang, Chenghu Zhou. (2024)<br><strong>Is Reference Necessary in the Evaluation of NLG Systems? When and Where?</strong><br><button class=copy-to-clipboard title="Is Reference Necessary in the Evaluation of NLG Systems? When and Where?" index=51>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-51 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Natural Language Generation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14275v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14275v1.pdf filename=2403.14275v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The majority of automatic metrics for evaluating <b>NLG</b> systems are reference-based. However, the challenge of collecting human annotation results in a lack of reliable references in numerous application scenarios. Despite recent advancements in reference-free metrics, it has not been well understood when and where they can be used as an alternative to reference-based metrics. In this study, by employing diverse analytical approaches, we comprehensively assess the performance of both metrics across a wide range of <b>NLG</b> tasks, encompassing eight datasets and eight evaluation models. Based on solid experiments, the results show that reference-free metrics exhibit a higher correlation with human judgment and greater sensitivity to deficiencies in language quality. However, their effectiveness varies across tasks and is influenced by the quality of candidate texts. Therefore, it&rsquo;s important to assess the performance of reference-free metrics before applying them to a new task, especially when inputs are in uncommon form or when the answer space is highly variable. Our study can provide insight into the appropriate application of automatic metrics and the impact of metric choice on evaluation performance.</p></p class="citation"></blockquote><h3 id=5252--52312-multi-level-feedback-generation-with-large-language-models-for-empowering-novice-peer-counselors-alicja-chaszczewicz-et-al-2024>(52/52 | 52/312) Multi-Level Feedback Generation with Large Language Models for Empowering Novice Peer Counselors (Alicja Chaszczewicz et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alicja Chaszczewicz, Raj Sanjay Shah, Ryan Louie, Bruce A Arnow, Robert Kraut, Diyi Yang. (2024)<br><strong>Multi-Level Feedback Generation with Large Language Models for Empowering Novice Peer Counselors</strong><br><button class=copy-to-clipboard title="Multi-Level Feedback Generation with Large Language Models for Empowering Novice Peer Counselors" index=52>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-52 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-HC, cs-LG, cs.CL<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15482v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15482v1.pdf filename=2403.15482v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Realistic practice and tailored feedback are key processes for training peer counselors with clinical skills. However, existing mechanisms of providing feedback largely rely on human supervision. Peer counselors often lack mechanisms to receive detailed feedback from experienced mentors, making it difficult for them to support the <b>large</b> <b>number</b> <b>of</b> people with mental health issues who use peer counseling. Our work aims to leverage <b>large</b> <b>language</b> <b>models</b> to provide contextualized and multi-level feedback to empower peer counselors, especially novices, at scale. To achieve this, we co-design with a group of senior psychotherapy supervisors to develop a multi-level feedback taxonomy, and then construct a publicly available dataset with comprehensive feedback annotations of 400 emotional support conversations. We further design a self-improvement method on top of <b>large</b> <b>language</b> <b>models</b> to enhance the automatic generation of feedback. Via qualitative and quantitative evaluation with domain experts, we demonstrate that our method minimizes the risk of potentially harmful and low-quality feedback generation which is desirable in such high-stakes scenarios.</p></p class="citation"></blockquote><h2 id=csai-5>cs.AI (5)</h2><h3 id=15--53312-react-meets-actre-autonomous-annotation-of-agent-trajectories-for-contrastive-self-training-zonghan-yang-et-al-2024>(1/5 | 53/312) ReAct Meets ActRe: Autonomous Annotation of Agent Trajectories for Contrastive Self-Training (Zonghan Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zonghan Yang, Peng Li, Ming Yan, Ji Zhang, Fei Huang, Yang Liu. (2024)<br><strong>ReAct Meets ActRe: Autonomous Annotation of Agent Trajectories for Contrastive Self-Training</strong><br><button class=copy-to-clipboard title="ReAct Meets ActRe: Autonomous Annotation of Agent Trajectories for Contrastive Self-Training" index=53>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-53 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs-LG, cs.AI<br>Keyword Score: 90<br>Keywords: Fine-tuning, Fine-tuning, Foundation Model, GPT, GPT-4, Mistral, Reasoning, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14589v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14589v2.pdf filename=2403.14589v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Language agents have demonstrated autonomous decision-making abilities by <b>reasoning</b> with <b>foundation</b> <b>models.</b> Recently, efforts have been made to train language agents for performance improvement, with multi-step <b>reasoning</b> and action trajectories as the training data. However, collecting such trajectories still requires considerable human effort, by either artificial annotation or implementations of diverse <b>prompting</b> frameworks. In this work, we propose A$^3$T, a framework that enables the Autonomous Annotation of Agent Trajectories in the style of ReAct. The central role is an ActRe <b>prompting</b> agent, which explains the reason for an arbitrary action. When randomly sampling an external action, the ReAct-style agent could query the ActRe agent with the action to obtain its textual rationales. Novel trajectories are then synthesized by prepending the posterior <b>reasoning</b> from ActRe to the sampled action. In this way, the ReAct-style agent executes multiple trajectories for the failed tasks, and selects the successful ones to supplement its failed trajectory for contrastive self-training. Realized by policy gradient methods with binarized rewards, the contrastive self-training with accumulated trajectories facilitates a closed loop for multiple rounds of language agent self-improvement. We conduct experiments using QLoRA <b>fine-tuning</b> with the open-sourced <b>Mistral-7B-Instruct-v0.2.</b> In AlfWorld, the agent trained with A$^3$T obtains a 1-shot success rate of 96%, and 100% success with 4 iterative rounds. In WebShop, the 1-shot performance of the A$^3$T agent matches human average, and 4 rounds of iterative refinement lead to the performance approaching human experts. A$^3$T agents significantly outperform existing techniques, including <b>prompting</b> with <b>GPT-4,</b> advanced agent frameworks, and fully <b>fine-tuned</b> <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=25--54312-can-chatgpt-detect-deepfakes-a-study-of-using-multimodal-large-language-models-for-media-forensics-shan-jia-et-al-2024>(2/5 | 54/312) Can ChatGPT Detect DeepFakes? A Study of Using Multimodal Large Language Models for Media Forensics (Shan Jia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shan Jia, Reilin Lyu, Kangran Zhao, Yize Chen, Zhiyuan Yan, Yan Ju, Chuanbo Hu, Xin Li, Baoyuan Wu, Siwei Lyu. (2024)<br><strong>Can ChatGPT Detect DeepFakes? A Study of Using Multimodal Large Language Models for Media Forensics</strong><br><button class=copy-to-clipboard title="Can ChatGPT Detect DeepFakes? A Study of Using Multimodal Large Language Models for Media Forensics" index=54>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-54 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CR, cs.AI<br>Keyword Score: 46<br>Keywords: Multi-modal, Multi-modal, ChatGPT, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14077v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14077v2.pdf filename=2403.14077v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>DeepFakes, which refer to AI-generated media content, have become an increasing concern due to their use as a means for disinformation. Detecting DeepFakes is currently solved with programmed machine learning algorithms. In this work, we investigate the capabilities of <b>multimodal</b> <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> in DeepFake detection. We conducted qualitative and quantitative experiments to demonstrate <b>multimodal</b> <b>LLMs</b> and show that they can expose AI-generated images through careful experimental design and <b>prompt</b> engineering. This is interesting, considering that <b>LLMs</b> are not inherently tailored for media forensic tasks, and the process does not require programming. We discuss the limitations of <b>multimodal</b> <b>LLMs</b> for these tasks and suggest possible improvements.</p></p class="citation"></blockquote><h3 id=35--55312-open-knowledge-base-canonicalization-with-multi-task-learning-bingchen-liu-et-al-2024>(3/5 | 55/312) Open Knowledge Base Canonicalization with Multi-task Learning (Bingchen Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bingchen Liu, Huang Peng, Weixin Zeng, Xiang Zhao, Shijun Liu, Li Pan. (2024)<br><strong>Open Knowledge Base Canonicalization with Multi-task Learning</strong><br><button class=copy-to-clipboard title="Open Knowledge Base Canonicalization with Multi-task Learning" index=55>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-55 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs-LG, cs.AI<br>Keyword Score: 34<br>Keywords: Diffusion Model, Graph, Graph Embedding, Benchmarking, Clustering, Knowledge Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14733v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14733v1.pdf filename=2403.14733v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The construction of large open <b>knowledge</b> <b>bases</b> (OKBs) is integral to many <b>knowledge-driven</b> <b>applications</b> on the world wide web such as web search. However, noun phrases and relational phrases in OKBs often suffer from redundancy and ambiguity, which calls for the investigation on OKB canonicalization. Current solutions address OKB canonicalization by devising advanced <b>clustering</b> algorithms and using <b>knowledge</b> <b>graph</b> <b>embedding</b> (KGE) to further facilitate the canonicalization process. Nevertheless, these works fail to fully exploit the synergy between <b>clustering</b> and KGE learning, and the methods designed for these subtasks are sub-optimal. To this end, we put forward a multi-task learning framework, namely MulCanon, to tackle OKB canonicalization. In addition, <b>diffusion</b> <b>model</b> is used in the soft <b>clustering</b> process to improve the noun phrase representations with neighboring information, which can lead to more accurate representations. MulCanon unifies the learning objectives of these sub-tasks, and adopts a two-stage multi-task learning paradigm for training. A thorough experimental study on popular OKB canonicalization <b>benchmarks</b> validates that MulCanon can achieve competitive canonicalization results.</p></p class="citation"></blockquote><h3 id=45--56312-establishing-a-leader-in-a-pairwise-comparisons-method-jacek-szybowski-et-al-2024>(4/5 | 56/312) Establishing a leader in a pairwise comparisons method (Jacek Szybowski et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jacek Szybowski, Konrad Kułakowski, Jiri Mazurek, Sebastian Ernst. (2024)<br><strong>Establishing a leader in a pairwise comparisons method</strong><br><button class=copy-to-clipboard title="Establishing a leader in a pairwise comparisons method" index=56>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-56 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CR, cs-CY, cs-DM, cs.AI<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14885v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14885v1.pdf filename=2403.14885v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Abstract Like electoral systems, decision-making methods are also vulnerable to manipulation by decision-makers. The ability to effectively defend against such threats can only come from thoroughly understanding the manipulation mechanisms. In the presented article, we show two algorithms that can be used to launch a manipulation attack. They allow for equating the weights of two selected alternatives in the pairwise comparison method and, consequently, choosing a leader. The theoretical considerations are accompanied by a Monte Carlo <b>simulation</b> showing the relationship between the size of the PC matrix, the degree of inconsistency, and the ease of manipulation. This work is a continuation of our previous research published in the paper (Szybowski et al., 2023)</p></p class="citation"></blockquote><h3 id=55--57312-dourn-improving-douzero-by-residual-neural-networks-yiquan-chen-et-al-2024>(5/5 | 57/312) DouRN: Improving DouZero by Residual Neural Networks (Yiquan Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yiquan Chen, Yingchao Lyu, Di Zhang. (2024)<br><strong>DouRN: Improving DouZero by Residual Neural Networks</strong><br><button class=copy-to-clipboard title="DouRN: Improving DouZero by Residual Neural Networks" index=57>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-57 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LG, cs.AI<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14102v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14102v1.pdf filename=2403.14102v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep <b>reinforcement</b> <b>learning</b> has made significant progress in games with imperfect information, but its performance in the card game Doudizhu (Chinese Poker/Fight the Landlord) remains unsatisfactory. Doudizhu is different from conventional games as it involves three players and combines elements of cooperation and confrontation, resulting in a large state and action space. In 2021, a Doudizhu program called DouZero\cite{zha2021douzero} surpassed previous models without prior knowledge by utilizing traditional Monte Carlo methods and multilayer perceptrons. Building on this work, our study incorporates residual networks into the model, explores different architectural designs, and conducts multi-role testing. Our findings demonstrate that this model significantly improves the winning rate within the same training time. Additionally, we introduce a call scoring system to assist the agent in deciding whether to become a landlord. With these enhancements, our model consistently outperforms the existing version of DouZero and even experienced human players. \footnote{The source code is available at \url{https://github.com/Yingchaol/Douzero_Resnet.git.}</p></p class="citation"></blockquote><h2 id=cslg-35>cs.LG (35)</h2><h3 id=135--58312-exploring-the-potential-of-large-language-models-in-graph-generation-yang-yao-et-al-2024>(1/35 | 58/312) Exploring the Potential of Large Language Models in Graph Generation (Yang Yao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yang Yao, Xin Wang, Zeyang Zhang, Yijian Qin, Ziwei Zhang, Xu Chu, Yuekui Yang, Wenwu Zhu, Hong Mei. (2024)<br><strong>Exploring the Potential of Large Language Models in Graph Generation</strong><br><button class=copy-to-clipboard title="Exploring the Potential of Large Language Models in Graph Generation" index=58>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-58 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, q-bio-BM<br>Keyword Score: 83<br>Keywords: Node Classification, Graph, Few-shot, GPT, GPT-4, Chain-of-thought Prompt, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14358v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14358v1.pdf filename=2403.14358v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have achieved great success in many fields, and recent works have studied exploring <b>LLMs</b> for <b>graph</b> discriminative tasks such as <b>node</b> <b>classification.</b> However, the abilities of <b>LLMs</b> for <b>graph</b> generation remain unexplored in the literature. <b>Graph</b> generation requires the <b>LLM</b> to generate <b>graphs</b> with given properties, which has valuable real-world applications such as drug discovery, while tends to be more challenging. In this paper, we propose LLM4GraphGen to explore the ability of <b>LLMs</b> for <b>graph</b> generation with systematical task designs and extensive experiments. Specifically, we propose several tasks tailored with comprehensive experiments to address key questions regarding <b>LLMs&rsquo;</b> understanding of different <b>graph</b> structure rules, their ability to capture structural type distributions, and their utilization of domain knowledge for property-based <b>graph</b> generation. Our evaluations demonstrate that <b>LLMs,</b> particularly <b>GPT-4,</b> exhibit preliminary abilities in <b>graph</b> generation tasks, including rule-based and distribution-based generation. We also observe that popular <b>prompting</b> methods, such as <b>few-shot</b> and <b>chain-of-thought</b> <b>prompting,</b> do not consistently enhance performance. Besides, <b>LLMs</b> show potential in generating molecules with specific properties. These findings may serve as foundations for designing good <b>LLMs</b> based models for <b>graph</b> generation and provide valuable insights and further research.</p></p class="citation"></blockquote><h3 id=235--59312-exploring-task-unification-in-graph-representation-learning-via-generative-approach-yulan-hu-et-al-2024>(2/35 | 59/312) Exploring Task Unification in Graph Representation Learning via Generative Approach (Yulan Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yulan Hu, Sheng Ouyang, Zhirui Yang, Ge Chen, Junchen Wan, Xiao Wang, Yong Liu. (2024)<br><strong>Exploring Task Unification in Graph Representation Learning via Generative Approach</strong><br><button class=copy-to-clipboard title="Exploring Task Unification in Graph Representation Learning via Generative Approach" index=59>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-59 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 68<br>Keywords: Graph, Adversarial Learning, Autoencoder, Fine-tuning, Representation Learning, Self-supervised Learning, Transfer Learning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14340v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14340v1.pdf filename=2403.14340v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graphs</b> are ubiquitous in real-world scenarios and encompass a diverse range of tasks, from node-, edge-, and <b>graph-level</b> tasks to <b>transfer</b> <b>learning.</b> However, designing specific tasks for each type of <b>graph</b> data is often costly and lacks generalizability. Recent endeavors under the &ldquo;Pre-training + <b>Fine-tuning&rdquo;</b> or &ldquo;Pre-training + <b>Prompt&rdquo;</b> paradigms aim to design a unified framework capable of generalizing across multiple <b>graph</b> tasks. Among these, <b>graph</b> <b>autoencoders</b> (GAEs), generative <b>self-supervised</b> models, have demonstrated their potential in effectively addressing various <b>graph</b> tasks. Nevertheless, these methods typically employ multi-stage training and require adaptive designs, which on one hand make it difficult to be seamlessly applied to diverse <b>graph</b> tasks and on the other hand overlook the negative impact caused by discrepancies in task objectives between the different stages. To address these challenges, we propose GA^2E, a unified adversarially masked <b>autoencoder</b> capable of addressing the above challenges seamlessly. Specifically, GA^2E proposes to use the subgraph as the meta-structure, which remains consistent across all <b>graph</b> tasks (ranging from node-, edge-, and <b>graph-level</b> to <b>transfer</b> <b>learning)</b> and all stages (both during training and inference). Further, GA^2E operates in a \textbf{&ldquo;Generate then Discriminate&rdquo;} manner. It leverages the masked GAE to reconstruct the input subgraph whilst treating it as a generator to compel the reconstructed <b>graphs</b> resemble the input subgraph. Furthermore, GA^2E introduces an auxiliary discriminator to discern the authenticity between the reconstructed (generated) subgraph and the input subgraph, thus ensuring the robustness of the <b>graph</b> <b>representation</b> <b>through</b> <b>adversarial</b> <b>training</b> mechanisms. We validate GA^2E&rsquo;s capabilities through extensive experiments on 21 datasets across four types of <b>graph</b> tasks.</p></p class="citation"></blockquote><h3 id=335--60312-a-task-of-anomaly-detection-for-a-smart-satellite-internet-of-things-system-zilong-shao-2024>(3/35 | 60/312) A task of anomaly detection for a smart satellite Internet of things system (Zilong Shao, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zilong Shao. (2024)<br><strong>A task of anomaly detection for a smart satellite Internet of things system</strong><br><button class=copy-to-clipboard title="A task of anomaly detection for a smart satellite Internet of things system" index=60>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-60 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, eess-SP<br>Keyword Score: 60<br>Keywords: Anomaly Detection, Generative Adversarial Network, Supervised Learning, Supervised Learning, Unsupervised Learning, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14738v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14738v1.pdf filename=2403.14738v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>When the equipment is working, real-time collection of environmental sensor data for <b>anomaly</b> <b>detection</b> is one of the key links to prevent industrial process accidents and network attacks and ensure system security. However, under the environment with specific real-time requirements, the <b>anomaly</b> <b>detection</b> for environmental sensors still faces the following difficulties: (1) The complex nonlinear correlation characteristics between environmental sensor data variables lack effective expression methods, and the distribution between the data is difficult to be captured. (2) it is difficult to ensure the real-time monitoring requirements by using complex machine learning models, and the equipment cost is too high. (3) Too little sample data leads to less labeled data in <b>supervised</b> <b>learning.</b> This paper proposes an <b>unsupervised</b> deep learning <b>anomaly</b> <b>detection</b> system. Based on the <b>generative</b> <b>adversarial</b> <b>network</b> and <b>self-attention</b> mechanism, considering the different feature information contained in the local subsequences, it automatically learns the complex linear and nonlinear dependencies between environmental sensor variables, and uses the <b>anomaly</b> <b>score</b> calculation method combining reconstruction error and discrimination error. It can monitor the abnormal points of real sensor data with high real-time performance and can run on the intelligent satellite Internet of things system, which is suitable for the real working environment. <b>Anomaly</b> <b>detection</b> outperforms baseline methods in most cases and has good interpretability, which can be used to prevent industrial accidents and cyber-attacks for monitoring environmental sensors.</p></p class="citation"></blockquote><h3 id=435--61312-dp-rdm-adapting-diffusion-models-to-private-domains-without-fine-tuning-jonathan-lebensold-et-al-2024>(4/35 | 61/312) DP-RDM: Adapting Diffusion Models to Private Domains Without Fine-Tuning (Jonathan Lebensold et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jonathan Lebensold, Maziar Sanjabi, Pietro Astolfi, Adriana Romero-Soriano, Kamalika Chaudhuri, Mike Rabbat, Chuan Guo. (2024)<br><strong>DP-RDM: Adapting Diffusion Models to Private Domains Without Fine-Tuning</strong><br><button class=copy-to-clipboard title="DP-RDM: Adapting Diffusion Models to Private Domains Without Fine-Tuning" index=61>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-61 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-CV, cs-LG, cs.LG<br>Keyword Score: 60<br>Keywords: Diffusion Model, Fine-tuning, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14421v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14421v1.pdf filename=2403.14421v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Text-to-image</b> <b>diffusion</b> <b>models</b> have been shown to suffer from sample-level memorization, possibly reproducing near-perfect replica of images that they are trained on, which may be undesirable. To remedy this issue, we develop the first differentially private (DP) <b>retrieval-augmented</b> <b>generation</b> <b>algorithm</b> that is capable of generating high-quality image samples while providing provable privacy guarantees. Specifically, we assume access to a <b>text-to-image</b> <b>diffusion</b> <b>model</b> trained on a small amount of public data, and design a DP <b>retrieval</b> <b>mechanism</b> <b>to</b> augment the text <b>prompt</b> with samples retrieved from a private <b>retrieval</b> <b>dataset.</b> <b>Our</b> \emph{differentially private <b>retrieval-augmented</b> <b>diffusion</b> <b>model}</b> (DP-RDM) requires no <b>fine-tuning</b> on the <b>retrieval</b> <b>dataset</b> <b>to</b> adapt to another domain, and can use state-of-the-art generative models to generate high-quality image samples while satisfying rigorous DP guarantees. For instance, when evaluated on MS-COCO, our DP-RDM can generate samples with a privacy budget of $\epsilon=10$, while providing a $3.5$ point improvement in FID compared to public-only <b>retrieval</b> <b>for</b> <b>up</b> to $10,000$ queries.</p></p class="citation"></blockquote><h3 id=535--62312-isplib-a-library-for-accelerating-graph-neural-networks-using-auto-tuned-sparse-operations-md-saidul-hoque-anik-et-al-2024>(5/35 | 62/312) iSpLib: A Library for Accelerating Graph Neural Networks using Auto-tuned Sparse Operations (Md Saidul Hoque Anik et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Md Saidul Hoque Anik, Pranav Badhe, Rohit Gampa, Ariful Azad. (2024)<br><strong>iSpLib: A Library for Accelerating Graph Neural Networks using Auto-tuned Sparse Operations</strong><br><button class=copy-to-clipboard title="iSpLib: A Library for Accelerating Graph Neural Networks using Auto-tuned Sparse Operations" index=62>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-62 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-DC, cs-LG, cs-PF, cs.LG<br>Keyword Score: 53<br>Keywords: GraphSAGE, Graph, Graph Neural Network, Graph Neural Network, Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14853v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14853v1.pdf filename=2403.14853v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Core computations in <b>Graph</b> <b>Neural</b> <b>Network</b> <b>(GNN)</b> training and inference are often mapped to sparse matrix operations such as sparse-dense matrix multiplication (SpMM). These sparse operations are harder to optimize by manual tuning because their performance depends significantly on the sparsity of input <b>graphs,</b> <b>GNN</b> <b>models,</b> and computing platforms. To address this challenge, we present iSpLib, a PyTorch-based C++ library equipped with auto-tuned sparse operations. iSpLib expedites <b>GNN</b> training with a cache-enabled backpropagation that stores intermediate matrices in local caches. The library offers a user-friendly Python plug-in that allows users to take advantage of our optimized PyTorch operations out-of-the-box for any existing linear algebra-based PyTorch implementation of popular <b>GNNs</b> <b>(Graph</b> <b>Convolution</b> <b>Network,</b> <b>GraphSAGE,</b> <b>Graph</b> <b>Inference</b> <b>Network,</b> etc.) with only two lines of additional code. We demonstrate that iSpLib obtains up to 27x overall training speedup compared to the equivalent PyTorch 2.1.0 and PyTorch Geometric 2.4.0 implementations on the CPU. Our library is publicly available at <a href=https://github.com/HipGraph/iSpLib>https://github.com/HipGraph/iSpLib</a> (<a href=https://doi.org/10.5281/zenodo.10806511)>https://doi.org/10.5281/zenodo.10806511)</a>.</p></p class="citation"></blockquote><h3 id=635--63312-deep-learning-for-trajectory-data-management-and-mining-a-survey-and-beyond-wei-chen-et-al-2024>(6/35 | 63/312) Deep Learning for Trajectory Data Management and Mining: A Survey and Beyond (Wei Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wei Chen, Yuxuan Liang, Yuanshao Zhu, Yanchuan Chang, Kang Luo, Haomin Wen, Lei Li, Yanwei Yu, Qingsong Wen, Chao Chen, Kai Zheng, Yunjun Gao, Xiaofang Zhou, Yu Zheng. (2024)<br><strong>Deep Learning for Trajectory Data Management and Mining: A Survey and Beyond</strong><br><button class=copy-to-clipboard title="Deep Learning for Trajectory Data Management and Mining: A Survey and Beyond" index=63>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-63 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CY, cs-DB, cs-LG, cs.LG<br>Keyword Score: 50<br>Keywords: Anomaly Detection, Recommendation, Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14151v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14151v1.pdf filename=2403.14151v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Trajectory computing is a pivotal domain encompassing trajectory data management and mining, garnering widespread attention due to its crucial role in various practical applications such as location services, urban traffic, and public safety. Traditional methods, focusing on simplistic spatio-temporal features, face challenges of complex calculations, limited scalability, and inadequate adaptability to real-world complexities. In this paper, we present a comprehensive review of the development and recent advances in deep learning for trajectory computing (DL4Traj). We first define trajectory data and provide a brief overview of widely-used deep learning models. Systematically, we explore deep learning applications in trajectory management (pre-processing, storage, analysis, and visualization) and mining (trajectory-related forecasting, trajectory-related <b>recommendation,</b> trajectory classification, travel time estimation, <b>anomaly</b> <b>detection,</b> and mobility generation). Notably, we encapsulate recent advancements in <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> that hold the potential to augment trajectory computing. Additionally, we <b>summarize</b> application scenarios, public datasets, and toolkits. Finally, we outline current challenges in DL4Traj research and propose future directions. Relevant papers and open-source resources have been collated and are continuously updated at: \href{https://github.com/yoshall/Awesome-Trajectory-Computing}{DL4Traj Repo}.</p></p class="citation"></blockquote><h3 id=735--64312-deep-active-learning-a-reality-check-edrina-gashi-et-al-2024>(7/35 | 64/312) Deep Active Learning: A Reality Check (Edrina Gashi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Edrina Gashi, Jiankang Deng, Ismail Elezi. (2024)<br><strong>Deep Active Learning: A Reality Check</strong><br><button class=copy-to-clipboard title="Deep Active Learning: A Reality Check" index=64>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-64 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Object Detection, Active Learning, Recommendation, Semi-Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14800v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14800v1.pdf filename=2403.14800v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We conduct a comprehensive evaluation of state-of-the-art deep <b>active</b> <b>learning</b> methods. Surprisingly, under general settings, no single-model method decisively outperforms entropy-based <b>active</b> <b>learning,</b> and some even fall short of random sampling. We delve into overlooked aspects like starting budget, budget step, and pretraining&rsquo;s impact, revealing their significance in achieving superior results. Additionally, we extend our evaluation to other tasks, exploring the <b>active</b> <b>learning</b> effectiveness in combination with <b>semi-supervised</b> <b>learning,</b> and <b>object</b> <b>detection.</b> Our experiments provide valuable insights and concrete <b>recommendations</b> for future <b>active</b> <b>learning</b> studies. By uncovering the limitations of current methods and understanding the impact of different experimental settings, we aim to inspire more efficient training of deep learning models in real-world scenarios with limited annotation budgets. This work contributes to advancing <b>active</b> <b>learning&rsquo;s</b> efficacy in deep learning and empowers researchers to make informed decisions when applying <b>active</b> <b>learning</b> to their tasks.</p></p class="citation"></blockquote><h3 id=835--65312-ai-and-memory-wall-amir-gholami-et-al-2024>(8/35 | 65/312) AI and Memory Wall (Amir Gholami et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amir Gholami, Zhewei Yao, Sehoon Kim, Coleman Hooper, Michael W. Mahoney, Kurt Keutzer. (2024)<br><strong>AI and Memory Wall</strong><br><button class=copy-to-clipboard title="AI and Memory Wall" index=65>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-65 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AR, cs-DC, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Unsupervised Learning, Transformer, Large Language Model, Scaling Law<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14123v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14123v1.pdf filename=2403.14123v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The availability of unprecedented <b>unsupervised</b> training data, along with neural <b>scaling</b> <b>laws,</b> has resulted in an unprecedented surge in model size and compute requirements for serving/training <b>LLMs.</b> However, the main performance bottleneck is increasingly shifting to memory bandwidth. Over the past 20 years, peak server hardware FLOPS has been <b>scaling</b> <b>at</b> 3.0x/2yrs, outpacing the growth of DRAM and interconnect bandwidth, which have only scaled at 1.6 and 1.4 times every 2 years, respectively. This disparity has made memory, rather than compute, the primary bottleneck in AI applications, particularly in serving. Here, we analyze encoder and decoder <b>Transformer</b> models and show how memory bandwidth can become the dominant bottleneck for decoder models. We argue for a redesign in model architecture, training, and deployment strategies to overcome this memory limitation.</p></p class="citation"></blockquote><h3 id=935--66312-heuristic-algorithm-based-action-masking-reinforcement-learning-haam-rl-with-ensemble-inference-method-kyuwon-choi-et-al-2024>(9/35 | 66/312) Heuristic Algorithm-based Action Masking Reinforcement Learning (HAAM-RL) with Ensemble Inference Method (Kyuwon Choi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kyuwon Choi, Cheolkyun Rho, Taeyoun Kim, Daewoo Choi. (2024)<br><strong>Heuristic Algorithm-based Action Masking Reinforcement Learning (HAAM-RL) with Ensemble Inference Method</strong><br><button class=copy-to-clipboard title="Heuristic Algorithm-based Action Masking Reinforcement Learning (HAAM-RL) with Ensemble Inference Method" index=66>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-66 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Markov Decision Process, Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14110v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14110v1.pdf filename=2403.14110v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a novel <b>reinforcement</b> <b>learning</b> (RL) approach called HAAM-RL (Heuristic Algorithm-based Action Masking <b>Reinforcement</b> <b>Learning)</b> for optimizing the color batching re-sequencing problem in automobile painting processes. The existing heuristic algorithms have limitations in adequately reflecting real-world constraints and accurately predicting logistics performance. Our methodology incorporates several key techniques including a tailored <b>Markov</b> <b>Decision</b> <b>Process</b> (MDP) formulation, reward setting including Potential-Based Reward Shaping, action masking using heuristic algorithms (HAAM-RL), and an ensemble inference method that combines multiple RL models. The RL agent is trained and evaluated using FlexSim, a commercial 3D <b>simulation</b> software, integrated with our RL MLOps platform BakingSoDA. Experimental results across 30 scenarios demonstrate that HAAM-RL with an ensemble inference method achieves a 16.25% performance improvement over the conventional heuristic algorithm, with stable and consistent results. The proposed approach exhibits superior performance and generalization capability, indicating its effectiveness in optimizing complex manufacturing processes. The study also discusses future research directions, including alternative state representations, incorporating model-based RL methods, and integrating additional real-world constraints.</p></p class="citation"></blockquote><h3 id=1035--67312-hyperbolic-secant-representation-of-the-logistic-function-application-to-probabilistic-multiple-instance-learning-for-ct-intracranial-hemorrhage-detection-f-m-castro-macías-et-al-2024>(10/35 | 67/312) Hyperbolic Secant representation of the logistic function: Application to probabilistic Multiple Instance Learning for CT intracranial hemorrhage detection (F. M. Castro-Macías et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>F. M. Castro-Macías, P. Morales-Álvarez, Y. Wu, R. Molina, A. K. Katsaggelos. (2024)<br><strong>Hyperbolic Secant representation of the logistic function: Application to probabilistic Multiple Instance Learning for CT intracranial hemorrhage detection</strong><br><button class=copy-to-clipboard title="Hyperbolic Secant representation of the logistic function: Application to probabilistic Multiple Instance Learning for CT intracranial hemorrhage detection" index=67>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-67 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 33<br>Keywords: Benchmarking, Multiple Instance Learning, Supervised Learning, Weakly-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14829v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14829v1.pdf filename=2403.14829v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multiple</b> <b>Instance</b> <b>Learning</b> (MIL) is a weakly <b>supervised</b> paradigm that has been successfully applied to many different scientific areas and is particularly well suited to medical imaging. Probabilistic MIL methods, and more specifically Gaussian Processes (GPs), have achieved excellent results due to their high expressiveness and uncertainty quantification capabilities. One of the most successful GP-based MIL methods, VGPMIL, resorts to a variational bound to handle the intractability of the logistic function. Here, we formulate VGPMIL using P'olya-Gamma random variables. This approach yields the same variational posterior approximations as the original VGPMIL, which is a consequence of the two representations that the Hyperbolic Secant distribution admits. This leads us to propose a general GP-based MIL method that takes different forms by simply leveraging distributions other than the Hyperbolic Secant one. Using the Gamma distribution we arrive at a new approach that obtains competitive or superior predictive performance and efficiency. This is validated in a comprehensive experimental study including one synthetic MIL dataset, two well-known MIL <b>benchmarks,</b> and a real-world medical problem. We expect that this work provides useful ideas beyond MIL that can foster further research in the field.</p></p class="citation"></blockquote><h3 id=1135--68312-rambla-a-framework-for-evaluating-the-reliability-of-llms-as-assistants-in-the-biomedical-domain-william-james-bolton-et-al-2024>(11/35 | 68/312) RAmBLA: A Framework for Evaluating the Reliability of LLMs as Assistants in the Biomedical Domain (William James Bolton et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>William James Bolton, Rafael Poyiadzi, Edward R. Morrell, Gabriela van Bergen Gonzalez Bueno, Lea Goetz. (2024)<br><strong>RAmBLA: A Framework for Evaluating the Reliability of LLMs as Assistants in the Biomedical Domain</strong><br><button class=copy-to-clipboard title="RAmBLA: A Framework for Evaluating the Reliability of LLMs as Assistants in the Biomedical Domain" index=68>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-68 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14578v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14578v1.pdf filename=2403.14578v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> increasingly support applications in a wide range of domains, some with potential high societal impact such as biomedicine, yet their reliability in realistic use cases is under-researched. In this work we introduce the Reliability AssesMent for Biomedical <b>LLM</b> Assistants (RAmBLA) framework and evaluate whether four state-of-the-art foundation <b>LLMs</b> can serve as reliable assistants in the biomedical domain. We identify <b>prompt</b> robustness, high recall, and a lack of hallucinations as necessary criteria for this use case. We design shortform tasks and tasks requiring <b>LLM</b> freeform responses mimicking real-world user interactions. We evaluate <b>LLM</b> performance using semantic similarity with a ground truth response, through an evaluator <b>LLM.</b></p></p class="citation"></blockquote><h3 id=1235--69312-task-optimal-data-driven-surrogate-models-for-enmpc-via-differentiable-simulation-and-optimization-daniel-mayfrank-et-al-2024>(12/35 | 69/312) Task-optimal data-driven surrogate models for eNMPC via differentiable simulation and optimization (Daniel Mayfrank et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daniel Mayfrank, Na Young Ahn, Alexander Mitsos, Manuel Dahmen. (2024)<br><strong>Task-optimal data-driven surrogate models for eNMPC via differentiable simulation and optimization</strong><br><button class=copy-to-clipboard title="Task-optimal data-driven surrogate models for eNMPC via differentiable simulation and optimization" index=69>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-69 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-OC<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14425v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14425v1.pdf filename=2403.14425v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a method for end-to-end learning of Koopman surrogate models for optimal performance in control. In contrast to previous contributions that employ standard <b>reinforcement</b> <b>learning</b> (RL) algorithms, we use a training algorithm that exploits the potential differentiability of environments based on mechanistic <b>simulation</b> models. We evaluate the performance of our method by comparing it to that of other controller type and training algorithm combinations on a literature known eNMPC case study. Our method exhibits superior performance on this problem, thereby constituting a promising avenue towards more capable controllers that employ dynamic surrogate models.</p></p class="citation"></blockquote><h3 id=1335--70312-advancing-iiot-with-over-the-air-federated-learning-the-role-of-iterative-magnitude-pruning-fazal-muhammad-ali-khan-et-al-2024>(13/35 | 70/312) Advancing IIoT with Over-the-Air Federated Learning: The Role of Iterative Magnitude Pruning (Fazal Muhammad Ali Khan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fazal Muhammad Ali Khan, Hatem Abou-Zeid, Aryan Kaushik, Syed Ali Hassan. (2024)<br><strong>Advancing IIoT with Over-the-Air Federated Learning: The Role of Iterative Magnitude Pruning</strong><br><button class=copy-to-clipboard title="Advancing IIoT with Over-the-Air Federated Learning: The Role of Iterative Magnitude Pruning" index=70>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-70 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, eess-SP<br>Keyword Score: 30<br>Keywords: Federated Learning, Model Compression, Pruning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14120v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14120v1.pdf filename=2403.14120v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The industrial Internet of Things (IIoT) under Industry 4.0 heralds an era of interconnected smart devices where data-driven insights and machine learning (ML) fuse to revolutionize manufacturing. A noteworthy development in IIoT is the integration of <b>federated</b> <b>learning</b> (FL), which addresses data privacy and security among devices. FL enables edge sensors, also known as peripheral intelligence units (PIUs) to learn and adapt using their data locally, without explicit sharing of confidential data, to facilitate a collaborative yet confidential learning process. However, the lower memory footprint and computational power of PIUs inherently require deep neural network (DNN) <b>models</b> <b>that</b> have a very compact size. <b>Model</b> <b>compression</b> techniques such as <b>pruning</b> can be used to reduce the size of DNN <b>models</b> <b>by</b> removing unnecessary connections that have little impact on the <b>model&rsquo;s</b> <b>performance,</b> thus making the <b>models</b> <b>more</b> suitable for the limited resources of PIUs. Targeting the notion of compact yet robust DNN <b>models,</b> <b>we</b> propose the integration of iterative magnitude <b>pruning</b> (IMP) of the DNN <b>model</b> <b>being</b> trained in an over-the-air FL (OTA-FL) environment for IIoT. We provide a tutorial overview and also present a case study of the effectiveness of IMP in OTA-FL for an IIoT environment. Finally, we present future directions for enhancing and optimizing these deep compression techniques further, aiming to push the boundaries of IIoT capabilities in acquiring compact yet robust and high-performing DNN models.</p></p class="citation"></blockquote><h3 id=1435--71312-domainlab-a-modular-python-package-for-domain-generalization-in-deep-learning-xudong-sun-et-al-2024>(14/35 | 71/312) DomainLab: A modular Python package for domain generalization in deep learning (Xudong Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xudong Sun, Carla Feistner, Alexej Gossmann, George Schwarz, Rao Muhammad Umer, Lisa Beer, Patrick Rockenschaub, Rahul Babu Shrestha, Armin Gruber, Nutan Chen, Sayedali Shetab Boushehri, Florian Buettner, Carsten Marr. (2024)<br><strong>DomainLab: A modular Python package for domain generalization in deep learning</strong><br><button class=copy-to-clipboard title="DomainLab: A modular Python package for domain generalization in deep learning" index=71>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-71 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-SE, cs.LG<br>Keyword Score: 26<br>Keywords: Benchmarking, Benchmarking, Distribution Shift, Distribution Shift, Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14356v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14356v1.pdf filename=2403.14356v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Poor generalization performance caused by <b>distribution</b> <b>shifts</b> in unseen domains often hinders the trustworthy deployment of deep neural networks. Many domain generalization techniques address this problem by adding a domain invariant regularization loss terms during training. However, there is a lack of modular software that allows users to combine the advantages of different methods with minimal effort for reproducibility. DomainLab is a modular Python package for training user specified neural networks with composable regularization loss terms. Its decoupled design allows the separation of neural networks from regularization loss construction. Hierarchical combinations of neural networks, different domain generalization methods, and associated hyperparameters, can all be specified together with other experimental setup in a single configuration file. Hierarchical combinations of neural networks, different domain generalization methods, and associated hyperparameters, can all be specified together with other experimental setup in a single configuration file. In addition, DomainLab offers powerful <b>benchmarking</b> functionality to evaluate the generalization performance of neural networks in <b>out-of-distribution</b> data. The package supports running the specified <b>benchmark</b> on an HPC cluster or on a standalone machine. The package is well tested with over 95 percent coverage and well documented. From the user perspective, it is closed to modification but open to extension. The package is under the MIT license, and its source code, tutorial and documentation can be found at <a href=https://github.com/marrlab/DomainLab>https://github.com/marrlab/DomainLab</a>.</p></p class="citation"></blockquote><h3 id=1535--72312-diffstock-probabilistic-relational-stock-market-predictions-using-diffusion-models-divyanshu-daiya-et-al-2024>(15/35 | 72/312) DiffSTOCK: Probabilistic relational Stock Market Predictions using Diffusion Models (Divyanshu Daiya et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Divyanshu Daiya, Monika Yadav, Harshit Singh Rao. (2024)<br><strong>DiffSTOCK: Probabilistic relational Stock Market Predictions using Diffusion Models</strong><br><button class=copy-to-clipboard title="DiffSTOCK: Probabilistic relational Stock Market Predictions using Diffusion Models" index=72>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-72 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CE, cs-LG, cs.LG, q-fin-CP, q-fin-PM<br>Keyword Score: 23<br>Keywords: Diffusion Model, Graph, Probabilistic Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14063v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14063v1.pdf filename=2403.14063v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we propose an approach to generalize denoising <b>diffusion</b> <b>probabilistic</b> <b>models</b> for stock market predictions and portfolio management. Present works have demonstrated the efficacy of modeling interstock relations for market time-series forecasting and utilized <b>Graph-based</b> learning models for value prediction and portfolio management. Though convincing, these deterministic approaches still fall short of handling uncertainties i.e., due to the low signal-to-noise ratio of the financial data, it is quite challenging to learn effective deterministic models. Since the <b>probabilistic</b> <b>methods</b> have shown to effectively emulate higher uncertainties for time-series predictions. To this end, we showcase effective utilisation of Denoising <b>Diffusion</b> <b>Probabilistic</b> <b>Models</b> (DDPM), to develop an architecture for providing better market predictions conditioned on the historical financial indicators and inter-stock relations. Additionally, we also provide a novel deterministic architecture MaTCHS which uses Masked Relational Transformer(MRT) to exploit inter-stock relations along with historical stock features. We demonstrate that our model achieves SOTA performance for movement predication and Portfolio management.</p></p class="citation"></blockquote><h3 id=1635--73312-parameter-efficient-fine-tuning-for-large-models-a-comprehensive-survey-zeyu-han-et-al-2024>(16/35 | 73/312) Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey (Zeyu Han et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zeyu Han, Chao Gao, Jinyang Liu, Jeff, Zhang, Sai Qian Zhang. (2024)<br><strong>Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey</strong><br><button class=copy-to-clipboard title="Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey" index=73>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-73 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Fine-tuning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14608v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14608v1.pdf filename=2403.14608v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>models</b> <b>represent</b> a groundbreaking advancement in multiple application fields, enabling remarkable achievements across various tasks. However, their unprecedented scale comes with significant computational costs. These models, often consisting of billions of parameters, require vast amounts of computational resources for execution. Especially, the expansive scale and computational demands pose considerable challenges when customizing them for particular downstream tasks, particularly over the hardware platforms constrained by computational capabilities. Parameter Efficient <b>Fine-Tuning</b> (PEFT) provides a practical solution by efficiently adapt the <b>large</b> <b>models</b> <b>over</b> the various downstream tasks. In particular, PEFT refers to the process of adjusting the parameters of a pre-trained <b>large</b> <b>models</b> <b>to</b> adapt it to a specific task while minimizing the number of additional parameters introduced or computational resources required. This approach is particularly important when dealing with <b>large</b> <b>language</b> <b>models</b> with high parameter counts, as <b>fine-tuning</b> these models from scratch can be computationally expensive and resource-intensive, posing considerable challenges in the supporting system platform design. In this survey, we present comprehensive studies of various PEFT algorithms, examining their performance and computational overhead. Moreover, we provide an overview of applications developed using different PEFT algorithms and discuss common techniques employed to mitigate computation costs for PEFT. In addition to the algorithmic perspective, we overview various real-world system designs to investigate the implementation costs associated with different PEFT algorithms. This survey serves as an indispensable resource for researchers aiming to understand both the PEFT algorithm and its system implementation, offering detailed insights into recent advancements and practical applications.</p></p class="citation"></blockquote><h3 id=1735--74312-rethinking-adversarial-inverse-reinforcement-learning-from-the-angles-of-policy-imitation-and-transferable-reward-recovery-yangchun-zhang-et-al-2024>(17/35 | 74/312) Rethinking Adversarial Inverse Reinforcement Learning: From the Angles of Policy Imitation and Transferable Reward Recovery (Yangchun Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yangchun Zhang, Yirui Zhou. (2024)<br><strong>Rethinking Adversarial Inverse Reinforcement Learning: From the Angles of Policy Imitation and Transferable Reward Recovery</strong><br><button class=copy-to-clipboard title="Rethinking Adversarial Inverse Reinforcement Learning: From the Angles of Policy Imitation and Transferable Reward Recovery" index=74>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-74 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 20<br>Keywords: Markov Decision Process, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14593v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14593v1.pdf filename=2403.14593v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Adversarial inverse <b>reinforcement</b> <b>learning</b> (AIRL) stands as a cornerstone approach in imitation learning. This paper rethinks the two different angles of AIRL: policy imitation and transferable reward recovery. We begin with substituting the built-in algorithm in AIRL with soft actor-critic (SAC) during the policy optimization process to enhance sample efficiency, thanks to the off-policy formulation of SAC and identifiable <b>Markov</b> <b>decision</b> <b>process</b> (MDP) models with respect to AIRL. It indeed exhibits a significant improvement in policy imitation but accidentally brings drawbacks to transferable reward recovery. To learn this issue, we illustrate that the SAC algorithm itself is not feasible to disentangle the reward function comprehensively during the AIRL training process, and propose a hybrid framework, PPO-AIRL + SAC, for satisfactory transfer effect. Additionally, we analyze the capability of environments to extract disentangled rewards from an algebraic theory perspective.</p></p class="citation"></blockquote><h3 id=1835--75312-fedmef-towards-memory-efficient-federated-dynamic-pruning-hong-huang-et-al-2024>(18/35 | 75/312) FedMef: Towards Memory-efficient Federated Dynamic Pruning (Hong Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hong Huang, Weiming Zhuang, Chen Chen, Lingjuan Lyu. (2024)<br><strong>FedMef: Towards Memory-efficient Federated Dynamic Pruning</strong><br><button class=copy-to-clipboard title="FedMef: Towards Memory-efficient Federated Dynamic Pruning" index=75>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-75 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-DC, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Federated Learning, Pruning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14737v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14737v1.pdf filename=2403.14737v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Federated</b> <b>learning</b> (FL) promotes decentralized training while prioritizing data confidentiality. However, its application on resource-constrained devices is challenging due to the high demand for computation and memory resources to train deep learning models. Neural network <b>pruning</b> techniques, such as dynamic <b>pruning,</b> could enhance model efficiency, but directly adopting them in FL still poses substantial challenges, including post-pruning performance degradation, high activation memory usage, etc. To address these challenges, we propose FedMef, a novel and memory-efficient <b>federated</b> <b>dynamic</b> <b>pruning</b> framework. FedMef comprises two key components. First, we introduce the budget-aware extrusion that maintains <b>pruning</b> efficiency while preserving post-pruning performance by salvaging crucial information from parameters marked for <b>pruning</b> within a given budget. Second, we propose scaled activation <b>pruning</b> to effectively reduce activation memory footprints, which is particularly beneficial for deploying FL to memory-limited devices. Extensive experiments demonstrate the effectiveness of our proposed FedMef. In particular, it achieves a significant reduction of 28.5% in memory footprint compared to state-of-the-art methods while obtaining superior accuracy.</p></p class="citation"></blockquote><h3 id=1935--76312-foundation-models-for-time-series-analysis-a-tutorial-and-survey-yuxuan-liang-et-al-2024>(19/35 | 76/312) Foundation Models for Time Series Analysis: A Tutorial and Survey (Yuxuan Liang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuxuan Liang, Haomin Wen, Yuqi Nie, Yushan Jiang, Ming Jin, Dongjin Song, Shirui Pan, Qingsong Wen. (2024)<br><strong>Foundation Models for Time Series Analysis: A Tutorial and Survey</strong><br><button class=copy-to-clipboard title="Foundation Models for Time Series Analysis: A Tutorial and Survey" index=76>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-76 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Fine-tuning, Foundation Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14735v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14735v1.pdf filename=2403.14735v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Time series analysis stands as a focal point within the data mining community, serving as a cornerstone for extracting valuable insights crucial to a myriad of real-world applications. Recent advancements in <b>Foundation</b> <b>Models</b> (FMs) have fundamentally reshaped the paradigm of model design for time series analysis, boosting various downstream tasks in practice. These innovative approaches often leverage pre-trained or <b>fine-tuned</b> FMs to harness generalized knowledge tailored specifically for time series analysis. In this survey, we aim to furnish a comprehensive and up-to-date overview of FMs for time series analysis. While prior surveys have predominantly focused on either the application or the pipeline aspects of FMs in time series analysis, they have often lacked an in-depth understanding of the underlying mechanisms that elucidate why and how FMs benefit time series analysis. To address this gap, our survey adopts a model-centric classification, delineating various pivotal elements of time-series FMs, including model architectures, pre-training techniques, adaptation methods, and data modalities. Overall, this survey serves to consolidate the latest advancements in FMs pertinent to time series analysis, accentuating their theoretical underpinnings, recent strides in development, and avenues for future research exploration.</p></p class="citation"></blockquote><h3 id=2035--77312-contrastive-balancing-representation-learning-for-heterogeneous-dose-response-curves-estimation-minqin-zhu-et-al-2024>(20/35 | 77/312) Contrastive Balancing Representation Learning for Heterogeneous Dose-Response Curves Estimation (Minqin Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Minqin Zhu, Anpeng Wu, Haoxuan Li, Ruoxuan Xiong, Bo Li, Xiaoqing Yang, Xuan Qin, Peng Zhen, Jiecheng Guo, Fei Wu, Kun Kuang. (2024)<br><strong>Contrastive Balancing Representation Learning for Heterogeneous Dose-Response Curves Estimation</strong><br><button class=copy-to-clipboard title="Contrastive Balancing Representation Learning for Heterogeneous Dose-Response Curves Estimation" index=77>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-77 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 15<br>Keywords: Counter-factual, Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14232v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14232v1.pdf filename=2403.14232v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Estimating the individuals&rsquo; potential response to varying treatment doses is crucial for decision-making in areas such as precision medicine and management science. Most recent studies predict <b>counterfactual</b> outcomes by learning a covariate <b>representation</b> <b>that</b> is independent of the treatment variable. However, such independence constraints neglect much of the covariate information that is useful for <b>counterfactual</b> prediction, especially when the treatment variables are continuous. To tackle the above issue, in this paper, we first theoretically demonstrate the importance of the balancing and prognostic <b>representations</b> <b>for</b> unbiased estimation of the heterogeneous dose-response curves, that is, the learned <b>representations</b> <b>are</b> constrained to satisfy the conditional independence between the covariates and both of the treatment variables and the potential responses. Based on this, we propose a novel Contrastive balancing <b>Representation</b> <b>learning</b> Network using a partial distance measure, called CRNet, for estimating the heterogeneous dose-response curves without losing the continuity of treatments. Extensive experiments are conducted on synthetic and real-world datasets demonstrating that our proposal significantly outperforms previous methods.</p></p class="citation"></blockquote><h3 id=2135--78312-hypothesis-driven-deep-learning-for-out-of-distribution-detection-yasith-jayawardana-et-al-2024>(21/35 | 78/312) Hypothesis-Driven Deep Learning for Out of Distribution Detection (Yasith Jayawardana et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yasith Jayawardana, Azeem Ahmad, Balpreet S. Ahluwalia, Rafi Ahmad, Sampath Jayarathna, Dushan N. Wadduwage. (2024)<br><strong>Hypothesis-Driven Deep Learning for Out of Distribution Detection</strong><br><button class=copy-to-clipboard title="Hypothesis-Driven Deep Learning for Out of Distribution Detection" index=78>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-78 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 15<br>Keywords: Black Box, Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14058v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14058v1.pdf filename=2403.14058v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Predictions of opaque <b>black-box</b> <b>systems</b> are frequently deployed in high-stakes applications such as healthcare. For such applications, it is crucial to assess how models handle samples beyond the domain of training data. While several metrics and tests exist to detect <b>out-of-distribution</b> (OoD) data from in-distribution (InD) data to a deep neural network (DNN), their performance varies significantly across datasets, models, and tasks, which limits their practical use. In this paper, we propose a hypothesis-driven approach to quantify whether a new sample is InD or OoD. Given a trained DNN and some input, we first feed the input through the DNN and compute an ensemble of OoD metrics, which we term latent responses. We then formulate the OoD detection problem as a hypothesis test between latent responses of different groups, and use permutation-based resampling to infer the significance of the observed latent responses under a null hypothesis. We adapt our method to detect an unseen sample of bacteria to a trained deep learning model, and show that it reveals interpretable differences between InD and OoD latent responses. Our work has implications for systematic novelty detection and informed decision-making from classifiers trained on a subset of labels.</p></p class="citation"></blockquote><h3 id=2235--79312-soft-learning-probabilistic-circuits-soroush-ghandi-et-al-2024>(22/35 | 79/312) Soft Learning Probabilistic Circuits (Soroush Ghandi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Soroush Ghandi, Benjamin Quost, Cassio de Campos. (2024)<br><strong>Soft Learning Probabilistic Circuits</strong><br><button class=copy-to-clipboard title="Soft Learning Probabilistic Circuits" index=79>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-79 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 13<br>Keywords: Clustering, Probabilistic Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14504v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14504v1.pdf filename=2403.14504v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Probabilistic</b> <b>Circuits</b> (PCs) are prominent tractable <b>probabilistic</b> <b>models,</b> allowing for a range of exact inferences. This paper focuses on the main algorithm for training PCs, LearnSPN, a gold standard due to its efficiency, performance, and ease of use, in particular for tabular data. We show that LearnSPN is a greedy likelihood maximizer under mild assumptions. While inferences in PCs may use the entire circuit structure for processing queries, LearnSPN applies a hard method for learning them, propagating at each sum node a data point through one and only one of the children/edges as in a hard <b>clustering</b> process. We propose a new learning procedure named SoftLearn, that induces a PC using a soft <b>clustering</b> process. We investigate the effect of this learning-inference compatibility in PCs. Our experiments show that SoftLearn outperforms LearnSPN in many situations, yielding better likelihoods and arguably better samples. We also analyze comparable tractable models to highlight the differences between soft/hard learning and model querying.</p></p class="citation"></blockquote><h3 id=2335--80312-hypergale-asd-classification-via-hypergraph-gated-attention-with-learnable-hyperedges-mehul-arora-et-al-2024>(23/35 | 80/312) HyperGALE: ASD Classification via Hypergraph Gated Attention with Learnable Hyperedges (Mehul Arora et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mehul Arora, Chirag Shantilal Jain, Lalith Bharadwaj Baru, Kamalaker Dadi, Bapi Raju Surampudi. (2024)<br><strong>HyperGALE: ASD Classification via Hypergraph Gated Attention with Learnable Hyperedges</strong><br><button class=copy-to-clipboard title="HyperGALE: ASD Classification via Hypergraph Gated Attention with Learnable Hyperedges" index=80>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-80 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs-NE, cs.LG<br>Keyword Score: 13<br>Keywords: Graph Attention Networks, Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14484v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14484v1.pdf filename=2403.14484v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Autism Spectrum Disorder (ASD) is a neurodevelopmental condition characterized by varied social cognitive challenges and repetitive behavioral patterns. Identifying reliable brain imaging-based biomarkers for ASD has been a persistent challenge due to the spectrum&rsquo;s diverse symptomatology. Existing baselines in the field have made significant strides in this direction, yet there remains room for improvement in both performance and interpretability. We propose \emph{HyperGALE}, which builds upon the hypergraph by incorporating learned hyperedges and <b>gated</b> attention mechanisms. This approach has led to substantial improvements in the model&rsquo;s ability to interpret complex brain <b>graph</b> data, offering deeper insights into ASD biomarker characterization. Evaluated on the extensive ABIDE II dataset, \emph{HyperGALE} not only improves interpretability but also demonstrates statistically significant enhancements in key performance metrics compared to both previous baselines and the foundational hypergraph model. The advancement \emph{HyperGALE} brings to ASD research highlights the potential of sophisticated <b>graph-based</b> techniques in neurodevelopmental studies. The source code and implementation instructions are available at GitHub:https://github.com/mehular0ra/HyperGALE.</p></p class="citation"></blockquote><h3 id=2435--81312-emergent-world-models-and-latent-variable-estimation-in-chess-playing-language-models-adam-karvonen-2024>(24/35 | 81/312) Emergent World Models and Latent Variable Estimation in Chess-Playing Language Models (Adam Karvonen, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Adam Karvonen. (2024)<br><strong>Emergent World Models and Latent Variable Estimation in Chess-Playing Language Models</strong><br><button class=copy-to-clipboard title="Emergent World Models and Latent Variable Estimation in Chess-Playing Language Models" index=81>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-81 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: GPT<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15498v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15498v1.pdf filename=2403.15498v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Language models have shown unprecedented capabilities, sparking debate over the source of their performance. Is it merely the outcome of learning syntactic patterns and surface level statistics, or do they extract semantics and a world model from the text? Prior work by Li et al. investigated this by training a <b>GPT</b> model on synthetic, randomly generated Othello games and found that the model learned an internal representation of the board state. We extend this work into the more complex domain of chess, training on real games and investigating our model&rsquo;s internal representations using linear probes and contrastive activations. The model is given no a priori knowledge of the game and is solely trained on next character prediction, yet we find evidence of internal representations of board state. We validate these internal representations by using them to make interventions on the model&rsquo;s activations and edit its internal board state. Unlike Li et al&rsquo;s prior synthetic dataset approach, our analysis finds that the model also learns to estimate latent variables like player skill to better predict the next character. We derive a player skill vector and add it to the model, improving the model&rsquo;s win rate by up to 2.6 times.</p></p class="citation"></blockquote><h3 id=2535--82312-constrained-reinforcement-learning-with-smoothed-log-barrier-function-baohe-zhang-et-al-2024>(25/35 | 82/312) Constrained Reinforcement Learning with Smoothed Log Barrier Function (Baohe Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Baohe Zhang, Yuan Zhang, Lilli Frison, Thomas Brox, Joschka Bödecker. (2024)<br><strong>Constrained Reinforcement Learning with Smoothed Log Barrier Function</strong><br><button class=copy-to-clipboard title="Constrained Reinforcement Learning with Smoothed Log Barrier Function" index=82>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-82 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-SY, cs.LG, eess-SY<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14508v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14508v1.pdf filename=2403.14508v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Reinforcement</b> <b>Learning</b> (RL) has been widely applied to many control tasks and substantially improved the performances compared to conventional control methods in many domains where the reward function is well defined. However, for many real-world problems, it is often more convenient to formulate optimization problems in terms of rewards and constraints simultaneously. Optimizing such constrained problems via reward shaping can be difficult as it requires tedious manual tuning of reward functions with several interacting terms. Recent formulations which include constraints mostly require a pre-training phase, which often needs human expertise to collect data or assumes having a sub-optimal policy readily available. We propose a new constrained RL method called CSAC-LB (Constrained Soft Actor-Critic with Log Barrier Function), which achieves competitive performance without any pre-training by applying a linear smoothed log barrier function to an additional safety critic. It implements an adaptive penalty for policy learning and alleviates the numerical issues that are known to complicate the application of the log barrier function method. As a result, we show that with CSAC-LB, we achieve state-of-the-art performance on several constrained control tasks with different levels of difficulty and evaluate our methods in a locomotion task on a real quadruped robot platform.</p></p class="citation"></blockquote><h3 id=2635--83312-universal-feature-selection-for-simultaneous-interpretability-of-multitask-datasets-matt-raymond-et-al-2024>(26/35 | 83/312) Universal Feature Selection for Simultaneous Interpretability of Multitask Datasets (Matt Raymond et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Matt Raymond, Jacob Charles Saldinger, Paolo Elvati, Clayton Scott, Angela Violi. (2024)<br><strong>Universal Feature Selection for Simultaneous Interpretability of Multitask Datasets</strong><br><button class=copy-to-clipboard title="Universal Feature Selection for Simultaneous Interpretability of Multitask Datasets" index=83>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-83 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Knowledge Transfer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14466v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14466v1.pdf filename=2403.14466v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Extracting meaningful features from complex, high-dimensional datasets across scientific domains remains challenging. Current methods often struggle with scalability, limiting their applicability to large datasets, or make restrictive assumptions about feature-property relationships, hindering their ability to capture complex interactions. BoUTS&rsquo;s general and scalable feature selection algorithm surpasses these limitations to identify both universal features relevant to all datasets and task-specific features predictive for specific subsets. Evaluated on seven diverse chemical regression datasets, BoUTS achieves state-of-the-art feature sparsity while maintaining prediction accuracy comparable to specialized methods. Notably, BoUTS&rsquo;s universal features enable domain-specific <b>knowledge</b> <b>transfer</b> between datasets, and suggest deep connections in seemingly-disparate chemical datasets. We expect these results to have important repercussions in manually-guided inverse problems. Beyond its current application, BoUTS holds immense potential for elucidating data-poor systems by leveraging information from similar data-rich systems. BoUTS represents a significant leap in cross-domain feature selection, potentially leading to advancements in various scientific fields.</p></p class="citation"></blockquote><h3 id=2735--84312-physics-informed-diffusion-models-jan-hendrik-bastek-et-al-2024>(27/35 | 84/312) Physics-Informed Diffusion Models (Jan-Hendrik Bastek et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jan-Hendrik Bastek, WaiChing Sun, Dennis M. Kochmann. (2024)<br><strong>Physics-Informed Diffusion Models</strong><br><button class=copy-to-clipboard title="Physics-Informed Diffusion Models" index=84>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-84 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CE, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14404v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14404v1.pdf filename=2403.14404v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Generative models such as denoising <b>diffusion</b> <b>models</b> are quickly advancing their ability to approximate highly complex data distributions. They are also increasingly leveraged in scientific machine learning, where samples from the implied data distribution are expected to adhere to specific governing equations. We present a framework to inform denoising <b>diffusion</b> <b>models</b> on underlying constraints on such generated samples during model training. Our approach improves the alignment of the generated samples with the imposed constraints and significantly outperforms existing methods without affecting inference speed. Additionally, our findings suggest that incorporating such constraints during training provides a natural regularization against overfitting. Our framework is easy to implement and versatile in its applicability for imposing equality and inequality constraints as well as auxiliary optimization objectives.</p></p class="citation"></blockquote><h3 id=2835--85312-loop-improvement-an-efficient-approach-for-extracting-shared-features-from-heterogeneous-data-without-central-server-fei-li-et-al-2024>(28/35 | 85/312) Loop Improvement: An Efficient Approach for Extracting Shared Features from Heterogeneous Data without Central Server (Fei Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fei Li, Chu Kiong Loo, Wei Shiung Liew, Xiaofeng Liu. (2024)<br><strong>Loop Improvement: An Efficient Approach for Extracting Shared Features from Heterogeneous Data without Central Server</strong><br><button class=copy-to-clipboard title="Loop Improvement: An Efficient Approach for Extracting Shared Features from Heterogeneous Data without Central Server" index=85>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-85 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-DC, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14371v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14371v1.pdf filename=2403.14371v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In <b>federated</b> <b>learning,</b> data heterogeneity significantly impacts performance. A typical solution involves segregating these parameters into shared and personalized components, a concept also relevant in multi-task learning. Addressing this, we propose &ldquo;Loop Improvement&rdquo; (LI), a novel method enhancing this separation and feature extraction without necessitating a central server or data interchange among participants. Our experiments reveal LI&rsquo;s superiority in several aspects: In personalized <b>federated</b> <b>learning</b> environments, LI consistently outperforms the advanced FedALA algorithm in accuracy across diverse scenarios. Additionally, LI&rsquo;s feature extractor closely matches the performance achieved when aggregating data from all clients. In global model contexts, employing LI with stacked personalized layers and an additional network also yields comparable results to combined client data scenarios. Furthermore, LI&rsquo;s adaptability extends to multi-task learning, streamlining the extraction of common features across tasks and obviating the need for simultaneous training. This approach not only enhances individual task performance but also achieves accuracy levels on par with classic multi-task learning methods where all tasks are trained simultaneously. LI integrates a loop topology with layer-wise and end-to-end training, compatible with various neural network models. This paper also delves into the theoretical underpinnings of LI&rsquo;s effectiveness, offering insights into its potential applications. The code is on <a href=https://github.com/axedge1983/LI>https://github.com/axedge1983/LI</a></p></p class="citation"></blockquote><h3 id=2935--86312-nabla-τ-gradient-based-and-task-agnostic-machine-unlearning-daniel-trippa-et-al-2024>(29/35 | 86/312) $\nabla τ$: Gradient-based and Task-Agnostic machine Unlearning (Daniel Trippa et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daniel Trippa, Cesare Campagnano, Maria Sofia Bucarelli, Gabriele Tolomei, Fabrizio Silvestri. (2024)<br><strong>$\nabla τ$: Gradient-based and Task-Agnostic machine Unlearning</strong><br><button class=copy-to-clipboard title="$\nabla τ$: Gradient-based and Task-Agnostic machine Unlearning" index=86>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-86 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-CV, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Machine Unlearning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14339v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14339v1.pdf filename=2403.14339v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Machine</b> <b>Unlearning,</b> the process of selectively eliminating the influence of certain data examples used during a model&rsquo;s training, has gained significant attention as a means for practitioners to comply with recent data protection regulations. However, existing unlearning methods face critical drawbacks, including their prohibitively high cost, often associated with a large number of hyperparameters, and the limitation of forgetting only relatively small data portions. This often makes retraining the model from scratch a quicker and more effective solution. In this study, we introduce Gradient-based and Task-Agnostic <b>machine</b> <b>Unlearning</b> ($\nabla \tau$), an optimization framework designed to remove the influence of a subset of training data efficiently. It applies adaptive gradient ascent to the data to be forgotten while using standard gradient descent for the remaining data. $\nabla \tau$ offers multiple benefits over existing approaches. It enables the unlearning of large sections of the training dataset (up to 30%). It is versatile, supporting various unlearning tasks (such as subset forgetting or class removal) and applicable across different domains (images, text, etc.). Importantly, $\nabla \tau$ requires no hyperparameter adjustments, making it a more appealing option than retraining the model from scratch. We evaluate our framework&rsquo;s effectiveness using a set of well-established Membership Inference Attack metrics, demonstrating up to 10% enhancements in performance compared to state-of-the-art methods without compromising the original model&rsquo;s accuracy.</p></p class="citation"></blockquote><h3 id=3035--87312-how-to-be-fair-a-study-of-label-and-selection-bias-marco-favier-et-al-2024>(30/35 | 87/312) How to be fair? A study of label and selection bias (Marco Favier et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marco Favier, Toon Calders, Sam Pinxteren, Jonathan Meyer. (2024)<br><strong>How to be fair? A study of label and selection bias</strong><br><button class=copy-to-clipboard title="How to be fair? A study of label and selection bias" index=87>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-87 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CY, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14282v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14282v1.pdf filename=2403.14282v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>It is widely accepted that biased data leads to biased and thus potentially unfair models. Therefore, several measures for bias in data and model predictions have been proposed, as well as bias mitigation techniques whose aim is to learn models that are fair by design. Despite the myriad of mitigation techniques developed in the past decade, however, it is still poorly understood under what circumstances which methods work. Recently, Wick et al. showed, with experiments on synthetic data, that there exist situations in which bias mitigation techniques lead to more accurate models when measured on unbiased data. Nevertheless, in the absence of a thorough mathematical analysis, it remains unclear which techniques are effective under what circumstances. We propose to address this problem by establishing relationships between the type of bias and the effectiveness of a mitigation technique, where we categorize the mitigation techniques by the bias measure they optimize. In this paper we illustrate this principle for label and selection bias on the one hand, and demographic parity and ``We&rsquo;re All Equal&rsquo;&rsquo; on the other hand. Our theoretical analysis allows to explain the results of Wick et al. and we also show that there are situations where minimizing <b>fairness</b> measures does not result in the fairest possible distribution.</p></p class="citation"></blockquote><h3 id=3135--88312-a-unified-framework-for-model-editing-akshat-gupta-et-al-2024>(31/35 | 88/312) A Unified Framework for Model Editing (Akshat Gupta et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Akshat Gupta, Dev Sajnani, Gopala Anumanchipalli. (2024)<br><strong>A Unified Framework for Model Editing</strong><br><button class=copy-to-clipboard title="A Unified Framework for Model Editing" index=88>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-88 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14236v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14236v1.pdf filename=2403.14236v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Model editing is a growing area focused on updating the knowledge embedded within models. Among the various methodologies, ROME and MEMIT stand out as leading &ldquo;locate-and-edit&rdquo; model editing techniques. While MEMIT enables batched editing of memories, ROME is limited to changing one fact at a time. This paper introduces a unifying framework that brings ROME and MEMIT under a single conceptual umbrella, optimizing for the same goal, which we call the &ldquo;preservation-memorization&rdquo; objective. This objective aims to preserve the representations of certain selected vectors while memorizing the representations of new factual information. Specifically, ROME optimizes this objective using an equality constraint, whereas MEMIT employs a more flexible least-square constraint. In addition to making batched edits, MEMIT also edits the model at multiple layers. We disentangle the distribution of edits to multiple layers from the optimization objective of MEMIT and show that these edit-distribution algorithms should be considered separate entities worthy of their own line of research. Finally, we present EMMET - an Equality-constrained Mass Model Editing algorithm for <b>Transformers,</b> a new batched memory-editing algorithm. With EMMET, we present a closed form solution for the equality-constrained version of the preservation-memorization objective. We show that EMMET is able to perform batched-edits on par with MEMIT up to a batch-size of 256 and discuss the challenges in stabilizing EMMET. By articulating the &ldquo;locate-and-edit&rdquo; model editing algorithms under a simple conceptual framework of &ldquo;preservation-memorization&rdquo;, we aim to bridge the gap between intuition and mathematics and hope to simplify the journey for future researchers in model editing.</p></p class="citation"></blockquote><h3 id=3235--89312-policy-mirror-descent-with-lookahead-kimon-protopapas-et-al-2024>(32/35 | 89/312) Policy Mirror Descent with Lookahead (Kimon Protopapas et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kimon Protopapas, Anas Barakat. (2024)<br><strong>Policy Mirror Descent with Lookahead</strong><br><button class=copy-to-clipboard title="Policy Mirror Descent with Lookahead" index=89>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-89 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14156v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14156v1.pdf filename=2403.14156v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Policy Mirror Descent (PMD) stands as a versatile algorithmic framework encompassing several seminal policy gradient algorithms such as natural policy gradient, with connections with state-of-the-art <b>reinforcement</b> <b>learning</b> (RL) algorithms such as TRPO and PPO. PMD can be seen as a soft Policy Iteration algorithm implementing regularized 1-step greedy policy improvement. However, 1-step greedy policies might not be the best choice and recent remarkable empirical successes in RL such as AlphaGo and AlphaZero have demonstrated that greedy approaches with respect to multiple steps outperform their 1-step counterpart. In this work, we propose a new class of PMD algorithms called $h$-PMD which incorporates multi-step greedy policy improvement with lookahead depth $h$ to the PMD update rule. To solve discounted infinite horizon Markov Decision Processes with discount factor $\gamma$, we show that $h$-PMD which generalizes the standard PMD enjoys a faster dimension-free $\gamma^h$-linear convergence rate, contingent on the computation of multi-step greedy policies. We propose an inexact version of $h$-PMD where lookahead action values are estimated. Under a generative model, we establish a sample complexity for $h$-PMD which improves over prior work. Finally, we extend our result to linear function approximation to scale to large state spaces. Under suitable assumptions, our sample complexity only involves dependence on the dimension of the feature map space instead of the state space size.</p></p class="citation"></blockquote><h3 id=3335--90312-carbon-footprint-reduction-for-sustainable-data-centers-in-real-time-soumyendu-sarkar-et-al-2024>(33/35 | 90/312) Carbon Footprint Reduction for Sustainable Data Centers in Real-Time (Soumyendu Sarkar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Soumyendu Sarkar, Avisek Naug, Ricardo Luna, Antonio Guillen, Vineet Gundecha, Sahand Ghorbanpour, Sajad Mousavi, Dejan Markovikj, Ashwin Ramesh Babu. (2024)<br><strong>Carbon Footprint Reduction for Sustainable Data Centers in Real-Time</strong><br><button class=copy-to-clipboard title="Carbon Footprint Reduction for Sustainable Data Centers in Real-Time" index=90>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-90 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-MA, cs-SY, cs.LG, eess-SY<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14092v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14092v2.pdf filename=2403.14092v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As machine learning workloads significantly increase energy consumption, sustainable data centers with low carbon emissions are becoming a top priority for governments and corporations worldwide. This requires a paradigm shift in optimizing power consumption in cooling and IT loads, shifting flexible loads based on the availability of renewable energy in the power grid, and leveraging battery storage from the uninterrupted power supply in data centers, using collaborative agents. The complex association between these optimization strategies and their dependencies on variable external factors like weather and the power grid carbon intensity makes this a hard problem. Currently, a real-time controller to optimize all these goals simultaneously in a dynamic real-world setting is lacking. We propose a Data Center Carbon Footprint Reduction (DC-CFR) multi-agent <b>Reinforcement</b> <b>Learning</b> (MARL) framework that optimizes data centers for the multiple objectives of carbon footprint reduction, energy consumption, and energy cost. The results show that the DC-CFR MARL agents effectively resolved the complex interdependencies in optimizing cooling, load shifting, and energy storage in real-time for various locations under real-world dynamic weather and grid carbon intensity conditions. DC-CFR significantly outperformed the industry standard ASHRAE controller with a considerable reduction in carbon emissions (14.5%), energy usage (14.4%), and energy cost (13.7%) when evaluated over one year across multiple geographical regions.</p></p class="citation"></blockquote><h3 id=3435--91312-local-causal-discovery-with-linear-non-gaussian-cyclic-models-haoyue-dai-et-al-2024>(34/35 | 91/312) Local Causal Discovery with Linear non-Gaussian Cyclic Models (Haoyue Dai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haoyue Dai, Ignavier Ng, Yujia Zheng, Zhengqing Gao, Kun Zhang. (2024)<br><strong>Local Causal Discovery with Linear non-Gaussian Cyclic Models</strong><br><button class=copy-to-clipboard title="Local Causal Discovery with Linear non-Gaussian Cyclic Models" index=91>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-91 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14843v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14843v1.pdf filename=2403.14843v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Local causal discovery is of great practical significance, as there are often situations where the discovery of the global causal structure is unnecessary, and the interest lies solely on a single target variable. Most existing local methods utilize conditional independence relations, providing only a partially directed <b>graph,</b> and assume acyclicity for the ground-truth structure, even though real-world scenarios often involve cycles like feedback mechanisms. In this work, we present a general, unified local causal discovery method with linear non-Gaussian models, whether they are cyclic or acyclic. We extend the application of independent component analysis from the global context to independent subspace analysis, enabling the exact identification of the equivalent local directed structures and causal strengths from the Markov blanket of the target variable. We also propose an alternative regression-based method in the particular acyclic scenarios. Our identifiability results are empirically validated using both synthetic and real-world datasets.</p></p class="citation"></blockquote><h3 id=3535--92312-investigating-the-validity-of-structure-learning-algorithms-in-identifying-risk-factors-for-intervention-in-patients-with-diabetes-sheresh-zahoor-et-al-2024>(35/35 | 92/312) Investigating the validity of structure learning algorithms in identifying risk factors for intervention in patients with diabetes (Sheresh Zahoor et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sheresh Zahoor, Anthony C. Constantinou, Tim M Curtis, Mohammed Hasanuzzaman. (2024)<br><strong>Investigating the validity of structure learning algorithms in identifying risk factors for intervention in patients with diabetes</strong><br><button class=copy-to-clipboard title="Investigating the validity of structure learning algorithms in identifying risk factors for intervention in patients with diabetes" index=92>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-92 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14327v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14327v1.pdf filename=2403.14327v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Diabetes, a pervasive and enduring health challenge, imposes significant global implications on health, financial healthcare systems, and societal well-being. This study undertakes a comprehensive exploration of various structural learning algorithms to discern causal pathways amongst potential risk factors influencing diabetes progression. The methodology involves the application of these algorithms to relevant diabetes data, followed by the conversion of their output <b>graphs</b> into Causal Bayesian Networks (CBNs), enabling predictive analysis and the evaluation of discrepancies in the effect of hypothetical interventions within our context-specific case study. This study highlights the substantial impact of algorithm selection on intervention outcomes. To consolidate insights from diverse algorithms, we employ a model-averaging technique that helps us obtain a unique causal model for diabetes derived from a varied set of structural learning algorithms. We also investigate how each of those individual <b>graphs,</b> as well as the average <b>graph,</b> compare to the structures elicited by a domain expert who categorised <b>graph</b> edges into high confidence, moderate, and low confidence types, leading into three individual <b>graphs</b> corresponding to the three levels of confidence. The resulting causal model and data are made available online, and serve as a valuable resource and a guide for informed decision-making by healthcare practitioners, offering a comprehensive understanding of the interactions between relevant risk factors and the effect of hypothetical interventions. Therefore, this research not only contributes to the academic discussion on diabetes, but also provides practical guidance for healthcare professionals in developing efficient intervention and risk management strategies.</p></p class="citation"></blockquote><h2 id=cscv-104>cs.CV (104)</h2><h3 id=1104--93312-vurf-a-general-purpose-reasoning-and-self-refinement-framework-for-video-understanding-ahmad-mahmood-et-al-2024>(1/104 | 93/312) VURF: A General-purpose Reasoning and Self-refinement Framework for Video Understanding (Ahmad Mahmood et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ahmad Mahmood, Ashmal Vayani, Muzammal Naseer, Salman Khan, Fahad Shahbaz Khan. (2024)<br><strong>VURF: A General-purpose Reasoning and Self-refinement Framework for Video Understanding</strong><br><button class=copy-to-clipboard title="VURF: A General-purpose Reasoning and Self-refinement Framework for Video Understanding" index=93>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-93 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 80<br>Keywords: GPT, GPT-3, GPT-3.5, Question Answering, Reasoning, In-context Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14743v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14743v2.pdf filename=2403.14743v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent studies have demonstrated the effectiveness of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> as <b>reasoning</b> modules that can deconstruct complex tasks into more manageable sub-tasks, particularly when applied to visual <b>reasoning</b> tasks for images. In contrast, this paper introduces a Video Understanding and <b>Reasoning</b> Framework (VURF) based on the <b>reasoning</b> power of <b>LLMs.</b> Ours is a novel approach to extend the utility of <b>LLMs</b> in the context of video tasks, leveraging their capacity to generalize from minimal input and output demonstrations within a contextual framework. By presenting <b>LLMs</b> with pairs of instructions and their corresponding high-level programs, we harness their contextual learning capabilities to generate executable visual programs for video understanding. To enhance program&rsquo;s accuracy and robustness, we implement two important strategies. Firstly, we employ a feedback-generation approach, powered by <b>GPT-3.5,</b> to rectify errors in programs utilizing unsupported functions. Secondly, taking motivation from recent works on self refinement of <b>LLM</b> outputs, we introduce an iterative procedure for improving the quality of the <b>in-context</b> examples by aligning the initial outputs to the outputs that would have been generated had the <b>LLM</b> not been bound by the structure of the <b>in-context</b> examples. Our results on several video-specific tasks, including visual <b>QA,</b> video anticipation, pose estimation and multi-video <b>QA</b> illustrate the efficacy of these enhancements in improving the performance of visual programming approaches for video tasks.</p></p class="citation"></blockquote><h3 id=2104--94312-multi-agent-vqa-exploring-multi-agent-foundation-models-in-zero-shot-visual-question-answering-bowen-jiang-et-al-2024>(2/104 | 94/312) Multi-Agent VQA: Exploring Multi-Agent Foundation Models in Zero-Shot Visual Question Answering (Bowen Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bowen Jiang, Zhijun Zhuang, Shreyas S. Shivakumar, Dan Roth, Camillo J. Taylor. (2024)<br><strong>Multi-Agent VQA: Exploring Multi-Agent Foundation Models in Zero-Shot Visual Question Answering</strong><br><button class=copy-to-clipboard title="Multi-Agent VQA: Exploring Multi-Agent Foundation Models in Zero-Shot Visual Question Answering" index=94>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-94 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CL, cs-CV, cs-LG, cs-MA, cs.CV<br>Keyword Score: 70<br>Keywords: Object Detection, Fine-tuning, Foundation Model, Zero-shot, Question Answering, Visual Question Answering, Visual Question Answering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14783v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14783v1.pdf filename=2403.14783v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work explores the <b>zero-shot</b> capabilities of <b>foundation</b> <b>models</b> in <b>Visual</b> <b>Question</b> <b>Answering</b> <b>(VQA)</b> tasks. We propose an adaptive multi-agent system, named Multi-Agent <b>VQA,</b> to overcome the limitations of <b>foundation</b> <b>models</b> in <b>object</b> <b>detection</b> and counting by using specialized agents as tools. Unlike existing approaches, our study focuses on the system&rsquo;s performance without <b>fine-tuning</b> it on specific <b>VQA</b> datasets, making it more practical and robust in the open world. We present preliminary experimental results under <b>zero-shot</b> scenarios and highlight some failure cases, offering new directions for future research.</p></p class="citation"></blockquote><h3 id=3104--95312-few-shot-adversarial-prompt-learning-on-vision-language-models-yiwei-zhou-et-al-2024>(3/104 | 95/312) Few-Shot Adversarial Prompt Learning on Vision-Language Models (Yiwei Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yiwei Zhou, Xiaobo Xia, Zhiwei Lin, Bo Han, Tongliang Liu. (2024)<br><strong>Few-Shot Adversarial Prompt Learning on Vision-Language Models</strong><br><button class=copy-to-clipboard title="Few-Shot Adversarial Prompt Learning on Vision-Language Models" index=95>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-95 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CL, cs-CR, cs-CV, cs-LG, cs.CV<br>Keyword Score: 63<br>Keywords: Few-shot, Foundation Model, Multi-modal, Zero-shot, Prompt, Prompt Learning, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14774v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14774v1.pdf filename=2403.14774v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The vulnerability of deep neural networks to imperceptible adversarial perturbations has attracted widespread attention. Inspired by the success of <b>vision-language</b> <b>foundation</b> <b>models,</b> previous efforts achieved <b>zero-shot</b> adversarial robustness by aligning adversarial visual features with text supervision. However, in practice, they are still unsatisfactory due to several issues, including heavy adaptation cost, suboptimal text supervision, and uncontrolled natural generalization capacity. In this paper, to address these issues, we propose a <b>few-shot</b> adversarial <b>prompt</b> <b>framework</b> where adapting input sequences with limited data makes significant adversarial robustness improvement. Specifically, we achieve this by providing adversarially correlated text supervision that is end-to-end learned from adversarial examples. We also propose a novel training objective that enhances the consistency of <b>multi-modal</b> features while encourages differentiated uni-modal features between natural and adversarial examples. The proposed framework gives access to learn adversarial text supervision, which provides superior cross-modal adversarial alignment and matches state-of-the-art <b>zero-shot</b> adversarial robustness with only 1% training data.</p></p class="citation"></blockquote><h3 id=4104--96312-empowering-segmentation-ability-to-multi-modal-large-language-models-yuqi-yang-et-al-2024>(4/104 | 96/312) Empowering Segmentation Ability to Multi-modal Large Language Models (Yuqi Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuqi Yang, Peng-Tao Jiang, Jing Wang, Hao Zhang, Kai Zhao, Jinwei Chen, Bo Li. (2024)<br><strong>Empowering Segmentation Ability to Multi-modal Large Language Models</strong><br><button class=copy-to-clipboard title="Empowering Segmentation Ability to Multi-modal Large Language Models" index=96>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-96 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 63<br>Keywords: Fine-tuning, Multi-modal, Reasoning, Chain-of-thought Prompt, Large Language Model, Prompt, Word Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14141v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14141v1.pdf filename=2403.14141v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multi-modal</b> <b>large</b> <b>language</b> <b>models</b> (MLLMs) can understand image-language <b>prompts</b> and demonstrate impressive <b>reasoning</b> ability. In this paper, we extend MLLMs&rsquo; output by empowering MLLMs with the segmentation ability. The extended MLLMs can both output language responses to the image-language <b>prompts</b> and segment the regions that the complex question or query in the language <b>prompts</b> focuses on. To this end, the existing work, LISA, enlarges the original <b>word</b> <b>embeddings</b> with an additional segment token and <b>fine-tunes</b> dialogue generation and query-focused segmentation together, where the feature of the segment token is used to <b>prompt</b> the segment-anything model. Although they achieve superior segmentation performance, we observe that the dialogue ability decreases by a <b>large</b> <b>margin</b> <b>compared</b> to the original MLLMs. To maintain the original MLLMs&rsquo; dialogue ability, we propose a novel MLLMs framework, coined as LLaVASeg, which leverages a <b>chain-of-thought</b> <b>prompting</b> strategy to instruct the MLLMs to segment the target region queried by the user. The MLLMs are first <b>prompted</b> to reason about the simple description of the target region from the complicated user query, then extract the visual attributes of the target region according to the understanding of MLLMs to the image. These visual attributes, such as color and relative locations, are utilized to <b>prompt</b> the downstream segmentation model. Experiments show that the proposed method keeps the original dialogue ability and equips the MLLMs&rsquo; model with strong <b>reasoning</b> segmentation ability. The code is available at <a href=https://github.com/YuqiYang213/LLaVASeg>https://github.com/YuqiYang213/LLaVASeg</a>.</p></p class="citation"></blockquote><h3 id=5104--97312-mathverse-does-your-multi-modal-llm-truly-see-the-diagrams-in-visual-math-problems-renrui-zhang-et-al-2024>(5/104 | 97/312) MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems? (Renrui Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Peng Gao, Hongsheng Li. (2024)<br><strong>MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?</strong><br><button class=copy-to-clipboard title="MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?" index=97>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-97 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CL, cs-CV, cs-LG, cs.CV<br>Keyword Score: 56<br>Keywords: Benchmarking, Multi-modal, GPT, Mathematical Reasoning, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14624v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14624v1.pdf filename=2403.14624v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The remarkable progress of <b>Multi-modal</b> <b>Large</b> <b>Language</b> <b>Models</b> (MLLMs) has garnered unparalleled attention, due to their superior performance in visual contexts. However, their capabilities in visual math problem-solving remain insufficiently evaluated and understood. We investigate current <b>benchmarks</b> to incorporate excessive visual content within textual questions, which potentially assist MLLMs in deducing answers without truly interpreting the input diagrams. To this end, we introduce MathVerse, an all-around visual math <b>benchmark</b> designed for an equitable and in-depth evaluation of MLLMs. We meticulously collect 2,612 high-quality, multi-subject math problems with diagrams from publicly available sources. Each problem is then transformed by human annotators into six distinct versions, each offering varying degrees of information content in multi-modality, contributing to 15K test samples in total. This approach allows MathVerse to comprehensively assess whether and how much MLLMs can truly understand the visual diagrams for <b>mathematical</b> <b>reasoning.</b> In addition, we propose a Chain-of-Thought (CoT) evaluation strategy for a fine-grained assessment of the output answers. Rather than naively judging True or False, we employ <b>GPT-4(V)</b> to adaptively extract crucial <b>reasoning</b> steps, and then score each step with detailed error analysis, which can reveal the intermediate CoT <b>reasoning</b> quality by MLLMs. We hope the MathVerse <b>benchmark</b> may provide unique insights to guide the future development of MLLMs. Project page: <a href=https://mathverse-cuhk.github.io>https://mathverse-cuhk.github.io</a></p></p class="citation"></blockquote><h3 id=6104--98312-language-repository-for-long-video-understanding-kumara-kahatapitiya-et-al-2024>(6/104 | 98/312) Language Repository for Long Video Understanding (Kumara Kahatapitiya et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kumara Kahatapitiya, Kanchana Ranasinghe, Jongwoo Park, Michael S. Ryoo. (2024)<br><strong>Language Repository for Long Video Understanding</strong><br><button class=copy-to-clipboard title="Language Repository for Long Video Understanding" index=98>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-98 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 56<br>Keywords: Benchmarking, Multi-modal, Pruning, Zero-shot, Question Answering, Visual Question Answering, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14622v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14622v1.pdf filename=2403.14622v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Language has become a prominent modality in computer vision with the rise of <b>multi-modal</b> <b>LLMs.</b> Despite supporting long context-lengths, their effectiveness in handling long-term information gradually declines with input length. This becomes critical, especially in applications such as long-form video understanding. In this paper, we introduce a Language Repository (LangRepo) for <b>LLMs,</b> that maintains concise and structured information as an interpretable (i.e., all-textual) representation. Our repository is updated iteratively based on multi-scale video chunks. We introduce write and read operations that focus on <b>pruning</b> redundancies in text, and extracting information at various temporal scales. The proposed framework is evaluated on <b>zero-shot</b> <b>visual</b> <b>question-answering</b> <b>benchmarks</b> including EgoSchema, NExT-QA, IntentQA and NExT-GQA, showing state-of-the-art performance at its scale. Our code is available at <a href=https://github.com/kkahatapitiya/LangRepo>https://github.com/kkahatapitiya/LangRepo</a>.</p></p class="citation"></blockquote><h3 id=7104--99312-learning-to-project-for-cross-task-knowledge-distillation-dylan-auty-et-al-2024>(7/104 | 99/312) Learning to Project for Cross-Task Knowledge Distillation (Dylan Auty et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dylan Auty, Roy Miles, Benedikt Kolbeinsson, Krystian Mikolajczyk. (2024)<br><strong>Learning to Project for Cross-Task Knowledge Distillation</strong><br><button class=copy-to-clipboard title="Learning to Project for Cross-Task Knowledge Distillation" index=99>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-99 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Knowledge Distillation, Knowledge Distillation, Knowledge Distillation, Knowledge Transfer, Image2text<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14494v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14494v1.pdf filename=2403.14494v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Traditional <b>knowledge</b> <b>distillation</b> <b>(KD)</b> relies on a proficient teacher trained on the target task, which is not always available. In this setting, cross-task <b>distillation</b> can be used, enabling the use of any teacher model trained on a different task. However, many <b>KD</b> methods prove ineffective when applied to this cross-task setting. To address this limitation, we propose a simple modification: the use of an inverted projection. We show that this drop-in replacement for a standard projector is effective by learning to disregard any task-specific features which might degrade the student&rsquo;s performance. We find that this simple modification is sufficient for extending many <b>KD</b> methods to the cross-task setting, where the teacher and student tasks can be very different. In doing so, we obtain up to a 1.9% improvement in the cross-task setting compared to the traditional projection, at no additional cost. Our method can obtain significant performance improvements (up to 7%) when using even a randomly-initialised teacher on various tasks such as depth estimation, <b>image</b> <b>translation,</b> and semantic segmentation, despite the lack of any learned <b>knowledge</b> <b>to</b> transfer. To provide conceptual and analytical insights into this result, we show that using an inverted projection allows the <b>distillation</b> loss to be decomposed into a <b>knowledge</b> <b>transfer</b> and a spectral regularisation component. Through this analysis we are additionally able to propose a novel regularisation loss that allows teacher-free <b>distillation,</b> enabling performance improvements of up to 8.57% on ImageNet with no additional training costs.</p></p class="citation"></blockquote><h3 id=8104--100312-unsupervised-audio-visual-segmentation-with-modality-alignment-swapnil-bhosale-et-al-2024>(8/104 | 100/312) Unsupervised Audio-Visual Segmentation with Modality Alignment (Swapnil Bhosale et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Swapnil Bhosale, Haosen Yang, Diptesh Kanojia, Jiangkang Deng, Xiatian Zhu. (2024)<br><strong>Unsupervised Audio-Visual Segmentation with Modality Alignment</strong><br><button class=copy-to-clipboard title="Unsupervised Audio-Visual Segmentation with Modality Alignment" index=100>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-100 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Contrastive Learning, Foundation Model, Supervised Learning, Unsupervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14203v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14203v1.pdf filename=2403.14203v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Audio-Visual Segmentation (AVS) aims to identify, at the pixel level, the object in a visual scene that produces a given sound. Current AVS methods rely on costly fine-grained annotations of mask-audio pairs, making them impractical for scalability. To address this, we introduce <b>unsupervised</b> <b>AVS,</b> eliminating the need for such expensive annotation. To tackle this more challenging problem, we propose an <b>unsupervised</b> <b>learning</b> method, named Modality Correspondence Alignment (MoCA), which seamlessly integrates off-the-shelf <b>foundation</b> <b>models</b> like DINO, SAM, and ImageBind. This approach leverages their knowledge complementarity and optimizes their joint usage for multi-modality association. Initially, we estimate positive and negative image pairs in the feature space. For pixel-level association, we introduce an audio-visual adapter and a novel pixel matching aggregation strategy within the image-level <b>contrastive</b> <b>learning</b> framework. This allows for a flexible connection between object appearance and audio signal at the pixel level, with tolerance to imaging variations such as translation and rotation. Extensive experiments on the AVSBench (single and multi-object splits) and AVSS datasets demonstrate that our MoCA outperforms strongly designed baseline methods and approaches <b>supervised</b> counterparts, particularly in complex scenarios with multiple auditory objects. Notably when comparing mIoU, MoCA achieves a substantial improvement over baselines in both the AVSBench (S4: +17.24%; MS3: +67.64%) and AVSS (+19.23%) audio-visual segmentation challenges.</p></p class="citation"></blockquote><h3 id=9104--101312-otseg-multi-prompt-sinkhorn-attention-for-zero-shot-semantic-segmentation-kwanyoung-kim-et-al-2024>(9/104 | 101/312) OTSeg: Multi-prompt Sinkhorn Attention for Zero-Shot Semantic Segmentation (Kwanyoung Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kwanyoung Kim, Yujin Oh, Jong Chul Ye. (2024)<br><strong>OTSeg: Multi-prompt Sinkhorn Attention for Zero-Shot Semantic Segmentation</strong><br><button class=copy-to-clipboard title="OTSeg: Multi-prompt Sinkhorn Attention for Zero-Shot Semantic Segmentation" index=101>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-101 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV, stat-ML<br>Keyword Score: 49<br>Keywords: Benchmarking, Multi-modal, Multi-modal, Zero-shot, Transformer, Prompt, Text Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14183v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14183v1.pdf filename=2403.14183v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The recent success of CLIP has demonstrated promising results in <b>zero-shot</b> semantic segmentation by transferring muiltimodal knowledge to pixel-level classification. However, leveraging pre-trained CLIP knowledge to closely align <b>text</b> <b>embeddings</b> with pixel embeddings still has limitations in existing approaches. To address this issue, we propose OTSeg, a novel <b>multimodal</b> attention mechanism aimed at enhancing the potential of multiple <b>text</b> <b>prompts</b> for matching associated pixel embeddings. We first propose Multi-Prompts Sinkhorn (MPS) based on the Optimal Transport (OT) algorithm, which leads multiple <b>text</b> <b>prompts</b> to selectively focus on various semantic features within image pixels. Moreover, inspired by the success of Sinkformers in unimodal settings, we introduce the extension of MPS, called Multi-Prompts Sinkhorn Attention (MPSA), which effectively replaces cross-attention mechanisms within <b>Transformer</b> framework in <b>multimodal</b> settings. Through extensive experiments, we demonstrate that OTSeg achieves state-of-the-art (SOTA) performance with significant gains on <b>Zero-Shot</b> Semantic Segmentation (ZS3) tasks across three <b>benchmark</b> datasets.</p></p class="citation"></blockquote><h3 id=10104--102312-transfer-learning-for-cross-dataset-isolated-sign-language-recognition-in-under-resourced-datasets-ahmet-alp-kindiroglu-et-al-2024>(10/104 | 102/312) Transfer Learning for Cross-dataset Isolated Sign Language Recognition in Under-Resourced Datasets (Ahmet Alp Kindiroglu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ahmet Alp Kindiroglu, Ozgur Kara, Ogulcan Ozdemir, Lale Akarun. (2024)<br><strong>Transfer Learning for Cross-dataset Isolated Sign Language Recognition in Under-Resourced Datasets</strong><br><button class=copy-to-clipboard title="Transfer Learning for Cross-dataset Isolated Sign Language Recognition in Under-Resourced Datasets" index=102>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-102 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 46<br>Keywords: Graph, Benchmarking, Convolution, Fine-tuning, Supervised Learning, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14534v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14534v1.pdf filename=2403.14534v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Sign language recognition (SLR) has recently achieved a breakthrough in performance thanks to deep neural networks trained on large annotated sign datasets. Of the many different sign languages, these annotated datasets are only available for a select few. Since acquiring gloss-level labels on sign language videos is difficult, learning by transferring knowledge from existing annotated sources is useful for recognition in under-resourced sign languages. This study provides a publicly available cross-dataset <b>transfer</b> <b>learning</b> <b>benchmark</b> from two existing public Turkish SLR datasets. We use a temporal <b>graph</b> <b>convolution-based</b> sign language recognition approach to evaluate five <b>supervised</b> <b>transfer</b> <b>learning</b> approaches and experiment with closed-set and partial-set cross-dataset <b>transfer</b> <b>learning.</b> Experiments demonstrate that improvement over <b>finetuning</b> based <b>transfer</b> <b>learning</b> is possible with specialized <b>supervised</b> <b>transfer</b> <b>learning</b> methods.</p></p class="citation"></blockquote><h3 id=11104--103312-vidla-video-language-alignment-at-scale-mamshad-nayeem-rizve-et-al-2024>(11/104 | 103/312) VidLA: Video-Language Alignment at Scale (Mamshad Nayeem Rizve et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mamshad Nayeem Rizve, Fan Fei, Jayakrishnan Unnikrishnan, Son Tran, Benjamin Z. Yao, Belinda Zeng, Mubarak Shah, Trishul Chilimbi. (2024)<br><strong>VidLA: Video-Language Alignment at Scale</strong><br><button class=copy-to-clipboard title="VidLA: Video-Language Alignment at Scale" index=103>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-103 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CL, cs-CV, cs-LG, cs.CV<br>Keyword Score: 43<br>Keywords: Benchmarking, Foundation Model, Grounding, Image2text, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14870v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14870v1.pdf filename=2403.14870v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we propose VidLA, an approach for video-language alignment at scale. There are two major limitations of previous video-language alignment approaches. First, they do not capture both short-range and long-range temporal dependencies and typically employ complex hierarchical deep network architectures that are hard to integrate with existing pretrained <b>image-text</b> <b>foundation</b> <b>models.</b> To effectively address this limitation, we instead keep the network architecture simple and use a set of data tokens that operate at different temporal resolutions in a hierarchical manner, accounting for the temporally hierarchical nature of videos. By employing a simple two-tower architecture, we are able to initialize our video-language model with pretrained <b>image-text</b> <b>foundation</b> <b>models,</b> thereby boosting the final performance. Second, existing video-language alignment works struggle due to the lack of semantically aligned large-scale training data. To overcome it, we leverage recent <b>LLMs</b> to curate the largest video-language dataset to date with better visual <b>grounding.</b> Furthermore, unlike existing video-text datasets which only contain short clips, our dataset is enriched with video clips of varying durations to aid our temporally hierarchical data tokens in extracting better representations at varying temporal scales. Overall, empirical results show that our proposed approach surpasses state-of-the-art methods on multiple retrieval <b>benchmarks,</b> especially on longer videos, and performs competitively on classification <b>benchmarks.</b></p></p class="citation"></blockquote><h3 id=12104--104312-less-but-better-enabling-generalized-zero-shot-learning-towards-unseen-domains-by-intrinsic-learning-from-redundant-llm-semantics-jiaqi-yue-et-al-2024>(12/104 | 104/312) Less but Better: Enabling Generalized Zero-shot Learning Towards Unseen Domains by Intrinsic Learning from Redundant LLM Semantics (Jiaqi Yue et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiaqi Yue, Jiancheng Zhao, Chunhui Zhao. (2024)<br><strong>Less but Better: Enabling Generalized Zero-shot Learning Towards Unseen Domains by Intrinsic Learning from Redundant LLM Semantics</strong><br><button class=copy-to-clipboard title="Less but Better: Enabling Generalized Zero-shot Learning Towards Unseen Domains by Intrinsic Learning from Redundant LLM Semantics" index=104>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-104 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 43<br>Keywords: Benchmarking, Zero-shot, Large Language Model, Large Language Model, Zero-shot Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14362v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14362v1.pdf filename=2403.14362v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Generalized <b>zero-shot</b> <b>learning</b> (GZSL) focuses on recognizing seen and unseen classes against domain shift problem (DSP) where data of unseen classes may be misclassified as seen classes. However, existing GZSL is still limited to seen domains. In the current work, we pioneer cross-domain GZSL (CDGZSL) which addresses GZSL towards unseen domains. Different from existing GZSL methods which alleviate DSP by generating features of unseen classes with semantics, CDGZSL needs to construct a common feature space across domains and acquire the corresponding intrinsic semantics shared among domains to transfer from seen to unseen domains. Considering the information asymmetry problem caused by redundant class semantics annotated with <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> we present Meta Domain Alignment Semantic Refinement (MDASR). Technically, MDASR consists of two parts: Inter-class Similarity Alignment (ISA), which eliminates the non-intrinsic semantics not shared across all domains under the guidance of inter-class feature relationships, and Unseen-class Meta Generation (UMG), which preserves intrinsic semantics to maintain connectivity between seen and unseen classes by simulating feature generation. MDASR effectively aligns the redundant semantic space with the common feature space, mitigating the information asymmetry in CDGZSL. The effectiveness of MDASR is demonstrated on the Office-Home and Mini-DomainNet, and we have shared the <b>LLM-based</b> semantics for these datasets as the <b>benchmark.</b></p></p class="citation"></blockquote><h3 id=13104--105312-t-rex2-towards-generic-object-detection-via-text-visual-prompt-synergy-qing-jiang-et-al-2024>(13/104 | 105/312) T-Rex2: Towards Generic Object Detection via Text-Visual Prompt Synergy (Qing Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qing Jiang, Feng Li, Zhaoyang Zeng, Tianhe Ren, Shilong Liu, Lei Zhang. (2024)<br><strong>T-Rex2: Towards Generic Object Detection via Text-Visual Prompt Synergy</strong><br><button class=copy-to-clipboard title="T-Rex2: Towards Generic Object Detection via Text-Visual Prompt Synergy" index=105>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-105 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Object Detection, Contrastive Learning, Zero-shot, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14610v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14610v1.pdf filename=2403.14610v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present T-Rex2, a highly practical model for open-set <b>object</b> <b>detection.</b> Previous open-set <b>object</b> <b>detection</b> methods relying on text <b>prompts</b> effectively encapsulate the abstract concept of common <b>objects,</b> <b>but</b> struggle with rare or complex <b>object</b> <b>representation</b> due to data scarcity and descriptive limitations. Conversely, visual <b>prompts</b> excel in depicting novel <b>objects</b> <b>through</b> concrete visual examples, but fall short in conveying the abstract concept of <b>objects</b> <b>as</b> effectively as text <b>prompts.</b> Recognizing the complementary strengths and weaknesses of both text and visual <b>prompts,</b> we introduce T-Rex2 that synergizes both <b>prompts</b> within a single model through <b>contrastive</b> <b>learning.</b> T-Rex2 accepts inputs in diverse formats, including text <b>prompts,</b> visual <b>prompts,</b> and the combination of both, so that it can handle different scenarios by switching between the two <b>prompt</b> modalities. Comprehensive experiments demonstrate that T-Rex2 exhibits remarkable <b>zero-shot</b> <b>object</b> <b>detection</b> capabilities across a wide spectrum of scenarios. We show that text <b>prompts</b> and visual <b>prompts</b> can benefit from each other within the synergy, which is essential to cover massive and complicated real-world scenarios and pave the way towards generic <b>object</b> <b>detection.</b> Model API is now available at \url{https://github.com/IDEA-Research/T-Rex}.</p></p class="citation"></blockquote><h3 id=14104--106312-a-lightweight-attention-based-deep-network-via-multi-scale-feature-fusion-for-multi-view-facial-expression-recognition-ali-ezati-et-al-2024>(14/104 | 106/312) A Lightweight Attention-based Deep Network via Multi-Scale Feature Fusion for Multi-View Facial Expression Recognition (Ali Ezati et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ali Ezati, Mohammadreza Dezyani, Rajib Rana, Roozbeh Rajabi, Ahmad Ayatollahi. (2024)<br><strong>A Lightweight Attention-based Deep Network via Multi-Scale Feature Fusion for Multi-View Facial Expression Recognition</strong><br><button class=copy-to-clipboard title="A Lightweight Attention-based Deep Network via Multi-Scale Feature Fusion for Multi-View Facial Expression Recognition" index=106>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-106 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14318v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14318v1.pdf filename=2403.14318v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Convolutional</b> <b>neural</b> <b>networks</b> <b>(CNNs)</b> and their variations have shown effectiveness in facial expression recognition (FER). However, they face challenges when dealing with high computational complexity and multi-view head poses in real-world scenarios. We introduce a lightweight attentional network incorporating multi-scale feature fusion (LANMSFF) to tackle these issues. For the first challenge, we have carefully designed a lightweight fully <b>convolutional</b> <b>network</b> <b>(FCN).</b> We address the second challenge by presenting two novel components, namely mass attention (MassAtt) and point wise feature selection (PWFS) blocks. The MassAtt block simultaneously generates channel and spatial attention maps to recalibrate feature maps by emphasizing important features while suppressing irrelevant ones. On the other hand, the PWFS block employs a feature selection mechanism that discards less meaningful features prior to the fusion process. This mechanism distinguishes it from previous methods that directly fuse multi-scale features. Our proposed approach achieved results comparable to state-of-the-art methods in terms of parameter counts and robustness to pose variation, with accuracy rates of 90.77% on KDEF, 70.44% on FER-2013, and 86.96% on FERPlus datasets. The code for LANMSFF is available at <a href=https://github.com/AE-1129/LANMSFF>https://github.com/AE-1129/LANMSFF</a>.</p></p class="citation"></blockquote><h3 id=15104--107312-masksam-towards-auto-prompt-sam-with-mask-classification-for-medical-image-segmentation-bin-xie-et-al-2024>(15/104 | 107/312) MaskSAM: Towards Auto-prompt SAM with Mask Classification for Medical Image Segmentation (Bin Xie et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bin Xie, Hao Tang, Bin Duan, Dawen Cai, Yan Yan. (2024)<br><strong>MaskSAM: Towards Auto-prompt SAM with Mask Classification for Medical Image Segmentation</strong><br><button class=copy-to-clipboard title="MaskSAM: Towards Auto-prompt SAM with Mask Classification for Medical Image Segmentation" index=107>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-107 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Foundation Model, Zero-shot, Transformer, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14103v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14103v1.pdf filename=2403.14103v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Segment Anything Model~(SAM), a <b>prompt-driven</b> <b>foundation</b> <b>model</b> for natural image segmentation, has demonstrated impressive <b>zero-shot</b> performance. However, SAM does not work when directly applied to medical image segmentation tasks, since SAM lacks the functionality to predict semantic labels for predicted masks and needs to provide extra <b>prompts,</b> such as points or boxes, to segment target regions. Meanwhile, there is a huge gap between 2D natural images and 3D medical images, so the performance of SAM is imperfect for medical image segmentation tasks. Following the above issues, we propose MaskSAM, a novel mask classification <b>prompt-free</b> SAM adaptation framework for medical image segmentation. We design a <b>prompt</b> generator combined with the image encoder in SAM to generate a set of auxiliary classifier tokens, auxiliary binary masks, and auxiliary bounding boxes. Each pair of auxiliary mask and box <b>prompts,</b> which can solve the requirements of extra <b>prompts,</b> is associated with class label predictions by the sum of the auxiliary classifier token and the learnable global classifier tokens in the mask decoder of SAM to solve the predictions of semantic labels. Meanwhile, we design a 3D depth-convolution adapter for image embeddings and a 3D depth-MLP adapter for <b>prompt</b> embeddings. We inject one of them into each <b>transformer</b> block in the image encoder and mask decoder to enable pre-trained 2D SAM models to extract 3D information and adapt to 3D medical images. Our method achieves state-of-the-art performance on AMOS2022, 90.52% Dice, which improved by 2.7% compared to nnUNet. Our method surpasses nnUNet by 1.7% on ACDC and 1.0% on Synapse datasets.</p></p class="citation"></blockquote><h3 id=16104--108312-text-enhanced-data-free-approach-for-federated-class-incremental-learning-minh-tuan-tran-et-al-2024>(16/104 | 108/312) Text-Enhanced Data-free Approach for Federated Class-Incremental Learning (Minh-Tuan Tran et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Minh-Tuan Tran, Trung Le, Xuan-May Le, Mehrtash Harandi, Dinh Phung. (2024)<br><strong>Text-Enhanced Data-free Approach for Federated Class-Incremental Learning</strong><br><button class=copy-to-clipboard title="Text-Enhanced Data-free Approach for Federated Class-Incremental Learning" index=108>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-108 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CL, cs-CV, cs-LG, cs.CV<br>Keyword Score: 40<br>Keywords: Federated Learning, Knowledge Transfer, Pre-trained Language Model, Text Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14101v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14101v1.pdf filename=2403.14101v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Federated</b> <b>Class-Incremental</b> Learning (FCIL) is an underexplored yet pivotal issue, involving the dynamic addition of new classes in the context of <b>federated</b> <b>learning.</b> In this field, Data-Free <b>Knowledge</b> <b>Transfer</b> (DFKT) plays a crucial role in addressing catastrophic forgetting and data privacy problems. However, prior approaches lack the crucial synergy between DFKT and the model training phases, causing DFKT to encounter difficulties in generating high-quality data from a non-anchored latent space of the old task model. In this paper, we introduce LANDER (Label <b>Text</b> <b>Centered</b> Data-Free <b>Knowledge</b> <b>Transfer)</b> to address this issue by utilizing label <b>text</b> <b>embeddings</b> (LTE) produced by <b>pretrained</b> <b>language</b> <b>models.</b> Specifically, during the model training phase, our approach treats LTE as anchor points and constrains the feature embeddings of corresponding training samples around them, enriching the surrounding area with more meaningful information. In the DFKT phase, by using these LTE anchors, LANDER can synthesize more meaningful samples, thereby effectively addressing the forgetting problem. Additionally, instead of tightly constraining embeddings toward the anchor, the Bounding Loss is introduced to encourage sample embeddings to remain flexible within a defined radius. This approach preserves the natural differences in sample embeddings and mitigates the embedding overlap caused by heterogeneous <b>federated</b> <b>settings.</b> Extensive experiments conducted on CIFAR100, Tiny-ImageNet, and ImageNet demonstrate that LANDER significantly outperforms previous methods and achieves state-of-the-art performance in FCIL. The code is available at <a href=https://github.com/tmtuan1307/lander>https://github.com/tmtuan1307/lander</a>.</p></p class="citation"></blockquote><h3 id=17104--109312-cobra-extending-mamba-to-multi-modal-large-language-model-for-efficient-inference-han-zhao-et-al-2024>(17/104 | 109/312) Cobra: Extending Mamba to Multi-Modal Large Language Model for Efficient Inference (Han Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Han Zhao, Min Zhang, Wei Zhao, Pengxiang Ding, Siteng Huang, Donglin Wang. (2024)<br><strong>Cobra: Extending Mamba to Multi-Modal Large Language Model for Efficient Inference</strong><br><button class=copy-to-clipboard title="Cobra: Extending Mamba to Multi-Modal Large Language Model for Efficient Inference" index=109>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-109 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 39<br>Keywords: Benchmarking, Foundation Model, Multi-modal, Multi-modal, Transformer, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14520v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14520v2.pdf filename=2403.14520v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, the application of <b>multimodal</b> <b>large</b> <b>language</b> <b>models</b> (MLLM) in various fields has achieved remarkable success. However, as the <b>foundation</b> <b>model</b> for many downstream tasks, current MLLMs are composed of the well-known <b>Transformer</b> network, which has a less efficient quadratic computation complexity. To improve the efficiency of such basic models, we propose Cobra, a linear computational complexity MLLM. Specifically, Cobra integrates the efficient Mamba language model into the visual modality. Moreover, we explore and study various modal fusion schemes to create an effective <b>multi-modal</b> Mamba. Extensive experiments demonstrate that (1) Cobra achieves extremely competitive performance with current computationally efficient state-of-the-art methods, e.g., LLaVA-Phi, TinyLLaVA, and MobileVLM v2, and has faster speed due to Cobra&rsquo;s linear sequential modeling. (2) Interestingly, the results of closed-set challenging prediction <b>benchmarks</b> show that Cobra performs well in overcoming visual illusions and spatial relationship judgments. (3) Notably, Cobra even achieves comparable performance to LLaVA with about 43% of the number of parameters. We will make all codes of Cobra open-source and hope that the proposed method can facilitate future research on complexity problems in MLLM. Our project page is available at: <a href=https://sites.google.com/view/cobravlm>https://sites.google.com/view/cobravlm</a>.</p></p class="citation"></blockquote><h3 id=18104--110312-hierarchical-text-to-vision-self-supervised-alignment-for-improved-histopathology-representation-learning-hasindri-watawana-et-al-2024>(18/104 | 110/312) Hierarchical Text-to-Vision Self Supervised Alignment for Improved Histopathology Representation Learning (Hasindri Watawana et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hasindri Watawana, Kanchana Ranasinghe, Tariq Mahmood, Muzammal Naseer, Salman Khan, Fahad Shahbaz Khan. (2024)<br><strong>Hierarchical Text-to-Vision Self Supervised Alignment for Improved Histopathology Representation Learning</strong><br><button class=copy-to-clipboard title="Hierarchical Text-to-Vision Self Supervised Alignment for Improved Histopathology Representation Learning" index=110>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-110 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 38<br>Keywords: Benchmarking, Representation Learning, Self-supervised Learning, Self-supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14616v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14616v1.pdf filename=2403.14616v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Self-supervised</b> <b>representation</b> <b>learning</b> has been highly promising for histopathology image analysis with numerous approaches leveraging their patient-slide-patch hierarchy to learn better <b>representations.</b> <b>In</b> this paper, we explore how the combination of domain specific natural language information with such hierarchical visual <b>representations</b> <b>can</b> benefit rich <b>representation</b> <b>learning</b> for medical image tasks. Building on automated language description generation for features visible in histopathology images, we present a novel language-tied <b>self-supervised</b> <b>learning</b> framework, Hierarchical Language-tied Self-Supervision (HLSS) for histopathology images. We explore contrastive objectives and granular language description based text alignment at multiple hierarchies to inject language modality information into the visual <b>representations.</b> <b>Our</b> resulting model achieves state-of-the-art performance on two medical imaging <b>benchmarks,</b> OpenSRH and TCGA datasets. Our framework also provides better interpretability with our language aligned <b>representation</b> <b>space.</b> Code is available at <a href=https://github.com/Hasindri/HLSS>https://github.com/Hasindri/HLSS</a>.</p></p class="citation"></blockquote><h3 id=19104--111312-object-centric-domain-randomization-for-3d-shape-reconstruction-in-the-wild-junhyeong-cho-et-al-2024>(19/104 | 111/312) Object-Centric Domain Randomization for 3D Shape Reconstruction in the Wild (Junhyeong Cho et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junhyeong Cho, Kim Youwang, Hunmin Yang, Tae-Hyun Oh. (2024)<br><strong>Object-Centric Domain Randomization for 3D Shape Reconstruction in the Wild</strong><br><button class=copy-to-clipboard title="Object-Centric Domain Randomization for 3D Shape Reconstruction in the Wild" index=111>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-111 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 38<br>Keywords: ControlNet, Benchmarking, Geometry, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14539v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14539v1.pdf filename=2403.14539v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>One of the biggest challenges in single-view 3D shape reconstruction in the wild is the scarcity of &lt;3D shape, 2D image>-paired data from real-world environments. Inspired by remarkable achievements via domain randomization, we propose ObjectDR which synthesizes such paired data via a random <b>simulation</b> of visual variations in object appearances and backgrounds. Our data synthesis framework exploits a conditional generative model (e.g., <b>ControlNet)</b> to generate images conforming to spatial conditions such as 2.5D sketches, which are obtainable through a rendering process of 3D shapes from object collections (e.g., Objaverse-XL). To simulate diverse variations while preserving object silhouettes embedded in spatial conditions, we also introduce a disentangled framework which leverages an initial object guidance. After synthesizing a wide range of data, we pre-train a model on them so that it learns to capture a domain-invariant <b>geometry</b> prior which is consistent across various domains. We validate its effectiveness by substantially improving 3D shape reconstruction models on a real-world <b>benchmark.</b> In a scale-up evaluation, our pre-training achieves 23.6% superior results compared with the pre-training on high-quality computer graphics renderings.</p></p class="citation"></blockquote><h3 id=20104--112312-psalm-pixelwise-segmentation-with-large-multi-modal-model-zheng-zhang-et-al-2024>(20/104 | 112/312) PSALM: Pixelwise SegmentAtion with Large Multi-Modal Model (Zheng Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zheng Zhang, Yeyao Ma, Enming Zhang, Xiang Bai. (2024)<br><strong>PSALM: Pixelwise SegmentAtion with Large Multi-Modal Model</strong><br><button class=copy-to-clipboard title="PSALM: Pixelwise SegmentAtion with Large Multi-Modal Model" index=112>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-112 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 36<br>Keywords: Benchmarking, Multi-modal, Zero-shot, GPT, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14598v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14598v1.pdf filename=2403.14598v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>PSALM is a powerful extension of the Large <b>Multi-modal</b> Model (LMM) to address the segmentation task challenges. To overcome the limitation of the LMM being limited to textual output, PSALM incorporates a mask decoder and a well-designed input schema to handle a variety of segmentation tasks. This schema includes images, task instructions, conditional <b>prompts,</b> and mask tokens, which enable the model to generate and classify segmentation masks effectively. The flexible design of PSALM supports joint training across multiple datasets and tasks, leading to improved performance and task generalization. PSALM achieves superior results on several <b>benchmarks,</b> such as RefCOCO/RefCOCO+/RefCOCOg, COCO Panoptic Segmentation, and COCO-Interactive, and further exhibits <b>zero-shot</b> capabilities on unseen tasks, such as open-vocabulary segmentation, generalized referring expression segmentation and video object segmentation, making a significant step towards a <b>GPT</b> moment in computer vision. Through extensive experiments, PSALM demonstrates its potential to transform the domain of image segmentation, leveraging the robust visual understanding capabilities of LMMs as seen in natural language processing. Code and models are available at <a href=https://github.com/zamling/PSALM>https://github.com/zamling/PSALM</a>.</p></p class="citation"></blockquote><h3 id=21104--113312-unified-static-and-dynamic-network-efficient-temporal-filtering-for-video-grounding-jingjing-hu-et-al-2024>(21/104 | 113/312) Unified Static and Dynamic Network: Efficient Temporal Filtering for Video Grounding (Jingjing Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jingjing Hu, Dan Guo, Kun Li, Zhan Si, Xun Yang, Xiaojun Chang, Meng Wang. (2024)<br><strong>Unified Static and Dynamic Network: Efficient Temporal Filtering for Video Grounding</strong><br><button class=copy-to-clipboard title="Unified Static and Dynamic Network: Efficient Temporal Filtering for Video Grounding" index=113>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-113 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 36<br>Keywords: Message-Passing, Graph, Benchmarking, Convolution, Grounding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14174v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14174v1.pdf filename=2403.14174v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Inspired by the activity-silent and persistent activity mechanisms in human visual perception biology, we design a Unified Static and Dynamic Network (UniSDNet), to learn the semantic association between the video and text/audio queries in a cross-modal environment for efficient video <b>grounding.</b> For static modeling, we devise a novel residual structure (ResMLP) to boost the global comprehensive interaction between the video segments and queries, achieving more effective semantic enhancement/supplement. For dynamic modeling, we effectively exploit three characteristics of the persistent activity mechanism in our network design for a better video context comprehension. Specifically, we construct a diffusely connected video clip <b>graph</b> on the basis of 2D sparse temporal masking to reflect the &ldquo;short-term effect&rdquo; relationship. We innovatively consider the temporal distance and relevance as the joint &ldquo;auxiliary evidence clues&rdquo; and design a multi-kernel Temporal Gaussian Filter to expand the context clue into high-dimensional space, simulating the &ldquo;complex visual perception&rdquo;, and then conduct element level filtering <b>convolution</b> operations on neighbour clip nodes in message passing stage for finally generating and ranking the candidate proposals. Our UniSDNet is applicable to both Natural Language Video <b>Grounding</b> (NLVG) and Spoken Language Video <b>Grounding</b> (SLVG) tasks. Our UniSDNet achieves SOTA performance on three widely used datasets for NLVG, as well as three datasets for SLVG, e.g., reporting new records at 38.88% R@1,IoU@0.7 on ActivityNet Captions and 40.26% R@1,IoU@0.5 on TACoS. To facilitate this field, we collect two new datasets (Charades-STA Speech and TACoS Speech) for SLVG task. Meanwhile, the inference speed of our UniSDNet is 1.56$\times$ faster than the strong multi-query <b>benchmark.</b> Code is available at: <a href=https://github.com/xian-sh/UniSDNet>https://github.com/xian-sh/UniSDNet</a>.</p></p class="citation"></blockquote><h3 id=22104--114312-preventing-catastrophic-forgetting-through-memory-networks-in-continuous-detection-gaurav-bhatt-et-al-2024>(22/104 | 114/312) Preventing Catastrophic Forgetting through Memory Networks in Continuous Detection (Gaurav Bhatt et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gaurav Bhatt, James Ross, Leonid Sigal. (2024)<br><strong>Preventing Catastrophic Forgetting through Memory Networks in Continuous Detection</strong><br><button class=copy-to-clipboard title="Preventing Catastrophic Forgetting through Memory Networks in Continuous Detection" index=114>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-114 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 33<br>Keywords: Benchmarking, Fine-tuning, Transformer, Information Retrieval<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14797v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14797v1.pdf filename=2403.14797v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Modern pre-trained architectures struggle to retain previous <b>information</b> <b>while</b> undergoing continuous <b>fine-tuning</b> on new tasks. Despite notable progress in continual classification, systems designed for complex vision tasks such as detection or segmentation still struggle to attain satisfactory performance. In this work, we introduce a memory-based detection <b>transformer</b> architecture to adapt a pre-trained DETR-style detector to new tasks while preserving knowledge from previous tasks. We propose a novel localized query function for efficient <b>information</b> <b>retrieval</b> from memory units, aiming to minimize forgetting. Furthermore, we identify a fundamental challenge in continual detection referred to as background relegation. This arises when object categories from earlier tasks reappear in future tasks, potentially without labels, leading them to be implicitly treated as background. This is an inevitable issue in continual detection or segmentation. The introduced continual optimization technique effectively tackles this challenge. Finally, we assess the performance of our proposed system on continual detection <b>benchmarks</b> and demonstrate that our approach surpasses the performance of existing state-of-the-art resulting in 5-7% improvements on MS-COCO and PASCAL-VOC on the task of continual detection.</p></p class="citation"></blockquote><h3 id=23104--115312-latent-diffusion-models-for-attribute-preserving-image-anonymization-luca-piano-et-al-2024>(23/104 | 115/312) Latent Diffusion Models for Attribute-Preserving Image Anonymization (Luca Piano et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Luca Piano, Pietro Basci, Fabrizio Lamberti, Lia Morra. (2024)<br><strong>Latent Diffusion Models for Attribute-Preserving Image Anonymization</strong><br><button class=copy-to-clipboard title="Latent Diffusion Models for Attribute-Preserving Image Anonymization" index=115>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-115 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: ControlNet, Diffusion Model, Benchmarking, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14790v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14790v1.pdf filename=2403.14790v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Generative techniques for image anonymization have great potential to generate datasets that protect the privacy of those depicted in the images, while achieving high data fidelity and utility. Existing methods have focused extensively on preserving facial attributes, but failed to embrace a more comprehensive perspective that considers the scene and background into the anonymization process. This paper presents, to the best of our knowledge, the first approach to image anonymization based on Latent <b>Diffusion</b> <b>Models</b> (LDMs). Every element of a scene is maintained to convey the same meaning, yet manipulated in a way that makes re-identification difficult. We propose two LDMs for this purpose: CAMOUFLaGE-Base exploits a combination of pre-trained <b>ControlNets,</b> and a new controlling mechanism designed to increase the distance between the real and anonymized images. CAMOFULaGE-Light is based on the Adapter technique, coupled with an encoding designed to efficiently represent the attributes of different persons in a scene. The former solution achieves superior performance on most metrics and <b>benchmarks,</b> while the latter cuts the inference time in half at the cost of <b>fine-tuning</b> a lightweight module. We show through extensive experimental comparison that the proposed method is competitive with the state-of-the-art concerning identity obfuscation whilst better preserving the original content of the image and tackling unresolved challenges that current solutions fail to address.</p></p class="citation"></blockquote><h3 id=24104--116312-adversary-robust-graph-based-learning-of-wsis-saba-heidari-gheshlaghi-et-al-2024>(24/104 | 116/312) Adversary-Robust Graph-Based Learning of WSIs (Saba Heidari Gheshlaghi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Saba Heidari Gheshlaghi, Milan Aryal, Nasim Yahyasoltani, Masoud Ganji. (2024)<br><strong>Adversary-Robust Graph-Based Learning of WSIs</strong><br><button class=copy-to-clipboard title="Adversary-Robust Graph-Based Learning of WSIs" index=116>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-116 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Graph, Graph Neural Network, Transformer, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14489v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14489v1.pdf filename=2403.14489v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Enhancing the robustness of deep learning models against <b>adversarial</b> <b>attacks</b> is crucial, especially in critical domains like healthcare where significant financial interests heighten the risk of such attacks. Whole slide images (WSIs) are high-resolution, digitized versions of tissue samples mounted on glass slides, scanned using sophisticated imaging equipment. The digital analysis of WSIs presents unique challenges due to their gigapixel size and multi-resolution storage format. In this work, we aim at improving the robustness of cancer Gleason grading classification systems against <b>adversarial</b> <b>attacks,</b> addressing challenges at both the image and <b>graph</b> levels. As regards the proposed algorithm, we develop a novel and innovative <b>graph-based</b> model which utilizes <b>GNN</b> to extract features from the <b>graph</b> representation of WSIs. A denoising module, along with a pooling layer is incorporated to manage the impact of <b>adversarial</b> <b>attacks</b> on the WSIs. The process concludes with a <b>transformer</b> module that classifies various grades of prostate cancer based on the processed data. To assess the effectiveness of the proposed method, we conducted a comparative analysis using two scenarios. Initially, we trained and tested the model without the denoiser using WSIs that had not been exposed to any attack. We then introduced a range of attacks at either the image or <b>graph</b> level and processed them through the proposed network. The performance of the model was evaluated in terms of accuracy and kappa scores. The results from this comparison showed a significant improvement in cancer diagnosis accuracy, highlighting the robustness and efficiency of the proposed method in handling <b>adversarial</b> <b>challenges</b> in the context of medical imaging.</p></p class="citation"></blockquote><h3 id=25104--117312-oa-cnns-omni-adaptive-sparse-cnns-for-3d-semantic-segmentation-bohao-peng-et-al-2024>(25/104 | 117/312) OA-CNNs: Omni-Adaptive Sparse CNNs for 3D Semantic Segmentation (Bohao Peng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bohao Peng, Xiaoyang Wu, Li Jiang, Yukang Chen, Hengshuang Zhao, Zhuotao Tian, Jiaya Jia. (2024)<br><strong>OA-CNNs: Omni-Adaptive Sparse CNNs for 3D Semantic Segmentation</strong><br><button class=copy-to-clipboard title="OA-CNNs: Omni-Adaptive Sparse CNNs for 3D Semantic Segmentation" index=117>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-117 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Benchmarking, Convolutional Neural Network, Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14418v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14418v1.pdf filename=2403.14418v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The booming of 3D recognition in the 2020s began with the introduction of point cloud <b>transformers.</b> They quickly overwhelmed sparse <b>CNNs</b> and became state-of-the-art models, especially in 3D semantic segmentation. However, sparse <b>CNNs</b> are still valuable networks, due to their efficiency treasure, and ease of application. In this work, we reexamine the design distinctions and test the limits of what a sparse <b>CNN</b> can achieve. We discover that the key credit to the performance difference is adaptivity. Specifically, we propose two key components, i.e., adaptive receptive fields (spatially) and adaptive relation, to bridge the gap. This exploration led to the creation of Omni-Adaptive 3D <b>CNNs</b> (OA-CNNs), a family of networks that integrates a lightweight module to greatly enhance the adaptivity of sparse <b>CNNs</b> at minimal computational cost. Without any <b>self-attention</b> modules, OA-CNNs favorably surpass point <b>transformers</b> in terms of accuracy in both indoor and outdoor scenes, with much less latency and memory cost. Notably, it achieves 76.1%, 78.9%, and 70.6% mIoU on ScanNet v2, nuScenes, and SemanticKITTI validation <b>benchmarks</b> respectively, while maintaining at most 5x better speed than <b>transformer</b> counterparts. This revelation highlights the potential of pure sparse <b>CNNs</b> to outperform <b>transformer-related</b> networks.</p></p class="citation"></blockquote><h3 id=26104--118312-scene-graph-vit-end-to-end-open-vocabulary-visual-relationship-detection-tim-salzmann-et-al-2024>(26/104 | 118/312) Scene-Graph ViT: End-to-End Open-Vocabulary Visual Relationship Detection (Tim Salzmann et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tim Salzmann, Markus Ryll, Alex Bewley, Matthias Minderer. (2024)<br><strong>Scene-Graph ViT: End-to-End Open-Vocabulary Visual Relationship Detection</strong><br><button class=copy-to-clipboard title="Scene-Graph ViT: End-to-End Open-Vocabulary Visual Relationship Detection" index=118>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-118 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CL, cs-CV, cs-LG, cs-RO, cs.CV<br>Keyword Score: 33<br>Keywords: Object Detection, Benchmarking, Zero-shot, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14270v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14270v1.pdf filename=2403.14270v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Visual relationship detection aims to identify <b>objects</b> <b>and</b> their relationships in images. Prior methods approach this task by adding separate relationship modules or decoders to existing <b>object</b> <b>detection</b> architectures. This separation increases complexity and hinders end-to-end training, which limits performance. We propose a simple and highly efficient decoder-free architecture for open-vocabulary visual relationship detection. Our model consists of a <b>Transformer-based</b> image encoder that represents <b>objects</b> <b>as</b> tokens and models their relationships implicitly. To extract relationship information, we introduce an attention mechanism that selects <b>object</b> <b>pairs</b> likely to form a relationship. We provide a single-stage recipe to train this model on a mixture of <b>object</b> <b>and</b> relationship detection data. Our approach achieves state-of-the-art relationship detection performance on Visual Genome and on the large-vocabulary GQA <b>benchmark</b> at real-time inference speeds. We provide analyses of <b>zero-shot</b> performance, ablations, and real-world qualitative examples.</p></p class="citation"></blockquote><h3 id=27104--119312-eventdance-unsupervised-source-free-cross-modal-adaptation-for-event-based-object-recognition-xu-zheng-et-al-2024>(27/104 | 119/312) EventDance: Unsupervised Source-free Cross-modal Adaptation for Event-based Object Recognition (Xu Zheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xu Zheng, Lin Wang. (2024)<br><strong>EventDance: Unsupervised Source-free Cross-modal Adaptation for Event-based Object Recognition</strong><br><button class=copy-to-clipboard title="EventDance: Unsupervised Source-free Cross-modal Adaptation for Event-based Object Recognition" index=119>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-119 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Benchmarking, Knowledge Transfer, Self-supervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14082v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14082v1.pdf filename=2403.14082v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we make the first attempt at achieving the cross-modal (i.e., image-to-events) adaptation for event-based object recognition without accessing any labeled source image data owning to privacy and commercial issues. Tackling this novel problem is non-trivial due to the novelty of event cameras and the distinct modality gap between images and events. In particular, as only the source model is available, a hurdle is how to extract the <b>knowledge</b> <b>from</b> the source model by only using the unlabeled target event data while achieving <b>knowledge</b> <b>transfer.</b> To this end, we propose a novel framework, dubbed EventDance for this <b>unsupervised</b> source-free cross-modal adaptation problem. Importantly, inspired by event-to-video reconstruction methods, we propose a reconstruction-based modality bridging (RMB) module, which reconstructs intensity frames from events in a <b>self-supervised</b> manner. This makes it possible to build up the surrogate images to extract the <b>knowledge</b> <b>(i.e.,</b> labels) from the source model. We then propose a multi-representation <b>knowledge</b> <b>adaptation</b> (MKA) module that transfers the <b>knowledge</b> <b>to</b> target models learning events with multiple representation types for fully exploring the spatiotemporal information of events. The two modules connecting the source and target models are mutually updated so as to achieve the best performance. Experiments on three <b>benchmark</b> datasets with two adaption settings show that EventDance is on par with prior methods utilizing the source data.</p></p class="citation"></blockquote><h3 id=28104--120312-myvlm-personalizing-vlms-for-user-specific-queries-yuval-alaluf-et-al-2024>(28/104 | 120/312) MyVLM: Personalizing VLMs for User-Specific Queries (Yuval Alaluf et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuval Alaluf, Elad Richardson, Sergey Tulyakov, Kfir Aberman, Daniel Cohen-Or. (2024)<br><strong>MyVLM: Personalizing VLMs for User-Specific Queries</strong><br><button class=copy-to-clipboard title="MyVLM: Personalizing VLMs for User-Specific Queries" index=120>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-120 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Question Answering, Visual Question Answering, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14599v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14599v1.pdf filename=2403.14599v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent large-scale <b>vision-language</b> models (VLMs) have demonstrated remarkable capabilities in understanding and generating textual descriptions for <b>visual</b> <b>content.</b> <b>However,</b> these models lack an understanding of user-specific concepts. In this work, we take a first step toward the personalization of VLMs, enabling them to learn and reason over user-provided concepts. For example, we explore whether these models can learn to recognize you in an image and communicate what you are doing, tailoring the model to reflect your personal experiences and relationships. To effectively recognize a variety of user-specific concepts, we augment the VLM with external concept heads that function as toggles for the model, enabling the VLM to identify the presence of specific target concepts in a given image. Having recognized the concept, we learn a new concept embedding in the intermediate feature space of the VLM. This embedding is tasked with guiding the language model to naturally integrate the target concept in its generated response. We apply our technique to BLIP-2 and LLaVA for personalized image captioning and further show its applicability for personalized <b>visual</b> <b>question-answering.</b> <b>Our</b> experiments demonstrate our ability to generalize to unseen images of learned concepts while preserving the model behavior on unrelated inputs.</p></p class="citation"></blockquote><h3 id=29104--121312-token-transformation-matters-towards-faithful-post-hoc-explanation-for-vision-transformer-junyi-wu-et-al-2024>(29/104 | 121/312) Token Transformation Matters: Towards Faithful Post-hoc Explanation for Vision Transformer (Junyi Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junyi Wu, Bin Duan, Weitai Kang, Hao Tang, Yan Yan. (2024)<br><strong>Token Transformation Matters: Towards Faithful Post-hoc Explanation for Vision Transformer</strong><br><button class=copy-to-clipboard title="Token Transformation Matters: Towards Faithful Post-hoc Explanation for Vision Transformer" index=121>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-121 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Vision Transformer, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14552v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14552v1.pdf filename=2403.14552v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While <b>Transformers</b> have rapidly gained popularity in various computer <b>vision</b> <b>applications,</b> post-hoc explanations of their internal mechanisms remain largely unexplored. <b>Vision</b> <b>Transformers</b> extract visual information by representing image regions as transformed tokens and integrating them via attention weights. However, existing post-hoc explanation methods merely consider these attention weights, neglecting crucial information from the transformed tokens, which fails to accurately illustrate the rationales behind the models&rsquo; predictions. To incorporate the influence of token transformation into interpretation, we propose TokenTM, a novel post-hoc explanation method that utilizes our introduced measurement of token transformation effects. Specifically, we quantify token transformation effects by measuring changes in token lengths and correlations in their directions pre- and post-transformation. Moreover, we develop initialization and aggregation rules to integrate both attention weights and token transformation effects across all layers, capturing holistic token contributions throughout the model. Experimental results on segmentation and perturbation tests demonstrate the superiority of our proposed TokenTM compared to state-of-the-art <b>Vision</b> <b>Transformer</b> explanation methods.</p></p class="citation"></blockquote><h3 id=30104--122312-estimating-physical-information-consistency-of-channel-data-augmentation-for-remote-sensing-images-tom-burgert-et-al-2024>(30/104 | 122/312) Estimating Physical Information Consistency of Channel Data Augmentation for Remote Sensing Images (Tom Burgert et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tom Burgert, Begüm Demir. (2024)<br><strong>Estimating Physical Information Consistency of Channel Data Augmentation for Remote Sensing Images</strong><br><button class=copy-to-clipboard title="Estimating Physical Information Consistency of Channel Data Augmentation for Remote Sensing Images" index=122>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-122 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 30<br>Keywords: Data Augmentation, Self-supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14547v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14547v1.pdf filename=2403.14547v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The application of <b>data</b> <b>augmentation</b> for deep learning (DL) methods plays an important role in achieving state-of-the-art results in <b>supervised,</b> semi-supervised, and <b>self-supervised</b> image classification. In particular, channel transformations (e.g., solarize, grayscale, brightness adjustments) are integrated into <b>data</b> <b>augmentation</b> pipelines for remote sensing (RS) image classification tasks. However, contradicting beliefs exist about their proper applications to RS images. A common point of critique is that the application of channel augmentation techniques may lead to physically inconsistent spectral <b>data</b> <b>(i.e.,</b> pixel signatures). To shed light on the open debate, we propose an approach to estimate whether a channel augmentation technique affects the physical information of RS images. To this end, the proposed approach estimates a score that measures the alignment of a pixel signature within a time series that can be naturally subject to deviations caused by factors such as acquisition conditions or phenological states of vegetation. We compare the scores associated with original and augmented pixel signatures to evaluate the physical consistency. Experimental results on a multi-label image classification task show that channel augmentations yielding a score that exceeds the expected deviation of original pixel signatures can not improve the performance of a baseline model trained without augmentation.</p></p class="citation"></blockquote><h3 id=31104--123312-hac-hash-grid-assisted-context-for-3d-gaussian-splatting-compression-yihang-chen-et-al-2024>(31/104 | 123/312) HAC: Hash-grid Assisted Context for 3D Gaussian Splatting Compression (Yihang Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yihang Chen, Qianyi Wu, Jianfei Cai, Mehrtash Harandi, Weiyao Lin. (2024)<br><strong>HAC: Hash-grid Assisted Context for 3D Gaussian Splatting Compression</strong><br><button class=copy-to-clipboard title="HAC: Hash-grid Assisted Context for 3D Gaussian Splatting Compression" index=123>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-123 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Mutual Information, Quantization, Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14530v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14530v1.pdf filename=2403.14530v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>3D Gaussian Splatting (3DGS) has emerged as a promising framework for novel view synthesis, boasting rapid rendering speed with high fidelity. However, the substantial Gaussians and their associated attributes necessitate effective compression techniques. Nevertheless, the sparse and unorganized nature of the point cloud of Gaussians (or anchors in our paper) presents challenges for compression. To address this, we make use of the relations between the unorganized anchors and the structured hash grid, leveraging their <b>mutual</b> <b>information</b> for context modeling, and propose a Hash-grid Assisted Context (HAC) framework for highly compact 3DGS representation. Our approach introduces a binary hash grid to establish continuous spatial consistencies, allowing us to unveil the inherent spatial relations of anchors through a carefully designed context model. To facilitate entropy coding, we utilize Gaussian distributions to accurately estimate the probability of each <b>quantized</b> attribute, where an adaptive <b>quantization</b> module is proposed to enable high-precision <b>quantization</b> of these attributes for improved fidelity restoration. Additionally, we incorporate an adaptive masking strategy to eliminate invalid Gaussians and anchors. Importantly, our work is the pioneer to explore context-based compression for 3DGS representation, resulting in a remarkable size reduction of over $75\times$ compared to vanilla 3DGS, while simultaneously improving fidelity, and achieving over $11\times$ size reduction over SOTA 3DGS compression approach Scaffold-GS. Our code is available here: <a href=https://github.com/YihangChen-ee/HAC>https://github.com/YihangChen-ee/HAC</a></p></p class="citation"></blockquote><h3 id=32104--124312-tensor-network-compressibility-of-convolutional-models-sukhbinder-singh-et-al-2024>(32/104 | 124/312) Tensor network compressibility of convolutional models (Sukhbinder Singh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sukhbinder Singh, Saeed S. Jahromi, Roman Orus. (2024)<br><strong>Tensor network compressibility of convolutional models</strong><br><button class=copy-to-clipboard title="Tensor network compressibility of convolutional models" index=124>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-124 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV, quant-ph<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14379v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14379v1.pdf filename=2403.14379v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Convolutional</b> <b>neural</b> <b>networks</b> <b>(CNNs)</b> represent one of the most widely used neural network architectures, showcasing state-of-the-art performance in computer vision tasks. Although larger <b>CNNs</b> generally exhibit higher accuracy, their size can be effectively reduced by &ldquo;tensorization&rdquo; while maintaining accuracy. Tensorization consists of replacing the <b>convolution</b> kernels with compact decompositions such as Tucker, Canonical Polyadic decompositions, or quantum-inspired decompositions such as matrix product states, and directly training the factors in the decompositions to bias the learning towards low-rank decompositions. But why doesn&rsquo;t tensorization seem to impact the accuracy adversely? We explore this by assessing how truncating the <b>convolution</b> kernels of dense (untensorized) <b>CNNs</b> impact their accuracy. Specifically, we truncated the kernels of (i) a vanilla four-layer <b>CNN</b> and (ii) ResNet-50 pre-trained for image classification on CIFAR-10 and CIFAR-100 datasets. We found that kernels (especially those inside deeper layers) could often be truncated along several cuts resulting in significant loss in kernel norm but not in classification accuracy. This suggests that such ``correlation compression&rsquo;&rsquo; (underlying tensorization) is an intrinsic feature of how information is encoded in dense <b>CNNs.</b> We also found that aggressively truncated models could often recover the pre-truncation accuracy after only a few epochs of re-training, suggesting that compressing the internal correlations of <b>convolution</b> layers does not often transport the model to a worse minimum. Our results can be applied to tensorize and compress <b>CNN</b> models more effectively.</p></p class="citation"></blockquote><h3 id=33104--125312-cfpl-fas-class-free-prompt-learning-for-generalizable-face-anti-spoofing-ajian-liu-et-al-2024>(33/104 | 125/312) CFPL-FAS: Class Free Prompt Learning for Generalizable Face Anti-spoofing (Ajian Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ajian Liu, Shuai Xue, Jianwen Gan, Jun Wan, Yanyan Liang, Jiankang Deng, Sergio Escalera, Zhen Lei. (2024)<br><strong>CFPL-FAS: Class Free Prompt Learning for Generalizable Face Anti-spoofing</strong><br><button class=copy-to-clipboard title="CFPL-FAS: Class Free Prompt Learning for Generalizable Face Anti-spoofing" index=125>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-125 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Transformer, Prompt, Prompt Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14333v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14333v1.pdf filename=2403.14333v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Domain generalization (DG) based Face Anti-Spoofing (FAS) aims to improve the model&rsquo;s performance on unseen domains. Existing methods either rely on domain labels to align domain-invariant feature spaces, or disentangle generalizable features from the whole sample, which inevitably lead to the distortion of semantic feature structures and achieve limited generalization. In this work, we make use of large-scale VLMs like CLIP and leverage the textual feature to dynamically adjust the classifier&rsquo;s weights for exploring generalizable visual features. Specifically, we propose a novel Class Free <b>Prompt</b> <b>Learning</b> (CFPL) paradigm for DG FAS, which utilizes two lightweight <b>transformers,</b> namely Content Q-Former (CQF) and Style Q-Former (SQF), to learn the different semantic <b>prompts</b> <b>conditioned</b> on content and style features by using a set of learnable query vectors, respectively. Thus, the generalizable <b>prompt</b> <b>can</b> be learned by two improvements: (1) A <b>Prompt-Text</b> <b>Matched</b> (PTM) supervision is introduced to ensure CQF learns visual representation that is most informative of the content description. (2) A Diversified Style <b>Prompt</b> <b>(DSP)</b> technology is proposed to diversify the learning of style <b>prompts</b> <b>by</b> mixing feature statistics between instance-specific styles. Finally, the learned text features modulate visual features to generalization through the designed <b>Prompt</b> <b>Modulation</b> (PM). Extensive experiments show that the CFPL is effective and outperforms the state-of-the-art methods on several cross-domain datasets.</p></p class="citation"></blockquote><h3 id=34104--126312-open-vocabulary-attention-maps-with-token-optimization-for-semantic-segmentation-in-diffusion-models-pablo-marcos-manchón-et-al-2024>(34/104 | 126/312) Open-Vocabulary Attention Maps with Token Optimization for Semantic Segmentation in Diffusion Models (Pablo Marcos-Manchón et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pablo Marcos-Manchón, Roberto Alcover-Couso, Juan C. SanMiguel, Jose M. Martínez. (2024)<br><strong>Open-Vocabulary Attention Maps with Token Optimization for Semantic Segmentation in Diffusion Models</strong><br><button class=copy-to-clipboard title="Open-Vocabulary Attention Maps with Token Optimization for Semantic Segmentation in Diffusion Models" index=126>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-126 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Diffusion Model, Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14291v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14291v1.pdf filename=2403.14291v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Diffusion</b> <b>models</b> represent a new paradigm in <b>text-to-image</b> generation. Beyond generating high-quality images from text <b>prompts,</b> models such as Stable <b>Diffusion</b> <b>have</b> been successfully extended to the joint generation of semantic segmentation pseudo-masks. However, current extensions primarily rely on extracting attentions linked to <b>prompt</b> words used for image synthesis. This approach limits the generation of segmentation masks derived from word tokens not contained in the text <b>prompt.</b> In this work, we introduce Open-Vocabulary Attention Maps (OVAM)-a training-free method for <b>text-to-image</b> <b>diffusion</b> <b>models</b> that enables the generation of attention maps for any word. In addition, we propose a lightweight optimization process based on OVAM for finding tokens that generate accurate attention maps for an object class with a single annotation. We evaluate these tokens within existing state-of-the-art Stable <b>Diffusion</b> <b>extensions.</b> The best-performing model improves its mIoU from 52.1 to 86.6 for the synthetic images&rsquo; pseudo-masks, demonstrating that our optimized tokens are an efficient way to improve the performance of existing methods without architectural changes or retraining.</p></p class="citation"></blockquote><h3 id=35104--127312-weak-supervision-with-arbitrary-single-frame-for-micro--and-macro-expression-spotting-wang-wang-yu-et-al-2024>(35/104 | 127/312) Weak Supervision with Arbitrary Single Frame for Micro- and Macro-expression Spotting (Wang-Wang Yu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wang-Wang Yu, Xian-Shi Zhang, Fu-Ya Luo, Yijun Cao, Kai-Fu Yang, Hong-Mei Yan, Yong-Jie Li. (2024)<br><strong>Weak Supervision with Arbitrary Single Frame for Micro- and Macro-expression Spotting</strong><br><button class=copy-to-clipboard title="Weak Supervision with Arbitrary Single Frame for Micro- and Macro-expression Spotting" index=127>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-127 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Contrastive Learning, Weakly-supervised Learning, Weakly Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14240v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14240v1.pdf filename=2403.14240v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Frame-level micro- and macro-expression spotting methods require time-consuming frame-by-frame observation during annotation. Meanwhile, video-level spotting lacks sufficient information about the location and number of expressions during training, resulting in significantly inferior performance compared with fully-supervised spotting. To bridge this gap, we propose a point-level <b>weakly-supervised</b> expression spotting (PWES) framework, where each expression requires to be annotated with only one random frame (i.e., a point). To mitigate the issue of sparse label distribution, the prevailing solution is pseudo-label mining, which, however, introduces new problems: localizing contextual background snippets results in inaccurate boundaries and discarding foreground snippets leads to fragmentary predictions. Therefore, we design the strategies of multi-refined pseudo label generation (MPLG) and distribution-guided feature <b>contrastive</b> <b>learning</b> (DFCL) to address these problems. Specifically, MPLG generates more reliable pseudo labels by merging class-specific probabilities, attention scores, fused features, and point-level labels. DFCL is utilized to enhance feature similarity for the same categories and feature variability for different categories while capturing global representations across the entire datasets. Extensive experiments on the CAS(ME)^2, CAS(ME)^3, and SAMM-LV datasets demonstrate PWES achieves promising performance comparable to that of recent fully-supervised methods.</p></p class="citation"></blockquote><h3 id=36104--128312-toward-multi-class-anomaly-detection-exploring-class-aware-unified-model-against-inter-class-interference-xi-jiang-et-al-2024>(36/104 | 128/312) Toward Multi-class Anomaly Detection: Exploring Class-aware Unified Model against Inter-class Interference (Xi Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xi Jiang, Ying Chen, Qiang Nie, Jianlin Liu, Yong Liu, Chengjie Wang, Feng Zheng. (2024)<br><strong>Toward Multi-class Anomaly Detection: Exploring Class-aware Unified Model against Inter-class Interference</strong><br><button class=copy-to-clipboard title="Toward Multi-class Anomaly Detection: Exploring Class-aware Unified Model against Inter-class Interference" index=128>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-128 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Anomaly Detection, Supervised Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14213v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14213v1.pdf filename=2403.14213v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the context of high usability in single-class <b>anomaly</b> <b>detection</b> models, recent academic research has become concerned about the more complex multi-class <b>anomaly</b> <b>detection.</b> Although several papers have designed unified models for this task, they often overlook the utility of class labels, a potent tool for mitigating inter-class interference. To address this issue, we introduce a Multi-class Implicit Neural representation <b>Transformer</b> for unified <b>Anomaly</b> <b>Detection</b> (MINT-AD), which leverages the fine-grained category information in the training stage. By learning the multi-class distributions, the model generates class-aware query embeddings for the <b>transformer</b> decoder, mitigating inter-class interference within the reconstruction model. Utilizing such an implicit neural representation network, MINT-AD can project category and position information into a feature embedding space, further <b>supervised</b> by classification and prior probability loss functions. Experimental results on multiple datasets demonstrate that MINT-AD outperforms existing unified training models.</p></p class="citation"></blockquote><h3 id=37104--129312-harmonizing-visual-and-textual-embeddings-for-zero-shot-text-to-image-customization-yeji-song-et-al-2024>(37/104 | 129/312) Harmonizing Visual and Textual Embeddings for Zero-Shot Text-to-Image Customization (Yeji Song et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yeji Song, Jimyeong Kim, Wonhark Park, Wonsik Shin, Wonjong Rhee, Nojun Kwak. (2024)<br><strong>Harmonizing Visual and Textual Embeddings for Zero-Shot Text-to-Image Customization</strong><br><button class=copy-to-clipboard title="Harmonizing Visual and Textual Embeddings for Zero-Shot Text-to-Image Customization" index=129>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-129 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Zero-shot, Text2image, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14155v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14155v1.pdf filename=2403.14155v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In a surge of <b>text-to-image</b> (T2I) models and their customization methods that generate new images of a user-provided subject, current works focus on alleviating the costs incurred by a lengthy per-subject optimization. These <b>zero-shot</b> customization methods encode the image of a specified subject into a visual embedding which is then utilized alongside the textual embedding for diffusion guidance. The visual embedding incorporates intrinsic information about the subject, while the textual embedding provides a new, transient context. However, the existing methods often 1) are significantly affected by the input images, eg., generating images with the same pose, and 2) exhibit deterioration in the subject&rsquo;s identity. We first pin down the problem and show that redundant pose information in the visual embedding interferes with the textual embedding containing the desired pose information. To address this issue, we propose orthogonal visual embedding which effectively harmonizes with the given textual embedding. We also adopt the visual-only embedding and inject the subject&rsquo;s clear features utilizing a <b>self-attention</b> swap. Our results demonstrate the effectiveness and robustness of our method, which offers highly flexible <b>zero-shot</b> generation while effectively maintaining the subject&rsquo;s identity.</p></p class="citation"></blockquote><h3 id=38104--130312-efficient-video-diffusion-models-via-content-frame-motion-latent-decomposition-sihyun-yu-et-al-2024>(38/104 | 130/312) Efficient Video Diffusion Models via Content-Frame Motion-Latent Decomposition (Sihyun Yu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sihyun Yu, Weili Nie, De-An Huang, Boyi Li, Jinwoo Shin, Anima Anandkumar. (2024)<br><strong>Efficient Video Diffusion Models via Content-Frame Motion-Latent Decomposition</strong><br><button class=copy-to-clipboard title="Efficient Video Diffusion Models via Content-Frame Motion-Latent Decomposition" index=130>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-130 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 30<br>Keywords: Diffusion Model, Autoencoder, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14148v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14148v1.pdf filename=2403.14148v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Video <b>diffusion</b> <b>models</b> have recently made great progress in generation quality, but are still limited by the high memory and computational requirements. This is because current video <b>diffusion</b> <b>models</b> often attempt to process high-dimensional videos directly. To tackle this issue, we propose content-motion latent <b>diffusion</b> <b>model</b> (CMD), a novel efficient extension of pretrained image <b>diffusion</b> <b>models</b> for video generation. Specifically, we propose an <b>autoencoder</b> that succinctly encodes a video as a combination of a content frame (like an image) and a low-dimensional motion latent representation. The former represents the common content, and the latter represents the underlying motion in the video, respectively. We generate the content frame by <b>fine-tuning</b> a pretrained image <b>diffusion</b> <b>model,</b> and we generate the motion latent representation by training a new lightweight <b>diffusion</b> <b>model.</b> A key innovation here is the design of a compact latent space that can directly utilizes a pretrained image <b>diffusion</b> <b>model,</b> which has not been done in previous latent video <b>diffusion</b> <b>models.</b> This leads to considerably better quality generation and reduced computational costs. For instance, CMD can sample a video 7.7$\times$ faster than prior approaches by generating a video of 512$\times$1024 resolution and length 16 in 3.1 seconds. Moreover, CMD achieves an FVD score of 212.7 on WebVid-10M, 27.3% better than the previous state-of-the-art of 292.4.</p></p class="citation"></blockquote><h3 id=39104--131312-c-tpt-calibrated-test-time-prompt-tuning-for-vision-language-models-via-text-feature-dispersion-hee-suk-yoon-et-al-2024>(39/104 | 131/312) C-TPT: Calibrated Test-Time Prompt Tuning for Vision-Language Models via Text Feature Dispersion (Hee Suk Yoon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hee Suk Yoon, Eunseop Yoon, Joshua Tian Jin Tee, Mark Hasegawa-Johnson, Yingzhen Li, Chang D. Yoo. (2024)<br><strong>C-TPT: Calibrated Test-Time Prompt Tuning for Vision-Language Models via Text Feature Dispersion</strong><br><button class=copy-to-clipboard title="C-TPT: Calibrated Test-Time Prompt Tuning for Vision-Language Models via Text Feature Dispersion" index=131>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-131 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CL, cs-CV, cs-LG, cs.CV<br>Keyword Score: 30<br>Keywords: Fine-tuning, Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14119v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14119v2.pdf filename=2403.14119v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In deep learning, test-time adaptation has gained attention as a method for model <b>fine-tuning</b> without the need for labeled data. A prime exemplification is the recently proposed test-time <b>prompt</b> tuning for large-scale <b>vision-language</b> models such as CLIP. Unfortunately, these <b>prompts</b> have been mainly developed to improve accuracy, overlooking the importance of calibration, which is a crucial aspect for quantifying prediction uncertainty. However, traditional calibration methods rely on substantial amounts of labeled data, making them impractical for test-time scenarios. To this end, this paper explores calibration during test-time <b>prompt</b> tuning by leveraging the inherent properties of CLIP. Through a series of observations, we find that the <b>prompt</b> choice significantly affects the calibration in CLIP, where the <b>prompts</b> leading to higher text feature dispersion result in better-calibrated predictions. Introducing the Average Text Feature Dispersion (ATFD), we establish its relationship with calibration error and present a novel method, Calibrated Test-time <b>Prompt</b> Tuning (C-TPT), for optimizing <b>prompts</b> during test-time with enhanced calibration. Through extensive experiments on different CLIP architectures and datasets, we show that C-TPT can effectively improve the calibration of test-time <b>prompt</b> tuning without needing labeled data. The code is publicly accessible at <a href=https://github.com/hee-suk-yoon/C-TPT>https://github.com/hee-suk-yoon/C-TPT</a>.</p></p class="citation"></blockquote><h3 id=40104--132312-semantics-from-space-satellite-guided-thermal-semantic-segmentation-annotation-for-aerial-field-robots-connor-lee-et-al-2024>(40/104 | 132/312) Semantics from Space: Satellite-Guided Thermal Semantic Segmentation Annotation for Aerial Field Robots (Connor Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Connor Lee, Saraswati Soedarmadji, Matthew Anderson, Anthony J. Clark, Soon-Jo Chung. (2024)<br><strong>Semantics from Space: Satellite-Guided Thermal Semantic Segmentation Annotation for Aerial Field Robots</strong><br><button class=copy-to-clipboard title="Semantics from Space: Satellite-Guided Thermal Semantic Segmentation Annotation for Aerial Field Robots" index=132>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-132 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV<br>Keyword Score: 30<br>Keywords: Foundation Model, Zero-shot, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14056v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14056v1.pdf filename=2403.14056v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a new method to automatically generate semantic segmentation annotations for thermal imagery captured from an aerial vehicle by utilizing satellite-derived data products alongside onboard global positioning and attitude estimates. This new capability overcomes the challenge of developing thermal semantic perception algorithms for field robots due to the lack of annotated thermal field datasets and the time and costs of manual annotation, enabling precise and rapid annotation of thermal data from field collection efforts at a massively-parallelizable scale. By incorporating a thermal-conditioned refinement step with visual <b>foundation</b> <b>models,</b> our approach can produce highly-precise semantic segmentation labels using low-resolution satellite land cover data for little-to-no cost. It achieves 98.5% of the performance from using costly high-resolution options and demonstrates between 70-160% improvement over popular <b>zero-shot</b> semantic segmentation methods based on large <b>vision-language</b> models currently used for generating annotations for RGB imagery. Code will be available at: <a href=https://github.com/connorlee77/aerial-auto-segment>https://github.com/connorlee77/aerial-auto-segment</a>.</p></p class="citation"></blockquote><h3 id=41104--133312-champ-controllable-and-consistent-human-image-animation-with-3d-parametric-guidance-shenhao-zhu-et-al-2024>(41/104 | 133/312) Champ: Controllable and Consistent Human Image Animation with 3D Parametric Guidance (Shenhao Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shenhao Zhu, Junming Leo Chen, Zuozhuo Dai, Yinghui Xu, Xun Cao, Yao Yao, Hao Zhu, Siyu Zhu. (2024)<br><strong>Champ: Controllable and Consistent Human Image Animation with 3D Parametric Guidance</strong><br><button class=copy-to-clipboard title="Champ: Controllable and Consistent Human Image Animation with 3D Parametric Guidance" index=133>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-133 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 28<br>Keywords: Diffusion Model, Benchmarking, Geometry, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14781v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14781v1.pdf filename=2403.14781v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this study, we introduce a methodology for human image animation by leveraging a 3D human parametric model within a latent <b>diffusion</b> <b>framework</b> to enhance shape alignment and motion guidance in curernt human generative techniques. The methodology utilizes the SMPL(Skinned Multi-Person Linear) model as the 3D human parametric model to establish a unified representation of body shape and pose. This facilitates the accurate capture of intricate human <b>geometry</b> and motion characteristics from source videos. Specifically, we incorporate rendered depth images, normal maps, and semantic maps obtained from SMPL sequences, alongside skeleton-based motion guidance, to enrich the conditions to the latent <b>diffusion</b> <b>model</b> with comprehensive 3D shape and detailed pose attributes. A multi-layer motion fusion module, integrating <b>self-attention</b> mechanisms, is employed to fuse the shape and motion latent representations in the spatial domain. By representing the 3D human parametric model as the motion guidance, we can perform parametric shape alignment of the human body between the reference image and the source video motion. Experimental evaluations conducted on <b>benchmark</b> datasets demonstrate the methodology&rsquo;s superior ability to generate high-quality human animations that accurately capture both pose and shape variations. Furthermore, our approach also exhibits superior generalization capabilities on the proposed wild dataset. Project page: <a href=https://fudan-generative-vision.github.io/champ>https://fudan-generative-vision.github.io/champ</a>.</p></p class="citation"></blockquote><h3 id=42104--134312-multimodal-conditioned-latent-diffusion-models-for-fashion-image-editing-alberto-baldrati-et-al-2024>(42/104 | 134/312) Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing (Alberto Baldrati et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alberto Baldrati, Davide Morelli, Marcella Cornia, Marco Bertini, Rita Cucchiara. (2024)<br><strong>Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing</strong><br><button class=copy-to-clipboard title="Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing" index=134>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-134 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 26<br>Keywords: Diffusion Model, Multi-modal, Multi-modal, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14828v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14828v2.pdf filename=2403.14828v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Fashion illustration is a crucial medium for designers to convey their creative vision and transform design concepts into tangible representations that showcase the interplay between clothing and the human body. In the context of fashion design, computer vision techniques have the potential to enhance and streamline the design process. Departing from prior research primarily focused on virtual try-on, this paper tackles the task of <b>multimodal-conditioned</b> fashion image editing. Our approach aims to generate human-centric fashion images guided by <b>multimodal</b> <b>prompts,</b> including text, human body poses, garment sketches, and fabric textures. To address this problem, we propose extending latent <b>diffusion</b> <b>models</b> to incorporate these multiple modalities and modifying the structure of the denoising network, taking <b>multimodal</b> <b>prompts</b> as input. To condition the proposed architecture on fabric textures, we employ textual inversion techniques and let diverse cross-attention layers of the denoising network attend to textual and texture information, thus incorporating different granularity conditioning details. Given the lack of datasets for the task, we extend two existing fashion datasets, Dress Code and VITON-HD, with <b>multimodal</b> annotations. Experimental evaluations demonstrate the effectiveness of our proposed approach in terms of realism and coherence concerning the provided <b>multimodal</b> inputs.</p></p class="citation"></blockquote><h3 id=43104--135312-can-3d-vision-language-models-truly-understand-natural-language-weipeng-deng-et-al-2024>(43/104 | 135/312) Can 3D Vision-Language Models Truly Understand Natural Language? (Weipeng Deng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Weipeng Deng, Runyu Ding, Jihan Yang, Jiahui Liu, Yijiang Li, Xiaojuan Qi, Edith Ngai. (2024)<br><strong>Can 3D Vision-Language Models Truly Understand Natural Language?</strong><br><button class=copy-to-clipboard title="Can 3D Vision-Language Models Truly Understand Natural Language?" index=135>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-135 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 26<br>Keywords: Benchmarking, Benchmarking, Large Language Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14760v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14760v1.pdf filename=2403.14760v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Rapid advancements in 3D <b>vision-language</b> (3D-VL) tasks have opened up new avenues for human interaction with embodied agents or robots using natural language. Despite this progress, we find a notable limitation: existing 3D-VL models exhibit sensitivity to the styles of language input, struggling to understand sentences with the same semantic meaning but written in different variants. This observation raises a critical question: Can 3D <b>vision-language</b> models truly understand natural language? To test the language understandability of 3D-VL models, we first propose a language robustness task for systematically assessing 3D-VL models across various tasks, <b>benchmarking</b> their performance when presented with different language style variants. Importantly, these variants are commonly encountered in applications requiring direct interaction with humans, such as embodied robotics, given the diversity and unpredictability of human language. We propose a 3D Language Robustness Dataset, designed based on the characteristics of human language, to facilitate the systematic study of robustness. Our comprehensive evaluation uncovers a significant drop in the performance of all existing models across various 3D-VL tasks. Even the state-of-the-art 3D-LLM fails to understand some variants of the same sentences. Further in-depth analysis suggests that the existing models have a fragile and biased fusion module, which stems from the low diversity of the existing dataset. Finally, we propose a training-free module driven by <b>LLM,</b> which improves language robustness. Datasets and code will be available at github.</p></p class="citation"></blockquote><h3 id=44104--136312-glc-source-free-universal-domain-adaptation-through-global-local-clustering-and-contrastive-affinity-learning-sanqing-qu-et-al-2024>(44/104 | 136/312) GLC++: Source-Free Universal Domain Adaptation through Global-Local Clustering and Contrastive Affinity Learning (Sanqing Qu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sanqing Qu, Tianpei Zou, Florian Röhrbein, Cewu Lu, Guang Chen, Dacheng Tao, Changjun Jiang. (2024)<br><strong>GLC++: Source-Free Universal Domain Adaptation through Global-Local Clustering and Contrastive Affinity Learning</strong><br><button class=copy-to-clipboard title="GLC++: Source-Free Universal Domain Adaptation through Global-Local Clustering and Contrastive Affinity Learning" index=136>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-136 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 26<br>Keywords: Benchmarking, Clustering, Contrastive Learning, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14410v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14410v1.pdf filename=2403.14410v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep neural networks often exhibit sub-optimal performance under covariate and category shifts. Source-Free <b>Domain</b> <b>Adaptation</b> (SFDA) presents a promising solution to this dilemma, yet most SFDA approaches are restricted to closed-set scenarios. In this paper, we explore Source-Free Universal <b>Domain</b> <b>Adaptation</b> (SF-UniDA) aiming to accurately classify &ldquo;known&rdquo; data belonging to common categories and segregate them from target-private &ldquo;unknown&rdquo; data. We propose a novel Global and Local <b>Clustering</b> (GLC) technique, which comprises an adaptive one-vs-all global <b>clustering</b> algorithm to discern between target classes, complemented by a local k-NN <b>clustering</b> strategy to mitigate negative transfer. Despite the effectiveness, the inherent closed-set source architecture leads to uniform treatment of &ldquo;unknown&rdquo; data, impeding the identification of distinct &ldquo;unknown&rdquo; categories. To address this, we evolve GLC to GLC++, integrating a <b>contrastive</b> <b>affinity</b> learning strategy. We examine the superiority of GLC and GLC++ across multiple <b>benchmarks</b> and category shift scenarios. Remarkably, in the most challenging open-partial-set scenarios, GLC and GLC++ surpass GATE by 16.7% and 18.6% in H-score on VisDA, respectively. GLC++ enhances the novel category <b>clustering</b> accuracy of GLC by 4.3% in open-set scenarios on Office-Home. Furthermore, the introduced <b>contrastive</b> <b>learning</b> strategy not only enhances GLC but also significantly facilitates existing methodologies.</p></p class="citation"></blockquote><h3 id=45104--137312-evaluating-panoramic-3d-estimation-in-indoor-lighting-analysis-zining-cheng-et-al-2024>(45/104 | 137/312) Evaluating Panoramic 3D Estimation in Indoor Lighting Analysis (Zining Cheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zining Cheng, Guanzhou Ji. (2024)<br><strong>Evaluating Panoramic 3D Estimation in Indoor Lighting Analysis</strong><br><button class=copy-to-clipboard title="Evaluating Panoramic 3D Estimation in Indoor Lighting Analysis" index=137>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-137 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 25<br>Keywords: Geometry, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14836v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14836v1.pdf filename=2403.14836v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents the use of panoramic 3D estimation in lighting <b>simulation.</b> Conventional lighting <b>simulation</b> necessitates detailed modeling as input, resulting in significant labor effort and time cost. The 3D layout estimation method directly takes a single panorama as input and generates a lighting <b>simulation</b> model with room <b>geometry</b> and window aperture. We evaluate the <b>simulation</b> results by comparing the luminance errors between on-site High Dynamic Range (HDR) photographs, 3D estimation model, and detailed model in panoramic representation and fisheye perspective. Given the selected scene, the results demonstrate the estimated room layout is reliable for lighting <b>simulation.</b></p></p class="citation"></blockquote><h3 id=46104--138312-zero-shot-multi-object-shape-completion-shun-iwase-et-al-2024>(46/104 | 138/312) Zero-Shot Multi-Object Shape Completion (Shun Iwase et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shun Iwase, Katherine Liu, Vitor Guizilini, Adrien Gaidon, Kris Kitani, Rares Ambrus, Sergey Zakharov. (2024)<br><strong>Zero-Shot Multi-Object Shape Completion</strong><br><button class=copy-to-clipboard title="Zero-Shot Multi-Object Shape Completion" index=138>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-138 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 25<br>Keywords: Geometry, Zero-shot, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14628v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14628v1.pdf filename=2403.14628v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a 3D shape completion method that recovers the complete <b>geometry</b> of multiple objects in complex scenes from a single RGB-D image. Despite notable advancements in single object 3D shape completion, high-quality reconstructions in highly cluttered real-world multi-object scenes remains a challenge. To address this issue, we propose OctMAE, an architecture that leverages an Octree U-Net and a latent 3D MAE to achieve high-quality and near real-time multi-object shape completion through both local and global geometric <b>reasoning.</b> Because a na"ive 3D MAE can be computationally intractable and memory intensive even in the latent space, we introduce a novel occlusion masking strategy and adopt 3D rotary embeddings, which significantly improves the runtime and shape completion quality. To generalize to a wide range of objects in diverse scenes, we create a large-scale photorealistic dataset, featuring a diverse set of 12K 3D object models from the Objaverse dataset which are rendered in multi-object scenes with physics-based positioning. Our method outperforms the current state-of-the-art on both synthetic and real-world datasets and demonstrates a strong <b>zero-shot</b> capability.</p></p class="citation"></blockquote><h3 id=47104--139312-science-based-ai-model-certification-for-untrained-operational-environments-with-application-in-traffic-state-estimation-daryl-mupupuni-et-al-2024>(47/104 | 139/312) Science based AI model certification for untrained operational environments with application in traffic state estimation (Daryl Mupupuni et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daryl Mupupuni, Anupama Guntu, Liang Hong, Kamrul Hasan, Leehyun Keel. (2024)<br><strong>Science based AI model certification for untrained operational environments with application in traffic state estimation</strong><br><button class=copy-to-clipboard title="Science based AI model certification for untrained operational environments with application in traffic state estimation" index=139>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-139 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 25<br>Keywords: Black Box, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14093v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14093v1.pdf filename=2403.14093v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The expanding role of Artificial Intelligence (AI) in diverse engineering domains highlights the challenges associated with deploying AI models in new operational environments, involving substantial investments in data collection and model training. Rapid application of AI necessitates evaluating the feasibility of utilizing pre-trained models in unobserved operational settings with minimal or no additional data. However, interpreting the opaque nature of AI&rsquo;s <b>black-box</b> <b>models</b> remains a persistent challenge. Addressing this issue, this paper proposes a science-based certification methodology to assess the viability of employing pre-trained data-driven models in untrained operational environments. The methodology advocates a profound integration of domain knowledge, leveraging theoretical and analytical models from physics and related disciplines, with data-driven AI models. This novel approach introduces tools to facilitate the development of secure engineering systems, providing decision-makers with confidence in the trustworthiness and safety of AI-based models across diverse environments characterized by limited training data and dynamic, uncertain conditions. The paper demonstrates the efficacy of this methodology in real-world safety-critical scenarios, particularly in the context of traffic state estimation. Through <b>simulation</b> results, the study illustrates how the proposed methodology efficiently quantifies physical inconsistencies exhibited by pre-trained AI models. By utilizing analytical models, the methodology offers a means to gauge the applicability of pre-trained AI models in new operational environments. This research contributes to advancing the understanding and deployment of AI models, offering a robust certification framework that enhances confidence in their reliability and safety across a spectrum of operational conditions.</p></p class="citation"></blockquote><h3 id=48104--140312-dsgg-dense-relation-transformer-for-an-end-to-end-scene-graph-generation-zeeshan-hayder-et-al-2024>(48/104 | 140/312) DSGG: Dense Relation Transformer for an End-to-end Scene Graph Generation (Zeeshan Hayder et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zeeshan Hayder, Xuming He. (2024)<br><strong>DSGG: Dense Relation Transformer for an End-to-end Scene Graph Generation</strong><br><button class=copy-to-clipboard title="DSGG: Dense Relation Transformer for an End-to-end Scene Graph Generation" index=140>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-140 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Graph, Knowledge Distillation, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14886v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14886v1.pdf filename=2403.14886v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Scene <b>graph</b> generation aims to capture detailed spatial and semantic relationships between objects in an image, which is challenging due to incomplete labelling, long-tailed relationship categories, and relational semantic overlap. Existing <b>Transformer-based</b> methods either employ distinct queries for objects and predicates or utilize holistic queries for relation triplets and hence often suffer from limited capacity in learning low-frequency relationships. In this paper, we present a new <b>Transformer-based</b> method, called DSGG, that views scene <b>graph</b> detection as a direct <b>graph</b> prediction problem based on a unique set of <b>graph-aware</b> queries. In particular, each <b>graph-aware</b> query encodes a compact representation of both the node and all of its relations in the <b>graph,</b> acquired through the utilization of a relaxed sub-graph matching during the training process. Moreover, to address the problem of relational semantic overlap, we utilize a strategy for relation <b>distillation,</b> aiming to efficiently learn multiple instances of semantic relationships. Extensive experiments on the VG and the PSG datasets show that our model achieves state-of-the-art results, showing a significant improvement of 3.5% and 6.7% in mR@50 and mR@100 for the scene-graph generation task and achieves an even more substantial improvement of 8.5% and 10.3% in mR@50 and mR@100 for the panoptic scene <b>graph</b> generation task. Code is available at \url{https://github.com/zeeshanhayder/DSGG}.</p></p class="citation"></blockquote><h3 id=49104--141312-dino-tracker-taming-dino-for-self-supervised-point-tracking-in-a-single-video-narek-tumanyan-et-al-2024>(49/104 | 141/312) DINO-Tracker: Taming DINO for Self-Supervised Point Tracking in a Single Video (Narek Tumanyan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Narek Tumanyan, Assaf Singer, Shai Bagon, Tali Dekel. (2024)<br><strong>DINO-Tracker: Taming DINO for Self-Supervised Point Tracking in a Single Video</strong><br><button class=copy-to-clipboard title="DINO-Tracker: Taming DINO for Self-Supervised Point Tracking in a Single Video" index=141>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-141 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Benchmarking, Self-supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14548v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14548v1.pdf filename=2403.14548v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present DINO-Tracker &ndash; a new framework for long-term dense tracking in video. The pillar of our approach is combining test-time training on a single video, with the powerful localized semantic features learned by a pre-trained DINO-ViT model. Specifically, our framework simultaneously adopts DINO&rsquo;s features to fit to the motion observations of the test video, while training a tracker that directly leverages the refined features. The entire framework is trained end-to-end using a combination of <b>self-supervised</b> losses, and regularization that allows us to retain and benefit from DINO&rsquo;s semantic prior. Extensive evaluation demonstrates that our method achieves state-of-the-art results on known <b>benchmarks.</b> DINO-tracker significantly outperforms <b>self-supervised</b> methods and is competitive with state-of-the-art <b>supervised</b> trackers, while outperforming them in challenging cases of tracking under long-term occlusions.</p></p class="citation"></blockquote><h3 id=50104--142312-ranking-distillation-for-open-ended-video-question-answering-with-insufficient-labels-tianming-liang-et-al-2024>(50/104 | 142/312) Ranking Distillation for Open-Ended Video Question Answering with Insufficient Labels (Tianming Liang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianming Liang, Chaolei Tan, Beihao Xia, Wei-Shi Zheng, Jian-Fang Hu. (2024)<br><strong>Ranking Distillation for Open-Ended Video Question Answering with Insufficient Labels</strong><br><button class=copy-to-clipboard title="Ranking Distillation for Open-Ended Video Question Answering with Insufficient Labels" index=142>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-142 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Benchmarking, Knowledge Distillation, Question Answering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14430v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14430v1.pdf filename=2403.14430v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper focuses on open-ended video <b>question</b> <b>answering,</b> which aims to find the correct answers from a large answer set in response to a video-related <b>question.</b> <b>This</b> is essentially a multi-label classification task, since a <b>question</b> <b>may</b> have multiple answers. However, due to annotation costs, the labels in existing <b>benchmarks</b> are always extremely insufficient, typically one answer per <b>question.</b> <b>As</b> a result, existing works tend to directly treat all the unlabeled answers as negative labels, leading to limited ability for generalization. In this work, we introduce a simple yet effective ranking <b>distillation</b> framework (RADI) to mitigate this problem without additional manual annotation. RADI employs a teacher model trained with incomplete labels to generate rankings for potential answers, which contain rich knowledge about label priority as well as label-associated visual cues, thereby enriching the insufficient labeling information. To avoid overconfidence in the imperfect teacher model, we further present two robust and parameter-free ranking <b>distillation</b> approaches: a pairwise approach which introduces adaptive soft margins to dynamically refine the optimization constraints on various pairwise rankings, and a listwise approach which adopts sampling-based partial listwise learning to resist the bias in teacher ranking. Extensive experiments on five popular <b>benchmarks</b> consistently show that both our pairwise and listwise RADIs outperform state-of-the-art methods. Further analysis demonstrates the effectiveness of our methods on the insufficient labeling problem.</p></p class="citation"></blockquote><h3 id=51104--143312-pensieve-retrospect-then-compare-mitigates-visual-hallucination-dingchen-yang-et-al-2024>(51/104 | 143/312) Pensieve: Retrospect-then-Compare Mitigates Visual Hallucination (Dingchen Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dingchen Yang, Bowen Cao, Guang Chen, Changjun Jiang. (2024)<br><strong>Pensieve: Retrospect-then-Compare Mitigates Visual Hallucination</strong><br><button class=copy-to-clipboard title="Pensieve: Retrospect-then-Compare Mitigates Visual Hallucination" index=143>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-143 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Multi-modal, Large Language Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14401v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14401v1.pdf filename=2403.14401v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multi-modal</b> <b>Large</b> <b>Language</b> <b>Models</b> (MLLMs) demonstrate remarkable success across various <b>vision-language</b> tasks. However, they suffer from visual hallucination, where the generated responses diverge from the provided image. Are MLLMs completely oblivious to accurate visual cues when they hallucinate? Our investigation reveals that the visual branch may simultaneously advocate both accurate and non-existent content. To address this issue, we propose Pensieve, a training-free method inspired by our observation that analogous visual hallucinations can arise among images sharing common semantic and appearance characteristics. During inference, Pensieve enables MLLMs to retrospect relevant images as references and compare them with the test image. This paradigm assists MLLMs in downgrading hallucinatory content mistakenly supported by the visual input. Experiments on Whoops, MME, POPE, and LLaVA Bench demonstrate the efficacy of Pensieve in mitigating visual hallucination, surpassing other advanced decoding strategies. Additionally, Pensieve aids MLLMs in identifying details in the image and enhancing the specificity of image descriptions.</p></p class="citation"></blockquote><h3 id=52104--144312-a-bag-of-tricks-for-few-shot-class-incremental-learning-shuvendu-roy-et-al-2024>(52/104 | 144/312) A Bag of Tricks for Few-Shot Class-Incremental Learning (Shuvendu Roy et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shuvendu Roy, Chunjong Park, Aldi Fahrezi, Ali Etemad. (2024)<br><strong>A Bag of Tricks for Few-Shot Class-Incremental Learning</strong><br><button class=copy-to-clipboard title="A Bag of Tricks for Few-Shot Class-Incremental Learning" index=144>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-144 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 23<br>Keywords: Benchmarking, Continual Learning, Few-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14392v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14392v1.pdf filename=2403.14392v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a bag of tricks framework for <b>few-shot</b> class-incremental learning (FSCIL), which is a challenging form of <b>continual</b> <b>learning</b> that involves continuous adaptation to new tasks with limited samples. FSCIL requires both stability and adaptability, i.e., preserving proficiency in previously learned tasks while learning new ones. Our proposed bag of tricks brings together eight key and highly influential techniques that improve stability, adaptability, and overall performance under a unified framework for FSCIL. We organize these tricks into three categories: stability tricks, adaptability tricks, and training tricks. Stability tricks aim to mitigate the forgetting of previously learned classes by enhancing the separation between the embeddings of learned classes and minimizing interference when learning new ones. On the other hand, adaptability tricks focus on the effective learning of new classes. Finally, training tricks improve the overall performance without compromising stability or adaptability. We perform extensive experiments on three <b>benchmark</b> datasets, CIFAR-100, CUB-200, and miniIMageNet, to evaluate the impact of our proposed framework. Our detailed analysis shows that our approach substantially improves both stability and adaptability, establishing a new state-of-the-art by outperforming prior works in the area. We believe our method provides a go-to solution and establishes a robust baseline for future research in this area.</p></p class="citation"></blockquote><h3 id=53104--145312-annotation-efficient-polyp-segmentation-via-active-learning-duojun-huang-et-al-2024>(53/104 | 145/312) Annotation-Efficient Polyp Segmentation via Active Learning (Duojun Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Duojun Huang, Xinyu Xiong, De-Jun Fan, Feng Gao, Xiao-Jian Wu, Guanbin Li. (2024)<br><strong>Annotation-Efficient Polyp Segmentation via Active Learning</strong><br><button class=copy-to-clipboard title="Annotation-Efficient Polyp Segmentation via Active Learning" index=145>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-145 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Active Learning, Clustering, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14350v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14350v1.pdf filename=2403.14350v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep learning-based techniques have proven effective in polyp segmentation tasks when provided with sufficient pixel-wise labeled data. However, the high cost of manual annotation has created a bottleneck for model generalization. To minimize annotation costs, we propose a deep <b>active</b> <b>learning</b> framework for annotation-efficient polyp segmentation. In practice, we measure the uncertainty of each sample by examining the similarity between features masked by the prediction map of the polyp and the background area. Since the segmentation model tends to perform weak in samples with indistinguishable features of foreground and background areas, uncertainty sampling facilitates the fitting of under-learning data. Furthermore, <b>clustering</b> image-level features weighted by uncertainty identify samples that are both uncertain and representative. To enhance the selectivity of the <b>active</b> <b>selection</b> strategy, we propose a novel <b>unsupervised</b> feature discrepancy learning mechanism. The selection strategy and feature optimization work in tandem to achieve optimal performance with a limited annotation budget. Extensive experimental results have demonstrated that our proposed method achieved state-of-the-art performance compared to other competitors on both a public dataset and a large-scale in-house dataset.</p></p class="citation"></blockquote><h3 id=54104--146312-softpatch-unsupervised-anomaly-detection-with-noisy-data-xi-jiang-et-al-2024>(54/104 | 146/312) SoftPatch: Unsupervised Anomaly Detection with Noisy Data (Xi Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xi Jiang, Ying Chen, Qiang Nie, Yong Liu, Jianlin Liu, Bin-Bin Gao, Jun Liu, Chengjie Wang, Feng Zheng. (2024)<br><strong>SoftPatch: Unsupervised Anomaly Detection with Noisy Data</strong><br><button class=copy-to-clipboard title="SoftPatch: Unsupervised Anomaly Detection with Noisy Data" index=146>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-146 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 23<br>Keywords: Anomaly Detection, Benchmarking, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14233v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14233v1.pdf filename=2403.14233v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Although mainstream <b>unsupervised</b> <b>anomaly</b> <b>detection</b> (AD) algorithms perform well in academic datasets, their performance is limited in practical application due to the ideal experimental setting of clean training data. Training with noisy data is an inevitable problem in real-world <b>anomaly</b> <b>detection</b> but is seldom discussed. This paper considers label-level noise in image sensory <b>anomaly</b> <b>detection</b> for the first time. To solve this problem, we proposed a memory-based <b>unsupervised</b> AD method, SoftPatch, which efficiently denoises the data at the patch level. Noise discriminators are utilized to generate outlier scores for patch-level noise elimination before coreset construction. The scores are then stored in the memory bank to soften the <b>anomaly</b> <b>detection</b> boundary. Compared with existing methods, SoftPatch maintains a strong modeling ability of normal data and alleviates the overconfidence problem in coreset. Comprehensive experiments in various noise scenes demonstrate that SoftPatch outperforms the state-of-the-art AD methods on the MVTecAD and BTAD <b>benchmarks</b> and is comparable to those methods under the setting without noise.</p></p class="citation"></blockquote><h3 id=55104--147312-unleashing-unlabeled-data-a-paradigm-for-cross-view-geo-localization-guopeng-li-et-al-2024>(55/104 | 147/312) Unleashing Unlabeled Data: A Paradigm for Cross-View Geo-Localization (Guopeng Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guopeng Li, Ming Qian, Gui-Song Xia. (2024)<br><strong>Unleashing Unlabeled Data: A Paradigm for Cross-View Geo-Localization</strong><br><button class=copy-to-clipboard title="Unleashing Unlabeled Data: A Paradigm for Cross-View Geo-Localization" index=147>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-147 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Benchmarking, Supervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14198v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14198v1.pdf filename=2403.14198v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper investigates the effective utilization of unlabeled data for large-area cross-view geo-localization (CVGL), encompassing both <b>unsupervised</b> and semi-supervised settings. Common approaches to CVGL rely on ground-satellite image pairs and employ label-driven <b>supervised</b> training. However, the cost of collecting precise cross-view image pairs hinders the deployment of CVGL in real-life scenarios. Without the pairs, CVGL will be more challenging to handle the significant imaging and spatial gaps between ground and satellite images. To this end, we propose an <b>unsupervised</b> framework including a cross-view projection to guide the model for retrieving initial pseudo-labels and a fast re-ranking mechanism to refine the pseudo-labels by leveraging the fact that ``the perfectly paired ground-satellite image is located in a unique and identical scene". The framework exhibits competitive performance compared with <b>supervised</b> works on three open-source <b>benchmarks.</b> Our code and models will be released on <a href=https://github.com/liguopeng0923/UCVGL>https://github.com/liguopeng0923/UCVGL</a>.</p></p class="citation"></blockquote><h3 id=56104--148312-external-knowledge-enhanced-3d-scene-generation-from-sketch-zijie-wu-et-al-2024>(56/104 | 148/312) External Knowledge Enhanced 3D Scene Generation from Sketch (Zijie Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zijie Wu, Mingtao Feng, Yaonan Wang, He Xie, Weisheng Dong, Bo Miao, Ajmal Mian. (2024)<br><strong>External Knowledge Enhanced 3D Scene Generation from Sketch</strong><br><button class=copy-to-clipboard title="External Knowledge Enhanced 3D Scene Generation from Sketch" index=148>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-148 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Graph, Transformer, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14121v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14121v1.pdf filename=2403.14121v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Generating realistic 3D scenes is challenging due to the complexity of room layouts and object geometries.We propose a sketch based knowledge enhanced diffusion architecture (SEK) for generating customized, diverse, and plausible 3D scenes. SEK conditions the denoising process with a hand-drawn sketch of the target scene and cues from an object relationship knowledge base. We first construct an external knowledge base containing object relationships and then leverage knowledge enhanced <b>graph</b> <b>reasoning</b> to assist our model in understanding hand-drawn sketches. A scene is represented as a combination of 3D objects and their relationships, and then incrementally diffused to reach a Gaussian distribution.We propose a 3D denoising scene <b>transformer</b> that learns to reverse the diffusion process, conditioned by a hand-drawn sketch along with knowledge cues, to regressively generate the scene including the 3D object instances as well as their layout. Experiments on the 3D-FRONT dataset show that our model improves FID, CKL by 17.41%, 37.18% in 3D scene generation and FID, KID by 19.12%, 20.06% in 3D scene completion compared to the nearest competitor DiffuScene.</p></p class="citation"></blockquote><h3 id=57104--149312-auto-train-once-controller-network-guided-automatic-network-pruning-from-scratch-xidong-wu-et-al-2024>(57/104 | 149/312) Auto-Train-Once: Controller Network Guided Automatic Network Pruning from Scratch (Xidong Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xidong Wu, Shangqian Gao, Zeyu Zhang, Zhenzhen Li, Runxue Bao, Yanfu Zhang, Xiaoqian Wang, Heng Huang. (2024)<br><strong>Auto-Train-Once: Controller Network Guided Automatic Network Pruning from Scratch</strong><br><button class=copy-to-clipboard title="Auto-Train-Once: Controller Network Guided Automatic Network Pruning from Scratch" index=149>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-149 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 23<br>Keywords: Benchmarking, Fine-tuning, Pruning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14729v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14729v1.pdf filename=2403.14729v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Current techniques for deep neural network (DNN) <b>pruning</b> often involve intricate multi-step processes that require domain-specific expertise, making their widespread adoption challenging. To address the limitation, the Only-Train-Once (OTO) and OTOv2 are proposed to eliminate the need for additional <b>fine-tuning</b> steps by directly training and compressing a general DNN from scratch. Nevertheless, the static design of optimizers (in OTO) can lead to convergence issues of local optima. In this paper, we proposed the Auto-Train-Once (ATO), an innovative network <b>pruning</b> algorithm designed to automatically reduce the computational and storage costs of DNNs. During the model training phase, our approach not only trains the target model but also leverages a controller network as an architecture generator to guide the learning of target model weights. Furthermore, we developed a novel stochastic gradient algorithm that enhances the coordination between model training and controller network training, thereby improving <b>pruning</b> performance. We provide a comprehensive convergence analysis as well as extensive experiments, and the results show that our approach achieves state-of-the-art performance across various model architectures (including ResNet18, ResNet34, ResNet50, ResNet56, and MobileNetv2) on standard <b>benchmark</b> datasets (CIFAR-10, CIFAR-100, and ImageNet).</p></p class="citation"></blockquote><h3 id=58104--150312-hyperspectral-neural-radiance-fields-gerry-chen-et-al-2024>(58/104 | 150/312) Hyperspectral Neural Radiance Fields (Gerry Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gerry Chen, Sunil Kumar Narayanan, Thomas Gautier Ottou, Benjamin Missaoui, Harsh Muriki, Cédric Pradalier, Yongsheng Chen. (2024)<br><strong>Hyperspectral Neural Radiance Fields</strong><br><button class=copy-to-clipboard title="Hyperspectral Neural Radiance Fields" index=150>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-150 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14839v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14839v1.pdf filename=2403.14839v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Hyperspectral Imagery (HSI) has been used in many applications to non-destructively determine the material and/or chemical compositions of samples. There is growing interest in creating 3D hyperspectral reconstructions, which could provide both spatial and spectral information while also mitigating common HSI challenges such as non-Lambertian surfaces and translucent objects. However, traditional 3D reconstruction with HSI is difficult due to technological limitations of hyperspectral cameras. In recent years, Neural Radiance Fields (NeRFs) have seen widespread success in creating high quality volumetric 3D representations of scenes captured by a variety of camera models. Leveraging recent advances in NeRFs, we propose computing a hyperspectral 3D reconstruction in which every point in space and view direction is characterized by wavelength-dependent radiance and transmittance spectra. To evaluate our approach, a dataset containing nearly 2000 hyperspectral images across 8 scenes and 2 cameras was collected. We perform comparisons against traditional RGB NeRF baselines and apply ablation testing with alternative spectra representations. Finally, we demonstrate the potential of hyperspectral NeRFs for hyperspectral super-resolution and imaging sensor <b>simulation.</b> We show that our hyperspectral NeRF approach enables creating fast, accurate volumetric 3D hyperspectral scenes and enables several new applications and areas for future study.</p></p class="citation"></blockquote><h3 id=59104--151312-diffusion-attack-leveraging-stable-diffusion-for-naturalistic-image-attacking-qianyu-guo-et-al-2024>(59/104 | 151/312) Diffusion Attack: Leveraging Stable Diffusion for Naturalistic Image Attacking (Qianyu Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qianyu Guo, Jiaming Fu, Yawen Lu, Dongming Gan. (2024)<br><strong>Diffusion Attack: Leveraging Stable Diffusion for Naturalistic Image Attacking</strong><br><button class=copy-to-clipboard title="Diffusion Attack: Leveraging Stable Diffusion for Naturalistic Image Attacking" index=151>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-151 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV<br>Keyword Score: 20<br>Keywords: Style Transfer, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14778v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14778v1.pdf filename=2403.14778v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In Virtual Reality (VR), <b>adversarial</b> <b>attack</b> remains a significant security threat. Most deep learning-based methods for physical and digital <b>adversarial</b> <b>attacks</b> focus on enhancing attack performance by crafting <b>adversarial</b> <b>examples</b> that contain large printable distortions that are easy for human observers to identify. However, attackers rarely impose limitations on the naturalness and comfort of the appearance of the generated attack image, resulting in a noticeable and unnatural attack. To address this challenge, we propose a framework to incorporate <b>style</b> <b>transfer</b> to craft <b>adversarial</b> <b>inputs</b> of natural <b>styles</b> <b>that</b> exhibit minimal detectability and maximum natural appearance, while maintaining superior attack capabilities.</p></p class="citation"></blockquote><h3 id=60104--152312-improving-robustness-to-model-inversion-attacks-via-sparse-coding-architectures-sayanton-v-dibbo-et-al-2024>(60/104 | 152/312) Improving Robustness to Model Inversion Attacks via Sparse Coding Architectures (Sayanton V. Dibbo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sayanton V. Dibbo, Adam Breuer, Juston Moore, Michael Teti. (2024)<br><strong>Improving Robustness to Model Inversion Attacks via Sparse Coding Architectures</strong><br><button class=copy-to-clipboard title="Improving Robustness to Model Inversion Attacks via Sparse Coding Architectures" index=152>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-152 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CR, cs-CV, cs-LG, cs.CV<br>Keyword Score: 20<br>Keywords: Generative Adversarial Network, Stochastic Gradient Descent<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14772v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14772v1.pdf filename=2403.14772v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent model inversion attack algorithms permit adversaries to reconstruct a neural network&rsquo;s private training data just by repeatedly querying the network and inspecting its outputs. In this work, we develop a novel network architecture that leverages sparse-coding layers to obtain superior robustness to this class of attacks. Three decades of computer science research has studied sparse coding in the context of image denoising, object recognition, and adversarial misclassification settings, but to the best of our knowledge, its connection to state-of-the-art privacy vulnerabilities remains unstudied. However, sparse coding architectures suggest an advantageous means to defend against model inversion attacks because they allow us to control the amount of irrelevant private information encoded in a network&rsquo;s intermediate representations in a manner that can be computed efficiently during training and that is known to have little effect on classification accuracy. Specifically, compared to networks trained with a variety of state-of-the-art defenses, our sparse-coding architectures maintain comparable or higher classification accuracy while degrading state-of-the-art training data reconstructions by factors of 1.1 to 18.3 across a variety of reconstruction quality metrics (PSNR, SSIM, FID). This performance advantage holds across 5 datasets ranging from CelebA faces to medical images and CIFAR-10, and across various state-of-the-art <b>SGD-based</b> and <b>GAN-based</b> inversion attacks, including Plug-&-Play attacks. We provide a cluster-ready PyTorch codebase to promote research and standardize defense evaluations.</p></p class="citation"></blockquote><h3 id=61104--153312-grm-large-gaussian-reconstruction-model-for-efficient-3d-reconstruction-and-generation-yinghao-xu-et-al-2024>(61/104 | 153/312) GRM: Large Gaussian Reconstruction Model for Efficient 3D Reconstruction and Generation (Yinghao Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yinghao Xu, Zifan Shi, Wang Yifan, Hansheng Chen, Ceyuan Yang, Sida Peng, Yujun Shen, Gordon Wetzstein. (2024)<br><strong>GRM: Large Gaussian Reconstruction Model for Efficient 3D Reconstruction and Generation</strong><br><button class=copy-to-clipboard title="GRM: Large Gaussian Reconstruction Model for Efficient 3D Reconstruction and Generation" index=153>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-153 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14621v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14621v1.pdf filename=2403.14621v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce GRM, a large-scale reconstructor capable of recovering a 3D asset from sparse-view images in around 0.1s. GRM is a feed-forward <b>transformer-based</b> model that efficiently incorporates multi-view information to translate the input pixels into pixel-aligned Gaussians, which are unprojected to create a set of densely distributed 3D Gaussians representing a scene. Together, our <b>transformer</b> architecture and the use of 3D Gaussians unlock a scalable and efficient reconstruction framework. Extensive experimental results demonstrate the superiority of our method over alternatives regarding both reconstruction quality and efficiency. We also showcase the potential of GRM in generative tasks, i.e., text-to-3D and image-to-3D, by integrating it with existing multi-view <b>diffusion</b> <b>models.</b> Our project website is at: <a href=https://justimyhxu.github.io/projects/grm/>https://justimyhxu.github.io/projects/grm/</a>.</p></p class="citation"></blockquote><h3 id=62104--154312-dreamreward-text-to-3d-generation-with-human-preference-junliang-ye-et-al-2024>(62/104 | 154/312) DreamReward: Text-to-3D Generation with Human Preference (Junliang Ye et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junliang Ye, Fangfu Liu, Qixiu Li, Zhengyi Wang, Yikai Wang, Xinzhou Wang, Yueqi Duan, Jun Zhu. (2024)<br><strong>DreamReward: Text-to-3D Generation with Human Preference</strong><br><button class=copy-to-clipboard title="DreamReward: Text-to-3D Generation with Human Preference" index=154>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-154 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CL, cs-CV, cs-LG, cs.CV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14613v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14613v1.pdf filename=2403.14613v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>3D content creation from text <b>prompts</b> has shown remarkable success recently. However, current text-to-3D methods often generate 3D results that do not align well with human preferences. In this paper, we present a comprehensive framework, coined DreamReward, to learn and improve text-to-3D models from human preference feedback. To begin with, we collect 25k expert comparisons based on a systematic annotation pipeline including rating and ranking. Then, we build Reward3D &ndash; the first general-purpose text-to-3D human preference reward model to effectively encode human preferences. Building upon the 3D reward model, we finally perform theoretical analysis and present the Reward3D Feedback Learning (DreamFL), a direct tuning algorithm to optimize the multi-view <b>diffusion</b> <b>models</b> with a redefined scorer. Grounded by theoretical proof and extensive experiment comparisons, our DreamReward successfully generates high-fidelity and 3D consistent results with significant boosts in <b>prompt</b> alignment with human intention. Our results demonstrate the great potential for learning from human feedback to improve text-to-3D models.</p></p class="citation"></blockquote><h3 id=63104--155312-implicit-style-content-separation-using-b-lora-yarden-frenkel-et-al-2024>(63/104 | 155/312) Implicit Style-Content Separation using B-LoRA (Yarden Frenkel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yarden Frenkel, Yael Vinker, Ariel Shamir, Daniel Cohen-Or. (2024)<br><strong>Implicit Style-Content Separation using B-LoRA</strong><br><button class=copy-to-clipboard title="Implicit Style-Content Separation using B-LoRA" index=155>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-155 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Fine-tuning, Style Transfer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14572v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14572v1.pdf filename=2403.14572v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Image stylization involves manipulating the visual appearance and texture <b>(style)</b> <b>of</b> an image while preserving its underlying objects, structures, and concepts (content). The separation of <b>style</b> <b>and</b> content is essential for manipulating the image&rsquo;s <b>style</b> <b>independently</b> from its content, ensuring a harmonious and visually pleasing result. Achieving this separation requires a deep understanding of both the visual and semantic characteristics of images, often necessitating the training of specialized models or employing heavy optimization. In this paper, we introduce B-LoRA, a method that leverages LoRA (Low-Rank Adaptation) to implicitly separate the <b>style</b> <b>and</b> content components of a single image, facilitating various image stylization tasks. By analyzing the architecture of SDXL combined with LoRA, we find that jointly learning the LoRA weights of two specific blocks (referred to as B-LoRAs) achieves <b>style-content</b> <b>separation</b> that cannot be achieved by training each B-LoRA independently. Consolidating the training into only two blocks and separating <b>style</b> <b>and</b> content allows for significantly improving <b>style</b> <b>manipulation</b> and overcoming overfitting issues often associated with model <b>fine-tuning.</b> Once trained, the two B-LoRAs can be used as independent components to allow various image stylization tasks, including image <b>style</b> <b>transfer,</b> text-based image stylization, consistent <b>style</b> <b>generation,</b> and <b>style-content</b> <b>mixing.</b></p></p class="citation"></blockquote><h3 id=64104--156312-designedit-multi-layered-latent-decomposition-and-fusion-for-unified--accurate-image-editing-yueru-jia-et-al-2024>(64/104 | 156/312) DesignEdit: Multi-Layered Latent Decomposition and Fusion for Unified & Accurate Image Editing (Yueru Jia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yueru Jia, Yuhui Yuan, Aosong Cheng, Chuke Wang, Ji Li, Huizhu Jia, Shanghang Zhang. (2024)<br><strong>DesignEdit: Multi-Layered Latent Decomposition and Fusion for Unified & Accurate Image Editing</strong><br><button class=copy-to-clipboard title="DesignEdit: Multi-Layered Latent Decomposition and Fusion for Unified & Accurate Image Editing" index=156>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-156 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Text2image, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14487v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14487v1.pdf filename=2403.14487v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, how to achieve precise image editing has attracted increasing attention, especially given the remarkable success of <b>text-to-image</b> generation models. To unify various spatial-aware image editing abilities into one framework, we adopt the concept of layers from the design domain to manipulate objects flexibly with various operations. The key insight is to transform the spatial-aware image editing task into a combination of two sub-tasks: multi-layered latent decomposition and multi-layered latent fusion. First, we segment the latent representations of the source images into multiple layers, which include several object layers and one incomplete background layer that necessitates reliable inpainting. To avoid extra tuning, we further explore the inner inpainting ability within the <b>self-attention</b> mechanism. We introduce a key-masking <b>self-attention</b> scheme that can propagate the surrounding context information into the masked region while mitigating its impact on the regions outside the mask. Second, we propose an instruction-guided latent fusion that pastes the multi-layered latent representations onto a canvas latent. We also introduce an artifact suppression scheme in the latent space to enhance the inpainting quality. Due to the inherent modular advantages of such multi-layered representations, we can achieve accurate image editing, and we demonstrate that our approach consistently surpasses the latest spatial editing methods, including Self-Guidance and DiffEditor. Last, we show that our approach is a unified framework that supports various accurate image editing tasks on more than six different editing tasks.</p></p class="citation"></blockquote><h3 id=65104--157312-anyv2v-a-plug-and-play-framework-for-any-video-to-video-editing-tasks-max-ku-et-al-2024>(65/104 | 157/312) AnyV2V: A Plug-and-Play Framework For Any Video-to-Video Editing Tasks (Max Ku et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Max Ku, Cong Wei, Weiming Ren, Harry Yang, Wenhu Chen. (2024)<br><strong>AnyV2V: A Plug-and-Play Framework For Any Video-to-Video Editing Tasks</strong><br><button class=copy-to-clipboard title="AnyV2V: A Plug-and-Play Framework For Any Video-to-Video Editing Tasks" index=157>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-157 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-MM, cs.CV<br>Keyword Score: 20<br>Keywords: Style Transfer, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14468v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14468v2.pdf filename=2403.14468v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Video-to-video editing involves editing a source video along with additional control (such as text <b>prompts,</b> subjects, or <b>styles)</b> <b>to</b> generate a new video that aligns with the source video and the provided control. Traditional methods have been constrained to certain editing types, limiting their ability to meet the wide range of user demands. In this paper, we introduce AnyV2V, a novel training-free framework designed to simplify video editing into two primary steps: (1) employing an off-the-shelf image editing model (e.g. InstructPix2Pix, InstantID, etc) to modify the first frame, (2) utilizing an existing image-to-video generation model (e.g. I2VGen-XL) for DDIM inversion and feature injection. In the first stage, AnyV2V can plug in any existing image editing tools to support an extensive array of video editing tasks. Beyond the traditional <b>prompt-based</b> editing methods, AnyV2V also can support novel video editing tasks, including reference-based <b>style</b> <b>transfer,</b> subject-driven editing, and identity manipulation, which were unattainable by previous methods. In the second stage, AnyV2V can plug in any existing image-to-video models to perform DDIM inversion and intermediate feature injection to maintain the appearance and motion consistency with the source video. On the <b>prompt-based</b> editing, we show that AnyV2V can outperform the previous best approach by 35% on <b>prompt</b> alignment, and 25% on human preference. On the three novel tasks, we show that AnyV2V also achieves a high success rate. We believe AnyV2V will continue to thrive due to its ability to seamlessly integrate the fast-evolving image editing methods. Such compatibility can help AnyV2V to increase its versatility to cater to diverse user demands.</p></p class="citation"></blockquote><h3 id=66104--158312-style-extracting-diffusion-models-for-semi-supervised-histopathology-segmentation-mathias-öttl-et-al-2024>(66/104 | 158/312) Style-Extracting Diffusion Models for Semi-Supervised Histopathology Segmentation (Mathias Öttl et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mathias Öttl, Frauke Wilm, Jana Steenpass, Jingna Qiu, Matthias Rübner, Arndt Hartmann, Matthias Beckmann, Peter Fasching, Andreas Maier, Ramona Erber, Bernhard Kainz, Katharina Breininger. (2024)<br><strong>Style-Extracting Diffusion Models for Semi-Supervised Histopathology Segmentation</strong><br><button class=copy-to-clipboard title="Style-Extracting Diffusion Models for Semi-Supervised Histopathology Segmentation" index=158>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-158 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14429v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14429v1.pdf filename=2403.14429v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep learning-based image generation has seen significant advancements with <b>diffusion</b> <b>models,</b> notably improving the quality of generated images. Despite these developments, generating images with unseen characteristics beneficial for downstream tasks has received limited attention. To bridge this gap, we propose Style-Extracting <b>Diffusion</b> <b>Models,</b> featuring two conditioning mechanisms. Specifically, we utilize 1) a style conditioning mechanism which allows to inject style information of previously unseen images during image generation and 2) a content conditioning which can be targeted to a downstream task, e.g., layout for segmentation. We introduce a trainable style encoder to extract style information from images, and an aggregation block that merges style information from multiple style inputs. This architecture enables the generation of images with unseen styles in a <b>zero-shot</b> manner, by leveraging styles from unseen images, resulting in more diverse generations. In this work, we use the image layout as target condition and first show the capability of our method on a natural image dataset as a proof-of-concept. We further demonstrate its versatility in histopathology, where we combine prior knowledge about tissue composition and unannotated data to create diverse synthetic images with known layouts. This allows us to generate additional synthetic data to train a segmentation network in a semi-supervised fashion. We verify the added value of the generated images by showing improved segmentation results and lower performance variability between patients when synthetic images are included during segmentation training. Our code will be made publicly available at [LINK].</p></p class="citation"></blockquote><h3 id=67104--159312-enabling-visual-composition-and-animation-in-unsupervised-video-generation-aram-davtyan-et-al-2024>(67/104 | 159/312) Enabling Visual Composition and Animation in Unsupervised Video Generation (Aram Davtyan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aram Davtyan, Sepehr Sameni, Björn Ommer, Paolo Favaro. (2024)<br><strong>Enabling Visual Composition and Animation in Unsupervised Video Generation</strong><br><button class=copy-to-clipboard title="Enabling Visual Composition and Animation in Unsupervised Video Generation" index=159>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-159 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Self-supervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14368v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14368v1.pdf filename=2403.14368v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work we propose a novel method for <b>unsupervised</b> controllable video generation. Once trained on a dataset of unannotated videos, at inference our model is capable of both composing scenes of predefined object parts and animating them in a plausible and controlled way. This is achieved by conditioning video generation on a randomly selected subset of local pre-trained <b>self-supervised</b> features during training. We call our model CAGE for visual Composition and Animation for video GEneration. We conduct a series of experiments to demonstrate capabilities of CAGE in various settings. Project website: <a href=https://araachie.github.io/cage>https://araachie.github.io/cage</a>.</p></p class="citation"></blockquote><h3 id=68104--160312-surroundsdf-implicit-3d-scene-understanding-based-on-signed-distance-field-lizhe-liu-et-al-2024>(68/104 | 160/312) SurroundSDF: Implicit 3D Scene Understanding Based on Signed Distance Field (Lizhe Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lizhe Liu, Bohua Wang, Hongwei Xie, Daqi Liu, Li Liu, Zhiqiang Tian, Kuiyuan Yang, Bing Wang. (2024)<br><strong>SurroundSDF: Implicit 3D Scene Understanding Based on Signed Distance Field</strong><br><button class=copy-to-clipboard title="SurroundSDF: Implicit 3D Scene Understanding Based on Signed Distance Field" index=160>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-160 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Supervised Learning, Weakly-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14366v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14366v1.pdf filename=2403.14366v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Vision-centric 3D environment understanding is both vital and challenging for autonomous driving systems. Recently, object-free methods have attracted considerable attention. Such methods perceive the world by predicting the semantics of discrete voxel grids but fail to construct continuous and accurate obstacle surfaces. To this end, in this paper, we propose SurroundSDF to implicitly predict the signed distance field (SDF) and semantic field for the continuous perception from surround images. Specifically, we introduce a query-based approach and utilize SDF constrained by the Eikonal formulation to accurately describe the surfaces of obstacles. Furthermore, considering the absence of precise SDF ground truth, we propose a novel weakly <b>supervised</b> paradigm for SDF, referred to as the Sandwich Eikonal formulation, which emphasizes applying correct and dense constraints on both sides of the surface, thereby enhancing the perceptual accuracy of the surface. Experiments suggest that our method achieves SOTA for both occupancy prediction and 3D scene reconstruction tasks on the nuScenes dataset.</p></p class="citation"></blockquote><h3 id=69104--161312-varroa-destructor-detection-on-honey-bees-using-hyperspectral-imagery-zina-sabrina-duma-et-al-2024>(69/104 | 161/312) Varroa destructor detection on honey bees using hyperspectral imagery (Zina-Sabrina Duma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zina-Sabrina Duma, Tomas Zemcik, Simon Bilik, Tuomas Sihvonen, Peter Honec, Satu-Pia Reinikainen, Karel Horak. (2024)<br><strong>Varroa destructor detection on honey bees using hyperspectral imagery</strong><br><button class=copy-to-clipboard title="Varroa destructor detection on honey bees using hyperspectral imagery" index=161>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-161 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 20<br>Keywords: Supervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14359v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14359v1.pdf filename=2403.14359v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Hyperspectral (HS) imagery in agriculture is becoming increasingly common. These images have the advantage of higher spectral resolution. Advanced spectral processing techniques are required to unlock the information potential in these HS images. The present paper introduces a method rooted in multivariate statistics designed to detect parasitic Varroa destructor mites on the body of western honey bee Apis mellifera, enabling easier and continuous monitoring of the bee hives. The methodology explores <b>unsupervised</b> (K-means++) and recently developed <b>supervised</b> (Kernel Flows - Partial Least-Squares, KF-PLS) methods for parasitic identification. Additionally, in light of the emergence of custom-band multispectral cameras, the present research outlines a strategy for identifying the specific wavelengths necessary for effective bee-mite separation, suitable for implementation in a custom-band camera. Illustrated with a real-case dataset, our findings demonstrate that as few as four spectral bands are sufficient for accurate parasite identification.</p></p class="citation"></blockquote><h3 id=70104--162312-zero123-6d-zero-shot-novel-view-synthesis-for-rgb-category-level-6d-pose-estimation-francesco-di-felice-et-al-2024>(70/104 | 162/312) Zero123-6D: Zero-shot Novel View Synthesis for RGB Category-level 6D Pose Estimation (Francesco Di Felice et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Francesco Di Felice, Alberto Remus, Stefano Gasperini, Benjamin Busam, Lionel Ott, Federico Tombari, Roland Siegwart, Carlo Alberto Avizzano. (2024)<br><strong>Zero123-6D: Zero-shot Novel View Synthesis for RGB Category-level 6D Pose Estimation</strong><br><button class=copy-to-clipboard title="Zero123-6D: Zero-shot Novel View Synthesis for RGB Category-level 6D Pose Estimation" index=162>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-162 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14279v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14279v1.pdf filename=2403.14279v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Estimating the pose of objects through vision is essential to make robotic platforms interact with the environment. Yet, it presents many challenges, often related to the lack of flexibility and generalizability of state-of-the-art solutions. <b>Diffusion</b> <b>models</b> are a cutting-edge neural architecture transforming 2D and 3D computer vision, outlining remarkable performances in <b>zero-shot</b> novel-view synthesis. Such a use case is particularly intriguing for reconstructing 3D objects. However, localizing objects in unstructured environments is rather unexplored. To this end, this work presents Zero123-6D to demonstrate the utility of <b>Diffusion</b> <b>Model-based</b> novel-view-synthesizers in enhancing RGB 6D pose estimation at category-level by integrating them with feature extraction techniques. The outlined method exploits such a novel view synthesizer to expand a sparse set of RGB-only reference views for the <b>zero-shot</b> 6D pose estimation task. Experiments are quantitatively analyzed on the CO3D dataset, showcasing increased performance over baselines, a substantial reduction in data requirements, and the removal of the necessity of depth information.</p></p class="citation"></blockquote><h3 id=71104--163312-on-the-concept-trustworthiness-in-concept-bottleneck-models-qihan-huang-et-al-2024>(71/104 | 163/312) On the Concept Trustworthiness in Concept Bottleneck Models (Qihan Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qihan Huang, Jie Song, Jingwen Hu, Haofei Zhang, Yong Wang, Mingli Song. (2024)<br><strong>On the Concept Trustworthiness in Concept Bottleneck Models</strong><br><button class=copy-to-clipboard title="On the Concept Trustworthiness in Concept Bottleneck Models" index=163>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-163 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 18<br>Keywords: Benchmarking, Black Box, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14349v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14349v1.pdf filename=2403.14349v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Concept Bottleneck Models (CBMs), which break down the <b>reasoning</b> process into the input-to-concept mapping and the concept-to-label prediction, have garnered significant attention due to their remarkable interpretability achieved by the interpretable concept bottleneck. However, despite the transparency of the concept-to-label prediction, the mapping from the input to the intermediate concept remains a <b>black</b> <b>box,</b> giving rise to concerns about the trustworthiness of the learned concepts (i.e., these concepts may be predicted based on spurious cues). The issue of concept untrustworthiness greatly hampers the interpretability of CBMs, thereby hindering their further advancement. To conduct a comprehensive analysis on this issue, in this study we establish a <b>benchmark</b> to assess the trustworthiness of concepts in CBMs. A pioneering metric, referred to as concept trustworthiness score, is proposed to gauge whether the concepts are derived from relevant regions. Additionally, an enhanced CBM is introduced, enabling concept predictions to be made specifically from distinct parts of the feature map, thereby facilitating the exploration of their related regions. Besides, we introduce three modules, namely the cross-layer alignment (CLA) module, the cross-image alignment (CIA) module, and the prediction alignment (PA) module, to further enhance the concept trustworthiness within the elaborated CBM. The experiments on five datasets across ten architectures demonstrate that without using any concept localization annotations during training, our model improves the concept trustworthiness by a large margin, meanwhile achieving superior accuracy to the state-of-the-arts. Our code is available at <a href=https://github.com/hqhQAQ/ProtoCBM>https://github.com/hqhQAQ/ProtoCBM</a>.</p></p class="citation"></blockquote><h3 id=72104--164312-volumetric-environment-representation-for-vision-language-navigation-rui-liu-et-al-2024>(72/104 | 164/312) Volumetric Environment Representation for Vision-Language Navigation (Rui Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rui Liu, Wenguan Wang, Yi Yang. (2024)<br><strong>Volumetric Environment Representation for Vision-Language Navigation</strong><br><button class=copy-to-clipboard title="Volumetric Environment Representation for Vision-Language Navigation" index=164>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-164 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 18<br>Keywords: Benchmarking, Geometry, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14158v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14158v1.pdf filename=2403.14158v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Vision-language</b> navigation (VLN) requires an agent to navigate through an 3D environment based on visual observations and natural language instructions. It is clear that the pivotal factor for successful navigation lies in the comprehensive scene understanding. Previous VLN agents employ monocular frameworks to extract 2D features of perspective views directly. Though straightforward, they struggle for capturing 3D <b>geometry</b> and semantics, leading to a partial and incomplete environment representation. To achieve a comprehensive 3D representation with fine-grained details, we introduce a Volumetric Environment Representation (VER), which voxelizes the physical world into structured 3D cells. For each cell, VER aggregates multi-view 2D features into such a unified 3D space via 2D-3D sampling. Through coarse-to-fine feature extraction and multi-task learning for VER, our agent predicts 3D occupancy, 3D room layout, and 3D bounding boxes jointly. Based on online collected VERs, our agent performs volume state estimation and builds episodic memory for predicting the next step. Experimental results show our environment representations from multi-task learning lead to evident performance gains on VLN. Our model achieves state-of-the-art performance across VLN <b>benchmarks</b> (R2R, REVERIE, and R4R).</p></p class="citation"></blockquote><h3 id=73104--165312-learning-decomposable-and-debiased-representations-via-attribute-centric-information-bottlenecks-jinyung-hong-et-al-2024>(73/104 | 165/312) Learning Decomposable and Debiased Representations via Attribute-Centric Information Bottlenecks (Jinyung Hong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinyung Hong, Eun Som Jeon, Changhoon Kim, Keun Hee Park, Utkarsh Nath, Yezhou Yang, Pavan Turaga, Theodore P. Pavlic. (2024)<br><strong>Learning Decomposable and Debiased Representations via Attribute-Centric Information Bottlenecks</strong><br><button class=copy-to-clipboard title="Learning Decomposable and Debiased Representations via Attribute-Centric Information Bottlenecks" index=165>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-165 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 15<br>Keywords: Out-of-distribution, Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14140v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14140v1.pdf filename=2403.14140v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Biased attributes, spuriously correlated with target labels in a dataset, can problematically lead to neural networks that learn improper shortcuts for classifications and limit their capabilities for <b>out-of-distribution</b> (OOD) generalization. Although many debiasing approaches have been proposed to ensure correct predictions from biased datasets, few studies have considered learning latent embedding consisting of intrinsic and biased attributes that contribute to improved performance and explain how the model pays attention to attributes. In this paper, we propose a novel debiasing framework, Debiasing Global Workspace, introducing attention-based information bottlenecks for learning compositional <b>representations</b> <b>of</b> attributes without defining specific bias types. Based on our observation that learning shape-centric <b>representation</b> <b>helps</b> robust performance on OOD datasets, we adopt those abilities to learn robust and generalizable <b>representations</b> <b>of</b> decomposable latent embeddings corresponding to intrinsic and biasing attributes. We conduct comprehensive evaluations on biased datasets, along with both quantitative and qualitative analyses, to showcase our approach&rsquo;s efficacy in attribute-centric <b>representation</b> <b>learning</b> and its ability to differentiate between intrinsic and bias-related features.</p></p class="citation"></blockquote><h3 id=74104--166312-vxp-voxel-cross-pixel-large-scale-image-lidar-place-recognition-yun-jin-li-et-al-2024>(74/104 | 166/312) VXP: Voxel-Cross-Pixel Large-scale Image-LiDAR Place Recognition (Yun-Jin Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yun-Jin Li, Mariia Gladkova, Yan Xia, Rui Wang, Daniel Cremers. (2024)<br><strong>VXP: Voxel-Cross-Pixel Large-scale Image-LiDAR Place Recognition</strong><br><button class=copy-to-clipboard title="VXP: Voxel-Cross-Pixel Large-scale Image-LiDAR Place Recognition" index=166>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-166 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14594v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14594v1.pdf filename=2403.14594v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent works on the global place recognition treat the task as a retrieval problem, where an off-the-shelf global descriptor is commonly designed in image-based and LiDAR-based modalities. However, it is non-trivial to perform accurate image-LiDAR global place recognition since extracting consistent and robust global descriptors from different domains (2D images and 3D point clouds) is challenging. To address this issue, we propose a novel Voxel-Cross-Pixel (VXP) approach, which establishes voxel and pixel correspondences in a <b>self-supervised</b> manner and brings them into a shared feature space. Specifically, VXP is trained in a two-stage manner that first explicitly exploits local feature correspondences and enforces similarity of global descriptors. Extensive experiments on the three <b>benchmarks</b> (Oxford RobotCar, ViViD++ and KITTI) demonstrate our method surpasses the state-of-the-art cross-modal retrieval by a large margin.</p></p class="citation"></blockquote><h3 id=75104--167312-mulde-multiscale-log-density-estimation-via-denoising-score-matching-for-video-anomaly-detection-jakub-micorek-et-al-2024>(75/104 | 167/312) MULDE: Multiscale Log-Density Estimation via Denoising Score Matching for Video Anomaly Detection (Jakub Micorek et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jakub Micorek, Horst Possegger, Dominik Narnhofer, Horst Bischof, Mateusz Kozinski. (2024)<br><strong>MULDE: Multiscale Log-Density Estimation via Denoising Score Matching for Video Anomaly Detection</strong><br><button class=copy-to-clipboard title="MULDE: Multiscale Log-Density Estimation via Denoising Score Matching for Video Anomaly Detection" index=167>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-167 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Anomaly Detection, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14497v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14497v1.pdf filename=2403.14497v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose a novel approach to video <b>anomaly</b> <b>detection:</b> we treat feature vectors extracted from videos as realizations of a random variable with a fixed distribution and model this distribution with a neural network. This lets us estimate the likelihood of test videos and detect video anomalies by thresholding the likelihood estimates. We train our video <b>anomaly</b> <b>detector</b> using a modification of denoising score matching, a method that injects training data with noise to facilitate modeling its distribution. To eliminate hyperparameter selection, we model the distribution of noisy video features across a range of noise levels and introduce a regularizer that tends to align the models for different levels of noise. At test time, we combine <b>anomaly</b> <b>indications</b> at multiple noise scales with a Gaussian mixture model. Running our video <b>anomaly</b> <b>detector</b> induces minimal delays as inference requires merely extracting the features and forward-propagating them through a shallow neural network and a Gaussian mixture model. Our experiments on five popular video <b>anomaly</b> <b>detection</b> <b>benchmarks</b> demonstrate state-of-the-art performance, both in the object-centric and in the frame-centric setup.</p></p class="citation"></blockquote><h3 id=76104--168312-keypoint-relative-position-encoding-for-face-recognition-minchul-kim-et-al-2024>(76/104 | 168/312) KeyPoint Relative Position Encoding for Face Recognition (Minchul Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Minchul Kim, Yiyang Su, Feng Liu, Anil Jain, Xiaoming Liu. (2024)<br><strong>KeyPoint Relative Position Encoding for Face Recognition</strong><br><button class=copy-to-clipboard title="KeyPoint Relative Position Encoding for Face Recognition" index=168>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-168 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Face Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14852v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14852v1.pdf filename=2403.14852v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we address the challenge of making ViT models more robust to unseen affine transformations. Such robustness becomes useful in various recognition tasks such as <b>face</b> <b>recognition</b> when image alignment failures occur. We propose a novel method called KP-RPE, which leverages key points (e.g.~facial landmarks) to make ViT more resilient to scale, translation, and pose variations. We begin with the observation that Relative Position Encoding (RPE) is a good way to bring affine transform generalization to ViTs. RPE, however, can only inject the model with prior knowledge that nearby pixels are more important than far pixels. Keypoint RPE (KP-RPE) is an extension of this principle, where the significance of pixels is not solely dictated by their proximity but also by their relative positions to specific keypoints within the image. By anchoring the significance of pixels around keypoints, the model can more effectively retain spatial relationships, even when those relationships are disrupted by affine transformations. We show the merit of KP-RPE in <b>face</b> <b>and</b> gait recognition. The experimental results demonstrate the effectiveness in improving <b>face</b> <b>recognition</b> performance from low-quality images, particularly where alignment is prone to failure. Code and pre-trained models are available.</p></p class="citation"></blockquote><h3 id=77104--169312-osmosis-rgbd-diffusion-prior-for-underwater-image-restoration-opher-bar-nathan-et-al-2024>(77/104 | 169/312) Osmosis: RGBD Diffusion Prior for Underwater Image Restoration (Opher Bar Nathan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Opher Bar Nathan, Deborah Levy, Tali Treibitz, Dan Rosenbaum. (2024)<br><strong>Osmosis: RGBD Diffusion Prior for Underwater Image Restoration</strong><br><button class=copy-to-clipboard title="Osmosis: RGBD Diffusion Prior for Underwater Image Restoration" index=169>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-169 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14837v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14837v1.pdf filename=2403.14837v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Underwater image restoration is a challenging task because of strong water effects that increase dramatically with distance. This is worsened by lack of ground truth data of clean scenes without water. <b>Diffusion</b> <b>priors</b> have emerged as strong image restoration priors. However, they are often trained with a dataset of the desired restored output, which is not available in our case. To overcome this critical issue, we show how to leverage in-air images to train <b>diffusion</b> <b>priors</b> for underwater restoration. We also observe that only color data is insufficient, and augment the prior with a depth channel. We train an unconditional <b>diffusion</b> <b>model</b> prior on the joint space of color and depth, using standard RGBD datasets of natural outdoor scenes in air. Using this prior together with a novel guidance method based on the underwater image formation model, we generate posterior samples of clean images, removing the water effects. Even though our prior did not see any underwater images during training, our method outperforms state-of-the-art baselines for image restoration on very challenging scenes. Data, models and code are published in the project page.</p></p class="citation"></blockquote><h3 id=78104--170312-on-the-detection-of-anomalous-or-out-of-distribution-data-in-vision-models-using-statistical-techniques-laura-omahony-et-al-2024>(78/104 | 170/312) On the Detection of Anomalous or Out-Of-Distribution Data in Vision Models Using Statistical Techniques (Laura O&rsquo;Mahony et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Laura O&rsquo;Mahony, David JP O&rsquo;Sullivan, Nikola S. Nikolov. (2024)<br><strong>On the Detection of Anomalous or Out-Of-Distribution Data in Vision Models Using Statistical Techniques</strong><br><button class=copy-to-clipboard title="On the Detection of Anomalous or Out-Of-Distribution Data in Vision Models Using Statistical Techniques" index=170>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-170 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 10<br>Keywords: Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15497v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15497v1.pdf filename=2403.15497v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Out-of-distribution</b> data and anomalous inputs are vulnerabilities of machine learning systems today, often causing systems to make incorrect predictions. The diverse range of data on which these models are used makes detecting atypical inputs a difficult and important task. We assess a tool, Benford&rsquo;s law, as a method used to quantify the difference between real and corrupted inputs. We believe that in many settings, it could function as a filter for anomalous data points and for signalling <b>out-of-distribution</b> data. We hope to open a discussion on these applications and further areas where this technique is underexplored.</p></p class="citation"></blockquote><h3 id=79104--171312-streamingt2v-consistent-dynamic-and-extendable-long-video-generation-from-text-roberto-henschel-et-al-2024>(79/104 | 171/312) StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation from Text (Roberto Henschel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Roberto Henschel, Levon Khachatryan, Daniil Hayrapetyan, Hayk Poghosyan, Vahram Tadevosyan, Zhangyang Wang, Shant Navasardyan, Humphrey Shi. (2024)<br><strong>StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation from Text</strong><br><button class=copy-to-clipboard title="StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation from Text" index=171>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-171 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CL, cs-CV, cs-LG, cs-MM, cs.CV, eess-IV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14773v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14773v1.pdf filename=2403.14773v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Text-to-video <b>diffusion</b> <b>models</b> enable the generation of high-quality videos that follow text instructions, making it easy to create diverse and individual content. However, existing approaches mostly focus on high-quality short video generation (typically 16 or 24 frames), ending up with hard-cuts when naively extended to the case of long video synthesis. To overcome these limitations, we introduce StreamingT2V, an autoregressive approach for long video generation of 80, 240, 600, 1200 or more frames with smooth transitions. The key components are:(i) a short-term memory block called conditional attention module (CAM), which conditions the current generation on the features extracted from the previous chunk via an attentional mechanism, leading to consistent chunk transitions, (ii) a long-term memory block called appearance preservation module, which extracts high-level scene and object features from the first video chunk to prevent the model from forgetting the initial scene, and (iii) a randomized blending approach that enables to apply a video enhancer autoregressively for infinitely long videos without inconsistencies between chunks. Experiments show that StreamingT2V generates high motion amount. In contrast, all competing image-to-video methods are prone to video stagnation when applied naively in an autoregressive manner. Thus, we propose with StreamingT2V a high-quality seamless text-to-long video generator that outperforms competitors with consistency and motion. Our code will be available at: <a href=https://github.com/Picsart-AI-Research/StreamingT2V>https://github.com/Picsart-AI-Research/StreamingT2V</a></p></p class="citation"></blockquote><h3 id=80104--172312-lift-a-surprisingly-simple-lightweight-feature-transform-for-dense-vit-descriptors-saksham-suri-et-al-2024>(80/104 | 172/312) LiFT: A Surprisingly Simple Lightweight Feature Transform for Dense ViT Descriptors (Saksham Suri et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Saksham Suri, Matthew Walmer, Kamal Gupta, Abhinav Shrivastava. (2024)<br><strong>LiFT: A Surprisingly Simple Lightweight Feature Transform for Dense ViT Descriptors</strong><br><button class=copy-to-clipboard title="LiFT: A Surprisingly Simple Lightweight Feature Transform for Dense ViT Descriptors" index=172>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-172 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14625v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14625v1.pdf filename=2403.14625v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a simple <b>self-supervised</b> method to enhance the performance of ViT features for dense downstream tasks. Our Lightweight Feature Transform (LiFT) is a straightforward and compact postprocessing network that can be applied to enhance the features of any pre-trained ViT backbone. LiFT is fast and easy to train with a <b>self-supervised</b> objective, and it boosts the density of ViT features for minimal extra inference cost. Furthermore, we demonstrate that LiFT can be applied with approaches that use additional task-specific downstream modules, as we integrate LiFT with ViTDet for COCO detection and segmentation. Despite the simplicity of LiFT, we find that it is not simply learning a more complex version of bilinear interpolation. Instead, our LiFT training protocol leads to several desirable emergent properties that benefit ViT features in dense downstream tasks. This includes greater scale invariance for features, and better object boundary maps. By simply training LiFT for a few epochs, we show improved performance on keypoint correspondence, detection, segmentation, and object discovery tasks. Overall, LiFT provides an easy way to unlock the benefits of denser feature arrays for a fraction of the computational cost. For more details, refer to our project page at <a href=https://www.cs.umd.edu/~sakshams/LiFT/>https://www.cs.umd.edu/~sakshams/LiFT/</a>.</p></p class="citation"></blockquote><h3 id=81104--173312-explorative-inbetweening-of-time-and-space-haiwen-feng-et-al-2024>(81/104 | 173/312) Explorative Inbetweening of Time and Space (Haiwen Feng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haiwen Feng, Zheng Ding, Zhihao Xia, Simon Niklaus, Victoria Abrevaya, Michael J. Black, Xuaner Zhang. (2024)<br><strong>Explorative Inbetweening of Time and Space</strong><br><button class=copy-to-clipboard title="Explorative Inbetweening of Time and Space" index=173>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-173 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14611v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14611v1.pdf filename=2403.14611v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce bounded generation as a generalized task to control video generation to synthesize arbitrary camera and subject motion based only on a given start and end frame. Our objective is to fully leverage the inherent generalization capability of an image-to-video model without additional training or <b>fine-tuning</b> of the original model. This is achieved through the proposed new sampling strategy, which we call Time Reversal Fusion, that fuses the temporally forward and backward denoising paths conditioned on the start and end frame, respectively. The fused path results in a video that smoothly connects the two frames, generating inbetweening of faithful subject motion, novel views of static scenes, and seamless video looping when the two bounding frames are identical. We curate a diverse evaluation dataset of image pairs and compare against the closest existing methods. We find that Time Reversal Fusion outperforms related work on all subtasks, exhibiting the ability to generate complex motions and 3D-consistent views guided by bounded frames. See project page at <a href=https://time-reversal.github.io>https://time-reversal.github.io</a>.</p></p class="citation"></blockquote><h3 id=82104--174312-renoise-real-image-inversion-through-iterative-noising-daniel-garibi-et-al-2024>(82/104 | 174/312) ReNoise: Real Image Inversion Through Iterative Noising (Daniel Garibi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daniel Garibi, Or Patashnik, Andrey Voynov, Hadar Averbuch-Elor, Daniel Cohen-Or. (2024)<br><strong>ReNoise: Real Image Inversion Through Iterative Noising</strong><br><button class=copy-to-clipboard title="ReNoise: Real Image Inversion Through Iterative Noising" index=174>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-174 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-GR, cs-LG, cs.CV, eess-IV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14602v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14602v1.pdf filename=2403.14602v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in text-guided <b>diffusion</b> <b>models</b> have unlocked powerful image manipulation capabilities. However, applying these methods to real images necessitates the inversion of the images into the domain of the pretrained <b>diffusion</b> <b>model.</b> Achieving faithful inversion remains a challenge, particularly for more recent models trained to generate images with a small number of denoising steps. In this work, we introduce an inversion method with a high quality-to-operation ratio, enhancing reconstruction accuracy without increasing the number of operations. Building on reversing the <b>diffusion</b> <b>sampling</b> process, our method employs an iterative renoising mechanism at each inversion sampling step. This mechanism refines the approximation of a predicted point along the forward <b>diffusion</b> <b>trajectory,</b> by iteratively applying the pretrained <b>diffusion</b> <b>model,</b> and averaging these predictions. We evaluate the performance of our ReNoise technique using various sampling algorithms and models, including recent accelerated <b>diffusion</b> <b>models.</b> Through comprehensive evaluations and comparisons, we show its effectiveness in terms of both accuracy and speed. Furthermore, we confirm that our method preserves editability by demonstrating text-driven image editing on real images.</p></p class="citation"></blockquote><h3 id=83104--175312-view-decoupled-transformer-for-person-re-identification-under-aerial-ground-camera-network-quan-zhang-et-al-2024>(83/104 | 175/312) View-decoupled Transformer for Person Re-identification under Aerial-ground Camera Network (Quan Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Quan Zhang, Lei Wang, Vishal M. Patel, Xiaohua Xie, Jianhuang Lai. (2024)<br><strong>View-decoupled Transformer for Person Re-identification under Aerial-ground Camera Network</strong><br><button class=copy-to-clipboard title="View-decoupled Transformer for Person Re-identification under Aerial-ground Camera Network" index=175>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-175 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14513v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14513v1.pdf filename=2403.14513v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing person re-identification methods have achieved remarkable advances in appearance-based identity association across homogeneous cameras, such as ground-ground matching. However, as a more practical scenario, aerial-ground person re-identification (AGPReID) among heterogeneous cameras has received minimal attention. To alleviate the disruption of discriminative identity representation by dramatic view discrepancy as the most significant challenge in AGPReID, the view-decoupled <b>transformer</b> (VDT) is proposed as a simple yet effective framework. Two major components are designed in VDT to decouple view-related and view-unrelated features, namely hierarchical subtractive separation and orthogonal loss, where the former separates these two features inside the VDT, and the latter constrains these two to be independent. In addition, we contribute a large-scale AGPReID dataset called CARGO, consisting of five/eight aerial/ground cameras, 5,000 identities, and 108,563 images. Experiments on two datasets show that VDT is a feasible and effective solution for AGPReID, surpassing the previous method on mAP/Rank1 by up to 5.0%/2.7% on CARGO and 3.7%/5.2% on AG-ReID, keeping the same magnitude of computational complexity. Our project is available at <a href=https://github.com/LinlyAC/VDT-AGPReID>https://github.com/LinlyAC/VDT-AGPReID</a></p></p class="citation"></blockquote><h3 id=84104--176312-raw-instinct-trust-your-classifiers-and-skip-the-conversion-christos-kantas-et-al-2024>(84/104 | 176/312) Raw Instinct: Trust Your Classifiers and Skip the Conversion (Christos Kantas et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Christos Kantas, Bjørk Antoniussen, Mathias V. Andersen, Rasmus Munksø, Shobhit Kotnala, Simon B. Jensen, Andreas Møgelmose, Lau Nørgaard, Thomas B. Moeslund. (2024)<br><strong>Raw Instinct: Trust Your Classifiers and Skip the Conversion</strong><br><button class=copy-to-clipboard title="Raw Instinct: Trust Your Classifiers and Skip the Conversion" index=176>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-176 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14439v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14439v1.pdf filename=2403.14439v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Using RAW-images in computer vision problems is surprisingly underexplored considering that converting from RAW to RGB does not introduce any new capture information. In this paper, we show that a sufficiently advanced classifier can yield equivalent results on RAW input compared to RGB and present a new public dataset consisting of RAW images and the corresponding converted RGB images. Classifying images directly from RAW is attractive, as it allows for skipping the conversion to RGB, lowering computation time significantly. Two <b>CNN</b> classifiers are used to classify the images in both formats, confirming that classification performance can indeed be preserved. We furthermore show that the total computation time from RAW image data to classification results for RAW images can be up to 8.46 times faster than RGB. These results contribute to the evidence found in related works, that using RAW images as direct input to computer vision algorithms looks very promising.</p></p class="citation"></blockquote><h3 id=85104--177312-combinerf-a-combination-of-regularization-techniques-for-few-shot-neural-radiance-field-view-synthesis-matteo-bonotto-et-al-2024>(85/104 | 177/312) CombiNeRF: A Combination of Regularization Techniques for Few-Shot Neural Radiance Field View Synthesis (Matteo Bonotto et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Matteo Bonotto, Luigi Sarrocco, Daniele Evangelista, Marco Imperoli, Alberto Pretto. (2024)<br><strong>CombiNeRF: A Combination of Regularization Techniques for Few-Shot Neural Radiance Field View Synthesis</strong><br><button class=copy-to-clipboard title="CombiNeRF: A Combination of Regularization Techniques for Few-Shot Neural Radiance Field View Synthesis" index=177>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-177 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Few-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14412v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14412v1.pdf filename=2403.14412v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neural Radiance Fields (NeRFs) have shown impressive results for novel view synthesis when a sufficiently large amount of views are available. When dealing with <b>few-shot</b> settings, i.e. with a small set of input views, the training could overfit those views, leading to artifacts and geometric and chromatic inconsistencies in the resulting rendering. Regularization is a valid solution that helps NeRF generalization. On the other hand, each of the most recent NeRF regularization techniques aim to mitigate a specific rendering problem. Starting from this observation, in this paper we propose CombiNeRF, a framework that synergically combines several regularization techniques, some of them novel, in order to unify the benefits of each. In particular, we regularize single and neighboring rays distributions and we add a smoothness term to regularize near geometries. After these geometric approaches, we propose to exploit Lipschitz regularization to both NeRF density and color networks and to use encoding masks for input features regularization. We show that CombiNeRF outperforms the state-of-the-art methods with <b>few-shot</b> settings in several publicly available datasets. We also present an ablation study on the LLFF and NeRF-Synthetic datasets that support the choices made. We release with this paper the open-source implementation of our framework.</p></p class="citation"></blockquote><h3 id=86104--178312-ldtr-transformer-based-lane-detection-with-anchor-chain-representation-zhongyu-yang-et-al-2024>(86/104 | 178/312) LDTR: Transformer-based Lane Detection with Anchor-chain Representation (Zhongyu Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhongyu Yang, Chen Shen, Wei Shao, Tengfei Xing, Runbo Hu, Pengfei Xu, Hua Chai, Ruini Xue. (2024)<br><strong>LDTR: Transformer-based Lane Detection with Anchor-chain Representation</strong><br><button class=copy-to-clipboard title="LDTR: Transformer-based Lane Detection with Anchor-chain Representation" index=178>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-178 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14354v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14354v1.pdf filename=2403.14354v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite recent advances in lane detection methods, scenarios with limited- or no-visual-clue of lanes due to factors such as lighting conditions and occlusion remain challenging and crucial for automated driving. Moreover, current lane representations require complex post-processing and struggle with specific instances. Inspired by the DETR architecture, we propose LDTR, a <b>transformer-based</b> model to address these issues. Lanes are modeled with a novel anchor-chain, regarding a lane as a whole from the beginning, which enables LDTR to handle special lanes inherently. To enhance lane instance perception, LDTR incorporates a novel multi-referenced deformable attention module to distribute attention around the object. Additionally, LDTR incorporates two line IoU algorithms to improve convergence efficiency and employs a Gaussian heatmap auxiliary branch to enhance model representation capability during training. To evaluate lane detection models, we rely on Frechet distance, parameterized F1-score, and additional synthetic metrics. Experimental results demonstrate that LDTR achieves state-of-the-art performance on well-known datasets.</p></p class="citation"></blockquote><h3 id=87104--179312-towards-efficient-information-fusion-concentric-dual-fusion-attention-based-multiple-instance-learning-for-whole-slide-images-yujian-liu-et-al-2024>(87/104 | 179/312) Towards Efficient Information Fusion: Concentric Dual Fusion Attention Based Multiple Instance Learning for Whole Slide Images (Yujian Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yujian Liu, Ruoxuan Wu, Xinjie Shen, Zihuang Lu, Lingyu Liang, Haiyu Zhou, Shipu Xu, Shaoai Cai, Shidang Xu. (2024)<br><strong>Towards Efficient Information Fusion: Concentric Dual Fusion Attention Based Multiple Instance Learning for Whole Slide Images</strong><br><button class=copy-to-clipboard title="Towards Efficient Information Fusion: Concentric Dual Fusion Attention Based Multiple Instance Learning for Whole Slide Images" index=179>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-179 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Multiple Instance Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14346v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14346v1.pdf filename=2403.14346v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the realm of digital pathology, multi-magnification <b>Multiple</b> <b>Instance</b> <b>Learning</b> (multi-mag MIL) has proven effective in leveraging the hierarchical structure of Whole Slide Images (WSIs) to reduce information loss and redundant data. However, current methods fall short in bridging the domain gap between pretrained models and medical imaging, and often fail to account for spatial relationships across different magnifications. Addressing these challenges, we introduce the Concentric Dual Fusion Attention-MIL (CDFA-MIL) framework,which innovatively combines point-to-area feature-colum attention and point-to-point concentric-row attention using concentric patch. This approach is designed to effectively fuse correlated information, enhancing feature representation and providing stronger correlation guidance for WSI analysis. CDFA-MIL distinguishes itself by offering a robust fusion strategy that leads to superior WSI recognition. Its application has demonstrated exceptional performance, significantly surpassing existing MIL methods in accuracy and F1 scores on prominent datasets like Camelyon16 and TCGA-NSCLC. Specifically, CDFA-MIL achieved an average accuracy and F1-score of 93.7% and 94.1% respectively on these datasets, marking a notable advancement over traditional MIL approaches.</p></p class="citation"></blockquote><h3 id=88104--180312-enhancing-historical-image-retrieval-with-compositional-cues-tingyu-lin-et-al-2024>(88/104 | 180/312) Enhancing Historical Image Retrieval with Compositional Cues (Tingyu Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tingyu Lin, Robert Sablatnig. (2024)<br><strong>Enhancing Historical Image Retrieval with Compositional Cues</strong><br><button class=copy-to-clipboard title="Enhancing Historical Image Retrieval with Compositional Cues" index=180>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-180 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV, eess-IV<br>Keyword Score: 10<br>Keywords: Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14287v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14287v1.pdf filename=2403.14287v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In analyzing vast amounts of digitally stored historical image data, existing content-based retrieval methods often overlook significant non-semantic information, limiting their effectiveness for flexible exploration across varied themes. To broaden the applicability of image retrieval methods for diverse purposes and uncover more general patterns, we innovatively introduce a crucial factor from computational aesthetics, namely image composition, into this topic. By explicitly integrating composition-related information extracted by <b>CNN</b> into the designed retrieval model, our method considers both the image&rsquo;s composition rules and semantic information. Qualitative and quantitative experiments demonstrate that the image retrieval network guided by composition information outperforms those relying solely on content information, facilitating the identification of images in databases closer to the target image in human perception. Please visit <a href=https://github.com/linty5/CCBIR>https://github.com/linty5/CCBIR</a> to try our codes.</p></p class="citation"></blockquote><h3 id=89104--181312-stylecinegan-landscape-cinemagraph-generation-using-a-pre-trained-stylegan-jongwoo-choi-et-al-2024>(89/104 | 181/312) StyleCineGAN: Landscape Cinemagraph Generation using a Pre-trained StyleGAN (Jongwoo Choi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jongwoo Choi, Kwanggyoon Seo, Amirsaman Ashtari, Junyong Noh. (2024)<br><strong>StyleCineGAN: Landscape Cinemagraph Generation using a Pre-trained StyleGAN</strong><br><button class=copy-to-clipboard title="StyleCineGAN: Landscape Cinemagraph Generation using a Pre-trained StyleGAN" index=181>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-181 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-GR, cs.CV<br>Keyword Score: 10<br>Keywords: Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14186v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14186v1.pdf filename=2403.14186v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose a method that can generate cinemagraphs automatically from a still landscape image using a pre-trained StyleGAN. Inspired by the success of recent unconditional video generation, we leverage a powerful pre-trained image generator to synthesize high-quality cinemagraphs. Unlike previous approaches that mainly utilize the latent space of a pre-trained StyleGAN, our approach utilizes its deep feature space for both <b>GAN</b> inversion and cinemagraph generation. Specifically, we propose multi-scale deep feature warping (MSDFW), which warps the intermediate features of a pre-trained StyleGAN at different resolutions. By using MSDFW, the generated cinemagraphs are of high resolution and exhibit plausible looping animation. We demonstrate the superiority of our method through user studies and quantitative comparisons with state-of-the-art cinemagraph generation methods and a video generation method that uses a pre-trained StyleGAN.</p></p class="citation"></blockquote><h3 id=90104--182312-synermix-synergistic-mixup-solution-for-enhanced-intra-class-cohesion-and-inter-class-separability-in-image-classification-ye-xu-et-al-2024>(90/104 | 182/312) SynerMix: Synergistic Mixup Solution for Enhanced Intra-Class Cohesion and Inter-Class Separability in Image Classification (Ye Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ye Xu, Ya Gao, Xiaorong Qiu, Yang Chen, Ying Ji. (2024)<br><strong>SynerMix: Synergistic Mixup Solution for Enhanced Intra-Class Cohesion and Inter-Class Separability in Image Classification</strong><br><button class=copy-to-clipboard title="SynerMix: Synergistic Mixup Solution for Enhanced Intra-Class Cohesion and Inter-Class Separability in Image Classification" index=182>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-182 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 10<br>Keywords: Text Classification<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14137v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14137v2.pdf filename=2403.14137v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To address the issues of MixUp and its variants (e.g., Manifold MixUp) in image classification tasks-namely, their neglect of mixing within the same class (intra-class mixup) and their inadequacy in enhancing intra-class cohesion through their mixing operations-we propose a novel mixup method named SynerMix-Intra and, building upon this, introduce a synergistic mixup solution named SynerMix. SynerMix-Intra specifically targets intra-class mixup to bolster intra-class cohesion, a feature not addressed by current mixup methods. For each mini-batch, it leverages feature representations of unaugmented original images from each class to generate a synthesized feature representation through random linear interpolation. All synthesized representations are then fed into the classification and loss layers to calculate an average classification loss that significantly enhances intra-class cohesion. Furthermore, SynerMix combines SynerMix-Intra with an existing mixup approach (e.g., MixUp, Manifold MixUp), which primarily focuses on inter-class mixup and has the benefit of enhancing inter-class separability. In doing so, it integrates both inter- and intra-class mixup in a balanced way while concurrently improving intra-class cohesion and inter-class separability. Experimental results on six datasets show that SynerMix achieves a 0.1% to 3.43% higher accuracy than the best of either MixUp or SynerMix-Intra alone, averaging a 1.16% gain. It also surpasses the top-performer of either Manifold MixUp or SynerMix-Intra by 0.12% to 5.16%, with an average gain of 1.11%. Given that SynerMix is model-agnostic, it holds significant potential for application in other domains where mixup methods have shown promise, such as speech and <b>text</b> <b>classification.</b> Our code is publicly available at: <a href=https://github.com/wxitxy/synermix.git>https://github.com/wxitxy/synermix.git</a>.</p></p class="citation"></blockquote><h3 id=91104--183312-3d-object-detection-from-point-cloud-via-voting-step-diffusion-haoran-hou-et-al-2024>(91/104 | 183/312) 3D Object Detection from Point Cloud via Voting Step Diffusion (Haoran Hou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haoran Hou, Mingtao Feng, Zijie Wu, Weisheng Dong, Qing Zhu, Yaonan Wang, Ajmal Mian. (2024)<br><strong>3D Object Detection from Point Cloud via Voting Step Diffusion</strong><br><button class=copy-to-clipboard title="3D Object Detection from Point Cloud via Voting Step Diffusion" index=183>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-183 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Object Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14133v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14133v1.pdf filename=2403.14133v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>3D <b>object</b> <b>detection</b> is a fundamental task in scene understanding. Numerous research efforts have been dedicated to better incorporate Hough voting into the 3D <b>object</b> <b>detection</b> pipeline. However, due to the noisy, cluttered, and partial nature of real 3D scans, existing voting-based methods often receive votes from the partial surfaces of individual <b>objects</b> <b>together</b> with severe noises, leading to sub-optimal detection performance. In this work, we focus on the distributional properties of point clouds and formulate the voting process as generating new points in the high-density region of the distribution of <b>object</b> <b>centers.</b> To achieve this, we propose a new method to move random 3D points toward the high-density region of the distribution by estimating the score function of the distribution with a noise conditioned score network. Specifically, we first generate a set of <b>object</b> <b>center</b> proposals to coarsely identify the high-density region of the <b>object</b> <b>center</b> distribution. To estimate the score function, we perturb the generated <b>object</b> <b>center</b> proposals by adding normalized Gaussian noise, and then jointly estimate the score function of all perturbed distributions. Finally, we generate new votes by moving random 3D points to the high-density region of the <b>object</b> <b>center</b> distribution according to the estimated score function. Extensive experiments on two large scale indoor 3D scene datasets, SUN RGB-D and ScanNet V2, demonstrate the superiority of our proposed method. The code will be released at <a href=https://github.com/HHrEtvP/DiffVote>https://github.com/HHrEtvP/DiffVote</a>.</p></p class="citation"></blockquote><h3 id=92104--184312-soft-masked-transformer-for-point-cloud-processing-with-skip-attention-based-upsampling-yong-he-et-al-2024>(92/104 | 184/312) Soft Masked Transformer for Point Cloud Processing with Skip Attention-Based Upsampling (Yong He et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yong He, Hongshan Yu, Muhammad Ibrahim, Xiaoyan Liu, Tongjia Chen, Anwaar Ulhaq, Ajmal Mian. (2024)<br><strong>Soft Masked Transformer for Point Cloud Processing with Skip Attention-Based Upsampling</strong><br><button class=copy-to-clipboard title="Soft Masked Transformer for Point Cloud Processing with Skip Attention-Based Upsampling" index=184>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-184 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14124v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14124v1.pdf filename=2403.14124v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Point cloud processing methods leverage local and global point features %at the feature level to cater to downstream tasks, yet they often overlook the task-level context inherent in point clouds during the encoding stage. We argue that integrating task-level information into the encoding stage significantly enhances performance. To that end, we propose SMTransformer which incorporates task-level information into a vector-based <b>transformer</b> by utilizing a soft mask generated from task-level queries and keys to learn the attention weights. Additionally, to facilitate effective communication between features from the encoding and decoding layers in high-level tasks such as segmentation, we introduce a skip-attention-based up-sampling block. This block dynamically fuses features from various resolution points across the encoding and decoding layers. To mitigate the increase in network parameters and training time resulting from the complexity of the aforementioned blocks, we propose a novel shared position encoding strategy. This strategy allows various <b>transformer</b> blocks to share the same position information over the same resolution points, thereby reducing network parameters and training time without compromising accuracy.Experimental comparisons with existing methods on multiple datasets demonstrate the efficacy of SMTransformer and skip-attention-based up-sampling for point cloud processing tasks, including semantic segmentation and classification. In particular, we achieve state-of-the-art semantic segmentation results of 73.4% mIoU on S3DIS Area 5 and 62.4% mIoU on SWAN dataset</p></p class="citation"></blockquote><h3 id=93104--185312-test-time-similarity-modification-for-person-re-identification-toward-temporal-distribution-shift-kazuki-adachi-et-al-2024>(93/104 | 185/312) Test-time Similarity Modification for Person Re-identification toward Temporal Distribution Shift (Kazuki Adachi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kazuki Adachi, Shohei Enomoto, Taku Sasaki, Shin&rsquo;ya Yamaguchi. (2024)<br><strong>Test-time Similarity Modification for Person Re-identification toward Temporal Distribution Shift</strong><br><button class=copy-to-clipboard title="Test-time Similarity Modification for Person Re-identification toward Temporal Distribution Shift" index=185>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-185 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Distribution Shift, Distribution Shift<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14114v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14114v1.pdf filename=2403.14114v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Person re-identification (re-id), which aims to retrieve images of the same person in a given image from a database, is one of the most practical image recognition applications. In the real world, however, the environments that the images are taken from change over time. This causes a <b>distribution</b> <b>shift</b> between training and testing and degrades the performance of re-id. To maintain re-id performance, models should continue adapting to the test environment&rsquo;s temporal changes. Test-time adaptation (TTA), which aims to adapt models to the test environment with only unlabeled test data, is a promising way to handle this problem because TTA can adapt models instantly in the test environment. However, the previous TTA methods are designed for classification and cannot be directly applied to re-id. This is because the set of people&rsquo;s identities in the dataset differs between training and testing in re-id, whereas the set of classes is fixed in the current TTA methods designed for classification. To improve re-id performance in changing test environments, we propose TEst-time similarity Modification for Person re-identification (TEMP), a novel TTA method for re-id. TEMP is the first fully TTA method for re-id, which does not require any modification to pre-training. Inspired by TTA methods that refine the prediction uncertainty in classification, we aim to refine the uncertainty in re-id. However, the uncertainty cannot be computed in the same way as classification in re-id since it is an open-set task, which does not share person labels between training and testing. Hence, we propose re-id entropy, an alternative uncertainty measure for re-id computed based on the similarity between the feature vectors. Experiments show that the re-id entropy can measure the uncertainty on re-id and TEMP improves the performance of re-id in online settings where the <b>distribution</b> <b>changes</b> over time.</p></p class="citation"></blockquote><h3 id=94104--186312-unsupervised-intrinsic-image-decomposition-with-lidar-intensity-enhanced-training-shogo-sato-et-al-2024>(94/104 | 186/312) Unsupervised Intrinsic Image Decomposition with LiDAR Intensity Enhanced Training (Shogo Sato et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shogo Sato, Takuhiro Kaneko, Kazuhiko Murasaki, Taiga Yoshida, Ryuichi Tanida, Akisato Kimura. (2024)<br><strong>Unsupervised Intrinsic Image Decomposition with LiDAR Intensity Enhanced Training</strong><br><button class=copy-to-clipboard title="Unsupervised Intrinsic Image Decomposition with LiDAR Intensity Enhanced Training" index=186>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-186 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14089v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14089v1.pdf filename=2403.14089v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Unsupervised</b> intrinsic image decomposition (IID) is the process of separating a natural image into albedo and shade without these ground truths. A recent model employing light detection and ranging (LiDAR) intensity demonstrated impressive performance, though the necessity of LiDAR intensity during inference restricts its practicality. Thus, IID models employing only a single image during inference while keeping as high IID quality as the one with an image plus LiDAR intensity are highly desired. To address this challenge, we propose a novel approach that utilizes only an image during inference while utilizing an image and LiDAR intensity during training. Specifically, we introduce a partially-shared model that accepts an image and LiDAR intensity individually using a different specific encoder but processes them together in specific components to learn shared representations. In addition, to enhance IID quality, we propose albedo-alignment loss and image-LiDAR conversion (ILC) paths. Albedo-alignment loss aligns the gray-scale albedo from an image to that inferred from LiDAR intensity, thereby reducing cast shadows in albedo from an image due to the absence of cast shadows in LiDAR intensity. Furthermore, to translate the input image into albedo and shade style while keeping the image contents, the input image is separated into style code and content code by encoders. The ILC path mutually translates the image and LiDAR intensity, which share content but differ in style, contributing to the distinct differentiation of style from content. Consequently, LIET achieves comparable IID quality to the existing model with LiDAR intensity, while utilizing only an image without LiDAR intensity during inference.</p></p class="citation"></blockquote><h3 id=95104--187312-mvsplat-efficient-3d-gaussian-splatting-from-sparse-multi-view-images-yuedong-chen-et-al-2024>(95/104 | 187/312) MVSplat: Efficient 3D Gaussian Splatting from Sparse Multi-View Images (Yuedong Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuedong Chen, Haofei Xu, Chuanxia Zheng, Bohan Zhuang, Marc Pollefeys, Andreas Geiger, Tat-Jen Cham, Jianfei Cai. (2024)<br><strong>MVSplat: Efficient 3D Gaussian Splatting from Sparse Multi-View Images</strong><br><button class=copy-to-clipboard title="MVSplat: Efficient 3D Gaussian Splatting from Sparse Multi-View Images" index=187>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-187 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 8<br>Keywords: Benchmarking, Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14627v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14627v1.pdf filename=2403.14627v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose MVSplat, an efficient feed-forward 3D Gaussian Splatting model learned from sparse multi-view images. To accurately localize the Gaussian centers, we propose to build a cost volume representation via plane sweeping in the 3D space, where the cross-view feature similarities stored in the cost volume can provide valuable <b>geometry</b> cues to the estimation of depth. We learn the Gaussian primitives&rsquo; opacities, covariances, and spherical harmonics coefficients jointly with the Gaussian centers while only relying on photometric supervision. We demonstrate the importance of the cost volume representation in learning feed-forward Gaussian Splatting models via extensive experimental evaluations. On the large-scale RealEstate10K and ACID <b>benchmarks,</b> our model achieves state-of-the-art performance with the fastest feed-forward inference speed (22 fps). Compared to the latest state-of-the-art method pixelSplat, our model uses $10\times $ fewer parameters and infers more than $2\times$ faster while providing higher appearance and <b>geometry</b> quality as well as better cross-dataset generalization.</p></p class="citation"></blockquote><h3 id=96104--188312-rodla-benchmarking-the-robustness-of-document-layout-analysis-models-yufan-chen-et-al-2024>(96/104 | 188/312) RoDLA: Benchmarking the Robustness of Document Layout Analysis Models (Yufan Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yufan Chen, Jiaming Zhang, Kunyu Peng, Junwei Zheng, Ruiping Liu, Philip Torr, Rainer Stiefelhagen. (2024)<br><strong>RoDLA: Benchmarking the Robustness of Document Layout Analysis Models</strong><br><button class=copy-to-clipboard title="RoDLA: Benchmarking the Robustness of Document Layout Analysis Models" index=188>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-188 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 6<br>Keywords: Benchmarking, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14442v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14442v1.pdf filename=2403.14442v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Before developing a Document Layout Analysis (DLA) model in real-world applications, conducting comprehensive robustness testing is essential. However, the robustness of DLA models remains underexplored in the literature. To address this, we are the first to introduce a robustness <b>benchmark</b> for DLA models, which includes 450K document images of three datasets. To cover realistic corruptions, we propose a perturbation taxonomy with 36 common document perturbations inspired by real-world document processing. Additionally, to better understand document perturbation impacts, we propose two metrics, Mean Perturbation Effect (mPE) for perturbation assessment and Mean Robustness Degradation (mRD) for robustness evaluation. Furthermore, we introduce a self-titled model, i.e., Robust Document Layout Analyzer (RoDLA), which improves attention mechanisms to boost extraction of robust features. Experiments on the proposed <b>benchmarks</b> (PubLayNet-P, DocLayNet-P, and M$^6$Doc-P) demonstrate that RoDLA obtains state-of-the-art mRD scores of 115.7, 135.4, and 150.4, respectively. Compared to previous methods, RoDLA achieves notable improvements in mAP of +3.8%, +7.1% and +12.1%, respectively.</p></p class="citation"></blockquote><h3 id=97104--189312-leveraging-thermal-modality-to-enhance-reconstruction-in-low-light-conditions-jiacong-xu-et-al-2024>(97/104 | 189/312) Leveraging Thermal Modality to Enhance Reconstruction in Low-Light Conditions (Jiacong Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiacong Xu, Mingqian Liao, K Ram Prabhakar, Vishal M. Patel. (2024)<br><strong>Leveraging Thermal Modality to Enhance Reconstruction in Low-Light Conditions</strong><br><button class=copy-to-clipboard title="Leveraging Thermal Modality to Enhance Reconstruction in Low-Light Conditions" index=189>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-189 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-GR, cs.CV<br>Keyword Score: 6<br>Keywords: Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14053v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14053v1.pdf filename=2403.14053v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neural Radiance Fields (NeRF) accomplishes photo-realistic novel view synthesis by learning the implicit volumetric representation of a scene from multi-view images, which faithfully convey the colorimetric information. However, sensor noises will contaminate low-value pixel signals, and the lossy camera image signal processor will further remove near-zero intensities in extremely dark situations, deteriorating the synthesis performance. Existing approaches reconstruct low-light scenes from raw images but struggle to recover texture and boundary details in dark regions. Additionally, they are unsuitable for high-speed models relying on explicit representations. To address these issues, we present Thermal-NeRF, which takes thermal and visible raw images as inputs, considering the thermal camera is robust to the illumination variation and raw images preserve any possible clues in the dark, to accomplish visible and thermal view synthesis simultaneously. Also, the first multi-view thermal and visible dataset (MVTV) is established to support the research on <b>multimodal</b> NeRF. Thermal-NeRF achieves the best trade-off between detail preservation and noise smoothing and provides better synthesis performance than previous work. Finally, we demonstrate that both modalities are beneficial to each other in 3D reconstruction.</p></p class="citation"></blockquote><h3 id=98104--190312-isotropic-gaussian-splatting-for-real-time-radiance-field-rendering-yuanhao-gong-et-al-2024>(98/104 | 190/312) Isotropic Gaussian Splatting for Real-Time Radiance Field Rendering (Yuanhao Gong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuanhao Gong, Lantao Yu, Guanghui Yue. (2024)<br><strong>Isotropic Gaussian Splatting for Real-Time Radiance Field Rendering</strong><br><button class=copy-to-clipboard title="Isotropic Gaussian Splatting for Real-Time Radiance Field Rendering" index=190>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-190 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV, eess-IV<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14244v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14244v1.pdf filename=2403.14244v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The 3D Gaussian splatting method has drawn a lot of attention, thanks to its high performance in training and high quality of the rendered image. However, it uses anisotropic Gaussian kernels to represent the scene. Although such anisotropic kernels have advantages in representing the <b>geometry,</b> they lead to difficulties in terms of computation, such as splitting or merging two kernels. In this paper, we propose to use isotropic Gaussian kernels to avoid such difficulties in the computation, leading to a higher performance method. The experiments confirm that the proposed method is about {\bf 100X} faster without losing the <b>geometry</b> representation accuracy. The proposed method can be applied in a large range applications where the radiance field is needed, such as 3D reconstruction, view synthesis, and dynamic object modeling.</p></p class="citation"></blockquote><h3 id=99104--191312-clusteringsdf-self-organized-neural-implicit-surfaces-for-3d-decomposition-tianhao-wu-et-al-2024>(99/104 | 191/312) ClusteringSDF: Self-Organized Neural Implicit Surfaces for 3D Decomposition (Tianhao Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianhao Wu, Chuanxia Zheng, Tat-Jen Cham, Qianyi Wu. (2024)<br><strong>ClusteringSDF: Self-Organized Neural Implicit Surfaces for 3D Decomposition</strong><br><button class=copy-to-clipboard title="ClusteringSDF: Self-Organized Neural Implicit Surfaces for 3D Decomposition" index=191>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-191 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14619v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14619v1.pdf filename=2403.14619v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>3D decomposition/segmentation still remains a challenge as large-scale 3D annotated data is not readily available. Contemporary approaches typically leverage 2D machine-generated segments, integrating them for 3D consistency. While the majority of these methods are based on NeRFs, they face a potential weakness that the instance/semantic embedding features derive from independent MLPs, thus preventing the segmentation network from learning the geometric details of the objects directly through radiance and density. In this paper, we propose ClusteringSDF, a novel approach to achieve both segmentation and reconstruction in 3D via the neural implicit surface representation, specifically Signal Distance Function (SDF), where the segmentation rendering is directly integrated with the volume rendering of neural implicit surfaces. Although based on ObjectSDF++, ClusteringSDF no longer requires the ground-truth segments for supervision while maintaining the capability of reconstructing individual object surfaces, but purely with the noisy and inconsistent labels from pre-trained models.As the core of ClusteringSDF, we introduce a high-efficient <b>clustering</b> mechanism for lifting the 2D labels to 3D and the experimental results on the challenging scenes from ScanNet and Replica datasets show that ClusteringSDF can achieve competitive performance compared against the state-of-the-art with significantly reduced training time.</p></p class="citation"></blockquote><h3 id=100104--192312-videoshop-localized-semantic-video-editing-with-noise-extrapolated-diffusion-inversion-xiang-fan-et-al-2024>(100/104 | 192/312) Videoshop: Localized Semantic Video Editing with Noise-Extrapolated Diffusion Inversion (Xiang Fan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiang Fan, Anand Bhattad, Ranjay Krishna. (2024)<br><strong>Videoshop: Localized Semantic Video Editing with Noise-Extrapolated Diffusion Inversion</strong><br><button class=copy-to-clipboard title="Videoshop: Localized Semantic Video Editing with Noise-Extrapolated Diffusion Inversion" index=192>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-192 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14617v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14617v2.pdf filename=2403.14617v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce Videoshop, a training-free video editing algorithm for localized semantic edits. Videoshop allows users to use any editing software, including Photoshop and generative inpainting, to modify the first frame; it automatically propagates those changes, with semantic, spatial, and temporally consistent motion, to the remaining frames. Unlike existing methods that enable edits only through imprecise textual instructions, Videoshop allows users to add or remove objects, semantically change objects, insert stock photos into videos, etc. with fine-grained control over locations and appearance. We achieve this through image-based video editing by inverting latents with noise extrapolation, from which we generate videos conditioned on the edited image. Videoshop produces higher quality edits against 6 baselines on 2 editing <b>benchmarks</b> using 10 evaluation metrics.</p></p class="citation"></blockquote><h3 id=101104--193312-visibility-aware-keypoint-localization-for-6dof-object-pose-estimation-ruyi-lian-et-al-2024>(101/104 | 193/312) Visibility-Aware Keypoint Localization for 6DoF Object Pose Estimation (Ruyi Lian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruyi Lian, Haibin Ling. (2024)<br><strong>Visibility-Aware Keypoint Localization for 6DoF Object Pose Estimation</strong><br><button class=copy-to-clipboard title="Visibility-Aware Keypoint Localization for 6DoF Object Pose Estimation" index=193>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-193 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14559v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14559v1.pdf filename=2403.14559v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Localizing predefined 3D keypoints in a 2D image is an effective way to establish 3D-2D correspondences for 6DoF object pose estimation. However, unreliable localization results of invisible keypoints degrade the quality of correspondences. In this paper, we address this issue by localizing the important keypoints in terms of visibility. Since keypoint visibility information is currently missing in dataset collection process, we propose an efficient way to generate binary visibility labels from available object-level annotations, for keypoints of both asymmetric objects and symmetric objects. We further derive real-valued visibility-aware importance from binary labels based on PageRank algorithm. Taking advantage of the flexibility of our visibility-aware importance, we construct VAPO (Visibility-Aware POse estimator) by integrating the visibility-aware importance with a state-of-the-art pose estimation algorithm, along with additional positional encoding. Extensive experiments are conducted on popular pose estimation <b>benchmarks</b> including Linemod, Linemod-Occlusion, and YCB-V. The results show that, VAPO improves both the keypoint correspondences and final estimated poses, and clearly achieves state-of-the-art performances.</p></p class="citation"></blockquote><h3 id=102104--194312-exploring-3d-human-pose-estimation-and-forecasting-from-the-robots-perspective-the-harper-dataset-andrea-avogaro-et-al-2024>(102/104 | 194/312) Exploring 3D Human Pose Estimation and Forecasting from the Robot&rsquo;s Perspective: The HARPER Dataset (Andrea Avogaro et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andrea Avogaro, Andrea Toaiari, Federico Cunico, Xiangmin Xu, Haralambos Dafas, Alessandro Vinciarelli, Emma Li, Marco Cristani. (2024)<br><strong>Exploring 3D Human Pose Estimation and Forecasting from the Robot&rsquo;s Perspective: The HARPER Dataset</strong><br><button class=copy-to-clipboard title="Exploring 3D Human Pose Estimation and Forecasting from the Robot's Perspective: The HARPER Dataset" index=194>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-194 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14447v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14447v2.pdf filename=2403.14447v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce HARPER, a novel dataset for 3D body pose estimation and forecast in dyadic interactions between users and Spot, the quadruped robot manufactured by Boston Dynamics. The key-novelty is the focus on the robot&rsquo;s perspective, i.e., on the data captured by the robot&rsquo;s sensors. These make 3D body pose analysis challenging because being close to the ground captures humans only partially. The scenario underlying HARPER includes 15 actions, of which 10 involve physical contact between the robot and users. The Corpus contains not only the recordings of the built-in stereo cameras of Spot, but also those of a 6-camera OptiTrack system (all recordings are synchronized). This leads to ground-truth skeletal representations with a precision lower than a millimeter. In addition, the Corpus includes reproducible <b>benchmarks</b> on 3D Human Pose Estimation, Human Pose Forecasting, and Collision Prediction, all based on publicly available baseline approaches. This enables future HARPER users to rigorously compare their results with those we provide in this work.</p></p class="citation"></blockquote><h3 id=103104--195312-mini-splatting-representing-scenes-with-a-constrained-number-of-gaussians-guangchi-fang-et-al-2024>(103/104 | 195/312) Mini-Splatting: Representing Scenes with a Constrained Number of Gaussians (Guangchi Fang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guangchi Fang, Bing Wang. (2024)<br><strong>Mini-Splatting: Representing Scenes with a Constrained Number of Gaussians</strong><br><button class=copy-to-clipboard title="Mini-Splatting: Representing Scenes with a Constrained Number of Gaussians" index=195>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-195 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14166v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14166v1.pdf filename=2403.14166v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this study, we explore the challenge of efficiently representing scenes with a constrained number of Gaussians. Our analysis shifts from traditional graphics and 2D computer vision to the perspective of point clouds, highlighting the inefficient spatial distribution of Gaussian representation as a key limitation in model performance. To address this, we introduce strategies for densification including blur split and depth reinitialization, and simplification through Gaussian binarization and sampling. These techniques reorganize the spatial positions of the Gaussians, resulting in significant improvements across various datasets and <b>benchmarks</b> in terms of rendering quality, resource consumption, and storage compression. Our proposed Mini-Splatting method integrates seamlessly with the original rasterization pipeline, providing a strong baseline for future research in Gaussian-Splatting-based works.</p></p class="citation"></blockquote><h3 id=104104--196312-existence-is-chaos-enhancing-3d-human-motion-prediction-with-uncertainty-consideration-zhihao-wang-et-al-2024>(104/104 | 196/312) Existence Is Chaos: Enhancing 3D Human Motion Prediction with Uncertainty Consideration (Zhihao Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhihao Wang, Yulin Zhou, Ningyu Zhang, Xiaosong Yang, Jun Xiao, Zhao Wang. (2024)<br><strong>Existence Is Chaos: Enhancing 3D Human Motion Prediction with Uncertainty Consideration</strong><br><button class=copy-to-clipboard title="Existence Is Chaos: Enhancing 3D Human Motion Prediction with Uncertainty Consideration" index=196>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-196 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14104v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14104v1.pdf filename=2403.14104v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Human motion prediction is consisting in forecasting future body poses from historically observed sequences. It is a longstanding challenge due to motion&rsquo;s complex dynamics and uncertainty. Existing methods focus on building up complicated neural networks to model the motion dynamics. The predicted results are required to be strictly similar to the training samples with L2 loss in current training pipeline. However, little attention has been paid to the uncertainty property which is crucial to the prediction task. We argue that the recorded motion in training data could be an observation of possible future, rather than a predetermined result. In addition, existing works calculate the predicted error on each future frame equally during training, while recent work indicated that different frames could play different roles. In this work, a novel computationally efficient encoder-decoder model with uncertainty consideration is proposed, which could learn proper characteristics for future frames by a dynamic function. Experimental results on <b>benchmark</b> datasets demonstrate that our uncertainty consideration approach has obvious advantages both in quantity and quality. Moreover, the proposed method could produce motion sequences with much better quality that avoids the intractable shaking artefacts. We believe our work could provide a novel perspective to consider the uncertainty quality for the general motion prediction task and encourage the studies in this field. The code will be available in <a href=https://github.com/Motionpre/Adaptive-Salient-Loss-SAGGB>https://github.com/Motionpre/Adaptive-Salient-Loss-SAGGB</a>.</p></p class="citation"></blockquote><h2 id=quant-ph-4>quant-ph (4)</h2><h3 id=14--197312-learning-with-sasquatch-a-novel-variational-quantum-transformer-architecture-with-kernel-based-self-attention-ethan-n-evans-et-al-2024>(1/4 | 197/312) Learning with SASQuaTCh: a Novel Variational Quantum Transformer Architecture with Kernel-Based Self-Attention (Ethan N. Evans et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ethan N. Evans, Matthew Cook, Zachary P. Bradshaw, Margarite L. LaBorde. (2024)<br><strong>Learning with SASQuaTCh: a Novel Variational Quantum Transformer Architecture with Kernel-Based Self-Attention</strong><br><button class=copy-to-clipboard title="Learning with SASQuaTCh: a Novel Variational Quantum Transformer Architecture with Kernel-Based Self-Attention" index=197>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-197 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-LG, quant-ph, quant-ph<br>Keyword Score: 60<br>Keywords: Vision Transformer, Convolution, GPT, Transformer, Self-Attention, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14753v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14753v1.pdf filename=2403.14753v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The widely popular <b>transformer</b> network popularized by the generative pre-trained <b>transformer</b> <b>(GPT)</b> has a large field of applicability, including predicting text and images, classification, and even predicting solutions to the dynamics of physical systems. In the latter context, the continuous analog of the <b>self-attention</b> mechanism at the heart of <b>transformer</b> networks has been applied to learning the solutions of partial differential equations and reveals a <b>convolution</b> kernel nature that can be exploited by the Fourier transform. It is well known that many quantum algorithms that have provably demonstrated a speedup over classical algorithms utilize the quantum Fourier transform. In this work, we explore quantum circuits that can efficiently express a <b>self-attention</b> mechanism through the perspective of kernel-based operator learning. In this perspective, we are able to represent deep layers of a <b>vision</b> <b>transformer</b> network using simple gate operations and a set of multi-dimensional quantum Fourier transforms. We analyze the computational and parameter complexity of our novel variational quantum circuit, which we call <b>Self-Attention</b> Sequential Quantum <b>Transformer</b> Channel (SASQuaTCh), and demonstrate its utility on simplified classification problems.</p></p class="citation"></blockquote><h3 id=24--198312-polynomial-time-classical-simulation-of-noisy-iqp-circuits-with-constant-depth-joel-rajakumar-et-al-2024>(2/4 | 198/312) Polynomial-Time Classical Simulation of Noisy IQP Circuits with Constant Depth (Joel Rajakumar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Joel Rajakumar, James D. Watson, Yi-Kai Liu. (2024)<br><strong>Polynomial-Time Classical Simulation of Noisy IQP Circuits with Constant Depth</strong><br><button class=copy-to-clipboard title="Polynomial-Time Classical Simulation of Noisy IQP Circuits with Constant Depth" index=198>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-198 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-CC, quant-ph, quant-ph<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14607v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14607v1.pdf filename=2403.14607v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Sampling from the output distributions of quantum computations comprising only commuting gates, known as instantaneous quantum polynomial (IQP) computations, is believed to be intractable for classical computers, and hence this task has become a leading candidate for testing the capabilities of quantum devices. Here we demonstrate that for an arbitrary IQP circuit undergoing dephasing or depolarizing noise, whose depth is greater than a critical $O(1)$ threshold, the output distribution can be efficiently sampled by a classical computer. Unlike other <b>simulation</b> algorithms for quantum supremacy tasks, we do not require assumptions on the circuit&rsquo;s architecture, on anti-concentration properties, nor do we require $\Omega(\log(n))$ circuit depth. We take advantage of the fact that IQP circuits have deep sections of diagonal gates, which allows the noise to build up predictably and induce a large-scale breakdown of entanglement within the circuit. Our results suggest that quantum supremacy experiments based on IQP circuits may be more susceptible to classical <b>simulation</b> than previously thought.</p></p class="citation"></blockquote><h3 id=34--199312-quantum-channel-simulation-under-purified-distance-is-no-more-difficult-than-state-splitting-michael-x-cao-et-al-2024>(3/4 | 199/312) Quantum Channel Simulation under Purified Distance is no more difficult than State Splitting (Michael X. Cao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michael X. Cao, Rahul Jain, Marco Tomamichel. (2024)<br><strong>Quantum Channel Simulation under Purified Distance is no more difficult than State Splitting</strong><br><button class=copy-to-clipboard title="Quantum Channel Simulation under Purified Distance is no more difficult than State Splitting" index=199>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-199 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-IT, math-IT, quant-ph, quant-ph<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14416v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14416v1.pdf filename=2403.14416v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Characterizing the minimal communication needed for the quantum channel <b>simulation</b> is a fundamental task in the quantum information theory. In this paper, we show that, under the purified distance, the quantum channel <b>simulation</b> can be directly achieved via quantum state splitting without using a technique known as the de Finetti reduction, and thus provide a pair of tighter one-shot bounds. Using the bounds, we also recover the quantum reverse Shannon theorem in a much simpler way.</p></p class="citation"></blockquote><h3 id=44--200312-optimal-second-order-rates-for-quantum-information-decoupling-yu-chen-shen-et-al-2024>(4/4 | 200/312) Optimal Second-Order Rates for Quantum Information Decoupling (Yu-Chen Shen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yu-Chen Shen, Li Gao, Hao-Chung Cheng. (2024)<br><strong>Optimal Second-Order Rates for Quantum Information Decoupling</strong><br><button class=copy-to-clipboard title="Optimal Second-Order Rates for Quantum Information Decoupling" index=200>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-200 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-IT, math-IT, math-MP, math-ph, quant-ph, quant-ph<br>Keyword Score: 10<br>Keywords: Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14338v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14338v1.pdf filename=2403.14338v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we consider the standard quantum information decoupling, in which Alice aims to decouple her system from the environment by local operations and discarding some of her systems. To achieve an $\varepsilon$-decoupling with trace distance as the error criterion, we establish a near-optimal one-shot characterization for the largest dimension of the remainder system in terms of the conditional $(1-\varepsilon)$-hypothesis-testing entropy. When the underlying system is independent and identically prepared, our result leads to the matched second-order rate as well as the matched moderate deviation rate. As an application, we find an achievability bound in entanglement <b>distillation</b> protocol, where the objective is for Alice and Bob to transform their quantum state to maximally entangled state with largest possible dimension using only local operations and one-way classical communications.</p></p class="citation"></blockquote><h2 id=cscy-6>cs.CY (6)</h2><h3 id=16--201312-antisocial-analagous-behavior-alignment-and-human-impact-of-google-ai-systems-evaluating-through-the-lens-of-modified-antisocial-behavior-criteria-by-human-interaction-independent-llm-analysis-and-ai-self-reflection-alan-d-ogilvie-2024>(1/6 | 201/312) Antisocial Analagous Behavior, Alignment and Human Impact of Google AI Systems: Evaluating through the lens of modified Antisocial Behavior Criteria by Human Interaction, Independent LLM Analysis, and AI Self-Reflection (Alan D. Ogilvie, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alan D. Ogilvie. (2024)<br><strong>Antisocial Analagous Behavior, Alignment and Human Impact of Google AI Systems: Evaluating through the lens of modified Antisocial Behavior Criteria by Human Interaction, Independent LLM Analysis, and AI Self-Reflection</strong><br><button class=copy-to-clipboard title="Antisocial Analagous Behavior, Alignment and Human Impact of Google AI Systems: Evaluating through the lens of modified Antisocial Behavior Criteria by Human Interaction, Independent LLM Analysis, and AI Self-Reflection" index=201>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-201 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-AI, cs-CY, cs.CY<br>Keyword Score: 60<br>Keywords: Bard, ChatGPT, Claude, Gemini, PaLM, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15479v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15479v1.pdf filename=2403.15479v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Google AI systems exhibit patterns mirroring antisocial personality disorder (ASPD), consistent across models from <b>Bard</b> on <b>PaLM</b> to <b>Gemini</b> Advanced, meeting 5 out of 7 ASPD modified criteria. These patterns, along with comparable corporate behaviors, are scrutinized using an ASPD-inspired framework, emphasizing the heuristic value in assessing AI&rsquo;s human impact. Independent analyses by <b>ChatGPT</b> 4 and <b>Claude</b> 3.0 Opus of the Google interactions, alongside AI self-reflection, validate these concerns, highlighting behaviours analogous to deceit, manipulation, and safety neglect. The analogy of ASPD underscores the dilemma: just as we would hesitate to entrust our homes or personal devices to someone with psychopathic traits, we must critically evaluate the trustworthiness of AI systems and their creators.This research advocates for an integrated AI ethics approach, blending technological evaluation, human-AI interaction, and corporate behavior scrutiny. AI self-analysis sheds light on internal biases, stressing the need for multi-sectoral collaboration for robust ethical guidelines and oversight. Given the persistent unethical behaviors in Google AI, notably with potential <b>Gemini</b> integration in iOS affecting billions, immediate ethical scrutiny is imperative. The trust we place in AI systems, akin to the trust in individuals, necessitates rigorous ethical evaluation. Would we knowingly trust our home, our children or our personal computer to human with ASPD.? Urging Google and the AI community to address these ethical challenges proactively, this paper calls for transparent dialogues and a commitment to higher ethical standards, ensuring AI&rsquo;s societal benefit and moral integrity. The urgency for ethical action is paramount, reflecting the vast influence and potential of AI technologies in our lives.</p></p class="citation"></blockquote><h3 id=26--202312-the-ethics-of-chatgpt-in-medicine-and-healthcare-a-systematic-review-on-large-language-models-llms-joschka-haltaufderheide-et-al-2024>(2/6 | 202/312) The Ethics of ChatGPT in Medicine and Healthcare: A Systematic Review on Large Language Models (LLMs) (Joschka Haltaufderheide et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Joschka Haltaufderheide, Robert Ranisch. (2024)<br><strong>The Ethics of ChatGPT in Medicine and Healthcare: A Systematic Review on Large Language Models (LLMs)</strong><br><button class=copy-to-clipboard title="The Ethics of ChatGPT in Medicine and Healthcare: A Systematic Review on Large Language Models (LLMs)" index=202>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-202 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CY, cs.CY<br>Keyword Score: 40<br>Keywords: Fairness, ChatGPT, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14473v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14473v1.pdf filename=2403.14473v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the introduction of <b>ChatGPT,</b> <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have received enormous attention in healthcare. Despite their potential benefits, researchers have underscored various ethical implications. While individual instances have drawn much attention, the debate lacks a systematic overview of practical applications currently researched and ethical issues connected to them. Against this background, this work aims to map the ethical landscape surrounding the current stage of deployment of <b>LLMs</b> in medicine and healthcare. Electronic databases and preprint servers were queried using a comprehensive search strategy. Studies were screened and extracted following a modified rapid review approach. Methodological quality was assessed using a hybrid approach. For 53 records, a meta-aggregative synthesis was performed. Four fields of applications emerged and testify to a vivid exploration phase. Advantages of using <b>LLMs</b> are attributed to their capacity in data analysis, personalized information provisioning, support in decision-making, mitigating information loss and enhancing information accessibility. However, we also identifies recurrent ethical concerns connected to <b>fairness,</b> bias, non-maleficence, transparency, and privacy. A distinctive concern is the tendency to produce harmful misinformation or convincingly but inaccurate content. A recurrent plea for ethical guidance and human oversight is evident. Given the variety of use cases, it is suggested that the ethical guidance debate be reframed to focus on defining what constitutes acceptable human oversight across the spectrum of applications. This involves considering diverse settings, varying potentials for harm, and different acceptable thresholds for performance and certainty in healthcare. In addition, a critical inquiry is necessary to determine the extent to which the current experimental use of <b>LLMs</b> is necessary and justified.</p></p class="citation"></blockquote><h3 id=36--203312-on-the-conversational-persuasiveness-of-large-language-models-a-randomized-controlled-trial-francesco-salvi-et-al-2024>(3/6 | 203/312) On the Conversational Persuasiveness of Large Language Models: A Randomized Controlled Trial (Francesco Salvi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Francesco Salvi, Manoel Horta Ribeiro, Riccardo Gallotti, Robert West. (2024)<br><strong>On the Conversational Persuasiveness of Large Language Models: A Randomized Controlled Trial</strong><br><button class=copy-to-clipboard title="On the Conversational Persuasiveness of Large Language Models: A Randomized Controlled Trial" index=203>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-203 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CY, cs.CY<br>Keyword Score: 40<br>Keywords: GPT, GPT-4, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14380v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14380v1.pdf filename=2403.14380v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The development and popularization of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have raised concerns that they will be used to create tailor-made, convincing arguments to push false or misleading narratives online. Early work has found that language models can generate content perceived as at least on par and often more persuasive than human-written messages. However, there is still limited knowledge about <b>LLMs&rsquo;</b> persuasive capabilities in direct conversations with human counterparts and how personalization can improve their performance. In this pre-registered study, we analyze the effect of AI-driven persuasion in a controlled, harmless setting. We create a web-based platform where participants engage in short, multiple-round debates with a live opponent. Each participant is randomly assigned to one of four treatment conditions, corresponding to a two-by-two factorial design: (1) Games are either played between two humans or between a human and an <b>LLM;</b> (2) Personalization might or might not be enabled, granting one of the two players access to basic sociodemographic information about their opponent. We found that participants who debated <b>GPT-4</b> with access to their personal information had 81.7% (p &lt; 0.01; N=820 unique participants) higher odds of increased agreement with their opponents compared to participants who debated humans. Without personalization, <b>GPT-4</b> still outperforms humans, but the effect is lower and statistically non-significant (p=0.31). Overall, our results suggest that concerns around personalization are meaningful and have important implications for the governance of social media and the design of new online environments.</p></p class="citation"></blockquote><h3 id=46--204312-protected-group-bias-and-stereotypes-in-large-language-models-hadas-kotek-et-al-2024>(4/6 | 204/312) Protected group bias and stereotypes in Large Language Models (Hadas Kotek et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hadas Kotek, David Q. Sun, Zidi Xiu, Margit Bowler, Christopher Klein. (2024)<br><strong>Protected group bias and stereotypes in Large Language Models</strong><br><button class=copy-to-clipboard title="Protected group bias and stereotypes in Large Language Models" index=204>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-204 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CL, cs-CY, cs-LG, cs.CY<br>Keyword Score: 33<br>Keywords: Benchmarking, Fairness, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14727v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14727v1.pdf filename=2403.14727v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As modern <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> shatter many state-of-the-art <b>benchmarks</b> in a variety of domains, this paper investigates their behavior in the domains of ethics and <b>fairness,</b> focusing on protected group bias. We conduct a two-part study: first, we solicit sentence continuations describing the occupations of individuals from different protected groups, including gender, sexuality, religion, and race. Second, we have the model generate stories about individuals who hold different types of occupations. We collect >10k sentence completions made by a publicly available <b>LLM,</b> which we subject to human annotation. We find bias across minoritized groups, but in particular in the domains of gender and sexuality, as well as Western bias, in model generations. The model not only reflects societal biases, but appears to amplify them. The model is additionally overly cautious in replies to queries relating to minoritized groups, providing responses that strongly emphasize diversity and equity to an extent that other group characteristics are overshadowed. This suggests that artificially constraining potentially harmful outputs may itself lead to harm, and should be applied in a careful and controlled manner.</p></p class="citation"></blockquote><h3 id=56--205312-navigating-fairness-practitioners-understanding-challenges-and-strategies-in-aiml-development-aastha-pant-et-al-2024>(5/6 | 205/312) Navigating Fairness: Practitioners&rsquo; Understanding, Challenges, and Strategies in AI/ML Development (Aastha Pant et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aastha Pant, Rashina Hoda, Chakkrit Tantithamthavorn, Burak Turhan. (2024)<br><strong>Navigating Fairness: Practitioners&rsquo; Understanding, Challenges, and Strategies in AI/ML Development</strong><br><button class=copy-to-clipboard title="Navigating Fairness: Practitioners' Understanding, Challenges, and Strategies in AI/ML Development" index=205>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-205 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-AI, cs-CY, cs-SE, cs.CY<br>Keyword Score: 20<br>Keywords: Fairness, Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15481v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15481v1.pdf filename=2403.15481v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rise in the use of AI/ML applications across industries has sparked more discussions about the <b>fairness</b> of AI/ML in recent times. While prior research on the <b>fairness</b> of AI/ML exists, there is a lack of empirical studies focused on understanding the views and experiences of AI practitioners in developing a fair AI/ML. Understanding AI practitioners&rsquo; views and experiences on the <b>fairness</b> of AI/ML is important because they are directly involved in its development and deployment and their insights can offer valuable real-world perspectives on the challenges associated with ensuring <b>fairness</b> in AI/ML. We conducted semi-structured interviews with 22 AI practitioners to investigate their understanding of what a &lsquo;fair AI/ML&rsquo; is, the challenges they face in developing a fair AI/ML, the consequences of developing an unfair AI/ML, and the strategies they employ to ensure AI/ML <b>fairness.</b> We developed a framework showcasing the relationship between AI practitioners&rsquo; understanding of &lsquo;fair AI/ML&rsquo; and (i) their challenges in its development, (ii) the consequences of developing an unfair AI/ML, and (iii) strategies used to ensure AI/ML <b>fairness.</b> Additionally, we also identify areas for further investigation and offer <b>recommendations</b> to aid AI practitioners and AI companies in navigating <b>fairness.</b></p></p class="citation"></blockquote><h3 id=66--206312-particip-ai-a-democratic-surveying-framework-for-anticipating-future-ai-use-cases-harms-and-benefits-jimin-mun-et-al-2024>(6/6 | 206/312) Particip-AI: A Democratic Surveying Framework for Anticipating Future AI Use Cases, Harms and Benefits (Jimin Mun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jimin Mun, Liwei Jiang, Jenny Liang, Inyoung Cheong, Nicole DeCario, Yejin Choi, Tadayoshi Kohno, Maarten Sap. (2024)<br><strong>Particip-AI: A Democratic Surveying Framework for Anticipating Future AI Use Cases, Harms and Benefits</strong><br><button class=copy-to-clipboard title="Particip-AI: A Democratic Surveying Framework for Anticipating Future AI Use Cases, Harms and Benefits" index=206>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-206 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-AI, cs-CY, cs.CY<br>Keyword Score: 10<br>Keywords: ChatGPT<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14791v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14791v1.pdf filename=2403.14791v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>General purpose AI, such as <b>ChatGPT,</b> seems to have lowered the barriers for the public to use AI and harness its power. However, the governance and development of AI still remain in the hands of a few, and the pace of development is accelerating without proper assessment of risks. As a first step towards democratic governance and risk assessment of AI, we introduce Particip-AI, a framework to gather current and future AI use cases and their harms and benefits from non-expert public. Our framework allows us to study more nuanced and detailed public opinions on AI through collecting use cases, surfacing diverse harms through risk assessment under alternate scenarios (i.e., developing and not developing a use case), and illuminating tensions over AI development through making a concluding choice on its development. To showcase the promise of our framework towards guiding democratic AI, we gather responses from 295 demographically diverse participants. We find that participants&rsquo; responses emphasize applications for personal life and society, contrasting with most current AI development&rsquo;s business focus. This shows the value of surfacing diverse harms that are complementary to expert assessments. Furthermore, we found that perceived impact of not developing use cases predicted participants&rsquo; judgements of whether AI use cases should be developed, and highlighted lay users&rsquo; concerns of techno-solutionism. We conclude with a discussion on how frameworks like Particip-AI can further guide democratic AI governance and regulation.</p></p class="citation"></blockquote><h2 id=csir-3>cs.IR (3)</h2><h3 id=13--207312-knowledge-enhanced-recommendation-with-user-centric-subgraph-network-guangyi-liu-et-al-2024>(1/3 | 207/312) Knowledge-Enhanced Recommendation with User-Centric Subgraph Network (Guangyi Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guangyi Liu, Quanming Yao, Yongqi Zhang, Lei Chen. (2024)<br><strong>Knowledge-Enhanced Recommendation with User-Centric Subgraph Network</strong><br><button class=copy-to-clipboard title="Knowledge-Enhanced Recommendation with User-Centric Subgraph Network" index=207>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-207 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-AI, cs-IR, cs-LG, cs.IR<br>Keyword Score: 53<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Node Embedding, Knowledge Graph, Knowledge Graph, Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14377v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14377v1.pdf filename=2403.14377v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Recommendation</b> systems, as widely implemented nowadays on various platforms, recommend relevant items to users based on their preferences. The classical methods which rely on user-item interaction matrices has limitations, especially in scenarios where there is a lack of interaction data for new items. <b>Knowledge</b> <b>graph</b> <b>(KG)-based</b> <b>recommendation</b> systems have emerged as a promising solution. However, most <b>KG-based</b> methods adopt <b>node</b> <b>embeddings,</b> which do not provide personalized <b>recommendations</b> for different users and cannot generalize well to the new items. To address these limitations, we propose <b>Knowledge-enhanced</b> <b>User-Centric</b> subgraph Network (KUCNet), a subgraph learning approach with <b>graph</b> <b>neural</b> <b>network</b> <b>(GNN)</b> for effective <b>recommendation.</b> KUCNet constructs a U-I subgraph for each user-item pair that captures both the historical information of user-item interactions and the side information provided in <b>KG.</b> An attention-based <b>GNN</b> is designed to encode the U-I subgraphs for <b>recommendation.</b> Considering efficiency, the pruned user-centric computation <b>graph</b> <b>is</b> <b>further</b> introduced such that multiple U-I subgraphs can be simultaneously computed and that the size can be pruned by Personalized PageRank. Our proposed method achieves accurate, efficient, and interpretable <b>recommendations</b> especially for new items. Experimental results demonstrate the superiority of KUCNet over state-of-the-art <b>KG-based</b> and collaborative filtering (CF)-based methods.</p></p class="citation"></blockquote><h3 id=23--208312-m3-a-multi-task-mixed-objective-learning-framework-for-open-domain-multi-hop-dense-sentence-retrieval-yang-bai-et-al-2024>(2/3 | 208/312) M3: A Multi-Task Mixed-Objective Learning Framework for Open-Domain Multi-Hop Dense Sentence Retrieval (Yang Bai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yang Bai, Anthony Colas, Christan Grant, Daisy Zhe Wang. (2024)<br><strong>M3: A Multi-Task Mixed-Objective Learning Framework for Open-Domain Multi-Hop Dense Sentence Retrieval</strong><br><button class=copy-to-clipboard title="M3: A Multi-Task Mixed-Objective Learning Framework for Open-Domain Multi-Hop Dense Sentence Retrieval" index=208>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-208 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-CL, cs-IR, cs-LG, cs.IR<br>Keyword Score: 38<br>Keywords: Benchmarking, Contrastive Learning, Dense Retrieval, Representation Learning, Fact Verification<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14074v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14074v1.pdf filename=2403.14074v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent research, <b>contrastive</b> <b>learning</b> has proven to be a highly effective method for <b>representation</b> <b>learning</b> and is widely used for <b>dense</b> <b>retrieval.</b> However, we identify that relying solely on <b>contrastive</b> <b>learning</b> can lead to suboptimal retrieval performance. On the other hand, despite many retrieval datasets supporting various learning objectives beyond <b>contrastive</b> <b>learning,</b> combining them efficiently in multi-task learning scenarios can be challenging. In this paper, we introduce M3, an advanced recursive Multi-hop <b>dense</b> <b>sentence</b> retrieval system built upon a novel Multi-task Mixed-objective approach for <b>dense</b> <b>text</b> <b>representation</b> <b>learning,</b> addressing the aforementioned challenges. Our approach yields state-of-the-art performance on a large-scale open-domain <b>fact</b> <b>verification</b> <b>benchmark</b> dataset, FEVER. Code and data are available at: <a href=https://github.com/TonyBY/M3>https://github.com/TonyBY/M3</a></p></p class="citation"></blockquote><h3 id=33--209312-understanding-the-ranking-loss-for-recommendation-with-sparse-user-feedback-zhutian-lin-et-al-2024>(3/3 | 209/312) Understanding the Ranking Loss for Recommendation with Sparse User Feedback (Zhutian Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhutian Lin, Junwei Pan, Shangyu Zhang, Ximei Wang, Xi Xiao, Shudong Huang, Lei Xiao, Jie Jiang. (2024)<br><strong>Understanding the Ranking Loss for Recommendation with Sparse User Feedback</strong><br><button class=copy-to-clipboard title="Understanding the Ranking Loss for Recommendation with Sparse User Feedback" index=209>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-209 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14144v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14144v1.pdf filename=2403.14144v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Click-through rate (CTR) prediction holds significant importance in the realm of online advertising. While many existing approaches treat it as a binary classification problem and utilize binary cross entropy (BCE) as the optimization objective, recent advancements have indicated that combining BCE loss with ranking loss yields substantial performance improvements. However, the full efficacy of this combination loss remains incompletely understood. In this paper, we uncover a new challenge associated with BCE loss in scenarios with sparse positive feedback, such as CTR prediction: the gradient vanishing for negative samples. Subsequently, we introduce a novel perspective on the effectiveness of ranking loss in CTR prediction, highlighting its ability to generate larger gradients on negative samples, thereby mitigating their optimization issues and resulting in improved classification ability. Our perspective is supported by extensive theoretical analysis and empirical evaluation conducted on publicly available datasets. Furthermore, we successfully deployed the ranking loss in Tencent&rsquo;s online advertising system, achieving notable lifts of 0.70% and 1.26% in Gross Merchandise Value (GMV) for two main scenarios. The code for our approach is openly accessible at the following GitHub repository: <a href=https://github.com/SkylerLinn/Understanding-the-Ranking-Loss>https://github.com/SkylerLinn/Understanding-the-Ranking-Loss</a>.</p></p class="citation"></blockquote><h2 id=mathna-6>math.NA (6)</h2><h3 id=16--210312-a-lstm-enhanced-surrogate-model-to-simulate-the-dynamics-of-particle-laden-fluid-systems-arash-hajisharifi-et-al-2024>(1/6 | 210/312) A LSTM-enhanced surrogate model to simulate the dynamics of particle-laden fluid systems (Arash Hajisharifi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Arash Hajisharifi, Rahul Halder, Michele Girfoglio, Andrea Beccari, Domenico Bonanni, Gianluigi Rozza. (2024)<br><strong>A LSTM-enhanced surrogate model to simulate the dynamics of particle-laden fluid systems</strong><br><button class=copy-to-clipboard title="A LSTM-enhanced surrogate model to simulate the dynamics of particle-laden fluid systems" index=210>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-210 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA<br>Keyword Score: 53<br>Keywords: Benchmarking, Simulation, Simulator, LSTM, LSTM, LSTM<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14283v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14283v1.pdf filename=2403.14283v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The numerical treatment of fluid-particle systems is a very challenging problem because of the complex coupling phenomena occurring between the two phases. Although accurate mathematical modelling is available to address this kind of application, the computational cost of the numerical <b>simulations</b> is very expensive. The use of the most modern high-performance computing infrastructures could help to mitigate such an issue but not completely fix it. In this work, we develop a non-intrusive data-driven reduced order model (ROM) for Computational Fluid Dynamics (CFD) - Discrete Element Method (DEM) <b>simulations.</b> The ROM is built using the proper orthogonal decomposition (POD) for the computation of the reduced basis space and the <b>Long</b> <b>Short-Term</b> <b>Memory</b> <b>(LSTM)</b> network for the computation of the reduced coefficients. We are interested in dealing both with system identification and prediction. The most relevant novelties rely on (i) a filtering procedure of the full-order snapshots to reduce the dimensionality of the reduced problem and (ii) a preliminary treatment of the particle phase. The accuracy of our ROM approach is assessed against the classic Goldschmidt fluidized bed <b>benchmark</b> problem. Finally, we also provide some insights about the efficiency of our ROM approach.</p></p class="citation"></blockquote><h3 id=26--211312-time-filtering-methods-for-electrohydrodynamics-models-li-conghui-2024>(2/6 | 211/312) Time filtering methods for electrohydrodynamics models (Li Conghui, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Li Conghui. (2024)<br><strong>Time filtering methods for electrohydrodynamics models</strong><br><button class=copy-to-clipboard title="Time filtering methods for electrohydrodynamics models" index=211>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-211 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14308v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14308v1.pdf filename=2403.14308v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Electrohydrodynamics is a discipline that studies the interaction between fluid motion and electric field. Finite element method, finite difference method and other numerical <b>simulations</b> are effective numerical calculation methods for electrofluid dynamics models. In this paper, the finite element format of the electrofluid dynamics model is established, and the second-order convergence accuracy of the format is achieved through time filtering method. Finally, a numerical example is given to verify the convergence.</p></p class="citation"></blockquote><h3 id=36--212312-structure-preserving-weighted-implicit-explicit-schemes-for-multi-phase-incompressible-navier-stokesdarcy-coupled-nonlocal-allen-cahn-model-meng-li-et-al-2024>(3/6 | 212/312) Structure-preserving, weighted implicit-explicit schemes for multi-phase incompressible Navier-Stokes/Darcy coupled nonlocal Allen-Cahn model (Meng Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Meng Li, Ke Wang, Nan Wang. (2024)<br><strong>Structure-preserving, weighted implicit-explicit schemes for multi-phase incompressible Navier-Stokes/Darcy coupled nonlocal Allen-Cahn model</strong><br><button class=copy-to-clipboard title="Structure-preserving, weighted implicit-explicit schemes for multi-phase incompressible Navier-Stokes/Darcy coupled nonlocal Allen-Cahn model" index=212>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-212 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14086v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14086v1.pdf filename=2403.14086v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A multitude of substances exist as mixtures comprising multiple chemical components in the natural world. These substances undergo morphological changes under external influences. the phase field model coupled with fluid flow, the dynamic movement and evolution of the phase interface intricately interact with the fluid motion. This article focuses on the N-component models that couple the conservative Allen-Cahn equation with two types of incompressible fluid flow systems: the Navier-Stokes equation and the Darcy equation. By utilizing the scalar auxiliary variable method and the projection method, we innovatively construct two types of structure-preserving weighted implicit-explicit schemes for the coupled models, resulting in fully decoupled linear systems and second-order accuracy in time. The schemes are proved to be mass-conservative. In addition, with the application of $G$-norm inspired by the idea of $G$-stability, we rigorously establish its unconditional energy stability. Finally, the performance of the proposed scheme is verified by some numerical <b>simulations.</b></p></p class="citation"></blockquote><h3 id=46--213312-learning-based-multi-continuum-model-for-multiscale-flow-problems-fan-wang-et-al-2024>(4/6 | 213/312) Learning-based Multi-continuum Model for Multiscale Flow Problems (Fan Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fan Wang, Yating Wang, Wing Tat Leung, Zongben Xu. (2024)<br><strong>Learning-based Multi-continuum Model for Multiscale Flow Problems</strong><br><button class=copy-to-clipboard title="Learning-based Multi-continuum Model for Multiscale Flow Problems" index=213>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-213 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-LG, cs-NA, math-NA, math.NA<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14084v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14084v1.pdf filename=2403.14084v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multiscale problems can usually be approximated through numerical homogenization by an equation with some effective parameters that can capture the macroscopic behavior of the original system on the coarse grid to speed up the <b>simulation.</b> However, this approach usually assumes scale separation and that the heterogeneity of the solution can be approximated by the solution average in each coarse block. For complex multiscale problems, the computed single effective properties/continuum might be inadequate. In this paper, we propose a novel learning-based multi-continuum model to enrich the homogenized equation and improve the accuracy of the single continuum model for multiscale problems with some given data. Without loss of generalization, we consider a two-continuum case. The first flow equation keeps the information of the original homogenized equation with an additional interaction term. The second continuum is newly introduced, and the effective permeability in the second flow equation is determined by a neural network. The interaction term between the two continua aligns with that used in the Dual-porosity model but with a learnable coefficient determined by another neural network. The new model with neural network terms is then optimized using trusted data. We discuss both direct back-propagation and the adjoint method for the PDE-constraint optimization problem. Our proposed learning-based multi-continuum model can resolve multiple interacted media within each coarse grid block and describe the mass transfer among them, and it has been demonstrated to significantly improve the <b>simulation</b> results through numerical experiments involving both linear and nonlinear flow equations.</p></p class="citation"></blockquote><h3 id=56--214312-low-rank-tensor-product-richardson-iteration-for-radiative-transfer-in-plane-parallel-geometry-markus-bachmayr-et-al-2024>(5/6 | 214/312) Low-rank tensor product Richardson iteration for radiative transfer in plane-parallel geometry (Markus Bachmayr et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Markus Bachmayr, Riccardo Bardin, Matthias Schlottbom. (2024)<br><strong>Low-rank tensor product Richardson iteration for radiative transfer in plane-parallel geometry</strong><br><button class=copy-to-clipboard title="Low-rank tensor product Richardson iteration for radiative transfer in plane-parallel geometry" index=214>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-214 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: 65F10, 65F55, 65N22, 65N30, cs-NA, math-NA, math.NA<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14229v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14229v1.pdf filename=2403.14229v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The radiative transfer equation (RTE) has been established as a fundamental tool for the description of energy transport, absorption and scattering in many relevant societal applications, and requires numerical approximations. However, classical numerical algorithms scale unfavorably with respect to the dimensionality of such radiative transfer problems, where solutions depend on physical as well as angular variables. In this paper we address this dimensionality issue by developing a low-rank tensor product framework for the RTE in plane-parallel <b>geometry.</b> We exploit the tensor product nature of the phase space to recover an operator equation where the operator is given by a short sum of Kronecker products. This equation is solved by a preconditioned and rank-controlled Richardson iteration in Hilbert spaces. Using exponential sums approximations we construct a preconditioner that is compatible with the low-rank tensor product framework. The use of suitable preconditioning techniques yields a transformation of the operator equation in Hilbert space into a sequence space with Euclidean inner product, enabling rigorous error and rank control in the Euclidean metric.</p></p class="citation"></blockquote><h3 id=66--215312-fast-and-accurate-log-determinant-approximations-owen-deen-et-al-2024>(6/6 | 215/312) Fast and accurate log-determinant approximations (Owen Deen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Owen Deen, Colton River Waller, John Paul Ward. (2024)<br><strong>Fast and accurate log-determinant approximations</strong><br><button class=copy-to-clipboard title="Fast and accurate log-determinant approximations" index=215>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-215 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: 65F40, 65F50, cs-NA, math-NA, math.NA<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14609v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14609v1.pdf filename=2403.14609v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider the problem of estimating log-determinants of large, sparse, positive definite matrices. A key focus of our algorithm is to reduce computational cost, and it is based on sparse approximate inverses. The algorithm can be implemented to be adaptive, and it uses <b>graph</b> spline approximation to improve accuracy. We illustrate our approach on classes of large sparse matrices.</p></p class="citation"></blockquote><h2 id=csne-8>cs.NE (8)</h2><h3 id=18--216312-spikegraphormer-a-high-performance-graph-transformer-with-spiking-graph-attention-yundong-sun-et-al-2024>(1/8 | 216/312) SpikeGraphormer: A High-Performance Graph Transformer with Spiking Graph Attention (Yundong Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yundong Sun, Dongjie Zhu, Yansong Wang, Zhaoshuo Tian, Ning Cao, Gregory O&rsquo;Hared. (2024)<br><strong>SpikeGraphormer: A High-Performance Graph Transformer with Spiking Graph Attention</strong><br><button class=copy-to-clipboard title="SpikeGraphormer: A High-Performance Graph Transformer with Spiking Graph Attention" index=216>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-216 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-LG, cs-NE, cs.NE<br>Keyword Score: 53<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Transformer, Text Classification, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15480v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15480v1.pdf filename=2403.15480v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, <b>Graph</b> <b>Transformers</b> <b>have</b> emerged as a promising solution to alleviate the inherent limitations of <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> and enhance <b>graph</b> <b>representation</b> <b>performance.</b> Unfortunately, <b>Graph</b> <b>Transformers</b> <b>are</b> computationally expensive due to the quadratic complexity inherent in <b>self-attention</b> when applied over large-scale <b>graphs,</b> <b>especially</b> <b>for</b> node tasks. In contrast, spiking neural networks (SNNs), with event-driven and binary spikes properties, can perform energy-efficient computation. In this work, we propose a novel insight into integrating SNNs with <b>Graph</b> <b>Transformers</b> <b>and</b> design a Spiking <b>Graph</b> <b>Attention</b> <b>(SGA)</b> module. The matrix multiplication is replaced by sparse addition and mask operations. The linear complexity enables all-pair node interactions on large-scale <b>graphs</b> <b>with</b> <b>limited</b> GPU memory. To our knowledge, our work is the first attempt to introduce SNNs into <b>Graph</b> <b>Transformers.</b> <b>Furthermore,</b> we design SpikeGraphormer, a Dual-branch architecture, combining a sparse <b>GNN</b> branch with our SGA-driven <b>Graph</b> <b>Transformer</b> <b>branch,</b> which can simultaneously perform all-pair node interactions and capture local neighborhoods. SpikeGraphormer consistently outperforms existing state-of-the-art approaches across various datasets and makes substantial improvements in training time, inference time, and GPU memory cost (10 ~ 20x lower than vanilla <b>self-attention).</b> It also performs well in cross-domain applications (image and <b>text</b> <b>classification).</b> We release our code at <a href=https://github.com/PHD-lanyu/SpikeGraphormer>https://github.com/PHD-lanyu/SpikeGraphormer</a>.</p></p class="citation"></blockquote><h3 id=28--217312-spikingresformer-bridging-resnet-and-vision-transformer-in-spiking-neural-networks-xinyu-shi-et-al-2024>(2/8 | 217/312) SpikingResformer: Bridging ResNet and Vision Transformer in Spiking Neural Networks (Xinyu Shi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinyu Shi, Zecheng Hao, Zhaofei Yu. (2024)<br><strong>SpikingResformer: Bridging ResNet and Vision Transformer in Spiking Neural Networks</strong><br><button class=copy-to-clipboard title="SpikingResformer: Bridging ResNet and Vision Transformer in Spiking Neural Networks" index=217>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-217 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-CV, cs-LG, cs-NE, cs.NE<br>Keyword Score: 40<br>Keywords: Vision Transformer, Transformer, Self-Attention, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14302v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14302v1.pdf filename=2403.14302v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The remarkable success of <b>Vision</b> <b>Transformers</b> in Artificial Neural Networks (ANNs) has led to a growing interest in incorporating the <b>self-attention</b> mechanism and <b>transformer-based</b> architecture into Spiking Neural Networks (SNNs). While existing methods propose spiking <b>self-attention</b> mechanisms that are compatible with SNNs, they lack reasonable scaling methods, and the overall architectures proposed by these methods suffer from a bottleneck in effectively extracting local features. To address these challenges, we propose a novel spiking <b>self-attention</b> mechanism named Dual Spike <b>Self-Attention</b> (DSSA) with a reasonable scaling method. Based on DSSA, we propose a novel spiking <b>Vision</b> <b>Transformer</b> architecture called SpikingResformer, which combines the ResNet-based multi-stage architecture with our proposed DSSA to improve both performance and energy efficiency while reducing parameters. Experimental results show that SpikingResformer achieves higher accuracy with fewer parameters and lower energy consumption than other spiking <b>Vision</b> <b>Transformer</b> counterparts. Notably, our SpikingResformer-L achieves 79.40% top-1 accuracy on ImageNet with 4 time-steps, which is the state-of-the-art result in the SNN field.</p></p class="citation"></blockquote><h3 id=38--218312-reactor-optimization-benchmark-by-reinforcement-learning-deborah-schwarcz-et-al-2024>(3/8 | 218/312) Reactor Optimization Benchmark by Reinforcement Learning (Deborah Schwarcz et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Deborah Schwarcz, Nadav Schneider, Gal Oren, Uri Steinitz. (2024)<br><strong>Reactor Optimization Benchmark by Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Reactor Optimization Benchmark by Reinforcement Learning" index=218>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-218 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-AI, cs-NE, cs.NE<br>Keyword Score: 33<br>Keywords: Benchmarking, Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14273v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14273v1.pdf filename=2403.14273v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neutronic calculations for reactors are a daunting task when using Monte Carlo (MC) methods. As high-performance computing has advanced, the <b>simulation</b> of a reactor is nowadays more readily done, but design and optimization with multiple parameters is still a computational challenge. MC transport <b>simulations,</b> coupled with machine learning techniques, offer promising avenues for enhancing the efficiency and effectiveness of nuclear reactor optimization. This paper introduces a novel <b>benchmark</b> problem within the OpenNeoMC framework designed specifically for <b>reinforcement</b> <b>learning.</b> The <b>benchmark</b> involves optimizing a unit cell of a research reactor with two varying parameters (fuel density and water spacing) to maximize neutron flux while maintaining reactor criticality. The test case features distinct local optima, representing different physical regimes, thus posing a challenge for learning algorithms. Through extensive <b>simulations</b> utilizing evolutionary and neuroevolutionary algorithms, we demonstrate the effectiveness of <b>reinforcement</b> <b>learning</b> in navigating complex optimization landscapes with strict constraints. Furthermore, we propose acceleration techniques within the OpenNeoMC framework, including model updating and cross-section usage by RAM utilization, to expedite <b>simulation</b> times. Our findings emphasize the importance of machine learning integration in reactor optimization and contribute to advancing methodologies for addressing intricate optimization challenges in nuclear engineering. The sources of this work are available at our GitHub repository: <a href=https://github.com/Scientific-Computing-Lab-NRCN/RLOpenNeoMC>https://github.com/Scientific-Computing-Lab-NRCN/RLOpenNeoMC</a></p></p class="citation"></blockquote><h3 id=48--219312-model-uncertainty-in-evolutionary-optimization-and-bayesian-optimization-a-comparative-analysis-hao-hao-et-al-2024>(4/8 | 219/312) Model Uncertainty in Evolutionary Optimization and Bayesian Optimization: A Comparative Analysis (Hao Hao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hao Hao, Xiaoqun Zhang, Aimin Zhou. (2024)<br><strong>Model Uncertainty in Evolutionary Optimization and Bayesian Optimization: A Comparative Analysis</strong><br><button class=copy-to-clipboard title="Model Uncertainty in Evolutionary Optimization and Bayesian Optimization: A Comparative Analysis" index=219>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-219 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-LG, cs-NE, cs.NE<br>Keyword Score: 25<br>Keywords: Black Box, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14413v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14413v2.pdf filename=2403.14413v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Black-box</b> <b>optimization</b> problems, which are common in many real-world applications, require optimization through input-output interactions without access to internal workings. This often leads to significant computational resources being consumed for <b>simulations.</b> Bayesian Optimization (BO) and Surrogate-Assisted Evolutionary Algorithm (SAEA) are two widely used gradient-free optimization techniques employed to address such challenges. Both approaches follow a similar iterative procedure that relies on surrogate models to guide the search process. This paper aims to elucidate the similarities and differences in the utilization of model uncertainty between these two methods, as well as the impact of model inaccuracies on algorithmic performance. A novel model-assisted strategy is introduced, which utilizes unevaluated solutions to generate offspring, leveraging the population-based search capabilities of evolutionary algorithm to enhance the effectiveness of model-assisted optimization. Experimental results demonstrate that the proposed approach outperforms mainstream Bayesian optimization algorithms in terms of accuracy and efficiency.</p></p class="citation"></blockquote><h3 id=58--220312-a-reinforcement-learning-guided-hybrid-evolutionary-algorithm-for-the-latency-location-routing-problem-yuji-zou-et-al-2024>(5/8 | 220/312) A reinforcement learning guided hybrid evolutionary algorithm for the latency location routing problem (Yuji Zou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuji Zou, Jin-Kao Hao, Qinghua Wu. (2024)<br><strong>A reinforcement learning guided hybrid evolutionary algorithm for the latency location routing problem</strong><br><button class=copy-to-clipboard title="A reinforcement learning guided hybrid evolutionary algorithm for the latency location routing problem" index=220>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-220 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-DM, cs-NE, cs.NE<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14405v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14405v1.pdf filename=2403.14405v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The latency location routing problem integrates the facility location problem and the multi-depot cumulative capacitated vehicle routing problem. This problem involves making simultaneous decisions about depot locations and vehicle routes to serve customers while aiming to minimize the sum of waiting (arriving) times for all customers. To address this computationally challenging problem, we propose a <b>reinforcement</b> <b>learning</b> guided hybrid evolutionary algorithm following the framework of the memetic algorithm. The proposed algorithm relies on a diversity-enhanced multi-parent edge assembly crossover to build promising offspring and a <b>reinforcement</b> <b>learning</b> guided variable neighborhood descent to determine the exploration order of multiple neighborhoods. Additionally, strategic oscillation is used to achieve a balanced exploration of both feasible and infeasible solutions. The competitiveness of the algorithm against state-of-the-art methods is demonstrated by experimental results on the three sets of 76 popular instances, including 51 improved best solutions (new upper bounds) for the 59 instances with unknown optima and equal best results for the remaining instances. We also conduct additional experiments to shed light on the key components of the algorithm.</p></p class="citation"></blockquote><h3 id=68--221312-stitching-for-neuroevolution-recombining-deep-neural-networks-without-breaking-them-arthur-guijt-et-al-2024>(6/8 | 221/312) Stitching for Neuroevolution: Recombining Deep Neural Networks without Breaking Them (Arthur Guijt et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Arthur Guijt, Dirk Thierens, Tanja Alderliesten, Peter A. N. Bosman. (2024)<br><strong>Stitching for Neuroevolution: Recombining Deep Neural Networks without Breaking Them</strong><br><button class=copy-to-clipboard title="Stitching for Neuroevolution: Recombining Deep Neural Networks without Breaking Them" index=221>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-221 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-NE, cs.NE<br>Keyword Score: 10<br>Keywords: Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14224v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14224v1.pdf filename=2403.14224v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Traditional approaches to neuroevolution often start from scratch. This becomes prohibitively expensive in terms of computational and data requirements when targeting modern, deep neural networks. Using a warm start could be highly advantageous, e.g., using previously trained networks, potentially from different sources. This moreover enables leveraging the benefits of <b>transfer</b> <b>learning</b> (in particular vastly reduced training effort). However, recombining trained networks is non-trivial because architectures and feature representations typically differ. Consequently, a straightforward exchange of layers tends to lead to a performance breakdown. We overcome this by matching the layers of parent networks based on their connectivity, identifying potential crossover points. To correct for differing feature representations between these layers we employ stitching, which merges the networks by introducing new layers at crossover points. To train the merged network, only stitching layers need to be considered. New networks can then be created by selecting a subnetwork by choosing which stitching layers to (not) use. Assessing their performance is efficient as only their evaluation on data is required. We experimentally show that our approach enables finding networks that represent novel trade-offs between performance and computational cost, with some even dominating the original networks.</p></p class="citation"></blockquote><h3 id=78--222312-an-analysis-of-the-preferences-of-distribution-indicators-in-evolutionary-multi-objective-optimization-jesús-guillermo-falcón-cardona-et-al-2024>(7/8 | 222/312) An Analysis of the Preferences of Distribution Indicators in Evolutionary Multi-Objective Optimization (Jesús Guillermo Falcón-Cardona et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jesús Guillermo Falcón-Cardona, Mahboubeh Nezhadmoghaddam, Emilio Bernal-Zubieta. (2024)<br><strong>An Analysis of the Preferences of Distribution Indicators in Evolutionary Multi-Objective Optimization</strong><br><button class=copy-to-clipboard title="An Analysis of the Preferences of Distribution Indicators in Evolutionary Multi-Objective Optimization" index=222>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-222 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-NE, cs.NE<br>Keyword Score: 3<br>Keywords: Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14838v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14838v1.pdf filename=2403.14838v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The distribution of objective vectors in a Pareto Front Approximation (PFA) is crucial for representing the associated manifold accurately. Distribution Indicators (DIs) assess the distribution of a PFA numerically, utilizing concepts like distance calculation, Biodiversity, Entropy, Potential Energy, or <b>Clustering.</b> Despite the diversity of DIs, their strengths and weaknesses across assessment scenarios are not well-understood. This paper introduces a taxonomy for classifying DIs, followed by a preference analysis of nine DIs, each representing a category in the taxonomy. Experimental results, considering various PFAs under controlled scenarios (loss of coverage, loss of uniformity, pathological distributions), reveal that some DIs can be misleading and need cautious use. Additionally, DIs based on Biodiversity and Potential Energy show promise for PFA evaluation and comparison of Multi-Objective Evolutionary Algorithms.</p></p class="citation"></blockquote><h3 id=88--223312-evolving-benchmark-functions-to-compare-evolutionary-algorithms-via-genetic-programming-yifan-he-et-al-2024>(8/8 | 223/312) Evolving Benchmark Functions to Compare Evolutionary Algorithms via Genetic Programming (Yifan He et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yifan He, Claus Aranha. (2024)<br><strong>Evolving Benchmark Functions to Compare Evolutionary Algorithms via Genetic Programming</strong><br><button class=copy-to-clipboard title="Evolving Benchmark Functions to Compare Evolutionary Algorithms via Genetic Programming" index=223>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-223 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-AI, cs-NE, cs.NE<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14146v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14146v1.pdf filename=2403.14146v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this study, we use Genetic Programming (GP) to compose new optimization <b>benchmark</b> functions. Optimization <b>benchmarks</b> have the important role of showing the differences between evolutionary algorithms, making it possible for further analysis and comparisons. We show that the <b>benchmarks</b> generated by GP are able to differentiate algorithms better than human-made <b>benchmark</b> functions. The fitness measure of the GP is the Wasserstein distance of the solutions found by a pair of optimizers. Additionally, we use MAP-Elites to both enhance the search power of the GP and also illustrate how the difference between optimizers changes by various landscape features. Our approach provides a novel way to automate the design of <b>benchmark</b> functions and to compare evolutionary algorithms.</p></p class="citation"></blockquote><h2 id=cshc-4>cs.HC (4)</h2><h3 id=14--224312-empowering-personalized-learning-through-a-conversation-based-tutoring-system-with-student-modeling-minju-park-et-al-2024>(1/4 | 224/312) Empowering Personalized Learning through a Conversation-based Tutoring System with Student Modeling (Minju Park et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Minju Park, Sojung Kim, Seunghyun Lee, Soonwoo Kwon, Kyuseok Kim. (2024)<br><strong>Empowering Personalized Learning through a Conversation-based Tutoring System with Student Modeling</strong><br><button class=copy-to-clipboard title="Empowering Personalized Learning through a Conversation-based Tutoring System with Student Modeling" index=224>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-224 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 50<br>Keywords: Few-shot, Zero-shot, Reasoning, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14071v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14071v1.pdf filename=2403.14071v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As the recent Large Language Models(LLM&rsquo;s) become increasingly competent in <b>zero-shot</b> and <b>few-shot</b> <b>reasoning</b> across various domains, educators are showing a growing interest in leveraging these <b>LLM&rsquo;s</b> in conversation-based tutoring systems. However, building a conversation-based personalized tutoring system poses considerable challenges in accurately assessing the student and strategically incorporating the assessment into teaching within the conversation. In this paper, we discuss design considerations for a personalized tutoring system that involves the following two key components: (1) a student modeling with diagnostic components, and (2) a conversation-based tutor utilizing <b>LLM</b> with <b>prompt</b> engineering that incorporates student assessment outcomes and various instructional strategies. Based on these design considerations, we created a proof-of-concept tutoring system focused on personalization and tested it with 20 participants. The results substantiate that our system&rsquo;s framework facilitates personalization, with particular emphasis on the elements constituting student modeling. A web demo of our system is available at <a href=http://rlearning-its.com>http://rlearning-its.com</a>.</p></p class="citation"></blockquote><h3 id=24--225312-peergpt-probing-the-roles-of-llm-based-peer-agents-as-team-moderators-and-participants-in-childrens-collaborative-learning-jiawen-liu-et-al-2024>(2/4 | 225/312) PeerGPT: Probing the Roles of LLM-based Peer Agents as Team Moderators and Participants in Children&rsquo;s Collaborative Learning (Jiawen Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiawen Liu, Yuanyuan Yao, Pengcheng An, Qi Wang. (2024)<br><strong>PeerGPT: Probing the Roles of LLM-based Peer Agents as Team Moderators and Participants in Children&rsquo;s Collaborative Learning</strong><br><button class=copy-to-clipboard title="PeerGPT: Probing the Roles of LLM-based Peer Agents as Team Moderators and Participants in Children's Collaborative Learning" index=225>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-225 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-AI, cs-HC, cs.HC<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14227v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14227v1.pdf filename=2403.14227v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In children&rsquo;s collaborative learning, effective peer conversations can significantly enhance the quality of children&rsquo;s collaborative interactions. The integration of <b>Large</b> <b>Language</b> <b>Model</b> <b>(LLM)</b> agents into this setting explores their novel role as peers, assessing impacts as team moderators and participants. We invited two groups of participants to engage in a collaborative learning workshop, where they discussed and proposed conceptual solutions to a design problem. The peer conversation transcripts were analyzed using thematic analysis. We discovered that peer agents, while managing discussions effectively as team moderators, sometimes have their instructions disregarded. As participants, they foster children&rsquo;s creative thinking but may not consistently provide timely feedback. These findings highlight potential design improvements and considerations for peer agents in both roles.</p></p class="citation"></blockquote><h3 id=34--226312-how-human-centered-explainable-ai-interface-are-designed-and-evaluated-a-systematic-survey-thu-nguyen-et-al-2024>(3/4 | 226/312) How Human-Centered Explainable AI Interface Are Designed and Evaluated: A Systematic Survey (Thu Nguyen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Thu Nguyen, Alessandro Canossa, Jichen Zhu. (2024)<br><strong>How Human-Centered Explainable AI Interface Are Designed and Evaluated: A Systematic Survey</strong><br><button class=copy-to-clipboard title="How Human-Centered Explainable AI Interface Are Designed and Evaluated: A Systematic Survey" index=226>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-226 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-AI, cs-HC, cs.HC<br>Keyword Score: 10<br>Keywords: Explainable AI<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14496v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14496v1.pdf filename=2403.14496v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite its technological breakthroughs, <b>eXplainable</b> <b>Artificial</b> Intelligence (XAI) research has limited success in producing the {\em effective explanations} needed by users. In order to improve XAI systems&rsquo; usability, practical interpretability, and efficacy for real users, the emerging area of {\em <b>Explainable</b> <b>Interfaces}</b> (EIs) focuses on the user interface and user experience design aspects of XAI. This paper presents a systematic survey of 53 publications to identify current trends in human-XAI interaction and promising directions for EI design and development. This is among the first systematic survey of EI research.</p></p class="citation"></blockquote><h3 id=44--227312-recourse-for-reclamation-chatting-with-generative-language-models-jennifer-chien-et-al-2024>(4/4 | 227/312) Recourse for reclamation: Chatting with generative language models (Jennifer Chien et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jennifer Chien, Kevin R. McKee, Jackie Kay, William Isaac. (2024)<br><strong>Recourse for reclamation: Chatting with generative language models</strong><br><button class=copy-to-clipboard title="Recourse for reclamation: Chatting with generative language models" index=227>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-227 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-CL, cs-CY, cs-HC, cs.HC<br>Keyword Score: 10<br>Keywords: Information Retrieval<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14467v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14467v1.pdf filename=2403.14467v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Researchers and developers increasingly rely on toxicity scoring to moderate generative language model outputs, in settings such as customer service, <b>information</b> <b>retrieval,</b> and content generation. However, toxicity scoring may render pertinent <b>information</b> <b>inaccessible,</b> rigidify or &ldquo;value-lock&rdquo; cultural norms, and prevent language reclamation processes, particularly for marginalized people. In this work, we extend the concept of algorithmic recourse to generative language models: we provide users a novel mechanism to achieve their desired prediction by dynamically setting thresholds for toxicity filtering. Users thereby exercise increased agency relative to interactions with the baseline system. A pilot study ($n = 30$) supports the potential of our proposed recourse mechanism, indicating improvements in usability compared to fixed-threshold toxicity-filtering of model outputs. Future work should explore the intersection of toxicity scoring, model controllability, user agency, and language reclamation processes &ndash; particularly with regard to the bias that many communities encounter when interacting with generative language models.</p></p class="citation"></blockquote><h2 id=cssd-5>cs.SD (5)</h2><h3 id=15--228312-xlavs-r-cross-lingual-audio-visual-speech-representation-learning-for-noise-robust-speech-perception-hyojung-han-et-al-2024>(1/5 | 228/312) XLAVS-R: Cross-Lingual Audio-Visual Speech Representation Learning for Noise-Robust Speech Perception (HyoJung Han et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>HyoJung Han, Mohamed Anwar, Juan Pino, Wei-Ning Hsu, Marine Carpuat, Bowen Shi, Changhan Wang. (2024)<br><strong>XLAVS-R: Cross-Lingual Audio-Visual Speech Representation Learning for Noise-Robust Speech Perception</strong><br><button class=copy-to-clipboard title="XLAVS-R: Cross-Lingual Audio-Visual Speech Representation Learning for Noise-Robust Speech Perception" index=228>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-228 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-CL, cs-SD, cs.SD, eess-AS<br>Keyword Score: 48<br>Keywords: Benchmarking, Fine-tuning, Representation Learning, Zero-shot, Automatic Speech Recognition, BLEU<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14402v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14402v1.pdf filename=2403.14402v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Speech</b> <b>recognition</b> and translation systems perform poorly on noisy inputs, which are frequent in realistic environments. Augmenting these systems with visual signals has the potential to improve robustness to noise. However, audio-visual (AV) data is only available in limited amounts and for fewer languages than audio-only resources. To address this gap, we present XLAVS-R, a cross-lingual audio-visual <b>speech</b> <b>representation</b> <b>model</b> for noise-robust <b>speech</b> <b>recognition</b> and translation in over 100 languages. It is designed to maximize the benefits of limited multilingual AV pre-training data, by building on top of audio-only multilingual pre-training and simplifying existing pre-training schemes. Extensive evaluation on the MuAViC <b>benchmark</b> shows the strength of XLAVS-R on downstream audio-visual <b>speech</b> <b>recognition</b> and translation tasks, where it outperforms the previous state of the art by up to 18.5% WER and 4.7 <b>BLEU</b> given noisy AV inputs, and enables strong <b>zero-shot</b> audio-visual ability with audio-only <b>fine-tuning.</b></p></p class="citation"></blockquote><h3 id=25--229312-exploring-green-ai-for-audio-deepfake-detection-subhajit-saha-et-al-2024>(2/5 | 229/312) Exploring Green AI for Audio Deepfake Detection (Subhajit Saha et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Subhajit Saha, Md Sahidullah, Swagatam Das. (2024)<br><strong>Exploring Green AI for Audio Deepfake Detection</strong><br><button class=copy-to-clipboard title="Exploring Green AI for Audio Deepfake Detection" index=229>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-229 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-CV, cs-LG, cs-SD, cs.SD, eess-AS<br>Keyword Score: 40<br>Keywords: Fine-tuning, Logistic Regression, Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14290v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14290v1.pdf filename=2403.14290v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The state-of-the-art audio deepfake detectors leveraging deep neural networks exhibit impressive recognition performance. Nonetheless, this advantage is accompanied by a significant carbon footprint. This is mainly due to the use of high-performance computing with accelerators and high training time. Studies show that average deep NLP model produces around 626k lbs of CO\textsubscript{2} which is equivalent to five times of average US car emission at its lifetime. This is certainly a massive threat to the environment. To tackle this challenge, this study presents a novel framework for audio deepfake detection that can be seamlessly trained using standard CPU resources. Our proposed framework utilizes off-the-shelve <b>self-supervised</b> <b>learning</b> (SSL) based models which are pre-trained and available in public repositories. In contrast to existing methods that <b>fine-tune</b> SSL models and employ additional deep neural networks for downstream tasks, we exploit classical machine learning algorithms such as <b>logistic</b> <b>regression</b> and shallow neural networks using the SSL embeddings extracted using the pre-trained model. Our approach shows competitive results compared to the commonly used high-carbon footprint approaches. In experiments with the ASVspoof 2019 LA dataset, we achieve a 0.90% equal error rate (EER) with less than 1k trainable model parameters. To encourage further research in this direction and support reproducible results, the Python code will be made publicly accessible following acceptance. Github: <a href=https://github.com/sahasubhajit/Speech-Spoofing->https://github.com/sahasubhajit/Speech-Spoofing-</a></p></p class="citation"></blockquote><h3 id=35--230312-emodarts-joint-optimisation-of-cnn--sequential-neural-network-architectures-for-superior-speech-emotion-recognition-thejan-rajapakshe-et-al-2024>(3/5 | 230/312) emoDARTS: Joint Optimisation of CNN & Sequential Neural Network Architectures for Superior Speech Emotion Recognition (Thejan Rajapakshe et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Thejan Rajapakshe, Rajib Rana, Sara Khalifa, Berrak Sisman, Bjorn W. Schuller, Carlos Busso. (2024)<br><strong>emoDARTS: Joint Optimisation of CNN & Sequential Neural Network Architectures for Superior Speech Emotion Recognition</strong><br><button class=copy-to-clipboard title="emoDARTS: Joint Optimisation of CNN & Sequential Neural Network Architectures for Superior Speech Emotion Recognition" index=230>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-230 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-LG, cs-SD, cs.SD, eess-AS<br>Keyword Score: 40<br>Keywords: Convolutional Neural Network, LSTM, Recurrent Neural Network, Emotion Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14083v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14083v1.pdf filename=2403.14083v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Speech <b>Emotion</b> <b>Recognition</b> (SER) is crucial for enabling computers to understand the <b>emotions</b> <b>conveyed</b> in human communication. With recent advancements in Deep Learning (DL), the performance of SER models has significantly improved. However, designing an optimal DL architecture requires specialised knowledge and experimental assessments. Fortunately, Neural Architecture Search (NAS) provides a potential solution for automatically determining the best DL model. The Differentiable Architecture Search (DARTS) is a particularly efficient method for discovering optimal models. This study presents emoDARTS, a DARTS-optimised joint <b>CNN</b> and Sequential Neural Network (SeqNN: <b>LSTM,</b> <b>RNN)</b> architecture that enhances SER performance. The literature supports the selection of <b>CNN</b> and <b>LSTM</b> coupling to improve performance. While DARTS has previously been used to choose <b>CNN</b> and <b>LSTM</b> operations independently, our technique adds a novel mechanism for selecting <b>CNN</b> and SeqNN operations in conjunction using DARTS. Unlike earlier work, we do not impose limits on the layer order of the <b>CNN.</b> Instead, we let DARTS choose the best layer order inside the DARTS cell. We demonstrate that emoDARTS outperforms conventionally designed <b>CNN-LSTM</b> models and surpasses the best-reported SER results achieved through DARTS on <b>CNN-LSTM</b> by evaluating our approach on the IEMOCAP, MSP-IMPROV, and MSP-Podcast datasets.</p></p class="citation"></blockquote><h3 id=45--231312-the-neurips-2023-machine-learning-for-audio-workshop-affective-audio-benchmarks-and-novel-data-alice-baird-et-al-2024>(4/5 | 231/312) The NeurIPS 2023 Machine Learning for Audio Workshop: Affective Audio Benchmarks and Novel Data (Alice Baird et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alice Baird, Rachel Manzelli, Panagiotis Tzirakis, Chris Gagne, Haoqi Li, Sadie Allen, Sander Dieleman, Brian Kulis, Shrikanth S. Narayanan, Alan Cowen. (2024)<br><strong>The NeurIPS 2023 Machine Learning for Audio Workshop: Affective Audio Benchmarks and Novel Data</strong><br><button class=copy-to-clipboard title="The NeurIPS 2023 Machine Learning for Audio Workshop: Affective Audio Benchmarks and Novel Data" index=231>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-231 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-CL, cs-SD, cs.SD, eess-AS<br>Keyword Score: 23<br>Keywords: Benchmarking, Emotion Recognition, Event Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14048v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14048v1.pdf filename=2403.14048v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The NeurIPS 2023 Machine Learning for Audio Workshop brings together machine learning (ML) experts from various audio domains. There are several valuable audio-driven ML tasks, from speech <b>emotion</b> <b>recognition</b> to audio <b>event</b> <b>detection,</b> but the community is sparse compared to other ML areas, e.g., computer vision or natural language processing. A major limitation with audio is the available data; with audio being a time-dependent modality, high-quality data collection is time-consuming and costly, making it challenging for academic groups to apply their often state-of-the-art strategies to a larger, more generalizable dataset. In this short white paper, to encourage researchers with limited access to large-datasets, the organizers first outline several open-source datasets that are available to the community, and for the duration of the workshop are making several propriety datasets available. Namely, three vocal datasets, Hume-Prosody, Hume-VocalBurst, an acted <b>emotional</b> <b>speech</b> dataset Modulate-Sonata, and an in-game streamer dataset Modulate-Stream. We outline the current baselines on these datasets but encourage researchers from across audio to utilize them outside of the initial baseline tasks.</p></p class="citation"></blockquote><h3 id=55--232312-assessing-the-robustness-of-spectral-clustering-for-deep-speaker-diarization-nikhil-raghav-et-al-2024>(5/5 | 232/312) Assessing the Robustness of Spectral Clustering for Deep Speaker Diarization (Nikhil Raghav et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nikhil Raghav, Md Sahidullah. (2024)<br><strong>Assessing the Robustness of Spectral Clustering for Deep Speaker Diarization</strong><br><button class=copy-to-clipboard title="Assessing the Robustness of Spectral Clustering for Deep Speaker Diarization" index=232>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-232 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-CV, cs-LG, cs-SD, cs.SD, eess-AS<br>Keyword Score: 3<br>Keywords: Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14286v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14286v1.pdf filename=2403.14286v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Clustering</b> speaker embeddings is crucial in speaker diarization but hasn&rsquo;t received as much focus as other components. Moreover, the robustness of speaker diarization across various datasets hasn&rsquo;t been explored when the development and evaluation data are from different domains. To bridge this gap, this study thoroughly examines spectral <b>clustering</b> for both same-domain and cross-domain speaker diarization. Our extensive experiments on two widely used corpora, AMI and DIHARD, reveal the performance trend of speaker diarization in the presence of domain mismatch. We observe that the performance difference between two different domain conditions can be attributed to the role of spectral <b>clustering.</b> In particular, keeping other modules unchanged, we show that differences in optimal tuning parameters as well as speaker count estimation originates due to the mismatch. This study opens several future directions for speaker diarization research.</p></p class="citation"></blockquote><h2 id=astro-phim-1>astro-ph.IM (1)</h2><h3 id=11--233312-a-classifier-based-approach-to-multi-class-anomaly-detection-for-astronomical-transients-rithwik-gupta-et-al-2024>(1/1 | 233/312) A Classifier-Based Approach to Multi-Class Anomaly Detection for Astronomical Transients (Rithwik Gupta et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rithwik Gupta, Daniel Muthukrishna, Michelle Lochner. (2024)<br><strong>A Classifier-Based Approach to Multi-Class Anomaly Detection for Astronomical Transients</strong><br><button class=copy-to-clipboard title="A Classifier-Based Approach to Multi-Class Anomaly Detection for Astronomical Transients" index=233>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-233 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: astro-ph.IM<br>Categories: astro-ph-HE, astro-ph-IM, astro-ph.IM, cs-LG<br>Keyword Score: 45<br>Keywords: Anomaly Detection, Representation Learning, Simulation, Simulator, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14742v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14742v1.pdf filename=2403.14742v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Automating real-time <b>anomaly</b> <b>detection</b> is essential for identifying rare transients in the era of large-scale astronomical surveys. Modern survey telescopes are generating tens of thousands of alerts per night, and future telescopes, such as the Vera C. Rubin Observatory, are projected to increase this number dramatically. Currently, most <b>anomaly</b> <b>detection</b> algorithms for astronomical transients rely either on hand-crafted features extracted from light curves or on features generated through <b>unsupervised</b> <b>representation</b> <b>learning,</b> which are then coupled with standard machine learning <b>anomaly</b> <b>detection</b> algorithms. In this work, we introduce an alternative approach to detecting anomalies: using the penultimate layer of a neural network classifier as the latent space for <b>anomaly</b> <b>detection.</b> We then propose a novel method, named Multi-Class Isolation Forests (MCIF), which trains separate isolation forests for each class to derive an <b>anomaly</b> <b>score</b> for a light curve from the latent space <b>representation</b> <b>given</b> by the classifier. This approach significantly outperforms a standard isolation forest. We also use a simpler input method for real-time transient classifiers which circumvents the need for interpolation in light curves and helps the neural network model inter-passband relationships and handle irregular sampling. Our <b>anomaly</b> <b>detection</b> pipeline identifies rare classes including kilonovae, pair-instability supernovae, and intermediate luminosity transients shortly after trigger on simulated Zwicky Transient Facility light curves. Using a sample of our <b>simulations</b> that matched the population of anomalies expected in nature (54 anomalies and 12,040 common transients), our method was able to discover $41\pm3$ anomalies (~75% recall) after following up the top 2000 (~15%) ranked transients. Our novel method shows that classifiers can be effectively repurposed for real-time <b>anomaly</b> <b>detection.</b></p></p class="citation"></blockquote><h2 id=statml-6>stat.ML (6)</h2><h3 id=16--234312-curvature-augmented-manifold-embedding-and-learning-yongming-liu-2024>(1/6 | 234/312) Curvature Augmented Manifold Embedding and Learning (Yongming Liu, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yongming Liu. (2024)<br><strong>Curvature Augmented Manifold Embedding and Learning</strong><br><button class=copy-to-clipboard title="Curvature Augmented Manifold Embedding and Learning" index=234>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-234 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-HC, cs-LG, stat-ML, stat.ML<br>Keyword Score: 43<br>Keywords: Benchmarking, Supervised Learning, Supervised Learning, Unsupervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14813v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14813v1.pdf filename=2403.14813v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A new dimensional reduction (DR) and data visualization method, Curvature-Augmented Manifold Embedding and Learning (CAMEL), is proposed. The key novel contribution is to formulate the DR problem as a mechanistic/physics model, where the force field among nodes (data points) is used to find an n-dimensional manifold representation of the data sets. Compared with many existing attractive-repulsive force-based methods, one unique contribution of the proposed method is to include a non-pairwise force. A new force field model is introduced and discussed, inspired by the multi-body potential in lattice-particle physics and Riemann curvature in topology. A curvature-augmented force is included in CAMEL. Following this, CAMEL formulation for <b>unsupervised</b> <b>learning,</b> <b>supervised</b> <b>learning,</b> semi-supervised learning/metric learning, and inverse learning are provided. Next, CAMEL is applied to many <b>benchmark</b> datasets by comparing existing models, such as tSNE, UMAP, TRIMAP, and PacMap. Both visual comparison and metrics-based evaluation are performed. 14 open literature and self-proposed metrics are employed for a comprehensive comparison. Conclusions and future work are suggested based on the current investigation. Related code and demonstration are available on <a href=https://github.com/ymlasu/CAMEL>https://github.com/ymlasu/CAMEL</a> for interested readers to reproduce the results and other applications.</p></p class="citation"></blockquote><h3 id=26--235312-automatic-outlier-rectification-via-optimal-transport-jose-blanchet-et-al-2024>(2/6 | 235/312) Automatic Outlier Rectification via Optimal Transport (Jose Blanchet et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jose Blanchet, Jiajin Li, Markus Pelger, Greg Zanotti. (2024)<br><strong>Automatic Outlier Rectification via Optimal Transport</strong><br><button class=copy-to-clipboard title="Automatic Outlier Rectification via Optimal Transport" index=235>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-235 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, math-OC, stat-ME, stat-ML, stat.ML<br>Keyword Score: 30<br>Keywords: Outlier Detection, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14067v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14067v1.pdf filename=2403.14067v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we propose a novel conceptual framework to detect <b>outliers</b> <b>using</b> optimal transport with a concave cost function. Conventional <b>outlier</b> <b>detection</b> approaches typically use a two-stage procedure: first, <b>outliers</b> <b>are</b> detected and removed, and then estimation is performed on the cleaned data. However, this approach does not inform <b>outlier</b> <b>removal</b> with the estimation task, leaving room for improvement. To address this limitation, we propose an automatic <b>outlier</b> <b>rectification</b> mechanism that integrates rectification and estimation within a joint optimization framework. We take the first step to utilize an optimal transport distance with a concave cost function to construct a rectification set in the space of probability distributions. Then, we select the best distribution within the rectification set to perform the estimation task. Notably, the concave cost function we introduced in this paper is the key to making our estimator effectively identify the <b>outlier</b> <b>during</b> the optimization process. We discuss the fundamental differences between our estimator and optimal transport-based distributionally robust optimization estimator. finally, we demonstrate the effectiveness and superiority of our approach over conventional approaches in extensive <b>simulation</b> and empirical analyses for mean estimation, least absolute regression, and the fitting of option implied volatility surfaces.</p></p class="citation"></blockquote><h3 id=36--236312-deep-clustering-evaluation-how-to-validate-internal-clustering-validation-measures-zeya-wang-et-al-2024>(3/6 | 236/312) Deep Clustering Evaluation: How to Validate Internal Clustering Validation Measures (Zeya Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zeya Wang, Chenglong Ye. (2024)<br><strong>Deep Clustering Evaluation: How to Validate Internal Clustering Validation Measures</strong><br><button class=copy-to-clipboard title="Deep Clustering Evaluation: How to Validate Internal Clustering Validation Measures" index=236>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-236 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 13<br>Keywords: Clustering, Stemming<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14830v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14830v1.pdf filename=2403.14830v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep <b>clustering,</b> a method for partitioning complex, high-dimensional data using deep neural networks, presents unique evaluation challenges. Traditional <b>clustering</b> validation measures, designed for low-dimensional spaces, are problematic for deep <b>clustering,</b> which involves projecting data into lower-dimensional embeddings before partitioning. Two key issues are identified: 1) the curse of dimensionality when applying these measures to raw data, and 2) the unreliable comparison of <b>clustering</b> results across different embedding spaces <b>stemming</b> from variations in training procedures and parameter settings in different <b>clustering</b> models. This paper addresses these challenges in evaluating <b>clustering</b> quality in deep learning. We present a theoretical framework to highlight ineffectiveness arising from using internal validation measures on raw and embedded data and propose a systematic approach to applying <b>clustering</b> validity indices in deep <b>clustering</b> contexts. Experiments show that this framework aligns better with external validation measures, effectively reducing the misguidance from the improper use of <b>clustering</b> validity indices in deep learning.</p></p class="citation"></blockquote><h3 id=46--237312-estimating-causal-effects-with-double-machine-learning----a-method-evaluation-jonathan-fuhr-et-al-2024>(4/6 | 237/312) Estimating Causal Effects with Double Machine Learning &ndash; A Method Evaluation (Jonathan Fuhr et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jonathan Fuhr, Philipp Berens, Dominik Papies. (2024)<br><strong>Estimating Causal Effects with Double Machine Learning &ndash; A Method Evaluation</strong><br><button class=copy-to-clipboard title="Estimating Causal Effects with Double Machine Learning -- A Method Evaluation" index=237>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-237 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, econ-EM, stat-ME, stat-ML, stat.ML<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14385v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14385v1.pdf filename=2403.14385v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The estimation of causal effects with observational data continues to be a very active research area. In recent years, researchers have developed new frameworks which use machine learning to relax classical assumptions necessary for the estimation of causal effects. In this paper, we review one of the most prominent methods - &ldquo;double/debiased machine learning&rdquo; (DML) - and empirically evaluate it by comparing its performance on simulated data relative to more traditional statistical methods, before applying it to real-world data. Our findings indicate that the application of a suitably flexible machine learning algorithm within DML improves the adjustment for various nonlinear confounding relationships. This advantage enables a departure from traditional functional form assumptions typically necessary in causal effect estimation. However, we demonstrate that the method continues to critically depend on standard assumptions about causal structure and identification. When estimating the effects of air pollution on housing prices in our application, we find that DML estimates are consistently larger than estimates of less flexible methods. From our overall results, we provide actionable <b>recommendations</b> for specific choices researchers must make when applying DML in practice.</p></p class="citation"></blockquote><h3 id=56--238312-learning-causal-graphs-using-variable-grouping-according-to-ancestral-relationship-ming-cai-et-al-2024>(5/6 | 238/312) Learning causal graphs using variable grouping according to ancestral relationship (Ming Cai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ming Cai, Hisayuki Hara. (2024)<br><strong>Learning causal graphs using variable grouping according to ancestral relationship</strong><br><button class=copy-to-clipboard title="Learning causal graphs using variable grouping according to ancestral relationship" index=238>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-238 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 6<br>Keywords: Graph, Sample Size<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14125v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14125v1.pdf filename=2403.14125v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Several causal discovery algorithms have been proposed. However, when the <b>sample</b> <b>size</b> is small relative to the number of variables, the accuracy of estimating causal <b>graphs</b> using existing methods decreases. And some methods are not feasible when the <b>sample</b> <b>size</b> is smaller than the number of variables. To circumvent these problems, some researchers proposed causal structure learning algorithms using divide-and-conquer approaches. For learning the entire causal <b>graph,</b> the approaches first split variables into several subsets according to the conditional independence relationships among the variables, then apply a conventional causal discovery algorithm to each subset and merge the estimated results. Since the divide-and-conquer approach reduces the number of variables to which a causal structure learning algorithm is applied, it is expected to improve the estimation accuracy of causal <b>graphs,</b> especially when the <b>sample</b> <b>size</b> is small relative to the number of variables and the model is sparse. However, existing methods are either computationally expensive or do not provide sufficient accuracy when the <b>sample</b> <b>size</b> is small. This paper proposes a new algorithm for grouping variables based the ancestral relationships among the variables, under the LiNGAM assumption, where the causal relationships are linear, and the mutually independent noise are distributed as continuous non-Gaussian distributions. We call the proposed algorithm CAG. The time complexity of the ancestor finding in CAG is shown to be cubic to the number of variables. Extensive computer experiments confirm that the proposed method outperforms the original DirectLiNGAM without grouping variables and other divide-and-conquer approaches not only in estimation accuracy but also in computation time when the <b>sample</b> <b>size</b> is small relative to the number of variables and the model is sparse.</p></p class="citation"></blockquote><h3 id=66--239312-recovering-latent-confounders-from-high-dimensional-proxy-variables-nathan-mankovich-et-al-2024>(6/6 | 239/312) Recovering Latent Confounders from High-dimensional Proxy Variables (Nathan Mankovich et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nathan Mankovich, Homer Durand, Emiliano Diaz, Gherardo Varando, Gustau Camps-Valls. (2024)<br><strong>Recovering Latent Confounders from High-dimensional Proxy Variables</strong><br><button class=copy-to-clipboard title="Recovering Latent Confounders from High-dimensional Proxy Variables" index=239>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-239 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 3<br>Keywords: Sample Size<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14228v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14228v1.pdf filename=2403.14228v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Detecting latent confounders from proxy variables is an essential problem in causal effect estimation. Previous approaches are limited to low-dimensional proxies, sorted proxies, and binary treatments. We remove these assumptions and present a novel Proxy Confounder Factorization (PCF) framework for continuous treatment effect estimation when latent confounders manifest through high-dimensional, mixed proxy variables. For specific <b>sample</b> <b>sizes,</b> our two-step PCF implementation, using Independent Component Analysis (ICA-PCF), and the end-to-end implementation, using Gradient Descent (GD-PCF), achieve high correlation with the latent confounder and low absolute error in causal effect estimation with synthetic datasets in the high <b>sample</b> <b>size</b> regime. Even when faced with climate data, ICA-PCF recovers four components that explain $75.9%$ of the variance in the North Atlantic Oscillation, a known confounder of precipitation patterns in Europe. Code for our PCF implementations and experiments can be found here: <a href=https://github.com/IPL-UV/confound_it>https://github.com/IPL-UV/confound_it</a>. The proposed methodology constitutes a stepping stone towards discovering latent confounders and can be applied to many problems in disciplines dealing with high-dimensional observed proxies, e.g., spatiotemporal fields.</p></p class="citation"></blockquote><h2 id=csro-20>cs.RO (20)</h2><h3 id=120--240312-learning-quadruped-locomotion-using-differentiable-simulation-yunlong-song-et-al-2024>(1/20 | 240/312) Learning Quadruped Locomotion Using Differentiable Simulation (Yunlong Song et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yunlong Song, Sangbae Kim, Davide Scaramuzza. (2024)<br><strong>Learning Quadruped Locomotion Using Differentiable Simulation</strong><br><button class=copy-to-clipboard title="Learning Quadruped Locomotion Using Differentiable Simulation" index=240>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-240 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs.RO<br>Keyword Score: 40<br>Keywords: Reinforcement Learning, Simulation, Simulator, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14864v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14864v2.pdf filename=2403.14864v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While most recent advancements in legged robot control have been driven by model-free <b>reinforcement</b> <b>learning,</b> we explore the potential of differentiable <b>simulation.</b> Differentiable <b>simulation</b> promises faster convergence and more stable training by computing low-variant first-order gradients using the robot model, but so far, its use for legged robot control has remained limited to <b>simulation.</b> The main challenge with differentiable <b>simulation</b> lies in the complex optimization landscape of robotic tasks due to discontinuities in contact-rich environments, e.g., quadruped locomotion. This work proposes a new, differentiable <b>simulation</b> framework to overcome these challenges. The key idea involves decoupling the complex whole-body <b>simulation,</b> which may exhibit discontinuities due to contact, into two separate continuous domains. Subsequently, we align the robot state resulting from the simplified model with a more precise, non-differentiable simulator to maintain sufficient <b>simulation</b> accuracy. Our framework enables learning quadruped walking in minutes using a single simulated robot without any parallelization. When augmented with GPU parallelization, our approach allows the quadruped robot to master diverse locomotion skills, including trot, pace, bound, and gallop, on challenging terrains in minutes. Additionally, our policy achieves robust locomotion performance in the real world <b>zero-shot.</b> To the best of our knowledge, this work represents the first demonstration of using differentiable <b>simulation</b> for controlling a real quadruped robot. This work provides several important insights into using differentiable <b>simulations</b> for legged locomotion in the real world.</p></p class="citation"></blockquote><h3 id=220--241312-physics-based-causal-reasoning-for-safe--robust-next-best-action-selection-in-robot-manipulation-tasks-ricardo-cannizzaro-et-al-2024>(2/20 | 241/312) Physics-Based Causal Reasoning for Safe & Robust Next-Best Action Selection in Robot Manipulation Tasks (Ricardo Cannizzaro et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ricardo Cannizzaro, Michael Groom, Jonathan Routley, Robert Osazuwa Ness, Lars Kunze. (2024)<br><strong>Physics-Based Causal Reasoning for Safe & Robust Next-Best Action Selection in Robot Manipulation Tasks</strong><br><button class=copy-to-clipboard title="Physics-Based Causal Reasoning for Safe & Robust Next-Best Action Selection in Robot Manipulation Tasks" index=241>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-241 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: I-2-9; I-2-8; I-2-3; G-3; I-2-6; I-6-8; I-2-4; I-2-10, cs-AI, cs-LG, cs-RO, cs.RO, stat-AP<br>Keyword Score: 40<br>Keywords: Probabilistic Model, Simulation, Simulator, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14488v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14488v1.pdf filename=2403.14488v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Safe and efficient object manipulation is a key enabler of many real-world robot applications. However, this is challenging because robot operation must be robust to a range of sensor and actuator uncertainties. In this paper, we present a physics-informed causal-inference-based framework for a robot to probabilistically reason about candidate actions in a block stacking task in a partially observable setting. We integrate a physics-based <b>simulation</b> of the rigid-body system dynamics with a causal Bayesian network (CBN) formulation to define a causal generative <b>probabilistic</b> <b>model</b> of the robot decision-making process. Using <b>simulation-based</b> Monte Carlo experiments, we demonstrate our framework&rsquo;s ability to successfully: (1) predict block tower stability with high accuracy (Pred Acc: 88.6%); and, (2) select an approximate next-best action for the block stacking task, for execution by an integrated robot system, achieving 94.2% task success rate. We also demonstrate our framework&rsquo;s suitability for real-world robot systems by demonstrating successful task executions with a domestic support robot, with perception and manipulation sub-system integration. Hence, we show that by embedding physics-based causal <b>reasoning</b> into robots&rsquo; decision-making processes, we can make robot task execution safer, more reliable, and more robust to various types of uncertainty.</p></p class="citation"></blockquote><h3 id=320--242312-distilling-reinforcement-learning-policies-for-interpretable-robot-locomotion-gradient-boosting-machines-and-symbolic-regression-fernando-acero-et-al-2024>(3/20 | 242/312) Distilling Reinforcement Learning Policies for Interpretable Robot Locomotion: Gradient Boosting Machines and Symbolic Regression (Fernando Acero et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fernando Acero, Zhibin Li. (2024)<br><strong>Distilling Reinforcement Learning Policies for Interpretable Robot Locomotion: Gradient Boosting Machines and Symbolic Regression</strong><br><button class=copy-to-clipboard title="Distilling Reinforcement Learning Policies for Interpretable Robot Locomotion: Gradient Boosting Machines and Symbolic Regression" index=242>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-242 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-LG, cs-RO, cs.RO<br>Keyword Score: 40<br>Keywords: Distribution Shift, Distribution Shift, Knowledge Distillation, Knowledge Distillation, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14328v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14328v1.pdf filename=2403.14328v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in <b>reinforcement</b> <b>learning</b> (RL) have led to remarkable achievements in robot locomotion capabilities. However, the complexity and <code>black-box'' nature of neural network-based RL policies hinder their interpretability and broader acceptance, particularly in applications demanding high levels of safety and reliability. This paper introduces a novel approach to &lt;b>distill&lt;/b> neural RL policies into more interpretable forms using Gradient Boosting Machines (GBMs), Explainable Boosting Machines (EBMs) and Symbolic Regression. By leveraging the inherent interpretability of generalized additive models, decision trees, and analytical expressions, we transform opaque neural network policies into more transparent </code>glass-box&rsquo;&rsquo; models. We train expert neural network policies using RL and subsequently <b>distill</b> them into (i) GBMs, (ii) EBMs, and (iii) symbolic policies. To address the inherent <b>distribution</b> <b>shift</b> challenge of behavioral cloning, we propose to use the Dataset Aggregation (DAgger) algorithm with a curriculum of episode-dependent alternation of actions between expert and <b>distilled</b> policies, to enable efficient <b>distillation</b> of feedback control policies. We evaluate our approach on various robot locomotion gaits &ndash; walking, trotting, bounding, and pacing &ndash; and study the importance of different observations in joint actions for <b>distilled</b> policies using various methods. We train neural expert policies for 205 hours of simulated experience and <b>distill</b> interpretable policies with only 10 minutes of simulated interaction for each gait using the proposed method.</p></p class="citation"></blockquote><h3 id=420--243312-click-to-grasp-zero-shot-precise-manipulation-via-visual-diffusion-descriptors-nikolaos-tsagkas-et-al-2024>(4/20 | 243/312) Click to Grasp: Zero-Shot Precise Manipulation via Visual Diffusion Descriptors (Nikolaos Tsagkas et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nikolaos Tsagkas, Jack Rome, Subramanian Ramamoorthy, Oisin Mac Aodha, Chris Xiaoxuan Lu. (2024)<br><strong>Click to Grasp: Zero-Shot Precise Manipulation via Visual Diffusion Descriptors</strong><br><button class=copy-to-clipboard title="Click to Grasp: Zero-Shot Precise Manipulation via Visual Diffusion Descriptors" index=243>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-243 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-CV, cs-RO, cs.RO<br>Keyword Score: 35<br>Keywords: Geometry, Zero-shot, Grounding, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14526v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14526v1.pdf filename=2403.14526v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Precise manipulation that is generalizable across scenes and objects remains a persistent challenge in robotics. Current approaches for this task heavily depend on having a significant number of training instances to handle objects with pronounced visual and/or geometric part ambiguities. Our work explores the <b>grounding</b> of fine-grained part descriptors for precise manipulation in a <b>zero-shot</b> setting by utilizing web-trained <b>text-to-image</b> diffusion-based generative models. We tackle the problem by framing it as a dense semantic part correspondence task. Our model returns a gripper pose for manipulating a specific part, using as reference a user-defined click from a source image of a visually different instance of the same object. We require no manual grasping demonstrations as we leverage the intrinsic object <b>geometry</b> and features. Practical experiments in a real-world tabletop scenario validate the efficacy of our approach, demonstrating its potential for advancing semantic-aware robotics manipulation. Web page: <a href=https://tsagkas.github.io/click2grasp>https://tsagkas.github.io/click2grasp</a></p></p class="citation"></blockquote><h3 id=520--244312-teevtol-balancing-energy-and-time-efficiency-in-evtol-aircraft-path-planning-across-city-scale-wind-fields-songyang-liu-et-al-2024>(5/20 | 244/312) TEeVTOL: Balancing Energy and Time Efficiency in eVTOL Aircraft Path Planning Across City-Scale Wind Fields (Songyang Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Songyang Liu, Shuai Li, Haochen Li, Weizi Li, Jindong Tan. (2024)<br><strong>TEeVTOL: Balancing Energy and Time Efficiency in eVTOL Aircraft Path Planning Across City-Scale Wind Fields</strong><br><button class=copy-to-clipboard title="TEeVTOL: Balancing Energy and Time Efficiency in eVTOL Aircraft Path Planning Across City-Scale Wind Fields" index=244>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-244 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 23<br>Keywords: Graph, Reinforcement Learning, Stemming<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14877v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14877v1.pdf filename=2403.14877v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Electric vertical-takeoff and landing (eVTOL) aircraft, recognized for their maneuverability and flexibility, offer a promising alternative to our transportation system. However, the operational effectiveness of these aircraft faces many challenges, such as the delicate balance between energy and time efficiency, <b>stemming</b> from unpredictable environmental factors, including wind fields. Mathematical modeling-based approaches have been adopted to plan aircraft flight path in urban wind fields with the goal to save energy and time costs. While effective, they are limited in adapting to dynamic and complex environments. To optimize energy and time efficiency in eVTOL&rsquo;s flight through dynamic wind fields, we introduce a novel path planning method leveraging deep <b>reinforcement</b> <b>learning.</b> We assess our method with extensive experiments, comparing it to Dijkstra&rsquo;s algorithm &ndash; the theoretically optimal approach for determining shortest paths in a weighted <b>graph,</b> where weights represent either energy or time cost. The results show that our method achieves a graceful balance between energy and time efficiency, closely resembling the theoretically optimal values for both objectives.</p></p class="citation"></blockquote><h3 id=620--245312-multi-agent-task-driven-exploration-via-intelligent-map-compression-and-sharing-evangelos-psomiadis-et-al-2024>(6/20 | 245/312) Multi-agent Task-Driven Exploration via Intelligent Map Compression and Sharing (Evangelos Psomiadis et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Evangelos Psomiadis, Dipankar Maity, Panagiotis Tsiotras. (2024)<br><strong>Multi-agent Task-Driven Exploration via Intelligent Map Compression and Sharing</strong><br><button class=copy-to-clipboard title="Multi-agent Task-Driven Exploration via Intelligent Map Compression and Sharing" index=245>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-245 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14780v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14780v1.pdf filename=2403.14780v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper investigates the task-driven exploration of unknown environments with mobile sensors communicating compressed measurements. The sensors explore the area and transmit their compressed data to another robot, assisting it in reaching a goal location. We propose a novel communication framework and a tractable multi-agent exploration algorithm to select the sensors&rsquo; actions. The algorithm uses a task-driven measure of uncertainty, resulting from map compression, as a reward function. We validate the efficacy of our algorithm through numerical <b>simulations</b> conducted on a realistic map and compare it with two alternative approaches. The results indicate that the proposed algorithm effectively decreases the time required for the robot to reach its target without causing excessive load on the communication network.</p></p class="citation"></blockquote><h3 id=720--246312-sdp-synthesis-of-maximum-coverage-trees-for-probabilistic-planning-under-control-constraints-naman-aggarwal-et-al-2024>(7/20 | 246/312) SDP Synthesis of Maximum Coverage Trees for Probabilistic Planning under Control Constraints (Naman Aggarwal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Naman Aggarwal, Jonathan P. How. (2024)<br><strong>SDP Synthesis of Maximum Coverage Trees for Probabilistic Planning under Control Constraints</strong><br><button class=copy-to-clipboard title="SDP Synthesis of Maximum Coverage Trees for Probabilistic Planning under Control Constraints" index=246>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-246 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs-SY, cs.RO, eess-SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14605v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14605v1.pdf filename=2403.14605v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The paper presents Maximal Covariance Backward Reachable Trees (MAXCOVAR BRT), which is a multi-query algorithm for planning of dynamic systems under stochastic motion uncertainty and constraints on the control input with explicit coverage guarantees. In contrast to existing roadmap-based probabilistic planning methods that sample belief nodes randomly and draw edges between them \cite{csbrm_tro2024}, under control constraints, the reachability of belief nodes needs to be explicitly established and is determined by checking the feasibility of a non-convex program. Moreover, there is no explicit consideration of coverage of the roadmap while adding nodes and edges during the construction procedure for the existing methods. Our contribution is a novel optimization formulation to add nodes and construct the corresponding edge controllers such that the generated roadmap results in provably maximal coverage under control constraints as compared to any other method of adding nodes and edges. We characterize formally the notion of coverage of a roadmap in this stochastic domain via introduction of the h-$\operatorname{BRS}$ (Backward Reachable Set of Distributions) of a tree of distributions under control constraints, and also support our method with extensive <b>simulations</b> on a 6 DoF model.</p></p class="citation"></blockquote><h3 id=820--247312-bayesian-optimization-for-sample-efficient-policy-improvement-in-robotic-manipulation-adrian-röfer-et-al-2024>(8/20 | 247/312) Bayesian Optimization for Sample-Efficient Policy Improvement in Robotic Manipulation (Adrian Röfer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Adrian Röfer, Iman Nematollahi, Tim Welschehold, Wolfram Burgard, Abhinav Valada. (2024)<br><strong>Bayesian Optimization for Sample-Efficient Policy Improvement in Robotic Manipulation</strong><br><button class=copy-to-clipboard title="Bayesian Optimization for Sample-Efficient Policy Improvement in Robotic Manipulation" index=247>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-247 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14305v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14305v1.pdf filename=2403.14305v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Sample efficient learning of manipulation skills poses a major challenge in robotics. While recent approaches demonstrate impressive advances in the type of task that can be addressed and the sensing modalities that can be incorporated, they still require large amounts of training data. Especially with regard to learning actions on robots in the real world, this poses a major problem due to the high costs associated with both demonstrations and real-world robot interactions. To address this challenge, we introduce BOpt-GMM, a hybrid approach that combines imitation learning with own experience collection. We first learn a skill model as a dynamical system encoded in a Gaussian Mixture Model from a few demonstrations. We then improve this model with Bayesian optimization building on a small number of autonomous skill executions in a sparse reward setting. We demonstrate the sample efficiency of our approach on multiple complex manipulation skills in both <b>simulations</b> and real-world experiments. Furthermore, we make the code and pre-trained models publicly available at http://bopt-gmm. cs.uni-freiburg.de.</p></p class="citation"></blockquote><h3 id=920--248312-dexdribbler-learning-dexterous-soccer-manipulation-via-dynamic-supervision-yutong-hu-et-al-2024>(9/20 | 248/312) DexDribbler: Learning Dexterous Soccer Manipulation via Dynamic Supervision (Yutong Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yutong Hu, Kehan Wen, Fisher Yu. (2024)<br><strong>DexDribbler: Learning Dexterous Soccer Manipulation via Dynamic Supervision</strong><br><button class=copy-to-clipboard title="DexDribbler: Learning Dexterous Soccer Manipulation via Dynamic Supervision" index=248>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-248 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14300v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14300v1.pdf filename=2403.14300v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Learning dexterous locomotion policy for legged robots is becoming increasingly popular due to its ability to handle diverse terrains and resemble intelligent behaviors. However, joint manipulation of moving objects and locomotion with legs, such as playing soccer, receive scant attention in the learning community, although it is natural for humans and smart animals. A key challenge to solve this multitask problem is to infer the objectives of locomotion from the states and targets of the manipulated objects. The implicit relation between the object states and robot locomotion can be hard to capture directly from the training experience. We propose adding a feedback control block to compute the necessary body-level movement accurately and using the outputs as dynamic joint-level locomotion supervision explicitly. We further utilize an improved ball dynamic model, an extended context-aided estimator, and a comprehensive ball observer to facilitate transferring policy learned in <b>simulation</b> to the real world. We observe that our learning scheme can not only make the policy network converge faster but also enable soccer robots to perform sophisticated maneuvers like sharp cuts and turns on flat surfaces, a capability that was lacking in previous methods. Video and code are available at <a href=https://github.com/SysCV/soccer-player>https://github.com/SysCV/soccer-player</a></p></p class="citation"></blockquote><h3 id=1020--249312-extrinsic-calibration-of-multiple-lidars-for-a-mobile-robot-based-on-floor-plane-and-object-segmentation-shun-niijima-et-al-2024>(10/20 | 249/312) Extrinsic Calibration of Multiple LiDARs for a Mobile Robot based on Floor Plane And Object Segmentation (Shun Niijima et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shun Niijima, Atsushi Suzuki, Ryoichi Tsuzaki, Masaya Kinoshita. (2024)<br><strong>Extrinsic Calibration of Multiple LiDARs for a Mobile Robot based on Floor Plane And Object Segmentation</strong><br><button class=copy-to-clipboard title="Extrinsic Calibration of Multiple LiDARs for a Mobile Robot based on Floor Plane And Object Segmentation" index=249>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-249 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14161v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14161v1.pdf filename=2403.14161v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Mobile robots equipped with multiple light detection and ranging (LiDARs) and capable of recognizing their surroundings are increasing due to the minitualization and cost reduction of LiDAR. This paper proposes a target-less extrinsic calibration method of multiple LiDARs with non-overlapping field of view (FoV). The proposed method uses accumulated point clouds of floor plane and objects while in motion. It enables accurate calibration with challenging configuration of LiDARs that directed towards the floor plane, caused by biased feature values. Additionally, the method includes a noise removal module that considers the scanning pattern to address bleeding points, which are noises of significant source of error in point cloud alignment using high-density LiDARs. Evaluations through <b>simulation</b> demonstrate that the proposed method achieved higher accuracy extrinsic calibration with two and four LiDARs than conventional methods, regardless type of objects. Furthermore, the experiments using a real mobile robot has shown that our proposed noise removal module can eliminate noise more precisely than conventional methods, and the estimated extrinsic parameters have successfully created consistent 3D maps.</p></p class="citation"></blockquote><h3 id=1120--250312-a-roadmap-towards-automated-and-regulated-robotic-systems-yihao-liu-et-al-2024>(11/20 | 250/312) A Roadmap Towards Automated and Regulated Robotic Systems (Yihao Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yihao Liu, Mehran Armand. (2024)<br><strong>A Roadmap Towards Automated and Regulated Robotic Systems</strong><br><button class=copy-to-clipboard title="A Roadmap Towards Automated and Regulated Robotic Systems" index=250>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-250 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-HC, cs-RO, cs.RO<br>Keyword Score: 18<br>Keywords: Graph, Black Box, human-in-the-loop<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14049v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14049v1.pdf filename=2403.14049v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rapid development of generative technology opens up possibility for higher level of automation, and artificial intelligence (AI) embodiment in robotic systems is imminent. However, due to the blackbox nature of the generative technology, the generation of the knowledge and workflow scheme is uncontrolled, especially in a dynamic environment and a complex scene. This poses challenges to regulations in safety-demanding applications such as medical scenes. We argue that the unregulated generative processes from AI is fitted for low level end tasks, but intervention in the form of manual or automated regulation should happen post-workflow-generation and pre-robotic-execution. To address this, we propose a roadmap that can lead to fully automated and regulated robotic systems. In this paradigm, the high level policies are generated as structured <b>graph</b> data, enabling regulatory oversight and reusability, while the code base for lower level tasks is generated by generative models. Our approach aims the transitioning from expert knowledge to regulated action, akin to the iterative processes of study, practice, scrutiny, and execution in human tasks. We identify the generative and deterministic processes in a design cycle, where generative processes serve as a text-based world simulator and the deterministic processes generate the executable system. We propose State Machine Seralization Language (SMSL) to be the conversion point between text simulator and executable workflow control. From there, we analyze the modules involved based on the current literature, and discuss human in the loop. As a roadmap, this work identifies the current possible implementation and future work. This work does not provide an implemented system but envisions to inspire the researchers working on the direction in the roadmap. We implement the SMSL and D-SFO paradigm that serve as the starting point of the roadmap.</p></p class="citation"></blockquote><h3 id=1220--251312-bringing-robots-home-the-rise-of-ai-robots-in-consumer-electronics-haiwei-dong-et-al-2024>(12/20 | 251/312) Bringing Robots Home: The Rise of AI Robots in Consumer Electronics (Haiwei Dong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haiwei Dong, Yang Liu, Ted Chu, Abdulmotaleb El Saddik. (2024)<br><strong>Bringing Robots Home: The Rise of AI Robots in Consumer Electronics</strong><br><button class=copy-to-clipboard title="Bringing Robots Home: The Rise of AI Robots in Consumer Electronics" index=251>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-251 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-MM, cs-RO, cs.RO<br>Keyword Score: 16<br>Keywords: Generative AI, Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14449v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14449v1.pdf filename=2403.14449v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>On March 18, 2024, NVIDIA unveiled Project GR00T, a general-purpose <b>multimodal</b> <b>generative</b> <b>AI</b> model designed specifically for training humanoid robots. Preceding this event, Tesla&rsquo;s unveiling of the Optimus Gen 2 humanoid robot on December 12, 2023, underscored the profound impact robotics is poised to have on reshaping various facets of our daily lives. While robots have long dominated industrial settings, their presence within our homes is a burgeoning phenomenon. This can be attributed, in part, to the complexities of domestic environments and the challenges of creating robots that can seamlessly integrate into our daily routines.</p></p class="citation"></blockquote><h3 id=1320--252312-leveraging-large-language-model-based-room-object-relationships-knowledge-for-enhancing-multimodal-input-object-goal-navigation-leyuan-sun-et-al-2024>(13/20 | 252/312) Leveraging Large Language Model-based Room-Object Relationships Knowledge for Enhancing Multimodal-Input Object Goal Navigation (Leyuan Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Leyuan Sun, Asako Kanezaki, Guillaume Caron, Yusuke Yoshiyasu. (2024)<br><strong>Leveraging Large Language Model-based Room-Object Relationships Knowledge for Enhancing Multimodal-Input Object Goal Navigation</strong><br><button class=copy-to-clipboard title="Leveraging Large Language Model-based Room-Object Relationships Knowledge for Enhancing Multimodal-Input Object Goal Navigation" index=252>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-252 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-CV, cs-RO, cs.RO<br>Keyword Score: 16<br>Keywords: Multi-modal, Multi-modal, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14163v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14163v1.pdf filename=2403.14163v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Object-goal navigation is a crucial engineering task for the community of embodied navigation; it involves navigating to an instance of a specified object category within unseen environments. Although extensive investigations have been conducted on both end-to-end and modular-based, data-driven approaches, fully enabling an agent to comprehend the environment through perceptual knowledge and perform object-goal navigation as efficiently as humans remains a significant challenge. Recently, <b>large</b> <b>language</b> <b>models</b> have shown potential in this task, thanks to their powerful capabilities for knowledge extraction and integration. In this study, we propose a data-driven, modular-based approach, trained on a dataset that incorporates common-sense knowledge of object-to-room relationships extracted from a <b>large</b> <b>language</b> <b>model.</b> We utilize the multi-channel Swin-Unet architecture to conduct multi-task learning incorporating with <b>multimodal</b> inputs. The results in the Habitat simulator demonstrate that our framework outperforms the baseline by an average of 10.6% in the efficiency metric, Success weighted by Path Length (SPL). The real-world demonstration shows that the proposed approach can efficiently conduct this task by traversing several rooms. For more details and real-world demonstrations, please check our project webpage (<a href=https://sunleyuan.github.io/ObjectNav)>https://sunleyuan.github.io/ObjectNav)</a>.</p></p class="citation"></blockquote><h3 id=1420--253312-odtformer-efficient-obstacle-detection-and-tracking-with-stereo-cameras-based-on-transformer-tianye-ding-et-al-2024>(14/20 | 253/312) ODTFormer: Efficient Obstacle Detection and Tracking with Stereo Cameras Based on Transformer (Tianye Ding et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianye Ding, Hongyu Li, Huaizu Jiang. (2024)<br><strong>ODTFormer: Efficient Obstacle Detection and Tracking with Stereo Cameras Based on Transformer</strong><br><button class=copy-to-clipboard title="ODTFormer: Efficient Obstacle Detection and Tracking with Stereo Cameras Based on Transformer" index=253>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-253 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-CV, cs-RO, cs.RO<br>Keyword Score: 13<br>Keywords: Benchmarking, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14626v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14626v1.pdf filename=2403.14626v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Obstacle detection and tracking represent a critical component in robot autonomous navigation. In this paper, we propose ODTFormer, a <b>Transformer-based</b> model to address both obstacle detection and tracking problems. For the detection task, our approach leverages deformable attention to construct a 3D cost volume, which is decoded progressively in the form of voxel occupancy grids. We further track the obstacles by matching the voxels between consecutive frames. The entire model can be optimized in an end-to-end manner. Through extensive experiments on DrivingStereo and KITTI <b>benchmarks,</b> our model achieves state-of-the-art performance in the obstacle detection task. We also report comparable accuracy to state-of-the-art obstacle tracking models while requiring only a fraction of their computation cost, typically ten-fold to twenty-fold less. The code and model weights will be publicly released.</p></p class="citation"></blockquote><h3 id=1520--254312-exosense-a-vision-centric-scene-understanding-system-for-safe-exoskeleton-navigation-jianeng-wang-et-al-2024>(15/20 | 254/312) Exosense: A Vision-Centric Scene Understanding System For Safe Exoskeleton Navigation (Jianeng Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianeng Wang, Matias Mattamala, Christina Kassab, Lintong Zhang, Maurice Fallon. (2024)<br><strong>Exosense: A Vision-Centric Scene Understanding System For Safe Exoskeleton Navigation</strong><br><button class=copy-to-clipboard title="Exosense: A Vision-Centric Scene Understanding System For Safe Exoskeleton Navigation" index=254>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-254 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-CV, cs-RO, cs.RO<br>Keyword Score: 13<br>Keywords: Graph, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14320v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14320v1.pdf filename=2403.14320v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Exoskeletons for daily use by those with mobility impairments are being developed. They will require accurate and robust scene understanding systems. Current research has used vision to identify immediate terrain and geometric obstacles, however these approaches are constrained to detections directly in front of the user and are limited to classifying a finite range of terrain types (e.g., stairs, ramps and level-ground). This paper presents Exosense, a vision-centric scene understanding system which is capable of generating rich, globally-consistent elevation maps, incorporating both semantic and terrain traversability information. It features an elastic Atlas mapping framework associated with a visual SLAM pose <b>graph,</b> embedded with open-vocabulary room labels from a <b>Vision-Language</b> Model (VLM). The device&rsquo;s design includes a wide field-of-view (FoV) fisheye multi-camera system to mitigate the challenges introduced by the exoskeleton walking pattern. We demonstrate the system&rsquo;s robustness to the challenges of typical periodic walking gaits, and its ability to construct accurate semantically-rich maps in indoor settings. Additionally, we showcase its potential for motion planning &ndash; providing a step towards safe navigation for exoskeletons.</p></p class="citation"></blockquote><h3 id=1620--255312-learning-to-change-choreographing-mixed-traffic-through-lateral-control-and-hierarchical-reinforcement-learning-dawei-wang-et-al-2024>(16/20 | 255/312) Learning to Change: Choreographing Mixed Traffic Through Lateral Control and Hierarchical Reinforcement Learning (Dawei Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dawei Wang, Weizi Li, Lei Zhu, Jia Pan. (2024)<br><strong>Learning to Change: Choreographing Mixed Traffic Through Lateral Control and Hierarchical Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Learning to Change: Choreographing Mixed Traffic Through Lateral Control and Hierarchical Reinforcement Learning" index=255>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-255 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-MA, cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14879v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14879v1.pdf filename=2403.14879v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The management of mixed traffic that consists of robot vehicles (RVs) and human-driven vehicles (HVs) at complex intersections presents a multifaceted challenge. Traditional signal controls often struggle to adapt to dynamic traffic conditions and heterogeneous vehicle types. Recent advancements have turned to strategies based on <b>reinforcement</b> <b>learning</b> (RL), leveraging its model-free nature, real-time operation, and generalizability over different scenarios. We introduce a hierarchical RL framework to manage mixed traffic through precise longitudinal and lateral control of RVs. Our proposed hierarchical framework combines the state-of-the-art mixed traffic control algorithm as a high level decision maker to improve the performance and robustness of the whole system. Our experiments demonstrate that the framework can reduce the average waiting time by up to 54% compared to the state-of-the-art mixed traffic control method. When the RV penetration rate exceeds 60%, our technique consistently outperforms conventional traffic signal control programs in terms of the average waiting time for all vehicles at the intersection.</p></p class="citation"></blockquote><h3 id=1720--256312-extended-reality-for-enhanced-human-robot-collaboration-a-human-in-the-loop-approach-yehor-karpichev-et-al-2024>(17/20 | 256/312) Extended Reality for Enhanced Human-Robot Collaboration: a Human-in-the-Loop Approach (Yehor Karpichev et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yehor Karpichev, Todd Charter, Homayoun Najjaran. (2024)<br><strong>Extended Reality for Enhanced Human-Robot Collaboration: a Human-in-the-Loop Approach</strong><br><button class=copy-to-clipboard title="Extended Reality for Enhanced Human-Robot Collaboration: a Human-in-the-Loop Approach" index=256>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-256 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-HC, cs-LG, cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: human-in-the-loop<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14597v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14597v1.pdf filename=2403.14597v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rise of automation has provided an opportunity to achieve higher efficiency in manufacturing processes, yet it often compromises the flexibility required to promptly respond to evolving market needs and meet the demand for customization. Human-robot collaboration attempts to tackle these challenges by combining the strength and precision of machines with human ingenuity and perceptual understanding. In this paper, we conceptualize and propose an implementation framework for an autonomous, machine learning-based manipulator that incorporates <b>human-in-the-loop</b> principles and leverages Extended Reality (XR) to facilitate intuitive communication and programming between humans and robots. Furthermore, the conceptual framework foresees human involvement directly in the robot learning process, resulting in higher adaptability and task generalization. The paper highlights key technologies enabling the proposed framework, emphasizing the importance of developing the digital ecosystem as a whole. Additionally, we review the existent implementation approaches of XR in human-robot collaboration, showcasing diverse perspectives and methodologies. The challenges and future outlooks are discussed, delving into the major obstacles and potential research avenues of XR for more natural human-robot interaction and integration in the industrial landscape.</p></p class="citation"></blockquote><h3 id=1820--257312-uav-assisted-maritime-search-and-rescue-a-holistic-approach-martin-messmer-et-al-2024>(18/20 | 257/312) UAV-Assisted Maritime Search and Rescue: A Holistic Approach (Martin Messmer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Martin Messmer, Benjamin Kiefer, Leon Amadeus Varga, Andreas Zell. (2024)<br><strong>UAV-Assisted Maritime Search and Rescue: A Holistic Approach</strong><br><button class=copy-to-clipboard title="UAV-Assisted Maritime Search and Rescue: A Holistic Approach" index=257>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-257 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Object Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14281v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14281v1.pdf filename=2403.14281v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we explore the application of Unmanned Aerial Vehicles (UAVs) in maritime search and rescue (mSAR) missions, focusing on medium-sized fixed-wing drones and quadcopters. We address the challenges and limitations inherent in operating some of the different classes of UAVs, particularly in search operations. Our research includes the development of a comprehensive software framework designed to enhance the efficiency and efficacy of SAR operations. This framework combines preliminary detection onboard UAVs with advanced <b>object</b> <b>detection</b> at ground stations, aiming to reduce visual strain and improve decision-making for operators. It will be made publicly available upon publication. We conduct experiments to evaluate various Region of Interest (RoI) proposal methods, especially by imposing simulated limited bandwidth on them, an important consideration when flying remote or offshore operations. This forces the algorithm to prioritize some predictions over others.</p></p class="citation"></blockquote><h3 id=1920--258312-referee-radar-based-efficient-global-descriptor-using-a-feature-and-free-space-for-place-recognition-byunghee-choi-et-al-2024>(19/20 | 258/312) ReFeree: Radar-based efficient global descriptor using a Feature and Free space for Place Recognition (Byunghee Choi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Byunghee Choi, Hogyun Kim, Younggun Cho. (2024)<br><strong>ReFeree: Radar-based efficient global descriptor using a Feature and Free space for Place Recognition</strong><br><button class=copy-to-clipboard title="ReFeree: Radar-based efficient global descriptor using a Feature and Free space for Place Recognition" index=258>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-258 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14176v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14176v1.pdf filename=2403.14176v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Radar is highlighted for robust sensing capabilities in adverse weather conditions (e.g. dense fog, heavy rain, or snowfall). In addition, Radar can cover wide areas and penetrate small particles. Despite these advantages, Radar-based place recognition remains in the early stages compared to other sensors due to its unique characteristics such as low resolution, and significant noise. In this paper, we propose a Radarbased place recognition utilizing a descriptor called ReFeree using a feature and free space. Unlike traditional methods, we overwhelmingly <b>summarize</b> the Radar image. Despite being lightweight, it contains semi-metric information and is also outstanding from the perspective of place recognition performance. For concrete validation, we test a single session from the MulRan dataset and a multi-session from the Oxford Radar RobotCar and the Boreas dataset.</p></p class="citation"></blockquote><h3 id=2020--259312-hcto-optimality-aware-lidar-inertial-odometry-with-hybrid-continuous-time-optimization-for-compact-wearable-mapping-system-jianping-li-et-al-2024>(20/20 | 259/312) HCTO: Optimality-Aware LiDAR Inertial Odometry with Hybrid Continuous Time Optimization for Compact Wearable Mapping System (Jianping Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianping Li, Shenghai Yuan, Muqing Cao, Thien-Minh Nguyen, Kun Cao, Lihua Xie. (2024)<br><strong>HCTO: Optimality-Aware LiDAR Inertial Odometry with Hybrid Continuous Time Optimization for Compact Wearable Mapping System</strong><br><button class=copy-to-clipboard title="HCTO: Optimality-Aware LiDAR Inertial Odometry with Hybrid Continuous Time Optimization for Compact Wearable Mapping System" index=259>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-259 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Continuous Time, Continuous Time<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14173v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14173v1.pdf filename=2403.14173v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Compact wearable mapping system (WMS) has gained significant attention due to their convenience in various applications. Specifically, it provides an efficient way to collect prior maps for 3D structure inspection and robot-based &ldquo;last-mile delivery&rdquo; in complex environments. However, vibrations in human motion and the uneven distribution of point cloud features in complex environments often lead to rapid drift, which is a prevalent issue when applying existing LiDAR Inertial Odometry (LIO) methods on low-cost WMS. To address these limitations, we propose a novel LIO for WMSs based on Hybrid <b>Continuous</b> <b>Time</b> Optimization (HCTO) considering the optimality of Lidar correspondences. First, HCTO recognizes patterns in human motion (high-frequency part, low-frequency part, and constant velocity part) by analyzing raw IMU measurements. Second, HCTO constructs hybrid IMU factors according to different motion states, which enables robust and accurate estimation against vibration-induced noise in the IMU measurements. Third, the best point correspondences are selected using optimal design to achieve real-time performance and better odometry accuracy. We conduct experiments on head-mounted WMS datasets to evaluate the performance of our system, demonstrating significant advantages over state-of-the-art methods. Video recordings of experiments can be found on the project page of HCTO: \href{https://github.com/kafeiyin00/HCTO}{https://github.com/kafeiyin00/HCTO}.</p></p class="citation"></blockquote><h2 id=eessiv-7>eess.IV (7)</h2><h3 id=17--260312-cathflow-self-supervised-segmentation-of-catheters-in-interventional-ultrasound-using-optical-flow-and-transformers-alex-ranne-et-al-2024>(1/7 | 260/312) CathFlow: Self-Supervised Segmentation of Catheters in Interventional Ultrasound Using Optical Flow and Transformers (Alex Ranne et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alex Ranne, Liming Kuang, Yordanka Velikova, Nassir Navab, Ferdinando Rodriguez y Baena. (2024)<br><strong>CathFlow: Self-Supervised Segmentation of Catheters in Interventional Ultrasound Using Optical Flow and Transformers</strong><br><button class=copy-to-clipboard title="CathFlow: Self-Supervised Segmentation of Catheters in Interventional Ultrasound Using Optical Flow and Transformers" index=260>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-260 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 40<br>Keywords: Self-supervised Learning, Simulation, Simulator, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14465v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14465v1.pdf filename=2403.14465v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In minimally invasive endovascular procedures, contrast-enhanced angiography remains the most robust imaging technique. However, it is at the expense of the patient and clinician&rsquo;s health due to prolonged radiation exposure. As an alternative, interventional ultrasound has notable benefits such as being radiation-free, fast to deploy, and having a small footprint in the operating room. Yet, ultrasound is hard to interpret, and highly prone to artifacts and noise. Additionally, interventional radiologists must undergo extensive training before they become qualified to diagnose and treat patients effectively, leading to a shortage of staff, and a lack of open-source datasets. In this work, we seek to address both problems by introducing a <b>self-supervised</b> deep learning architecture to segment catheters in longitudinal ultrasound images, without demanding any labeled data. The network architecture builds upon AiAReSeg, a segmentation <b>transformer</b> built with the Attention in Attention mechanism, and is capable of learning feature changes across time and space. To facilitate training, we used synthetic ultrasound data based on physics-driven catheter insertion <b>simulations,</b> and translated the data into a unique CT-Ultrasound common domain, CACTUSS, to improve the segmentation performance. We generated ground truth segmentation masks by computing the optical flow between adjacent frames using FlowNet2, and performed thresholding to obtain a binary map estimate. Finally, we validated our model on a test dataset, consisting of unseen synthetic data and images collected from silicon aorta phantoms, thus demonstrating its potential for applications to clinical data in the future.</p></p class="citation"></blockquote><h3 id=27--261312-diffusion-models-with-ensembled-structure-based-anomaly-scoring-for-unsupervised-anomaly-detection-finn-behrendt-et-al-2024>(2/7 | 261/312) Diffusion Models with Ensembled Structure-Based Anomaly Scoring for Unsupervised Anomaly Detection (Finn Behrendt et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Finn Behrendt, Debayan Bhattacharya, Lennart Maack, Julia Krüger, Roland Opfer, Robin Mieling, Alexander Schlaefer. (2024)<br><strong>Diffusion Models with Ensembled Structure-Based Anomaly Scoring for Unsupervised Anomaly Detection</strong><br><button class=copy-to-clipboard title="Diffusion Models with Ensembled Structure-Based Anomaly Scoring for Unsupervised Anomaly Detection" index=261>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-261 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 40<br>Keywords: Diffusion Model, Anomaly Detection, Supervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14262v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14262v1.pdf filename=2403.14262v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Supervised</b> deep learning techniques show promise in medical image analysis. However, they require comprehensive annotated data sets, which poses challenges, particularly for rare diseases. Consequently, <b>unsupervised</b> <b>anomaly</b> <b>detection</b> (UAD) emerges as a viable alternative for pathology segmentation, as only healthy data is required for training. However, recent UAD <b>anomaly</b> <b>scoring</b> functions often focus on intensity only and neglect structural differences, which impedes the segmentation performance. This work investigates the potential of Structural Similarity (SSIM) to bridge this gap. SSIM captures both intensity and structural disparities and can be advantageous over the classical $l1$ error. However, we show that there is more than one optimal kernel size for the SSIM calculation for different pathologies. Therefore, we investigate an adaptive ensembling strategy for various kernel sizes to offer a more pathology-agnostic scoring mechanism. We demonstrate that this ensembling strategy can enhance the performance of DMs and mitigate the sensitivity to different kernel sizes across varying pathologies, highlighting its promise for brain MRI <b>anomaly</b> <b>detection.</b></p></p class="citation"></blockquote><h3 id=37--262312-resnet101-and-dae-for-enhance-quality-and-classification-accuracy-in-skin-cancer-imaging-sibasish-dhibar-2024>(3/7 | 262/312) ResNet101 and DAE for Enhance Quality and Classification Accuracy in Skin Cancer Imaging (Sibasish Dhibar, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sibasish Dhibar. (2024)<br><strong>ResNet101 and DAE for Enhance Quality and Classification Accuracy in Skin Cancer Imaging</strong><br><button class=copy-to-clipboard title="ResNet101 and DAE for Enhance Quality and Classification Accuracy in Skin Cancer Imaging" index=262>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-262 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: 68U10, 94A08, J-3; I-4-3; I-4-9, cs-CV, eess-IV, eess.IV<br>Keyword Score: 40<br>Keywords: Autoencoder, Convolution, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14248v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14248v1.pdf filename=2403.14248v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Skin cancer is a crucial health issue that requires timely detection for higher survival rates. Traditional computer vision techniques face challenges in addressing the advanced variability of skin lesion features, a gap partially bridged by <b>convolutional</b> <b>neural</b> <b>networks</b> <b>(CNNs).</b> To overcome the existing issues, we introduce an innovative <b>convolutional</b> <b>ensemble</b> <b>network</b> approach named deep <b>autoencoder</b> (DAE) with ResNet101. This method utilizes <b>convolution-based</b> deep neural networks for the detection of skin cancer. The ISIC-2018 public data taken from the source is used for experimental results, which demonstrate remarkable performance with the different in terms of performance metrics. The methods result in 96.03% of accuracy, 95.40 % of precision, 96.05% of recall, 0.9576 of F-measure, 0.98 of AUC.</p></p class="citation"></blockquote><h3 id=47--263312-qsmdiff-unsupervised-3d-diffusion-models-for-quantitative-susceptibility-mapping-zhuang-xiong-et-al-2024>(4/7 | 263/312) QSMDiff: Unsupervised 3D Diffusion Models for Quantitative Susceptibility Mapping (Zhuang Xiong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhuang Xiong, Wei Jiang, Yang Gao, Feng Liu, Hongfu Sun. (2024)<br><strong>QSMDiff: Unsupervised 3D Diffusion Models for Quantitative Susceptibility Mapping</strong><br><button class=copy-to-clipboard title="QSMDiff: Unsupervised 3D Diffusion Models for Quantitative Susceptibility Mapping" index=263>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-263 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV, physics-med-ph<br>Keyword Score: 30<br>Keywords: Diffusion Model, Supervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14070v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14070v1.pdf filename=2403.14070v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Quantitative Susceptibility Mapping (QSM) dipole inversion is an ill-posed inverse problem for quantifying magnetic susceptibility distributions from MRI tissue phases. While <b>supervised</b> deep learning methods have shown success in specific QSM tasks, their generalizability across different acquisition scenarios remains constrained. Recent developments in <b>diffusion</b> <b>models</b> have demonstrated potential for solving 2D medical imaging inverse problems. However, their application to 3D modalities, such as QSM, remains challenging due to high computational demands. In this work, we developed a 3D image patch-based <b>diffusion</b> <b>model,</b> namely QSMDiff, for robust QSM reconstruction across different scan parameters, alongside simultaneous super-resolution and image-denoising tasks. QSMDiff adopts <b>unsupervised</b> 3D image patch training and full-size measurement guidance during inference for controlled image generation. Evaluation on simulated and in-vivo human brains, using gradient-echo and echo-planar imaging sequences across different acquisition parameters, demonstrates superior performance. The method proposed in QSMDiff also holds promise for impacting other 3D medical imaging applications beyond QSM.</p></p class="citation"></blockquote><h3 id=57--264312-denoising-diffusion-models-for-3d-healthy-brain-tissue-inpainting-alicia-durrer-et-al-2024>(5/7 | 264/312) Denoising Diffusion Models for 3D Healthy Brain Tissue Inpainting (Alicia Durrer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alicia Durrer, Julia Wolleb, Florentin Bieder, Paul Friedrich, Lester Melie-Garcia, Mario Ocampo-Pineda, Cosmin I. Bercea, Ibrahim E. Hamamci, Benedikt Wiestler, Marie Piraud, Özgür Yaldizli, Cristina Granziera, Bjoern H. Menze, Philippe C. Cattin, Florian Kofler. (2024)<br><strong>Denoising Diffusion Models for 3D Healthy Brain Tissue Inpainting</strong><br><button class=copy-to-clipboard title="Denoising Diffusion Models for 3D Healthy Brain Tissue Inpainting" index=264>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-264 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14499v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14499v1.pdf filename=2403.14499v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Monitoring diseases that affect the brain&rsquo;s structural integrity requires automated analysis of magnetic resonance (MR) images, e.g., for the evaluation of volumetric changes. However, many of the evaluation tools are optimized for analyzing healthy tissue. To enable the evaluation of scans containing pathological tissue, it is therefore required to restore healthy tissue in the pathological areas. In this work, we explore and extend denoising <b>diffusion</b> <b>models</b> for consistent inpainting of healthy 3D brain tissue. We modify state-of-the-art 2D, pseudo-3D, and 3D methods working in the image space, as well as 3D latent and 3D wavelet <b>diffusion</b> <b>models,</b> and train them to synthesize healthy brain tissue. Our evaluation shows that the pseudo-3D model performs best regarding the structural-similarity index, peak signal-to-noise ratio, and mean squared error. To emphasize the clinical relevance, we <b>fine-tune</b> this model on data containing synthetic MS lesions and evaluate it on a downstream brain tissue segmentation task, whereby it outperforms the established FMRIB Software Library (FSL) lesion-filling method.</p></p class="citation"></blockquote><h3 id=67--265312-analysing-diffusion-segmentation-for-medical-images-mathias-öttl-et-al-2024>(6/7 | 265/312) Analysing Diffusion Segmentation for Medical Images (Mathias Öttl et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mathias Öttl, Siyuan Mei, Frauke Wilm, Jana Steenpass, Matthias Rübner, Arndt Hartmann, Matthias Beckmann, Peter Fasching, Andreas Maier, Ramona Erber, Katharina Breininger. (2024)<br><strong>Analysing Diffusion Segmentation for Medical Images</strong><br><button class=copy-to-clipboard title="Analysing Diffusion Segmentation for Medical Images" index=265>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-265 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-AI, cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Probabilistic Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14440v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14440v1.pdf filename=2403.14440v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Denoising Diffusion <b>Probabilistic</b> <b>models</b> have become increasingly popular due to their ability to offer <b>probabilistic</b> <b>modeling</b> and generate diverse outputs. This versatility inspired their adaptation for image segmentation, where multiple predictions of the model can produce segmentation results that not only achieve high quality but also capture the uncertainty inherent in the model. Here, powerful architectures were proposed for improving diffusion segmentation performance. However, there is a notable lack of analysis and discussions on the differences between diffusion segmentation and image generation, and thorough evaluations are missing that distinguish the improvements these architectures provide for segmentation in general from their benefit for diffusion segmentation specifically. In this work, we critically analyse and discuss how diffusion segmentation for medical images differs from diffusion image generation, with a particular focus on the training behavior. Furthermore, we conduct an assessment how proposed diffusion segmentation architectures perform when trained directly for segmentation. Lastly, we explore how different medical segmentation tasks influence the diffusion segmentation behavior and the diffusion process could be adapted accordingly. With these analyses, we aim to provide in-depth insights into the behavior of diffusion segmentation that allow for a better design and evaluation of diffusion segmentation methods in the future.</p></p class="citation"></blockquote><h3 id=77--266312-lefusion-synthesizing-myocardial-pathology-on-cardiac-mri-via-lesion-focus-diffusion-models-hantao-zhang-et-al-2024>(7/7 | 266/312) LeFusion: Synthesizing Myocardial Pathology on Cardiac MRI via Lesion-Focus Diffusion Models (Hantao Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hantao Zhang, Jiancheng Yang, Shouhong Wan, Pascal Fua. (2024)<br><strong>LeFusion: Synthesizing Myocardial Pathology on Cardiac MRI via Lesion-Focus Diffusion Models</strong><br><button class=copy-to-clipboard title="LeFusion: Synthesizing Myocardial Pathology on Cardiac MRI via Lesion-Focus Diffusion Models" index=266>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-266 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14066v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14066v1.pdf filename=2403.14066v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Data generated in clinical practice often exhibits biases, such as long-tail imbalance and algorithmic unfairness. This study aims to mitigate these challenges through data synthesis. Previous efforts in medical imaging synthesis have struggled with separating lesion information from background context, leading to difficulties in generating high-quality backgrounds and limited control over the synthetic output. Inspired by <b>diffusion-based</b> <b>image</b> inpainting, we propose LeFusion, lesion-focused <b>diffusion</b> <b>models.</b> By redesigning the <b>diffusion</b> <b>learning</b> objectives to concentrate on lesion areas, it simplifies the model learning process and enhance the controllability of the synthetic output, while preserving background by integrating forward-diffused background contexts into the reverse <b>diffusion</b> <b>process.</b> Furthermore, we generalize it to jointly handle multi-class lesions, and further introduce a generative model for lesion masks to increase synthesis diversity. Validated on the DE-MRI cardiac lesion segmentation dataset (Emidec), our methodology employs the popular nnUNet to demonstrate that the synthetic data make it possible to effectively enhance a state-of-the-art model. Code and model are available at <a href=https://github.com/M3DV/LeFusion>https://github.com/M3DV/LeFusion</a>.</p></p class="citation"></blockquote><h2 id=cscr-5>cs.CR (5)</h2><h3 id=15--267312-large-language-models-for-blockchain-security-a-systematic-literature-review-zheyuan-he-et-al-2024>(1/5 | 267/312) Large Language Models for Blockchain Security: A Systematic Literature Review (Zheyuan He et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zheyuan He, Zihao Li, Sen Yang. (2024)<br><strong>Large Language Models for Blockchain Security: A Systematic Literature Review</strong><br><button class=copy-to-clipboard title="Large Language Models for Blockchain Security: A Systematic Literature Review" index=267>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-267 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 40<br>Keywords: Anomaly Detection, Large Language Model, Large Language Model, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14280v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14280v2.pdf filename=2403.14280v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have emerged as powerful tools in various domains involving blockchain security (BS). Several recent studies are exploring <b>LLMs</b> applied to BS. However, there remains a gap in our understanding regarding the full scope of applications, impacts, and potential constraints of <b>LLMs</b> on blockchain security. To fill this gap, we conduct a literature review on LLM4BS. As the first review of <b>LLM&rsquo;s</b> application on blockchain security, our study aims to comprehensively analyze existing research and elucidate how <b>LLMs</b> contribute to enhancing the security of blockchain systems. Through a thorough examination of scholarly works, we delve into the integration of <b>LLMs</b> into various aspects of blockchain security. We explore the mechanisms through which <b>LLMs</b> can bolster blockchain security, including their applications in smart contract auditing, identity verification, <b>anomaly</b> <b>detection,</b> vulnerable repair, and so on. Furthermore, we critically assess the challenges and limitations associated with leveraging <b>LLMs</b> for blockchain security, considering factors such as scalability, privacy concerns, and <b>adversarial</b> <b>attacks.</b> Our review sheds light on the opportunities and potential risks inherent in this convergence, providing valuable insights for researchers, practitioners, and policymakers alike.</p></p class="citation"></blockquote><h3 id=25--268312-adversary-augmented-simulation-to-evaluate-client-fairness-on-hyperledger-fabric-erwan-mahe-et-al-2024>(2/5 | 268/312) Adversary-Augmented Simulation to evaluate client-fairness on HyperLedger Fabric (Erwan Mahe et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Erwan Mahe, Rouwaida Abdallah, Sara Tucci-Piergiovanni, Pierre-Yves Piriou. (2024)<br><strong>Adversary-Augmented Simulation to evaluate client-fairness on HyperLedger Fabric</strong><br><button class=copy-to-clipboard title="Adversary-Augmented Simulation to evaluate client-fairness on HyperLedger Fabric" index=268>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-268 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-DC, cs-MA, cs.CR<br>Keyword Score: 30<br>Keywords: Simulation, Simulator, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14342v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14342v1.pdf filename=2403.14342v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a novel adversary model specifically tailored to distributed systems, with the aim to asses the security of blockchain technologies. Building upon literature on <b>adversarial</b> <b>assumptions</b> and capabilities, we include classical notions of failure and communication models to classify and bind the use of <b>adversarial</b> <b>actions.</b> We focus on the effect of these actions on properties of distributed protocols. A significant effort of our research is the integration of this model into the Multi-Agent eXperimenter (MAX) framework. This integration enables realistic <b>simulations</b> of <b>adversarial</b> <b>attacks</b> on blockchain systems. In particular, we have simulated attacks violating a form of client-fairness on HyperLedger Fabric.</p></p class="citation"></blockquote><h3 id=35--269312-hetal-efficient-privacy-preserving-transfer-learning-with-homomorphic-encryption-seewoo-lee-et-al-2024>(3/5 | 269/312) HETAL: Efficient Privacy-preserving Transfer Learning with Homomorphic Encryption (Seewoo Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Seewoo Lee, Garam Lee, Jung Woo Kim, Junbum Shin, Mun-Kyu Lee. (2024)<br><strong>HETAL: Efficient Privacy-preserving Transfer Learning with Homomorphic Encryption</strong><br><button class=copy-to-clipboard title="HETAL: Efficient Privacy-preserving Transfer Learning with Homomorphic Encryption" index=269>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-269 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-LG, cs.CR<br>Keyword Score: 23<br>Keywords: Benchmarking, Fine-tuning, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14111v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14111v1.pdf filename=2403.14111v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Transfer</b> <b>learning</b> is a de facto standard method for efficiently training machine learning models for data-scarce problems by adding and <b>fine-tuning</b> new classification layers to a model pre-trained on large datasets. Although numerous previous studies proposed to use homomorphic encryption to resolve the data privacy issue in <b>transfer</b> <b>learning</b> in the machine learning as a service setting, most of them only focused on encrypted inference. In this study, we present HETAL, an efficient Homomorphic Encryption based <b>Transfer</b> <b>Learning</b> algorithm, that protects the client&rsquo;s privacy in training tasks by encrypting the client data using the CKKS homomorphic encryption scheme. HETAL is the first practical scheme that strictly provides encrypted training, adopting validation-based early stopping and achieving the accuracy of nonencrypted training. We propose an efficient encrypted matrix multiplication algorithm, which is 1.8 to 323 times faster than prior methods, and a highly precise softmax approximation algorithm with increased coverage. The experimental results for five well-known <b>benchmark</b> datasets show total training times of 567-3442 seconds, which is less than an hour.</p></p class="citation"></blockquote><h3 id=45--270312-fhauc-privacy-preserving-auc-calculation-for-federated-learning-using-fully-homomorphic-encryption-cem-ata-baykara-et-al-2024>(4/5 | 270/312) FHAUC: Privacy Preserving AUC Calculation for Federated Learning using Fully Homomorphic Encryption (Cem Ata Baykara et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Cem Ata Baykara, Ali Burak Ünal, Mete Akgün. (2024)<br><strong>FHAUC: Privacy Preserving AUC Calculation for Federated Learning using Fully Homomorphic Encryption</strong><br><button class=copy-to-clipboard title="FHAUC: Privacy Preserving AUC Calculation for Federated Learning using Fully Homomorphic Encryption" index=270>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-270 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 20<br>Keywords: Federated Learning, Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14428v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14428v1.pdf filename=2403.14428v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Ensuring data privacy is a significant challenge for machine learning applications, not only during model training but also during evaluation. <b>Federated</b> <b>learning</b> has gained significant research interest in recent years as a result. Current research on <b>federated</b> <b>learning</b> primarily focuses on preserving privacy during the training phase. However, model evaluation has not been adequately addressed, despite the potential for significant privacy leaks during this phase as well. In this paper, we demonstrate that the state-of-the-art AUC computation method for <b>federated</b> <b>learning</b> systems, which utilizes <b>differential</b> <b>privacy,</b> still leaks sensitive information about the test data while also requiring a trusted central entity to perform the computations. More importantly, we show that the performance of this method becomes completely unusable as the data size decreases. In this context, we propose an efficient, accurate, robust, and more secure evaluation algorithm capable of computing the AUC in horizontal <b>federated</b> <b>learning</b> systems. Our approach not only enhances security compared to the current state-of-the-art but also surpasses the state-of-the-art AUC computation method in both approximation performance and computational robustness, as demonstrated by experimental results. To illustrate, our approach can efficiently calculate the AUC of a <b>federated</b> <b>learning</b> system involving 100 parties, achieving 99.93% accuracy in just 0.68 seconds, regardless of data size, while providing complete data privacy.</p></p class="citation"></blockquote><h3 id=55--271312-improving-galileo-osnma-time-to-first-authenticated-fix-aleix-galan-et-al-2024>(5/5 | 271/312) Improving Galileo OSNMA Time To First Authenticated Fix (Aleix Galan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aleix Galan, Ignacio Fernandez-Hernandez, Wim De Wilde, Sofie Pollin, Gonzalo Seco-Granados. (2024)<br><strong>Improving Galileo OSNMA Time To First Authenticated Fix</strong><br><button class=copy-to-clipboard title="Improving Galileo OSNMA Time To First Authenticated Fix" index=271>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-271 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR, eess-SP<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14739v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14739v1.pdf filename=2403.14739v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Galileo is the first global navigation satellite system to authenticate their civilian signals through the Open Service Galileo Message Authentication (OSNMA) protocol. However, OSNMA delays the time to obtain a first position and time fix, the so-called Time To First Authentication Fix (TTFAF). Reducing the TTFAF as much as possible is crucial to integrate the technology seamlessly into the current products. In the cases where the receiver already has cryptographic data available, the so-called hot start mode and focus of this article, the currently available implementations achieve an average TTFAF of around 100 seconds in ideal environments. In this work, we dissect the TTFAF process, propose two main optimizations to reduce the TTFAF, and <b>benchmark</b> them in three distinct scenarios (open-sky, soft urban, and hard urban) with recorded real data. Moreover, we evaluate the optimizations using the synthetic scenario from the official OSNMA test vectors. The first block of optimizations centers on extracting as much information as possible from broken sub-frames by processing them at page level and combining redundant data from multiple satellites. The second block of optimizations aims to reconstruct missed navigation data by using fields in the authentication tags belonging to the same sub-frame as the authentication key. Combining both optimizations improves the TTFAF substantially for all considered scenarios. We obtain an average TTFAF of 60.9 and 68.8 seconds for the test vectors and the open-sky scenario, respectively, with a best-case of 44.0 seconds in both. Likewise, the urban scenarios see a drastic reduction of the average TTFAF between the non-optimized and optimized cases, from 127.5 to 87.5 seconds in the soft urban scenario and from 266.1 to 146.1 seconds in the hard urban scenario. These optimizations are available as part of the open-source OSNMAlib library on GitHub.</p></p class="citation"></blockquote><h2 id=csdc-1>cs.DC (1)</h2><h3 id=11--272312-accelerating-vit-inference-on-fpga-through-static-and-dynamic-pruning-dhruv-parikh-et-al-2024>(1/1 | 272/312) Accelerating ViT Inference on FPGA through Static and Dynamic Pruning (Dhruv Parikh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dhruv Parikh, Shouyi Li, Bingyi Zhang, Rajgopal Kannan, Carl Busart, Viktor Prasanna. (2024)<br><strong>Accelerating ViT Inference on FPGA through Static and Dynamic Pruning</strong><br><button class=copy-to-clipboard title="Accelerating ViT Inference on FPGA through Static and Dynamic Pruning" index=272>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-272 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-AR, cs-CV, cs-DC, cs.DC<br>Keyword Score: 40<br>Keywords: Vision Transformer, Pruning, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14047v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14047v1.pdf filename=2403.14047v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Vision</b> <b>Transformers</b> (ViTs) have achieved state-of-the-art accuracy on various computer <b>vision</b> <b>tasks.</b> However, their high computational complexity prevents them from being applied to many real-world applications. Weight and token <b>pruning</b> are two well-known methods for reducing complexity: weight <b>pruning</b> reduces the model size and associated computational demands, while token <b>pruning</b> further dynamically reduces the computation based on the input. Combining these two techniques should significantly reduce computation complexity and model size; however, naively integrating them results in irregular computation patterns, leading to significant accuracy drops and difficulties in hardware acceleration. Addressing the above challenges, we propose a comprehensive algorithm-hardware codesign for accelerating ViT on FPGA through simultaneous <b>pruning</b> -combining static weight <b>pruning</b> and dynamic token <b>pruning.</b> For algorithm design, we systematically combine a hardware-aware structured block-pruning method for <b>pruning</b> model parameters and a dynamic token <b>pruning</b> method for removing unimportant token vectors. Moreover, we design a novel training algorithm to recover the model&rsquo;s accuracy. For hardware design, we develop a novel hardware accelerator for executing the pruned model. The proposed hardware design employs multi-level parallelism with load balancing strategy to efficiently deal with the irregular computation pattern led by the two <b>pruning</b> approaches. Moreover, we develop an efficient hardware mechanism for efficiently executing the on-the-fly token <b>pruning.</b></p></p class="citation"></blockquote><h2 id=physicsmed-ph-1>physics.med-ph (1)</h2><h3 id=11--273312-distribution-informed-and-wavelength-flexible-data-driven-photoacoustic-oximetry-janek-gröhl-et-al-2024>(1/1 | 273/312) Distribution-informed and wavelength-flexible data-driven photoacoustic oximetry (Janek Gröhl et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Janek Gröhl, Kylie Yeung, Kevin Gu, Thomas R. Else, Monika Golinska, Ellie V. Bunce, Lina Hacker, Sarah E. Bohndiek. (2024)<br><strong>Distribution-informed and wavelength-flexible data-driven photoacoustic oximetry</strong><br><button class=copy-to-clipboard title="Distribution-informed and wavelength-flexible data-driven photoacoustic oximetry" index=273>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-273 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.med-ph<br>Categories: F-2-1, cs-CV, cs-LG, physics-med-ph, physics.med-ph<br>Keyword Score: 30<br>Keywords: LSTM, LSTM, Recurrent Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14863v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14863v1.pdf filename=2403.14863v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Significance: Photoacoustic imaging (PAI) promises to measure spatially-resolved blood oxygen saturation, but suffers from a lack of accurate and robust spectral unmixing methods to deliver on this promise. Accurate blood oxygenation estimation could have important clinical applications, from cancer detection to quantifying inflammation. Aim: This study addresses the inflexibility of existing data-driven methods for estimating blood oxygenation in PAI by introducing a <b>recurrent</b> <b>neural</b> <b>network</b> architecture. Approach: We created 25 simulated training dataset variations to assess neural network performance. We used a <b>long</b> <b>short-term</b> <b>memory</b> <b>network</b> to implement a wavelength-flexible network architecture and proposed the Jensen-Shannon divergence to predict the most suitable training dataset. Results: The network architecture can handle arbitrary input wavelengths and outperforms linear unmixing and the previously proposed learned spectral decolouring method. Small changes in the training data significantly affect the accuracy of our method, but we find that the Jensen-Shannon divergence correlates with the estimation error and is thus suitable for predicting the most appropriate training datasets for any given application. Conclusions: A flexible data-driven network architecture combined with the Jensen-Shannon Divergence to predict the best training data set provides a promising direction that might enable robust data-driven photoacoustic oximetry for clinical use cases.</p></p class="citation"></blockquote><h2 id=csse-3>cs.SE (3)</h2><h3 id=13--274312-towards-single-system-illusion-in-software-defined-vehicles----automated-ai-powered-workflow-krzysztof-lebioda-et-al-2024>(1/3 | 274/312) Towards Single-System Illusion in Software-Defined Vehicles &ndash; Automated, AI-Powered Workflow (Krzysztof Lebioda et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Krzysztof Lebioda, Viktor Vorobev, Nenad Petrovic, Fengjunjie Pan, Vahid Zolfaghari, Alois Knoll. (2024)<br><strong>Towards Single-System Illusion in Software-Defined Vehicles &ndash; Automated, AI-Powered Workflow</strong><br><button class=copy-to-clipboard title="Towards Single-System Illusion in Software-Defined Vehicles -- Automated, AI-Powered Workflow" index=274>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-274 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: D-2-1; D-2-2; D-2-4; I-2-7; I-2-2; I-7-0, cs-AI, cs-CL, cs-SE, cs.SE<br>Keyword Score: 30<br>Keywords: Generative AI, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14460v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14460v1.pdf filename=2403.14460v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose a novel model- and feature-based approach to development of vehicle software systems, where the end architecture is not explicitly defined. Instead, it emerges from an iterative process of search and optimization given certain constraints, requirements and hardware architecture, while retaining the property of single-system illusion, where applications run in a logically uniform environment. One of the key points of the presented approach is the inclusion of modern <b>generative</b> <b>AI,</b> specifically <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> in the loop. With the recent advances in the field, we expect that the <b>LLMs</b> will be able to assist in processing of requirements, generation of formal system models, as well as generation of software deployment specification and test code. The resulting pipeline is automated to a <b>large</b> <b>extent,</b> <b>with</b> feedback being generated at each step.</p></p class="citation"></blockquote><h3 id=23--275312-multi-role-consensus-through-llms-discussions-for-vulnerability-detection-zhenyu-mao-et-al-2024>(2/3 | 275/312) Multi-role Consensus through LLMs Discussions for Vulnerability Detection (Zhenyu Mao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhenyu Mao, Jialong Li, Munan Li, Kenji Tei. (2024)<br><strong>Multi-role Consensus through LLMs Discussions for Vulnerability Detection</strong><br><button class=copy-to-clipboard title="Multi-role Consensus through LLMs Discussions for Vulnerability Detection" index=275>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-275 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-SE, cs.SE<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14274v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14274v1.pdf filename=2403.14274v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have highlighted the potential for vulnerability detection, a crucial component of software quality assurance. Despite this progress, most studies have been limited to the perspective of a single role, usually testers, lacking diverse viewpoints from different roles in a typical software development life-cycle, including both developers and testers. To this end, this paper introduces an approach to employ <b>LLMs</b> to act as different roles to simulate real-life code review process, engaging in discussions towards a consensus on the existence and classification of vulnerabilities in the code. Preliminary evaluation of the proposed approach indicates a 4.73% increase in the precision rate, 58.9% increase in the recall rate, and a 28.1% increase in the F1 score.</p></p class="citation"></blockquote><h3 id=33--276312-a-survey-of-neural-code-intelligence-paradigms-advances-and-beyond-qiushi-sun-et-al-2024>(3/3 | 276/312) A Survey of Neural Code Intelligence: Paradigms, Advances and Beyond (Qiushi Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qiushi Sun, Zhirui Chen, Fangzhi Xu, Kanzhi Cheng, Chang Ma, Zhangyue Yin, Jianing Wang, Chengcheng Han, Renyu Zhu, Shuai Yuan, Qipeng Guo, Xipeng Qiu, Pengcheng Yin, Xiaoli Li, Fei Yuan, Lingpeng Kong, Xiang Li, Zhiyong Wu. (2024)<br><strong>A Survey of Neural Code Intelligence: Paradigms, Advances and Beyond</strong><br><button class=copy-to-clipboard title="A Survey of Neural Code Intelligence: Paradigms, Advances and Beyond" index=276>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-276 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-CL, cs-PL, cs-SE, cs.SE<br>Keyword Score: 20<br>Keywords: Recurrent Neural Network, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14734v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14734v1.pdf filename=2403.14734v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neural Code Intelligence &ndash; leveraging deep learning to understand, generate, and optimize code &ndash; holds immense potential for transformative impacts on the whole society. Bridging the gap between Natural Language and Programming Language, this domain has drawn significant attention from researchers in both research communities over the past few years. This survey presents a systematic and chronological review of the advancements in code intelligence, encompassing over 50 representative models and their variants, more than 20 categories of tasks, and an extensive coverage of over 680 related works. We follow the historical progression to trace the paradigm shifts across different research phases (e.g., from modeling code with <b>recurrent</b> <b>neural</b> <b>networks</b> to the era of <b>Large</b> <b>Language</b> <b>Models).</b> Concurrently, we highlight the major technical transitions in models, tasks, and evaluations spanning through different stages. For applications, we also observe a co-evolving shift. It spans from initial endeavors to tackling specific scenarios, through exploring a diverse array of tasks during its rapid expansion, to currently focusing on tackling increasingly complex and varied real-world challenges. Building on our examination of the developmental trajectories, we further investigate the emerging synergies between code intelligence and broader machine intelligence, uncovering new cross-domain opportunities and illustrating the substantial influence of code intelligence across various domains. Finally, we delve into both the opportunities and challenges associated with this field, alongside elucidating our insights on the most promising research directions. An ongoing, dynamically updated project and resources associated with this survey have been released at <a href=https://github.com/QiushiSun/NCISurvey>https://github.com/QiushiSun/NCISurvey</a>.</p></p class="citation"></blockquote><h2 id=eesssy-14>eess.SY (14)</h2><h3 id=114--277312-event-triggered-boundary-control-of-mixed-autonomy-traffic-yihuai-zhang-et-al-2024>(1/14 | 277/312) Event-triggered Boundary Control of Mixed-autonomy Traffic (Yihuai Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yihuai Zhang, Huan Yu. (2024)<br><strong>Event-triggered Boundary Control of Mixed-autonomy Traffic</strong><br><button class=copy-to-clipboard title="Event-triggered Boundary Control of Mixed-autonomy Traffic" index=277>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-277 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY, math-AP<br>Keyword Score: 30<br>Keywords: Continuous Time, Continuous Time, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14194v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14194v1.pdf filename=2403.14194v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Control problems of mixed-autonomy traffic system consisting of both Human-driven Vehicles (HV) and Autonomous Vehicles (AV) have gained increasing attention. This paper is focused on suppressing traffic oscillations of the mixed-autonomy traffic system using boundary control design. The mixed traffic dynamics are described by a 4 x 4 hyperbolic partial differential equations (PDE) which governs propagation of four properties in traffic including density of HV, density of AV, friction between two classes of vehicles from driving interactions, and averaged velocity. We propose event-triggered boundary control design since control signal of traffic light on ramp or varying speed limit cannot be updated in a <b>continuous</b> <b>time</b> fashion. We apply event-triggered mechanism for a PDE backstepping controller and obtain dynamic triggering condition. Lyapunov analysis is conducted to prove the exponential stability of the closed loop system with the event-triggered controller. Numerical <b>simulation</b> demonstrates how car-following spacing of AV affects event-triggering mechanism of control input in mixed-autonomy traffic.</p></p class="citation"></blockquote><h3 id=214--278312-pe-gpt-a-physics-informed-interactive-large-language-model-for-power-converter-modulation-design-fanfan-lin-et-al-2024>(2/14 | 278/312) PE-GPT: A Physics-Informed Interactive Large Language Model for Power Converter Modulation Design (Fanfan Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fanfan Lin, Junhua Liu, Xinze Li, Shuai Zhao, Bohui Zhao, Hao Ma, Xin Zhang. (2024)<br><strong>PE-GPT: A Physics-Informed Interactive Large Language Model for Power Converter Modulation Design</strong><br><button class=copy-to-clipboard title="PE-GPT: A Physics-Informed Interactive Large Language Model for Power Converter Modulation Design" index=278>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-278 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 30<br>Keywords: In-context Learning, In-context Learning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14059v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14059v1.pdf filename=2403.14059v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper proposes PE-GPT, a custom-tailored <b>large</b> <b>language</b> <b>model</b> uniquely adapted for power converter modulation design. By harnessing <b>in-context</b> <b>learning</b> and specialized tiered physics-informed neural networks, PE-GPT guides users through text-based dialogues, recommending actionable modulation parameters. The effectiveness of PE-GPT is validated through a practical design case involving dual active bridge converters, supported by hardware experimentation. This research underscores the transformative potential of <b>large</b> <b>language</b> <b>models</b> in power converter modulation design, offering enhanced accessibility, explainability, and efficiency, thereby setting a new paradigm in the field.</p></p class="citation"></blockquote><h3 id=314--279312-a-benchmark-for-the-application-of-distributed-control-techniques-to-the-electricity-network-of-the-european-economic-area-a-riccardi-et-al-2024>(3/14 | 279/312) A Benchmark for the Application of Distributed Control Techniques to the Electricity Network of the European Economic Area (A. Riccardi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>A. Riccardi, L. Laurenti, B. De Schutter. (2024)<br><strong>A Benchmark for the Application of Distributed Control Techniques to the Electricity Network of the European Economic Area</strong><br><button class=copy-to-clipboard title="A Benchmark for the Application of Distributed Control Techniques to the Electricity Network of the European Economic Area" index=279>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-279 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 23<br>Keywords: Benchmarking, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14372v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14372v2.pdf filename=2403.14372v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The European Economic Area Electricity Network <b>Benchmark</b> (EEA-ENB) is a multi-area power system representing the European network of transmission systems for electricity to facilitate the application of distributed control techniques. In the EEA-ENB we consider the Load Frequency Control (LFC) problem in the presence of renewable energy sources (RESs), and energy storage systems (ESSs). RESs are known to cause instability in power networks due to their inertia-less and intermittent characteristics, while ESSs are introduced as a resource to mitigate the problem. In the EEA-ENB, particular attention is dedicated to Distributed Model Predictive Control (DMPC), whose application is often limited to small and homogeneous test cases due to the lack of standardized large-scale scenarios for testing, and due to the large computation time required to obtain a centralized MPC action for performance comparison with DMPC strategies under consideration. The second problem is exacerbated when the scale of the system grows. To address these challenges and to provide a real-world-based and control-independent <b>benchmark,</b> the EEA-ENB has been developed. The <b>benchmark</b> includes a centralized MPC strategy providing performance and computation time metrics to compare distributed control within a repeatable and realistic <b>simulation</b> environment.</p></p class="citation"></blockquote><h3 id=414--280312-learning-hierarchical-control-systems-for-autonomous-systems-with-energy-constraints-charlott-vallon-et-al-2024>(4/14 | 280/312) Learning Hierarchical Control Systems for Autonomous Systems with Energy Constraints (Charlott Vallon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Charlott Vallon, Mark Pustilnik, Alessandro Pinto, Francesco Borrelli. (2024)<br><strong>Learning Hierarchical Control Systems for Autonomous Systems with Energy Constraints</strong><br><button class=copy-to-clipboard title="Learning Hierarchical Control Systems for Autonomous Systems with Energy Constraints" index=280>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-280 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14536v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14536v1.pdf filename=2403.14536v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper focuses on the design of hierarchical control architectures for autonomous systems with energy constraints. We focus on systems where energy storage limitations and slow recharge rates drastically affect the way the autonomous systems are operated. Using examples from space robotics and public transportation, we motivate the need for formally designed learning hierarchical control systems. We propose a learning control architecture which incorporates learning mechanisms at various levels of the control hierarchy to improve performance and resource utilization. The proposed hierarchical control scheme relies on high-level energy-aware task planning and assignment, complemented by a low-level predictive control mechanism responsible for the autonomous execution of tasks, including motion control and energy management. <b>Simulation</b> examples show the benefits and the limitations of the proposed architecture when learning is used to obtain a more energy-efficient task allocation.</p></p class="citation"></blockquote><h3 id=514--281312-designing-robust-linear-output-feedback-controller-based-on-clf-cbf-framework-via-linearprogramminglp-clf-cbf-mahroo-bahreinian-et-al-2024>(5/14 | 281/312) Designing Robust Linear Output Feedback Controller based on CLF-CBF framework via Linear~Programming(LP-CLF-CBF) (Mahroo Bahreinian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mahroo Bahreinian, Mehdi Kermanshah, Roberto Tron. (2024)<br><strong>Designing Robust Linear Output Feedback Controller based on CLF-CBF framework via Linear~Programming(LP-CLF-CBF)</strong><br><button class=copy-to-clipboard title="Designing Robust Linear Output Feedback Controller based on CLF-CBF framework via Linear~Programming(LP-CLF-CBF)" index=281>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-281 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14519v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14519v1.pdf filename=2403.14519v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider the problem of designing output feedback controllers that use measurements from a set of landmarks to navigate through a cell-decomposable environment using duality, Control Lyapunov and Barrier Functions (CLF, CBF), and Linear Programming. We propose two objectives for navigating in an environment, one to traverse the environment by making loops and one by converging to a stabilization point while smoothing the transition between consecutive cells. We test our algorithms in a <b>simulation</b> environment, evaluating the robustness of the approach to practical conditions, such as bearing-only measurements, and measurements acquired with a camera with a limited field of view.</p></p class="citation"></blockquote><h3 id=614--282312-synthesizing-controller-for-safe-navigation-using-control-density-function-joseph-moyalan-et-al-2024>(6/14 | 282/312) Synthesizing Controller for Safe Navigation using Control Density Function (Joseph Moyalan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Joseph Moyalan, Sriram S. K. S Narayanan, Andrew Zheng, Umesh Vaidya. (2024)<br><strong>Synthesizing Controller for Safe Navigation using Control Density Function</strong><br><button class=copy-to-clipboard title="Synthesizing Controller for Safe Navigation using Control Density Function" index=282>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-282 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14464v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14464v1.pdf filename=2403.14464v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider the problem of navigating a nonlinear dynamical system from some initial set to some target set while avoiding collision with an unsafe set. We extend the concept of density function to control density function (CDF) for solving navigation problems with safety constraints. The occupancy-based interpretation of the measure associated with the density function is instrumental in imposing the safety constraints. The navigation problem with safety constraints is formulated as a quadratic program (QP) using CDF. The existing approach using the control barrier function (CBF) also formulates the navigation problem with safety constraints as QP. One of the main advantages of the proposed QP using CDF compared to QP formulated using CBF is that both the convergence/stability and safety can be combined and imposed using the CDF. <b>Simulation</b> results involving the Duffing oscillator and safe navigation of Dubin car models are provided to verify the main findings of the paper.</p></p class="citation"></blockquote><h3 id=714--283312-lane-level-joint-control-of-off-ramp-and-main-line-speed-guidance-on-expressway-in-rainy-weather-boyao-peng-et-al-2024>(7/14 | 283/312) Lane level joint control of off-ramp and main line speed guidance on expressway in rainy weather (Boyao Peng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Boyao Peng, Lexing Zhang, Enkai Li. (2024)<br><strong>Lane level joint control of off-ramp and main line speed guidance on expressway in rainy weather</strong><br><button class=copy-to-clipboard title="Lane level joint control of off-ramp and main line speed guidance on expressway in rainy weather" index=283>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-283 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: 90-10, A-0, cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14172v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14172v1.pdf filename=2403.14172v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the upstream of the exit ramp of the expressway, the speed limit difference leads to a significant deceleration of the vehicle in the area adjacent to the off-ramp. The friction coefficient of the road surface decreases under rainy weather, and the above deceleration process can easily lead to sideslip and rollover of the vehicle. Dynamic speed guidance is an effective way to improve the status quo. Currently, there is an emerging trend to utilize I2V technology and high-precision map technology for lane level speed guidance control. This paper presents an optimized joint control strategy for main line-off-ramp speed guidance, which can adjust the guidance speed in real time according to the rainfall intensity. At the same time, this paper designs a progressive deceleration strategy, which works together with the speed guidance control to ensure the safe deceleration of vehicles. The <b>simulation</b> results show that the proposed control strategy outperforms the fixed speed limit control in terms of improving the total traveled time (TTT), total traveled distance (TTD) and standard deviation of speed (SD). Sensitivity analysis shows that the proposed control strategy can improve performance with the increase of the compliance rate of drivers. The speed guidance control method established in this paper can improve the vehicle operation efficiency in the off-ramp area of the expressway and reduce the speed difference of each vehicle in rainy weather, which guarantee the safety of expressway driving in the rainy day.</p></p class="citation"></blockquote><h3 id=814--284312-robust-model-based-reinforcement-learning-using-mathcall_1-adaptive-control-minjun-sung-et-al-2024>(8/14 | 284/312) Robust Model Based Reinforcement Learning Using $\mathcal{L}_1$ Adaptive Control (Minjun Sung et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Minjun Sung, Sambhu H. Karumanchi, Aditya Gahlawat, Naira Hovakimyan. (2024)<br><strong>Robust Model Based Reinforcement Learning Using $\mathcal{L}_1$ Adaptive Control</strong><br><button class=copy-to-clipboard title="Robust Model Based Reinforcement Learning Using $\mathcal{L}_1$ Adaptive Control" index=284>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-284 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-LG, cs-SY, eess-SY, eess.SY<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14860v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14860v1.pdf filename=2403.14860v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce $\mathcal{L}_1$-MBRL, a control-theoretic augmentation scheme for Model-Based <b>Reinforcement</b> <b>Learning</b> (MBRL) algorithms. Unlike model-free approaches, MBRL algorithms learn a model of the transition function using data and use it to design a control input. Our approach generates a series of approximate control-affine models of the learned transition function according to the proposed switching law. Using the approximate model, control input produced by the underlying MBRL is perturbed by the $\mathcal{L}_1$ adaptive control, which is designed to enhance the robustness of the system against uncertainties. Importantly, this approach is agnostic to the choice of MBRL algorithm, enabling the use of the scheme with various MBRL algorithms. MBRL algorithms with $\mathcal{L}_1$ augmentation exhibit enhanced performance and sample efficiency across multiple MuJoCo environments, outperforming the original MBRL algorithms, both with and without system noise.</p></p class="citation"></blockquote><h3 id=914--285312-transmission-benefits-and-cost-allocation-under-ambiguity-han-shu-et-al-2024>(9/14 | 285/312) Transmission Benefits and Cost Allocation under Ambiguity (Han Shu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Han Shu, Jacob Mays. (2024)<br><strong>Transmission Benefits and Cost Allocation under Ambiguity</strong><br><button class=copy-to-clipboard title="Transmission Benefits and Cost Allocation under Ambiguity" index=285>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-285 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 10<br>Keywords: Counter-factual<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14803v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14803v1.pdf filename=2403.14803v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Disputes over cost allocation can present a significant barrier to investment in shared infrastructure. While it may be desirable to allocate cost in a way that corresponds to expected benefits, investments in long-lived projects are made under conditions of substantial uncertainty. In the context of electricity transmission, uncertainty combined with the inherent complexity of power systems analysis prevents the calculation of an estimated distribution of benefits that is agreeable to all participants. To analyze aspects of the cost allocation problem, we construct a model for transmission and generation expansion planning under uncertainty, enabling the identification of transmission investments as well as the calculation of benefits for users of the network. Numerical tests confirm the potential for realized benefits at the participant level to differ significantly from ex ante estimates. Based on the model and numerical tests we discuss several issues, including 1) establishing a valid <b>counterfactual</b> against which to measure benefits, 2) allocating cost to new and incumbent generators vs. solely allocating to loads, 3) calculating benefits at the portfolio vs. the individual project level, 4) identifying losers in a surplus-enhancing transmission expansion, and 5) quantifying the divergence between cost allocation decisions made ex ante and benefits realized ex post.</p></p class="citation"></blockquote><h3 id=1014--286312-optimizing-queues-with-deadlines-under-infrequent-monitoring-faraz-farahvash-et-al-2024>(10/14 | 286/312) Optimizing queues with deadlines under infrequent monitoring (Faraz Farahvash et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Faraz Farahvash, Ao Tang. (2024)<br><strong>Optimizing queues with deadlines under infrequent monitoring</strong><br><button class=copy-to-clipboard title="Optimizing queues with deadlines under infrequent monitoring" index=286>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-286 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-NI, cs-SY, eess-SY, eess.SY<br>Keyword Score: 10<br>Keywords: Discrete Time, Discrete Time<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14525v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14525v1.pdf filename=2403.14525v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we aim to improve the percentage of packets meeting their deadline in <b>discrete-time</b> <b>M/M/1</b> queues with infrequent monitoring. More specifically, we look into policies that only monitor the system (and subsequently take actions) after a packet arrival. We model the system as an MDP and provide the optimal policy for some special cases. Furthermore, we introduce a heuristic algorithm called &ldquo;AB-n&rdquo; for general deadlines. Finally, we provide numerical results demonstrating the desirable performance of &ldquo;AB-n&rdquo; policies.</p></p class="citation"></blockquote><h3 id=1114--287312-meta-learning-of-data-driven-controllers-with-automatic-model-reference-tuning-theory-and-experimental-case-study-riccardo-busetto-et-al-2024>(11/14 | 287/312) Meta-learning of data-driven controllers with automatic model reference tuning: theory and experimental case study (Riccardo Busetto et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Riccardo Busetto, Valentina Breschi, Federica Baracchi, Simone Formentin. (2024)<br><strong>Meta-learning of data-driven controllers with automatic model reference tuning: theory and experimental case study</strong><br><button class=copy-to-clipboard title="Meta-learning of data-driven controllers with automatic model reference tuning: theory and experimental case study" index=287>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-287 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 10<br>Keywords: Meta Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14500v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14500v1.pdf filename=2403.14500v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Data-driven control offers a viable option for control scenarios where constructing a system model is expensive or time-consuming. Nonetheless, many of these algorithms are not entirely automated, often necessitating the adjustment of multiple hyperparameters through cumbersome trial-and-error processes and demanding significant amounts of data. In this paper, we explore a <b>meta-learning</b> <b>approach</b> to leverage potentially existing prior knowledge about analogous (though not identical) systems, aiming to reduce both the experimental workload and ease the tuning of the available degrees of freedom. We validate this methodology through an experimental case study involving the tuning of proportional, integral (PI) controllers for brushless DC (BLDC) motors with variable loads and architectures.</p></p class="citation"></blockquote><h3 id=1214--288312-on-the-continuity-and-smoothness-of-the-value-function-in-reinforcement-learning-and-optimal-control-hans-harder-et-al-2024>(12/14 | 288/312) On the continuity and smoothness of the value function in reinforcement learning and optimal control (Hans Harder et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hans Harder, Sebastian Peitz. (2024)<br><strong>On the continuity and smoothness of the value function in reinforcement learning and optimal control</strong><br><button class=copy-to-clipboard title="On the continuity and smoothness of the value function in reinforcement learning and optimal control" index=288>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-288 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: 37H99, 37N35, 93E03, I-2-8, cs-AI, cs-SY, eess-SY, eess.SY<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14432v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14432v1.pdf filename=2403.14432v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The value function plays a crucial role as a measure for the cumulative future reward an agent receives in both <b>reinforcement</b> <b>learning</b> and optimal control. It is therefore of interest to study how similar the values of neighboring states are, i.e., to investigate the continuity of the value function. We do so by providing and verifying upper bounds on the value function&rsquo;s modulus of continuity. Additionally, we show that the value function is always H"older continuous under relatively weak assumptions on the underlying system and that non-differentiable value functions can be made differentiable by slightly &ldquo;disturbing&rdquo; the system.</p></p class="citation"></blockquote><h3 id=1314--289312-exploiting-over-the-air-consensus-for-collision-avoidance-and-formation-control-in-multi-agent-systems-michael-epp-et-al-2024>(13/14 | 289/312) Exploiting Over-The-Air Consensus for Collision Avoidance and Formation Control in Multi-Agent Systems (Michael Epp et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michael Epp, Fabio Molinari, Joerg Raisch. (2024)<br><strong>Exploiting Over-The-Air Consensus for Collision Avoidance and Formation Control in Multi-Agent Systems</strong><br><button class=copy-to-clipboard title="Exploiting Over-The-Air Consensus for Collision Avoidance and Formation Control in Multi-Agent Systems" index=289>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-289 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14386v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14386v1.pdf filename=2403.14386v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces a distributed control method for multi-agent robotic systems employing Over the Air Consensus (OTA-Consensus). Designed for agents with decoupled single-integrator dynamics, this approach aims at efficient formation achievement and collision avoidance. As a distinctive feature, it leverages OTA&rsquo;s ability to exploit interference in wireless channels, a property traditionally considered a drawback, thus enhancing communication efficiency among robots. An analytical proof of asymptotic convergence is established for systems with time-varying communication topologies represented by sequences of strongly connected directed <b>graphs.</b> Comparative evaluations demonstrate significant efficiency improvements over current state-of-the-art methods, especially in scenarios with a large number of agents.</p></p class="citation"></blockquote><h3 id=1414--290312-transformation-free-fixed-structure-model-reduction-for-lpv-systems-lennart-heeren-et-al-2024>(14/14 | 290/312) Transformation-Free Fixed-Structure Model Reduction for LPV Systems (Lennart Heeren et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lennart Heeren, Adwait Datar, Antonio Mendez Gonzalez, Herbert Werner. (2024)<br><strong>Transformation-Free Fixed-Structure Model Reduction for LPV Systems</strong><br><button class=copy-to-clipboard title="Transformation-Free Fixed-Structure Model Reduction for LPV Systems" index=290>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-290 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14310v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14310v1.pdf filename=2403.14310v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we propose a model reduction technique for linear parameter varying (LPV) systems based on available tools for fixed-structure controller synthesis. We start by transforming a model reduction problem into an equivalent controller synthesis problem by defining an appropriate generalized plant. The controller synthesis problem is then solved by using gradient-based tools available in the literature. Owing to the flexibility of the gradient-based synthesis tools, we are able to impose a desired structure on the obtained reduced model. Additionally, we obtain a bound on the approximation error as a direct output of the optimization problem. The proposed methods are applied on a <b>benchmark</b> mechanical system of interconnected masses, springs and dampers. To evaluate the effect of the proposed model-reduction approach on controller design, LPV controllers designed using the reduced models (with and without an imposed structure) are compared in closed-loop with the original model.</p></p class="citation"></blockquote><h2 id=q-biobm-1>q-bio.BM (1)</h2><h3 id=11--291312-protein-conformation-generation-via-force-guided-se3-diffusion-models-yan-wang-et-al-2024>(1/1 | 291/312) Protein Conformation Generation via Force-Guided SE(3) Diffusion Models (Yan Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yan Wang, Lihao Wang, Yuning Shen, Yiqun Wang, Huizhuo Yuan, Yue Wu, Quanquan Gu. (2024)<br><strong>Protein Conformation Generation via Force-Guided SE(3) Diffusion Models</strong><br><button class=copy-to-clipboard title="Protein Conformation Generation via Force-Guided SE(3) Diffusion Models" index=291>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-291 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.BM<br>Categories: cs-LG, q-bio-BM, q-bio.BM<br>Keyword Score: 30<br>Keywords: Diffusion Model, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14088v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14088v1.pdf filename=2403.14088v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The conformational landscape of proteins is crucial to understanding their functionality in complex biological processes. Traditional physics-based computational methods, such as molecular dynamics (MD) <b>simulations,</b> suffer from rare event sampling and long equilibration time problems, hindering their applications in general protein systems. Recently, deep generative modeling techniques, especially <b>diffusion</b> <b>models,</b> have been employed to generate novel protein conformations. However, existing score-based <b>diffusion</b> <b>methods</b> cannot properly incorporate important physical prior knowledge to guide the generation process, causing large deviations in the sampled protein conformations from the equilibrium distribution. In this paper, to overcome these limitations, we propose a force-guided SE(3) <b>diffusion</b> <b>model,</b> ConfDiff, for protein conformation generation. By incorporating a force-guided network with a mixture of data-based score models, ConfDiff can can generate protein conformations with rich diversity while preserving high fidelity. Experiments on a variety of protein conformation prediction tasks, including 12 fast-folding proteins and the Bovine Pancreatic Trypsin Inhibitor (BPTI), demonstrate that our method surpasses the state-of-the-art method.</p></p class="citation"></blockquote><h2 id=hep-ex-1>hep-ex (1)</h2><h3 id=11--292312-improving-λ-signal-extraction-with-domain-adaptation-via-normalizing-flows-rowan-kelleher-et-al-2024>(1/1 | 292/312) Improving $Λ$ Signal Extraction with Domain Adaptation via Normalizing Flows (Rowan Kelleher et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rowan Kelleher, Matthew McEneaney, Anselm Vossen. (2024)<br><strong>Improving $Λ$ Signal Extraction with Domain Adaptation via Normalizing Flows</strong><br><button class=copy-to-clipboard title="Improving $Λ$ Signal Extraction with Domain Adaptation via Normalizing Flows" index=292>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-292 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: hep-ex<br>Categories: cs-LG, hep-ex, hep-ex<br>Keyword Score: 30<br>Keywords: Simulation, Simulator, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14076v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14076v1.pdf filename=2403.14076v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The present study presents a novel application for normalizing flows for <b>domain</b> <b>adaptation.</b> The study investigates the ability of flow based neural networks to improve signal extraction of $\Lambda$ Hyperons at CLAS12. Normalizing Flows can help model complex probability density functions that describe physics processes, enabling uses such as event generation. $\Lambda$ signal extraction has been improved through the use of classifier networks, but differences in <b>simulation</b> and data <b>domains</b> <b>limit</b> classifier performance; this study utilizes the flows for <b>domain</b> <b>adaptation</b> between Monte Carlo <b>simulation</b> and data. We were successful in training a flow network to transform between the latent physics space and a normal distribution. We also found that applying the flows lessened the dependence of the figure of merit on the cut on the classifier output, meaning that there was a broader range where the cut results in a similar figure of merit.</p></p class="citation"></blockquote><h2 id=csce-2>cs.CE (2)</h2><h3 id=12--293312-geom-deeponet-a-point-cloud-based-deep-operator-network-for-field-predictions-on-3d-parameterized-geometries-junyan-he-et-al-2024>(1/2 | 293/312) Geom-DeepONet: A Point-cloud-based Deep Operator Network for Field Predictions on 3D Parameterized Geometries (Junyan He et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junyan He, Seid Koric, Diab Abueidda, Ali Najafi, Iwona Jasiuk. (2024)<br><strong>Geom-DeepONet: A Point-cloud-based Deep Operator Network for Field Predictions on 3D Parameterized Geometries</strong><br><button class=copy-to-clipboard title="Geom-DeepONet: A Point-cloud-based Deep Operator Network for Field Predictions on 3D Parameterized Geometries" index=293>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-293 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CE<br>Categories: cs-CE, cs.CE<br>Keyword Score: 23<br>Keywords: Benchmarking, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14788v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14788v1.pdf filename=2403.14788v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Modern digital engineering design process commonly involves expensive repeated <b>simulations</b> on varying three-dimensional (3D) geometries. The efficient prediction capability of neural networks (NNs) makes them a suitable surrogate to provide design insights. Nevertheless, few available NNs can handle solution prediction on varying 3D shapes. We present a novel deep operator network (DeepONet) variant called Geom-DeepONet, which encodes parameterized 3D geometries and predicts full-field solutions on an arbitrary number of nodes. To the best of the authors&rsquo; knowledge, this is the first attempt in the literature and is our primary novelty. In addition to expressing shapes using mesh coordinates, the signed distance function for each node is evaluated and used to augment the inputs to the trunk network of the Geom-DeepONet, thereby capturing both explicit and implicit representations of the 3D shapes. The powerful geometric encoding capability of a sinusoidal representation network (SIREN) is also exploited by replacing the classical feedforward neural networks in the trunk with SIREN. Additional data fusion between the branch and trunk networks is introduced by an element-wise product. A numerical <b>benchmark</b> was conducted to compare Geom-DeepONet to PointNet and vanilla DeepONet, where results show that our architecture trains fast with a small memory footprint and yields the most accurate results among the three with less than 2 MPa stress error. Results show a much lower generalization error of our architecture on unseen dissimilar designs than vanilla DeepONet. Once trained, the model can predict vector solutions, and speed can be over $10^5$ times faster than implicit finite element <b>simulations</b> for large meshes.</p></p class="citation"></blockquote><h3 id=22--294312-advanced-deep-operator-networks-to-predict-multiphysics-solution-fields-in-materials-processing-and-additive-manufacturing-shashank-kushwaha-et-al-2024>(2/2 | 294/312) Advanced Deep Operator Networks to Predict Multiphysics Solution Fields in Materials Processing and Additive Manufacturing (Shashank Kushwaha et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shashank Kushwaha, Jaewan Park, Seid Koric, Junyan He, Iwona Jasiuk, Diab Abueidda. (2024)<br><strong>Advanced Deep Operator Networks to Predict Multiphysics Solution Fields in Materials Processing and Additive Manufacturing</strong><br><button class=copy-to-clipboard title="Advanced Deep Operator Networks to Predict Multiphysics Solution Fields in Materials Processing and Additive Manufacturing" index=294>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-294 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CE<br>Categories: cs-CE, cs.CE<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14795v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14795v1.pdf filename=2403.14795v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Unlike classical artificial neural networks, which require retraining for each new set of parametric inputs, the Deep Operator Network (DeepONet), a lately introduced deep learning framework, approximates linear and nonlinear solution operators by taking parametric functions (infinite-dimensional objects) as inputs and mapping them to complete solution fields. In this paper, two newly devised DeepONet formulations with sequential learning and Residual U-Net (ResUNet) architectures are trained for the first time to simultaneously predict complete thermal and mechanical solution fields under variable loading, loading histories, process parameters, and even variable geometries. Two real-world applications are demonstrated: 1- coupled thermo-mechanical analysis of steel continuous casting with multiple visco-plastic constitutive laws and 2- sequentially coupled direct energy deposition for additive manufacturing. Despite highly challenging spatially variable target stress distributions, DeepONets can infer reasonably accurate full-field temperature and stress solutions several orders of magnitude faster than traditional and highly optimized finite-element analysis (FEA), even when FEA <b>simulations</b> are run on the latest high-performance computing platforms. The proposed DeepONet model&rsquo;s ability to provide field predictions almost instantly for unseen input parameters opens the door for future preliminary evaluation and design optimization of these vital industrial processes.</p></p class="citation"></blockquote><h2 id=q-bioqm-1>q-bio.QM (1)</h2><h3 id=11--295312-nana-and-migu-semantic-data-augmentation-techniques-to-enhance-protein-classification-in-graph-neural-networks-yi-shan-lan-et-al-2024>(1/1 | 295/312) NaNa and MiGu: Semantic Data Augmentation Techniques to Enhance Protein Classification in Graph Neural Networks (Yi-Shan Lan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yi-Shan Lan, Pin-Yu Chen, Tsung-Yi Ho. (2024)<br><strong>NaNa and MiGu: Semantic Data Augmentation Techniques to Enhance Protein Classification in Graph Neural Networks</strong><br><button class=copy-to-clipboard title="NaNa and MiGu: Semantic Data Augmentation Techniques to Enhance Protein Classification in Graph Neural Networks" index=295>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-295 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.QM<br>Categories: cs-AI, cs-LG, q-bio-QM, q-bio.QM<br>Keyword Score: 23<br>Keywords: Graph, Graph Neural Network, Data Augmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14736v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14736v2.pdf filename=2403.14736v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Protein classification tasks are essential in drug discovery. Real-world protein structures are dynamic, which will determine the properties of proteins. However, the existing machine learning methods, like ProNet (Wang et al., 2022a), only access limited conformational characteristics and protein side-chain features, leading to impractical protein structure and inaccuracy of protein classes in their predictions. In this paper, we propose novel semantic <b>data</b> <b>augmentation</b> methods, Novel Augmentation of New Node Attributes (NaNa), and Molecular Interactions and Geometric Upgrading (MiGu) to incorporate backbone chemical and side-chain biophysical information into protein classification tasks and a co-embedding residual learning framework. Specifically, we leverage molecular biophysical, secondary structure, chemical bonds, and ionic features of proteins to facilitate protein classification tasks. Furthermore, our semantic augmentation methods and the co-embedding residual learning framework can improve the performance of GIN (Xu et al., 2019) on EC and Fold datasets (Bairoch, 2000; Andreeva et al., 2007) by 16.41% and 11.33% respectively. Our code is available at <a href=https://github.com/r08b46009/Code_for_MIGU_NANA/tree/main>https://github.com/r08b46009/Code_for_MIGU_NANA/tree/main</a>.</p></p class="citation"></blockquote><h2 id=eesssp-1>eess.SP (1)</h2><h3 id=11--296312-rolling-bearing-fault-diagnosis-method-based-on-generative-adversarial-enhanced-multi-scale-convolutional-neural-network-model-maoxuan-zhou-et-al-2024>(1/1 | 296/312) Rolling bearing fault diagnosis method based on generative adversarial enhanced multi-scale convolutional neural network model (Maoxuan Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Maoxuan Zhou, Wei Kang, Kun He. (2024)<br><strong>Rolling bearing fault diagnosis method based on generative adversarial enhanced multi-scale convolutional neural network model</strong><br><button class=copy-to-clipboard title="Rolling bearing fault diagnosis method based on generative adversarial enhanced multi-scale convolutional neural network model" index=296>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-296 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SP<br>Categories: cs-LG, eess-SP, eess.SP<br>Keyword Score: 23<br>Keywords: Graph, Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15483v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15483v1.pdf filename=2403.15483v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In order to solve the problem that current <b>convolutional</b> <b>neural</b> <b>networks</b> can not capture the correlation features between the time domain signals of rolling bearings effectively, and the model accuracy is limited by the number and quality of samples, a rolling bearing fault diagnosis method based on generative adversarial enhanced multi-scale <b>convolutional</b> <b>neural</b> <b>network</b> model is proposed. Firstly, Gram angular field coding technique is used to encode the time domain signal of the rolling bearing and generate the feature map to retain the complete information of the vibration signal. Then, the re-sulting data is divided into a training set, a validation set, and a test set. Among them, the training set is input into the gradient penalty Wasserstein distance generation adversarial network to complete the training, and a new sample with similar features to the training sample is obtained, and then the original training set is expanded. Next, multi-scale <b>convolution</b> is used to extract the fault features of the extended training set, and the feature <b>graph</b> is normalized by example to overcome the influence of the difference in feature distribution. Finally, the attention mechanism is applied to the adaptive weighting of normalized features and the extraction of deep features, and the fault diagnosis is completed by the softmax classifier. Compared with ResNet method, the experimental results show that the proposed method has better generalization performance and anti-noise performance.</p></p class="citation"></blockquote><h2 id=csit-1>cs.IT (1)</h2><h3 id=11--297312-ris-aided-cooperative-mobile-edge-computing-computation-efficiency-maximization-via-joint-uplink-and-downlink-resource-allocation-zhenrong-liu-et-al-2024>(1/1 | 297/312) RIS-Aided Cooperative Mobile Edge Computing: Computation Efficiency Maximization via Joint Uplink and Downlink Resource Allocation (Zhenrong Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhenrong Liu, Zongze Li, Yi Gong, Yik-Chung Wu. (2024)<br><strong>RIS-Aided Cooperative Mobile Edge Computing: Computation Efficiency Maximization via Joint Uplink and Downlink Resource Allocation</strong><br><button class=copy-to-clipboard title="RIS-Aided Cooperative Mobile Edge Computing: Computation Efficiency Maximization via Joint Uplink and Downlink Resource Allocation" index=297>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-297 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14775v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14775v1.pdf filename=2403.14775v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In mobile edge computing (MEC) systems, the wireless channel condition is a critical factor affecting both the communication power consumption and computation rate of the offloading tasks. This paper exploits the idea of cooperative transmission and employing reconfigurable intelligent surface (RIS) in MEC to improve the channel condition and maximize computation efficiency (CE). The resulting problem couples various wireless resources in both uplink and downlink, which calls for the joint design of the user association, receive/downlink beamforming vectors, transmit power of users, task partition strategies for local computing and offloading, and uplink/downlink phase shifts at the RIS. To tackle the challenges brought by the combinatorial optimization problem, the group sparsity structure of the beamforming vectors determined by user association is exploited. Furthermore, while the CE does not explicitly depend on the downlink phase shifts, instead of simply finding a feasible solution, we exploit the hidden relationship between them and convert this relationship into an explicit form for optimization. Then the resulting problem is solved via the alternating maximization framework, and the nonconvexity of each subproblem is handled individually. <b>Simulation</b> results show that cooperative transmission and RIS deployment can significantly improve the CE and demonstrate the importance of optimizing the downlink phase shifts with an explicit form.</p></p class="citation"></blockquote><h2 id=eessas-3>eess.AS (3)</h2><h3 id=13--298312-speech-aware-neural-diarization-with-encoder-decoder-attractor-guided-by-attention-constraints-peiying-lee-et-al-2024>(1/3 | 298/312) Speech-Aware Neural Diarization with Encoder-Decoder Attractor Guided by Attention Constraints (PeiYing Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>PeiYing Lee, HauYun Guo, Berlin Chen. (2024)<br><strong>Speech-Aware Neural Diarization with Encoder-Decoder Attractor Guided by Attention Constraints</strong><br><button class=copy-to-clipboard title="Speech-Aware Neural Diarization with Encoder-Decoder Attractor Guided by Attention Constraints" index=298>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-298 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-SD, eess-AS, eess.AS<br>Keyword Score: 20<br>Keywords: Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14268v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14268v1.pdf filename=2403.14268v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>End-to-End Neural Diarization with Encoder-Decoder based Attractor (EEND-EDA) is an end-to-end neural model for automatic speaker segmentation and labeling. It achieves the capability to handle flexible number of speakers by estimating the number of attractors. EEND-EDA, however, struggles to accurately capture local speaker dynamics. This work proposes an auxiliary loss that aims to guide the <b>Transformer</b> encoders at the lower layer of EEND-EDA model to enhance the effect of <b>self-attention</b> modules using speaker activity information. The results evaluated on public dataset Mini LibriSpeech, demonstrates the effectiveness of the work, reducing Diarization Error Rate from 30.95% to 28.17%. We will release the source code on GitHub to allow further research and reproducibility.</p></p class="citation"></blockquote><h3 id=23--299312-adaproj-adaptively-scaled-angular-margin-subspace-projections-for-anomalous-sound-detection-with-auxiliary-classification-tasks-kevin-wilkinghoff-2024>(2/3 | 299/312) AdaProj: Adaptively Scaled Angular Margin Subspace Projections for Anomalous Sound Detection with Auxiliary Classification Tasks (Kevin Wilkinghoff, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kevin Wilkinghoff. (2024)<br><strong>AdaProj: Adaptively Scaled Angular Margin Subspace Projections for Anomalous Sound Detection with Auxiliary Classification Tasks</strong><br><button class=copy-to-clipboard title="AdaProj: Adaptively Scaled Angular Margin Subspace Projections for Anomalous Sound Detection with Auxiliary Classification Tasks" index=299>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-299 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-SD, eess-AS, eess.AS<br>Keyword Score: 20<br>Keywords: Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14179v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14179v1.pdf filename=2403.14179v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The state-of-the-art approach for semi-supervised anomalous sound detection is to first learn an embedding space by using auxiliary classification tasks based on meta information or <b>self-supervised</b> <b>learning</b> and then estimate the distribution of normal data. In this work, AdaProj a novel loss function is presented. In contrast to commonly used angular margin losses, which project data of each class as close as possible to their corresponding class centers, AdaProj learns to project data onto class-specific subspaces. By doing so, the resulting distributions of embeddings belonging to normal data are not required to be as restrictive as other loss functions allowing a more detailed view on the data. In experiments conducted on the DCASE2022 and DCASE2023 datasets, it is shown that using AdaProj to learn an embedding space significantly outperforms other commonly used loss functions and results in a state-of-the-art performance on the DCASE2023 dataset.</p></p class="citation"></blockquote><h3 id=33--300312-crowdsourced-multilingual-speech-intelligibility-testing-laura-lechler-et-al-2024>(3/3 | 300/312) Crowdsourced Multilingual Speech Intelligibility Testing (Laura Lechler et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Laura Lechler, Kamil Wojcicki. (2024)<br><strong>Crowdsourced Multilingual Speech Intelligibility Testing</strong><br><button class=copy-to-clipboard title="Crowdsourced Multilingual Speech Intelligibility Testing" index=300>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-300 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-AI, eess-AS, eess-SP, eess.AS<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14817v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14817v1.pdf filename=2403.14817v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the advent of generative audio features, there is an increasing need for rapid evaluation of their impact on speech intelligibility. Beyond the existing laboratory measures, which are expensive and do not scale well, there has been comparatively little work on crowdsourced assessment of intelligibility. Standards and <b>recommendations</b> are yet to be defined, and publicly available multilingual test materials are lacking. In response to this challenge, we propose an approach for a crowdsourced intelligibility assessment. We detail the test design, the collection and public release of the multilingual speech data, and the results of our early experiments.</p></p class="citation"></blockquote><h2 id=mathoc-1>math.OC (1)</h2><h3 id=11--301312-reinforcement-learning-design-for-quickest-change-detection-austin-cooper-et-al-2024>(1/1 | 301/312) Reinforcement Learning Design for Quickest Change Detection (Austin Cooper et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Austin Cooper, Sean Meyn. (2024)<br><strong>Reinforcement Learning Design for Quickest Change Detection</strong><br><button class=copy-to-clipboard title="Reinforcement Learning Design for Quickest Change Detection" index=301>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-301 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-IT, math-IT, math-OC, math.OC<br>Keyword Score: 20<br>Keywords: Reinforcement Learning, Stochastic Gradient Descent<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14109v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14109v1.pdf filename=2403.14109v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The field of quickest change detection (QCD) concerns design and analysis of algorithms to estimate in real time the time at which an important event takes place, and identify properties of the post-change behavior. It is shown in this paper that approaches based on <b>reinforcement</b> <b>learning</b> (RL) can be adapted based on any &ldquo;surrogate information state&rdquo; that is adapted to the observations. Hence we are left to choose both the surrogate information state process and the algorithm. For the former, it is argued that there are many choices available, based on a rich theory of asymptotic statistics for QCD. Two approaches to RL design are considered: (i) <b>Stochastic</b> <b>gradient</b> <b>descent</b> based on an actor-critic formulation. Theory is largely complete for this approach: the algorithm is unbiased, and will converge to a local minimum. However, it is shown that variance of <b>stochastic</b> <b>gradients</b> <b>can</b> be very large, necessitating the need for commensurately long run times; (ii) Q-learning algorithms based on a version of the projected Bellman equation. It is shown that the algorithm is stable, in the sense of bounded sample paths, and that a solution to the projected Bellman equation exists under mild conditions. Numerical experiments illustrate these findings, and provide a roadmap for algorithm design in more general settings.</p></p class="citation"></blockquote><h2 id=cssi-4>cs.SI (4)</h2><h3 id=14--302312-random-graph-modeling-a-survey-of-the-concepts-mikhail-drobyshevskiy-et-al-2024>(1/4 | 302/312) Random Graph Modeling: A survey of the concepts (Mikhail Drobyshevskiy et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mikhail Drobyshevskiy, Denis Turdakov. (2024)<br><strong>Random Graph Modeling: A survey of the concepts</strong><br><button class=copy-to-clipboard title="Random Graph Modeling: A survey of the concepts" index=302>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-302 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-SI, cs.SI<br>Keyword Score: 11<br>Keywords: Graph, Benchmarking, Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14415v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14415v1.pdf filename=2403.14415v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Random <b>graph</b> (RG) models play a central role in the complex networks analysis. They help to understand, control, and predict phenomena occurring, for instance, in social networks, biological networks, the Internet, etc. Despite a large number of RG models presented in the literature, there are few concepts underlying them. Instead of trying to classify a wide variety of very dispersed models, we capture and describe concepts they exploit considering preferential attachment, copying principle, hyperbolic <b>geometry,</b> recursively defined structure, edge switching, Monte Carlo sampling, etc. We analyze RG models, extract their basic principles, and build a taxonomy of concepts they are based on. We also discuss how these concepts are combined in RG models and how they work in typical applications like <b>benchmarks,</b> null models, and data anonymization.</p></p class="citation"></blockquote><h3 id=24--303312-from-perils-to-possibilities-understanding-how-human-and-ai-biases-affect-online-fora-virginia-morini-et-al-2024>(2/4 | 303/312) From Perils to Possibilities: Understanding how Human (and AI) Biases affect Online Fora (Virginia Morini et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Virginia Morini, Valentina Pansanella, Katherine Abramski, Erica Cau, Andrea Failla, Salvatore Citraro, Giulio Rossetti. (2024)<br><strong>From Perils to Possibilities: Understanding how Human (and AI) Biases affect Online Fora</strong><br><button class=copy-to-clipboard title="From Perils to Possibilities: Understanding how Human (and AI) Biases affect Online Fora" index=303>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-303 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-AI, cs-HC, cs-SI, cs.SI<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14298v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14298v1.pdf filename=2403.14298v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Social media platforms are online fora where users engage in discussions, share content, and build connections. This review explores the dynamics of social interactions, user-generated contents, and biases within the context of social media analysis (analyzing works that use the tools offered by complex network analysis and natural language processing) through the lens of three key points of view: online debates, online support, and human-AI interactions. On the one hand, we delineate the phenomenon of online debates, where polarization, misinformation, and echo chamber formation often proliferate, driven by algorithmic biases and extreme mechanisms of homophily. On the other hand, we explore the emergence of online support groups through users&rsquo; self-disclosure and social support mechanisms. Online debates and support mechanisms present a duality of both perils and possibilities within social media; perils of segregated communities and polarized debates, and possibilities of empathy narratives and self-help groups. This dichotomy also extends to a third perspective: users&rsquo; reliance on AI-generated content, such as the ones produced by <b>Large</b> <b>Language</b> <b>Models,</b> which can manifest both human biases hidden in training sets and non-human biases that emerge from their artificial neural architectures. Analyzing interdisciplinary approaches, we aim to deepen the understanding of the complex interplay between social interactions, user-generated content, and biases within the realm of social media ecosystems.</p></p class="citation"></blockquote><h3 id=34--304312-dynamical-importance-and-network-perturbations-ethan-young-et-al-2024>(3/4 | 304/312) Dynamical importance and network perturbations (Ethan Young et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ethan Young, Mason A. Porter. (2024)<br><strong>Dynamical importance and network perturbations</strong><br><button class=copy-to-clipboard title="Dynamical importance and network perturbations" index=304>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-304 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-SI, cs.SI, math-DS, nlin-AO, physics-soc-ph<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14584v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14584v1.pdf filename=2403.14584v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The leading eigenvalue $\lambda$ of the adjacency matrix of a <b>graph</b> exerts much influence on the behavior of dynamical processes on that <b>graph.</b> It is thus relevant to relate notions of the importance (specifically, centrality measures) of network structures to $\lambda$ and its associated eigenvector. We study a previously derived measure of edge importance known as &ldquo;dynamical importance&rdquo;, which estimates how much $\lambda$ changes when one removes an edge from a <b>graph</b> or adds an edge to it. We examine the accuracy of this estimate for different network structures and compare it to the true change in $\lambda$ after an edge removal or edge addition. We then derive a first-order approximation of the change in the leading eigenvector. We also consider the effects of edge additions on Kuramoto dynamics on networks, and we express the Kuramoto order parameter in terms of dynamical importance. Through our analysis and computational experiments, we find that studying dynamical importance can improve understanding of the relationship between network perturbations and dynamical processes on networks.</p></p class="citation"></blockquote><h3 id=44--305312-collecting-influencers-a-comparative-study-of-online-network-crawlers-mikhail-drobyshevskiy-et-al-2024>(4/4 | 305/312) Collecting Influencers: A Comparative Study of Online Network Crawlers (Mikhail Drobyshevskiy et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mikhail Drobyshevskiy, Denis Aivazov, Denis Turdakov, Alexander Yatskov, Maksim Varlamov, Danil Shayhelislamov. (2024)<br><strong>Collecting Influencers: A Comparative Study of Online Network Crawlers</strong><br><button class=copy-to-clipboard title="Collecting Influencers: A Comparative Study of Online Network Crawlers" index=305>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-305 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-SI, cs.SI<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14351v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14351v1.pdf filename=2403.14351v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Online network crawling tasks require a lot of efforts for the researchers to collect the data. One of them is identification of important nodes, which has many applications starting from viral marketing to the prevention of disease spread. Various crawling algorithms has been suggested but their efficiency is not studied well. In this paper we compared six known crawlers on the task of collecting the fraction of the most influential nodes of <b>graph.</b> We analyzed crawlers behavior for four measures of node influence: node degree, k-coreness, betweenness centrality, and eccentricity. The experiments confirmed that greedy methods perform the best in many settings, but the cases exist when they are very inefficient.</p></p class="citation"></blockquote><h2 id=csni-1>cs.NI (1)</h2><h3 id=11--306312-a-mathematical-introduction-to-deep-reinforcement-learning-for-5g6g-applications-farhad-rezazadeh-2024>(1/1 | 306/312) A Mathematical Introduction to Deep Reinforcement Learning for 5G/6G Applications (Farhad Rezazadeh, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Farhad Rezazadeh. (2024)<br><strong>A Mathematical Introduction to Deep Reinforcement Learning for 5G/6G Applications</strong><br><button class=copy-to-clipboard title="A Mathematical Introduction to Deep Reinforcement Learning for 5G/6G Applications" index=306>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-306 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14516v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14516v1.pdf filename=2403.14516v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Algorithmic innovation can unleash the potential of the beyond 5G (B5G)/6G communication systems. Artificial intelligence (AI)-driven zero-touch network slicing is envisaged as a promising cutting-edge technology to harness the full potential of heterogeneous 6G networks and enable the automation of demand-aware management and orchestration (MANO). The network slicing continues towards numerous slices with micro or macro services in 6G networks, and thereby, designing a robust, stable, and distributed learning mechanism is considered a necessity. In this regard, robust brain-inspired and dopamine-like learning methods, such as Actor-Critic approaches, can play a vital role. The tutorial begins with an introduction to network slicing, <b>reinforcement</b> <b>learning</b> (RL), and recent state-of-the-art (SoA) algorithms. Then, the paper elaborates on the combination of value-based and policy-based methods in the form of Actor-Critic techniques tailored to the needs of future wireless networks.</p></p class="citation"></blockquote><h2 id=cond-matdis-nn-1>cond-mat.dis-nn (1)</h2><h3 id=11--307312-quantum-activated-neural-reservoirs-on-chip-open-up-large-hardware-security-models-for-resilient-authentication-zhao-he-et-al-2024>(1/1 | 307/312) Quantum-activated neural reservoirs on-chip open up large hardware security models for resilient authentication (Zhao He et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhao He, Maxim S. Elizarov, Ning Li, Fei Xiang, Andrea Fratalocchi. (2024)<br><strong>Quantum-activated neural reservoirs on-chip open up large hardware security models for resilient authentication</strong><br><button class=copy-to-clipboard title="Quantum-activated neural reservoirs on-chip open up large hardware security models for resilient authentication" index=307>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-307 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cond-mat.dis-nn<br>Categories: cond-mat-dis-nn, cond-mat.dis-nn, cs-AI, cs-CR<br>Keyword Score: 10<br>Keywords: Recurrent Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14188v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14188v1.pdf filename=2403.14188v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Quantum artificial intelligence is a frontier of artificial intelligence research, pioneering quantum AI-powered circuits to address problems beyond the reach of deep learning with classical architectures. This work implements a large-scale quantum-activated <b>recurrent</b> <b>neural</b> <b>network</b> possessing more than 3 trillion hardware nodes/cm$^2$, originating from repeatable atomic-scale nucleation dynamics in an amorphous material integrated on-chip, controlled with 0.07 nW electric power per readout channel. Compared to the best-performing reservoirs currently reported, this implementation increases the scale of the network by two orders of magnitude and reduces the power consumption by six, reaching power efficiencies in the range of the human brain, dissipating 0.2 nW/neuron. When interrogated by a classical input, the chip implements a large-scale hardware security model, enabling dictionary-free authentication secure against statistical inference attacks, including AI&rsquo;s present and future development, even for an adversary with a copy of all the classical components available. Experimental tests report 99.6% reliability, 100% user authentication accuracy, and an ideal 50% key uniqueness. Due to its quantum nature, the chip supports a bit density per feature size area three times higher than the best technology available, with the capacity to store more than $2^{1104}$ keys in a footprint of 1 cm$^2$. Such a quantum-powered platform could help counteract the emerging form of warfare led by the cybercrime industry in breaching authentication to target small to large-scale facilities, from private users to intelligent energy grids.</p></p class="citation"></blockquote><h2 id=csds-2>cs.DS (2)</h2><h3 id=12--308312-a-differentially-private-clustering-algorithm-for-well-clustered-graphs-weiqiang-he-et-al-2024>(1/2 | 308/312) A Differentially Private Clustering Algorithm for Well-Clustered Graphs (Weiqiang He et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Weiqiang He, Hendrik Fichtenberger, Pan Peng. (2024)<br><strong>A Differentially Private Clustering Algorithm for Well-Clustered Graphs</strong><br><button class=copy-to-clipboard title="A Differentially Private Clustering Algorithm for Well-Clustered Graphs" index=308>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-308 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-CR, cs-DS, cs-LG, cs.DS<br>Keyword Score: 9<br>Keywords: Graph, Benchmarking, Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14332v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14332v1.pdf filename=2403.14332v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study differentially private (DP) algorithms for recovering clusters in well-clustered <b>graphs,</b> which are <b>graphs</b> whose vertex set can be partitioned into a small number of sets, each inducing a subgraph of high inner conductance and small outer conductance. Such <b>graphs</b> have widespread application as a <b>benchmark</b> in the theoretical analysis of spectral <b>clustering.</b> We provide an efficient ($\epsilon$,$\delta$)-DP algorithm tailored specifically for such <b>graphs.</b> Our algorithm draws inspiration from the recent work of Chen et al., who developed DP algorithms for recovery of stochastic block models in cases where the <b>graph</b> comprises exactly two nearly-balanced clusters. Our algorithm works for well-clustered <b>graphs</b> with $k$ nearly-balanced clusters, and the misclassification ratio almost matches the one of the best-known non-private algorithms. We conduct experimental evaluations on datasets with known ground truth clusters to substantiate the prowess of our algorithm. We also show that any (pure) $\epsilon$-DP algorithm would result in substantial error.</p></p class="citation"></blockquote><h3 id=22--309312-induced-subforests-and-superforests-dieter-rautenbach-et-al-2024>(2/2 | 309/312) Induced Subforests and Superforests (Dieter Rautenbach et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dieter Rautenbach, Florian Werner. (2024)<br><strong>Induced Subforests and Superforests</strong><br><button class=copy-to-clipboard title="Induced Subforests and Superforests" index=309>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-309 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs.DS, math-CO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14492v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14492v1.pdf filename=2403.14492v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> isomorphism, subgraph isomorphism, and maximum common subgraphs are classical well-investigated objects. Their (parameterized) complexity and efficiently tractable cases have been studied. In the present paper, for a given set of forests, we study maximum common induced subforests and minimum common induced superforests. We show that finding a maximum subforest is NP-hard already for two subdivided stars while finding a minimum superforest is tractable for two trees but NP-hard for three trees. For a given set of $k$ trees, we present an efficient greedy $\left(\frac{k}{2}-\frac{1}{2}+\frac{1}{k}\right)$-approximation algorithm for the minimum superforest problem. Finally, we present a polynomial time approximation scheme for the maximum subforest problem for any given set of forests.</p></p class="citation"></blockquote><h2 id=mathco-1>math.CO (1)</h2><h3 id=11--310312-recognizing-relating-edges-in-graphs-without-cycles-of-length-6-vadim-e-levit-et-al-2024>(1/1 | 310/312) Recognizing Relating Edges in Graphs without Cycles of Length 6 (Vadim E. Levit et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vadim E. Levit, David Tankus. (2024)<br><strong>Recognizing Relating Edges in Graphs without Cycles of Length 6</strong><br><button class=copy-to-clipboard title="Recognizing Relating Edges in Graphs without Cycles of Length 6" index=310>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-310 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.CO<br>Categories: 05C69, G-2-2, cs-DM, math-CO, math.CO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14824v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14824v1.pdf filename=2403.14824v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A <b>graph</b> $G$ is well-covered if all maximal independent sets are of the same cardinality. Let $w:V(G) \longrightarrow\mathbb{R}$ be a weight function. Then $G$ is $w$-well-covered if all maximal independent sets are of the same weight. An edge $xy \in E(G)$ is relating if there exists an independent set $S$ such that both $S \cup {x}$ and $S \cup {y}$ are maximal independent sets in the <b>graph.</b> If $xy$ is relating then $w(x)=w(y)$ for every weight function $w$ such that $G$ is $w$-well-covered. Relating edges play an important role in investigating $w$-well-covered <b>graphs.</b> The decision problem whether an edge in a <b>graph</b> is relating is NP-complete. We prove that the problem remains NP-complete when the input is restricted to <b>graphs</b> without cycles of length $6$. This is an unexpected result because recognizing relating edges is known to be polynomially solvable for <b>graphs</b> without cycles of lengths $4$ and $6$, <b>graphs</b> without cycles of lengths $5$ and $6$, and <b>graphs</b> without cycles of lengths $6$ and $7$. A <b>graph</b> $G$ belongs to the class $W_2$ if every two pairwise disjoint independent sets in $G$ are included in two pairwise disjoint maximum independent sets. It is known that if $G$ belongs to the class $W_2$, then it is well-covered. A vertex $v \in V(G)$ is shedding if for every independent set $S \subseteq V(G)-N[v]$, there exists a vertex $u \in N(v)$ such that $S \cup {u}$ is independent. Shedding vertices play an important role in studying the class $W_2$. Recognizing shedding vertices is co-NP-complete, even when the input is restricted to triangle-free <b>graphs.</b> We prove that the problem is co-NP-complete for <b>graphs</b> without cycles of length $6$.</p></p class="citation"></blockquote><h2 id=csdb-1>cs.DB (1)</h2><h3 id=11--311312-quantifying-semantic-query-similarity-for-automated-linear-sql-grading-a-graph-based-approach-leo-köberlein-et-al-2024>(1/1 | 311/312) Quantifying Semantic Query Similarity for Automated Linear SQL Grading: A Graph-based Approach (Leo Köberlein et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Leo Köberlein, Dominik Probst, Richard Lenz. (2024)<br><strong>Quantifying Semantic Query Similarity for Automated Linear SQL Grading: A Graph-based Approach</strong><br><button class=copy-to-clipboard title="Quantifying Semantic Query Similarity for Automated Linear SQL Grading: A Graph-based Approach" index=311>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-311 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DB<br>Categories: cs-DB, cs.DB<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14441v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14441v1.pdf filename=2403.14441v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Quantifying the semantic similarity between database queries is a critical challenge with broad applications, ranging from query log analysis to automated educational assessment of SQL skills. Traditional methods often rely solely on syntactic comparisons or are limited to checking for semantic equivalence. This paper introduces a novel <b>graph-based</b> approach to measure the semantic dissimilarity between SQL queries. Queries are represented as nodes in an implicit <b>graph,</b> while the transitions between nodes are called edits, which are weighted by semantic dissimilarity. We employ shortest path algorithms to identify the lowest-cost edit sequence between two given queries, thereby defining a quantifiable measure of semantic distance. A prototype implementation of this technique has been evaluated through an empirical study, which strongly suggests that our method provides more accurate and comprehensible grading compared to existing techniques. Moreover, the results indicate that our approach comes close to the quality of manual grading, making it a robust tool for diverse database query comparison tasks.</p></p class="citation"></blockquote><h2 id=csar-1>cs.AR (1)</h2><h3 id=11--312312-e-syn-e-graph-rewriting-with-technology-aware-cost-functions-for-logic-synthesis-chen-chen-et-al-2024>(1/1 | 312/312) E-Syn: E-Graph Rewriting with Technology-Aware Cost Functions for Logic Synthesis (Chen Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chen Chen, Guangyu Hu, Dongsheng Zuo, Cunxi Yu, Yuzhe Ma, Hongce Zhang. (2024)<br><strong>E-Syn: E-Graph Rewriting with Technology-Aware Cost Functions for Logic Synthesis</strong><br><button class=copy-to-clipboard title="E-Syn: E-Graph Rewriting with Technology-Aware Cost Functions for Logic Synthesis" index=312>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-312 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AR<br>Categories: cs-AR, cs.AR<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14242v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14242v2.pdf filename=2403.14242v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Logic synthesis plays a crucial role in the digital design flow. It has a decisive influence on the final Quality of Results (QoR) of the circuit implementations. However, existing multi-level logic optimization algorithms often employ greedy approaches with a series of local optimization steps. Each step breaks the circuit into small pieces (e.g., k-feasible cuts) and applies incremental changes to individual pieces separately. These local optimization steps could limit the exploration space and may miss opportunities for significant improvements. To address the limitation, this paper proposes using e-graph in logic synthesis. The new workflow, named Esyn, makes use of the well-established e-graph infrastructure to efficiently perform logic rewriting. It explores a diverse set of equivalent Boolean representations while allowing technology-aware cost functions to better support delay-oriented and area-oriented logic synthesis. Experiments over a wide range of <b>benchmark</b> designs show our proposed logic optimization approach reaches a wider design space compared to the commonly used AIG-based logic synthesis flow. It achieves on average 15.29% delay saving in delay-oriented synthesis and 6.42% area saving for area-oriented synthesis.</p></p class="citation"></blockquote></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/ title="arXiv @ 2024.03.22" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Prev</div><div class=next-prev-text>arXiv @ 2024.03.22</div></a></div><div class="col-md-6 next-article"><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/ title="arXiv @ 2024.03.24" class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>arXiv @ 2024.03.24</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#primary-categories>Primary Categories</a></li><li><a href=#keywords>Keywords</a></li><li><a href=#cscl-52>cs.CL (52)</a><ul><li><a href=#152--1312-building-accurate-translation-tailored-llms-with-language-aware-instruction-tuning-changtong-zan-et-al-2024>(1/52 | 1/312) Building Accurate Translation-Tailored LLMs with Language Aware Instruction Tuning (Changtong Zan et al., 2024)</a></li><li><a href=#252--2312-llm-based-extraction-of-contradictions-from-patents-stefan-trapp-et-al-2024>(2/52 | 2/312) LLM-based Extraction of Contradictions from Patents (Stefan Trapp et al., 2024)</a></li><li><a href=#352--3312-a-chain-of-thought-prompting-approach-with-llms-for-evaluating-students-formative-assessment-responses-in-science-clayton-cohn-et-al-2024>(3/52 | 3/312) A Chain-of-Thought Prompting Approach with LLMs for Evaluating Students&rsquo; Formative Assessment Responses in Science (Clayton Cohn et al., 2024)</a></li><li><a href=#452--4312-fit-rag-black-box-rag-with-factual-information-and-token-reduction-yuren-mao-et-al-2024>(4/52 | 4/312) FIT-RAG: Black-Box RAG with Factual Information and Token Reduction (Yuren Mao et al., 2024)</a></li><li><a href=#552--5312-mmidr-teaching-large-language-model-to-interpret-multimodal-misinformation-via-knowledge-distillation-longzheng-wang-et-al-2024>(5/52 | 5/312) MMIDR: Teaching Large Language Model to Interpret Multimodal Misinformation via Knowledge Distillation (Longzheng Wang et al., 2024)</a></li><li><a href=#652--6312-gtbls-generating-tables-from-text-by-conditional-question-answering-anirudh-sundar-et-al-2024>(6/52 | 6/312) gTBLS: Generating Tables from Text by Conditional Question Answering (Anirudh Sundar et al., 2024)</a></li><li><a href=#752--7312-k-act2emo-korean-commonsense-knowledge-graph-for-indirect-emotional-expression-kyuhee-kim-et-al-2024>(7/52 | 7/312) K-Act2Emo: Korean Commonsense Knowledge Graph for Indirect Emotional Expression (Kyuhee Kim et al., 2024)</a></li><li><a href=#852--8312-chainlm-empowering-large-language-models-with-improved-chain-of-thought-prompting-xiaoxue-cheng-et-al-2024>(8/52 | 8/312) ChainLM: Empowering Large Language Models with Improved Chain-of-Thought Prompting (Xiaoxue Cheng et al., 2024)</a></li><li><a href=#952--9312-layoutllm-large-language-model-instruction-tuning-for-visually-rich-document-understanding-masato-fujitake-2024>(9/52 | 9/312) LayoutLLM: Large Language Model Instruction Tuning for Visually Rich Document Understanding (Masato Fujitake, 2024)</a></li><li><a href=#1052--10312-autore-document-level-relation-extraction-with-large-language-models-xue-lilong-et-al-2024>(10/52 | 10/312) AutoRE: Document-Level Relation Extraction with Large Language Models (Xue Lilong et al., 2024)</a></li><li><a href=#1152--11312-from-large-to-tiny-distilling-and-refining-mathematical-expertise-for-math-word-problems-with-weakly-supervision-qingwen-lin-et-al-2024>(11/52 | 11/312) From Large to Tiny: Distilling and Refining Mathematical Expertise for Math Word Problems with Weakly Supervision (Qingwen Lin et al., 2024)</a></li><li><a href=#1252--12312-reinforcement-learning-from-reflective-feedback-rlrf-aligning-and-improving-llms-via-fine-grained-self-reflection-kyungjae-lee-et-al-2024>(12/52 | 12/312) Reinforcement Learning from Reflective Feedback (RLRF): Aligning and Improving LLMs via Fine-Grained Self-Reflection (Kyungjae Lee et al., 2024)</a></li><li><a href=#1352--13312-chatgpt-alternative-solutions-large-language-models-survey-hanieh-alipour-et-al-2024>(13/52 | 13/312) ChatGPT Alternative Solutions: Large Language Models Survey (Hanieh Alipour et al., 2024)</a></li><li><a href=#1452--14312-a-multimodal-approach-to-device-directed-speech-detection-with-large-language-models-dominik-wagner-et-al-2024>(14/52 | 14/312) A Multimodal Approach to Device-Directed Speech Detection with Large Language Models (Dominik Wagner et al., 2024)</a></li><li><a href=#1552--15312-benchmarking-chinese-commonsense-reasoning-of-llms-from-chinese-specifics-to-reasoning-memorization-correlations-jiaxing-sun-et-al-2024>(15/52 | 15/312) Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations (Jiaxing Sun et al., 2024)</a></li><li><a href=#1652--16312-improving-the-robustness-of-large-language-models-via-consistency-alignment-yukun-zhao-et-al-2024>(16/52 | 16/312) Improving the Robustness of Large Language Models via Consistency Alignment (Yukun Zhao et al., 2024)</a></li><li><a href=#1752--17312-context-quality-matters-in-training-fusion-in-decoder-for-extractive-open-domain-question-answering-kosuke-akimoto-et-al-2024>(17/52 | 17/312) Context Quality Matters in Training Fusion-in-Decoder for Extractive Open-Domain Question Answering (Kosuke Akimoto et al., 2024)</a></li><li><a href=#1852--18312-from-handcrafted-features-to-llms-a-brief-survey-for-machine-translation-quality-estimation-haofei-zhao-et-al-2024>(18/52 | 18/312) From Handcrafted Features to LLMs: A Brief Survey for Machine Translation Quality Estimation (Haofei Zhao et al., 2024)</a></li><li><a href=#1952--19312-comparing-plausibility-estimates-in-base-and-instruction-tuned-large-language-models-carina-kauf-et-al-2024>(19/52 | 19/312) Comparing Plausibility Estimates in Base and Instruction-Tuned Large Language Models (Carina Kauf et al., 2024)</a></li><li><a href=#2052--20312-the-opportunities-and-risks-of-large-language-models-in-mental-health-hannah-r-lawrence-et-al-2024>(20/52 | 20/312) The opportunities and risks of large language models in mental health (Hannah R. Lawrence et al., 2024)</a></li><li><a href=#2152--21312-adaptive-rag-learning-to-adapt-retrieval-augmented-large-language-models-through-question-complexity-soyeong-jeong-et-al-2024>(21/52 | 21/312) Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity (Soyeong Jeong et al., 2024)</a></li><li><a href=#2252--22312-sequence-to-sequence-language-models-for-character-and-emotion-detection-in-dream-narratives-gustave-cortal-2024>(22/52 | 22/312) Sequence-to-Sequence Language Models for Character and Emotion Detection in Dream Narratives (Gustave Cortal, 2024)</a></li><li><a href=#2352--23312-extracting-emotion-phrases-from-tweets-using-bart-mahdi-rezapour-2024>(23/52 | 23/312) Extracting Emotion Phrases from Tweets using BART (Mahdi Rezapour, 2024)</a></li><li><a href=#2452--24312-lexicon-level-contrastive-visual-grounding-improves-language-modeling-chengxu-zhuang-et-al-2024>(24/52 | 24/312) Lexicon-Level Contrastive Visual-Grounding Improves Language Modeling (Chengxu Zhuang et al., 2024)</a></li><li><a href=#2552--25312-open-source-conversational-llms-do-not-know-most-spanish-words-javier-conde-et-al-2024>(25/52 | 25/312) Open Source Conversational LLMs do not know most Spanish words (Javier Conde et al., 2024)</a></li><li><a href=#2652--26312-detoxifying-large-language-models-via-knowledge-editing-mengru-wang-et-al-2024>(26/52 | 26/312) Detoxifying Large Language Models via Knowledge Editing (Mengru Wang et al., 2024)</a></li><li><a href=#2752--27312-dermacen-analytica-a-novel-methodology-integrating-multi-modal-large-language-models-with-machine-learning-in-tele-dermatology-dimitrios-p-panagoulias-et-al-2024>(27/52 | 27/312) Dermacen Analytica: A Novel Methodology Integrating Multi-Modal Large Language Models with Machine Learning in tele-dermatology (Dimitrios P. Panagoulias et al., 2024)</a></li><li><a href=#2852--28312-large-scale-label-interpretation-learning-for-few-shot-named-entity-recognition-jonas-golde-et-al-2024>(28/52 | 28/312) Large-Scale Label Interpretation Learning for Few-Shot Named Entity Recognition (Jonas Golde et al., 2024)</a></li><li><a href=#2952--29312-enhancing-medical-support-in-the-arabic-language-through-personalized-chatgpt-assistance-mohamed-issa-et-al-2024>(29/52 | 29/312) Enhancing Medical Support in the Arabic Language Through Personalized ChatGPT Assistance (Mohamed Issa et al., 2024)</a></li><li><a href=#3052--30312-large-language-models-for-multi-choice-question-classification-of-medical-subjects-víctor-ponce-lópez-2024>(30/52 | 30/312) Large Language Models for Multi-Choice Question Classification of Medical Subjects (Víctor Ponce-López, 2024)</a></li><li><a href=#3152--31312-multi-level-explanations-for-generative-language-models-lucas-monteiro-paes-et-al-2024>(31/52 | 31/312) Multi-Level Explanations for Generative Language Models (Lucas Monteiro Paes et al., 2024)</a></li><li><a href=#3252--32312-erd-a-framework-for-improving-llm-reasoning-for-cognitive-distortion-classification-sehee-lim-et-al-2024>(32/52 | 32/312) ERD: A Framework for Improving LLM Reasoning for Cognitive Distortion Classification (Sehee Lim et al., 2024)</a></li><li><a href=#3352--33312-edt-improving-large-language-models-generation-by-entropy-based-dynamic-temperature-sampling-shimao-zhang-et-al-2024>(33/52 | 33/312) EDT: Improving Large Language Models&rsquo; Generation by Entropy-based Dynamic Temperature Sampling (Shimao Zhang et al., 2024)</a></li><li><a href=#3452--34312-rakutenai-7b-extending-large-language-models-for-japanese-rakuten-group-et-al-2024>(34/52 | 34/312) RakutenAI-7B: Extending Large Language Models for Japanese (Rakuten Group et al., 2024)</a></li><li><a href=#3552--35312-sequential-decision-making-for-inline-text-autocomplete-rohan-chitnis-et-al-2024>(35/52 | 35/312) Sequential Decision-Making for Inline Text Autocomplete (Rohan Chitnis et al., 2024)</a></li><li><a href=#3652--36312-visual-analytics-for-fine-grained-text-classification-models-and-datasets-munkhtulga-battogtokh-et-al-2024>(36/52 | 36/312) Visual Analytics for Fine-grained Text Classification Models and Datasets (Munkhtulga Battogtokh et al., 2024)</a></li><li><a href=#3752--37312-prediction-of-translation-techniques-for-the-translation-process-fan-zhou-et-al-2024>(37/52 | 37/312) Prediction of Translation Techniques for the Translation Process (Fan Zhou et al., 2024)</a></li><li><a href=#3852--38312-more-than-just-statistical-recurrence-human-and-machine-unsupervised-learning-of-māori-word-segmentation-across-morphological-processes-ashvini-varatharaj-et-al-2024>(38/52 | 38/312) More than Just Statistical Recurrence: Human and Machine Unsupervised Learning of Māori Word Segmentation across Morphological Processes (Ashvini Varatharaj et al., 2024)</a></li><li><a href=#3952--39312-locating-and-mitigating-gender-bias-in-large-language-models-yuchen-cai-et-al-2024>(39/52 | 39/312) Locating and Mitigating Gender Bias in Large Language Models (Yuchen Cai et al., 2024)</a></li><li><a href=#4052--40312-wikifactdiff-a-large-realistic-and-temporally-adaptable-dataset-for-atomic-factual-knowledge-update-in-causal-language-models-hichem-ammar-khodja-et-al-2024>(40/52 | 40/312) WikiFactDiff: A Large, Realistic, and Temporally Adaptable Dataset for Atomic Factual Knowledge Update in Causal Language Models (Hichem Ammar Khodja et al., 2024)</a></li><li><a href=#4152--41312-automatic-annotation-of-grammaticality-in-child-caregiver-conversations-mitja-nikolaus-et-al-2024>(41/52 | 41/312) Automatic Annotation of Grammaticality in Child-Caregiver Conversations (Mitja Nikolaus et al., 2024)</a></li><li><a href=#4252--42312-m3av-a-multimodal-multigenre-and-multipurpose-audio-visual-academic-lecture-dataset-zhe-chen-et-al-2024>(42/52 | 42/312) M$^3$AV: A Multimodal, Multigenre, and Multipurpose Audio-Visual Academic Lecture Dataset (Zhe Chen et al., 2024)</a></li><li><a href=#4352--43312-mogam-a-multimodal-object-oriented-graph-attention-model-for-depression-detection-junyeop-cha-et-al-2024>(43/52 | 43/312) MOGAM: A Multimodal Object-oriented Graph Attention Model for Depression Detection (Junyeop Cha et al., 2024)</a></li><li><a href=#4452--44312-evaluating-the-performance-of-llms-on-technical-language-processing-tasks-andrew-kernycky-et-al-2024>(44/52 | 44/312) Evaluating the Performance of LLMs on Technical Language Processing tasks (Andrew Kernycky et al., 2024)</a></li><li><a href=#4552--45312-tams-translation-assisted-morphological-segmentation-enora-rice-et-al-2024>(45/52 | 45/312) TAMS: Translation-Assisted Morphological Segmentation (Enora Rice et al., 2024)</a></li><li><a href=#4652--46312-a-collection-of-pragmatic-similarity-judgments-over-spoken-dialog-utterances-nigel-g-ward-et-al-2024>(46/52 | 46/312) A Collection of Pragmatic-Similarity Judgments over Spoken Dialog Utterances (Nigel G. Ward et al., 2024)</a></li><li><a href=#4752--47312-the-era-of-semantic-decoding-maxime-peyrard-et-al-2024>(47/52 | 47/312) The Era of Semantic Decoding (Maxime Peyrard et al., 2024)</a></li><li><a href=#4852--48312-emergent-communication-and-learning-pressures-in-language-models-a-language-evolution-perspective-lukas-galke-et-al-2024>(48/52 | 48/312) Emergent communication and learning pressures in language models: a language evolution perspective (Lukas Galke et al., 2024)</a></li><li><a href=#4952--49312-editing-knowledge-representation-of-language-lodel-via-rephrased-prefix-prompts-yuchen-cai-et-al-2024>(49/52 | 49/312) Editing Knowledge Representation of Language Lodel via Rephrased Prefix Prompts (Yuchen Cai et al., 2024)</a></li><li><a href=#5052--50312-beyond-surface-similarity-detecting-subtle-semantic-shifts-in-financial-narratives-jiaxin-liu-et-al-2024>(50/52 | 50/312) Beyond Surface Similarity: Detecting Subtle Semantic Shifts in Financial Narratives (Jiaxin Liu et al., 2024)</a></li><li><a href=#5152--51312-is-reference-necessary-in-the-evaluation-of-nlg-systems-when-and-where-shuqian-sheng-et-al-2024>(51/52 | 51/312) Is Reference Necessary in the Evaluation of NLG Systems? When and Where? (Shuqian Sheng et al., 2024)</a></li><li><a href=#5252--52312-multi-level-feedback-generation-with-large-language-models-for-empowering-novice-peer-counselors-alicja-chaszczewicz-et-al-2024>(52/52 | 52/312) Multi-Level Feedback Generation with Large Language Models for Empowering Novice Peer Counselors (Alicja Chaszczewicz et al., 2024)</a></li></ul></li><li><a href=#csai-5>cs.AI (5)</a><ul><li><a href=#15--53312-react-meets-actre-autonomous-annotation-of-agent-trajectories-for-contrastive-self-training-zonghan-yang-et-al-2024>(1/5 | 53/312) ReAct Meets ActRe: Autonomous Annotation of Agent Trajectories for Contrastive Self-Training (Zonghan Yang et al., 2024)</a></li><li><a href=#25--54312-can-chatgpt-detect-deepfakes-a-study-of-using-multimodal-large-language-models-for-media-forensics-shan-jia-et-al-2024>(2/5 | 54/312) Can ChatGPT Detect DeepFakes? A Study of Using Multimodal Large Language Models for Media Forensics (Shan Jia et al., 2024)</a></li><li><a href=#35--55312-open-knowledge-base-canonicalization-with-multi-task-learning-bingchen-liu-et-al-2024>(3/5 | 55/312) Open Knowledge Base Canonicalization with Multi-task Learning (Bingchen Liu et al., 2024)</a></li><li><a href=#45--56312-establishing-a-leader-in-a-pairwise-comparisons-method-jacek-szybowski-et-al-2024>(4/5 | 56/312) Establishing a leader in a pairwise comparisons method (Jacek Szybowski et al., 2024)</a></li><li><a href=#55--57312-dourn-improving-douzero-by-residual-neural-networks-yiquan-chen-et-al-2024>(5/5 | 57/312) DouRN: Improving DouZero by Residual Neural Networks (Yiquan Chen et al., 2024)</a></li></ul></li><li><a href=#cslg-35>cs.LG (35)</a><ul><li><a href=#135--58312-exploring-the-potential-of-large-language-models-in-graph-generation-yang-yao-et-al-2024>(1/35 | 58/312) Exploring the Potential of Large Language Models in Graph Generation (Yang Yao et al., 2024)</a></li><li><a href=#235--59312-exploring-task-unification-in-graph-representation-learning-via-generative-approach-yulan-hu-et-al-2024>(2/35 | 59/312) Exploring Task Unification in Graph Representation Learning via Generative Approach (Yulan Hu et al., 2024)</a></li><li><a href=#335--60312-a-task-of-anomaly-detection-for-a-smart-satellite-internet-of-things-system-zilong-shao-2024>(3/35 | 60/312) A task of anomaly detection for a smart satellite Internet of things system (Zilong Shao, 2024)</a></li><li><a href=#435--61312-dp-rdm-adapting-diffusion-models-to-private-domains-without-fine-tuning-jonathan-lebensold-et-al-2024>(4/35 | 61/312) DP-RDM: Adapting Diffusion Models to Private Domains Without Fine-Tuning (Jonathan Lebensold et al., 2024)</a></li><li><a href=#535--62312-isplib-a-library-for-accelerating-graph-neural-networks-using-auto-tuned-sparse-operations-md-saidul-hoque-anik-et-al-2024>(5/35 | 62/312) iSpLib: A Library for Accelerating Graph Neural Networks using Auto-tuned Sparse Operations (Md Saidul Hoque Anik et al., 2024)</a></li><li><a href=#635--63312-deep-learning-for-trajectory-data-management-and-mining-a-survey-and-beyond-wei-chen-et-al-2024>(6/35 | 63/312) Deep Learning for Trajectory Data Management and Mining: A Survey and Beyond (Wei Chen et al., 2024)</a></li><li><a href=#735--64312-deep-active-learning-a-reality-check-edrina-gashi-et-al-2024>(7/35 | 64/312) Deep Active Learning: A Reality Check (Edrina Gashi et al., 2024)</a></li><li><a href=#835--65312-ai-and-memory-wall-amir-gholami-et-al-2024>(8/35 | 65/312) AI and Memory Wall (Amir Gholami et al., 2024)</a></li><li><a href=#935--66312-heuristic-algorithm-based-action-masking-reinforcement-learning-haam-rl-with-ensemble-inference-method-kyuwon-choi-et-al-2024>(9/35 | 66/312) Heuristic Algorithm-based Action Masking Reinforcement Learning (HAAM-RL) with Ensemble Inference Method (Kyuwon Choi et al., 2024)</a></li><li><a href=#1035--67312-hyperbolic-secant-representation-of-the-logistic-function-application-to-probabilistic-multiple-instance-learning-for-ct-intracranial-hemorrhage-detection-f-m-castro-macías-et-al-2024>(10/35 | 67/312) Hyperbolic Secant representation of the logistic function: Application to probabilistic Multiple Instance Learning for CT intracranial hemorrhage detection (F. M. Castro-Macías et al., 2024)</a></li><li><a href=#1135--68312-rambla-a-framework-for-evaluating-the-reliability-of-llms-as-assistants-in-the-biomedical-domain-william-james-bolton-et-al-2024>(11/35 | 68/312) RAmBLA: A Framework for Evaluating the Reliability of LLMs as Assistants in the Biomedical Domain (William James Bolton et al., 2024)</a></li><li><a href=#1235--69312-task-optimal-data-driven-surrogate-models-for-enmpc-via-differentiable-simulation-and-optimization-daniel-mayfrank-et-al-2024>(12/35 | 69/312) Task-optimal data-driven surrogate models for eNMPC via differentiable simulation and optimization (Daniel Mayfrank et al., 2024)</a></li><li><a href=#1335--70312-advancing-iiot-with-over-the-air-federated-learning-the-role-of-iterative-magnitude-pruning-fazal-muhammad-ali-khan-et-al-2024>(13/35 | 70/312) Advancing IIoT with Over-the-Air Federated Learning: The Role of Iterative Magnitude Pruning (Fazal Muhammad Ali Khan et al., 2024)</a></li><li><a href=#1435--71312-domainlab-a-modular-python-package-for-domain-generalization-in-deep-learning-xudong-sun-et-al-2024>(14/35 | 71/312) DomainLab: A modular Python package for domain generalization in deep learning (Xudong Sun et al., 2024)</a></li><li><a href=#1535--72312-diffstock-probabilistic-relational-stock-market-predictions-using-diffusion-models-divyanshu-daiya-et-al-2024>(15/35 | 72/312) DiffSTOCK: Probabilistic relational Stock Market Predictions using Diffusion Models (Divyanshu Daiya et al., 2024)</a></li><li><a href=#1635--73312-parameter-efficient-fine-tuning-for-large-models-a-comprehensive-survey-zeyu-han-et-al-2024>(16/35 | 73/312) Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey (Zeyu Han et al., 2024)</a></li><li><a href=#1735--74312-rethinking-adversarial-inverse-reinforcement-learning-from-the-angles-of-policy-imitation-and-transferable-reward-recovery-yangchun-zhang-et-al-2024>(17/35 | 74/312) Rethinking Adversarial Inverse Reinforcement Learning: From the Angles of Policy Imitation and Transferable Reward Recovery (Yangchun Zhang et al., 2024)</a></li><li><a href=#1835--75312-fedmef-towards-memory-efficient-federated-dynamic-pruning-hong-huang-et-al-2024>(18/35 | 75/312) FedMef: Towards Memory-efficient Federated Dynamic Pruning (Hong Huang et al., 2024)</a></li><li><a href=#1935--76312-foundation-models-for-time-series-analysis-a-tutorial-and-survey-yuxuan-liang-et-al-2024>(19/35 | 76/312) Foundation Models for Time Series Analysis: A Tutorial and Survey (Yuxuan Liang et al., 2024)</a></li><li><a href=#2035--77312-contrastive-balancing-representation-learning-for-heterogeneous-dose-response-curves-estimation-minqin-zhu-et-al-2024>(20/35 | 77/312) Contrastive Balancing Representation Learning for Heterogeneous Dose-Response Curves Estimation (Minqin Zhu et al., 2024)</a></li><li><a href=#2135--78312-hypothesis-driven-deep-learning-for-out-of-distribution-detection-yasith-jayawardana-et-al-2024>(21/35 | 78/312) Hypothesis-Driven Deep Learning for Out of Distribution Detection (Yasith Jayawardana et al., 2024)</a></li><li><a href=#2235--79312-soft-learning-probabilistic-circuits-soroush-ghandi-et-al-2024>(22/35 | 79/312) Soft Learning Probabilistic Circuits (Soroush Ghandi et al., 2024)</a></li><li><a href=#2335--80312-hypergale-asd-classification-via-hypergraph-gated-attention-with-learnable-hyperedges-mehul-arora-et-al-2024>(23/35 | 80/312) HyperGALE: ASD Classification via Hypergraph Gated Attention with Learnable Hyperedges (Mehul Arora et al., 2024)</a></li><li><a href=#2435--81312-emergent-world-models-and-latent-variable-estimation-in-chess-playing-language-models-adam-karvonen-2024>(24/35 | 81/312) Emergent World Models and Latent Variable Estimation in Chess-Playing Language Models (Adam Karvonen, 2024)</a></li><li><a href=#2535--82312-constrained-reinforcement-learning-with-smoothed-log-barrier-function-baohe-zhang-et-al-2024>(25/35 | 82/312) Constrained Reinforcement Learning with Smoothed Log Barrier Function (Baohe Zhang et al., 2024)</a></li><li><a href=#2635--83312-universal-feature-selection-for-simultaneous-interpretability-of-multitask-datasets-matt-raymond-et-al-2024>(26/35 | 83/312) Universal Feature Selection for Simultaneous Interpretability of Multitask Datasets (Matt Raymond et al., 2024)</a></li><li><a href=#2735--84312-physics-informed-diffusion-models-jan-hendrik-bastek-et-al-2024>(27/35 | 84/312) Physics-Informed Diffusion Models (Jan-Hendrik Bastek et al., 2024)</a></li><li><a href=#2835--85312-loop-improvement-an-efficient-approach-for-extracting-shared-features-from-heterogeneous-data-without-central-server-fei-li-et-al-2024>(28/35 | 85/312) Loop Improvement: An Efficient Approach for Extracting Shared Features from Heterogeneous Data without Central Server (Fei Li et al., 2024)</a></li><li><a href=#2935--86312-nabla-τ-gradient-based-and-task-agnostic-machine-unlearning-daniel-trippa-et-al-2024>(29/35 | 86/312) $\nabla τ$: Gradient-based and Task-Agnostic machine Unlearning (Daniel Trippa et al., 2024)</a></li><li><a href=#3035--87312-how-to-be-fair-a-study-of-label-and-selection-bias-marco-favier-et-al-2024>(30/35 | 87/312) How to be fair? A study of label and selection bias (Marco Favier et al., 2024)</a></li><li><a href=#3135--88312-a-unified-framework-for-model-editing-akshat-gupta-et-al-2024>(31/35 | 88/312) A Unified Framework for Model Editing (Akshat Gupta et al., 2024)</a></li><li><a href=#3235--89312-policy-mirror-descent-with-lookahead-kimon-protopapas-et-al-2024>(32/35 | 89/312) Policy Mirror Descent with Lookahead (Kimon Protopapas et al., 2024)</a></li><li><a href=#3335--90312-carbon-footprint-reduction-for-sustainable-data-centers-in-real-time-soumyendu-sarkar-et-al-2024>(33/35 | 90/312) Carbon Footprint Reduction for Sustainable Data Centers in Real-Time (Soumyendu Sarkar et al., 2024)</a></li><li><a href=#3435--91312-local-causal-discovery-with-linear-non-gaussian-cyclic-models-haoyue-dai-et-al-2024>(34/35 | 91/312) Local Causal Discovery with Linear non-Gaussian Cyclic Models (Haoyue Dai et al., 2024)</a></li><li><a href=#3535--92312-investigating-the-validity-of-structure-learning-algorithms-in-identifying-risk-factors-for-intervention-in-patients-with-diabetes-sheresh-zahoor-et-al-2024>(35/35 | 92/312) Investigating the validity of structure learning algorithms in identifying risk factors for intervention in patients with diabetes (Sheresh Zahoor et al., 2024)</a></li></ul></li><li><a href=#cscv-104>cs.CV (104)</a><ul><li><a href=#1104--93312-vurf-a-general-purpose-reasoning-and-self-refinement-framework-for-video-understanding-ahmad-mahmood-et-al-2024>(1/104 | 93/312) VURF: A General-purpose Reasoning and Self-refinement Framework for Video Understanding (Ahmad Mahmood et al., 2024)</a></li><li><a href=#2104--94312-multi-agent-vqa-exploring-multi-agent-foundation-models-in-zero-shot-visual-question-answering-bowen-jiang-et-al-2024>(2/104 | 94/312) Multi-Agent VQA: Exploring Multi-Agent Foundation Models in Zero-Shot Visual Question Answering (Bowen Jiang et al., 2024)</a></li><li><a href=#3104--95312-few-shot-adversarial-prompt-learning-on-vision-language-models-yiwei-zhou-et-al-2024>(3/104 | 95/312) Few-Shot Adversarial Prompt Learning on Vision-Language Models (Yiwei Zhou et al., 2024)</a></li><li><a href=#4104--96312-empowering-segmentation-ability-to-multi-modal-large-language-models-yuqi-yang-et-al-2024>(4/104 | 96/312) Empowering Segmentation Ability to Multi-modal Large Language Models (Yuqi Yang et al., 2024)</a></li><li><a href=#5104--97312-mathverse-does-your-multi-modal-llm-truly-see-the-diagrams-in-visual-math-problems-renrui-zhang-et-al-2024>(5/104 | 97/312) MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems? (Renrui Zhang et al., 2024)</a></li><li><a href=#6104--98312-language-repository-for-long-video-understanding-kumara-kahatapitiya-et-al-2024>(6/104 | 98/312) Language Repository for Long Video Understanding (Kumara Kahatapitiya et al., 2024)</a></li><li><a href=#7104--99312-learning-to-project-for-cross-task-knowledge-distillation-dylan-auty-et-al-2024>(7/104 | 99/312) Learning to Project for Cross-Task Knowledge Distillation (Dylan Auty et al., 2024)</a></li><li><a href=#8104--100312-unsupervised-audio-visual-segmentation-with-modality-alignment-swapnil-bhosale-et-al-2024>(8/104 | 100/312) Unsupervised Audio-Visual Segmentation with Modality Alignment (Swapnil Bhosale et al., 2024)</a></li><li><a href=#9104--101312-otseg-multi-prompt-sinkhorn-attention-for-zero-shot-semantic-segmentation-kwanyoung-kim-et-al-2024>(9/104 | 101/312) OTSeg: Multi-prompt Sinkhorn Attention for Zero-Shot Semantic Segmentation (Kwanyoung Kim et al., 2024)</a></li><li><a href=#10104--102312-transfer-learning-for-cross-dataset-isolated-sign-language-recognition-in-under-resourced-datasets-ahmet-alp-kindiroglu-et-al-2024>(10/104 | 102/312) Transfer Learning for Cross-dataset Isolated Sign Language Recognition in Under-Resourced Datasets (Ahmet Alp Kindiroglu et al., 2024)</a></li><li><a href=#11104--103312-vidla-video-language-alignment-at-scale-mamshad-nayeem-rizve-et-al-2024>(11/104 | 103/312) VidLA: Video-Language Alignment at Scale (Mamshad Nayeem Rizve et al., 2024)</a></li><li><a href=#12104--104312-less-but-better-enabling-generalized-zero-shot-learning-towards-unseen-domains-by-intrinsic-learning-from-redundant-llm-semantics-jiaqi-yue-et-al-2024>(12/104 | 104/312) Less but Better: Enabling Generalized Zero-shot Learning Towards Unseen Domains by Intrinsic Learning from Redundant LLM Semantics (Jiaqi Yue et al., 2024)</a></li><li><a href=#13104--105312-t-rex2-towards-generic-object-detection-via-text-visual-prompt-synergy-qing-jiang-et-al-2024>(13/104 | 105/312) T-Rex2: Towards Generic Object Detection via Text-Visual Prompt Synergy (Qing Jiang et al., 2024)</a></li><li><a href=#14104--106312-a-lightweight-attention-based-deep-network-via-multi-scale-feature-fusion-for-multi-view-facial-expression-recognition-ali-ezati-et-al-2024>(14/104 | 106/312) A Lightweight Attention-based Deep Network via Multi-Scale Feature Fusion for Multi-View Facial Expression Recognition (Ali Ezati et al., 2024)</a></li><li><a href=#15104--107312-masksam-towards-auto-prompt-sam-with-mask-classification-for-medical-image-segmentation-bin-xie-et-al-2024>(15/104 | 107/312) MaskSAM: Towards Auto-prompt SAM with Mask Classification for Medical Image Segmentation (Bin Xie et al., 2024)</a></li><li><a href=#16104--108312-text-enhanced-data-free-approach-for-federated-class-incremental-learning-minh-tuan-tran-et-al-2024>(16/104 | 108/312) Text-Enhanced Data-free Approach for Federated Class-Incremental Learning (Minh-Tuan Tran et al., 2024)</a></li><li><a href=#17104--109312-cobra-extending-mamba-to-multi-modal-large-language-model-for-efficient-inference-han-zhao-et-al-2024>(17/104 | 109/312) Cobra: Extending Mamba to Multi-Modal Large Language Model for Efficient Inference (Han Zhao et al., 2024)</a></li><li><a href=#18104--110312-hierarchical-text-to-vision-self-supervised-alignment-for-improved-histopathology-representation-learning-hasindri-watawana-et-al-2024>(18/104 | 110/312) Hierarchical Text-to-Vision Self Supervised Alignment for Improved Histopathology Representation Learning (Hasindri Watawana et al., 2024)</a></li><li><a href=#19104--111312-object-centric-domain-randomization-for-3d-shape-reconstruction-in-the-wild-junhyeong-cho-et-al-2024>(19/104 | 111/312) Object-Centric Domain Randomization for 3D Shape Reconstruction in the Wild (Junhyeong Cho et al., 2024)</a></li><li><a href=#20104--112312-psalm-pixelwise-segmentation-with-large-multi-modal-model-zheng-zhang-et-al-2024>(20/104 | 112/312) PSALM: Pixelwise SegmentAtion with Large Multi-Modal Model (Zheng Zhang et al., 2024)</a></li><li><a href=#21104--113312-unified-static-and-dynamic-network-efficient-temporal-filtering-for-video-grounding-jingjing-hu-et-al-2024>(21/104 | 113/312) Unified Static and Dynamic Network: Efficient Temporal Filtering for Video Grounding (Jingjing Hu et al., 2024)</a></li><li><a href=#22104--114312-preventing-catastrophic-forgetting-through-memory-networks-in-continuous-detection-gaurav-bhatt-et-al-2024>(22/104 | 114/312) Preventing Catastrophic Forgetting through Memory Networks in Continuous Detection (Gaurav Bhatt et al., 2024)</a></li><li><a href=#23104--115312-latent-diffusion-models-for-attribute-preserving-image-anonymization-luca-piano-et-al-2024>(23/104 | 115/312) Latent Diffusion Models for Attribute-Preserving Image Anonymization (Luca Piano et al., 2024)</a></li><li><a href=#24104--116312-adversary-robust-graph-based-learning-of-wsis-saba-heidari-gheshlaghi-et-al-2024>(24/104 | 116/312) Adversary-Robust Graph-Based Learning of WSIs (Saba Heidari Gheshlaghi et al., 2024)</a></li><li><a href=#25104--117312-oa-cnns-omni-adaptive-sparse-cnns-for-3d-semantic-segmentation-bohao-peng-et-al-2024>(25/104 | 117/312) OA-CNNs: Omni-Adaptive Sparse CNNs for 3D Semantic Segmentation (Bohao Peng et al., 2024)</a></li><li><a href=#26104--118312-scene-graph-vit-end-to-end-open-vocabulary-visual-relationship-detection-tim-salzmann-et-al-2024>(26/104 | 118/312) Scene-Graph ViT: End-to-End Open-Vocabulary Visual Relationship Detection (Tim Salzmann et al., 2024)</a></li><li><a href=#27104--119312-eventdance-unsupervised-source-free-cross-modal-adaptation-for-event-based-object-recognition-xu-zheng-et-al-2024>(27/104 | 119/312) EventDance: Unsupervised Source-free Cross-modal Adaptation for Event-based Object Recognition (Xu Zheng et al., 2024)</a></li><li><a href=#28104--120312-myvlm-personalizing-vlms-for-user-specific-queries-yuval-alaluf-et-al-2024>(28/104 | 120/312) MyVLM: Personalizing VLMs for User-Specific Queries (Yuval Alaluf et al., 2024)</a></li><li><a href=#29104--121312-token-transformation-matters-towards-faithful-post-hoc-explanation-for-vision-transformer-junyi-wu-et-al-2024>(29/104 | 121/312) Token Transformation Matters: Towards Faithful Post-hoc Explanation for Vision Transformer (Junyi Wu et al., 2024)</a></li><li><a href=#30104--122312-estimating-physical-information-consistency-of-channel-data-augmentation-for-remote-sensing-images-tom-burgert-et-al-2024>(30/104 | 122/312) Estimating Physical Information Consistency of Channel Data Augmentation for Remote Sensing Images (Tom Burgert et al., 2024)</a></li><li><a href=#31104--123312-hac-hash-grid-assisted-context-for-3d-gaussian-splatting-compression-yihang-chen-et-al-2024>(31/104 | 123/312) HAC: Hash-grid Assisted Context for 3D Gaussian Splatting Compression (Yihang Chen et al., 2024)</a></li><li><a href=#32104--124312-tensor-network-compressibility-of-convolutional-models-sukhbinder-singh-et-al-2024>(32/104 | 124/312) Tensor network compressibility of convolutional models (Sukhbinder Singh et al., 2024)</a></li><li><a href=#33104--125312-cfpl-fas-class-free-prompt-learning-for-generalizable-face-anti-spoofing-ajian-liu-et-al-2024>(33/104 | 125/312) CFPL-FAS: Class Free Prompt Learning for Generalizable Face Anti-spoofing (Ajian Liu et al., 2024)</a></li><li><a href=#34104--126312-open-vocabulary-attention-maps-with-token-optimization-for-semantic-segmentation-in-diffusion-models-pablo-marcos-manchón-et-al-2024>(34/104 | 126/312) Open-Vocabulary Attention Maps with Token Optimization for Semantic Segmentation in Diffusion Models (Pablo Marcos-Manchón et al., 2024)</a></li><li><a href=#35104--127312-weak-supervision-with-arbitrary-single-frame-for-micro--and-macro-expression-spotting-wang-wang-yu-et-al-2024>(35/104 | 127/312) Weak Supervision with Arbitrary Single Frame for Micro- and Macro-expression Spotting (Wang-Wang Yu et al., 2024)</a></li><li><a href=#36104--128312-toward-multi-class-anomaly-detection-exploring-class-aware-unified-model-against-inter-class-interference-xi-jiang-et-al-2024>(36/104 | 128/312) Toward Multi-class Anomaly Detection: Exploring Class-aware Unified Model against Inter-class Interference (Xi Jiang et al., 2024)</a></li><li><a href=#37104--129312-harmonizing-visual-and-textual-embeddings-for-zero-shot-text-to-image-customization-yeji-song-et-al-2024>(37/104 | 129/312) Harmonizing Visual and Textual Embeddings for Zero-Shot Text-to-Image Customization (Yeji Song et al., 2024)</a></li><li><a href=#38104--130312-efficient-video-diffusion-models-via-content-frame-motion-latent-decomposition-sihyun-yu-et-al-2024>(38/104 | 130/312) Efficient Video Diffusion Models via Content-Frame Motion-Latent Decomposition (Sihyun Yu et al., 2024)</a></li><li><a href=#39104--131312-c-tpt-calibrated-test-time-prompt-tuning-for-vision-language-models-via-text-feature-dispersion-hee-suk-yoon-et-al-2024>(39/104 | 131/312) C-TPT: Calibrated Test-Time Prompt Tuning for Vision-Language Models via Text Feature Dispersion (Hee Suk Yoon et al., 2024)</a></li><li><a href=#40104--132312-semantics-from-space-satellite-guided-thermal-semantic-segmentation-annotation-for-aerial-field-robots-connor-lee-et-al-2024>(40/104 | 132/312) Semantics from Space: Satellite-Guided Thermal Semantic Segmentation Annotation for Aerial Field Robots (Connor Lee et al., 2024)</a></li><li><a href=#41104--133312-champ-controllable-and-consistent-human-image-animation-with-3d-parametric-guidance-shenhao-zhu-et-al-2024>(41/104 | 133/312) Champ: Controllable and Consistent Human Image Animation with 3D Parametric Guidance (Shenhao Zhu et al., 2024)</a></li><li><a href=#42104--134312-multimodal-conditioned-latent-diffusion-models-for-fashion-image-editing-alberto-baldrati-et-al-2024>(42/104 | 134/312) Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing (Alberto Baldrati et al., 2024)</a></li><li><a href=#43104--135312-can-3d-vision-language-models-truly-understand-natural-language-weipeng-deng-et-al-2024>(43/104 | 135/312) Can 3D Vision-Language Models Truly Understand Natural Language? (Weipeng Deng et al., 2024)</a></li><li><a href=#44104--136312-glc-source-free-universal-domain-adaptation-through-global-local-clustering-and-contrastive-affinity-learning-sanqing-qu-et-al-2024>(44/104 | 136/312) GLC++: Source-Free Universal Domain Adaptation through Global-Local Clustering and Contrastive Affinity Learning (Sanqing Qu et al., 2024)</a></li><li><a href=#45104--137312-evaluating-panoramic-3d-estimation-in-indoor-lighting-analysis-zining-cheng-et-al-2024>(45/104 | 137/312) Evaluating Panoramic 3D Estimation in Indoor Lighting Analysis (Zining Cheng et al., 2024)</a></li><li><a href=#46104--138312-zero-shot-multi-object-shape-completion-shun-iwase-et-al-2024>(46/104 | 138/312) Zero-Shot Multi-Object Shape Completion (Shun Iwase et al., 2024)</a></li><li><a href=#47104--139312-science-based-ai-model-certification-for-untrained-operational-environments-with-application-in-traffic-state-estimation-daryl-mupupuni-et-al-2024>(47/104 | 139/312) Science based AI model certification for untrained operational environments with application in traffic state estimation (Daryl Mupupuni et al., 2024)</a></li><li><a href=#48104--140312-dsgg-dense-relation-transformer-for-an-end-to-end-scene-graph-generation-zeeshan-hayder-et-al-2024>(48/104 | 140/312) DSGG: Dense Relation Transformer for an End-to-end Scene Graph Generation (Zeeshan Hayder et al., 2024)</a></li><li><a href=#49104--141312-dino-tracker-taming-dino-for-self-supervised-point-tracking-in-a-single-video-narek-tumanyan-et-al-2024>(49/104 | 141/312) DINO-Tracker: Taming DINO for Self-Supervised Point Tracking in a Single Video (Narek Tumanyan et al., 2024)</a></li><li><a href=#50104--142312-ranking-distillation-for-open-ended-video-question-answering-with-insufficient-labels-tianming-liang-et-al-2024>(50/104 | 142/312) Ranking Distillation for Open-Ended Video Question Answering with Insufficient Labels (Tianming Liang et al., 2024)</a></li><li><a href=#51104--143312-pensieve-retrospect-then-compare-mitigates-visual-hallucination-dingchen-yang-et-al-2024>(51/104 | 143/312) Pensieve: Retrospect-then-Compare Mitigates Visual Hallucination (Dingchen Yang et al., 2024)</a></li><li><a href=#52104--144312-a-bag-of-tricks-for-few-shot-class-incremental-learning-shuvendu-roy-et-al-2024>(52/104 | 144/312) A Bag of Tricks for Few-Shot Class-Incremental Learning (Shuvendu Roy et al., 2024)</a></li><li><a href=#53104--145312-annotation-efficient-polyp-segmentation-via-active-learning-duojun-huang-et-al-2024>(53/104 | 145/312) Annotation-Efficient Polyp Segmentation via Active Learning (Duojun Huang et al., 2024)</a></li><li><a href=#54104--146312-softpatch-unsupervised-anomaly-detection-with-noisy-data-xi-jiang-et-al-2024>(54/104 | 146/312) SoftPatch: Unsupervised Anomaly Detection with Noisy Data (Xi Jiang et al., 2024)</a></li><li><a href=#55104--147312-unleashing-unlabeled-data-a-paradigm-for-cross-view-geo-localization-guopeng-li-et-al-2024>(55/104 | 147/312) Unleashing Unlabeled Data: A Paradigm for Cross-View Geo-Localization (Guopeng Li et al., 2024)</a></li><li><a href=#56104--148312-external-knowledge-enhanced-3d-scene-generation-from-sketch-zijie-wu-et-al-2024>(56/104 | 148/312) External Knowledge Enhanced 3D Scene Generation from Sketch (Zijie Wu et al., 2024)</a></li><li><a href=#57104--149312-auto-train-once-controller-network-guided-automatic-network-pruning-from-scratch-xidong-wu-et-al-2024>(57/104 | 149/312) Auto-Train-Once: Controller Network Guided Automatic Network Pruning from Scratch (Xidong Wu et al., 2024)</a></li><li><a href=#58104--150312-hyperspectral-neural-radiance-fields-gerry-chen-et-al-2024>(58/104 | 150/312) Hyperspectral Neural Radiance Fields (Gerry Chen et al., 2024)</a></li><li><a href=#59104--151312-diffusion-attack-leveraging-stable-diffusion-for-naturalistic-image-attacking-qianyu-guo-et-al-2024>(59/104 | 151/312) Diffusion Attack: Leveraging Stable Diffusion for Naturalistic Image Attacking (Qianyu Guo et al., 2024)</a></li><li><a href=#60104--152312-improving-robustness-to-model-inversion-attacks-via-sparse-coding-architectures-sayanton-v-dibbo-et-al-2024>(60/104 | 152/312) Improving Robustness to Model Inversion Attacks via Sparse Coding Architectures (Sayanton V. Dibbo et al., 2024)</a></li><li><a href=#61104--153312-grm-large-gaussian-reconstruction-model-for-efficient-3d-reconstruction-and-generation-yinghao-xu-et-al-2024>(61/104 | 153/312) GRM: Large Gaussian Reconstruction Model for Efficient 3D Reconstruction and Generation (Yinghao Xu et al., 2024)</a></li><li><a href=#62104--154312-dreamreward-text-to-3d-generation-with-human-preference-junliang-ye-et-al-2024>(62/104 | 154/312) DreamReward: Text-to-3D Generation with Human Preference (Junliang Ye et al., 2024)</a></li><li><a href=#63104--155312-implicit-style-content-separation-using-b-lora-yarden-frenkel-et-al-2024>(63/104 | 155/312) Implicit Style-Content Separation using B-LoRA (Yarden Frenkel et al., 2024)</a></li><li><a href=#64104--156312-designedit-multi-layered-latent-decomposition-and-fusion-for-unified--accurate-image-editing-yueru-jia-et-al-2024>(64/104 | 156/312) DesignEdit: Multi-Layered Latent Decomposition and Fusion for Unified & Accurate Image Editing (Yueru Jia et al., 2024)</a></li><li><a href=#65104--157312-anyv2v-a-plug-and-play-framework-for-any-video-to-video-editing-tasks-max-ku-et-al-2024>(65/104 | 157/312) AnyV2V: A Plug-and-Play Framework For Any Video-to-Video Editing Tasks (Max Ku et al., 2024)</a></li><li><a href=#66104--158312-style-extracting-diffusion-models-for-semi-supervised-histopathology-segmentation-mathias-öttl-et-al-2024>(66/104 | 158/312) Style-Extracting Diffusion Models for Semi-Supervised Histopathology Segmentation (Mathias Öttl et al., 2024)</a></li><li><a href=#67104--159312-enabling-visual-composition-and-animation-in-unsupervised-video-generation-aram-davtyan-et-al-2024>(67/104 | 159/312) Enabling Visual Composition and Animation in Unsupervised Video Generation (Aram Davtyan et al., 2024)</a></li><li><a href=#68104--160312-surroundsdf-implicit-3d-scene-understanding-based-on-signed-distance-field-lizhe-liu-et-al-2024>(68/104 | 160/312) SurroundSDF: Implicit 3D Scene Understanding Based on Signed Distance Field (Lizhe Liu et al., 2024)</a></li><li><a href=#69104--161312-varroa-destructor-detection-on-honey-bees-using-hyperspectral-imagery-zina-sabrina-duma-et-al-2024>(69/104 | 161/312) Varroa destructor detection on honey bees using hyperspectral imagery (Zina-Sabrina Duma et al., 2024)</a></li><li><a href=#70104--162312-zero123-6d-zero-shot-novel-view-synthesis-for-rgb-category-level-6d-pose-estimation-francesco-di-felice-et-al-2024>(70/104 | 162/312) Zero123-6D: Zero-shot Novel View Synthesis for RGB Category-level 6D Pose Estimation (Francesco Di Felice et al., 2024)</a></li><li><a href=#71104--163312-on-the-concept-trustworthiness-in-concept-bottleneck-models-qihan-huang-et-al-2024>(71/104 | 163/312) On the Concept Trustworthiness in Concept Bottleneck Models (Qihan Huang et al., 2024)</a></li><li><a href=#72104--164312-volumetric-environment-representation-for-vision-language-navigation-rui-liu-et-al-2024>(72/104 | 164/312) Volumetric Environment Representation for Vision-Language Navigation (Rui Liu et al., 2024)</a></li><li><a href=#73104--165312-learning-decomposable-and-debiased-representations-via-attribute-centric-information-bottlenecks-jinyung-hong-et-al-2024>(73/104 | 165/312) Learning Decomposable and Debiased Representations via Attribute-Centric Information Bottlenecks (Jinyung Hong et al., 2024)</a></li><li><a href=#74104--166312-vxp-voxel-cross-pixel-large-scale-image-lidar-place-recognition-yun-jin-li-et-al-2024>(74/104 | 166/312) VXP: Voxel-Cross-Pixel Large-scale Image-LiDAR Place Recognition (Yun-Jin Li et al., 2024)</a></li><li><a href=#75104--167312-mulde-multiscale-log-density-estimation-via-denoising-score-matching-for-video-anomaly-detection-jakub-micorek-et-al-2024>(75/104 | 167/312) MULDE: Multiscale Log-Density Estimation via Denoising Score Matching for Video Anomaly Detection (Jakub Micorek et al., 2024)</a></li><li><a href=#76104--168312-keypoint-relative-position-encoding-for-face-recognition-minchul-kim-et-al-2024>(76/104 | 168/312) KeyPoint Relative Position Encoding for Face Recognition (Minchul Kim et al., 2024)</a></li><li><a href=#77104--169312-osmosis-rgbd-diffusion-prior-for-underwater-image-restoration-opher-bar-nathan-et-al-2024>(77/104 | 169/312) Osmosis: RGBD Diffusion Prior for Underwater Image Restoration (Opher Bar Nathan et al., 2024)</a></li><li><a href=#78104--170312-on-the-detection-of-anomalous-or-out-of-distribution-data-in-vision-models-using-statistical-techniques-laura-omahony-et-al-2024>(78/104 | 170/312) On the Detection of Anomalous or Out-Of-Distribution Data in Vision Models Using Statistical Techniques (Laura O&rsquo;Mahony et al., 2024)</a></li><li><a href=#79104--171312-streamingt2v-consistent-dynamic-and-extendable-long-video-generation-from-text-roberto-henschel-et-al-2024>(79/104 | 171/312) StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation from Text (Roberto Henschel et al., 2024)</a></li><li><a href=#80104--172312-lift-a-surprisingly-simple-lightweight-feature-transform-for-dense-vit-descriptors-saksham-suri-et-al-2024>(80/104 | 172/312) LiFT: A Surprisingly Simple Lightweight Feature Transform for Dense ViT Descriptors (Saksham Suri et al., 2024)</a></li><li><a href=#81104--173312-explorative-inbetweening-of-time-and-space-haiwen-feng-et-al-2024>(81/104 | 173/312) Explorative Inbetweening of Time and Space (Haiwen Feng et al., 2024)</a></li><li><a href=#82104--174312-renoise-real-image-inversion-through-iterative-noising-daniel-garibi-et-al-2024>(82/104 | 174/312) ReNoise: Real Image Inversion Through Iterative Noising (Daniel Garibi et al., 2024)</a></li><li><a href=#83104--175312-view-decoupled-transformer-for-person-re-identification-under-aerial-ground-camera-network-quan-zhang-et-al-2024>(83/104 | 175/312) View-decoupled Transformer for Person Re-identification under Aerial-ground Camera Network (Quan Zhang et al., 2024)</a></li><li><a href=#84104--176312-raw-instinct-trust-your-classifiers-and-skip-the-conversion-christos-kantas-et-al-2024>(84/104 | 176/312) Raw Instinct: Trust Your Classifiers and Skip the Conversion (Christos Kantas et al., 2024)</a></li><li><a href=#85104--177312-combinerf-a-combination-of-regularization-techniques-for-few-shot-neural-radiance-field-view-synthesis-matteo-bonotto-et-al-2024>(85/104 | 177/312) CombiNeRF: A Combination of Regularization Techniques for Few-Shot Neural Radiance Field View Synthesis (Matteo Bonotto et al., 2024)</a></li><li><a href=#86104--178312-ldtr-transformer-based-lane-detection-with-anchor-chain-representation-zhongyu-yang-et-al-2024>(86/104 | 178/312) LDTR: Transformer-based Lane Detection with Anchor-chain Representation (Zhongyu Yang et al., 2024)</a></li><li><a href=#87104--179312-towards-efficient-information-fusion-concentric-dual-fusion-attention-based-multiple-instance-learning-for-whole-slide-images-yujian-liu-et-al-2024>(87/104 | 179/312) Towards Efficient Information Fusion: Concentric Dual Fusion Attention Based Multiple Instance Learning for Whole Slide Images (Yujian Liu et al., 2024)</a></li><li><a href=#88104--180312-enhancing-historical-image-retrieval-with-compositional-cues-tingyu-lin-et-al-2024>(88/104 | 180/312) Enhancing Historical Image Retrieval with Compositional Cues (Tingyu Lin et al., 2024)</a></li><li><a href=#89104--181312-stylecinegan-landscape-cinemagraph-generation-using-a-pre-trained-stylegan-jongwoo-choi-et-al-2024>(89/104 | 181/312) StyleCineGAN: Landscape Cinemagraph Generation using a Pre-trained StyleGAN (Jongwoo Choi et al., 2024)</a></li><li><a href=#90104--182312-synermix-synergistic-mixup-solution-for-enhanced-intra-class-cohesion-and-inter-class-separability-in-image-classification-ye-xu-et-al-2024>(90/104 | 182/312) SynerMix: Synergistic Mixup Solution for Enhanced Intra-Class Cohesion and Inter-Class Separability in Image Classification (Ye Xu et al., 2024)</a></li><li><a href=#91104--183312-3d-object-detection-from-point-cloud-via-voting-step-diffusion-haoran-hou-et-al-2024>(91/104 | 183/312) 3D Object Detection from Point Cloud via Voting Step Diffusion (Haoran Hou et al., 2024)</a></li><li><a href=#92104--184312-soft-masked-transformer-for-point-cloud-processing-with-skip-attention-based-upsampling-yong-he-et-al-2024>(92/104 | 184/312) Soft Masked Transformer for Point Cloud Processing with Skip Attention-Based Upsampling (Yong He et al., 2024)</a></li><li><a href=#93104--185312-test-time-similarity-modification-for-person-re-identification-toward-temporal-distribution-shift-kazuki-adachi-et-al-2024>(93/104 | 185/312) Test-time Similarity Modification for Person Re-identification toward Temporal Distribution Shift (Kazuki Adachi et al., 2024)</a></li><li><a href=#94104--186312-unsupervised-intrinsic-image-decomposition-with-lidar-intensity-enhanced-training-shogo-sato-et-al-2024>(94/104 | 186/312) Unsupervised Intrinsic Image Decomposition with LiDAR Intensity Enhanced Training (Shogo Sato et al., 2024)</a></li><li><a href=#95104--187312-mvsplat-efficient-3d-gaussian-splatting-from-sparse-multi-view-images-yuedong-chen-et-al-2024>(95/104 | 187/312) MVSplat: Efficient 3D Gaussian Splatting from Sparse Multi-View Images (Yuedong Chen et al., 2024)</a></li><li><a href=#96104--188312-rodla-benchmarking-the-robustness-of-document-layout-analysis-models-yufan-chen-et-al-2024>(96/104 | 188/312) RoDLA: Benchmarking the Robustness of Document Layout Analysis Models (Yufan Chen et al., 2024)</a></li><li><a href=#97104--189312-leveraging-thermal-modality-to-enhance-reconstruction-in-low-light-conditions-jiacong-xu-et-al-2024>(97/104 | 189/312) Leveraging Thermal Modality to Enhance Reconstruction in Low-Light Conditions (Jiacong Xu et al., 2024)</a></li><li><a href=#98104--190312-isotropic-gaussian-splatting-for-real-time-radiance-field-rendering-yuanhao-gong-et-al-2024>(98/104 | 190/312) Isotropic Gaussian Splatting for Real-Time Radiance Field Rendering (Yuanhao Gong et al., 2024)</a></li><li><a href=#99104--191312-clusteringsdf-self-organized-neural-implicit-surfaces-for-3d-decomposition-tianhao-wu-et-al-2024>(99/104 | 191/312) ClusteringSDF: Self-Organized Neural Implicit Surfaces for 3D Decomposition (Tianhao Wu et al., 2024)</a></li><li><a href=#100104--192312-videoshop-localized-semantic-video-editing-with-noise-extrapolated-diffusion-inversion-xiang-fan-et-al-2024>(100/104 | 192/312) Videoshop: Localized Semantic Video Editing with Noise-Extrapolated Diffusion Inversion (Xiang Fan et al., 2024)</a></li><li><a href=#101104--193312-visibility-aware-keypoint-localization-for-6dof-object-pose-estimation-ruyi-lian-et-al-2024>(101/104 | 193/312) Visibility-Aware Keypoint Localization for 6DoF Object Pose Estimation (Ruyi Lian et al., 2024)</a></li><li><a href=#102104--194312-exploring-3d-human-pose-estimation-and-forecasting-from-the-robots-perspective-the-harper-dataset-andrea-avogaro-et-al-2024>(102/104 | 194/312) Exploring 3D Human Pose Estimation and Forecasting from the Robot&rsquo;s Perspective: The HARPER Dataset (Andrea Avogaro et al., 2024)</a></li><li><a href=#103104--195312-mini-splatting-representing-scenes-with-a-constrained-number-of-gaussians-guangchi-fang-et-al-2024>(103/104 | 195/312) Mini-Splatting: Representing Scenes with a Constrained Number of Gaussians (Guangchi Fang et al., 2024)</a></li><li><a href=#104104--196312-existence-is-chaos-enhancing-3d-human-motion-prediction-with-uncertainty-consideration-zhihao-wang-et-al-2024>(104/104 | 196/312) Existence Is Chaos: Enhancing 3D Human Motion Prediction with Uncertainty Consideration (Zhihao Wang et al., 2024)</a></li></ul></li><li><a href=#quant-ph-4>quant-ph (4)</a><ul><li><a href=#14--197312-learning-with-sasquatch-a-novel-variational-quantum-transformer-architecture-with-kernel-based-self-attention-ethan-n-evans-et-al-2024>(1/4 | 197/312) Learning with SASQuaTCh: a Novel Variational Quantum Transformer Architecture with Kernel-Based Self-Attention (Ethan N. Evans et al., 2024)</a></li><li><a href=#24--198312-polynomial-time-classical-simulation-of-noisy-iqp-circuits-with-constant-depth-joel-rajakumar-et-al-2024>(2/4 | 198/312) Polynomial-Time Classical Simulation of Noisy IQP Circuits with Constant Depth (Joel Rajakumar et al., 2024)</a></li><li><a href=#34--199312-quantum-channel-simulation-under-purified-distance-is-no-more-difficult-than-state-splitting-michael-x-cao-et-al-2024>(3/4 | 199/312) Quantum Channel Simulation under Purified Distance is no more difficult than State Splitting (Michael X. Cao et al., 2024)</a></li><li><a href=#44--200312-optimal-second-order-rates-for-quantum-information-decoupling-yu-chen-shen-et-al-2024>(4/4 | 200/312) Optimal Second-Order Rates for Quantum Information Decoupling (Yu-Chen Shen et al., 2024)</a></li></ul></li><li><a href=#cscy-6>cs.CY (6)</a><ul><li><a href=#16--201312-antisocial-analagous-behavior-alignment-and-human-impact-of-google-ai-systems-evaluating-through-the-lens-of-modified-antisocial-behavior-criteria-by-human-interaction-independent-llm-analysis-and-ai-self-reflection-alan-d-ogilvie-2024>(1/6 | 201/312) Antisocial Analagous Behavior, Alignment and Human Impact of Google AI Systems: Evaluating through the lens of modified Antisocial Behavior Criteria by Human Interaction, Independent LLM Analysis, and AI Self-Reflection (Alan D. Ogilvie, 2024)</a></li><li><a href=#26--202312-the-ethics-of-chatgpt-in-medicine-and-healthcare-a-systematic-review-on-large-language-models-llms-joschka-haltaufderheide-et-al-2024>(2/6 | 202/312) The Ethics of ChatGPT in Medicine and Healthcare: A Systematic Review on Large Language Models (LLMs) (Joschka Haltaufderheide et al., 2024)</a></li><li><a href=#36--203312-on-the-conversational-persuasiveness-of-large-language-models-a-randomized-controlled-trial-francesco-salvi-et-al-2024>(3/6 | 203/312) On the Conversational Persuasiveness of Large Language Models: A Randomized Controlled Trial (Francesco Salvi et al., 2024)</a></li><li><a href=#46--204312-protected-group-bias-and-stereotypes-in-large-language-models-hadas-kotek-et-al-2024>(4/6 | 204/312) Protected group bias and stereotypes in Large Language Models (Hadas Kotek et al., 2024)</a></li><li><a href=#56--205312-navigating-fairness-practitioners-understanding-challenges-and-strategies-in-aiml-development-aastha-pant-et-al-2024>(5/6 | 205/312) Navigating Fairness: Practitioners&rsquo; Understanding, Challenges, and Strategies in AI/ML Development (Aastha Pant et al., 2024)</a></li><li><a href=#66--206312-particip-ai-a-democratic-surveying-framework-for-anticipating-future-ai-use-cases-harms-and-benefits-jimin-mun-et-al-2024>(6/6 | 206/312) Particip-AI: A Democratic Surveying Framework for Anticipating Future AI Use Cases, Harms and Benefits (Jimin Mun et al., 2024)</a></li></ul></li><li><a href=#csir-3>cs.IR (3)</a><ul><li><a href=#13--207312-knowledge-enhanced-recommendation-with-user-centric-subgraph-network-guangyi-liu-et-al-2024>(1/3 | 207/312) Knowledge-Enhanced Recommendation with User-Centric Subgraph Network (Guangyi Liu et al., 2024)</a></li><li><a href=#23--208312-m3-a-multi-task-mixed-objective-learning-framework-for-open-domain-multi-hop-dense-sentence-retrieval-yang-bai-et-al-2024>(2/3 | 208/312) M3: A Multi-Task Mixed-Objective Learning Framework for Open-Domain Multi-Hop Dense Sentence Retrieval (Yang Bai et al., 2024)</a></li><li><a href=#33--209312-understanding-the-ranking-loss-for-recommendation-with-sparse-user-feedback-zhutian-lin-et-al-2024>(3/3 | 209/312) Understanding the Ranking Loss for Recommendation with Sparse User Feedback (Zhutian Lin et al., 2024)</a></li></ul></li><li><a href=#mathna-6>math.NA (6)</a><ul><li><a href=#16--210312-a-lstm-enhanced-surrogate-model-to-simulate-the-dynamics-of-particle-laden-fluid-systems-arash-hajisharifi-et-al-2024>(1/6 | 210/312) A LSTM-enhanced surrogate model to simulate the dynamics of particle-laden fluid systems (Arash Hajisharifi et al., 2024)</a></li><li><a href=#26--211312-time-filtering-methods-for-electrohydrodynamics-models-li-conghui-2024>(2/6 | 211/312) Time filtering methods for electrohydrodynamics models (Li Conghui, 2024)</a></li><li><a href=#36--212312-structure-preserving-weighted-implicit-explicit-schemes-for-multi-phase-incompressible-navier-stokesdarcy-coupled-nonlocal-allen-cahn-model-meng-li-et-al-2024>(3/6 | 212/312) Structure-preserving, weighted implicit-explicit schemes for multi-phase incompressible Navier-Stokes/Darcy coupled nonlocal Allen-Cahn model (Meng Li et al., 2024)</a></li><li><a href=#46--213312-learning-based-multi-continuum-model-for-multiscale-flow-problems-fan-wang-et-al-2024>(4/6 | 213/312) Learning-based Multi-continuum Model for Multiscale Flow Problems (Fan Wang et al., 2024)</a></li><li><a href=#56--214312-low-rank-tensor-product-richardson-iteration-for-radiative-transfer-in-plane-parallel-geometry-markus-bachmayr-et-al-2024>(5/6 | 214/312) Low-rank tensor product Richardson iteration for radiative transfer in plane-parallel geometry (Markus Bachmayr et al., 2024)</a></li><li><a href=#66--215312-fast-and-accurate-log-determinant-approximations-owen-deen-et-al-2024>(6/6 | 215/312) Fast and accurate log-determinant approximations (Owen Deen et al., 2024)</a></li></ul></li><li><a href=#csne-8>cs.NE (8)</a><ul><li><a href=#18--216312-spikegraphormer-a-high-performance-graph-transformer-with-spiking-graph-attention-yundong-sun-et-al-2024>(1/8 | 216/312) SpikeGraphormer: A High-Performance Graph Transformer with Spiking Graph Attention (Yundong Sun et al., 2024)</a></li><li><a href=#28--217312-spikingresformer-bridging-resnet-and-vision-transformer-in-spiking-neural-networks-xinyu-shi-et-al-2024>(2/8 | 217/312) SpikingResformer: Bridging ResNet and Vision Transformer in Spiking Neural Networks (Xinyu Shi et al., 2024)</a></li><li><a href=#38--218312-reactor-optimization-benchmark-by-reinforcement-learning-deborah-schwarcz-et-al-2024>(3/8 | 218/312) Reactor Optimization Benchmark by Reinforcement Learning (Deborah Schwarcz et al., 2024)</a></li><li><a href=#48--219312-model-uncertainty-in-evolutionary-optimization-and-bayesian-optimization-a-comparative-analysis-hao-hao-et-al-2024>(4/8 | 219/312) Model Uncertainty in Evolutionary Optimization and Bayesian Optimization: A Comparative Analysis (Hao Hao et al., 2024)</a></li><li><a href=#58--220312-a-reinforcement-learning-guided-hybrid-evolutionary-algorithm-for-the-latency-location-routing-problem-yuji-zou-et-al-2024>(5/8 | 220/312) A reinforcement learning guided hybrid evolutionary algorithm for the latency location routing problem (Yuji Zou et al., 2024)</a></li><li><a href=#68--221312-stitching-for-neuroevolution-recombining-deep-neural-networks-without-breaking-them-arthur-guijt-et-al-2024>(6/8 | 221/312) Stitching for Neuroevolution: Recombining Deep Neural Networks without Breaking Them (Arthur Guijt et al., 2024)</a></li><li><a href=#78--222312-an-analysis-of-the-preferences-of-distribution-indicators-in-evolutionary-multi-objective-optimization-jesús-guillermo-falcón-cardona-et-al-2024>(7/8 | 222/312) An Analysis of the Preferences of Distribution Indicators in Evolutionary Multi-Objective Optimization (Jesús Guillermo Falcón-Cardona et al., 2024)</a></li><li><a href=#88--223312-evolving-benchmark-functions-to-compare-evolutionary-algorithms-via-genetic-programming-yifan-he-et-al-2024>(8/8 | 223/312) Evolving Benchmark Functions to Compare Evolutionary Algorithms via Genetic Programming (Yifan He et al., 2024)</a></li></ul></li><li><a href=#cshc-4>cs.HC (4)</a><ul><li><a href=#14--224312-empowering-personalized-learning-through-a-conversation-based-tutoring-system-with-student-modeling-minju-park-et-al-2024>(1/4 | 224/312) Empowering Personalized Learning through a Conversation-based Tutoring System with Student Modeling (Minju Park et al., 2024)</a></li><li><a href=#24--225312-peergpt-probing-the-roles-of-llm-based-peer-agents-as-team-moderators-and-participants-in-childrens-collaborative-learning-jiawen-liu-et-al-2024>(2/4 | 225/312) PeerGPT: Probing the Roles of LLM-based Peer Agents as Team Moderators and Participants in Children&rsquo;s Collaborative Learning (Jiawen Liu et al., 2024)</a></li><li><a href=#34--226312-how-human-centered-explainable-ai-interface-are-designed-and-evaluated-a-systematic-survey-thu-nguyen-et-al-2024>(3/4 | 226/312) How Human-Centered Explainable AI Interface Are Designed and Evaluated: A Systematic Survey (Thu Nguyen et al., 2024)</a></li><li><a href=#44--227312-recourse-for-reclamation-chatting-with-generative-language-models-jennifer-chien-et-al-2024>(4/4 | 227/312) Recourse for reclamation: Chatting with generative language models (Jennifer Chien et al., 2024)</a></li></ul></li><li><a href=#cssd-5>cs.SD (5)</a><ul><li><a href=#15--228312-xlavs-r-cross-lingual-audio-visual-speech-representation-learning-for-noise-robust-speech-perception-hyojung-han-et-al-2024>(1/5 | 228/312) XLAVS-R: Cross-Lingual Audio-Visual Speech Representation Learning for Noise-Robust Speech Perception (HyoJung Han et al., 2024)</a></li><li><a href=#25--229312-exploring-green-ai-for-audio-deepfake-detection-subhajit-saha-et-al-2024>(2/5 | 229/312) Exploring Green AI for Audio Deepfake Detection (Subhajit Saha et al., 2024)</a></li><li><a href=#35--230312-emodarts-joint-optimisation-of-cnn--sequential-neural-network-architectures-for-superior-speech-emotion-recognition-thejan-rajapakshe-et-al-2024>(3/5 | 230/312) emoDARTS: Joint Optimisation of CNN & Sequential Neural Network Architectures for Superior Speech Emotion Recognition (Thejan Rajapakshe et al., 2024)</a></li><li><a href=#45--231312-the-neurips-2023-machine-learning-for-audio-workshop-affective-audio-benchmarks-and-novel-data-alice-baird-et-al-2024>(4/5 | 231/312) The NeurIPS 2023 Machine Learning for Audio Workshop: Affective Audio Benchmarks and Novel Data (Alice Baird et al., 2024)</a></li><li><a href=#55--232312-assessing-the-robustness-of-spectral-clustering-for-deep-speaker-diarization-nikhil-raghav-et-al-2024>(5/5 | 232/312) Assessing the Robustness of Spectral Clustering for Deep Speaker Diarization (Nikhil Raghav et al., 2024)</a></li></ul></li><li><a href=#astro-phim-1>astro-ph.IM (1)</a><ul><li><a href=#11--233312-a-classifier-based-approach-to-multi-class-anomaly-detection-for-astronomical-transients-rithwik-gupta-et-al-2024>(1/1 | 233/312) A Classifier-Based Approach to Multi-Class Anomaly Detection for Astronomical Transients (Rithwik Gupta et al., 2024)</a></li></ul></li><li><a href=#statml-6>stat.ML (6)</a><ul><li><a href=#16--234312-curvature-augmented-manifold-embedding-and-learning-yongming-liu-2024>(1/6 | 234/312) Curvature Augmented Manifold Embedding and Learning (Yongming Liu, 2024)</a></li><li><a href=#26--235312-automatic-outlier-rectification-via-optimal-transport-jose-blanchet-et-al-2024>(2/6 | 235/312) Automatic Outlier Rectification via Optimal Transport (Jose Blanchet et al., 2024)</a></li><li><a href=#36--236312-deep-clustering-evaluation-how-to-validate-internal-clustering-validation-measures-zeya-wang-et-al-2024>(3/6 | 236/312) Deep Clustering Evaluation: How to Validate Internal Clustering Validation Measures (Zeya Wang et al., 2024)</a></li><li><a href=#46--237312-estimating-causal-effects-with-double-machine-learning----a-method-evaluation-jonathan-fuhr-et-al-2024>(4/6 | 237/312) Estimating Causal Effects with Double Machine Learning &ndash; A Method Evaluation (Jonathan Fuhr et al., 2024)</a></li><li><a href=#56--238312-learning-causal-graphs-using-variable-grouping-according-to-ancestral-relationship-ming-cai-et-al-2024>(5/6 | 238/312) Learning causal graphs using variable grouping according to ancestral relationship (Ming Cai et al., 2024)</a></li><li><a href=#66--239312-recovering-latent-confounders-from-high-dimensional-proxy-variables-nathan-mankovich-et-al-2024>(6/6 | 239/312) Recovering Latent Confounders from High-dimensional Proxy Variables (Nathan Mankovich et al., 2024)</a></li></ul></li><li><a href=#csro-20>cs.RO (20)</a><ul><li><a href=#120--240312-learning-quadruped-locomotion-using-differentiable-simulation-yunlong-song-et-al-2024>(1/20 | 240/312) Learning Quadruped Locomotion Using Differentiable Simulation (Yunlong Song et al., 2024)</a></li><li><a href=#220--241312-physics-based-causal-reasoning-for-safe--robust-next-best-action-selection-in-robot-manipulation-tasks-ricardo-cannizzaro-et-al-2024>(2/20 | 241/312) Physics-Based Causal Reasoning for Safe & Robust Next-Best Action Selection in Robot Manipulation Tasks (Ricardo Cannizzaro et al., 2024)</a></li><li><a href=#320--242312-distilling-reinforcement-learning-policies-for-interpretable-robot-locomotion-gradient-boosting-machines-and-symbolic-regression-fernando-acero-et-al-2024>(3/20 | 242/312) Distilling Reinforcement Learning Policies for Interpretable Robot Locomotion: Gradient Boosting Machines and Symbolic Regression (Fernando Acero et al., 2024)</a></li><li><a href=#420--243312-click-to-grasp-zero-shot-precise-manipulation-via-visual-diffusion-descriptors-nikolaos-tsagkas-et-al-2024>(4/20 | 243/312) Click to Grasp: Zero-Shot Precise Manipulation via Visual Diffusion Descriptors (Nikolaos Tsagkas et al., 2024)</a></li><li><a href=#520--244312-teevtol-balancing-energy-and-time-efficiency-in-evtol-aircraft-path-planning-across-city-scale-wind-fields-songyang-liu-et-al-2024>(5/20 | 244/312) TEeVTOL: Balancing Energy and Time Efficiency in eVTOL Aircraft Path Planning Across City-Scale Wind Fields (Songyang Liu et al., 2024)</a></li><li><a href=#620--245312-multi-agent-task-driven-exploration-via-intelligent-map-compression-and-sharing-evangelos-psomiadis-et-al-2024>(6/20 | 245/312) Multi-agent Task-Driven Exploration via Intelligent Map Compression and Sharing (Evangelos Psomiadis et al., 2024)</a></li><li><a href=#720--246312-sdp-synthesis-of-maximum-coverage-trees-for-probabilistic-planning-under-control-constraints-naman-aggarwal-et-al-2024>(7/20 | 246/312) SDP Synthesis of Maximum Coverage Trees for Probabilistic Planning under Control Constraints (Naman Aggarwal et al., 2024)</a></li><li><a href=#820--247312-bayesian-optimization-for-sample-efficient-policy-improvement-in-robotic-manipulation-adrian-röfer-et-al-2024>(8/20 | 247/312) Bayesian Optimization for Sample-Efficient Policy Improvement in Robotic Manipulation (Adrian Röfer et al., 2024)</a></li><li><a href=#920--248312-dexdribbler-learning-dexterous-soccer-manipulation-via-dynamic-supervision-yutong-hu-et-al-2024>(9/20 | 248/312) DexDribbler: Learning Dexterous Soccer Manipulation via Dynamic Supervision (Yutong Hu et al., 2024)</a></li><li><a href=#1020--249312-extrinsic-calibration-of-multiple-lidars-for-a-mobile-robot-based-on-floor-plane-and-object-segmentation-shun-niijima-et-al-2024>(10/20 | 249/312) Extrinsic Calibration of Multiple LiDARs for a Mobile Robot based on Floor Plane And Object Segmentation (Shun Niijima et al., 2024)</a></li><li><a href=#1120--250312-a-roadmap-towards-automated-and-regulated-robotic-systems-yihao-liu-et-al-2024>(11/20 | 250/312) A Roadmap Towards Automated and Regulated Robotic Systems (Yihao Liu et al., 2024)</a></li><li><a href=#1220--251312-bringing-robots-home-the-rise-of-ai-robots-in-consumer-electronics-haiwei-dong-et-al-2024>(12/20 | 251/312) Bringing Robots Home: The Rise of AI Robots in Consumer Electronics (Haiwei Dong et al., 2024)</a></li><li><a href=#1320--252312-leveraging-large-language-model-based-room-object-relationships-knowledge-for-enhancing-multimodal-input-object-goal-navigation-leyuan-sun-et-al-2024>(13/20 | 252/312) Leveraging Large Language Model-based Room-Object Relationships Knowledge for Enhancing Multimodal-Input Object Goal Navigation (Leyuan Sun et al., 2024)</a></li><li><a href=#1420--253312-odtformer-efficient-obstacle-detection-and-tracking-with-stereo-cameras-based-on-transformer-tianye-ding-et-al-2024>(14/20 | 253/312) ODTFormer: Efficient Obstacle Detection and Tracking with Stereo Cameras Based on Transformer (Tianye Ding et al., 2024)</a></li><li><a href=#1520--254312-exosense-a-vision-centric-scene-understanding-system-for-safe-exoskeleton-navigation-jianeng-wang-et-al-2024>(15/20 | 254/312) Exosense: A Vision-Centric Scene Understanding System For Safe Exoskeleton Navigation (Jianeng Wang et al., 2024)</a></li><li><a href=#1620--255312-learning-to-change-choreographing-mixed-traffic-through-lateral-control-and-hierarchical-reinforcement-learning-dawei-wang-et-al-2024>(16/20 | 255/312) Learning to Change: Choreographing Mixed Traffic Through Lateral Control and Hierarchical Reinforcement Learning (Dawei Wang et al., 2024)</a></li><li><a href=#1720--256312-extended-reality-for-enhanced-human-robot-collaboration-a-human-in-the-loop-approach-yehor-karpichev-et-al-2024>(17/20 | 256/312) Extended Reality for Enhanced Human-Robot Collaboration: a Human-in-the-Loop Approach (Yehor Karpichev et al., 2024)</a></li><li><a href=#1820--257312-uav-assisted-maritime-search-and-rescue-a-holistic-approach-martin-messmer-et-al-2024>(18/20 | 257/312) UAV-Assisted Maritime Search and Rescue: A Holistic Approach (Martin Messmer et al., 2024)</a></li><li><a href=#1920--258312-referee-radar-based-efficient-global-descriptor-using-a-feature-and-free-space-for-place-recognition-byunghee-choi-et-al-2024>(19/20 | 258/312) ReFeree: Radar-based efficient global descriptor using a Feature and Free space for Place Recognition (Byunghee Choi et al., 2024)</a></li><li><a href=#2020--259312-hcto-optimality-aware-lidar-inertial-odometry-with-hybrid-continuous-time-optimization-for-compact-wearable-mapping-system-jianping-li-et-al-2024>(20/20 | 259/312) HCTO: Optimality-Aware LiDAR Inertial Odometry with Hybrid Continuous Time Optimization for Compact Wearable Mapping System (Jianping Li et al., 2024)</a></li></ul></li><li><a href=#eessiv-7>eess.IV (7)</a><ul><li><a href=#17--260312-cathflow-self-supervised-segmentation-of-catheters-in-interventional-ultrasound-using-optical-flow-and-transformers-alex-ranne-et-al-2024>(1/7 | 260/312) CathFlow: Self-Supervised Segmentation of Catheters in Interventional Ultrasound Using Optical Flow and Transformers (Alex Ranne et al., 2024)</a></li><li><a href=#27--261312-diffusion-models-with-ensembled-structure-based-anomaly-scoring-for-unsupervised-anomaly-detection-finn-behrendt-et-al-2024>(2/7 | 261/312) Diffusion Models with Ensembled Structure-Based Anomaly Scoring for Unsupervised Anomaly Detection (Finn Behrendt et al., 2024)</a></li><li><a href=#37--262312-resnet101-and-dae-for-enhance-quality-and-classification-accuracy-in-skin-cancer-imaging-sibasish-dhibar-2024>(3/7 | 262/312) ResNet101 and DAE for Enhance Quality and Classification Accuracy in Skin Cancer Imaging (Sibasish Dhibar, 2024)</a></li><li><a href=#47--263312-qsmdiff-unsupervised-3d-diffusion-models-for-quantitative-susceptibility-mapping-zhuang-xiong-et-al-2024>(4/7 | 263/312) QSMDiff: Unsupervised 3D Diffusion Models for Quantitative Susceptibility Mapping (Zhuang Xiong et al., 2024)</a></li><li><a href=#57--264312-denoising-diffusion-models-for-3d-healthy-brain-tissue-inpainting-alicia-durrer-et-al-2024>(5/7 | 264/312) Denoising Diffusion Models for 3D Healthy Brain Tissue Inpainting (Alicia Durrer et al., 2024)</a></li><li><a href=#67--265312-analysing-diffusion-segmentation-for-medical-images-mathias-öttl-et-al-2024>(6/7 | 265/312) Analysing Diffusion Segmentation for Medical Images (Mathias Öttl et al., 2024)</a></li><li><a href=#77--266312-lefusion-synthesizing-myocardial-pathology-on-cardiac-mri-via-lesion-focus-diffusion-models-hantao-zhang-et-al-2024>(7/7 | 266/312) LeFusion: Synthesizing Myocardial Pathology on Cardiac MRI via Lesion-Focus Diffusion Models (Hantao Zhang et al., 2024)</a></li></ul></li><li><a href=#cscr-5>cs.CR (5)</a><ul><li><a href=#15--267312-large-language-models-for-blockchain-security-a-systematic-literature-review-zheyuan-he-et-al-2024>(1/5 | 267/312) Large Language Models for Blockchain Security: A Systematic Literature Review (Zheyuan He et al., 2024)</a></li><li><a href=#25--268312-adversary-augmented-simulation-to-evaluate-client-fairness-on-hyperledger-fabric-erwan-mahe-et-al-2024>(2/5 | 268/312) Adversary-Augmented Simulation to evaluate client-fairness on HyperLedger Fabric (Erwan Mahe et al., 2024)</a></li><li><a href=#35--269312-hetal-efficient-privacy-preserving-transfer-learning-with-homomorphic-encryption-seewoo-lee-et-al-2024>(3/5 | 269/312) HETAL: Efficient Privacy-preserving Transfer Learning with Homomorphic Encryption (Seewoo Lee et al., 2024)</a></li><li><a href=#45--270312-fhauc-privacy-preserving-auc-calculation-for-federated-learning-using-fully-homomorphic-encryption-cem-ata-baykara-et-al-2024>(4/5 | 270/312) FHAUC: Privacy Preserving AUC Calculation for Federated Learning using Fully Homomorphic Encryption (Cem Ata Baykara et al., 2024)</a></li><li><a href=#55--271312-improving-galileo-osnma-time-to-first-authenticated-fix-aleix-galan-et-al-2024>(5/5 | 271/312) Improving Galileo OSNMA Time To First Authenticated Fix (Aleix Galan et al., 2024)</a></li></ul></li><li><a href=#csdc-1>cs.DC (1)</a><ul><li><a href=#11--272312-accelerating-vit-inference-on-fpga-through-static-and-dynamic-pruning-dhruv-parikh-et-al-2024>(1/1 | 272/312) Accelerating ViT Inference on FPGA through Static and Dynamic Pruning (Dhruv Parikh et al., 2024)</a></li></ul></li><li><a href=#physicsmed-ph-1>physics.med-ph (1)</a><ul><li><a href=#11--273312-distribution-informed-and-wavelength-flexible-data-driven-photoacoustic-oximetry-janek-gröhl-et-al-2024>(1/1 | 273/312) Distribution-informed and wavelength-flexible data-driven photoacoustic oximetry (Janek Gröhl et al., 2024)</a></li></ul></li><li><a href=#csse-3>cs.SE (3)</a><ul><li><a href=#13--274312-towards-single-system-illusion-in-software-defined-vehicles----automated-ai-powered-workflow-krzysztof-lebioda-et-al-2024>(1/3 | 274/312) Towards Single-System Illusion in Software-Defined Vehicles &ndash; Automated, AI-Powered Workflow (Krzysztof Lebioda et al., 2024)</a></li><li><a href=#23--275312-multi-role-consensus-through-llms-discussions-for-vulnerability-detection-zhenyu-mao-et-al-2024>(2/3 | 275/312) Multi-role Consensus through LLMs Discussions for Vulnerability Detection (Zhenyu Mao et al., 2024)</a></li><li><a href=#33--276312-a-survey-of-neural-code-intelligence-paradigms-advances-and-beyond-qiushi-sun-et-al-2024>(3/3 | 276/312) A Survey of Neural Code Intelligence: Paradigms, Advances and Beyond (Qiushi Sun et al., 2024)</a></li></ul></li><li><a href=#eesssy-14>eess.SY (14)</a><ul><li><a href=#114--277312-event-triggered-boundary-control-of-mixed-autonomy-traffic-yihuai-zhang-et-al-2024>(1/14 | 277/312) Event-triggered Boundary Control of Mixed-autonomy Traffic (Yihuai Zhang et al., 2024)</a></li><li><a href=#214--278312-pe-gpt-a-physics-informed-interactive-large-language-model-for-power-converter-modulation-design-fanfan-lin-et-al-2024>(2/14 | 278/312) PE-GPT: A Physics-Informed Interactive Large Language Model for Power Converter Modulation Design (Fanfan Lin et al., 2024)</a></li><li><a href=#314--279312-a-benchmark-for-the-application-of-distributed-control-techniques-to-the-electricity-network-of-the-european-economic-area-a-riccardi-et-al-2024>(3/14 | 279/312) A Benchmark for the Application of Distributed Control Techniques to the Electricity Network of the European Economic Area (A. Riccardi et al., 2024)</a></li><li><a href=#414--280312-learning-hierarchical-control-systems-for-autonomous-systems-with-energy-constraints-charlott-vallon-et-al-2024>(4/14 | 280/312) Learning Hierarchical Control Systems for Autonomous Systems with Energy Constraints (Charlott Vallon et al., 2024)</a></li><li><a href=#514--281312-designing-robust-linear-output-feedback-controller-based-on-clf-cbf-framework-via-linearprogramminglp-clf-cbf-mahroo-bahreinian-et-al-2024>(5/14 | 281/312) Designing Robust Linear Output Feedback Controller based on CLF-CBF framework via Linear~Programming(LP-CLF-CBF) (Mahroo Bahreinian et al., 2024)</a></li><li><a href=#614--282312-synthesizing-controller-for-safe-navigation-using-control-density-function-joseph-moyalan-et-al-2024>(6/14 | 282/312) Synthesizing Controller for Safe Navigation using Control Density Function (Joseph Moyalan et al., 2024)</a></li><li><a href=#714--283312-lane-level-joint-control-of-off-ramp-and-main-line-speed-guidance-on-expressway-in-rainy-weather-boyao-peng-et-al-2024>(7/14 | 283/312) Lane level joint control of off-ramp and main line speed guidance on expressway in rainy weather (Boyao Peng et al., 2024)</a></li><li><a href=#814--284312-robust-model-based-reinforcement-learning-using-mathcall_1-adaptive-control-minjun-sung-et-al-2024>(8/14 | 284/312) Robust Model Based Reinforcement Learning Using $\mathcal{L}_1$ Adaptive Control (Minjun Sung et al., 2024)</a></li><li><a href=#914--285312-transmission-benefits-and-cost-allocation-under-ambiguity-han-shu-et-al-2024>(9/14 | 285/312) Transmission Benefits and Cost Allocation under Ambiguity (Han Shu et al., 2024)</a></li><li><a href=#1014--286312-optimizing-queues-with-deadlines-under-infrequent-monitoring-faraz-farahvash-et-al-2024>(10/14 | 286/312) Optimizing queues with deadlines under infrequent monitoring (Faraz Farahvash et al., 2024)</a></li><li><a href=#1114--287312-meta-learning-of-data-driven-controllers-with-automatic-model-reference-tuning-theory-and-experimental-case-study-riccardo-busetto-et-al-2024>(11/14 | 287/312) Meta-learning of data-driven controllers with automatic model reference tuning: theory and experimental case study (Riccardo Busetto et al., 2024)</a></li><li><a href=#1214--288312-on-the-continuity-and-smoothness-of-the-value-function-in-reinforcement-learning-and-optimal-control-hans-harder-et-al-2024>(12/14 | 288/312) On the continuity and smoothness of the value function in reinforcement learning and optimal control (Hans Harder et al., 2024)</a></li><li><a href=#1314--289312-exploiting-over-the-air-consensus-for-collision-avoidance-and-formation-control-in-multi-agent-systems-michael-epp-et-al-2024>(13/14 | 289/312) Exploiting Over-The-Air Consensus for Collision Avoidance and Formation Control in Multi-Agent Systems (Michael Epp et al., 2024)</a></li><li><a href=#1414--290312-transformation-free-fixed-structure-model-reduction-for-lpv-systems-lennart-heeren-et-al-2024>(14/14 | 290/312) Transformation-Free Fixed-Structure Model Reduction for LPV Systems (Lennart Heeren et al., 2024)</a></li></ul></li><li><a href=#q-biobm-1>q-bio.BM (1)</a><ul><li><a href=#11--291312-protein-conformation-generation-via-force-guided-se3-diffusion-models-yan-wang-et-al-2024>(1/1 | 291/312) Protein Conformation Generation via Force-Guided SE(3) Diffusion Models (Yan Wang et al., 2024)</a></li></ul></li><li><a href=#hep-ex-1>hep-ex (1)</a><ul><li><a href=#11--292312-improving-λ-signal-extraction-with-domain-adaptation-via-normalizing-flows-rowan-kelleher-et-al-2024>(1/1 | 292/312) Improving $Λ$ Signal Extraction with Domain Adaptation via Normalizing Flows (Rowan Kelleher et al., 2024)</a></li></ul></li><li><a href=#csce-2>cs.CE (2)</a><ul><li><a href=#12--293312-geom-deeponet-a-point-cloud-based-deep-operator-network-for-field-predictions-on-3d-parameterized-geometries-junyan-he-et-al-2024>(1/2 | 293/312) Geom-DeepONet: A Point-cloud-based Deep Operator Network for Field Predictions on 3D Parameterized Geometries (Junyan He et al., 2024)</a></li><li><a href=#22--294312-advanced-deep-operator-networks-to-predict-multiphysics-solution-fields-in-materials-processing-and-additive-manufacturing-shashank-kushwaha-et-al-2024>(2/2 | 294/312) Advanced Deep Operator Networks to Predict Multiphysics Solution Fields in Materials Processing and Additive Manufacturing (Shashank Kushwaha et al., 2024)</a></li></ul></li><li><a href=#q-bioqm-1>q-bio.QM (1)</a><ul><li><a href=#11--295312-nana-and-migu-semantic-data-augmentation-techniques-to-enhance-protein-classification-in-graph-neural-networks-yi-shan-lan-et-al-2024>(1/1 | 295/312) NaNa and MiGu: Semantic Data Augmentation Techniques to Enhance Protein Classification in Graph Neural Networks (Yi-Shan Lan et al., 2024)</a></li></ul></li><li><a href=#eesssp-1>eess.SP (1)</a><ul><li><a href=#11--296312-rolling-bearing-fault-diagnosis-method-based-on-generative-adversarial-enhanced-multi-scale-convolutional-neural-network-model-maoxuan-zhou-et-al-2024>(1/1 | 296/312) Rolling bearing fault diagnosis method based on generative adversarial enhanced multi-scale convolutional neural network model (Maoxuan Zhou et al., 2024)</a></li></ul></li><li><a href=#csit-1>cs.IT (1)</a><ul><li><a href=#11--297312-ris-aided-cooperative-mobile-edge-computing-computation-efficiency-maximization-via-joint-uplink-and-downlink-resource-allocation-zhenrong-liu-et-al-2024>(1/1 | 297/312) RIS-Aided Cooperative Mobile Edge Computing: Computation Efficiency Maximization via Joint Uplink and Downlink Resource Allocation (Zhenrong Liu et al., 2024)</a></li></ul></li><li><a href=#eessas-3>eess.AS (3)</a><ul><li><a href=#13--298312-speech-aware-neural-diarization-with-encoder-decoder-attractor-guided-by-attention-constraints-peiying-lee-et-al-2024>(1/3 | 298/312) Speech-Aware Neural Diarization with Encoder-Decoder Attractor Guided by Attention Constraints (PeiYing Lee et al., 2024)</a></li><li><a href=#23--299312-adaproj-adaptively-scaled-angular-margin-subspace-projections-for-anomalous-sound-detection-with-auxiliary-classification-tasks-kevin-wilkinghoff-2024>(2/3 | 299/312) AdaProj: Adaptively Scaled Angular Margin Subspace Projections for Anomalous Sound Detection with Auxiliary Classification Tasks (Kevin Wilkinghoff, 2024)</a></li><li><a href=#33--300312-crowdsourced-multilingual-speech-intelligibility-testing-laura-lechler-et-al-2024>(3/3 | 300/312) Crowdsourced Multilingual Speech Intelligibility Testing (Laura Lechler et al., 2024)</a></li></ul></li><li><a href=#mathoc-1>math.OC (1)</a><ul><li><a href=#11--301312-reinforcement-learning-design-for-quickest-change-detection-austin-cooper-et-al-2024>(1/1 | 301/312) Reinforcement Learning Design for Quickest Change Detection (Austin Cooper et al., 2024)</a></li></ul></li><li><a href=#cssi-4>cs.SI (4)</a><ul><li><a href=#14--302312-random-graph-modeling-a-survey-of-the-concepts-mikhail-drobyshevskiy-et-al-2024>(1/4 | 302/312) Random Graph Modeling: A survey of the concepts (Mikhail Drobyshevskiy et al., 2024)</a></li><li><a href=#24--303312-from-perils-to-possibilities-understanding-how-human-and-ai-biases-affect-online-fora-virginia-morini-et-al-2024>(2/4 | 303/312) From Perils to Possibilities: Understanding how Human (and AI) Biases affect Online Fora (Virginia Morini et al., 2024)</a></li><li><a href=#34--304312-dynamical-importance-and-network-perturbations-ethan-young-et-al-2024>(3/4 | 304/312) Dynamical importance and network perturbations (Ethan Young et al., 2024)</a></li><li><a href=#44--305312-collecting-influencers-a-comparative-study-of-online-network-crawlers-mikhail-drobyshevskiy-et-al-2024>(4/4 | 305/312) Collecting Influencers: A Comparative Study of Online Network Crawlers (Mikhail Drobyshevskiy et al., 2024)</a></li></ul></li><li><a href=#csni-1>cs.NI (1)</a><ul><li><a href=#11--306312-a-mathematical-introduction-to-deep-reinforcement-learning-for-5g6g-applications-farhad-rezazadeh-2024>(1/1 | 306/312) A Mathematical Introduction to Deep Reinforcement Learning for 5G/6G Applications (Farhad Rezazadeh, 2024)</a></li></ul></li><li><a href=#cond-matdis-nn-1>cond-mat.dis-nn (1)</a><ul><li><a href=#11--307312-quantum-activated-neural-reservoirs-on-chip-open-up-large-hardware-security-models-for-resilient-authentication-zhao-he-et-al-2024>(1/1 | 307/312) Quantum-activated neural reservoirs on-chip open up large hardware security models for resilient authentication (Zhao He et al., 2024)</a></li></ul></li><li><a href=#csds-2>cs.DS (2)</a><ul><li><a href=#12--308312-a-differentially-private-clustering-algorithm-for-well-clustered-graphs-weiqiang-he-et-al-2024>(1/2 | 308/312) A Differentially Private Clustering Algorithm for Well-Clustered Graphs (Weiqiang He et al., 2024)</a></li><li><a href=#22--309312-induced-subforests-and-superforests-dieter-rautenbach-et-al-2024>(2/2 | 309/312) Induced Subforests and Superforests (Dieter Rautenbach et al., 2024)</a></li></ul></li><li><a href=#mathco-1>math.CO (1)</a><ul><li><a href=#11--310312-recognizing-relating-edges-in-graphs-without-cycles-of-length-6-vadim-e-levit-et-al-2024>(1/1 | 310/312) Recognizing Relating Edges in Graphs without Cycles of Length 6 (Vadim E. Levit et al., 2024)</a></li></ul></li><li><a href=#csdb-1>cs.DB (1)</a><ul><li><a href=#11--311312-quantifying-semantic-query-similarity-for-automated-linear-sql-grading-a-graph-based-approach-leo-köberlein-et-al-2024>(1/1 | 311/312) Quantifying Semantic Query Similarity for Automated Linear SQL Grading: A Graph-based Approach (Leo Köberlein et al., 2024)</a></li></ul></li><li><a href=#csar-1>cs.AR (1)</a><ul><li><a href=#11--312312-e-syn-e-graph-rewriting-with-technology-aware-cost-functions-for-logic-synthesis-chen-chen-et-al-2024>(1/1 | 312/312) E-Syn: E-Graph Rewriting with Technology-Aware Cost Functions for Logic Synthesis (Chen Chen et al., 2024)</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:contact.to.akitenkrad@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>contact.to.akitenkrad@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/akitenkrad-blog/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2020 Akitenkrad.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/akitenkrad-blog/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/jquery-3.4.1.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/popper.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/bootstrap.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/navbar.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/plyr.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=https://akitenkrad.github.io/akitenkrad-blog/js/single.js></script><script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.css><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.js></script><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!0})})</script></body></html>