<!doctype html><html><head><title>arXiv @ 2024.03.13</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/bootstrap.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/main.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/navbar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/plyr.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/flag-icon.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/custom/style.css><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM crossorigin=anonymous><script async src=https://cdn.jsdelivr.net/npm/es-module-shims@1/dist/es-module-shims.min.js crossorigin=anonymous></script><script type=importmap>
{
  "imports": {
    "@popperjs/core": "https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.8/dist/esm/popper.min.js",
    "bootstrap": "https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.esm.min.js"
  }
}
</script><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js integrity=sha384-geWF76RCwLtnZ8qwWowPQNguL3RmwHVBC9FhGdlKrxdiJJigb/j/68SIy3Te4Bkz crossorigin=anonymous></script><script type=module src=https://akitenkrad.github.io/akitenkrad-blog/js/custom/custom.js></script><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/fontawesome/css/all.min.css><link rel=icon type=image/png href=https://akitenkrad.github.io/akitenkrad-blog/images/favicons/favicon-96x96_huf1ee13f0caf27d1547f91fb46207d708_13005_42x0_resize_box_3.png><meta property="og:title" content="arXiv @ 2024.03.13"><meta property="og:description" content="Primary Categories cond-mat.mtrl-sci (1) cs.AI (4) cs.AR (3) cs.CE (2) cs.CG (1) cs.CL (39) cs.CR (7) cs.CV (97) cs.CY (3) cs.DB (3) cs.DC (6) cs.DM (1) cs.DS (2) cs.ET (2) cs.GR (2) cs.GT (1) cs.HC (5) cs.IR (8) cs.IT (2) cs.LG (63) cs.LO (1) cs.MA (2) cs.MM (1) cs.NE (1) cs.PL (1) cs.RO (12) cs.SD (1) cs.SE (4) cs.SI (1) eess.AS (2) eess.IV (11) eess.SP (3) eess.SY (5) hep-ph (1) math."><meta property="og:type" content="article"><meta property="og:url" content="https://akitenkrad.github.io/akitenkrad-blog/posts/arxiv/202403/20240313000000/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-03-13T00:00:00+00:00"><meta property="article:modified_time" content="2024-03-13T00:00:00+00:00"><meta name=description content="arXiv @ 2024.03.13"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/single.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/sidebar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/style.css><script type=text/javascript src=//ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js></script><script language=JavaScript>$(document).ready(function(){$("a[href^='http']:not([href*='"+location.hostname+"'])").attr("target","_blank")})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-1MYYZQG0WE"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-1MYYZQG0WE",{anonymize_ip:!1})}</script></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow" style=position:fixed><div class=container><button class="navbar-toggler navbar-light navbar-for-sidebar" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span>
</button>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav mr-auto"><li class=nav-item><a class=navbar-brand href=/akitenkrad-blog><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png alt=Logo>
Akitenkrad's Blog</a></li></ul><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=/akitenkrad-blog#home>Home</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#about>About</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#recent-posts>Recent Posts</a></li><li class=nav-item><a class=nav-link id=blog-link href=https://akitenkrad.github.io/akitenkrad-blog/posts>Posts</a></li><li class=nav-item><a class=nav-link id=tag-link href=https://akitenkrad.github.io/akitenkrad-blog/tags>Tags</a></li></ul></div></div><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/akitenkrad-blog/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/>Papers</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202205/>2022.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202205/20220518224923/ title="A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks">A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220508162318/ title="A Deep Cascade Model for Multi-Document Reading Comprehension">A Deep Cascade Model for Multi-Document Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220506021208/ title="A Primer in BERTology: What We Know About How BERT Works">A Primer in BERTology: What We Know About How BERT Works</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220529131339/ title="Attention Is All You Need">Attention Is All You Need</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220503010000/ title="DeBERTa: Decoding-Enhanced BERT with Disentangled Attention">DeBERTa: Decoding-Enhanced BERT with Disentangled Attention</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220505222900/ title="Dense Passage Retrieval for Open-Domain Question Answering">Dense Passage Retrieval for Open-Domain Question Answering</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220511010217/ title="Multi-Style Generative Reading Comprehension">Multi-Style Generative Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220530102936/ title="Neural Machine Translation of Rare Words with Subword Units">Neural Machine Translation of Rare Words with Subword Units</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220523223206/ title="RoBERTa: A Robustly Optimized BERT Pretraining Approach">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220520124748/ title="Semi-Supervised Classification with Graph Convolutional Networks">Semi-Supervised Classification with Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220509110738/ title="Survey on graph embeddings and their applications to machine learning problems on graphs">Survey on graph embeddings and their applications to machine learning problems on graphs</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220514151839/ title="UnitedQA: A Hybrid Approach for Open Domain Question Answering">UnitedQA: A Hybrid Approach for Open Domain Question Answering</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202206/>2022.06</a><ul><li><a href=/akitenkrad-blog/posts/papers/202206/20220612105422/ title="Attributed Network Embedding for Learning in a Dynamic Environment">Attributed Network Embedding for Learning in a Dynamic Environment</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220608085622/ title="CodeBERT: A Pre-Trained Model for Programming and Natural Languages">CodeBERT: A Pre-Trained Model for Programming and Natural Languages</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220618223844/ title="High-order Proximity Preserved Embedding for Dynamic Networks">High-order Proximity Preserved Embedding for Dynamic Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220602171700/ title="S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension">S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202207/>2022.07</a><ul><li><a href=/akitenkrad-blog/posts/papers/202207/20220727145036/ title="Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions">Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions</a></li><li><a href=/akitenkrad-blog/posts/papers/202207/20220726163444/ title="Dynamic Network Embedding Survey">Dynamic Network Embedding Survey</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/papers/202208/20220802103319/ title="Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN">Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/papers/202209/20220909180827/ title="Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting">Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202305/>2023.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202305/20230513094804/ title="Improving Language Understanding by Generative Pre-Training">Improving Language Understanding by Generative Pre-Training</a></li><li><a href=/akitenkrad-blog/posts/papers/202305/20230514200241/ title="Language Models are Unsupervised Multitask Learners">Language Models are Unsupervised Multitask Learners</a></li></ul></li><li><a href=/akitenkrad-blog/posts/papers/202306/ title=2023.06>2023.06</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/>arXiv</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202307/>2023.07</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230701000000/ title="arXiv @ 2023.07.01">arXiv @ 2023.07.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230702000000/ title="arXiv @ 2023.07.02">arXiv @ 2023.07.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230703000000/ title="arXiv @ 2023.07.03">arXiv @ 2023.07.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230704000000/ title="arXiv @ 2023.07.04">arXiv @ 2023.07.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230705000000/ title="arXiv @ 2023.07.05">arXiv @ 2023.07.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230706000000/ title="arXiv @ 2023.07.06">arXiv @ 2023.07.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230707000000/ title="arXiv @ 2023.07.07">arXiv @ 2023.07.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230708000000/ title="arXiv @ 2023.07.08">arXiv @ 2023.07.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230709000000/ title="arXiv @ 2023.07.09">arXiv @ 2023.07.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230710000000/ title="arXiv @ 2023.07.10">arXiv @ 2023.07.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230711000000/ title="arXiv @ 2023.07.11">arXiv @ 2023.07.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230712000000/ title="arXiv @ 2023.07.12">arXiv @ 2023.07.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230713000000/ title="arXiv @ 2023.07.13">arXiv @ 2023.07.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230714000000/ title="arXiv @ 2023.07.14">arXiv @ 2023.07.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230715000000/ title="arXiv @ 2023.07.15">arXiv @ 2023.07.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230716000000/ title="arXiv @ 2023.07.16">arXiv @ 2023.07.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230717000000/ title="arXiv @ 2023.07.17">arXiv @ 2023.07.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230718000000/ title="arXiv @ 2023.07.18">arXiv @ 2023.07.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230719000000/ title="arXiv @ 2023.07.19">arXiv @ 2023.07.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230720000000/ title="arXiv @ 2023.07.20">arXiv @ 2023.07.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230721000000/ title="arXiv @ 2023.07.21">arXiv @ 2023.07.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230722000000/ title="arXiv @ 2023.07.22">arXiv @ 2023.07.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230723000000/ title="arXiv @ 2023.07.23">arXiv @ 2023.07.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230724000000/ title="arXiv @ 2023.07.24">arXiv @ 2023.07.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230725000000/ title="arXiv @ 2023.07.25">arXiv @ 2023.07.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230726000000/ title="arXiv @ 2023.07.26">arXiv @ 2023.07.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230727000000/ title="arXiv @ 2023.07.27">arXiv @ 2023.07.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230728000000/ title="arXiv @ 2023.07.28">arXiv @ 2023.07.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230729000000/ title="arXiv @ 2023.07.29">arXiv @ 2023.07.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230730000000/ title="arXiv @ 2023.07.30">arXiv @ 2023.07.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230731000000/ title="arXiv @ 2023.07.31">arXiv @ 2023.07.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202308/>2023.08</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230801000000/ title="arXiv @ 2023.08.01">arXiv @ 2023.08.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230802000000/ title="arXiv @ 2023.08.02">arXiv @ 2023.08.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230803000000/ title="arXiv @ 2023.08.03">arXiv @ 2023.08.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230804000000/ title="arXiv @ 2023.08.04">arXiv @ 2023.08.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230805000000/ title="arXiv @ 2023.08.05">arXiv @ 2023.08.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230806000000/ title="arXiv @ 2023.08.06">arXiv @ 2023.08.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230807000000/ title="arXiv @ 2023.08.07">arXiv @ 2023.08.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230808000000/ title="arXiv @ 2023.08.08">arXiv @ 2023.08.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230809000000/ title="arXiv @ 2023.08.09">arXiv @ 2023.08.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230810000000/ title="arXiv @ 2023.08.10">arXiv @ 2023.08.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230811000000/ title="arXiv @ 2023.08.11">arXiv @ 2023.08.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230812000000/ title="arXiv @ 2023.08.12">arXiv @ 2023.08.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230813000000/ title="arXiv @ 2023.08.13">arXiv @ 2023.08.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230814000000/ title="arXiv @ 2023.08.14">arXiv @ 2023.08.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230815000000/ title="arXiv @ 2023.08.15">arXiv @ 2023.08.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230816000000/ title="arXiv @ 2023.08.16">arXiv @ 2023.08.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230817000000/ title="arXiv @ 2023.08.17">arXiv @ 2023.08.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230818000000/ title="arXiv @ 2023.08.18">arXiv @ 2023.08.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230819000000/ title="arXiv @ 2023.08.19">arXiv @ 2023.08.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230820000000/ title="arXiv @ 2023.08.20">arXiv @ 2023.08.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230821000000/ title="arXiv @ 2023.08.21">arXiv @ 2023.08.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230822000000/ title="arXiv @ 2023.08.22">arXiv @ 2023.08.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230823000000/ title="arXiv @ 2023.08.23">arXiv @ 2023.08.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230824000000/ title="arXiv @ 2023.08.24">arXiv @ 2023.08.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230825000000/ title="arXiv @ 2023.08.25">arXiv @ 2023.08.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230826000000/ title="arXiv @ 2023.08.26">arXiv @ 2023.08.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230827000000/ title="arXiv @ 2023.08.27">arXiv @ 2023.08.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230828000000/ title="arXiv @ 2023.08.28">arXiv @ 2023.08.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230829000000/ title="arXiv @ 2023.08.29">arXiv @ 2023.08.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230830000000/ title="arXiv @ 2023.08.30">arXiv @ 2023.08.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230831000000/ title="arXiv @ 2023.08.31">arXiv @ 2023.08.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202309/>2023.09</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230901000000/ title="arXiv @ 2023.09.01">arXiv @ 2023.09.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230902000000/ title="arXiv @ 2023.09.02">arXiv @ 2023.09.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230903000000/ title="arXiv @ 2023.09.03">arXiv @ 2023.09.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230904000000/ title="arXiv @ 2023.09.04">arXiv @ 2023.09.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230905000000/ title="arXiv @ 2023.09.05">arXiv @ 2023.09.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230906000000/ title="arXiv @ 2023.09.06">arXiv @ 2023.09.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230907000000/ title="arXiv @ 2023.09.07">arXiv @ 2023.09.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230908000000/ title="arXiv @ 2023.09.08">arXiv @ 2023.09.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230909000000/ title="arXiv @ 2023.09.09">arXiv @ 2023.09.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230910000000/ title="arXiv @ 2023.09.10">arXiv @ 2023.09.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230911000000/ title="arXiv @ 2023.09.11">arXiv @ 2023.09.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230912000000/ title="arXiv @ 2023.09.12">arXiv @ 2023.09.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230913000000/ title="arXiv @ 2023.09.13">arXiv @ 2023.09.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230914000000/ title="arXiv @ 2023.09.14">arXiv @ 2023.09.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230915000000/ title="arXiv @ 2023.09.15">arXiv @ 2023.09.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230916000000/ title="arXiv @ 2023.09.16">arXiv @ 2023.09.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230917000000/ title="arXiv @ 2023.09.17">arXiv @ 2023.09.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230918000000/ title="arXiv @ 2023.09.18">arXiv @ 2023.09.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230919000000/ title="arXiv @ 2023.09.19">arXiv @ 2023.09.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230920000000/ title="arXiv @ 2023.09.20">arXiv @ 2023.09.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230921000000/ title="arXiv @ 2023.09.21">arXiv @ 2023.09.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230922000000/ title="arXiv @ 2023.09.22">arXiv @ 2023.09.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230923000000/ title="arXiv @ 2023.09.23">arXiv @ 2023.09.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230924000000/ title="arXiv @ 2023.09.24">arXiv @ 2023.09.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230925000000/ title="arXiv @ 2023.09.25">arXiv @ 2023.09.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230926000000/ title="arXiv @ 2023.09.26">arXiv @ 2023.09.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230927000000/ title="arXiv @ 2023.09.27">arXiv @ 2023.09.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230928000000/ title="arXiv @ 2023.09.28">arXiv @ 2023.09.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230929000000/ title="arXiv @ 2023.09.29">arXiv @ 2023.09.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230930000000/ title="arXiv @ 2023.09.30">arXiv @ 2023.09.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202310/>2023.1</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231001000000/ title="arXiv @ 2023.10.01">arXiv @ 2023.10.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231002000000/ title="arXiv @ 2023.10.02">arXiv @ 2023.10.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231003000000/ title="arXiv @ 2023.10.03">arXiv @ 2023.10.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231004000000/ title="arXiv @ 2023.10.04">arXiv @ 2023.10.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231005000000/ title="arXiv @ 2023.10.05">arXiv @ 2023.10.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231006000000/ title="arXiv @ 2023.10.06">arXiv @ 2023.10.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231007000000/ title="arXiv @ 2023.10.07">arXiv @ 2023.10.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231008000000/ title="arXiv @ 2023.10.08">arXiv @ 2023.10.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231009000000/ title="arXiv @ 2023.10.09">arXiv @ 2023.10.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231010000000/ title="arXiv @ 2023.10.10">arXiv @ 2023.10.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231011000000/ title="arXiv @ 2023.10.11">arXiv @ 2023.10.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231012000000/ title="arXiv @ 2023.10.12">arXiv @ 2023.10.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231013000000/ title="arXiv @ 2023.10.13">arXiv @ 2023.10.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231014000000/ title="arXiv @ 2023.10.14">arXiv @ 2023.10.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231015000000/ title="arXiv @ 2023.10.15">arXiv @ 2023.10.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231016000000/ title="arXiv @ 2023.10.16">arXiv @ 2023.10.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231017000000/ title="arXiv @ 2023.10.17">arXiv @ 2023.10.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231018000000/ title="arXiv @ 2023.10.18">arXiv @ 2023.10.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231019000000/ title="arXiv @ 2023.10.19">arXiv @ 2023.10.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231020000000/ title="arXiv @ 2023.10.20">arXiv @ 2023.10.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231021000000/ title="arXiv @ 2023.10.21">arXiv @ 2023.10.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231022000000/ title="arXiv @ 2023.10.22">arXiv @ 2023.10.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231023000000/ title="arXiv @ 2023.10.23">arXiv @ 2023.10.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231024000000/ title="arXiv @ 2023.10.24">arXiv @ 2023.10.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231025000000/ title="arXiv @ 2023.10.25">arXiv @ 2023.10.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231026000000/ title="arXiv @ 2023.10.26">arXiv @ 2023.10.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231027000000/ title="arXiv @ 2023.10.27">arXiv @ 2023.10.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231028000000/ title="arXiv @ 2023.10.28">arXiv @ 2023.10.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231029000000/ title="arXiv @ 2023.10.29">arXiv @ 2023.10.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231030000000/ title="arXiv @ 2023.10.30">arXiv @ 2023.10.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231031000000/ title="arXiv @ 2023.10.31">arXiv @ 2023.10.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202311/>2023.11</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231101000000/ title="arXiv @ 2023.11.01">arXiv @ 2023.11.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231102000000/ title="arXiv @ 2023.11.02">arXiv @ 2023.11.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231103000000/ title="arXiv @ 2023.11.03">arXiv @ 2023.11.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231104000000/ title="arXiv @ 2023.11.04">arXiv @ 2023.11.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231105000000/ title="arXiv @ 2023.11.05">arXiv @ 2023.11.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231106000000/ title="arXiv @ 2023.11.06">arXiv @ 2023.11.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231107000000/ title="arXiv @ 2023.11.07">arXiv @ 2023.11.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231108000000/ title="arXiv @ 2023.11.08">arXiv @ 2023.11.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231109000000/ title="arXiv @ 2023.11.09">arXiv @ 2023.11.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231110000000/ title="arXiv @ 2023.11.10">arXiv @ 2023.11.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231111000000/ title="arXiv @ 2023.11.11">arXiv @ 2023.11.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231112000000/ title="arXiv @ 2023.11.12">arXiv @ 2023.11.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231113000000/ title="arXiv @ 2023.11.13">arXiv @ 2023.11.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231114000000/ title="arXiv @ 2023.11.14">arXiv @ 2023.11.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231115000000/ title="arXiv @ 2023.11.15">arXiv @ 2023.11.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231116000000/ title="arXiv @ 2023.11.16">arXiv @ 2023.11.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231117000000/ title="arXiv @ 2023.11.17">arXiv @ 2023.11.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231118000000/ title="arXiv @ 2023.11.18">arXiv @ 2023.11.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231119000000/ title="arXiv @ 2023.11.19">arXiv @ 2023.11.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231120000000/ title="arXiv @ 2023.11.20">arXiv @ 2023.11.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231121000000/ title="arXiv @ 2023.11.21">arXiv @ 2023.11.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231122000000/ title="arXiv @ 2023.11.22">arXiv @ 2023.11.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231123000000/ title="arXiv @ 2023.11.23">arXiv @ 2023.11.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231124000000/ title="arXiv @ 2023.11.24">arXiv @ 2023.11.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231125000000/ title="arXiv @ 2023.11.25">arXiv @ 2023.11.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231126000000/ title="arXiv @ 2023.11.26">arXiv @ 2023.11.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231127000000/ title="arXiv @ 2023.11.27">arXiv @ 2023.11.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231128000000/ title="arXiv @ 2023.11.28">arXiv @ 2023.11.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231129000000/ title="arXiv @ 2023.11.29">arXiv @ 2023.11.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231130000000/ title="arXiv @ 2023.11.30">arXiv @ 2023.11.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202312/>2023.12</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231201000000/ title="arXiv @ 2023.12.01">arXiv @ 2023.12.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231202000000/ title="arXiv @ 2023.12.02">arXiv @ 2023.12.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231203000000/ title="arXiv @ 2023.12.03">arXiv @ 2023.12.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231204000000/ title="arXiv @ 2023.12.04">arXiv @ 2023.12.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231205000000/ title="arXiv @ 2023.12.05">arXiv @ 2023.12.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231206000000/ title="arXiv @ 2023.12.06">arXiv @ 2023.12.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231207000000/ title="arXiv @ 2023.12.07">arXiv @ 2023.12.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231208000000/ title="arXiv @ 2023.12.08">arXiv @ 2023.12.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231209000000/ title="arXiv @ 2023.12.09">arXiv @ 2023.12.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231210000000/ title="arXiv @ 2023.12.10">arXiv @ 2023.12.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231211000000/ title="arXiv @ 2023.12.11">arXiv @ 2023.12.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231212000000/ title="arXiv @ 2023.12.12">arXiv @ 2023.12.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231213000000/ title="arXiv @ 2023.12.13">arXiv @ 2023.12.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231214000000/ title="arXiv @ 2023.12.14">arXiv @ 2023.12.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231215000000/ title="arXiv @ 2023.12.15">arXiv @ 2023.12.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231216000000/ title="arXiv @ 2023.12.16">arXiv @ 2023.12.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231217000000/ title="arXiv @ 2023.12.17">arXiv @ 2023.12.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231218000000/ title="arXiv @ 2023.12.18">arXiv @ 2023.12.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231219000000/ title="arXiv @ 2023.12.19">arXiv @ 2023.12.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231220000000/ title="arXiv @ 2023.12.20">arXiv @ 2023.12.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231221000000/ title="arXiv @ 2023.12.21">arXiv @ 2023.12.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231222000000/ title="arXiv @ 2023.12.22">arXiv @ 2023.12.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231223000000/ title="arXiv @ 2023.12.23">arXiv @ 2023.12.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231224000000/ title="arXiv @ 2023.12.24">arXiv @ 2023.12.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231225000000/ title="arXiv @ 2023.12.25">arXiv @ 2023.12.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231226000000/ title="arXiv @ 2023.12.26">arXiv @ 2023.12.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231227000000/ title="arXiv @ 2023.12.27">arXiv @ 2023.12.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231228000000/ title="arXiv @ 2023.12.28">arXiv @ 2023.12.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231229000000/ title="arXiv @ 2023.12.29">arXiv @ 2023.12.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231230000000/ title="arXiv @ 2023.12.30">arXiv @ 2023.12.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231231000000/ title="arXiv @ 2023.12.31">arXiv @ 2023.12.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202401/>2024.01</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240101000000/ title="arXiv @ 2024.01.01">arXiv @ 2024.01.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240102000000/ title="arXiv @ 2024.01.02">arXiv @ 2024.01.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240103000000/ title="arXiv @ 2024.01.03">arXiv @ 2024.01.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240104000000/ title="arXiv @ 2024.01.04">arXiv @ 2024.01.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240105000000/ title="arXiv @ 2024.01.05">arXiv @ 2024.01.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240106000000/ title="arXiv @ 2024.01.06">arXiv @ 2024.01.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240107000000/ title="arXiv @ 2024.01.07">arXiv @ 2024.01.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240108000000/ title="arXiv @ 2024.01.08">arXiv @ 2024.01.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240109000000/ title="arXiv @ 2024.01.09">arXiv @ 2024.01.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240110000000/ title="arXiv @ 2024.01.10">arXiv @ 2024.01.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240111000000/ title="arXiv @ 2024.01.11">arXiv @ 2024.01.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240112000000/ title="arXiv @ 2024.01.12">arXiv @ 2024.01.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240113000000/ title="arXiv @ 2024.01.13">arXiv @ 2024.01.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240114000000/ title="arXiv @ 2024.01.14">arXiv @ 2024.01.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240115000000/ title="arXiv @ 2024.01.15">arXiv @ 2024.01.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240116000000/ title="arXiv @ 2024.01.16">arXiv @ 2024.01.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240117000000/ title="arXiv @ 2024.01.17">arXiv @ 2024.01.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240118000000/ title="arXiv @ 2024.01.18">arXiv @ 2024.01.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240119000000/ title="arXiv @ 2024.01.19">arXiv @ 2024.01.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240120000000/ title="arXiv @ 2024.01.20">arXiv @ 2024.01.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240121000000/ title="arXiv @ 2024.01.21">arXiv @ 2024.01.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240122000000/ title="arXiv @ 2024.01.22">arXiv @ 2024.01.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240123000000/ title="arXiv @ 2024.01.23">arXiv @ 2024.01.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240124000000/ title="arXiv @ 2024.01.24">arXiv @ 2024.01.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240125000000/ title="arXiv @ 2024.01.25">arXiv @ 2024.01.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240126000000/ title="arXiv @ 2024.01.26">arXiv @ 2024.01.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240127000000/ title="arXiv @ 2024.01.27">arXiv @ 2024.01.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240128000000/ title="arXiv @ 2024.01.28">arXiv @ 2024.01.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240129000000/ title="arXiv @ 2024.01.29">arXiv @ 2024.01.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240130000000/ title="arXiv @ 2024.01.30">arXiv @ 2024.01.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240131000000/ title="arXiv @ 2024.01.31">arXiv @ 2024.01.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202402/>2024.02</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240201000000/ title="arXiv @ 2024.02.01">arXiv @ 2024.02.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240202000000/ title="arXiv @ 2024.02.02">arXiv @ 2024.02.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240203000000/ title="arXiv @ 2024.02.03">arXiv @ 2024.02.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240204000000/ title="arXiv @ 2024.02.04">arXiv @ 2024.02.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240205000000/ title="arXiv @ 2024.02.05">arXiv @ 2024.02.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240206000000/ title="arXiv @ 2024.02.06">arXiv @ 2024.02.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/ title="arXiv @ 2024.02.07">arXiv @ 2024.02.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/ title="arXiv @ 2024.02.08">arXiv @ 2024.02.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/ title="arXiv @ 2024.02.09">arXiv @ 2024.02.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/ title="arXiv @ 2024.02.10">arXiv @ 2024.02.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240211000000/ title="arXiv @ 2024.02.11">arXiv @ 2024.02.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/ title="arXiv @ 2024.02.12">arXiv @ 2024.02.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/ title="arXiv @ 2024.02.13">arXiv @ 2024.02.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/ title="arXiv @ 2024.02.14">arXiv @ 2024.02.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/ title="arXiv @ 2024.02.15">arXiv @ 2024.02.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240216000000/ title="arXiv @ 2024.02.16">arXiv @ 2024.02.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/ title="arXiv @ 2024.02.17">arXiv @ 2024.02.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240218000000/ title="arXiv @ 2024.02.18">arXiv @ 2024.02.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/ title="arXiv @ 2024.02.19">arXiv @ 2024.02.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/ title="arXiv @ 2024.02.20">arXiv @ 2024.02.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/ title="arXiv @ 2024.02.21">arXiv @ 2024.02.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/ title="arXiv @ 2024.02.22">arXiv @ 2024.02.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/ title="arXiv @ 2024.02.23">arXiv @ 2024.02.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/ title="arXiv @ 2024.02.24">arXiv @ 2024.02.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/ title="arXiv @ 2024.02.25">arXiv @ 2024.02.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/ title="arXiv @ 2024.02.26">arXiv @ 2024.02.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/ title="arXiv @ 2024.02.27">arXiv @ 2024.02.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/ title="arXiv @ 2024.02.28">arXiv @ 2024.02.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/ title="arXiv @ 2024.02.29">arXiv @ 2024.02.29</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/202403/>2024.03</a><ul class=active><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/ title="arXiv @ 2024.03.01">arXiv @ 2024.03.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/ title="arXiv @ 2024.03.02">arXiv @ 2024.03.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/ title="arXiv @ 2024.03.03">arXiv @ 2024.03.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240304000000/ title="arXiv @ 2024.03.04">arXiv @ 2024.03.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/ title="arXiv @ 2024.03.05">arXiv @ 2024.03.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/ title="arXiv @ 2024.03.06">arXiv @ 2024.03.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/ title="arXiv @ 2024.03.07">arXiv @ 2024.03.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/ title="arXiv @ 2024.03.08">arXiv @ 2024.03.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/ title="arXiv @ 2024.03.09">arXiv @ 2024.03.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/ title="arXiv @ 2024.03.10">arXiv @ 2024.03.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/ title="arXiv @ 2024.03.11">arXiv @ 2024.03.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/ title="arXiv @ 2024.03.12">arXiv @ 2024.03.12</a></li><li><a class=active href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/ title="arXiv @ 2024.03.13">arXiv @ 2024.03.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/ title="arXiv @ 2024.03.14">arXiv @ 2024.03.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/ title="arXiv @ 2024.03.15">arXiv @ 2024.03.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/ title="arXiv @ 2024.03.16">arXiv @ 2024.03.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/ title="arXiv @ 2024.03.17">arXiv @ 2024.03.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/ title="arXiv @ 2024.03.18">arXiv @ 2024.03.18</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/algorithms/>Algorithms</a><ul><li><a href=/akitenkrad-blog/posts/algorithms/bandit-algorithm-basic/ title="Bandit Algorithm Basic">Bandit Algorithm Basic</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/conference/>Conference</a><ul><li><a href=/akitenkrad-blog/posts/conference/acl/ title="Annual Meeting of the Association for Computational Linguistics">Annual Meeting of the Association for Computational Linguistics</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/>Figures</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/figures/202208/20220822092748/ title=1ヶ月当たりの実労働時間の推移>1ヶ月当たりの実労働時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220813115200/ title=コンビニエンスストアの店舗数の推移>コンビニエンスストアの店舗数の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220821113322/ title=地域別1世帯当たり1ヶ月間の支出額の推移>地域別1世帯当たり1ヶ月間の支出額の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220828111836/ title="夫婦別子供の有無別生活時間 (2016)">夫婦別子供の有無別生活時間 (2016)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220827103615/ title=家族類型の変遷>家族類型の変遷</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220815131016/ title="年齢別大学院入学者数 (2021)">年齢別大学院入学者数 (2021)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220818122628/ title=情報サービス業における企業特殊的人的資本（名目）の推移>情報サービス業における企業特殊的人的資本（名目）の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220824095127/ title=有業者の社会生活における活動の変化>有業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220814175022/ title=東京23区の乗用車保有台数>東京23区の乗用車保有台数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220817121117/ title="業種別企業特殊的人的資本 (2018)">業種別企業特殊的人的資本 (2018)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220825094111/ title=無業者の社会生活における活動の変化>無業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220823104100/ title=睡眠時間の推移>睡眠時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220826102652/ title=総人口の推移>総人口の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220820225528/ title=都道府県別国公立別学校数・学生数>都道府県別国公立別学校数・学生数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220816120845/ title="都道府県別消費者物価指数/教育（全国平均=100）">都道府県別消費者物価指数/教育（全国平均=100）</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220812173814/ title=金沢市の夏の月平均気温の遷移>金沢市の夏の月平均気温の遷移</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/figures/202209/20220907122906/ title="ACL-2022 WordCloud">ACL-2022 WordCloud</a></li></ul></li></ul></li><li><a href=/akitenkrad-blog/posts/latex/ title="Latex Mathematics Syntax Guide">Latex Mathematics Syntax Guide</a></li><li><a href=/akitenkrad-blog/posts/markdown/ title="Markdown Sample">Markdown Sample</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/akitenkrad-blog/posts/arxiv/202403/20240313000000/hero.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_120x120_fit_box_3.png alt="Author Image"><h5 class=author-name></h5><p>Wednesday, Mar 13, 2024</p></div><div class=title><h1>arXiv @ 2024.03.13</h1></div><div class=taxonomy-terms><ul><li class=rounded><a href=/akitenkrad-blog/tags/arxiv class="btn, btn-sm">arXiv</a></li><li class=rounded><a href=/akitenkrad-blog/tags/published2024 class="btn, btn-sm">2024</a></li></ul></div><div class=post-content id=post-content><figure style=border:none;width:100%;display:flex;justify-content:center><iframe src=pie.html width=900 height=620 style=border:none></iframe></figure><h2 id=primary-categories>Primary Categories</h2><ul><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/#cond-matmtrl-sci-1>cond-mat.mtrl-sci (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/#csai-4>cs.AI (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/#csar-3>cs.AR (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/#csce-2>cs.CE (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/#cscg-1>cs.CG (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/#cscl-39>cs.CL (39)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/#cscr-7>cs.CR (7)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/#cscv-97>cs.CV (97)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/#cscy-3>cs.CY (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/#csdb-3>cs.DB (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/#csdc-6>cs.DC (6)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/#csdm-1>cs.DM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/#csds-2>cs.DS (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/#cset-2>cs.ET (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/#csgr-2>cs.GR (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/#csgt-1>cs.GT (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/#cshc-5>cs.HC (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/#csir-8>cs.IR (8)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/#csit-2>cs.IT (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/#cslg-63>cs.LG (63)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/#cslo-1>cs.LO (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/#csma-2>cs.MA (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/#csmm-1>cs.MM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/#csne-1>cs.NE (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/#cspl-1>cs.PL (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/#csro-12>cs.RO (12)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/#cssd-1>cs.SD (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/#csse-4>cs.SE (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/#cssi-1>cs.SI (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/#eessas-2>eess.AS (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/#eessiv-11>eess.IV (11)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/#eesssp-3>eess.SP (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/#eesssy-5>eess.SY (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/#hep-ph-1>hep-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/#mathco-3>math.CO (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/#mathdg-2>math.DG (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/#mathlo-1>math.LO (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/#mathna-2>math.NA (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/#mathoc-1>math.OC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/#mathst-1>math.ST (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/#physicscomp-ph-1>physics.comp-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/#physicsdata-an-1>physics.data-an (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/#q-finrm-1>q-fin.RM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/#quant-ph-4>quant-ph (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/#statme-1>stat.ME (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/#statml-2>stat.ML (2)</a></li></ul><h2 id=keywords>Keywords</h2><table border=1 class=dataframe><thead><tr style=text-align:right><th>keyword</th><th>cs.CL</th><th>cs.CV</th><th>cs.LG</th><th>cs.RO</th><th>eess.IV</th></tr></thead><tbody><tr><td>Active Learning</td><td>1</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Adversarial Attack</td><td></td><td>2</td><td>1</td><td></td><td></td></tr><tr><td>Adversarial Learning</td><td></td><td></td><td></td><td></td><td>1</td></tr><tr><td>Adversarial Purification</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Anomaly Detection</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Autoencoder</td><td></td><td>1</td><td>4</td><td></td><td></td></tr><tr><td>Automatic Speech Recognition</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>BERT</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Bandit Algorithm</td><td></td><td></td><td>3</td><td></td><td></td></tr><tr><td>Benchmarking</td><td>10</td><td>36</td><td>8</td><td>3</td><td>1</td></tr><tr><td>Black Box</td><td></td><td>1</td><td>3</td><td></td><td></td></tr><tr><td>ChatGPT</td><td>3</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Chatbot</td><td>1</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Clustering</td><td>1</td><td>1</td><td></td><td></td><td>1</td></tr><tr><td>Code Generation</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Continual Learning</td><td></td><td></td><td>2</td><td></td><td></td></tr><tr><td>Contrastive Learning</td><td>1</td><td>2</td><td>1</td><td></td><td></td></tr><tr><td>Convolution</td><td></td><td>7</td><td>3</td><td></td><td>3</td></tr><tr><td>Convolutional Neural Network</td><td></td><td>5</td><td>3</td><td></td><td>3</td></tr><tr><td>Counter-factual</td><td></td><td>2</td><td>1</td><td></td><td></td></tr><tr><td>Counterfactual Reasoning</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Curriculum Learning</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Data Augmentation</td><td>1</td><td>5</td><td></td><td></td><td></td></tr><tr><td>Dependency Parsing</td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>Diffusion Model</td><td></td><td>13</td><td>2</td><td></td><td>2</td></tr><tr><td>Domain Adaptation</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Emotion Recognition</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Face Recognition</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Fairness</td><td></td><td>1</td><td>2</td><td></td><td></td></tr><tr><td>Federated Learning</td><td></td><td></td><td>3</td><td></td><td></td></tr><tr><td>Few-shot</td><td>1</td><td>6</td><td>3</td><td></td><td></td></tr><tr><td>Few-shot Learning</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Fine-tuning</td><td>11</td><td>17</td><td>8</td><td></td><td>1</td></tr><tr><td>Foundation Model</td><td></td><td>3</td><td>2</td><td></td><td>1</td></tr><tr><td>GPT</td><td>8</td><td></td><td>1</td><td></td><td></td></tr><tr><td>GPT-2</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>GPT-3</td><td>6</td><td></td><td></td><td></td><td></td></tr><tr><td>GPT-3.5</td><td>5</td><td></td><td></td><td></td><td></td></tr><tr><td>GPT-4</td><td>2</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Gaussian Process</td><td></td><td></td><td>2</td><td></td><td></td></tr><tr><td>Gemini</td><td>1</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Generative AI</td><td>1</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Generative Adversarial Network</td><td></td><td>3</td><td></td><td></td><td></td></tr><tr><td>Geometry</td><td></td><td>4</td><td>1</td><td></td><td>1</td></tr><tr><td>Graph</td><td>5</td><td>5</td><td>12</td><td>1</td><td>2</td></tr><tr><td>Graph Classification</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Graph Embedding</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Graph Neural Network</td><td></td><td>3</td><td>11</td><td></td><td></td></tr><tr><td>Grounding</td><td>1</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Hallucination Detection</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Hate Speech Detection</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Image2text</td><td></td><td>2</td><td>1</td><td></td><td></td></tr><tr><td>In-context Learning</td><td>7</td><td>3</td><td>2</td><td></td><td></td></tr><tr><td>Information Retrieval</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Instruction Tuning</td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>Knowledge Based Question Answering</td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>Knowledge Distillation</td><td>6</td><td>7</td><td>6</td><td></td><td></td></tr><tr><td>Knowledge Graph</td><td>6</td><td></td><td>2</td><td></td><td></td></tr><tr><td>LLaMA</td><td>2</td><td></td><td>1</td><td></td><td></td></tr><tr><td>LSTM</td><td>3</td><td></td><td>3</td><td></td><td></td></tr><tr><td>Label Smoothing</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Large Language Model</td><td>45</td><td>9</td><td>12</td><td>2</td><td></td></tr><tr><td>Low-Resource</td><td>4</td><td></td><td></td><td></td><td></td></tr><tr><td>Markov Decision Process</td><td></td><td></td><td>3</td><td></td><td></td></tr><tr><td>Meta Learning</td><td></td><td></td><td>2</td><td></td><td></td></tr><tr><td>Mistral</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Model Compression</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Model Quantization</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Multi-modal</td><td>6</td><td>16</td><td>1</td><td>3</td><td>2</td></tr><tr><td>Multiple Instance Learning</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Named Entity Recognition</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Natural Language Understanding</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Neural Machine Translation</td><td>3</td><td></td><td></td><td></td><td></td></tr><tr><td>Node Classification</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Object Detection</td><td></td><td>9</td><td>1</td><td></td><td></td></tr><tr><td>Offline Reinforcement Learning</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Out-of-distribution</td><td></td><td>4</td><td>1</td><td></td><td></td></tr><tr><td>Out-of-domain</td><td></td><td></td><td>2</td><td></td><td></td></tr><tr><td>Pre-trained Language Model</td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>Prompt</td><td>4</td><td>14</td><td>4</td><td>1</td><td></td></tr><tr><td>Prompt Learning</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Pruning</td><td></td><td>2</td><td>1</td><td></td><td></td></tr><tr><td>Quantization</td><td></td><td>5</td><td>3</td><td></td><td></td></tr><tr><td>Question Answering</td><td>5</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Reasoning</td><td>9</td><td>2</td><td>1</td><td></td><td></td></tr><tr><td>Recommendation</td><td></td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>Recurrent Neural Network</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Reinforcement Learning</td><td>1</td><td>3</td><td>13</td><td>2</td><td></td></tr><tr><td>Reinforcement Learning from Human Feedback</td><td>2</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Relation Extraction</td><td>1</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Representation Learning</td><td>1</td><td>3</td><td>3</td><td></td><td></td></tr><tr><td>Retrieval Augmentation</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Retrieval-Augmented Generation</td><td>6</td><td></td><td></td><td></td><td></td></tr><tr><td>Sample Size</td><td></td><td>1</td><td>2</td><td></td><td></td></tr><tr><td>Scaling Law</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Self-Attention</td><td>1</td><td>3</td><td>1</td><td></td><td></td></tr><tr><td>Self-Distillation</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Self-supervised Learning</td><td></td><td>6</td><td>3</td><td></td><td></td></tr><tr><td>Semi-Supervised Learning</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Sentiment Analysis</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Simulation</td><td>1</td><td>4</td><td>3</td><td>6</td><td>1</td></tr><tr><td>Simulator</td><td>1</td><td>4</td><td>3</td><td>6</td><td>1</td></tr><tr><td>Stemming</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Style Transfer</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Summarization</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Supervised Learning</td><td>2</td><td>12</td><td>7</td><td></td><td>1</td></tr><tr><td>T5</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Text Classification</td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>Text Generation</td><td>3</td><td></td><td></td><td></td><td></td></tr><tr><td>Text Understanding</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Text2image</td><td></td><td>7</td><td>2</td><td></td><td></td></tr><tr><td>Transfer Learning</td><td>1</td><td>5</td><td></td><td></td><td>1</td></tr><tr><td>Transformer</td><td>5</td><td>13</td><td>6</td><td>2</td><td>2</td></tr><tr><td>Unsupervised Learning</td><td>3</td><td>5</td><td>2</td><td></td><td></td></tr><tr><td>Vision Transformer</td><td></td><td>2</td><td>2</td><td></td><td></td></tr><tr><td>Vision-and-Language</td><td></td><td>5</td><td>2</td><td></td><td></td></tr><tr><td>Weakly-supervised Learning</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Zero-shot</td><td>1</td><td>6</td><td>2</td><td></td><td></td></tr><tr><td>Zero-shot Learning</td><td></td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>falcon</td><td>1</td><td></td><td>1</td><td></td><td></td></tr></tbody></table><script>$(function(){$("table").addClass("keyword-table table-bordered border-success"),$("table thead").addClass("sticky-top"),$("table tbody td").css("text-align","")})</script><h2 id=cscl-39>cs.CL (39)</h2><h3 id=139--1318-mend-meta-demonstration-distillation-for-efficient-and-effective-in-context-learning-yichuan-li-et-al-2024>(1/39 | 1/318) MEND: Meta dEmonstratioN Distillation for Efficient and Effective In-Context Learning (Yichuan Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yichuan Li, Xiyao Ma, Sixing Lu, Kyumin Lee, Xiaohu Liu, Chenlei Guo. (2024)<br><strong>MEND: Meta dEmonstratioN Distillation for Efficient and Effective In-Context Learning</strong><br><button class=copy-to-clipboard title="MEND: Meta dEmonstratioN Distillation for Efficient and Effective In-Context Learning" index=1>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-1 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 130<br>Keywords: Fine-tuning, Knowledge Distillation, Knowledge Distillation, Knowledge Distillation, GPT, GPT-2, T5, In-context Learning, In-context Learning, In-context Learning, Large Language Model, Large Language Model, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06914v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06914v2.pdf filename=2403.06914v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>models</b> <b>(LLMs)</b> have demonstrated impressive <b>in-context</b> <b>learning</b> <b>(ICL)</b> capabilities, where a <b>LLM</b> makes predictions for a given test input together with a few input-output pairs (demonstrations). Nevertheless, the inclusion of demonstrations leads to a quadratic increase in the computational overhead of the <b>self-attention</b> mechanism. Existing solutions attempt to <b>distill</b> lengthy demonstrations into compact vectors. However, they often require task-specific retraining or compromise <b>LLM&rsquo;s</b> <b>in-context</b> <b>learning</b> performance. To mitigate these challenges, we present Meta dEmonstratioN <b>Distillation</b> (MEND), where a language model learns to <b>distill</b> any lengthy demonstrations into vectors without retraining for a new downstream task. We exploit the <b>knowledge</b> <b>distillation</b> to enhance alignment between MEND and <b>LLM,</b> achieving both efficiency and effectiveness simultaneously. MEND is endowed with the meta-knowledge of <b>distilling</b> demonstrations through a two-stage training process, which includes meta-distillation pretraining and <b>fine-tuning.</b> Comprehensive evaluations across seven diverse <b>ICL</b> task partitions using decoder-only <b>(GPT-2)</b> and encoder-decoder <b>(T5)</b> attest to MEND&rsquo;s prowess. It not only matches but often outperforms the Vanilla <b>ICL</b> as well as other state-of-the-art <b>distillation</b> models, while significantly reducing the computational demands. This innovation promises enhanced scalability and efficiency for the practical deployment of <b>large</b> <b>language</b> <b>models</b></p></p class="citation"></blockquote><h3 id=239--2318-development-of-a-reliable-and-accessible-caregiving-language-model-calm-bambang-parmanto-et-al-2024>(2/39 | 2/318) Development of a Reliable and Accessible Caregiving Language Model (CaLM) (Bambang Parmanto et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bambang Parmanto, Bayu Aryoyudanta, Wilbert Soekinto, I Made Agus Setiawan, Yuhan Wang, Haomin Hu, Andi Saptono, Yong K. Choi. (2024)<br><strong>Development of a Reliable and Accessible Caregiving Language Model (CaLM)</strong><br><button class=copy-to-clipboard title="Development of a Reliable and Accessible Caregiving Language Model (CaLM)" index=2>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-2 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 123<br>Keywords: Benchmarking, Fine-tuning, Fine-tuning, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, GPT, GPT-3, GPT-3.5, LLaMA, falcon, Grounding, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06857v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06857v1.pdf filename=2403.06857v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Unlike professional caregivers, family caregivers often assume this role without formal preparation or training. Because of this, there is an urgent need to enhance the capacity of family caregivers to provide quality care. <b>Large</b> <b>language</b> <b>models</b> can potentially be used as a foundation technology for supporting caregivers as educational tools or as adjunct to care. This study aimed to develop a reliable Caregiving Language Model (CaLM) by using FMs and a caregiving knowledge base, develop an accessible CaLM using a small FM that requires fewer computing resources, and evaluate the performance of the model compared to a <b>large</b> <b>FM.</b> <b>We</b> developed CaLM using the <b>Retrieval</b> <b>Augmented</b> <b>Generation</b> <b>(RAG)</b> framework combined with FM <b>fine-tuning</b> for improving the quality of FM answers by <b>grounding</b> the model on a caregiving knowledge base. We used two small FMs as candidates for the FM of CaLM <b>(LLaMA-2</b> and <b>Falcon</b> with 7B parameters) and larger FM <b>GPT-3.5</b> as a <b>benchmark.</b> We developed the caregiving knowledge base by gathering various types of documents from the Internet. In this study, we focused on caregivers of individuals with Alzheimer&rsquo;s Disease Related Dementias. We evaluated the models&rsquo; performance using the <b>benchmark</b> metrics commonly used in evaluating language models and their reliability to provide accurate references with the answers. The <b>RAG</b> framework improved the performance of all FMs used in this study across all measures. As expected, the <b>large</b> <b>FM</b> <b>performed</b> better than small FMs across all metrics. The most interesting result is that small <b>fine-tuned</b> FMs with <b>RAG</b> performed significantly better than <b>GPT</b> 3.5 across all metrics. The <b>fine-tuned</b> <b>LLaMA-2</b> small FM performed better than <b>GPT</b> 3.5 (even with <b>RAG)</b> in returning references with the answers. The study shows that reliable and accessible CaLM can be developed by using small FMs with a knowledge base specific to the caregiving domain.</p></p class="citation"></blockquote><h3 id=339--3318-alarm-align-language-models-via-hierarchical-rewards-modeling-yuhang-lai-et-al-2024>(3/39 | 3/318) ALaRM: Align Language Models via Hierarchical Rewards Modeling (Yuhang Lai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuhang Lai, Siyuan Wang, Shujun Liu, Xuanjing Huang, Zhongyu Wei. (2024)<br><strong>ALaRM: Align Language Models via Hierarchical Rewards Modeling</strong><br><button class=copy-to-clipboard title="ALaRM: Align Language Models via Hierarchical Rewards Modeling" index=3>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-3 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 110<br>Keywords: Reinforcement Learning, Reinforcement Learning from Human Feedback, Reinforcement Learning from Human Feedback, GPT, GPT-3, GPT-3.5, Neural Machine Translation, Question Answering, Text Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06754v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06754v1.pdf filename=2403.06754v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce ALaRM, the first framework modeling hierarchical rewards in <b>reinforcement</b> <b>learning</b> <b>from</b> <b>human</b> <b>feedback</b> <b>(RLHF),</b> which is designed to enhance the alignment of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> with human preferences. The framework addresses the limitations of current alignment approaches, which often struggle with the inconsistency and sparsity of human supervision signals, by integrating holistic rewards with aspect-specific rewards. This integration enables more precise and consistent guidance of language models towards desired outcomes, particularly in complex and open <b>text</b> <b>generation</b> tasks. By employing a methodology that filters and combines multiple rewards based on their consistency, the framework provides a reliable mechanism for improving model alignment. We validate our approach through applications in long-form <b>question</b> <b>answering</b> and <b>machine</b> <b>translation</b> tasks, employing <b>gpt-3.5-turbo</b> for pairwise comparisons, and demonstrate improvements over existing baselines. Our work underscores the effectiveness of hierarchical rewards modeling in refining <b>LLM</b> training processes for better human preference alignment. We release our code at <a href=https://ALaRM-fdu.github.io>https://ALaRM-fdu.github.io</a>.</p></p class="citation"></blockquote><h3 id=439--4318-ra-isf-learning-to-answer-and-understand-from-retrieval-augmentation-via-iterative-self-feedback-yanming-liu-et-al-2024>(4/39 | 4/318) RA-ISF: Learning to Answer and Understand from Retrieval Augmentation via Iterative Self-Feedback (Yanming Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yanming Liu, Xinyue Peng, Xuhong Zhang, Weihao Liu, Jianwei Yin, Jiannan Cao, Tianyu Du. (2024)<br><strong>RA-ISF: Learning to Answer and Understand from Retrieval Augmentation via Iterative Self-Feedback</strong><br><button class=copy-to-clipboard title="RA-ISF: Learning to Answer and Understand from Retrieval Augmentation via Iterative Self-Feedback" index=4>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-4 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 93<br>Keywords: Benchmarking, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, GPT-3, GPT-3.5, Reasoning, Large Language Model, Large Language Model, Retrieval Augmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06840v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06840v1.pdf filename=2403.06840v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> demonstrate exceptional performance in numerous tasks but still heavily rely on knowledge stored in their parameters. Moreover, updating this knowledge incurs high training costs. <b>Retrieval-augmented</b> <b>generation</b> <b>(RAG)</b> methods address this issue by integrating external knowledge. The model can answer questions it couldn&rsquo;t previously by retrieving knowledge relevant to the query. This approach improves performance in certain scenarios for specific tasks. However, if irrelevant texts are retrieved, it may impair model performance. In this paper, we propose <b>Retrieval</b> <b>Augmented</b> <b>Iterative</b> Self-Feedback (RA-ISF), a framework that iteratively decomposes tasks and processes them in three submodules to enhance the model&rsquo;s problem-solving capabilities. Experiments show that our method outperforms existing <b>benchmarks,</b> performing well on models like <b>GPT3.5,</b> Llama2, significantly enhancing factual <b>reasoning</b> capabilities and reducing hallucinations.</p></p class="citation"></blockquote><h3 id=539--5318-amharic-llama-and-llava-multimodal-llms-for-low-resource-languages-michael-andersland-2024>(5/39 | 5/318) Amharic LLaMA and LLaVA: Multimodal LLMs for Low Resource Languages (Michael Andersland, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michael Andersland. (2024)<br><strong>Amharic LLaMA and LLaVA: Multimodal LLMs for Low Resource Languages</strong><br><button class=copy-to-clipboard title="Amharic LLaMA and LLaVA: Multimodal LLMs for Low Resource Languages" index=5>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-5 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 92<br>Keywords: Benchmarking, Benchmarking, Data Augmentation, Low-Resource, Multi-modal, Multi-modal, GPT, GPT-4, LLaMA, Instruction Tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06354v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06354v1.pdf filename=2403.06354v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> like <b>GPT-4</b> and <b>LLaMA</b> have shown incredible proficiency at natural language processing tasks and have even begun to excel at tasks across other modalities such as vision and audio. Despite their success, <b>LLMs</b> often struggle to perform well on <b>low-resource</b> languages because there is so little training <b>data</b> <b>available.</b> This shortcoming is especially prevalent with open source models. In this work, we explore training <b>LLaMA-2</b> to speak Amharic, a language which is spoken by over 50 million people world wide, but has orders of magnitude less <b>data</b> <b>available</b> than languages like English. We employ methods previously used for training <b>LLMs</b> on other languages with <b>data</b> <b>scarcity,</b> and use open source translation models to perform <b>data</b> <b>augmentation</b> and grow our dataset from millions of tokens to billions. We further enhance the capabilities of our model by connecting an image encoder and training on a translated visual <b>instruction</b> <b>tuning</b> dataset in the same manner as LLaVA, resulting in a <b>multimodal</b> Amharic <b>LLM</b> that can understand images along with text. We introduce an Amharic version of a popular <b>benchmarking</b> dataset to evaluate our work. Our models and dataset are open sourced and available on GitHub.</p></p class="citation"></blockquote><h3 id=639--6318-evolving-knowledge-distillation-with-large-language-models-and-active-learning-chengyuan-liu-et-al-2024>(6/39 | 6/318) Evolving Knowledge Distillation with Large Language Models and Active Learning (Chengyuan Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chengyuan Liu, Yangyang Kang, Fubang Zhao, Kun Kuang, Zhuoren Jiang, Changlong Sun, Fei Wu. (2024)<br><strong>Evolving Knowledge Distillation with Large Language Models and Active Learning</strong><br><button class=copy-to-clipboard title="Evolving Knowledge Distillation with Large Language Models and Active Learning" index=6>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-6 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 90<br>Keywords: Active Learning, Knowledge Distillation, Knowledge Distillation, Knowledge Distillation, Named Entity Recognition, Text Classification, Text Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06414v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06414v1.pdf filename=2403.06414v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have demonstrated remarkable capabilities across various NLP tasks. However, their computational costs are prohibitively high. To address this issue, previous research has attempted to <b>distill</b> the <b>knowledge</b> <b>of</b> <b>LLMs</b> into smaller models by generating annotated data. Nonetheless, these works have mainly focused on the direct use of <b>LLMs</b> for <b>text</b> <b>generation</b> and labeling, without fully exploring their potential to comprehend the target task and acquire valuable <b>knowledge.</b> <b>In</b> this paper, we propose EvoKD: Evolving <b>Knowledge</b> <b>Distillation,</b> which leverages the concept of <b>active</b> <b>learning</b> to interactively enhance the process of data generation using <b>large</b> <b>language</b> <b>models,</b> simultaneously improving the task capabilities of small domain model (student model). Different from previous work, we actively analyze the student model&rsquo;s weaknesses, and then synthesize labeled samples based on the analysis. In addition, we provide iterative feedback to the <b>LLMs</b> regarding the student model&rsquo;s performance to continuously construct diversified and challenging samples. Experiments and analysis on different NLP tasks, namely, <b>text</b> <b>classification</b> and <b>named</b> <b>entity</b> <b>recognition</b> show the effectiveness of EvoKD.</p></p class="citation"></blockquote><h3 id=739--7318-narrating-causal-graphs-with-large-language-models-atharva-phatak-et-al-2024>(7/39 | 7/318) Narrating Causal Graphs with Large Language Models (Atharva Phatak et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Atharva Phatak, Vijay K. Mago, Ameeta Agrawal, Aravind Inbasekaran, Philippe J. Giabbanelli. (2024)<br><strong>Narrating Causal Graphs with Large Language Models</strong><br><button class=copy-to-clipboard title="Narrating Causal Graphs with Large Language Models" index=7>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-7 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 88<br>Keywords: Graph, Fine-tuning, Generative AI, Knowledge Graph, Zero-shot, GPT, GPT-3, Reasoning, Large Language Model, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07118v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07118v1.pdf filename=2403.07118v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The use of <b>generative</b> <b>AI</b> to create text descriptions from <b>graphs</b> has mostly focused on <b>knowledge</b> <b>graphs,</b> which connect concepts using facts. In this work we explore the capability of <b>large</b> <b>pretrained</b> <b>language</b> <b>models</b> to generate text from causal <b>graphs,</b> where salient concepts are represented as nodes and causality is represented via directed, typed edges. The causal <b>reasoning</b> encoded in these <b>graphs</b> can support applications as diverse as healthcare or marketing. Using two publicly available causal <b>graph</b> datasets, we empirically investigate the performance of four <b>GPT-3</b> models under various settings. Our results indicate that while causal text descriptions improve with training data, compared to fact-based <b>graphs,</b> they are harder to generate under <b>zero-shot</b> settings. Results further suggest that users of <b>generative</b> <b>AI</b> can deploy future applications faster since similar performances are obtained when training a model with only a few examples as compared to <b>fine-tuning</b> via a <b>large</b> <b>curated</b> <b>dataset.</b></p></p class="citation"></blockquote><h3 id=839--8318-exploring-large-language-models-and-hierarchical-frameworks-for-classification-of-large-unstructured-legal-documents-nishchal-prasad-et-al-2024>(8/39 | 8/318) Exploring Large Language Models and Hierarchical Frameworks for Classification of Large Unstructured Legal Documents (Nishchal Prasad et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nishchal Prasad, Mohand Boughanem, Taoufiq Dkaki. (2024)<br><strong>Exploring Large Language Models and Hierarchical Frameworks for Classification of Large Unstructured Legal Documents</strong><br><button class=copy-to-clipboard title="Exploring Large Language Models and Hierarchical Frameworks for Classification of Large Unstructured Legal Documents" index=8>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-8 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 83<br>Keywords: Clustering, Fine-tuning, Supervised Learning, Transfer Learning, Unsupervised Learning, GPT, Transformer, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06872v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06872v1.pdf filename=2403.06872v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Legal judgment prediction suffers from the problem of long case documents exceeding tens of thousands of words, in general, and having a non-uniform structure. Predicting judgments from such documents becomes a challenging task, more so on documents with no structural annotation. We explore the classification of these <b>large</b> <b>legal</b> <b>documents</b> and their lack of structural information with a deep-learning-based hierarchical framework which we call MESc; &ldquo;Multi-stage Encoder-based <b>Supervised</b> with-clustering&rdquo;; for judgment prediction. Specifically, we divide a document into parts to extract their embeddings from the last four layers of a custom <b>fine-tuned</b> <b>Large</b> <b>Language</b> <b>Model,</b> and try to approximate their structure through <b>unsupervised</b> <b>clustering.</b> Which we use in another set of <b>transformer</b> encoder layers to learn the inter-chunk representations. We analyze the adaptability of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> with multi-billion parameters <b>(GPT-Neo,</b> and <b>GPT-J)</b> with the hierarchical framework of MESc and compare them with their standalone performance on legal texts. We also study their intra-domain(legal) <b>transfer</b> <b>learning</b> capability and the impact of combining embeddings from their last layers in MESc. We test these methods and their effectiveness with extensive experiments and ablation studies on legal documents from India, the European Union, and the United States with the ILDC dataset and a subset of the LexGLUE dataset. Our approach achieves a minimum total performance gain of approximately 2 points over previous state-of-the-art methods.</p></p class="citation"></blockquote><h3 id=939--9318-hybrid-human-llm-corpus-construction-and-llm-evaluation-for-rare-linguistic-phenomena-leonie-weissweiler-et-al-2024>(9/39 | 9/318) Hybrid Human-LLM Corpus Construction and LLM Evaluation for Rare Linguistic Phenomena (Leonie Weissweiler et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Leonie Weissweiler, Abdullatif Köksal, Hinrich Schütze. (2024)<br><strong>Hybrid Human-LLM Corpus Construction and LLM Evaluation for Rare Linguistic Phenomena</strong><br><button class=copy-to-clipboard title="Hybrid Human-LLM Corpus Construction and LLM Evaluation for Rare Linguistic Phenomena" index=9>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-9 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 80<br>Keywords: GPT, GPT-3, GPT-3.5, Gemini, Mistral, Dependency Parsing, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06965v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06965v1.pdf filename=2403.06965v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Argument Structure Constructions (ASCs) are one of the most well-studied construction groups, providing a unique opportunity to demonstrate the usefulness of Construction Grammar (CxG). For example, the caused-motion construction (CMC, <code>She sneezed the foam off her cappuccino'') demonstrates that constructions must carry meaning, otherwise the fact that </code>sneeze&rsquo;&rsquo; in this context causes movement cannot be explained. We form the hypothesis that this remains challenging even for state-of-the-art <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> for which we devise a test based on substituting the verb with a prototypical motion verb. To be able to perform this test at statistically significant scale, in the absence of adequate CxG corpora, we develop a novel pipeline of NLP-assisted collection of linguistically annotated text. We show how <b>dependency</b> <b>parsing</b> and <b>GPT-3.5</b> can be used to significantly reduce annotation cost and thus enable the annotation of rare phenomena at scale. We then evaluate <b>GPT,</b> <b>Gemini,</b> Llama2 and <b>Mistral</b> models for their understanding of the CMC using the newly collected corpus. We find that all models struggle with understanding the motion component that the CMC adds to a sentence.</p></p class="citation"></blockquote><h3 id=1039--10318-a-knowledge-injected-curriculum-pretraining-framework-for-question-answering-xin-lin-et-al-2024>(10/39 | 10/318) A Knowledge-Injected Curriculum Pretraining Framework for Question Answering (Xin Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xin Lin, Tianhuang Su, Zhenya Huang, Shangzi Xue, Haifeng Liu, Enhong Chen. (2024)<br><strong>A Knowledge-Injected Curriculum Pretraining Framework for Question Answering</strong><br><button class=copy-to-clipboard title="A Knowledge-Injected Curriculum Pretraining Framework for Question Answering" index=10>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-10 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 73<br>Keywords: Graph, Knowledge Graph, Knowledge Graph, Knowledge Based Question Answering, Natural Language Understanding, Question Answering, Reasoning, Knowledge Based Question Answering, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09712v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09712v1.pdf filename=2403.09712v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Knowledge-based</b> <b>question</b> <b>answering</b> <b>(KBQA)</b> is a key task in NLP research, and also an approach to access the web data and <b>knowledge,</b> <b>which</b> <b>requires</b> <b>exploiting</b> <b>knowledge</b> <b>graphs</b> <b>(KGs)</b> <b>for</b> <b>reasoning.</b> In the literature, one promising solution for <b>KBQA</b> is to incorporate the <b>pretrained</b> <b>language</b> <b>model</b> (LM) with <b>KGs</b> by generating <b>KG-centered</b> pretraining corpus, which has shown its superiority. However, these methods often depend on specific techniques and resources to work, which may not always be available and restrict its application. Moreover, existing methods focus more on improving language understanding with <b>KGs,</b> while neglect the more important human-like complex <b>reasoning.</b> To this end, in this paper, we propose a general <b>Knowledge-Injected</b> <b>Curriculum</b> <b>Pretraining</b> <b>framework</b> (KICP) to achieve comprehensive <b>KG</b> learning and exploitation for <b>KBQA</b> tasks, which is composed of <b>knowledge</b> <b>injection</b> <b>(KI),</b> <b>knowledge</b> <b>adaptation</b> <b>(KA)</b> <b>and</b> curriculum <b>reasoning</b> (CR). Specifically, the KI module first injects <b>knowledge</b> <b>into</b> <b>the</b> <b>LM</b> by generating <b>KG-centered</b> pretraining corpus, and generalizes the process into three key steps that could work with different implementations for flexible application. Next, the KA module learns <b>knowledge</b> <b>from</b> <b>the</b> <b>generated</b> corpus with LM equipped with an adapter as well as keeps its original <b>natural</b> <b>language</b> <b>understanding</b> ability to reduce the negative impacts of the difference between the generated and <b>natural</b> <b>corpus.</b> <b>Last,</b> to enable the LM with complex <b>reasoning,</b> the CR module follows human <b>reasoning</b> patterns to construct three corpora with increasing difficulties of <b>reasoning,</b> and further trains the LM from easy to hard in a curriculum manner. We provide an implementation of the general framework, and evaluate the proposed KICP on four real-word datasets. The results demonstrate that our framework can achieve higher performances.</p></p class="citation"></blockquote><h3 id=1139--11318-era-cot-improving-chain-of-thought-through-entity-relationship-analysis-yanming-liu-et-al-2024>(11/39 | 11/318) ERA-CoT: Improving Chain-of-Thought through Entity Relationship Analysis (Yanming Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yanming Liu, Xinyue Peng, Tianyu Du, Jianwei Yin, Weihao Liu, Xuhong Zhang. (2024)<br><strong>ERA-CoT: Improving Chain-of-Thought through Entity Relationship Analysis</strong><br><button class=copy-to-clipboard title="ERA-CoT: Improving Chain-of-Thought through Entity Relationship Analysis" index=11>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-11 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: GPT-3, GPT-3.5, Question Answering, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06932v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06932v1.pdf filename=2403.06932v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have achieved commendable accomplishments in various natural language processing tasks. However, <b>LLMs</b> still encounter significant challenges when dealing with complex scenarios involving multiple entities. These challenges arise from the presence of implicit relationships that demand multi-step <b>reasoning.</b> In this paper, we propose a novel approach ERA-CoT, which aids <b>LLMs</b> in understanding context by capturing relationships between entities and supports the <b>reasoning</b> of diverse tasks through Chain-of-Thoughts (CoT). Experimental results show that ERA-CoT demonstrates the superior performance of our proposed method compared to current CoT <b>prompting</b> methods, achieving a significant improvement of an average of 5.1% on <b>GPT3.5</b> compared to previous SOTA baselines. Our analysis indicates that ERA-CoT increases the <b>LLM&rsquo;s</b> understanding of entity relationships, significantly improves the accuracy of <b>question</b> <b>answering,</b> and enhances the <b>reasoning</b> ability of <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=1239--12318-act-mnmt-auto-constriction-turning-for-multilingual-neural-machine-translation-shaojie-dai-et-al-2024>(12/39 | 12/318) ACT-MNMT Auto-Constriction Turning for Multilingual Neural Machine Translation (Shaojie Dai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shaojie Dai, Xin Liu, Ping Luo, Yue Yu. (2024)<br><strong>ACT-MNMT Auto-Constriction Turning for Multilingual Neural Machine Translation</strong><br><button class=copy-to-clipboard title="ACT-MNMT Auto-Constriction Turning for Multilingual Neural Machine Translation" index=12>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-12 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: Fine-tuning, Supervised Learning, Neural Machine Translation, Neural Machine Translation, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06745v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06745v1.pdf filename=2403.06745v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>model</b> <b>(LLM)</b> has achieved promising performance in multilingual <b>machine</b> <b>translation</b> tasks through zero/few-shot <b>prompts</b> or <b>prompt-tuning.</b> However, due to the mixture of multilingual data during the pre-training of <b>LLM,</b> the <b>LLM-based</b> translation models face the off-target issue in both <b>prompt-based</b> methods, including a series of phenomena, namely instruction misunderstanding, translation with wrong language and over-generation. For this issue, this paper introduces an \textbf{\underline{A}}uto-\textbf{\underline{C}}onstriction \textbf{\underline{T}}urning mechanism for \textbf{\underline{M}}ultilingual \textbf{\underline{N}}eural \textbf{\underline{M}}achine \textbf{\underline{T}}ranslation (\model), which is a novel <b>supervised</b> <b>fine-tuning</b> mechanism and orthogonal to the traditional <b>prompt-based</b> methods. In this method, \model automatically constructs a constrained template in the target side by adding trigger tokens ahead of the ground truth. Furthermore, trigger tokens can be arranged and combined freely to represent different task semantics, and they can be iteratively updated to maximize the label likelihood. Experiments are performed on WMT test sets with multiple metrics, and the experimental results demonstrate that \model achieves substantially improved performance across multiple translation directions and reduce the off-target phenomena in the translation.</p></p class="citation"></blockquote><h3 id=1339--13318-guiding-clinical-reasoning-with-large-language-models-via-knowledge-seeds-jiageng-wu-et-al-2024>(13/39 | 13/318) Guiding Clinical Reasoning with Large Language Models via Knowledge Seeds (Jiageng WU et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiageng WU, Xian Wu, Jie Yang. (2024)<br><strong>Guiding Clinical Reasoning with Large Language Models via Knowledge Seeds</strong><br><button class=copy-to-clipboard title="Guiding Clinical Reasoning with Large Language Models via Knowledge Seeds" index=13>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-13 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: ChatGPT, GPT, GPT-4, Reasoning, In-context Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06609v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06609v1.pdf filename=2403.06609v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Clinical <b>reasoning</b> refers to the cognitive process that physicians employ in evaluating and managing patients. This process typically involves suggesting necessary examinations, diagnosing patients&rsquo; diseases, and deciding on appropriate therapies, etc. Accurate clinical <b>reasoning</b> requires extensive medical knowledge and rich clinical experience, setting a high bar for physicians. This is particularly challenging in developing countries due to the overwhelming number of patients and limited physician resources, contributing significantly to global health inequity and necessitating automated clinical <b>reasoning</b> approaches. Recently, the emergence of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> such as <b>ChatGPT</b> and <b>GPT-4</b> have demonstrated their potential in clinical <b>reasoning.</b> However, these <b>LLMs</b> are prone to hallucination problems, and the <b>reasoning</b> process of <b>LLMs</b> may not align with the clinical decision path of physicians. In this study, we introduce a novel framework, <b>In-Context</b> Padding (ICP), designed to enhance <b>LLMs</b> with medical knowledge. Specifically, we infer critical clinical <b>reasoning</b> elements (referred to as knowledge seeds) and use these as anchors to guide the generation process of <b>LLMs.</b> Experiments on two clinical question datasets demonstrate that ICP significantly improves the clinical <b>reasoning</b> ability of <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=1439--14318-one-size-doesnt-fit-all-learning-how-many-examples-to-use-for-in-context-learning-for-improved-text-classification-manish-chandra-et-al-2024>(14/39 | 14/318) &lsquo;One size doesn&rsquo;t fit all&rsquo;: Learning how many Examples to use for In-Context Learning for Improved Text Classification (Manish Chandra et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Manish Chandra, Debasis Ganguly, Yiwen Li, Iadh Ounis. (2024)<br><strong>&lsquo;One size doesn&rsquo;t fit all&rsquo;: Learning how many Examples to use for In-Context Learning for Improved Text Classification</strong><br><button class=copy-to-clipboard title="'One size doesn't fit all': Learning how many Examples to use for In-Context Learning for Improved Text Classification" index=14>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-14 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 70<br>Keywords: Few-shot, Fine-tuning, Text Classification, In-context Learning, In-context Learning, In-context Learning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06402v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06402v1.pdf filename=2403.06402v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Predictive models in natural language processing (NLP) have evolved from training models from scratch to <b>fine-tuning</b> pre-trained models with labelled data. An extreme form of this <b>fine-tuning</b> involves <b>in-context</b> <b>learning</b> <b>(ICL),</b> where the output of a pre-trained generative model (frozen decoder parameters) is controlled only with variations in the input strings (called instructions or <b>prompts).</b> An important component of <b>ICL</b> is the use of a small number of labelled data instances as examples in the <b>prompt.</b> While existing work uses a static number of examples during inference for each data instance, in this paper we propose a novel methodology of dynamically adapting the number of examples as per the data. This is analogous to the use of a variable-sized neighborhood in k-nearest neighbors (k-NN) classifier. In our proposed workflow of adaptive <b>ICL</b> (AICL), the number of demonstrations to employ during the inference on a particular data instance is predicted by the Softmax posteriors of a classifier. The parameters of this classifier are fitted on the optimal number of examples in <b>ICL</b> required to correctly infer the label of each instance in the training set with the hypothesis that a test instance that is similar to a training instance should use the same (or a closely matching) number of <b>few-shot</b> examples. Our experiments show that our AICL method results in improvement in <b>text</b> <b>classification</b> task on several standard datasets.</p></p class="citation"></blockquote><h3 id=1539--15318-the-power-of-noise-toward-a-unified-multi-modal-knowledge-graph-representation-framework-zhuo-chen-et-al-2024>(15/39 | 15/318) The Power of Noise: Toward a Unified Multi-modal Knowledge Graph Representation Framework (Zhuo Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhuo Chen, Yin Fang, Yichi Zhang, Lingbing Guo, Jiaoyan Chen, Huajun Chen, Wen Zhang. (2024)<br><strong>The Power of Noise: Toward a Unified Multi-modal Knowledge Graph Representation Framework</strong><br><button class=copy-to-clipboard title="The Power of Noise: Toward a Unified Multi-modal Knowledge Graph Representation Framework" index=15>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-15 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 51<br>Keywords: Graph, Knowledge Graph, Knowledge Graph, Multi-modal, Representation Learning, Transformer, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06832v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06832v1.pdf filename=2403.06832v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The advancement of <b>Multi-modal</b> Pre-training highlights the necessity for a robust <b>Multi-Modal</b> <b>Knowledge</b> <b>Graph</b> (MMKG) <b>representation</b> <b>learning</b> framework. This framework is crucial for integrating structured <b>knowledge</b> <b>into</b> <b>multi-modal</b> <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> at scale, aiming to alleviate issues like <b>knowledge</b> <b>misconceptions</b> and <b>multi-modal</b> hallucinations. In this work, to evaluate models&rsquo; ability to accurately embed entities within MMKGs, we focus on two widely researched tasks: <b>Multi-modal</b> <b>Knowledge</b> <b>Graph</b> Completion (MKGC) and <b>Multi-modal</b> Entity Alignment (MMEA). Building on this foundation, we propose a novel SNAG method that utilizes a <b>Transformer-based</b> architecture equipped with modality-level noise masking for the robust integration of <b>multi-modal</b> entity features in <b>KGs.</b> By incorporating specific training objectives for both MKGC and MMEA, our approach achieves SOTA performance across a total of ten datasets (three for MKGC and seven for MEMA), demonstrating its robustness and versatility. Besides, SNAG can not only function as a standalone model but also enhance other existing methods, providing stable performance improvements. Our code and data are available at: <a href=https://github.com/zjukg/SNAG>https://github.com/zjukg/SNAG</a>.</p></p class="citation"></blockquote><h3 id=1639--16318-mrl-parsing-without-tears-the-case-of-hebrew-shaltiel-shmidman-et-al-2024>(16/39 | 16/318) MRL Parsing Without Tears: The Case of Hebrew (Shaltiel Shmidman et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shaltiel Shmidman, Avi Shmidman, Moshe Koppel, Reut Tsarfaty. (2024)<br><strong>MRL Parsing Without Tears: The Case of Hebrew</strong><br><button class=copy-to-clipboard title="MRL Parsing Without Tears: The Case of Hebrew" index=16>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-16 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Low-Resource, Dependency Parsing, Information Retrieval, Relation Extraction, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06970v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06970v1.pdf filename=2403.06970v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Syntactic parsing remains a critical tool for <b>relation</b> <b>extraction</b> and <b>information</b> <b>extraction,</b> especially in <b>resource-scarce</b> languages where <b>LLMs</b> are lacking. Yet in morphologically rich languages (MRLs), where parsers need to identify multiple lexical units in each token, existing systems suffer in latency and setup complexity. Some use a pipeline to peel away the layers: first segmentation, then morphology tagging, and then syntax parsing; however, errors in earlier layers are then propagated forward. Others use a joint architecture to evaluate all permutations at once; while this improves accuracy, it is notoriously slow. In contrast, and taking Hebrew as a test case, we present a new &ldquo;flipped pipeline&rdquo;: decisions are made directly on the whole-token units by expert classifiers, each one dedicated to one specific task. The classifiers are independent of one another, and only at the end do we synthesize their predictions. This blazingly fast approach sets a new SOTA in Hebrew POS tagging and <b>dependency</b> <b>parsing,</b> while also reaching near-SOTA performance on other Hebrew NLP tasks. Because our architecture does not rely on any language-specific resources, it can serve as a model to develop similar parsers for other MRLs.</p></p class="citation"></blockquote><h3 id=1739--17318-conspemollm-conspiracy-theory-detection-using-an-emotion-based-large-language-model-zhiwei-liu-et-al-2024>(17/39 | 17/318) ConspEmoLLM: Conspiracy Theory Detection Using an Emotion-Based Large Language Model (Zhiwei Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhiwei Liu, Boyang Liu, Paul Thompson, Kailai Yang, Raghav Jain, Sophia Ananiadou. (2024)<br><strong>ConspEmoLLM: Conspiracy Theory Detection Using an Emotion-Based Large Language Model</strong><br><button class=copy-to-clipboard title="ConspEmoLLM: Conspiracy Theory Detection Using an Emotion-Based Large Language Model" index=17>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-17 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Fine-tuning, ChatGPT, Instruction Tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06765v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06765v1.pdf filename=2403.06765v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The internet has brought both benefits and harms to society. A prime example of the latter is misinformation, including conspiracy theories, which flood the web. Recent advances in natural language processing, particularly the emergence of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> have improved the prospects of accurate misinformation detection. However, most <b>LLM-based</b> approaches to conspiracy theory detection focus only on binary classification and fail to account for the important relationship between misinformation and affective features (i.e., sentiment and emotions). Driven by a comprehensive analysis of conspiracy text that reveals its distinctive affective features, we propose ConspEmoLLM, the first open-source <b>LLM</b> that integrates affective information and is able to perform diverse tasks relating to conspiracy theories. These tasks include not only conspiracy theory detection, but also classification of theory type and detection of related discussion (e.g., opinions towards theories). ConspEmoLLM is <b>fine-tuned</b> based on an emotion-oriented <b>LLM</b> using our novel ConDID dataset, which includes five tasks to support <b>LLM</b> <b>instruction</b> <b>tuning</b> and evaluation. We demonstrate that when applied to these tasks, ConspEmoLLM largely outperforms several open-source general domain <b>LLMs</b> and <b>ChatGPT,</b> as well as an <b>LLM</b> that has been <b>fine-tuned</b> using ConDID, but which does not use affective features. This project will be released on <a href=https://github.com/lzw108/ConspEmoLLM/>https://github.com/lzw108/ConspEmoLLM/</a>.</p></p class="citation"></blockquote><h3 id=1839--18318-ac-eval-evaluating-ancient-chinese-language-understanding-in-large-language-models-yuting-wei-et-al-2024>(18/39 | 18/318) AC-EVAL: Evaluating Ancient Chinese Language Understanding in Large Language Models (Yuting Wei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuting Wei, Yuanxing Xu, Xinru Wei, Simin Yang, Yangfu Zhu, Yuqing Li, Di Liu, Bin Wu. (2024)<br><strong>AC-EVAL: Evaluating Ancient Chinese Language Understanding in Large Language Models</strong><br><button class=copy-to-clipboard title="AC-EVAL: Evaluating Ancient Chinese Language Understanding in Large Language Models" index=18>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-18 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Benchmarking, Reasoning, Large Language Model, Large Language Model, Text Understanding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06574v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06574v1.pdf filename=2403.06574v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Given the importance of ancient Chinese in capturing the essence of rich historical and cultural heritage, the rapid advancements in <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> necessitate <b>benchmarks</b> that can effectively evaluate their understanding of ancient contexts. To meet this need, we present AC-EVAL, an innovative <b>benchmark</b> designed to assess the advanced knowledge and <b>reasoning</b> capabilities of <b>LLMs</b> within the context of ancient Chinese. AC-EVAL is structured across three levels of difficulty reflecting different facets of language comprehension: general historical knowledge, short <b>text</b> <b>understanding,</b> and long <b>text</b> <b>comprehension.</b> The <b>benchmark</b> comprises 13 tasks, spanning historical facts, geography, social customs, art, philosophy, classical poetry and prose, providing a comprehensive assessment framework. Our extensive evaluation of top-performing <b>LLMs,</b> tailored for both English and Chinese, reveals a substantial potential for enhancing ancient <b>text</b> <b>comprehension.</b> By highlighting the strengths and weaknesses of <b>LLMs,</b> AC-EVAL aims to promote their development and application forward in the realms of ancient Chinese language education and scholarly research. The AC-EVAL data and evaluation code are available at <a href=https://github.com/yuting-wei/AC-EVAL>https://github.com/yuting-wei/AC-EVAL</a>.</p></p class="citation"></blockquote><h3 id=1939--19318-unsupervised-real-time-hallucination-detection-based-on-the-internal-states-of-large-language-models-weihang-su-et-al-2024>(19/39 | 19/318) Unsupervised Real-Time Hallucination Detection based on the Internal States of Large Language Models (Weihang Su et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Weihang Su, Changyue Wang, Qingyao Ai, Yiran HU, Zhijing Wu, Yujia Zhou, Yiqun Liu. (2024)<br><strong>Unsupervised Real-Time Hallucination Detection based on the Internal States of Large Language Models</strong><br><button class=copy-to-clipboard title="Unsupervised Real-Time Hallucination Detection based on the Internal States of Large Language Models" index=19>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-19 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Benchmarking, Unsupervised Learning, Hallucination Detection, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06448v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06448v1.pdf filename=2403.06448v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Hallucinations</b> <b>in</b> <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> refer to the phenomenon of <b>LLMs</b> producing responses that are coherent yet factually inaccurate. This issue undermines the effectiveness of <b>LLMs</b> in practical applications, necessitating research into detecting and mitigating <b>hallucinations</b> <b>of</b> <b>LLMs.</b> Previous studies have mainly concentrated on post-processing techniques for <b>hallucination</b> <b>detection,</b> which tend to be computationally intensive and limited in effectiveness due to their separation from the <b>LLM&rsquo;s</b> inference process. To overcome these limitations, we introduce MIND, an <b>unsupervised</b> training framework that leverages the internal states of <b>LLMs</b> for real-time <b>hallucination</b> <b>detection</b> without requiring manual annotations. Additionally, we present HELM, a new <b>benchmark</b> for evaluating <b>hallucination</b> <b>detection</b> across multiple <b>LLMs,</b> featuring diverse <b>LLM</b> outputs and the internal states of <b>LLMs</b> during their inference process. Our experiments demonstrate that MIND outperforms existing state-of-the-art methods in <b>hallucination</b> <b>detection.</b></p></p class="citation"></blockquote><h3 id=2039--20318-click-a-benchmark-dataset-of-cultural-and-linguistic-intelligence-in-korean-eunsu-kim-et-al-2024>(20/39 | 20/318) CLIcK: A Benchmark Dataset of Cultural and Linguistic Intelligence in Korean (Eunsu Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Eunsu Kim, Juyoung Suk, Philhoon Oh, Haneul Yoo, James Thorne, Alice Oh. (2024)<br><strong>CLIcK: A Benchmark Dataset of Cultural and Linguistic Intelligence in Korean</strong><br><button class=copy-to-clipboard title="CLIcK: A Benchmark Dataset of Cultural and Linguistic Intelligence in Korean" index=20>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-20 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Benchmarking, Hate Speech Detection, Question Answering, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06412v3 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06412v3.pdf filename=2403.06412v3.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite the rapid development of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> for the Korean language, there remains an obvious lack of <b>benchmark</b> datasets that test the requisite Korean cultural and linguistic knowledge. Because many existing Korean <b>benchmark</b> datasets are derived from the English counterparts through translation, they often overlook the different cultural contexts. For the few <b>benchmark</b> datasets that are sourced from Korean data capturing cultural knowledge, only narrow tasks such as bias and <b>hate</b> <b>speech</b> <b>detection</b> are offered. To address this gap, we introduce a <b>benchmark</b> of Cultural and Linguistic Intelligence in Korean (CLIcK), a dataset comprising 1,995 <b>QA</b> pairs. CLIcK sources its data from official Korean exams and textbooks, partitioning the questions into eleven categories under the two main categories of language and culture. For each instance in CLIcK, we provide fine-grained annotation of which cultural and linguistic knowledge is required to answer the question correctly. Using CLIcK, we test 13 language models to assess their performance. Our evaluation uncovers insights into their performances across the categories, as well as the diverse factors affecting their comprehension. CLIcK offers the first <b>large-scale</b> <b>comprehensive</b> <b>Korean-centric</b> analysis of <b>LLMs&rsquo;</b> proficiency in Korean culture and language.</p></p class="citation"></blockquote><h3 id=2139--21318-lstm-based-text-generation-a-study-on-historical-datasets-mustafa-abbas-hussein-hussein-et-al-2024>(21/39 | 21/318) LSTM-Based Text Generation: A Study on Historical Datasets (Mustafa Abbas Hussein Hussein et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mustafa Abbas Hussein Hussein, Serkan Savaş. (2024)<br><strong>LSTM-Based Text Generation: A Study on Historical Datasets</strong><br><button class=copy-to-clipboard title="LSTM-Based Text Generation: A Study on Historical Datasets" index=21>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-21 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: LSTM, LSTM, LSTM, Text Generation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07087v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07087v1.pdf filename=2403.07087v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents an exploration of <b>Long</b> <b>Short-Term</b> <b>Memory</b> <b>(LSTM)</b> networks in the realm of <b>text</b> <b>generation,</b> focusing on the utilization of historical datasets for Shakespeare and Nietzsche. <b>LSTMs,</b> known for their effectiveness in handling sequential data, are applied here to model complex language patterns and structures inherent in historical <b>texts.</b> <b>The</b> study demonstrates that <b>LSTM-based</b> models, when trained on historical datasets, can not only generate <b>text</b> <b>that</b> is linguistically rich and contextually relevant but also provide insights into the evolution of language patterns over time. The finding presents models that are highly accurate and efficient in predicting <b>text</b> <b>from</b> works of Nietzsche, with low loss values and a training time of 100 iterations. The accuracy of the model is 0.9521, indicating high accuracy. The loss of the model is 0.2518, indicating its effectiveness. The accuracy of the model in predicting <b>text</b> <b>from</b> the work of Shakespeare is 0.9125, indicating a low error rate. The training time of the model is 100, mirroring the efficiency of the Nietzsche dataset. This efficiency demonstrates the effectiveness of the model design and training methodology, especially when handling complex literary <b>texts.</b> <b>This</b> research contributes to the field of natural language processing by showcasing the versatility of <b>LSTM</b> networks in <b>text</b> <b>generation</b> and offering a pathway for future explorations in historical linguistics and beyond.</p></p class="citation"></blockquote><h3 id=2239--22318-monitoring-ai-modified-content-at-scale-a-case-study-on-the-impact-of-chatgpt-on-ai-conference-peer-reviews-weixin-liang-et-al-2024>(22/39 | 22/318) Monitoring AI-Modified Content at Scale: A Case Study on the Impact of ChatGPT on AI Conference Peer Reviews (Weixin Liang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Weixin Liang, Zachary Izzo, Yaohui Zhang, Haley Lepp, Hancheng Cao, Xuandong Zhao, Lingjiao Chen, Haotian Ye, Sheng Liu, Zhi Huang, Daniel A. McFarland, James Y. Zou. (2024)<br><strong>Monitoring AI-Modified Content at Scale: A Case Study on the Impact of ChatGPT on AI Conference Peer Reviews</strong><br><button class=copy-to-clipboard title="Monitoring AI-Modified Content at Scale: A Case Study on the Impact of ChatGPT on AI Conference Peer Reviews" index=22>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-22 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: I-2-7, cs-AI, cs-CL, cs-LG, cs-SI, cs.CL<br>Keyword Score: 30<br>Keywords: ChatGPT, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07183v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07183v1.pdf filename=2403.07183v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present an approach for estimating the fraction of text in a <b>large</b> <b>corpus</b> <b>which</b> is likely to be substantially modified or produced by a <b>large</b> <b>language</b> <b>model</b> <b>(LLM).</b> Our maximum likelihood model leverages expert-written and AI-generated reference texts to accurately and efficiently examine real-world <b>LLM-use</b> at the corpus level. We apply this approach to a case study of scientific peer review in AI conferences that took place after the release of <b>ChatGPT:</b> ICLR 2024, NeurIPS 2023, CoRL 2023 and EMNLP 2023. Our results suggest that between 6.5% and 16.9% of text submitted as peer reviews to these conferences could have been substantially modified by <b>LLMs,</b> i.e. beyond spell-checking or minor writing updates. The circumstances in which generated text occurs offer insight into user behavior: the estimated fraction of <b>LLM-generated</b> text is higher in reviews which report lower confidence, were submitted close to the deadline, and from reviewers who are less likely to respond to author rebuttals. We also observe corpus-level trends in generated text which may be too subtle to detect at the individual level, and discuss the implications of such trends on peer review. We call for future interdisciplinary work to examine how <b>LLM</b> use is changing our information and knowledge practices.</p></p class="citation"></blockquote><h3 id=2339--23318-spa-towards-a-computational-friendly-cloud-base-and-on-devices-collaboration-seq2seq-personalized-generation-yanming-liu-et-al-2024>(23/39 | 23/318) SPA: Towards A Computational Friendly Cloud-Base and On-Devices Collaboration Seq2seq Personalized Generation (Yanming Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yanming Liu, Xinyue Peng, Jiannan Cao, Le Dai, Xingzu Liu, Weihao Liu, Mingbang Wang. (2024)<br><strong>SPA: Towards A Computational Friendly Cloud-Base and On-Devices Collaboration Seq2seq Personalized Generation</strong><br><button class=copy-to-clipboard title="SPA: Towards A Computational Friendly Cloud-Base and On-Devices Collaboration Seq2seq Personalized Generation" index=23>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-23 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Low-Resource, Question Answering, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07088v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07088v1.pdf filename=2403.07088v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Large language models(LLMs) have shown its outperforming ability on various tasks and <b>question</b> <b>answering.</b> However, <b>LLMs</b> require high computation cost and large memory cost. At the same time, <b>LLMs</b> may cause privacy leakage when training or prediction procedure contains sensitive information. In this paper, we propose SPA(Side Plugin Adaption), a lightweight architecture for fast on-devices inference and privacy retaining on the constraints of strict on-devices computation and memory constraints. Compared with other on-devices seq2seq generation, SPA could make a fast and stable inference on <b>low-resource</b> constraints, allowing it to obtain cost effiency. Our method establish an interaction between a pretrained <b>LLMs</b> on-cloud and additive parameters on-devices, which could provide the knowledge on both pretrained <b>LLMs</b> and private personal feature.Further more, SPA provides a framework to keep feature-base parameters on private guaranteed but low computational devices while leave the parameters containing general information on the high computational devices.</p></p class="citation"></blockquote><h3 id=2439--24318-naming-describing-and-quantifying-visual-objects-in-humans-and-llms-alberto-testoni-et-al-2024>(24/39 | 24/318) Naming, Describing, and Quantifying Visual Objects in Humans and LLMs (Alberto Testoni et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alberto Testoni, Juell Sprott, Sandro Pezzelle. (2024)<br><strong>Naming, Describing, and Quantifying Visual Objects in Humans and LLMs</strong><br><button class=copy-to-clipboard title="Naming, Describing, and Quantifying Visual Objects in Humans and LLMs" index=24>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-24 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06935v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06935v2.pdf filename=2403.06935v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While human speakers use a variety of different expressions when describing the same object in an image, giving rise to a distribution of plausible labels driven by pragmatic constraints, the extent to which current Vision & Language <b>Large</b> <b>Language</b> <b>Models</b> (VLLMs) can mimic this crucial feature of language use is an open question. This applies to common, everyday objects, but it is particularly interesting for uncommon or novel objects for which a category label may be lacking or fuzzy. Furthermore, humans show clear production preferences for highly context-sensitive expressions, such as the quantifiers <code>few' or </code>most&rsquo;. In our work, we evaluate VLLMs (FROMAGe, BLIP-2, LLaVA) on three categories (nouns, attributes, and quantifiers) where humans show great subjective variability concerning the distribution over plausible labels, using datasets and resources mostly under-explored in previous work. Our results reveal mixed evidence on the ability of VLLMs to capture human naming preferences, with all models failing in tasks that require high-level <b>reasoning</b> such as assigning quantifiers.</p></p class="citation"></blockquote><h3 id=2539--25318-indicllmsuite-a-blueprint-for-creating-pre-training-and-fine-tuning-datasets-for-indian-languages-mohammed-safi-ur-rahman-khan-et-al-2024>(25/39 | 25/318) IndicLLMSuite: A Blueprint for Creating Pre-training and Fine-Tuning Datasets for Indian Languages (Mohammed Safi Ur Rahman Khan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohammed Safi Ur Rahman Khan, Priyam Mehta, Ananth Sankar, Umashankar Kumaravelan, Sumanth Doddapaneni, Suriyaprasaad G, Varun Balan G, Sparsh Jain, Anoop Kunchukuttan, Pratyush Kumar, Raj Dabre, Mitesh M. Khapra. (2024)<br><strong>IndicLLMSuite: A Blueprint for Creating Pre-training and Fine-Tuning Datasets for Indian Languages</strong><br><button class=copy-to-clipboard title="IndicLLMSuite: A Blueprint for Creating Pre-training and Fine-Tuning Datasets for Indian Languages" index=25>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-25 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Fine-tuning, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06350v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06350v1.pdf filename=2403.06350v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite the considerable advancements in English <b>LLMs,</b> the progress in building comparable models for other languages has been hindered due to the scarcity of tailored resources. Our work aims to bridge this divide by introducing an expansive suite of resources specifically designed for the development of Indic <b>LLMs,</b> covering 22 languages, containing a total of 251B tokens and 74.8M instruction-response pairs. Recognizing the importance of both data quality and quantity, our approach combines highly curated manually verified data, unverified yet valuable data, and synthetic data. We build a clean, open-source pipeline for curating pre-training data from diverse sources, including websites, PDFs, and videos, incorporating best practices for crawling, cleaning, flagging, and deduplication. For instruction-fine tuning, we amalgamate existing Indic datasets, translate/transliterate English datasets into Indian languages, and utilize LLaMa2 and Mixtral models to create conversations grounded in articles from Indian Wikipedia and Wikihow. Additionally, we address toxicity alignment by generating toxic <b>prompts</b> for multiple scenarios and then generate non-toxic responses by feeding these toxic <b>prompts</b> to an aligned LLaMa2 model. We hope that the datasets, tools, and resources released as a part of this work will not only propel the research and development of Indic <b>LLMs</b> but also establish an open-source blueprint for extending such efforts to other languages. The data and other artifacts created as part of this work are released with permissive licenses.</p></p class="citation"></blockquote><h3 id=2639--26318-medkp-medical-dialogue-with-knowledge-enhancement-and-clinical-pathway-encoding-jiageng-wu-et-al-2024>(26/39 | 26/318) MedKP: Medical Dialogue with Knowledge Enhancement and Clinical Pathway Encoding (Jiageng Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiageng Wu, Xian Wu, Yefeng Zheng, Jie Yang. (2024)<br><strong>MedKP: Medical Dialogue with Knowledge Enhancement and Clinical Pathway Encoding</strong><br><button class=copy-to-clipboard title="MedKP: Medical Dialogue with Knowledge Enhancement and Clinical Pathway Encoding" index=26>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-26 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 28<br>Keywords: Graph, Knowledge Graph, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06611v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06611v1.pdf filename=2403.06611v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With appropriate data selection and training techniques, <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have demonstrated exceptional success in various medical examinations and multiple-choice questions. However, the application of <b>LLMs</b> in medical dialogue generation-a task more closely aligned with actual medical practice-has been less explored. This gap is attributed to the insufficient medical <b>knowledge</b> <b>of</b> <b>LLMs,</b> which leads to inaccuracies and hallucinated information in the generated medical responses. In this work, we introduce the Medical dialogue with <b>Knowledge</b> <b>enhancement</b> and clinical Pathway encoding (MedKP) framework, which integrates an external <b>knowledge</b> <b>enhancement</b> module through a medical <b>knowledge</b> <b>graph</b> and an internal clinical pathway encoding via medical entities and physician actions. Evaluated with comprehensive metrics, our experiments on two <b>large-scale,</b> <b>real-world</b> <b>online</b> medical consultation datasets (MedDG and KaMed) demonstrate that MedKP surpasses multiple baselines and mitigates the incidence of hallucinations, achieving a new state-of-the-art. Extensive ablation studies further reveal the effectiveness of each component of MedKP. This enhancement advances the development of reliable, automated medical consultation responses using <b>LLMs,</b> thereby broadening the potential accessibility of precise and real-time medical assistance.</p></p class="citation"></blockquote><h3 id=2739--27318-linguistic-structure-induction-from-language-models-omar-momen-2024>(27/39 | 27/318) Linguistic Structure Induction from Language Models (Omar Momen, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Omar Momen. (2024)<br><strong>Linguistic Structure Induction from Language Models</strong><br><button class=copy-to-clipboard title="Linguistic Structure Induction from Language Models" index=27>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-27 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 26<br>Keywords: Benchmarking, Benchmarking, Unsupervised Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09714v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09714v1.pdf filename=2403.09714v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Linear sequences of words are implicitly represented in our brains by hierarchical structures that organize the composition of words in sentences. Linguists formalize different frameworks to model this hierarchy; two of the most common syntactic frameworks are Constituency and Dependency. Constituency represents sentences as nested groups of phrases, while dependency represents a sentence by assigning relations between its words. Recently, the pursuit of intelligent machines has produced Language Models (LMs) capable of solving many language tasks with a human-level performance. Many studies now question whether LMs implicitly represent syntactic hierarchies. This thesis focuses on producing constituency and dependency structures from LMs in an <b>unsupervised</b> setting. I review the critical methods in this field and highlight a line of work that utilizes a numerical representation for binary constituency trees (Syntactic Distance). I present a detailed study on StructFormer (SF) (Shen et al., 2021), which retrofits a <b>transformer</b> encoder architecture with a parser network to produce constituency and dependency structures. I present six experiments to analyze and address this field&rsquo;s challenges; experiments include investigating the effect of repositioning the parser network within the SF architecture, evaluating subword-based induced trees, and <b>benchmarking</b> the models developed in the thesis experiments on linguistic tasks. Models <b>benchmarking</b> is performed by participating in the BabyLM challenge, published at CoNLL 2023 (Momen et al., 2023). The results of this thesis encourage further development in the direction of retrofitting <b>transformer-based</b> models to induce syntactic structures, supported by the acceptable performance of SF in different experimental settings and the observed limitations that require innovative solutions to advance the state of syntactic structure induction.</p></p class="citation"></blockquote><h3 id=2839--28318-restoring-ancient-ideograph-a-multimodal-multitask-neural-network-approach-siyu-duan-et-al-2024>(28/39 | 28/318) Restoring Ancient Ideograph: A Multimodal Multitask Neural Network Approach (Siyu Duan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Siyu Duan, Jun Wang, Qi Su. (2024)<br><strong>Restoring Ancient Ideograph: A Multimodal Multitask Neural Network Approach</strong><br><button class=copy-to-clipboard title="Restoring Ancient Ideograph: A Multimodal Multitask Neural Network Approach" index=28>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-28 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CV, cs-CY, cs.CL<br>Keyword Score: 26<br>Keywords: Multi-modal, Multi-modal, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06682v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06682v1.pdf filename=2403.06682v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Cultural heritage serves as the enduring record of human thought and history. Despite significant efforts dedicated to the preservation of cultural relics, many ancient artefacts have been ravaged irreversibly by natural deterioration and human actions. Deep learning technology has emerged as a valuable tool for restoring various kinds of cultural heritages, including ancient text restoration. Previous research has approached ancient text restoration from either visual or textual perspectives, often overlooking the potential of synergizing <b>multimodal</b> information. This paper proposes a novel <b>Multimodal</b> Multitask Restoring Model (MMRM) to restore ancient texts, particularly emphasising the ideograph. This model combines context understanding with residual visual information from damaged ancient artefacts, enabling it to predict damaged characters and generate restored images simultaneously. We tested the MMRM model through experiments conducted on both simulated datasets and authentic ancient inscriptions. The results show that the proposed method gives insightful restoration suggestions in both <b>simulation</b> experiments and real-world scenarios. To the best of our knowledge, this work represents the pioneering application of <b>multimodal</b> deep learning in ancient text restoration, which will contribute to the understanding of ancient society and culture in digital humanities fields.</p></p class="citation"></blockquote><h3 id=2939--29318-thought-graph-generating-thought-process-for-biological-reasoning-chi-yang-hsu-et-al-2024>(29/39 | 29/318) Thought Graph: Generating Thought Process for Biological Reasoning (Chi-Yang Hsu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chi-Yang Hsu, Kyle Cox, Jiawei Xu, Zhen Tan, Tianhua Zhai, Mengzhou Hu, Dexter Pratt, Tianlong Chen, Ziniu Hu, Ying Ding. (2024)<br><strong>Thought Graph: Generating Thought Process for Biological Reasoning</strong><br><button class=copy-to-clipboard title="Thought Graph: Generating Thought Process for Biological Reasoning" index=29>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-29 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 23<br>Keywords: Graph, Reasoning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07144v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07144v1.pdf filename=2403.07144v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present the Thought <b>Graph</b> as a novel framework to support complex <b>reasoning</b> and use gene set analysis as an example to uncover semantic relationships between biological processes. Our framework stands out for its ability to provide a deeper understanding of gene sets, significantly surpassing GSEA by 40.28% and <b>LLM</b> baselines by 5.38% based on cosine similarity to human annotations. Our analysis further provides insights into future directions of biological processes naming, and implications for bioinformatics and precision medicine.</p></p class="citation"></blockquote><h3 id=3039--30318-multi-modal-semantic-understanding-with-contrastive-cross-modal-feature-alignment-ming-zhang-et-al-2024>(30/39 | 30/318) Multi-modal Semantic Understanding with Contrastive Cross-modal Feature Alignment (Ming Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ming Zhang, Ke Chang, Yunfang Wu. (2024)<br><strong>Multi-modal Semantic Understanding with Contrastive Cross-modal Feature Alignment</strong><br><button class=copy-to-clipboard title="Multi-modal Semantic Understanding with Contrastive Cross-modal Feature Alignment" index=30>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-30 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CV, cs.CL<br>Keyword Score: 23<br>Keywords: Contrastive Learning, Multi-modal, Sentiment Analysis<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06355v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06355v1.pdf filename=2403.06355v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multi-modal</b> semantic understanding requires integrating information from different modalities to extract users&rsquo; real intention behind words. Most previous work applies a dual-encoder structure to separately encode image and text, but fails to learn cross-modal feature alignment, making it hard to achieve cross-modal deep information interaction. This paper proposes a novel CLIP-guided <b>contrastive-learning-based</b> <b>architecture</b> to perform <b>multi-modal</b> feature alignment, which projects the features derived from different modalities into a unified deep space. On <b>multi-modal</b> sarcasm detection (MMSD) and <b>multi-modal</b> <b>sentiment</b> <b>analysis</b> (MMSA) tasks, the experimental results show that our proposed model significantly outperforms several baselines, and our feature alignment strategy brings obvious performance gain over models with different aggregating methods and models even enriched with knowledge. More importantly, our model is simple to implement without using task-specific external knowledge, and thus can easily migrate to other <b>multi-modal</b> tasks. Our source codes are available at <a href=https://github.com/ChangKe123/CLFA>https://github.com/ChangKe123/CLFA</a>.</p></p class="citation"></blockquote><h3 id=3139--31318-academically-intelligent-llms-are-not-necessarily-socially-intelligent-ruoxi-xu-et-al-2024>(31/39 | 31/318) Academically intelligent LLMs are not necessarily socially intelligent (Ruoxi Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruoxi Xu, Hongyu Lin, Xianpei Han, Le Sun, Yingfei Sun. (2024)<br><strong>Academically intelligent LLMs are not necessarily socially intelligent</strong><br><button class=copy-to-clipboard title="Academically intelligent LLMs are not necessarily socially intelligent" index=31>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-31 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CY, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06591v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06591v1.pdf filename=2403.06591v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The academic intelligence of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> has made remarkable progress in recent times, but their social intelligence performance remains unclear. Inspired by established human social intelligence frameworks, particularly Daniel Goleman&rsquo;s social intelligence theory, we have developed a standardized social intelligence test based on real-world social scenarios to comprehensively assess the social intelligence of <b>LLMs,</b> termed as the Situational Evaluation of Social Intelligence (SESI). We conducted an extensive evaluation with 13 recent popular and state-of-art <b>LLM</b> agents on SESI. The results indicate the social intelligence of <b>LLMs</b> still has significant room for improvement, with superficially friendliness as a primary reason for errors. Moreover, there exists a relatively low correlation between the social intelligence and academic intelligence exhibited by <b>LLMs,</b> suggesting that social intelligence is distinct from academic intelligence for <b>LLMs.</b> Additionally, while it is observed that <b>LLMs</b> can&rsquo;t ``understand&rsquo;&rsquo; what social intelligence is, their social intelligence, similar to that of humans, is influenced by social factors.</p></p class="citation"></blockquote><h3 id=3239--32318-improving-speaker-assignment-in-speaker-attributed-asr-for-real-meeting-applications-can-cui-et-al-2024>(32/39 | 32/318) Improving Speaker Assignment in Speaker-Attributed ASR for Real Meeting Applications (Can Cui et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Can Cui, Imran Ahamad Sheikh, Mostafa Sadeghi, Emmanuel Vincent. (2024)<br><strong>Improving Speaker Assignment in Speaker-Attributed ASR for Real Meeting Applications</strong><br><button class=copy-to-clipboard title="Improving Speaker Assignment in Speaker-Attributed ASR for Real Meeting Applications" index=32>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-32 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Fine-tuning, Automatic Speech Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06570v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06570v1.pdf filename=2403.06570v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Past studies on end-to-end meeting transcription have focused on model architecture and have mostly been evaluated on simulated meeting data. We present a novel study aiming to optimize the use of a Speaker-Attributed <b>ASR</b> (SA-ASR) system in real-life scenarios, such as the AMI meeting corpus, for improved speaker assignment of speech segments. First, we propose a pipeline tailored to real-life applications involving Voice Activity Detection (VAD), Speaker Diarization (SD), and SA-ASR. Second, we advocate using VAD output segments to <b>fine-tune</b> the SA-ASR model, considering that it is also applied to VAD segments during test, and show that this results in a relative reduction of Speaker Error Rate (SER) up to 28%. Finally, we explore strategies to enhance the extraction of the speaker embedding templates used as inputs by the SA-ASR system. We show that extracting them from SD output rather than annotated speaker segments results in a relative SER reduction up to 20%.</p></p class="citation"></blockquote><h3 id=3339--33318-glosslm-multilingual-pretraining-for-low-resource-interlinear-glossing-michael-ginn-et-al-2024>(33/39 | 33/318) GlossLM: Multilingual Pretraining for Low-Resource Interlinear Glossing (Michael Ginn et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michael Ginn, Lindia Tjuatja, Taiqi He, Enora Rice, Graham Neubig, Alexis Palmer, Lori Levin. (2024)<br><strong>GlossLM: Multilingual Pretraining for Low-Resource Interlinear Glossing</strong><br><button class=copy-to-clipboard title="GlossLM: Multilingual Pretraining for Low-Resource Interlinear Glossing" index=33>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-33 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Fine-tuning, Low-Resource<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06399v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06399v1.pdf filename=2403.06399v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A key aspect of language documentation is the creation of annotated text in a format such as interlinear glossed text (IGT), which captures fine-grained morphosyntactic analyses in a morpheme-by-morpheme format. Prior work has explored methods to automatically generate IGT in order to reduce the time cost of language analysis. However, many languages (particularly those requiring preservation) lack sufficient IGT data to train effective models, and crosslingual transfer has been proposed as a method to overcome this limitation. We compile the largest existing corpus of IGT data from a variety of sources, covering over 450k examples across 1.8k languages, to enable research on crosslingual transfer and IGT generation. Then, we pretrain a large multilingual model on a portion of this corpus, and further <b>finetune</b> it to specific languages. Our model is competitive with state-of-the-art methods for segmented data and large monolingual datasets. Meanwhile, our model outperforms SOTA models on unsegmented text and small corpora by up to 6.6% morpheme accuracy, demonstrating the effectiveness of crosslingual transfer for <b>low-resource</b> languages.</p></p class="citation"></blockquote><h3 id=3439--34318-cuentosie-can-a-chatbot-about-tales-with-a-message-help-to-teach-emotional-intelligence-antonio-ferrández-et-al-2024>(34/39 | 34/318) CuentosIE: can a chatbot about &rsquo;tales with a message&rsquo; help to teach emotional intelligence? (Antonio Ferrández et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Antonio Ferrández, Rocío Lavigne-Cerván, Jesús Peral, Ignasi Navarro-Soria, Ángel Lloret, David Gil, Carmen Rocamora. (2024)<br><strong>CuentosIE: can a chatbot about &rsquo;tales with a message&rsquo; help to teach emotional intelligence?</strong><br><button class=copy-to-clipboard title="CuentosIE: can a chatbot about 'tales with a message' help to teach emotional intelligence?" index=34>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-34 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: I-2-0, cs-AI, cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Chatbot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07193v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07193v1.pdf filename=2403.07193v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this article, we present CuentosIE (TalesEI: <b>chatbot</b> of tales with a message to develop Emotional Intelligence), an educational <b>chatbot</b> on emotions that also provides teachers and psychologists with a tool to monitor their students/patients through indicators and data compiled by CuentosIE. The use of &ldquo;tales with a message&rdquo; is justified by their simplicity and easy understanding, thanks to their moral or associated metaphors. The main contributions of CuentosIE are the selection, collection, and classification of a set of highly specialized tales, as well as the provision of tools (searching, reading comprehension, chatting, recommending, and classifying) that are useful for both educating users about emotions and monitoring their emotional development. The preliminary evaluation of the tool has obtained encouraging results, which provides an affirmative answer to the question posed in the title of the article.</p></p class="citation"></blockquote><h3 id=3539--35318-the-pitfalls-of-next-token-prediction-gregor-bachmann-et-al-2024>(35/39 | 35/318) The pitfalls of next-token prediction (Gregor Bachmann et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gregor Bachmann, Vaishnavh Nagarajan. (2024)<br><strong>The pitfalls of next-token prediction</strong><br><button class=copy-to-clipboard title="The pitfalls of next-token prediction" index=35>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-35 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06963v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06963v1.pdf filename=2403.06963v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Can a mere next-token predictor faithfully model human intelligence? We crystallize this intuitive concern, which is fragmented in the literature. As a starting point, we argue that the two often-conflated phases of next-token prediction &ndash; autoregressive inference and teacher-forced training &ndash; must be treated distinctly. The popular criticism that errors can compound during autoregressive inference, crucially assumes that teacher-forcing has learned an accurate next-token predictor. This assumption sidesteps a more deep-rooted problem we expose: in certain classes of tasks, teacher-forcing can simply fail to learn an accurate next-token predictor in the first place. We describe a general mechanism of how teacher-forcing can fail, and design a minimal planning task where both the <b>Transformer</b> and the Mamba architecture empirically fail in that manner &ndash; remarkably, despite the task being straightforward to learn. We provide preliminary evidence that this failure can be resolved when training to predict multiple tokens in advance. We hope this finding can ground future debates and inspire explorations beyond the next-token prediction paradigm. We make our code available under <a href=https://github.com/gregorbachmann/Next-Token-Failures>https://github.com/gregorbachmann/Next-Token-Failures</a></p></p class="citation"></blockquote><h3 id=3639--36318-on-the-consideration-of-ai-openness-can-good-intent-be-abused-yeeun-kim-et-al-2024>(36/39 | 36/318) On the Consideration of AI Openness: Can Good Intent Be Abused? (Yeeun Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yeeun Kim, Eunkyung Choi, Hyunjun Kim, Hongseok Oh, Hyunseo Shin, Wonseok Hwang. (2024)<br><strong>On the Consideration of AI Openness: Can Good Intent Be Abused?</strong><br><button class=copy-to-clipboard title="On the Consideration of AI Openness: Can Good Intent Be Abused?" index=36>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-36 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06537v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06537v1.pdf filename=2403.06537v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Openness is critical for the advancement of science. In particular, recent rapid progress in AI has been made possible only by various open-source models, datasets, and libraries. However, this openness also means that technologies can be freely used for socially harmful purposes. Can open-source models or datasets be used for malicious purposes? If so, how easy is it to adapt technology for such goals? Here, we conduct a case study in the legal domain, a realm where individual decisions can have profound social consequences. To this end, we build EVE, a dataset consisting of 200 examples of questions and corresponding answers about criminal activities based on 200 Korean precedents. We found that a widely accepted open-source <b>LLM,</b> which initially refuses to answer unethical questions, can be easily tuned with EVE to provide unethical and informative answers about criminal activities. This implies that although open-source technologies contribute to scientific progress, some care must be taken to mitigate possible malicious use cases. Warning: This paper contains contents that some may find unethical.</p></p class="citation"></blockquote><h3 id=3739--37318-multilingual-turn-taking-prediction-using-voice-activity-projection-koji-inoue-et-al-2024>(37/39 | 37/318) Multilingual Turn-taking Prediction Using Voice Activity Projection (Koji Inoue et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Koji Inoue, Bing&rsquo;er Jiang, Erik Ekstedt, Tatsuya Kawahara, Gabriel Skantze. (2024)<br><strong>Multilingual Turn-taking Prediction Using Voice Activity Projection</strong><br><button class=copy-to-clipboard title="Multilingual Turn-taking Prediction Using Voice Activity Projection" index=37>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-37 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-SD, cs.CL, eess-AS<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06487v3 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06487v3.pdf filename=2403.06487v3.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper investigates the application of voice activity projection (VAP), a predictive turn-taking model for spoken dialogue, on multilingual data, encompassing English, Mandarin, and Japanese. The VAP model continuously predicts the upcoming voice activities of participants in dyadic dialogue, leveraging a cross-attention <b>Transformer</b> to capture the dynamic interplay between participants. The results show that a monolingual VAP model trained on one language does not make good predictions when applied to other languages. However, a multilingual model, trained on all three languages, demonstrates predictive performance on par with monolingual models across all languages. Further analyses show that the multilingual model has learned to discern the language of the input signal. We also analyze the sensitivity to pitch, a prosodic cue that is thought to be important for turn-taking. Finally, we compare two different audio encoders, contrastive predictive coding (CPC) pre-trained on English, with a recent model based on multilingual wav2vec 2.0 (MMS).</p></p class="citation"></blockquote><h3 id=3839--38318-a-logical-pattern-memory-pre-trained-model-for-entailment-tree-generation-li-yuan-et-al-2024>(38/39 | 38/318) A Logical Pattern Memory Pre-trained Model for Entailment Tree Generation (Li Yuan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Li Yuan, Yi Cai, Haopeng Ren, Jiexin Wang. (2024)<br><strong>A Logical Pattern Memory Pre-trained Model for Entailment Tree Generation</strong><br><button class=copy-to-clipboard title="A Logical Pattern Memory Pre-trained Model for Entailment Tree Generation" index=38>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-38 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06410v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06410v1.pdf filename=2403.06410v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Generating coherent and credible explanations remains a significant challenge in the field of AI. In recent years, researchers have delved into the utilization of entailment trees to depict explanations, which exhibit a <b>reasoning</b> process of how a hypothesis is deduced from the supporting facts. However, existing models often overlook the importance of generating intermediate conclusions with logical consistency from the given facts, leading to inaccurate conclusions and undermining the overall credibility of entailment trees. To address this limitation, we propose the logical pattern memory pre-trained model (LMPM). LMPM incorporates an external memory structure to learn and store the latent representations of logical patterns, which aids in generating logically consistent conclusions. Furthermore, to mitigate the influence of logically irrelevant domain knowledge in the Wikipedia-based data, we introduce an entity abstraction approach to construct the dataset for pre-training LMPM. The experimental results highlight the effectiveness of our approach in improving the quality of entailment tree generation. By leveraging logical entailment patterns, our model produces more coherent and reasonable conclusions that closely align with the underlying premises. Code and Data are released at <a href=https://github.com/YuanLi95/T5-LMPM>https://github.com/YuanLi95/T5-LMPM</a></p></p class="citation"></blockquote><h3 id=3939--39318-strength-lies-in-differences-towards-effective-non-collaborative-dialogues-via-tailored-strategy-planning-tong-zhang-et-al-2024>(39/39 | 39/318) Strength Lies in Differences! Towards Effective Non-collaborative Dialogues via Tailored Strategy Planning (Tong Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tong Zhang, Chen Huang, Yang Deng, Hongru Liang, Jia Liu, Zujie Wen, Wenqiang Lei, Tat-Seng Chua. (2024)<br><strong>Strength Lies in Differences! Towards Effective Non-collaborative Dialogues via Tailored Strategy Planning</strong><br><button class=copy-to-clipboard title="Strength Lies in Differences! Towards Effective Non-collaborative Dialogues via Tailored Strategy Planning" index=39>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-39 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06769v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06769v1.pdf filename=2403.06769v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We investigate non-collaborative dialogue agents that must engage in tailored strategic planning for diverse users to secure a favorable agreement. This poses challenges for existing dialogue agents due to two main reasons: their inability to integrate user-specific characteristics into their strategic planning and their training paradigm&rsquo;s failure to produce strategic planners that can generalize to diverse users. To address these challenges, we propose TRIP to enhance the capability in tailored strategic planning, incorporating a user-aware strategic planning module and a population-based training paradigm. Through experiments on <b>benchmark</b> non-collaborative dialogue tasks, we demonstrate the effectiveness of TRIP in catering to diverse users.</p></p class="citation"></blockquote><h2 id=cscv-97>cs.CV (97)</h2><h3 id=197--40318-cam-back-again-large-kernel-cnns-from-a-weakly-supervised-object-localization-perspective-shunsuke-yasuki-et-al-2024>(1/97 | 40/318) CAM Back Again: Large Kernel CNNs from a Weakly Supervised Object Localization Perspective (Shunsuke Yasuki et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shunsuke Yasuki, Masato Taki. (2024)<br><strong>CAM Back Again: Large Kernel CNNs from a Weakly Supervised Object Localization Perspective</strong><br><button class=copy-to-clipboard title="CAM Back Again: Large Kernel CNNs from a Weakly Supervised Object Localization Perspective" index=40>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-40 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 90<br>Keywords: Vision Transformer, Convolution, Convolutional Neural Network, Convolutional Neural Network, Data Augmentation, Supervised Learning, Weakly-supervised Learning, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06676v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06676v1.pdf filename=2403.06676v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, <b>convolutional</b> <b>neural</b> <b>networks</b> <b>(CNNs)</b> with large size kernels have attracted much attention in the computer <b>vision</b> <b>field,</b> following the success of the <b>Vision</b> <b>Transformers.</b> Large kernel <b>CNNs</b> have been reported to perform well in downstream <b>vision</b> <b>tasks</b> as well as in classification performance. The reason for the high-performance of large kernel <b>CNNs</b> in downstream tasks has been attributed to the large effective receptive field (ERF) produced by large size kernels, but this view has not been fully tested. We therefore revisit the performance of large kernel <b>CNNs</b> in downstream task, focusing on the weakly <b>supervised</b> object localization (WSOL) task. WSOL, a difficult downstream task that is not fully <b>supervised,</b> provides a new angle to explore the capabilities of the large kernel <b>CNNs.</b> Our study compares the modern large kernel <b>CNNs</b> ConvNeXt, RepLKNet, and SLaK to test the validity of the naive expectation that ERF size is important for improving downstream task performance. Our analysis of the factors contributing to high performance provides a different perspective, in which the main factor is feature map improvement. Furthermore, we find that modern <b>CNNs</b> are robust to the CAM problems of local regions of objects being activated, which has long been discussed in WSOL. CAM is the most classic WSOL method, but because of the above-mentioned problems, it is often used as a baseline method for comparison. However, experiments on the CUB-200-2011 dataset show that simply combining a large kernel <b>CNN,</b> CAM, and simple <b>data</b> <b>augmentation</b> methods can achieve performance (90.99% MaxBoxAcc) comparable to the latest WSOL method, which is <b>CNN-based</b> and requires special training or complex post-processing. The code is available at <a href=https://github.com/snskysk/CAM-Back-Again>https://github.com/snskysk/CAM-Back-Again</a>.</p></p class="citation"></blockquote><h3 id=297--41318-selma-learning-and-merging-skill-specific-text-to-image-experts-with-auto-generated-data-jialu-li-et-al-2024>(2/97 | 41/318) SELMA: Learning and Merging Skill-Specific Text-to-Image Experts with Auto-Generated Data (Jialu Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jialu Li, Jaemin Cho, Yi-Lin Sung, Jaehong Yoon, Mohit Bansal. (2024)<br><strong>SELMA: Learning and Merging Skill-Specific Text-to-Image Experts with Auto-Generated Data</strong><br><button class=copy-to-clipboard title="SELMA: Learning and Merging Skill-Specific Text-to-Image Experts with Auto-Generated Data" index=41>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-41 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CL, cs-CV, cs-LG, cs.CV<br>Keyword Score: 83<br>Keywords: Diffusion Model, Benchmarking, Fine-tuning, Image2text, Text2image, In-context Learning, In-context Learning, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06952v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06952v1.pdf filename=2403.06952v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent <b>text-to-image</b> (T2I) generation models have demonstrated impressive capabilities in creating images from text descriptions. However, these T2I generation models often fall short of generating images that precisely match the details of the text inputs, such as incorrect spatial relationship or missing objects. In this paper, we introduce SELMA: Skill-Specific Expert Learning and Merging with Auto-Generated Data, a novel paradigm to improve the faithfulness of T2I models by <b>fine-tuning</b> models on automatically generated, multi-skill <b>image-text</b> datasets, with skill-specific expert learning and merging. First, SELMA leverages an <b>LLM&rsquo;s</b> <b>in-context</b> <b>learning</b> capability to generate multiple datasets of text <b>prompts</b> that can teach different skills, and then generates the images with a T2I model based on the <b>prompts.</b> Next, SELMA adapts the T2I model to the new skills by learning multiple single-skill LoRA (low-rank adaptation) experts followed by expert merging. Our independent expert <b>fine-tuning</b> specializes multiple models for different skills, and expert merging helps build a joint multi-skill T2I model that can generate faithful images given diverse text <b>prompts,</b> while mitigating the knowledge conflict from different datasets. We empirically demonstrate that SELMA significantly improves the semantic alignment and text faithfulness of state-of-the-art T2I <b>diffusion</b> <b>models</b> on multiple <b>benchmarks</b> (+2.1% on TIFA and +6.9% on DSG), human preference metrics (PickScore, ImageReward, and HPS), as well as human evaluation. Moreover, <b>fine-tuning</b> with <b>image-text</b> pairs auto-collected via SELMA shows comparable performance to <b>fine-tuning</b> with ground truth data. Lastly, we show that <b>fine-tuning</b> with images from a weaker T2I model can help improve the generation quality of a stronger T2I model, suggesting promising weak-to-strong generalization in T2I models.</p></p class="citation"></blockquote><h3 id=397--42318-one-category-one-prompt-dataset-distillation-using-diffusion-models-ali-abbasi-et-al-2024>(3/97 | 42/318) One Category One Prompt: Dataset Distillation using Diffusion Models (Ali Abbasi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ali Abbasi, Ashkan Shahbazi, Hamed Pirsiavash, Soheil Kolouri. (2024)<br><strong>One Category One Prompt: Dataset Distillation using Diffusion Models</strong><br><button class=copy-to-clipboard title="One Category One Prompt: Dataset Distillation using Diffusion Models" index=42>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-42 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CL, cs-CV, cs-LG, cs.CV<br>Keyword Score: 73<br>Keywords: Diffusion Model, Benchmarking, Fine-tuning, Foundation Model, Knowledge Distillation, Knowledge Distillation, Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07142v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07142v1.pdf filename=2403.07142v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The extensive amounts of data required for training deep neural networks pose significant challenges on storage and transmission fronts. Dataset <b>distillation</b> has emerged as a promising technique to condense the information of massive datasets into a much smaller yet representative set of synthetic samples. However, traditional dataset <b>distillation</b> approaches often struggle to scale effectively with high-resolution images and more complex architectures due to the limitations in bi-level optimization. Recently, several works have proposed exploiting <b>knowledge</b> <b>distillation</b> with decoupled optimization schemes to scale up dataset <b>distillation.</b> Although these methods effectively address the scalability issue, they rely on extensive image augmentations requiring the storage of soft labels for augmented images. In this paper, we introduce Dataset <b>Distillation</b> using <b>Diffusion</b> <b>Models</b> (D3M) as a novel paradigm for dataset <b>distillation,</b> leveraging recent advancements in generative <b>text-to-image</b> <b>foundation</b> <b>models.</b> Our approach utilizes textual inversion, a technique for <b>fine-tuning</b> <b>text-to-image</b> generative models, to create concise and informative representations for large datasets. By employing these learned text <b>prompts,</b> we can efficiently store and infer new samples for introducing data variability within a fixed memory budget. We show the effectiveness of our method through extensive experiments across various computer vision <b>benchmark</b> datasets with different memory budgets.</p></p class="citation"></blockquote><h3 id=497--43318-can-llms-tuning-methods-work-in-medical-multimodal-domain-jiawei-chen-et-al-2024>(4/97 | 43/318) Can LLMs&rsquo; Tuning Methods Work in Medical Multimodal Domain? (Jiawei Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiawei Chen, Yue Jiang, Dingkang Yang, Mingcheng Li, Jinjie Wei, Ziyun Qian, Lihua Zhang. (2024)<br><strong>Can LLMs&rsquo; Tuning Methods Work in Medical Multimodal Domain?</strong><br><button class=copy-to-clipboard title="Can LLMs' Tuning Methods Work in Medical Multimodal Domain?" index=43>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-43 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 66<br>Keywords: Fine-tuning, Fine-tuning, Multi-modal, Multi-modal, Transfer Learning, Large Language Model, Large Language Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06407v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06407v1.pdf filename=2403.06407v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> excel in world knowledge understanding, adapting them to specific subfields requires precise adjustments. Due to the model&rsquo;s vast scale, traditional global <b>fine-tuning</b> methods for <b>large</b> <b>models</b> <b>can</b> be computationally expensive and impact generalization. To address this challenge, a range of innovative Parameters-Efficient <b>Fine-Tuning</b> (PEFT) methods have emerged and achieved remarkable success in both <b>LLMs</b> and <b>Large</b> <b>Vision-Language</b> <b>Models</b> (LVLMs). In the medical domain, <b>fine-tuning</b> a medical <b>Vision-Language</b> Pretrained (VLP) model is essential for adapting it to specific tasks. Can the <b>fine-tuning</b> methods for <b>large</b> <b>models</b> <b>be</b> transferred to the medical field to enhance <b>transfer</b> <b>learning</b> efficiency? In this paper, we delve into the <b>fine-tuning</b> methods of <b>LLMs</b> and conduct extensive experiments to investigate the impact of <b>fine-tuning</b> methods for <b>large</b> <b>models</b> <b>on</b> existing <b>multimodal</b> models in the medical domain from the training data level and the model structure level. We show the different impacts of <b>fine-tuning</b> methods for <b>large</b> <b>models</b> <b>on</b> medical VLMs and develop the most efficient ways to <b>fine-tune</b> medical VLP models. We hope this research can guide medical domain researchers in optimizing VLMs&rsquo; training costs, fostering the broader application of VLMs in healthcare fields. Code and dataset will be released upon acceptance.</p></p class="citation"></blockquote><h3 id=597--44318-leoclr-leveraging-original-images-for-contrastive-learning-of-visual-representations-mohammad-alkhalefi-et-al-2024>(5/97 | 44/318) LeOCLR: Leveraging Original Images for Contrastive Learning of Visual Representations (Mohammad Alkhalefi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohammad Alkhalefi, Georgios Leontidis, Mingjun Zhong. (2024)<br><strong>LeOCLR: Leveraging Original Images for Contrastive Learning of Visual Representations</strong><br><button class=copy-to-clipboard title="LeOCLR: Leveraging Original Images for Contrastive Learning of Visual Representations" index=44>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-44 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 65<br>Keywords: Object Detection, Contrastive Learning, Data Augmentation, Representation Learning, Supervised Learning, Supervised Learning, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06813v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06813v1.pdf filename=2403.06813v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Contrastive</b> <b>instance</b> discrimination outperforms <b>supervised</b> <b>learning</b> in downstream tasks like image classification and <b>object</b> <b>detection.</b> However, this approach heavily relies on <b>data</b> <b>augmentation</b> during <b>representation</b> <b>learning,</b> which may result in inferior results if not properly implemented. Random cropping followed by resizing is a common form of <b>data</b> <b>augmentation</b> used in <b>contrastive</b> <b>learning,</b> but it can lead to degraded <b>representation</b> <b>learning</b> if the two random crops contain distinct semantic content. To address this issue, this paper introduces LeOCLR (Leveraging Original Images for <b>Contrastive</b> <b>Learning</b> of Visual <b>Representations),</b> <b>a</b> framework that employs a new instance discrimination approach and an adapted loss function that ensures the shared region between positive pairs is semantically correct. The experimental results show that our approach consistently improves <b>representation</b> <b>learning</b> across different datasets compared to baseline models. For example, our approach outperforms MoCo-v2 by 5.1% on ImageNet-1K in linear evaluation and several other methods on <b>transfer</b> <b>learning</b> tasks.</p></p class="citation"></blockquote><h3 id=697--45318-enhanced-sparsification-via-stimulative-training-shengji-tang-et-al-2024>(6/97 | 45/318) Enhanced Sparsification via Stimulative Training (Shengji Tang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shengji Tang, Weihao Lin, Hancheng Ye, Peng Ye, Chong Yu, Baopu Li, Tao Chen. (2024)<br><strong>Enhanced Sparsification via Stimulative Training</strong><br><button class=copy-to-clipboard title="Enhanced Sparsification via Stimulative Training" index=45>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-45 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 63<br>Keywords: Benchmarking, Fine-tuning, Knowledge Distillation, Knowledge Distillation, Model Compression, Pruning, Self-Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06417v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06417v1.pdf filename=2403.06417v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Sparsification-based <b>pruning</b> has been an important category in <b>model</b> <b>compression.</b> Existing methods commonly set sparsity-inducing penalty terms to suppress the importance of dropped weights, which is regarded as the suppressed sparsification paradigm. However, this paradigm inactivates the dropped parts of networks causing capacity damage before <b>pruning,</b> thereby leading to performance degradation. To alleviate this issue, we first study and reveal the relative sparsity effect in emerging stimulative training and then propose a structured <b>pruning</b> framework, named STP, based on an enhanced sparsification paradigm which maintains the magnitude of dropped weights and enhances the expressivity of kept weights by <b>self-distillation.</b> Besides, to find an optimal architecture for the pruned network, we propose a multi-dimension architecture space and a <b>knowledge</b> <b>distillation-guided</b> exploration strategy. To reduce the huge capacity gap of <b>distillation,</b> we propose a subnet mutating expansion technique. Extensive experiments on various <b>benchmarks</b> indicate the effectiveness of STP. Specifically, without <b>fine-tuning,</b> our method consistently achieves superior performance at different budgets, especially under extremely aggressive <b>pruning</b> scenarios, e.g., remaining 95.11% Top-1 accuracy (72.43% in 76.15%) while reducing 85% FLOPs for ResNet-50 on ImageNet. Codes will be released soon.</p></p class="citation"></blockquote><h3 id=797--46318-deep-learning-approaches-for-human-action-recognition-in-video-data-yufei-xie-2024>(7/97 | 46/318) Deep Learning Approaches for Human Action Recognition in Video Data (Yufei Xie, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yufei Xie. (2024)<br><strong>Deep Learning Approaches for Human Action Recognition in Video Data</strong><br><button class=copy-to-clipboard title="Deep Learning Approaches for Human Action Recognition in Video Data" index=46>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-46 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: 68T45, I-2-10, cs-CV, cs.CV<br>Keyword Score: 60<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, Knowledge Distillation, Recurrent Neural Network, Recurrent Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06810v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06810v1.pdf filename=2403.06810v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Human action recognition in videos is a critical task with significant implications for numerous applications, including surveillance, sports analytics, and healthcare. The challenge lies in creating models that are both precise in their recognition capabilities and efficient enough for practical use. This study conducts an in-depth analysis of various deep learning models to address this challenge. Utilizing a subset of the UCF101 Videos dataset, we focus on <b>Convolutional</b> <b>Neural</b> <b>Networks</b> <b>(CNNs),</b> <b>Recurrent</b> <b>Neural</b> <b>Networks</b> <b>(RNNs),</b> and Two-Stream ConvNets. The research reveals that while <b>CNNs</b> effectively capture spatial features and <b>RNNs</b> encode temporal sequences, Two-Stream ConvNets exhibit superior performance by integrating spatial and temporal dimensions. These insights are <b>distilled</b> from the evaluation metrics of accuracy, precision, recall, and F1-score. The results of this study underscore the potential of composite models in achieving robust human action recognition and suggest avenues for future research in optimizing these models for real-world deployment.</p></p class="citation"></blockquote><h3 id=897--47318-quanttune-optimizing-model-quantization-with-adaptive-outlier-driven-fine-tuning-jiun-man-chen-et-al-2024>(8/97 | 47/318) QuantTune: Optimizing Model Quantization with Adaptive Outlier-Driven Fine Tuning (Jiun-Man Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiun-Man Chen, Yu-Hsuan Chao, Yu-Jie Wang, Ming-Der Shieh, Chih-Chung Hsu, Wei-Fen Lin. (2024)<br><strong>QuantTune: Optimizing Model Quantization with Adaptive Outlier-Driven Fine Tuning</strong><br><button class=copy-to-clipboard title="QuantTune: Optimizing Model Quantization with Adaptive Outlier-Driven Fine Tuning" index=47>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-47 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-MM, cs.CV<br>Keyword Score: 60<br>Keywords: Fine-tuning, Model Quantization, Quantization, Quantization, BERT, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06497v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06497v1.pdf filename=2403.06497v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Transformer-based</b> <b>models</b> <b>have</b> gained widespread popularity in both the computer vision (CV) and natural language processing (NLP) fields. However, significant challenges arise during post-training linear <b>quantization,</b> leading to noticeable reductions in inference accuracy. Our study focuses on uncovering the underlying causes of these accuracy drops and proposing a <b>quantization-friendly</b> <b>fine-tuning</b> method, \textbf{QuantTune}. Firstly, our analysis revealed that, on average, 65% of <b>quantization</b> errors result from the precision loss incurred by the dynamic range amplification effect of outliers across the target <b>Transformer-based</b> <b>models.</b> <b>Secondly,</b> \textbf{QuantTune} adjusts weights based on the deviation of outlier activations and effectively constrains the dynamic ranges of the problematic activations. As a result, it successfully mitigates the negative impact of outliers on the inference accuracy of <b>quantized</b> <b>models.</b> <b>Lastly,</b> \textbf{QuantTune} can be seamlessly integrated into the back-propagation pass in the <b>fine-tuning</b> process without requiring extra complexity in inference software and hardware design. Our approach showcases significant improvements in post-training <b>quantization</b> across a range of <b>Transformer-based</b> <b>models,</b> <b>including</b> ViT, <b>Bert-base,</b> and OPT. QuantTune reduces accuracy drops by 12.09% at 8-bit <b>quantization</b> and 33.8% at 7-bit compared to top calibration methods, outperforming state-of-the-art solutions by over 18.84% across ViT models.</p></p class="citation"></blockquote><h3 id=997--48318-split-to-merge-unifying-separated-modalities-for-unsupervised-domain-adaptation-xinyao-li-et-al-2024>(9/97 | 48/318) Split to Merge: Unifying Separated Modalities for Unsupervised Domain Adaptation (Xinyao Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinyao Li, Yuke Li, Zhekai Du, Fengling Li, Ke Lu, Jingjing Li. (2024)<br><strong>Split to Merge: Unifying Separated Modalities for Unsupervised Domain Adaptation</strong><br><button class=copy-to-clipboard title="Split to Merge: Unifying Separated Modalities for Unsupervised Domain Adaptation" index=48>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-48 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 53<br>Keywords: Benchmarking, Unsupervised Learning, Zero-shot, Domain Adaptation, Vision-and-Language, Zero-shot Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06946v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06946v1.pdf filename=2403.06946v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Large <b>vision-language</b> models (VLMs) like CLIP have demonstrated good <b>zero-shot</b> <b>learning</b> performance in the <b>unsupervised</b> <b>domain</b> <b>adaptation</b> task. Yet, most transfer approaches for VLMs focus on either the language or visual branches, overlooking the nuanced interplay between both modalities. In this work, we introduce a Unified Modality Separation (UniMoS) framework for <b>unsupervised</b> <b>domain</b> <b>adaptation.</b> Leveraging insights from modality gap studies, we craft a nimble modality separation network that distinctly disentangles CLIP&rsquo;s features into language-associated and vision-associated components. Our proposed Modality-Ensemble Training (MET) method fosters the exchange of modality-agnostic information while maintaining modality-specific nuances. We align features across <b>domains</b> <b>using</b> a modality discriminator. Comprehensive evaluations on three <b>benchmarks</b> reveal our approach sets a new state-of-the-art with minimal computational costs. Code: <a href=https://github.com/TL-UESTC/UniMoS>https://github.com/TL-UESTC/UniMoS</a></p></p class="citation"></blockquote><h3 id=1097--49318-real-time-transformer-based-open-vocabulary-detection-with-efficient-fusion-head-tiancheng-zhao-et-al-2024>(10/97 | 49/318) Real-time Transformer-based Open-Vocabulary Detection with Efficient Fusion Head (Tiancheng Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tiancheng Zhao, Peng Liu, Xuan He, Lu Zhang, Kyusong Lee. (2024)<br><strong>Real-time Transformer-based Open-Vocabulary Detection with Efficient Fusion Head</strong><br><button class=copy-to-clipboard title="Real-time Transformer-based Open-Vocabulary Detection with Efficient Fusion Head" index=49>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-49 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CL, cs-CV, cs.CV<br>Keyword Score: 53<br>Keywords: Object Detection, Benchmarking, Supervised Learning, Zero-shot, Transformer, Grounding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06892v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06892v1.pdf filename=2403.06892v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>End-to-end <b>transformer-based</b> detectors (DETRs) have shown exceptional performance in both closed-set and open-vocabulary <b>object</b> <b>detection</b> (OVD) tasks through the integration of language modalities. However, their demanding computational requirements have hindered their practical application in real-time <b>object</b> <b>detection</b> (OD) scenarios. In this paper, we scrutinize the limitations of two leading models in the OVDEval <b>benchmark,</b> OmDet and <b>Grounding-DINO,</b> and introduce OmDet-Turbo. This novel <b>transformer-based</b> real-time OVD model features an innovative Efficient Fusion Head (EFH) module designed to alleviate the bottlenecks observed in OmDet and <b>Grounding-DINO.</b> Notably, OmDet-Turbo-Base achieves a 100.2 frames per second (FPS) with TensorRT and language cache techniques applied. Notably, in <b>zero-shot</b> scenarios on COCO and LVIS datasets, OmDet-Turbo achieves performance levels nearly on par with current state-of-the-art <b>supervised</b> models. Furthermore, it establishes new state-of-the-art <b>benchmarks</b> on ODinW and OVDEval, boasting an AP of 30.1 and an NMS-AP of 26.86, respectively. The practicality of OmDet-Turbo in industrial applications is underscored by its exceptional performance on <b>benchmark</b> datasets and superior inference speed, positioning it as a compelling choice for real-time <b>object</b> <b>detection</b> tasks. Code: \url{https://github.com/om-ai-lab/OmDet}</p></p class="citation"></blockquote><h3 id=1197--50318-divcon-divide-and-conquer-for-progressive-text-to-image-generation-yuhao-jia-et-al-2024>(11/97 | 50/318) DivCon: Divide and Conquer for Progressive Text-to-Image Generation (Yuhao Jia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuhao Jia, Wenhan Tan. (2024)<br><strong>DivCon: Divide and Conquer for Progressive Text-to-Image Generation</strong><br><button class=copy-to-clipboard title="DivCon: Divide and Conquer for Progressive Text-to-Image Generation" index=50>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-50 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 53<br>Keywords: Diffusion Model, Benchmarking, Reasoning, Text2image, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06400v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06400v1.pdf filename=2403.06400v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Diffusion-driven</b> <b>text-to-image</b> (T2I) generation has achieved remarkable advancements. To further improve T2I models&rsquo; capability in numerical and spatial <b>reasoning,</b> the layout is employed as an intermedium to bridge <b>large</b> <b>language</b> <b>models</b> and layout-based <b>diffusion</b> <b>models.</b> However, these methods still struggle with generating images from textural <b>prompts</b> with multiple objects and complicated spatial relationships. To tackle this challenge, we introduce a divide-and-conquer approach which decouples the T2I generation task into simple subtasks. Our approach divides the layout prediction stage into numerical & spatial <b>reasoning</b> and bounding box prediction. Then, the layout-to-image generation stage is conducted in an iterative manner to reconstruct objects from easy ones to difficult ones. We conduct experiments on the HRS and NSR-1K <b>benchmarks</b> and our approach outperforms previous state-of-the-art models with notable margins. In addition, visual results demonstrate that our approach significantly improves the controllability and consistency in generating multiple objects from complex textural <b>prompts.</b></p></p class="citation"></blockquote><h3 id=1297--51318-enhancing-image-caption-generation-using-reinforcement-learning-with-human-feedback-adarsh-n-l-et-al-2024>(12/97 | 51/318) Enhancing Image Caption Generation Using Reinforcement Learning with Human Feedback (Adarsh N L et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Adarsh N L, Arun P V, Aravindh N L. (2024)<br><strong>Enhancing Image Caption Generation Using Reinforcement Learning with Human Feedback</strong><br><button class=copy-to-clipboard title="Enhancing Image Caption Generation Using Reinforcement Learning with Human Feedback" index=51>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-51 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Generative AI, Reinforcement Learning, Reinforcement Learning from Human Feedback, Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06735v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06735v1.pdf filename=2403.06735v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Research on <b>generative</b> <b>models</b> to produce human-aligned / human-preferred outputs has seen significant recent contributions. Between text and image-generative models, we narrowed our focus to text-based <b>generative</b> <b>models,</b> particularly to produce captions for images that align with human preferences. In this research, we explored a potential method to amplify the performance of the Deep Neural Network Model to generate captions that are preferred by humans. This was achieved by integrating <b>Supervised</b> <b>Learning</b> and <b>Reinforcement</b> <b>Learning</b> with Human Feedback <b>(RLHF)</b> using the Flickr8k dataset. Also, a novel loss function that is capable of optimizing the model based on human feedback is introduced. In this paper, we provide a concise sketch of our approach and results, hoping to contribute to the ongoing advances in the field of human-aligned <b>generative</b> <b>AI</b> models.</p></p class="citation"></blockquote><h3 id=1397--52318-fsviewfusion-few-shots-view-generation-of-novel-objects-rukhshanda-hussain-et-al-2024>(13/97 | 52/318) FSViewFusion: Few-Shots View Generation of Novel Objects (Rukhshanda Hussain et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rukhshanda Hussain, Hui Xian Grace Lim, Borchun Chen, Mubarak Shah, Ser Nam Lim. (2024)<br><strong>FSViewFusion: Few-Shots View Generation of Novel Objects</strong><br><button class=copy-to-clipboard title="FSViewFusion: Few-Shots View Generation of Novel Objects" index=52>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-52 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Diffusion Model, Few-shot, Fine-tuning, Out-of-distribution, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06394v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06394v2.pdf filename=2403.06394v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Novel view synthesis has observed tremendous developments since the arrival of NeRFs. However, Nerf models overfit on a single scene, lacking generalization to out of distribution objects. Recently, <b>diffusion</b> <b>models</b> have exhibited remarkable performance on introducing generalization in view synthesis. Inspired by these advancements, we explore the capabilities of a pretrained stable <b>diffusion</b> <b>model</b> for view synthesis without explicit 3D priors. Specifically, we base our method on a personalized text to image model, Dreambooth, given its strong ability to adapt to specific novel objects with a few shots. Our research reveals two interesting findings. First, we observe that Dreambooth can learn the high level concept of a view, compared to arguably more complex strategies which involve <b>finetuning</b> <b>diffusions</b> <b>on</b> large amounts of multi-view data. Second, we establish that the concept of a view can be disentangled and transferred to a novel object irrespective of the original object&rsquo;s identify from which the views are learnt. Motivated by this, we introduce a learning strategy, FSViewFusion, which inherits a specific view through only one image sample of a single scene, and transfers the knowledge to a novel object, learnt from few shots, using low rank adapters. Through extensive experiments we demonstrate that our method, albeit simple, is efficient in generating reliable view samples for in the wild images. Code and models will be released.</p></p class="citation"></blockquote><h3 id=1497--53318-focusclip-multimodal-subject-level-guidance-for-zero-shot-transfer-in-human-centric-tasks-muhammad-saif-ullah-khan-et-al-2024>(14/97 | 53/318) FocusCLIP: Multimodal Subject-Level Guidance for Zero-Shot Transfer in Human-Centric Tasks (Muhammad Saif Ullah Khan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Muhammad Saif Ullah Khan, Muhammad Ferjad Naeem, Federico Tombari, Luc Van Gool, Didier Stricker, Muhammad Zeshan Afzal. (2024)<br><strong>FocusCLIP: Multimodal Subject-Level Guidance for Zero-Shot Transfer in Human-Centric Tasks</strong><br><button class=copy-to-clipboard title="FocusCLIP: Multimodal Subject-Level Guidance for Zero-Shot Transfer in Human-Centric Tasks" index=53>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-53 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 46<br>Keywords: Multi-modal, Multi-modal, Zero-shot, Emotion Recognition, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06904v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06904v1.pdf filename=2403.06904v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose FocusCLIP, integrating subject-level guidance&ndash;a specialized mechanism for target-specific supervision&ndash;into the CLIP framework for improved <b>zero-shot</b> transfer on human-centric tasks. Our novel contributions enhance CLIP on both the vision and text sides. On the vision side, we incorporate ROI heatmaps emulating human visual attention mechanisms to emphasize subject-relevant image regions. On the text side, we introduce human pose descriptions to provide rich contextual information. For human-centric tasks, FocusCLIP is trained with images from the MPII Human Pose dataset. The proposed approach surpassed CLIP by an average of 8.61% across five previously unseen datasets covering three human-centric tasks. FocusCLIP achieved an average accuracy of 33.65% compared to 25.04% by CLIP. We observed a 3.98% improvement in activity recognition, a 14.78% improvement in age classification, and a 7.06% improvement in <b>emotion</b> <b>recognition.</b> Moreover, using our proposed single-shot <b>LLM</b> <b>prompting</b> strategy, we release a high-quality MPII Pose Descriptions dataset to encourage further research in <b>multimodal</b> learning for human-centric tasks. Furthermore, we also demonstrate the effectiveness of our subject-level supervision on non-human-centric tasks. FocusCLIP shows a 2.47% improvement over CLIP in <b>zero-shot</b> bird classification using the CUB dataset. Our findings emphasize the potential of integrating subject-level guidance with general pretraining methods for enhanced downstream performance.</p></p class="citation"></blockquote><h3 id=1597--54318-gritv2-efficient-and-light-weight-social-relation-recognition-n-k-sagar-reddy-et-al-2024>(15/97 | 54/318) GRITv2: Efficient and Light-weight Social Relation Recognition (N K Sagar Reddy et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>N K Sagar Reddy, Neeraj Kasera, Avinash Thakur. (2024)<br><strong>GRITv2: Efficient and Light-weight Social Relation Recognition</strong><br><button class=copy-to-clipboard title="GRITv2: Efficient and Light-weight Social Relation Recognition" index=54>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-54 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 46<br>Keywords: Graph, Benchmarking, Model Compression, Quantization, Transformer, Relation Extraction<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06895v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06895v1.pdf filename=2403.06895v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Our research focuses on the analysis and improvement of the <b>Graph-based</b> <b>Relation</b> <b>Inference</b> <b>Transformer</b> (GRIT), which serves as an important <b>benchmark</b> in the field. We conduct a comprehensive ablation study using the PISC-fine dataset, to find and explore improvement in efficiency and performance of GRITv2. Our research has provided a new state-of-the-art <b>relation</b> <b>recognition</b> <b>model</b> <b>on</b> the PISC <b>relation</b> <b>dataset.</b> We introduce several features in the GRIT <b>model</b> <b>and</b> analyse our new <b>benchmarks</b> in two versions: GRITv2-L (large) and GRITv2-S (small). Our proposed GRITv2-L surpasses existing methods on <b>relation</b> <b>recognition</b> and the GRITv2-S is within 2% performance gap of GRITv2-L, which has only 0.0625x the <b>model</b> <b>size</b> and parameters of GRITv2-L. Furthermore, we also address the need for <b>model</b> <b>compression,</b> an area crucial for deploying efficient <b>models</b> <b>on</b> resource-constrained platforms. By applying <b>quantization</b> techniques, we efficiently reduced the GRITv2-S size to 22MB and deployed it on the flagship OnePlus 12 mobile which still surpasses the PISC-fine <b>benchmarks</b> in performance, highlighting the practical viability and improved efficiency of our <b>model</b> <b>on</b> mobile devices.</p></p class="citation"></blockquote><h3 id=1697--55318-answering-diverse-questions-via-text-attached-with-key-audio-visual-clues-qilang-ye-et-al-2024>(16/97 | 55/318) Answering Diverse Questions via Text Attached with Key Audio-Visual Clues (Qilang Ye et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qilang Ye, Zitong Yu, Xin Liu. (2024)<br><strong>Answering Diverse Questions via Text Attached with Key Audio-Visual Clues</strong><br><button class=copy-to-clipboard title="Answering Diverse Questions via Text Attached with Key Audio-Visual Clues" index=55>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-55 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 46<br>Keywords: Knowledge Distillation, Knowledge Distillation, Multi-modal, Multi-modal, Question Answering, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06679v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06679v1.pdf filename=2403.06679v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Audio-visual <b>question</b> <b>answering</b> (AVQA) requires reference to video content and auditory information, followed by correlating the <b>question</b> <b>to</b> predict the most precise answer. Although mining deeper layers of audio-visual information to interact with <b>questions</b> <b>facilitates</b> the <b>multimodal</b> fusion process, the redundancy of audio-visual parameters tends to reduce the generalization of the inference engine to multiple <b>question-answer</b> <b>pairs</b> in a single video. Indeed, the natural heterogeneous relationship between audiovisuals and text makes the perfect fusion challenging, to prevent high-level audio-visual semantics from weakening the network&rsquo;s adaptability to diverse <b>question</b> <b>types,</b> we propose a framework for performing mutual correlation <b>distillation</b> (MCD) to aid <b>question</b> <b>inference.</b> MCD is divided into three main steps: 1) firstly, the residual structure is utilized to enhance the audio-visual soft associations based on <b>self-attention,</b> then key local audio-visual features relevant to the <b>question</b> <b>context</b> are captured hierarchically by shared aggregators and coupled in the form of clues with specific <b>question</b> <b>vectors.</b> 2) Secondly, <b>knowledge</b> <b>distillation</b> is enforced to align audio-visual-text pairs in a shared latent space to narrow the cross-modal semantic gap. 3) And finally, the audio-visual dependencies are decoupled by discarding the decision-level integrations. We evaluate the proposed method on two publicly available datasets containing multiple <b>question-and-answer</b> <b>pairs,</b> i.e., Music-AVQA and AVQA. Experiments show that our method outperforms other state-of-the-art methods, and one interesting finding behind is that removing deep audio-visual features during inference can effectively mitigate overfitting. The source code is released at <a href=http://github.com/rikeilong/MCD-forAVQA>http://github.com/rikeilong/MCD-forAVQA</a>.</p></p class="citation"></blockquote><h3 id=1797--56318-cross-domain-and-cross-dimension-learning-for-image-to-graph-transformers-alexander-h-berger-et-al-2024>(17/97 | 56/318) Cross-domain and Cross-dimension Learning for Image-to-Graph Transformers (Alexander H. Berger et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alexander H. Berger, Laurin Lux, Suprosanna Shit, Ivan Ezhov, Georgios Kaissis, Martin J. Menten, Daniel Rueckert, Johannes C. Paetzold. (2024)<br><strong>Cross-domain and Cross-dimension Learning for Image-to-Graph Transformers</strong><br><button class=copy-to-clipboard title="Cross-domain and Cross-dimension Learning for Image-to-Graph Transformers" index=56>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-56 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 46<br>Keywords: Object Detection, Graph, Benchmarking, Transfer Learning, Transformer, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06601v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06601v1.pdf filename=2403.06601v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Direct image-to-graph transformation is a challenging task that solves <b>object</b> <b>detection</b> and relationship prediction in a single model. Due to the complexity of this task, large training datasets are rare in many <b>domains,</b> <b>which</b> makes the training of large networks challenging. This data sparsity necessitates the establishment of pre-training strategies akin to the state-of-the-art in computer vision. In this work, we introduce a set of methods enabling cross-domain and cross-dimension <b>transfer</b> <b>learning</b> for image-to-graph <b>transformers.</b> We propose (1) a regularized edge sampling loss for sampling the optimal number of <b>object</b> <b>relationships</b> (edges) across <b>domains,</b> <b>(2)</b> a <b>domain</b> <b>adaptation</b> framework for image-to-graph <b>transformers</b> that aligns features from different <b>domains,</b> <b>and</b> (3) a simple projection function that allows us to pretrain 3D <b>transformers</b> on 2D input data. We demonstrate our method&rsquo;s utility in cross-domain and cross-dimension experiments, where we pretrain our models on 2D satellite images before applying them to vastly different target <b>domains</b> <b>in</b> 2D and 3D. Our method consistently outperforms a series of baselines on challenging <b>benchmarks,</b> such as retinal or whole-brain vessel <b>graph</b> extraction.</p></p class="citation"></blockquote><h3 id=1897--57318-toward-generalist-anomaly-detection-via-in-context-residual-learning-with-few-shot-sample-prompts-jiawen-zhu-et-al-2024>(18/97 | 57/318) Toward Generalist Anomaly Detection via In-context Residual Learning with Few-shot Sample Prompts (Jiawen Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiawen Zhu, Guansong Pang. (2024)<br><strong>Toward Generalist Anomaly Detection via In-context Residual Learning with Few-shot Sample Prompts</strong><br><button class=copy-to-clipboard title="Toward Generalist Anomaly Detection via In-context Residual Learning with Few-shot Sample Prompts" index=57>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-57 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 43<br>Keywords: Anomaly Detection, Benchmarking, Few-shot, In-context Learning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06495v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06495v2.pdf filename=2403.06495v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper explores the problem of Generalist <b>Anomaly</b> <b>Detection</b> (GAD), aiming to train one single detection model that can generalize to detect anomalies in diverse datasets from different application domains without any further training on the target data. Some recent studies have shown that large pre-trained Visual-Language Models (VLMs) like CLIP have strong generalization capabilities on detecting industrial defects from various datasets, but their methods rely heavily on handcrafted text <b>prompts</b> about defects, making them difficult to generalize to anomalies in other applications, e.g., medical image anomalies or semantic anomalies in natural images. In this work, we propose to train a GAD model with <b>few-shot</b> normal images as sample <b>prompts</b> for AD on diverse datasets on the fly. To this end, we introduce a novel approach that learns an <b>in-context</b> residual learning model for GAD, termed InCTRL. It is trained on an auxiliary dataset to discriminate anomalies from normal samples based on a holistic evaluation of the residuals between query images and <b>few-shot</b> normal sample <b>prompts.</b> Regardless of the datasets, per definition of <b>anomaly,</b> <b>larger</b> residuals are expected for anomalies than normal samples, thereby enabling InCTRL to generalize across different domains without further training. Comprehensive experiments on nine AD datasets are performed to establish a GAD <b>benchmark</b> that encapsulate the detection of industrial defect anomalies, medical anomalies, and semantic anomalies in both one-vs-all and multi-class setting, on which InCTRL is the best performer and significantly outperforms state-of-the-art competing methods.</p></p class="citation"></blockquote><h3 id=1997--58318-ensemble-quadratic-assignment-network-for-graph-matching-haoru-tan-et-al-2024>(19/97 | 58/318) Ensemble Quadratic Assignment Network for Graph Matching (Haoru Tan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haoru Tan, Chuang Wang, Sitong Wu, Xu-Yao Zhang, Fei Yin, Cheng-Lin Liu. (2024)<br><strong>Ensemble Quadratic Assignment Network for Graph Matching</strong><br><button class=copy-to-clipboard title="Ensemble Quadratic Assignment Network for Graph Matching" index=58>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-58 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 43<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Convolution, Few-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06457v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06457v1.pdf filename=2403.06457v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>matching</b> <b>is</b> a commonly used technique in computer vision and pattern recognition. Recent data-driven approaches have improved the <b>graph</b> <b>matching</b> <b>accuracy</b> remarkably, whereas some traditional algorithm-based methods are more robust to feature noises, outlier nodes, and global transformation (e.g.~rotation). In this paper, we propose a <b>graph</b> <b>neural</b> <b>network</b> <b>(GNN)</b> based approach to combine the advantages of data-driven and traditional methods. In the <b>GNN</b> framework, we transform traditional <b>graph-matching</b> <b>solvers</b> <b>as</b> single-channel <b>GNNs</b> on the association <b>graph</b> <b>and</b> <b>extend</b> the single-channel architecture to the multi-channel network. The proposed model can be seen as an ensemble method that fuses multiple algorithms at every iteration. Instead of averaging the estimates at the end of the ensemble, in our approach, the independent iterations of the ensembled algorithms exchange their information after each iteration via a 1x1 channel-wise <b>convolution</b> layer. Experiments show that our model improves the performance of traditional algorithms significantly. In addition, we propose a random sampling strategy to reduce the computational complexity and GPU memory usage, so the model applies to matching <b>graphs</b> <b>with</b> <b>thousands</b> of nodes. We evaluate the performance of our method on three tasks: geometric <b>graph</b> <b>matching,</b> <b>semantic</b> feature matching, and <b>few-shot</b> 3D shape classification. The proposed model performs comparably or outperforms the best existing <b>GNN-based</b> methods.</p></p class="citation"></blockquote><h3 id=2097--59318-action-reimagined-text-to-pose-video-editing-for-dynamic-human-actions-lan-wang-et-al-2024>(20/97 | 59/318) Action Reimagined: Text-to-Pose Video Editing for Dynamic Human Actions (Lan Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lan Wang, Vishnu Boddeti, Sernam Lim. (2024)<br><strong>Action Reimagined: Text-to-Pose Video Editing for Dynamic Human Actions</strong><br><button class=copy-to-clipboard title="Action Reimagined: Text-to-Pose Video Editing for Dynamic Human Actions" index=59>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-59 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Counter-factual, Reasoning, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07198v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07198v1.pdf filename=2403.07198v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce a novel text-to-pose video editing method, ReimaginedAct. While existing video editing tasks are limited to changes in attributes, backgrounds, and styles, our method aims to predict open-ended human action changes in video. Moreover, our method can accept not only direct instructional text <b>prompts</b> but also `what if&rsquo; questions to predict possible action changes. ReimaginedAct comprises video understanding, <b>reasoning,</b> and editing modules. First, an <b>LLM</b> is utilized initially to obtain a plausible answer for the instruction or question, which is then used for (1) <b>prompting</b> Grounded-SAM to produce bounding boxes of relevant individuals and (2) retrieving a set of pose videos that we have collected for editing human actions. The retrieved pose videos and the detected individuals are then utilized to alter the poses extracted from the original video. We also employ a timestep blending module to ensure the edited video retains its original content except where necessary modifications are needed. To facilitate research in text-to-pose video editing, we introduce a new evaluation dataset, WhatifVideo-1.0. This dataset includes videos of different scenarios spanning a range of difficulty levels, along with questions and text <b>prompts.</b> Experimental results demonstrate that existing video editing methods struggle with human action editing, while our approach can achieve effective action editing and even imaginary editing from <b>counterfactual</b> questions.</p></p class="citation"></blockquote><h3 id=2197--60318-v3d-video-diffusion-models-are-effective-3d-generators-zilong-chen-et-al-2024>(21/97 | 60/318) V3D: Video Diffusion Models are Effective 3D Generators (Zilong Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zilong Chen, Yikai Wang, Feng Wang, Zhengyi Wang, Huaping Liu. (2024)<br><strong>V3D: Video Diffusion Models are Effective 3D Generators</strong><br><button class=copy-to-clipboard title="V3D: Video Diffusion Models are Effective 3D Generators" index=60>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-60 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Diffusion Model, Fine-tuning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06738v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06738v1.pdf filename=2403.06738v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Automatic 3D generation has recently attracted widespread attention. Recent methods have greatly accelerated the generation speed, but usually produce less-detailed objects due to limited model capacity or 3D data. Motivated by recent advancements in video <b>diffusion</b> <b>models,</b> we introduce V3D, which leverages the world <b>simulation</b> capacity of pre-trained video <b>diffusion</b> <b>models</b> to facilitate 3D generation. To fully unleash the potential of video <b>diffusion</b> <b>to</b> perceive the 3D world, we further introduce geometrical consistency prior and extend the video <b>diffusion</b> <b>model</b> to a multi-view consistent 3D generator. Benefiting from this, the state-of-the-art video <b>diffusion</b> <b>model</b> could be <b>fine-tuned</b> to generate 360degree orbit frames surrounding an object given a single image. With our tailored reconstruction pipelines, we can generate high-quality meshes or 3D Gaussians within 3 minutes. Furthermore, our method can be extended to scene-level novel view synthesis, achieving precise control over the camera path with sparse input views. Extensive experiments demonstrate the superior performance of the proposed approach, especially in terms of generation quality and multi-view consistency. Our code is available at <a href=https://github.com/heheyas/V3D>https://github.com/heheyas/V3D</a></p></p class="citation"></blockquote><h3 id=2297--61318-fontclip-a-semantic-typography-visual-language-model-for-multilingual-font-applications-yuki-tatsukawa-et-al-2024>(22/97 | 61/318) FontCLIP: A Semantic Typography Visual-Language Model for Multilingual Font Applications (Yuki Tatsukawa et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuki Tatsukawa, I-Chao Shen, Anran Qi, Yuki Koyama, Takeo Igarashi, Ariel Shamir. (2024)<br><strong>FontCLIP: A Semantic Typography Visual-Language Model for Multilingual Font Applications</strong><br><button class=copy-to-clipboard title="FontCLIP: A Semantic Typography Visual-Language Model for Multilingual Font Applications" index=61>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-61 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-GR, cs.CV<br>Keyword Score: 40<br>Keywords: Fine-tuning, Fine-tuning, Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06453v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06453v1.pdf filename=2403.06453v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Acquiring the desired font for various design tasks can be challenging and requires professional typographic knowledge. While previous font retrieval or generation works have alleviated some of these difficulties, they often lack support for multiple languages and semantic attributes beyond the training data domains. To solve this problem, we present FontCLIP: a model that connects the semantic understanding of a large <b>vision-language</b> model with typographical knowledge. We integrate typography-specific knowledge into the comprehensive <b>vision-language</b> knowledge of a pretrained CLIP model through a novel <b>finetuning</b> approach. We propose to use a compound descriptive <b>prompt</b> that encapsulates adaptively sampled attributes from a font attribute dataset focusing on Roman alphabet characters. FontCLIP&rsquo;s semantic typographic latent space demonstrates two unprecedented generalization abilities. First, FontCLIP generalizes to different languages including Chinese, Japanese, and Korean (CJK), capturing the typographical features of fonts across different languages, even though it was only <b>finetuned</b> using fonts of Roman characters. Second, FontCLIP can recognize the semantic attributes that are not presented in the training data. FontCLIP&rsquo;s dual-modality and generalization abilities enable multilingual and cross-lingual font retrieval and letter shape optimization, reducing the burden of obtaining desired fonts.</p></p class="citation"></blockquote><h3 id=2397--62318-enhancing-semantic-fidelity-in-text-to-image-synthesis-attention-regulation-in-diffusion-models-yang-zhang-et-al-2024>(23/97 | 62/318) Enhancing Semantic Fidelity in Text-to-Image Synthesis: Attention Regulation in Diffusion Models (Yang Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yang Zhang, Teoh Tze Tzun, Lim Wei Hern, Tiviatis Sim, Kenji Kawaguchi. (2024)<br><strong>Enhancing Semantic Fidelity in Text-to-Image Synthesis: Attention Regulation in Diffusion Models</strong><br><button class=copy-to-clipboard title="Enhancing Semantic Fidelity in Text-to-Image Synthesis: Attention Regulation in Diffusion Models" index=62>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-62 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Diffusion Model, Fine-tuning, Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06381v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06381v1.pdf filename=2403.06381v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in <b>diffusion</b> <b>models</b> have notably improved the perceptual quality of generated images in <b>text-to-image</b> synthesis tasks. However, <b>diffusion</b> <b>models</b> often struggle to produce images that accurately reflect the intended semantics of the associated text <b>prompts.</b> We examine cross-attention layers in <b>diffusion</b> <b>models</b> and observe a propensity for these layers to disproportionately focus on certain tokens during the generation process, thereby undermining semantic fidelity. To address the issue of dominant attention, we introduce attention regulation, a computation-efficient on-the-fly optimization approach at inference time to align attention maps with the input text <b>prompt.</b> Notably, our method requires no additional training or <b>fine-tuning</b> and serves as a plug-in module on a model. Hence, the generation capacity of the original model is fully preserved. We compare our approach with alternative approaches across various datasets, evaluation metrics, and <b>diffusion</b> <b>models.</b> Experiment results show that our method consistently outperforms other baselines, yielding images that more faithfully reflect the desired concepts with reduced computation overhead. Code is available at <a href=https://github.com/YaNgZhAnG-V5/attention_regulation>https://github.com/YaNgZhAnG-V5/attention_regulation</a>.</p></p class="citation"></blockquote><h3 id=2497--63318-videomamba-state-space-model-for-efficient-video-understanding-kunchang-li-et-al-2024>(24/97 | 63/318) VideoMamba: State Space Model for Efficient Video Understanding (Kunchang Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kunchang Li, Xinhao Li, Yi Wang, Yinan He, Yali Wang, Limin Wang, Yu Qiao. (2024)<br><strong>VideoMamba: State Space Model for Efficient Video Understanding</strong><br><button class=copy-to-clipboard title="VideoMamba: State Space Model for Efficient Video Understanding" index=63>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-63 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 36<br>Keywords: Benchmarking, Convolution, Multi-modal, Self-Distillation, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06977v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06977v2.pdf filename=2403.06977v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Addressing the dual challenges of local redundancy and global dependencies in video understanding, this work innovatively adapts the Mamba to the video domain. The proposed VideoMamba overcomes the limitations of existing 3D <b>convolution</b> neural networks and video <b>transformers.</b> Its linear-complexity operator enables efficient long-term modeling, which is crucial for high-resolution long video understanding. Extensive evaluations reveal VideoMamba&rsquo;s four core abilities: (1) Scalability in the visual domain without extensive dataset pretraining, thanks to a novel <b>self-distillation</b> technique; (2) Sensitivity for recognizing short-term actions even with fine-grained motion differences; (3) Superiority in long-term video understanding, showcasing significant advancements over traditional feature-based models; and (4) Compatibility with other modalities, demonstrating robustness in <b>multi-modal</b> contexts. Through these distinct advantages, VideoMamba sets a new <b>benchmark</b> for video understanding, offering a scalable and efficient solution for comprehensive video understanding. All the code and models are available at <a href=https://github.com/OpenGVLab/VideoMamba>https://github.com/OpenGVLab/VideoMamba</a>.</p></p class="citation"></blockquote><h3 id=2597--64318-dialoc-an-iterative-approach-to-embodied-dialog-localization-chao-zhang-et-al-2024>(25/97 | 64/318) DiaLoc: An Iterative Approach to Embodied Dialog Localization (Chao Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chao Zhang, Mohan Li, Ignas Budvytis, Stephan Liwicki. (2024)<br><strong>DiaLoc: An Iterative Approach to Embodied Dialog Localization</strong><br><button class=copy-to-clipboard title="DiaLoc: An Iterative Approach to Embodied Dialog Localization" index=64>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-64 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 36<br>Keywords: Multi-modal, Multi-modal, Simulation, Simulator, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06846v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06846v1.pdf filename=2403.06846v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multimodal</b> learning has advanced the performance for many <b>vision-language</b> tasks. However, most existing works in embodied dialog research focus on navigation and leave the localization task understudied. The few existing dialog-based localization approaches assume the availability of entire dialog prior to localizaiton, which is impractical for deployed dialog-based localization. In this paper, we propose DiaLoc, a new dialog-based localization framework which aligns with a real human operator behavior. Specifically, we produce an iterative refinement of location predictions which can visualize current pose believes after each dialog turn. DiaLoc effectively utilizes the <b>multimodal</b> data for multi-shot localization, where a fusion encoder fuses vision and dialog information iteratively. We achieve state-of-the-art results on embodied dialog-based localization task, in single-shot (+7.08% in Acc5@valUnseen) and multi-shot settings (+10.85% in Acc5@valUnseen). DiaLoc narrows the gap between <b>simulation</b> and real-world applications, opening doors for future research on collaborative localization and navigation.</p></p class="citation"></blockquote><h3 id=2697--65318-large-model-driven-radiology-report-generation-with-clinical-quality-reinforcement-learning-zijian-zhou-et-al-2024>(26/97 | 65/318) Large Model driven Radiology Report Generation with Clinical Quality Reinforcement Learning (Zijian Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zijian Zhou, Miaojing Shi, Meng Wei, Oluwatosin Alabi, Zijie Yue, Tom Vercauteren. (2024)<br><strong>Large Model driven Radiology Report Generation with Clinical Quality Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Large Model driven Radiology Report Generation with Clinical Quality Reinforcement Learning" index=65>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-65 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 36<br>Keywords: Multi-modal, Multi-modal, Reinforcement Learning, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06728v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06728v1.pdf filename=2403.06728v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Radiology report generation (RRG) has attracted significant attention due to its potential to reduce the workload of radiologists. Current RRG approaches are still unsatisfactory against clinical standards. This paper introduces a novel RRG method, \textbf{LM-RRG}, that integrates <b>large</b> <b>models</b> <b>(LMs)</b> with clinical quality <b>reinforcement</b> <b>learning</b> to generate accurate and comprehensive chest X-ray radiology reports. Our method first designs a <b>large</b> <b>language</b> <b>model</b> driven feature extractor to analyze and interpret different regions of the chest X-ray image, emphasizing specific regions with medical significance. Next, based on the <b>large</b> <b>model&rsquo;s</b> <b>decoder,</b> we develop a <b>multimodal</b> report generator that leverages <b>multimodal</b> <b>prompts</b> from visual features and textual instruction to produce the radiology report in an auto-regressive way. Finally, to better reflect the clinical significant and insignificant errors that radiologists would normally assign in the report, we introduce a novel clinical quality <b>reinforcement</b> <b>learning</b> strategy. It utilizes the radiology report clinical quality (RadCliQ) metric as a reward function in the learning process. Extensive experiments on the MIMIC-CXR and IU-Xray datasets demonstrate the superiority of our method over the state of the art.</p></p class="citation"></blockquote><h3 id=2797--66318-leveraging-foundation-models-for-content-based-medical-image-retrieval-in-radiology-stefan-denner-et-al-2024>(27/97 | 66/318) Leveraging Foundation Models for Content-Based Medical Image Retrieval in Radiology (Stefan Denner et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Stefan Denner, David Zimmerer, Dimitrios Bounias, Markus Bujotzek, Shuhan Xiao, Lisa Kausch, Philipp Schader, Tobias Penzkofer, Paul F. Jäger, Klaus Maier-Hein. (2024)<br><strong>Leveraging Foundation Models for Content-Based Medical Image Retrieval in Radiology</strong><br><button class=copy-to-clipboard title="Leveraging Foundation Models for Content-Based Medical Image Retrieval in Radiology" index=66>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-66 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-IR, cs.CV<br>Keyword Score: 36<br>Keywords: Benchmarking, Benchmarking, Fine-tuning, Foundation Model, Weakly-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06567v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06567v1.pdf filename=2403.06567v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Content-based image retrieval (CBIR) has the potential to significantly improve diagnostic aid and medical research in radiology. Current CBIR systems face limitations due to their specialization to certain pathologies, limiting their utility. In response, we propose using vision <b>foundation</b> <b>models</b> as powerful and versatile off-the-shelf feature extractors for content-based medical image retrieval. By <b>benchmarking</b> these models on a comprehensive dataset of 1.6 million 2D radiological images spanning four modalities and 161 pathologies, we identify <b>weakly-supervised</b> models as superior, achieving a P@1 of up to 0.594. This performance not only competes with a specialized model but does so without the need for <b>fine-tuning.</b> Our analysis further explores the challenges in retrieving pathological versus anatomical structures, indicating that accurate retrieval of pathological features presents greater difficulty. Despite these challenges, our research underscores the vast potential of <b>foundation</b> <b>models</b> for CBIR in radiology, proposing a shift towards versatile, general-purpose medical image retrieval systems that do not require specific tuning.</p></p class="citation"></blockquote><h3 id=2897--67318-quasar-quality-and-aesthetics-scoring-with-advanced-representations-sergey-kastryulin-et-al-2024>(28/97 | 67/318) QUASAR: QUality and Aesthetics Scoring with Advanced Representations (Sergey Kastryulin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sergey Kastryulin, Denis Prokopenko, Artem Babenko, Dmitry V. Dylov. (2024)<br><strong>QUASAR: QUality and Aesthetics Scoring with Advanced Representations</strong><br><button class=copy-to-clipboard title="QUASAR: QUality and Aesthetics Scoring with Advanced Representations" index=67>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-67 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Benchmarking, Fine-tuning, Self-supervised Learning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06866v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06866v2.pdf filename=2403.06866v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces a new data-driven, non-parametric method for image quality and aesthetics assessment, surpassing existing approaches and requiring no <b>prompt</b> engineering or <b>fine-tuning.</b> We eliminate the need for expressive textual embeddings by proposing efficient image anchors in the data. Through extensive evaluations of 7 state-of-the-art <b>self-supervised</b> models, our method demonstrates superior performance and robustness across various datasets and <b>benchmarks.</b> Notably, it achieves high agreement with human assessments even with limited data and shows high robustness to the nature of data and their pre-processing pipeline. Our contributions offer a streamlined solution for assessment of images while providing insights into the perception of visual information.</p></p class="citation"></blockquote><h3 id=2997--68318-3d-aware-image-generation-and-editing-with-multi-modal-conditions-bo-li-et-al-2024>(29/97 | 68/318) 3D-aware Image Generation and Editing with Multi-modal Conditions (Bo Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bo Li, Yi-ke Li, Zhi-fen He, Bin Liu, Yun-Kun Lai. (2024)<br><strong>3D-aware Image Generation and Editing with Multi-modal Conditions</strong><br><button class=copy-to-clipboard title="3D-aware Image Generation and Editing with Multi-modal Conditions" index=68>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-68 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Generative Adversarial Network, Generative Adversarial Network, Multi-modal, Style Transfer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06470v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06470v1.pdf filename=2403.06470v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>3D-consistent image generation from a single 2D semantic label is an important and challenging research topic in computer graphics and computer vision. Although some related works have made great progress in this field, most of the existing methods suffer from poor disentanglement performance of shape and appearance, and lack <b>multi-modal</b> control. In this paper, we propose a novel end-to-end 3D-aware image generation and editing model incorporating multiple types of conditional inputs, including pure noise, text and reference image. On the one hand, we dive into the latent space of 3D <b>Generative</b> <b>Adversarial</b> <b>Networks</b> <b>(GANs)</b> and propose a novel disentanglement strategy to separate appearance features from shape features during the generation process. On the other hand, we propose a unified framework for flexible image generation and editing tasks with <b>multi-modal</b> conditions. Our method can generate diverse images with distinct noises, edit the attribute through a text description and conduct <b>style</b> <b>transfer</b> by giving a reference RGB image. Extensive experiments demonstrate that the proposed method outperforms alternative approaches both qualitatively and quantitatively on image generation and editing.</p></p class="citation"></blockquote><h3 id=3097--69318-pre-trained-model-recommendation-for-downstream-fine-tuning-jiameng-bai-et-al-2024>(30/97 | 69/318) Pre-Trained Model Recommendation for Downstream Fine-tuning (Jiameng Bai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiameng Bai, Sai Wu, Jie Song, Junbo Zhao, Gang Chen. (2024)<br><strong>Pre-Trained Model Recommendation for Downstream Fine-tuning</strong><br><button class=copy-to-clipboard title="Pre-Trained Model Recommendation for Downstream Fine-tuning" index=69>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-69 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 33<br>Keywords: Benchmarking, Fine-tuning, Recommendation, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06382v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06382v1.pdf filename=2403.06382v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As a fundamental problem in <b>transfer</b> <b>learning,</b> model selection aims to rank off-the-shelf pre-trained models and select the most suitable one for the new target task. Existing model selection techniques are often constrained in their scope and tend to overlook the nuanced relationships between models and tasks. In this paper, we present a pragmatic framework \textbf{Fennec}, delving into a diverse, large-scale model repository while meticulously considering the intricate connections between tasks and models. The key insight is to map all models and historical tasks into a <b>transfer-related</b> <b>subspace,</b> where the distance between model vectors and task vectors represents the magnitude of transferability. A large vision model, as a proxy, infers a new task&rsquo;s representation in the <b>transfer</b> <b>space,</b> thereby circumventing the computational burden of extensive forward passes. We also investigate the impact of the inherent inductive bias of models on <b>transfer</b> <b>results</b> and propose a novel method called \textbf{archi2vec} to encode the intricate structures of models. The <b>transfer</b> <b>score</b> is computed through straightforward vector arithmetic with a time complexity of $\mathcal{O}(1)$. Finally, we make a substantial contribution to the field by releasing a comprehensive <b>benchmark.</b> We validate the effectiveness of our framework through rigorous testing on two <b>benchmarks.</b> The <b>benchmark</b> and the code will be publicly available in the near future.</p></p class="citation"></blockquote><h3 id=3197--70318-structure-your-data-towards-semantic-graph-counterfactuals-angeliki-dimitriou-et-al-2024>(31/97 | 70/318) Structure Your Data: Towards Semantic Graph Counterfactuals (Angeliki Dimitriou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Angeliki Dimitriou, Maria Lymperaiou, Giorgos Filandrianos, Konstantinos Thomas, Giorgos Stamou. (2024)<br><strong>Structure Your Data: Towards Semantic Graph Counterfactuals</strong><br><button class=copy-to-clipboard title="Structure Your Data: Towards Semantic Graph Counterfactuals" index=70>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-70 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 31<br>Keywords: Graph, Graph Neural Network, Benchmarking, Black Box, Counter-factual<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06514v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06514v1.pdf filename=2403.06514v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Counterfactual</b> explanations (CEs) based on concepts are explanations that consider alternative scenarios to understand which high-level semantic features contributed to particular model predictions. In this work, we propose CEs based on the semantic <b>graphs</b> accompanying input data to achieve more descriptive, accurate, and human-aligned explanations. Building upon state-of-the-art (SoTA) conceptual attempts, we adopt a model-agnostic edit-based approach and introduce leveraging <b>GNNs</b> for efficient <b>Graph</b> Edit Distance (GED) computation. With a focus on the visual domain, we represent images as scene <b>graphs</b> and obtain their <b>GNN</b> embeddings to bypass solving the NP-hard <b>graph</b> similarity problem for all input pairs, an integral part of the CE computation process. We apply our method to <b>benchmark</b> and real-world datasets with varying difficulty and availability of semantic annotations. Testing on diverse classifiers, we find that our CEs outperform previous SoTA explanation models based on semantics, including both white and <b>black-box</b> <b>as</b> well as conceptual and pixel-level approaches. Their superiority is proven quantitatively and qualitatively, as validated by human subjects, highlighting the significance of leveraging semantic edges in the presence of intricate relationships. Our model-agnostic <b>graph-based</b> approach is widely applicable and easily extensible, producing actionable explanations across different contexts.</p></p class="citation"></blockquote><h3 id=3297--71318-attention-prompt-tuning-parameter-efficient-adaptation-of-pre-trained-models-for-spatiotemporal-modeling-wele-gedara-chaminda-bandara-et-al-2024>(32/97 | 71/318) Attention Prompt Tuning: Parameter-efficient Adaptation of Pre-trained Models for Spatiotemporal Modeling (Wele Gedara Chaminda Bandara et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wele Gedara Chaminda Bandara, Vishal M. Patel. (2024)<br><strong>Attention Prompt Tuning: Parameter-efficient Adaptation of Pre-trained Models for Spatiotemporal Modeling</strong><br><button class=copy-to-clipboard title="Attention Prompt Tuning: Parameter-efficient Adaptation of Pre-trained Models for Spatiotemporal Modeling" index=71>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-71 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Fine-tuning, Transformer, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06978v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06978v1.pdf filename=2403.06978v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we introduce Attention <b>Prompt</b> Tuning (APT) - a computationally efficient variant of <b>prompt</b> tuning for video-based applications such as action recognition. <b>Prompt</b> tuning approaches involve injecting a set of learnable <b>prompts</b> along with data tokens during <b>fine-tuning</b> while keeping the backbone frozen. This approach greatly reduces the number of learnable parameters compared to full tuning. For image-based downstream tasks, normally a couple of learnable <b>prompts</b> achieve results close to those of full tuning. However, videos, which contain more complex spatiotemporal information, require hundreds of tunable <b>prompts</b> to achieve reasonably good results. This reduces the parameter efficiency observed in images and significantly increases latency and the number of floating-point operations (FLOPs) during inference. To tackle these issues, we directly inject the <b>prompts</b> into the keys and values of the non-local attention mechanism within the <b>transformer</b> block. Additionally, we introduce a novel <b>prompt</b> reparameterization technique to make APT more robust against hyperparameter selection. The proposed APT approach greatly reduces the number of FLOPs and latency while achieving a significant performance boost over the existing parameter-efficient tuning methods on UCF101, HMDB51, and SSv2 datasets for action recognition. The code and pre-trained models are available at <a href=https://github.com/wgcban/apt>https://github.com/wgcban/apt</a></p></p class="citation"></blockquote><h3 id=3397--72318-trustworthy-partial-label-learning-with-out-of-distribution-detection-jintao-huang-et-al-2024>(33/97 | 72/318) Trustworthy Partial Label Learning with Out-of-distribution Detection (Jintao Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jintao Huang, Yiu-Ming Cheung. (2024)<br><strong>Trustworthy Partial Label Learning with Out-of-distribution Detection</strong><br><button class=copy-to-clipboard title="Trustworthy Partial Label Learning with Out-of-distribution Detection" index=72>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-72 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Out-of-distribution, Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06681v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06681v1.pdf filename=2403.06681v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Partial Label Learning (PLL) grapples with learning from ambiguously labelled data, and it has been successfully applied in fields such as image recognition. Nevertheless, traditional PLL methods rely on the closed-world assumption, which can be limiting in open-world scenarios and negatively impact model performance and generalization. To tackle these challenges, our study introduces a novel method called PLL-OOD, which is the first to incorporate <b>Out-of-Distribution</b> (OOD) detection into the PLL framework. PLL-OOD significantly enhances model adaptability and accuracy by merging <b>self-supervised</b> <b>learning</b> with partial label loss and pioneering the Partial-Energy (PE) score for OOD detection. This approach improves data feature representation and effectively disambiguates candidate labels, using a dynamic label confidence matrix to refine predictions. The PE score, adjusted by label confidence, precisely identifies OOD instances, optimizing model training towards in-distribution data. This innovative method markedly boosts PLL model robustness and performance in open-world settings. To validate our approach, we conducted a comprehensive comparative experiment combining the existing state-of-the-art PLL model with multiple OOD scores on the CIFAR-10 and CIFAR-100 datasets with various OOD datasets. The results demonstrate that the proposed PLL-OOD framework is highly effective and effectiveness outperforms existing models, showcasing its superiority and effectiveness.</p></p class="citation"></blockquote><h3 id=3497--73318-active-generation-for-image-classification-tao-huang-et-al-2024>(34/97 | 73/318) Active Generation for Image Classification (Tao Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tao Huang, Jiaqi Liu, Shan You, Chang Xu. (2024)<br><strong>Active Generation for Image Classification</strong><br><button class=copy-to-clipboard title="Active Generation for Image Classification" index=73>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-73 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Diffusion Model, Active Learning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06517v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06517v1.pdf filename=2403.06517v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, the growing capabilities of deep generative models have underscored their potential in enhancing image classification accuracy. However, existing methods often demand the generation of a disproportionately large number of images compared to the original dataset, while having only marginal improvements in accuracy. This computationally expensive and time-consuming process hampers the practicality of such approaches. In this paper, we propose to address the efficiency of image generation by focusing on the specific needs and characteristics of the model. With a central tenet of <b>active</b> <b>learning,</b> our method, named ActGen, takes a training-aware approach to image generation. It aims to create images akin to the challenging or misclassified samples encountered by the current model and incorporates these generated images into the training set to augment model performance. ActGen introduces an attentive image guidance technique, using real images as guides during the denoising process of a <b>diffusion</b> <b>model.</b> The model&rsquo;s attention on class <b>prompt</b> is leveraged to ensure the preservation of similar foreground object while diversifying the background. Furthermore, we introduce a gradient-based generation guidance method, which employs two losses to generate more challenging samples and prevent the generated images from being too similar to previously generated ones. Experimental results on the CIFAR and ImageNet datasets demonstrate that our method achieves better performance with a significantly reduced number of generated images.</p></p class="citation"></blockquote><h3 id=3597--74318-pointseg-a-training-free-paradigm-for-3d-scene-segmentation-via-foundation-models-qingdong-he-et-al-2024>(35/97 | 74/318) PointSeg: A Training-Free Paradigm for 3D Scene Segmentation via Foundation Models (Qingdong He et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qingdong He, Jinlong Peng, Zhengkai Jiang, Xiaobin Hu, Jiangning Zhang, Qiang Nie, Yabiao Wang, Chengjie Wang. (2024)<br><strong>PointSeg: A Training-Free Paradigm for 3D Scene Segmentation via Foundation Models</strong><br><button class=copy-to-clipboard title="PointSeg: A Training-Free Paradigm for 3D Scene Segmentation via Foundation Models" index=74>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-74 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Foundation Model, Supervised Learning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06403v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06403v1.pdf filename=2403.06403v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent success of vision <b>foundation</b> <b>models</b> have shown promising performance for the 2D perception tasks. However, it is difficult to train a 3D <b>foundation</b> <b>network</b> directly due to the limited dataset and it remains under explored whether existing <b>foundation</b> <b>models</b> can be lifted to 3D space seamlessly. In this paper, we present PointSeg, a novel training-free paradigm that leverages off-the-shelf vision <b>foundation</b> <b>models</b> to address 3D scene perception tasks. PointSeg can segment anything in 3D scene by acquiring accurate 3D <b>prompts</b> to align their corresponding pixels across frames. Concretely, we design a two-branch <b>prompts</b> learning structure to construct the 3D point-box <b>prompts</b> pairs, combining with the bidirectional matching strategy for accurate point and proposal <b>prompts</b> generation. Then, we perform the iterative post-refinement adaptively when cooperated with different vision <b>foundation</b> <b>models.</b> Moreover, we design a affinity-aware merging algorithm to improve the final ensemble masks. PointSeg demonstrates impressive segmentation performance across various datasets, all without training. Specifically, our approach significantly surpasses the state-of-the-art specialist model by 13.4$%$, 11.3$%$, and 12$%$ mAP on ScanNet, ScanNet++, and KITTI-360 datasets, respectively. On top of that, PointSeg can incorporate with various segmentation models and even surpasses the <b>supervised</b> methods.</p></p class="citation"></blockquote><h3 id=3697--75318-class-imbalance-in-object-detection-an-experimental-diagnosis-and-study-of-mitigation-strategies-nieves-crasto-2024>(36/97 | 75/318) Class Imbalance in Object Detection: An Experimental Diagnosis and Study of Mitigation Strategies (Nieves Crasto, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nieves Crasto. (2024)<br><strong>Class Imbalance in Object Detection: An Experimental Diagnosis and Study of Mitigation Strategies</strong><br><button class=copy-to-clipboard title="Class Imbalance in Object Detection: An Experimental Diagnosis and Study of Mitigation Strategies" index=75>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-75 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 26<br>Keywords: Object Detection, Benchmarking, Benchmarking, Data Augmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07113v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07113v1.pdf filename=2403.07113v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Object</b> <b>detection,</b> a pivotal task in computer vision, is frequently hindered by dataset imbalances, particularly the under-explored issue of foreground-foreground class imbalance. This lack of attention to foreground-foreground class imbalance becomes even more pronounced in the context of single-stage detectors. This study introduces a <b>benchmarking</b> framework utilizing the YOLOv5 single-stage detector to address the problem of foreground-foreground class imbalance. We crafted a novel 10-class long-tailed dataset from the COCO dataset, termed COCO-ZIPF, tailored to reflect common real-world detection scenarios with a limited number of <b>object</b> <b>classes.</b> Against this backdrop, we scrutinized three established techniques: sampling, loss weighing, and <b>data</b> <b>augmentation.</b> Our comparative analysis reveals that sampling and loss reweighing methods, while shown to be beneficial in two-stage detector settings, do not translate as effectively in improving YOLOv5&rsquo;s performance on the COCO-ZIPF dataset. On the other hand, <b>data</b> <b>augmentation</b> methods, specifically mosaic and mixup, significantly enhance the model&rsquo;s mean Average Precision (mAP), by introducing more variability and complexity into the training <b>data.</b> <b>(Code</b> available: <a href=https://github.com/craston/object_detection_cib>https://github.com/craston/object_detection_cib</a>)</p></p class="citation"></blockquote><h3 id=3797--76318-a-holistic-framework-towards-vision-based-traffic-signal-control-with-microscopic-simulation-pan-he-et-al-2024>(37/97 | 76/318) A Holistic Framework Towards Vision-based Traffic Signal Control with Microscopic Simulation (Pan He et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pan He, Quanyi Li, Xiaoyong Yuan, Bolei Zhou. (2024)<br><strong>A Holistic Framework Towards Vision-based Traffic Signal Control with Microscopic Simulation</strong><br><button class=copy-to-clipboard title="A Holistic Framework Towards Vision-based Traffic Signal Control with Microscopic Simulation" index=76>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-76 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 26<br>Keywords: Benchmarking, Benchmarking, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06884v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06884v1.pdf filename=2403.06884v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Traffic signal control (TSC) is crucial for reducing traffic congestion that leads to smoother traffic flow, reduced idling time, and mitigated CO2 emissions. In this study, we explore the computer vision approach for TSC that modulates on-road traffic flows through visual observation. Unlike traditional feature-based approaches, vision-based methods depend much less on heuristics and predefined features, bringing promising potentials for end-to-end learning and optimization of traffic signals. Thus, we introduce a holistic traffic <b>simulation</b> framework called TrafficDojo towards vision-based TSC and its <b>benchmarking</b> by integrating the microscopic traffic flow provided in SUMO into the driving simulator MetaDrive. This proposed framework offers a versatile traffic environment for in-depth analysis and comprehensive evaluation of traffic signal controllers across diverse traffic conditions and scenarios. We establish and compare baseline algorithms including both traditional and Reinforecment Learning (RL) approaches. This work sheds insights into the design and development of vision-based TSC approaches and open up new research opportunities. All the code and baselines will be made publicly available.</p></p class="citation"></blockquote><h3 id=3897--77318-bayesian-diffusion-models-for-3d-shape-reconstruction-haiyang-xu-et-al-2024>(38/97 | 77/318) Bayesian Diffusion Models for 3D Shape Reconstruction (Haiyang Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haiyang Xu, Yu Lei, Zeyuan Chen, Xiang Zhang, Yue Zhao, Yilin Wang, Zhuowen Tu. (2024)<br><strong>Bayesian Diffusion Models for 3D Shape Reconstruction</strong><br><button class=copy-to-clipboard title="Bayesian Diffusion Models for 3D Shape Reconstruction" index=77>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-77 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 23<br>Keywords: Diffusion Model, Benchmarking, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06973v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06973v1.pdf filename=2403.06973v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present Bayesian <b>Diffusion</b> <b>Models</b> (BDM), a prediction algorithm that performs effective Bayesian inference by tightly coupling the top-down (prior) information with the bottom-up (data-driven) procedure via joint <b>diffusion</b> <b>processes.</b> We show the effectiveness of BDM on the 3D shape reconstruction task. Compared to prototypical deep learning data-driven approaches trained on paired <b>(supervised)</b> data-labels (e.g. image-point clouds) datasets, our BDM brings in rich prior information from standalone labels (e.g. point clouds) to improve the bottom-up 3D reconstruction. As opposed to the standard Bayesian frameworks where explicit prior and likelihood are required for the inference, BDM performs seamless information fusion via coupled <b>diffusion</b> <b>processes</b> with learned gradient computation networks. The specialty of our BDM lies in its capability to engage the active and effective information exchange and fusion of the top-down and bottom-up processes where each itself is a <b>diffusion</b> <b>process.</b> We demonstrate state-of-the-art results on both synthetic and real-world <b>benchmarks</b> for 3D shape reconstruction.</p></p class="citation"></blockquote><h3 id=3997--78318-shape-non-rigid-kinematics-snk-a-zero-shot-method-for-non-rigid-shape-matching-via-unsupervised-functional-map-regularized-reconstruction-souhaib-attaiki-et-al-2024>(39/97 | 78/318) Shape Non-rigid Kinematics (SNK): A Zero-Shot Method for Non-Rigid Shape Matching via Unsupervised Functional Map Regularized Reconstruction (Souhaib Attaiki et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Souhaib Attaiki, Maks Ovsjanikov. (2024)<br><strong>Shape Non-rigid Kinematics (SNK): A Zero-Shot Method for Non-Rigid Shape Matching via Unsupervised Functional Map Regularized Reconstruction</strong><br><button class=copy-to-clipboard title="Shape Non-rigid Kinematics (SNK): A Zero-Shot Method for Non-Rigid Shape Matching via Unsupervised Functional Map Regularized Reconstruction" index=78>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-78 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Benchmarking, Unsupervised Learning, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06804v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06804v1.pdf filename=2403.06804v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present Shape Non-rigid Kinematics (SNK), a novel <b>zero-shot</b> method for non-rigid shape matching that eliminates the need for extensive training or ground truth data. SNK operates on a single pair of shapes, and employs a reconstruction-based strategy using an encoder-decoder architecture, which deforms the source shape to closely match the target shape. During the process, an <b>unsupervised</b> functional map is predicted and converted into a point-to-point map, serving as a supervisory mechanism for the reconstruction. To aid in training, we have designed a new decoder architecture that generates smooth, realistic deformations. SNK demonstrates competitive results on traditional <b>benchmarks,</b> simplifying the shape-matching process without compromising accuracy. Our code can be found online: <a href=https://github.com/pvnieo/SNK>https://github.com/pvnieo/SNK</a></p></p class="citation"></blockquote><h3 id=4097--79318-omh-structured-sparsity-via-optimally-matched-hierarchy-for-unsupervised-semantic-segmentation-baran-ozaydin-et-al-2024>(40/97 | 79/318) OMH: Structured Sparsity via Optimally Matched Hierarchy for Unsupervised Semantic Segmentation (Baran Ozaydin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Baran Ozaydin, Tong Zhang, Deblina Bhattacharjee, Sabine Süsstrunk, Mathieu Salzmann. (2024)<br><strong>OMH: Structured Sparsity via Optimally Matched Hierarchy for Unsupervised Semantic Segmentation</strong><br><button class=copy-to-clipboard title="OMH: Structured Sparsity via Optimally Matched Hierarchy for Unsupervised Semantic Segmentation" index=79>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-79 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 23<br>Keywords: Clustering, Self-supervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06546v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06546v1.pdf filename=2403.06546v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Unsupervised</b> Semantic Segmentation (USS) involves segmenting images without relying on predefined labels, aiming to alleviate the burden of extensive human labeling. Existing methods utilize features generated by <b>self-supervised</b> models and specific priors for <b>clustering.</b> However, their <b>clustering</b> objectives are not involved in the optimization of the features during training. Additionally, due to the lack of clear class definitions in USS, the resulting segments may not align well with the <b>clustering</b> objective. In this paper, we introduce a novel approach called Optimally Matched Hierarchy (OMH) to simultaneously address the above issues. The core of our method lies in imposing structured sparsity on the feature space, which allows the features to encode information with different levels of granularity. The structure of this sparsity stems from our hierarchy (OMH). To achieve this, we learn a soft but sparse hierarchy among parallel clusters through Optimal Transport. Our OMH yields better <b>unsupervised</b> segmentation performance compared to existing USS methods. Our extensive experiments demonstrate the benefits of OMH when utilizing our differentiable paradigm. We will make our code publicly available.</p></p class="citation"></blockquote><h3 id=4197--80318-sardet-100k-towards-open-source-benchmark-and-toolkit-for-large-scale-sar-object-detection-yuxuan-li-et-al-2024>(41/97 | 80/318) SARDet-100K: Towards Open-Source Benchmark and ToolKit for Large-Scale SAR Object Detection (Yuxuan Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuxuan Li, Xiang Li, Weijie Li, Qibin Hou, Li Liu, Ming-Ming Cheng, Jian Yang. (2024)<br><strong>SARDet-100K: Towards Open-Source Benchmark and ToolKit for Large-Scale SAR Object Detection</strong><br><button class=copy-to-clipboard title="SARDet-100K: Towards Open-Source Benchmark and ToolKit for Large-Scale SAR Object Detection" index=80>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-80 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CE, cs-CV, cs-LG, cs.CV<br>Keyword Score: 23<br>Keywords: Object Detection, Benchmarking, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06534v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06534v1.pdf filename=2403.06534v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Synthetic Aperture Radar (SAR) <b>object</b> <b>detection</b> has gained significant attention recently due to its irreplaceable all-weather imaging capabilities. However, this research field suffers from both limited public datasets (mostly comprising &lt;2K images with only mono-category <b>objects)</b> <b>and</b> inaccessible source code. To tackle these challenges, we establish a new <b>benchmark</b> dataset and an open-source method for large-scale SAR <b>object</b> <b>detection.</b> Our dataset, SARDet-100K, is a result of intense surveying, collecting, and standardizing 10 existing SAR detection datasets, providing a large-scale and diverse dataset for research purposes. To the best of our knowledge, SARDet-100K is the first COCO-level large-scale multi-class SAR <b>object</b> <b>detection</b> dataset ever created. With this high-quality dataset, we conducted comprehensive experiments and uncovered a crucial challenge in SAR <b>object</b> <b>detection:</b> the substantial disparities between the pretraining on RGB datasets and <b>finetuning</b> on SAR datasets in terms of both data domain and model structure. To bridge these gaps, we propose a novel Multi-Stage with Filter Augmentation (MSFA) pretraining framework that tackles the problems from the perspective of data input, domain transition, and model migration. The proposed MSFA method significantly enhances the performance of SAR <b>object</b> <b>detection</b> models while demonstrating exceptional generalizability and flexibility across diverse models. This work aims to pave the way for further advancements in SAR <b>object</b> <b>detection.</b> The dataset and code is available at <a href=https://github.com/zcablii/SARDet_100K>https://github.com/zcablii/SARDet_100K</a>.</p></p class="citation"></blockquote><h3 id=4297--81318-eliminating-warping-shakes-for-unsupervised-online-video-stitching-lang-nie-et-al-2024>(42/97 | 81/318) Eliminating Warping Shakes for Unsupervised Online Video Stitching (Lang Nie et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lang Nie, Chunyu Lin, Kang Liao, Yun Zhang, Shuaicheng Liu, Yao Zhao. (2024)<br><strong>Eliminating Warping Shakes for Unsupervised Online Video Stitching</strong><br><button class=copy-to-clipboard title="Eliminating Warping Shakes for Unsupervised Online Video Stitching" index=81>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-81 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Benchmarking, Unsupervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06378v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06378v1.pdf filename=2403.06378v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we retarget video stitching to an emerging issue, named warping shake, when extending image stitching to video stitching. It unveils the temporal instability of warped content in non-overlapping regions, despite image stitching having endeavored to preserve the natural structures. Therefore, in most cases, even if the input videos to be stitched are stable, the stitched video will inevitably cause undesired warping shakes and affect the visual experience. To eliminate the shakes, we propose StabStitch to simultaneously realize video stitching and video stabilization in a unified <b>unsupervised</b> <b>learning</b> framework. Starting from the camera paths in video stabilization, we first derive the expression of stitching trajectories in video stitching by elaborately integrating spatial and temporal warps. Then a warp smoothing model is presented to optimize them with a comprehensive consideration regarding content alignment, trajectory smoothness, spatial consistency, and online collaboration. To establish an evaluation <b>benchmark</b> and train the learning framework, we build a video stitching dataset with a rich diversity in camera motions and scenes. Compared with existing stitching solutions, StabStitch exhibits significant superiority in scene robustness and inference speed in addition to stitching and stabilization performance, contributing to a robust and real-time online video stitching system. The code and dataset will be available at <a href=https://github.com/nie-lang/StabStitch>https://github.com/nie-lang/StabStitch</a>.</p></p class="citation"></blockquote><h3 id=4397--82318-optimizing-latent-graph-representations-of-surgical-scenes-for-zero-shot-domain-transfer-siddhant-satyanaik-et-al-2024>(43/97 | 82/318) Optimizing Latent Graph Representations of Surgical Scenes for Zero-Shot Domain Transfer (Siddhant Satyanaik et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Siddhant Satyanaik, Aditya Murali, Deepak Alapatt, Xin Wang, Pietro Mascagni, Nicolas Padoy. (2024)<br><strong>Optimizing Latent Graph Representations of Surgical Scenes for Zero-Shot Domain Transfer</strong><br><button class=copy-to-clipboard title="Optimizing Latent Graph Representations of Surgical Scenes for Zero-Shot Domain Transfer" index=82>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-82 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 21<br>Keywords: Graph, Benchmarking, Representation Learning, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06953v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06953v1.pdf filename=2403.06953v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Purpose: Advances in deep learning have resulted in effective models for surgical video analysis; however, these models often fail to generalize across medical centers due to domain shift caused by variations in surgical workflow, camera setups, and patient demographics. Recently, object-centric learning has emerged as a promising approach for improved surgical scene understanding, capturing and disentangling visual and semantic properties of surgical tools and anatomy to improve downstream task performance. In this work, we conduct a multi-centric performance <b>benchmark</b> of object-centric approaches, focusing on Critical View of Safety assessment in laparoscopic cholecystectomy, then propose an improved approach for unseen domain generalization. Methods: We evaluate four object-centric approaches for domain generalization, establishing baseline performance. Next, leveraging the disentangled nature of object-centric <b>representations,</b> <b>we</b> dissect one of these methods through a series of ablations (e.g. ignoring either visual or semantic features for downstream classification). Finally, based on the results of these ablations, we develop an optimized method specifically tailored for domain generalization, LG-DG, that includes a novel disentanglement loss function. Results: Our optimized approach, LG-DG, achieves an improvement of 9.28% over the best baseline approach. More broadly, we show that object-centric approaches are highly effective for domain generalization thanks to their modular approach to <b>representation</b> <b>learning.</b> Conclusion: We investigate the use of object-centric methods for unseen domain generalization, identify method-agnostic factors critical for performance, and present an optimized approach that substantially outperforms existing methods.</p></p class="citation"></blockquote><h3 id=4497--83318-liso-lidar-only-self-supervised-3d-object-detection-stefan-baur-et-al-2024>(44/97 | 83/318) LISO: Lidar-only Self-Supervised 3D Object Detection (Stefan Baur et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Stefan Baur, Frank Moosmann, Andreas Geiger. (2024)<br><strong>LISO: Lidar-only Self-Supervised 3D Object Detection</strong><br><button class=copy-to-clipboard title="LISO: Lidar-only Self-Supervised 3D Object Detection" index=83>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-83 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Object Detection, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07071v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07071v1.pdf filename=2403.07071v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>3D <b>object</b> <b>detection</b> is one of the most important components in any Self-Driving stack, but current state-of-the-art (SOTA) lidar <b>object</b> <b>detectors</b> require costly & slow manual annotation of 3D bounding boxes to perform well. Recently, several methods emerged to generate pseudo ground truth without human supervision, however, all of these methods have various drawbacks: Some methods require sensor rigs with full camera coverage and accurate calibration, partly supplemented by an auxiliary optical flow engine. Others require expensive high-precision localization to find <b>objects</b> <b>that</b> disappeared over multiple drives. We introduce a novel <b>self-supervised</b> method to train SOTA lidar <b>object</b> <b>detection</b> networks which works on unlabeled sequences of lidar point clouds only, which we call trajectory-regularized self-training. It utilizes a SOTA <b>self-supervised</b> lidar scene flow network under the hood to generate, track, and iteratively refine pseudo ground truth. We demonstrate the effectiveness of our approach for multiple SOTA <b>object</b> <b>detection</b> networks across multiple real-world datasets. Code will be released.</p></p class="citation"></blockquote><h3 id=4597--84318-explainable-transformer-prototypes-for-medical-diagnoses-ugur-demir-et-al-2024>(45/97 | 84/318) Explainable Transformer Prototypes for Medical Diagnoses (Ugur Demir et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ugur Demir, Debesh Jha, Zheyuan Zhang, Elif Keles, Bradley Allen, Aggelos K. Katsaggelos, Ulas Bagci. (2024)<br><strong>Explainable Transformer Prototypes for Medical Diagnoses</strong><br><button class=copy-to-clipboard title="Explainable Transformer Prototypes for Medical Diagnoses" index=84>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-84 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06961v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06961v1.pdf filename=2403.06961v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deployments of artificial intelligence in medical diagnostics mandate not just accuracy and efficacy but also trust, emphasizing the need for explainability in machine decisions. The recent trend in automated medical image diagnostics leans towards the deployment of <b>Transformer-based</b> architectures, credited to their impressive capabilities. Since the <b>self-attention</b> feature of <b>transformers</b> contributes towards identifying crucial regions during the classification process, they enhance the trustability of the methods. However, the complex intricacies of these attention mechanisms may fall short of effectively pinpointing the regions of interest directly influencing AI decisions. Our research endeavors to innovate a unique attention block that underscores the correlation between &lsquo;regions&rsquo; rather than &lsquo;pixels&rsquo;. To address this challenge, we introduce an innovative system grounded in prototype learning, featuring an advanced <b>self-attention</b> mechanism that goes beyond conventional ad-hoc visual explanation techniques by offering comprehensible visual insights. A combined quantitative and qualitative methodological approach was used to demonstrate the effectiveness of the proposed method on the large-scale NIH chest X-ray dataset. Experimental results showed that our proposed method offers a promising direction for explainability, which can lead to the development of more trustable systems, which can facilitate easier and rapid adoption of such technology into routine clinics. The code is available at <a href=https://www.github.com/NUBagcilab/r2r_proto>www.github.com/NUBagcilab/r2r_proto</a>.</p></p class="citation"></blockquote><h3 id=4697--85318-deadiff-an-efficient-stylization-diffusion-model-with-disentangled-representations-tianhao-qi-et-al-2024>(46/97 | 85/318) DEADiff: An Efficient Stylization Diffusion Model with Disentangled Representations (Tianhao Qi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianhao Qi, Shancheng Fang, Yanze Wu, Hongtao Xie, Jiawei Liu, Lang Chen, Qian He, Yongdong Zhang. (2024)<br><strong>DEADiff: An Efficient Stylization Diffusion Model with Disentangled Representations</strong><br><button class=copy-to-clipboard title="DEADiff: An Efficient Stylization Diffusion Model with Disentangled Representations" index=85>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-85 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06951v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06951v2.pdf filename=2403.06951v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The <b>diffusion-based</b> <b>text-to-image</b> model harbors immense potential in transferring reference style. However, current encoder-based approaches significantly impair the text controllability of <b>text-to-image</b> models while transferring styles. In this paper, we introduce DEADiff to address this issue using the following two strategies: 1) a mechanism to decouple the style and semantics of reference images. The decoupled feature representations are first extracted by Q-Formers which are instructed by different text descriptions. Then they are injected into mutually exclusive subsets of cross-attention layers for better disentanglement. 2) A non-reconstructive learning method. The Q-Formers are trained using paired images rather than the identical target, in which the reference image and the ground-truth image are with the same style or semantics. We show that DEADiff attains the best visual stylization results and optimal balance between the text controllability inherent in the <b>text-to-image</b> model and style similarity to the reference image, as demonstrated both quantitatively and qualitatively. Our project page is <a href=https://tianhao-qi.github.io/DEADiff/>https://tianhao-qi.github.io/DEADiff/</a>.</p></p class="citation"></blockquote><h3 id=4797--86318-cood-combined-out-of-distribution-detection-using-multiple-measures-for-anomaly--novel-class-detection-in-large-scale-hierarchical-classification-l-e-hogeweg-et-al-2024>(47/97 | 86/318) COOD: Combined out-of-distribution detection using multiple measures for anomaly & novel class detection in large-scale hierarchical classification (L. E. Hogeweg et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>L. E. Hogeweg, R. Gangireddy, D. Brunink, V. J. Kalkman, L. Cornelissen, J. W. Kamminga. (2024)<br><strong>COOD: Combined out-of-distribution detection using multiple measures for anomaly & novel class detection in large-scale hierarchical classification</strong><br><button class=copy-to-clipboard title="COOD: Combined out-of-distribution detection using multiple measures for anomaly & novel class detection in large-scale hierarchical classification" index=86>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-86 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 20<br>Keywords: Out-of-distribution, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06874v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06874v1.pdf filename=2403.06874v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>High-performing <b>out-of-distribution</b> (OOD) detection, both anomaly and novel class, is an important prerequisite for the practical use of classification models. In this paper, we focus on the species recognition task in images concerned with large databases, a large number of fine-grained hierarchical classes, severe class imbalance, and varying image quality. We propose a framework for combining individual OOD measures into one combined OOD (COOD) measure using a <b>supervised</b> model. The individual measures are several existing state-of-the-art measures and several novel OOD measures developed with novel class detection and hierarchical class structure in mind. COOD was extensively evaluated on three large-scale (500k+ images) biodiversity datasets in the context of anomaly and novel class detection. We show that COOD outperforms individual, including state-of-the-art, OOD measures by a large margin in terms of TPR@1% FPR in the majority of experiments, e.g., improving detecting ImageNet images (OOD) from 54.3% to 85.4% for the iNaturalist 2018 dataset. SHAP (feature contribution) analysis shows that different individual OOD measures are essential for various tasks, indicating that multiple OOD measures and combinations are needed to generalize. Additionally, we show that explicitly considering ID images that are incorrectly classified for the original (species) recognition task is important for constructing high-performing OOD detection methods and for practical applicability. The framework can easily be extended or adapted to other tasks and media modalities.</p></p class="citation"></blockquote><h3 id=4897--87318-drivedreamer-2-llm-enhanced-world-models-for-diverse-driving-video-generation-guosheng-zhao-et-al-2024>(48/97 | 87/318) DriveDreamer-2: LLM-Enhanced World Models for Diverse Driving Video Generation (Guosheng Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guosheng Zhao, Xiaofeng Wang, Zheng Zhu, Xinze Chen, Guan Huang, Xiaoyi Bao, Xingang Wang. (2024)<br><strong>DriveDreamer-2: LLM-Enhanced World Models for Diverse Driving Video Generation</strong><br><button class=copy-to-clipboard title="DriveDreamer-2: LLM-Enhanced World Models for Diverse Driving Video Generation" index=87>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-87 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06845v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06845v1.pdf filename=2403.06845v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>World models have demonstrated superiority in autonomous driving, particularly in the generation of multi-view driving videos. However, significant challenges still exist in generating customized driving videos. In this paper, we propose DriveDreamer-2, which builds upon the framework of DriveDreamer and incorporates a <b>Large</b> <b>Language</b> <b>Model</b> <b>(LLM)</b> to generate user-defined driving videos. Specifically, an <b>LLM</b> interface is initially incorporated to convert a user&rsquo;s query into agent trajectories. Subsequently, a HDMap, adhering to traffic regulations, is generated based on the trajectories. Ultimately, we propose the Unified Multi-View Model to enhance temporal and spatial coherence in the generated driving videos. DriveDreamer-2 is the first world model to generate customized driving videos, it can generate uncommon driving videos (e.g., vehicles abruptly cut in) in a user-friendly manner. Besides, experimental results demonstrate that the generated videos enhance the training of driving perception methods (e.g., 3D detection and tracking). Furthermore, video generation quality of DriveDreamer-2 surpasses other state-of-the-art methods, showcasing FID and FVD scores of 11.2 and 55.7, representing relative improvements of 30% and 50%.</p></p class="citation"></blockquote><h3 id=4997--88318-medical-image-synthesis-via-fine-grained-image-text-alignment-and-anatomy-pathology-prompting-wenting-chen-et-al-2024>(49/97 | 88/318) Medical Image Synthesis via Fine-Grained Image-Text Alignment and Anatomy-Pathology Prompting (Wenting Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenting Chen, Pengyu Wang, Hui Ren, Lichao Sun, Quanzheng Li, Yixuan Yuan, Xiang Li. (2024)<br><strong>Medical Image Synthesis via Fine-Grained Image-Text Alignment and Anatomy-Pathology Prompting</strong><br><button class=copy-to-clipboard title="Medical Image Synthesis via Fine-Grained Image-Text Alignment and Anatomy-Pathology Prompting" index=88>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-88 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CL, cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Image2text, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06835v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06835v1.pdf filename=2403.06835v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Data scarcity and privacy concerns limit the availability of high-quality medical images for public use, which can be mitigated through medical image synthesis. However, current medical image synthesis methods often struggle to accurately capture the complexity of detailed anatomical structures and pathological conditions. To address these challenges, we propose a novel medical image synthesis model that leverages fine-grained <b>image-text</b> alignment and anatomy-pathology <b>prompts</b> to generate highly detailed and accurate synthetic medical images. Our method integrates advanced natural language processing techniques with image generative modeling, enabling precise alignment between descriptive text <b>prompts</b> and the synthesized images&rsquo; anatomical and pathological details. The proposed approach consists of two key components: an anatomy-pathology <b>prompting</b> module and a fine-grained alignment-based synthesis module. The anatomy-pathology <b>prompting</b> module automatically generates descriptive <b>prompts</b> for high-quality medical images. To further synthesize high-quality medical images from the generated <b>prompts,</b> the fine-grained alignment-based synthesis module pre-defines a visual codebook for the radiology dataset and performs fine-grained alignment between the codebook and generated <b>prompts</b> to obtain key patches as visual clues, facilitating accurate image synthesis. We validate the superiority of our method through experiments on public chest X-ray datasets and demonstrate that our synthetic images preserve accurate semantic information, making them valuable for various medical applications.</p></p class="citation"></blockquote><h3 id=5097--89318-hdrtransdc-high-dynamic-range-image-reconstruction-with-transformer-deformation-convolution-shuaikang-shang-et-al-2024>(50/97 | 89/318) HDRTransDC: High Dynamic Range Image Reconstruction with Transformer Deformation Convolution (Shuaikang Shang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shuaikang Shang, Xuejing Kang, Anlong Ming. (2024)<br><strong>HDRTransDC: High Dynamic Range Image Reconstruction with Transformer Deformation Convolution</strong><br><button class=copy-to-clipboard title="HDRTransDC: High Dynamic Range Image Reconstruction with Transformer Deformation Convolution" index=89>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-89 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Convolution, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06831v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06831v1.pdf filename=2403.06831v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>High Dynamic Range (HDR) imaging aims to generate an artifact-free HDR image with realistic details by fusing multi-exposure Low Dynamic Range (LDR) images. Caused by large motion and severe under-/over-exposure among input LDR images, HDR imaging suffers from ghosting artifacts and fusion distortions. To address these critical issues, we propose an HDR <b>Transformer</b> Deformation <b>Convolution</b> (HDRTransDC) network to generate high-quality HDR images, which consists of the <b>Transformer</b> Deformable <b>Convolution</b> Alignment Module (TDCAM) and the Dynamic Weight Fusion Block (DWFB). To solve the ghosting artifacts, the proposed TDCAM extracts long-distance content similar to the reference feature in the entire non-reference features, which can accurately remove misalignment and fill the content occluded by moving objects. For the purpose of eliminating fusion distortions, we propose DWFB to spatially adaptively select useful information across frames to effectively fuse multi-exposed features. Extensive experiments show that our method quantitatively and qualitatively achieves state-of-the-art performance.</p></p class="citation"></blockquote><h3 id=5197--90318-data-independent-operator-a-training-free-artifact-representation-extractor-for-generalizable-deepfake-detection-chuangchuang-tan-et-al-2024>(51/97 | 90/318) Data-Independent Operator: A Training-Free Artifact Representation Extractor for Generalizable Deepfake Detection (Chuangchuang Tan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chuangchuang Tan, Ping Liu, RenShuai Tao, Huan Liu, Yao Zhao, Baoyuan Wu, Yunchao Wei. (2024)<br><strong>Data-Independent Operator: A Training-Free Artifact Representation Extractor for Generalizable Deepfake Detection</strong><br><button class=copy-to-clipboard title="Data-Independent Operator: A Training-Free Artifact Representation Extractor for Generalizable Deepfake Detection" index=90>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-90 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Convolution, Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06803v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06803v1.pdf filename=2403.06803v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, the proliferation of increasingly realistic synthetic images generated by various <b>generative</b> <b>adversarial</b> <b>networks</b> has increased the risk of misuse. Consequently, there is a pressing need to develop a generalizable detector for accurately recognizing fake images. The conventional methods rely on generating diverse training sources or large pretrained models. In this work, we show that, on the contrary, the small and training-free filter is sufficient to capture more general artifact representations. Due to its unbias towards both the training and test sources, we define it as Data-Independent Operator (DIO) to achieve appealing improvements on unseen sources. In our framework, handcrafted filters and the randomly-initialized <b>convolutional</b> layer can be used as the training-free artifact representations extractor with excellent results. With the data-independent operator of a popular classifier, such as Resnet50, one could already reach a new state-of-the-art without bells and whistles. We evaluate the effectiveness of the DIO on 33 generation models, even DALLE and Midjourney. Our detector achieves a remarkable improvement of $13.3%$, establishing a new state-of-the-art performance. The DIO and its extension can serve as strong baselines for future methods. The code is available at \url{https://github.com/chuangchuangtan/Data-Independent-Operator}.</p></p class="citation"></blockquote><h3 id=5297--91318-genetic-learning-for-designing-sim-to-real-data-augmentations-bram-vanherle-et-al-2024>(52/97 | 91/318) Genetic Learning for Designing Sim-to-Real Data Augmentations (Bram Vanherle et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bram Vanherle, Nick Michiels, Frank Van Reeth. (2024)<br><strong>Genetic Learning for Designing Sim-to-Real Data Augmentations</strong><br><button class=copy-to-clipboard title="Genetic Learning for Designing Sim-to-Real Data Augmentations" index=91>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-91 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Object Detection, Data Augmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06786v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06786v1.pdf filename=2403.06786v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Data</b> <b>augmentations</b> are useful in closing the sim-to-real domain gap when training on synthetic <b>data.</b> <b>This</b> is because they widen the training <b>data</b> <b>distribution,</b> thus encouraging the model to generalize better to other domains. Many image augmentation techniques exist, parametrized by different settings, such as strength and probability. This leads to a large space of different possible augmentation policies. Some policies work better than others for overcoming the sim-to-real gap for specific datasets, and it is unclear why. This paper presents two different interpretable metrics that can be combined to predict how well a certain augmentation policy will work for a specific sim-to-real setting, focusing on <b>object</b> <b>detection.</b> We validate our metrics by training many models with different augmentation policies and showing a strong correlation with performance on real <b>data.</b> <b>Additionally,</b> we introduce GeneticAugment, a genetic programming method that can leverage these metrics to automatically design an augmentation policy for a specific dataset without needing to train a model.</p></p class="citation"></blockquote><h3 id=5397--92318-an-image-is-worth-12-tokens-after-layer-2-plug-and-play-inference-acceleration-for-large-vision-language-models-liang-chen-et-al-2024>(53/97 | 92/318) An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference Acceleration for Large Vision-Language Models (Liang Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Liang Chen, Haozhe Zhao, Tianyu Liu, Shuai Bai, Junyang Lin, Chang Zhou, Baobao Chang. (2024)<br><strong>An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference Acceleration for Large Vision-Language Models</strong><br><button class=copy-to-clipboard title="An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference Acceleration for Large Vision-Language Models" index=92>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-92 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CL, cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Pruning, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06764v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06764v1.pdf filename=2403.06764v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this study, we identify the inefficient attention phenomena in Large <b>Vision-Language</b> Models (LVLMs), notably within prominent models like LLaVA-1.5, QwenVL-Chat and Video-LLaVA. We find out that the attention computation over visual tokens is of extreme inefficiency in the deep layers of popular LVLMs, suggesting a need for a sparser approach compared to textual data handling. To this end, we introduce FastV, a versatile plug-and-play method designed to optimize computational efficiency by learning adaptive attention patterns in early layers and <b>pruning</b> visual tokens in subsequent ones. Our evaluations demonstrate FastV&rsquo;s ability to dramatically reduce computational costs (e.g., a 45 reduction in FLOPs for LLaVA-1.5-13B) without sacrificing performance in a wide range of image and video understanding tasks. The computational efficiency and performance trade-off of FastV are highly customizable and pareto-efficient. It can compress the FLOPs of a 13B-parameter model to achieve a lower budget than that of a 7B-parameter model, while still maintaining superior performance. We believe FastV has practical values for deployment of LVLMs in edge devices and commercial models. Code is released at <a href=https://github.com/pkunlp-icler/FastV>https://github.com/pkunlp-icler/FastV</a>.</p></p class="citation"></blockquote><h3 id=5497--93318-distribution-aware-data-expansion-with-diffusion-models-haowei-zhu-et-al-2024>(54/97 | 93/318) Distribution-Aware Data Expansion with Diffusion Models (Haowei Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haowei Zhu, Ling Yang, Jun-Hai Yong, Wentao Zhang, Bin Wang. (2024)<br><strong>Distribution-Aware Data Expansion with Diffusion Models</strong><br><button class=copy-to-clipboard title="Distribution-Aware Data Expansion with Diffusion Models" index=93>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-93 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06741v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06741v1.pdf filename=2403.06741v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The scale and quality of a dataset significantly impact the performance of deep models. However, acquiring large-scale annotated datasets is both a costly and time-consuming endeavor. To address this challenge, dataset expansion technologies aim to automatically augment datasets, unlocking the full potential of deep models. Current data expansion methods encompass image transformation-based and synthesis-based methods. The transformation-based methods introduce only local variations, resulting in poor diversity. While image synthesis-based methods can create entirely new content, significantly enhancing informativeness. However, existing synthesis methods carry the risk of distribution deviations, potentially degrading model performance with <b>out-of-distribution</b> samples. In this paper, we propose DistDiff, an effective data expansion framework based on the distribution-aware <b>diffusion</b> <b>model.</b> DistDiff constructs hierarchical prototypes to approximate the real data distribution, optimizing latent data points within <b>diffusion</b> <b>models</b> with hierarchical energy guidance. We demonstrate its ability to generate distribution-consistent samples, achieving substantial improvements in data expansion tasks. Specifically, without additional training, DistDiff achieves a 30.7% improvement in accuracy across six image datasets compared to the model trained on original datasets and a 9.8% improvement compared to the state-of-the-art <b>diffusion-based</b> <b>method.</b> Our code is available at <a href=https://github.com/haoweiz23/DistDiff>https://github.com/haoweiz23/DistDiff</a></p></p class="citation"></blockquote><h3 id=5597--94318-pcld-point-cloud-layerwise-diffusion-for-adversarial-purification-mert-gulsen-et-al-2024>(55/97 | 94/318) PCLD: Point Cloud Layerwise Diffusion for Adversarial Purification (Mert Gulsen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mert Gulsen, Batuhan Cengiz, Yusuf H. Sahin, Gozde Unal. (2024)<br><strong>PCLD: Point Cloud Layerwise Diffusion for Adversarial Purification</strong><br><button class=copy-to-clipboard title="PCLD: Point Cloud Layerwise Diffusion for Adversarial Purification" index=94>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-94 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Adversarial Attack, Adversarial Purification<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06698v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06698v1.pdf filename=2403.06698v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Point clouds are extensively employed in a variety of real-world applications such as robotics, autonomous driving and augmented reality. Despite the recent success of point cloud neural networks, especially for safety-critical tasks, it is essential to also ensure the robustness of the model. A typical way to assess a model&rsquo;s robustness is through <b>adversarial</b> <b>attacks,</b> where test-time examples are generated based on gradients to deceive the model. While many different defense mechanisms are studied in 2D, studies on 3D point clouds have been relatively limited in the academic field. Inspired from PointDP, which denoises the network inputs by diffusion, we propose Point Cloud Layerwise Diffusion (PCLD), a layerwise diffusion based 3D point cloud defense strategy. Unlike PointDP, we propagated the diffusion denoising after each layer to incrementally enhance the results. We apply our defense method to different types of commonly used point cloud models and <b>adversarial</b> <b>attacks</b> to evaluate its robustness. Our experiments demonstrate that the proposed defense method achieved results that are comparable to or surpass those of existing methodologies, establishing robustness through a novel technique. Code is available at <a href=https://github.com/batuceng/diffusion-layer-robustness-pc>https://github.com/batuceng/diffusion-layer-robustness-pc</a>.</p></p class="citation"></blockquote><h3 id=5697--95318-detection-of-object-throwing-behavior-in-surveillance-videos-ivo-p-c-kersten-et-al-2024>(56/97 | 95/318) Detection of Object Throwing Behavior in Surveillance Videos (Ivo P. C. Kersten et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ivo P. C. Kersten, Erkut Akdag, Egor Bondarev, Peter H. N. De With. (2024)<br><strong>Detection of Object Throwing Behavior in Surveillance Videos</strong><br><button class=copy-to-clipboard title="Detection of Object Throwing Behavior in Surveillance Videos" index=95>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-95 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Anomaly Detection, Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06552v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06552v1.pdf filename=2403.06552v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Anomalous behavior detection is a challenging research area within computer vision. Progress in this area enables automated detection of dangerous behavior using surveillance camera feeds. A dangerous behavior that is often overlooked in other research is the throwing action in traffic flow, which is one of the unique requirements of our Smart City project to enhance public safety. This paper proposes a solution for throwing action detection in surveillance videos using deep learning. At present, datasets for throwing actions are not publicly available. To address the use-case of our Smart City project, we first generate the novel public &lsquo;Throwing Action&rsquo; dataset, consisting of 271 videos of throwing actions performed by traffic participants, such as pedestrians, bicyclists, and car drivers, and 130 normal videos without throwing actions. Second, we compare the performance of different feature extractors for our <b>anomaly</b> <b>detection</b> method on the UCF-Crime and Throwing-Action datasets. The explored feature extractors are the <b>Convolutional</b> 3D (C3D) network, the Inflated 3D ConvNet (I3D) network, and the Multi-Fiber Network (MFNet). Finally, the performance of the <b>anomaly</b> <b>detection</b> algorithm is improved by applying the Adam optimizer instead of Adadelta, and proposing a mean normal loss function that covers the multitude of normal situations in traffic. Both aspects yield better <b>anomaly</b> <b>detection</b> performance. Besides this, the proposed mean normal loss function lowers the false alarm rate on the combined dataset. The experimental results reach an area under the ROC curve of 86.10 for the Throwing-Action dataset, and 80.13 on the combined dataset, respectively.</p></p class="citation"></blockquote><h3 id=5797--96318-multi-scale-implicit-transformer-with-re-parameterize-for-arbitrary-scale-super-resolution-jinchen-zhu-et-al-2024>(57/97 | 96/318) Multi-Scale Implicit Transformer with Re-parameterize for Arbitrary-Scale Super-Resolution (Jinchen Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinchen Zhu, Mingjian Zhang, Ling Zheng, Shizhuang Weng. (2024)<br><strong>Multi-Scale Implicit Transformer with Re-parameterize for Arbitrary-Scale Super-Resolution</strong><br><button class=copy-to-clipboard title="Multi-Scale Implicit Transformer with Re-parameterize for Arbitrary-Scale Super-Resolution" index=96>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-96 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06536v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06536v1.pdf filename=2403.06536v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, the methods based on implicit neural representations have shown excellent capabilities for arbitrary-scale super-resolution (ASSR). Although these methods represent the features of an image by generating latent codes, these latent codes are difficult to adapt for different magnification factors of super-resolution, which seriously affects their performance. Addressing this, we design Multi-Scale Implicit <b>Transformer</b> (MSIT), consisting of an Multi-scale Neural Operator (MSNO) and Multi-Scale <b>Self-Attention</b> (MSSA). Among them, MSNO obtains multi-scale latent codes through feature enhancement, multi-scale characteristics extraction, and multi-scale characteristics merging. MSSA further enhances the multi-scale characteristics of latent codes, resulting in better performance. Furthermore, to improve the performance of network, we propose the Re-Interaction Module (RIM) combined with the cumulative training strategy to improve the diversity of learned information for the network. We have systematically introduced multi-scale characteristics for the first time in ASSR, extensive experiments are performed to validate the effectiveness of MSIT, and our method achieves state-of-the-art performance in arbitrary super-resolution tasks.</p></p class="citation"></blockquote><h3 id=5897--97318-advancing-text-driven-chest-x-ray-generation-with-policy-based-reinforcement-learning-woojung-han-et-al-2024>(58/97 | 97/318) Advancing Text-Driven Chest X-Ray Generation with Policy-Based Reinforcement Learning (Woojung Han et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Woojung Han, Chanyoung Kim, Dayun Ju, Yumin Shim, Seong Jae Hwang. (2024)<br><strong>Advancing Text-Driven Chest X-Ray Generation with Policy-Based Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Advancing Text-Driven Chest X-Ray Generation with Policy-Based Reinforcement Learning" index=97>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-97 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06516v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06516v1.pdf filename=2403.06516v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advances in text-conditioned image generation <b>diffusion</b> <b>models</b> have begun paving the way for new opportunities in modern medical domain, in particular, generating Chest X-rays (CXRs) from diagnostic reports. Nonetheless, to further drive the <b>diffusion</b> <b>models</b> to generate CXRs that faithfully reflect the complexity and diversity of real data, it has become evident that a nontrivial learning approach is needed. In light of this, we propose CXRL, a framework motivated by the potential of <b>reinforcement</b> <b>learning</b> (RL). Specifically, we integrate a policy gradient RL approach with well-designed multiple distinctive CXR-domain specific reward models. This approach guides the <b>diffusion</b> <b>denoising</b> trajectory, achieving precise CXR posture and pathological details. Here, considering the complex medical image environment, we present &ldquo;RL with Comparative Feedback&rdquo; (RLCF) for the reward mechanism, a human-like comparative evaluation that is known to be more effective and reliable in complex scenarios compared to direct evaluation. Our CXRL framework includes jointly optimizing learnable adaptive condition embeddings (ACE) and the image generator, enabling the model to produce more accurate and higher perceptual CXR quality. Our extensive evaluation of the MIMIC-CXR-JPG dataset demonstrates the effectiveness of our RL-based tuning approach. Consequently, our CXRL generates pathologically realistic CXRs, establishing a new standard for generating CXRs with high fidelity to real-world clinical scenarios.</p></p class="citation"></blockquote><h3 id=5997--98318-towards-the-uncharted-density-descending-feature-perturbation-for-semi-supervised-semantic-segmentation-xiaoyang-wang-et-al-2024>(59/97 | 98/318) Towards the Uncharted: Density-Descending Feature Perturbation for Semi-supervised Semantic Segmentation (Xiaoyang Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaoyang Wang, Huihui Bai, Limin Yu, Yao Zhao, Jimin Xiao. (2024)<br><strong>Towards the Uncharted: Density-Descending Feature Perturbation for Semi-supervised Semantic Segmentation</strong><br><button class=copy-to-clipboard title="Towards the Uncharted: Density-Descending Feature Perturbation for Semi-supervised Semantic Segmentation" index=98>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-98 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Semi-Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06462v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06462v2.pdf filename=2403.06462v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Semi-supervised</b> <b>semantic</b> segmentation allows model to mine effective supervision from unlabeled data to complement label-guided training. Recent research has primarily focused on consistency regularization techniques, exploring perturbation-invariant training at both the image and feature levels. In this work, we proposed a novel feature-level consistency learning framework named Density-Descending Feature Perturbation (DDFP). Inspired by the low-density separation assumption in <b>semi-supervised</b> <b>learning,</b> our key insight is that feature density can shed a light on the most promising direction for the segmentation classifier to explore, which is the regions with lower density. We propose to shift features with confident predictions towards lower-density regions by perturbation injection. The perturbed features are then <b>supervised</b> by the predictions on the original features, thereby compelling the classifier to explore less dense regions to effectively regularize the decision boundary. Central to our method is the estimation of feature density. To this end, we introduce a lightweight density estimator based on normalizing flow, allowing for efficient capture of the feature density distribution in an online manner. By extracting gradients from the density estimator, we can determine the direction towards less dense regions for each feature. The proposed DDFP outperforms other designs on feature-level perturbations and shows state of the art performances on both Pascal VOC and Cityscapes dataset under various partition protocols. The project is available at <a href=https://github.com/Gavinwxy/DDFP>https://github.com/Gavinwxy/DDFP</a>.</p></p class="citation"></blockquote><h3 id=6097--99318-refining-segmentation-on-the-fly-an-interactive-framework-for-point-cloud-semantic-segmentation-peng-zhang-et-al-2024>(60/97 | 99/318) Refining Segmentation On-the-Fly: An Interactive Framework for Point Cloud Semantic Segmentation (Peng Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Peng Zhang, Ting Wu, Jinsheng Sun, Weiqing Li, Zhiyong Su. (2024)<br><strong>Refining Segmentation On-the-Fly: An Interactive Framework for Point Cloud Semantic Segmentation</strong><br><button class=copy-to-clipboard title="Refining Segmentation On-the-Fly: An Interactive Framework for Point Cloud Semantic Segmentation" index=99>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-99 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06401v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06401v1.pdf filename=2403.06401v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing interactive point cloud segmentation approaches primarily focus on the object segmentation, which aim to determine which points belong to the object of interest guided by user interactions. This paper concentrates on an unexplored yet meaningful task, i.e., interactive point cloud semantic segmentation, which assigns high-quality semantic labels to all points in a scene with user corrective clicks. Concretely, we presents the first interactive framework for point cloud semantic segmentation, named InterPCSeg, which seamlessly integrates with off-the-shelf semantic segmentation networks without offline re-training, enabling it to run in an on-the-fly manner. To achieve online refinement, we treat user interactions as sparse training examples during the test-time. To address the instability caused by the sparse supervision, we design a stabilization energy to regulate the test-time training process. For objective and reproducible evaluation, we develop an interaction <b>simulation</b> scheme tailored for the interactive point cloud semantic segmentation task. We evaluate our framework on the S3DIS and ScanNet datasets with off-the-shelf segmentation networks, incorporating interactions from both the proposed interaction simulator and real users. Quantitative and qualitative experimental results demonstrate the efficacy of our framework in refining the semantic segmentation results with user interactions. The source code will be publicly available.</p></p class="citation"></blockquote><h3 id=6197--100318-3d-semantic-segmentation-driven-representations-for-3d-object-detection-hayeon-o-et-al-2024>(61/97 | 100/318) 3D Semantic Segmentation-Driven Representations for 3D Object Detection (Hayeon O et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hayeon O, Kunsoo Huh. (2024)<br><strong>3D Semantic Segmentation-Driven Representations for 3D Object Detection</strong><br><button class=copy-to-clipboard title="3D Semantic Segmentation-Driven Representations for 3D Object Detection" index=100>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-100 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 16<br>Keywords: Object Detection, Benchmarking, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06501v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06501v1.pdf filename=2403.06501v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In autonomous driving, 3D detection provides more precise information to downstream tasks, including path planning and motion estimation, compared to 2D detection. Therefore, the need for 3D detection research has emerged. However, although single and multi-view images and depth maps obtained from the camera were used, detection accuracy was relatively low compared to other modality-based detectors due to the lack of geometric information. The proposed <b>multi-modal</b> 3D <b>object</b> <b>detection</b> combines semantic features obtained from images and geometric features obtained from point clouds, but there are difficulties in defining unified representation to fuse data existing in different domains and synchronization between them. In this paper, we propose SeSame : point-wise semantic feature as a new presentation to ensure sufficient semantic information of the existing LiDAR-only based 3D detection. Experiments show that our approach outperforms previous state-of-the-art at different levels of difficulty in car and performance improvement on the KITTI <b>object</b> <b>detection</b> <b>benchmark.</b> Our code is available at <a href=https://github.com/HAMA-DL-dev/SeSame>https://github.com/HAMA-DL-dev/SeSame</a></p></p class="citation"></blockquote><h3 id=6297--101318-memory-based-adapters-for-online-3d-scene-perception-xiuwei-xu-et-al-2024>(62/97 | 101/318) Memory-based Adapters for Online 3D Scene Perception (Xiuwei Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiuwei Xu, Chong Xia, Ziwei Wang, Linqing Zhao, Yueqi Duan, Jie Zhou, Jiwen Lu. (2024)<br><strong>Memory-based Adapters for Online 3D Scene Perception</strong><br><button class=copy-to-clipboard title="Memory-based Adapters for Online 3D Scene Perception" index=101>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-101 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 15<br>Keywords: Fine-tuning, Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06974v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06974v1.pdf filename=2403.06974v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we propose a new framework for online 3D scene perception. Conventional 3D scene perception methods are offline, i.e., take an already reconstructed 3D scene <b>geometry</b> as input, which is not applicable in robotic applications where the input data is streaming RGB-D videos rather than a complete 3D scene reconstructed from pre-collected RGB-D videos. To deal with online 3D scene perception tasks where data collection and perception should be performed simultaneously, the model should be able to process 3D scenes frame by frame and make use of the temporal information. To this end, we propose an adapter-based plug-and-play module for the backbone of 3D scene perception model, which constructs memory to cache and aggregate the extracted RGB-D features to empower offline models with temporal learning ability. Specifically, we propose a queued memory mechanism to cache the supporting point cloud and image features. Then we devise aggregation modules which directly perform on the memory and pass temporal information to current frame. We further propose 3D-to-2D adapter to enhance image features with strong global context. Our adapters can be easily inserted into mainstream offline architectures of different tasks and significantly boost their performance on online tasks. Extensive experiments on ScanNet and SceneNN datasets demonstrate our approach achieves leading performance on three 3D scene perception tasks compared with state-of-the-art online methods by simply <b>finetuning</b> existing offline models, without any model and task-specific designs. \href{https://xuxw98.github.io/Online3D/}{Project page}.</p></p class="citation"></blockquote><h3 id=6397--102318-dngaussian-optimizing-sparse-view-3d-gaussian-radiance-fields-with-global-local-depth-normalization-jiahe-li-et-al-2024>(63/97 | 102/318) DNGaussian: Optimizing Sparse-View 3D Gaussian Radiance Fields with Global-Local Depth Normalization (Jiahe Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiahe Li, Jiawei Zhang, Xiao Bai, Jin Zheng, Xin Ning, Jun Zhou, Lin Gu. (2024)<br><strong>DNGaussian: Optimizing Sparse-View 3D Gaussian Radiance Fields with Global-Local Depth Normalization</strong><br><button class=copy-to-clipboard title="DNGaussian: Optimizing Sparse-View 3D Gaussian Radiance Fields with Global-Local Depth Normalization" index=102>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-102 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 15<br>Keywords: Few-shot, Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06912v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06912v2.pdf filename=2403.06912v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Radiance fields have demonstrated impressive performance in synthesizing novel views from sparse input views, yet prevailing methods suffer from high training costs and slow inference speed. This paper introduces DNGaussian, a depth-regularized framework based on 3D Gaussian radiance fields, offering real-time and high-quality <b>few-shot</b> novel view synthesis at low costs. Our motivation stems from the highly efficient representation and surprising quality of the recent 3D Gaussian Splatting, despite it will encounter a <b>geometry</b> degradation when input views decrease. In the Gaussian radiance fields, we find this degradation in scene <b>geometry</b> primarily lined to the positioning of Gaussian primitives and can be mitigated by depth constraint. Consequently, we propose a Hard and Soft Depth Regularization to restore accurate scene <b>geometry</b> under coarse monocular depth supervision while maintaining a fine-grained color appearance. To further refine detailed <b>geometry</b> reshaping, we introduce Global-Local Depth Normalization, enhancing the focus on small local depth changes. Extensive experiments on LLFF, DTU, and Blender datasets demonstrate that DNGaussian outperforms state-of-the-art methods, achieving comparable or better results with significantly reduced memory cost, a $25 \times$ reduction in training time, and over $3000 \times$ faster rendering speed.</p></p class="citation"></blockquote><h3 id=6497--103318-skeleton-supervised-airway-segmentation-mingyue-zhao-et-al-2024>(64/97 | 103/318) Skeleton Supervised Airway Segmentation (Mingyue Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mingyue Zhao, Han Li, Li Fan, Shiyuan Liu, Xiaolan Qiu, S. Kevin Zhou. (2024)<br><strong>Skeleton Supervised Airway Segmentation</strong><br><button class=copy-to-clipboard title="Skeleton Supervised Airway Segmentation" index=103>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-103 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 15<br>Keywords: Geometry, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06510v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06510v1.pdf filename=2403.06510v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Fully-supervised airway segmentation has accomplished significant triumphs over the years in aiding pre-operative diagnosis and intra-operative navigation. However, full voxel-level annotation constitutes a labor-intensive and time-consuming task, often plagued by issues such as missing branches, branch annotation discontinuity, or erroneous edge delineation. label-efficient solutions for airway extraction are rarely explored yet primarily demanding in medical practice. To this end, we introduce a novel skeleton-level annotation (SkA) tailored to the airway, which simplifies the annotation workflow while enhancing annotation consistency and accuracy, preserving the complete topology. Furthermore, we propose a skeleton-supervised learning framework to achieve accurate airway segmentation. Firstly, a dual-stream buffer inference is introduced to realize initial label propagation from SkA, avoiding the collapse of direct learning from SkA. Then, we construct a <b>geometry-aware</b> dual-path propagation framework (GDP) to further promote complementary propagation learning, composed of hard <b>geometry-aware</b> propagation learning and soft <b>geometry-aware</b> propagation guidance. Experiments reveal that our proposed framework outperforms the competing methods with SKA, which amounts to only 1.96% airways, and achieves comparable performance with the baseline model that is fully <b>supervised</b> with 100% airways, demonstrating its significant potential in achieving label-efficient segmentation for other tubular structures, such as vessels.</p></p class="citation"></blockquote><h3 id=6597--104318-earthloc-astronaut-photography-localization-by-indexing-earth-from-space-gabriele-berton-et-al-2024>(65/97 | 104/318) EarthLoc: Astronaut Photography Localization by Indexing Earth from Space (Gabriele Berton et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gabriele Berton, Alex Stoken, Barbara Caputo, Carlo Masone. (2024)<br><strong>EarthLoc: Astronaut Photography Localization by Indexing Earth from Space</strong><br><button class=copy-to-clipboard title="EarthLoc: Astronaut Photography Localization by Indexing Earth from Space" index=104>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-104 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Data Augmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06758v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06758v1.pdf filename=2403.06758v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Astronaut photography, spanning six decades of human spaceflight, presents a unique Earth observations dataset with immense value for both scientific research and disaster response. Despite its significance, accurately localizing the geographical extent of these images, crucial for effective utilization, poses substantial challenges. Current manual localization efforts are time-consuming, motivating the need for automated solutions. We propose a novel approach - leveraging image retrieval - to address this challenge efficiently. We introduce innovative training techniques, including Year-Wise <b>Data</b> <b>Augmentation</b> and a Neutral-Aware Multi-Similarity Loss, which contribute to the development of a high-performance model, EarthLoc. We develop six evaluation datasets and perform a comprehensive <b>benchmark</b> comparing EarthLoc to existing methods, showcasing its superior efficiency and accuracy. Our approach marks a significant advancement in automating the localization of astronaut photography, which will help bridge a critical gap in Earth observations <b>data.</b> <b>Code</b> and datasets are available at <a href=https://github.com/gmberton/EarthLoc>https://github.com/gmberton/EarthLoc</a></p></p class="citation"></blockquote><h3 id=6697--105318-transferring-relative-monocular-depth-to-surgical-vision-with-temporal-consistency-charlie-budd-et-al-2024>(66/97 | 105/318) Transferring Relative Monocular Depth to Surgical Vision with Temporal Consistency (Charlie Budd et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Charlie Budd, Tom Vercauteren. (2024)<br><strong>Transferring Relative Monocular Depth to Surgical Vision with Temporal Consistency</strong><br><button class=copy-to-clipboard title="Transferring Relative Monocular Depth to Surgical Vision with Temporal Consistency" index=105>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-105 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06683v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06683v1.pdf filename=2403.06683v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Relative monocular depth, inferring depth up to shift and scale from a single image, is an active research topic. Recent deep learning models, trained on large and varied meta-datasets, now provide excellent performance in the domain of natural images. However, few datasets exist which provide ground truth depth for endoscopic images, making training such models from scratch unfeasible. This work investigates the transfer of these models into the surgical domain, and presents an effective and simple way to improve on standard supervision through the use of temporal consistency self-supervision. We show temporal consistency significantly improves <b>supervised</b> training alone when transferring to the low-data regime of endoscopy, and outperforms the prevalent self-supervision technique for this task. In addition we show our method drastically outperforms the state-of-the-art method from within the domain of endoscopy. We also release our code, model and ensembled meta-dataset, Meta-MED, establishing a strong <b>benchmark</b> for future work.</p></p class="citation"></blockquote><h3 id=6797--106318-ceat-continual-expansion-and-absorption-transformer-for-non-exemplar-class-incremental-learning-xinyuan-gao-et-al-2024>(67/97 | 106/318) CEAT: Continual Expansion and Absorption Transformer for Non-Exemplar Class-Incremental Learning (Xinyuan Gao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinyuan Gao, Songlin Dong, Yuhang He, Xing Wei, Yihong Gong. (2024)<br><strong>CEAT: Continual Expansion and Absorption Transformer for Non-Exemplar Class-Incremental Learning</strong><br><button class=copy-to-clipboard title="CEAT: Continual Expansion and Absorption Transformer for Non-Exemplar Class-Incremental Learning" index=106>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-106 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06670v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06670v2.pdf filename=2403.06670v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In real-world applications, dynamic scenarios require the models to possess the capability to learn new tasks continuously without forgetting the old knowledge. Experience-Replay methods store a subset of the old images for joint training. In the scenario of more strict privacy protection, storing the old images becomes infeasible, which leads to a more severe plasticity-stability dilemma and classifier bias. To meet the above challenges, we propose a new architecture, named continual expansion and absorption transformer~(CEAT). The model can learn the novel knowledge by extending the expanded-fusion layers in parallel with the frozen previous parameters. After the task ends, we losslessly absorb the extended parameters into the backbone to ensure that the number of parameters remains constant. To improve the learning ability of the model, we designed a novel prototype contrastive loss to reduce the overlap between old and new classes in the feature space. Besides, to address the classifier bias towards the new classes, we propose a novel approach to generate the pseudo-features to correct the classifier. We experiment with our methods on three standard Non-Exemplar Class-Incremental Learning~(NECIL) <b>benchmarks.</b> Extensive experiments demonstrate that our model gets a significant improvement compared with the previous works and achieves 5.38%, 5.20%, and 4.92% improvement on CIFAR-100, TinyImageNet, and ImageNet-Subset.</p></p class="citation"></blockquote><h3 id=6897--107318-exploiting-style-latent-flows-for-generalizing-deepfake-detection-video-detection-jongwook-choi-et-al-2024>(68/97 | 107/318) Exploiting Style Latent Flows for Generalizing Deepfake Detection Video Detection (Jongwook Choi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jongwook Choi, Taehoon Kim, Yonghyun Jeong, Seungryul Baek, Jongwon Choi. (2024)<br><strong>Exploiting Style Latent Flows for Generalizing Deepfake Detection Video Detection</strong><br><button class=copy-to-clipboard title="Exploiting Style Latent Flows for Generalizing Deepfake Detection Video Detection" index=107>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-107 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Contrastive Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06592v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06592v1.pdf filename=2403.06592v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a new approach for the detection of fake videos, based on the analysis of style latent vectors and their abnormal behavior in temporal changes in the generated videos. We discovered that the generated facial videos suffer from the temporal distinctiveness in the temporal changes of style latent vectors, which are inevitable during the generation of temporally stable videos with various facial expressions and geometric transformations. Our framework utilizes the StyleGRU module, trained by <b>contrastive</b> <b>learning,</b> to represent the dynamic properties of style latent vectors. Additionally, we introduce a style attention module that integrates StyleGRU-generated features with content-based features, enabling the detection of visual and temporal artifacts. We demonstrate our approach across various <b>benchmark</b> scenarios in deepfake detection, showing its superiority in cross-dataset and cross-manipulation scenarios. Through further analysis, we also validate the importance of using temporal changes of style latent vectors to improve the generality of deepfake video detection.</p></p class="citation"></blockquote><h3 id=6997--108318-confidence-aware-rgb-d-face-recognition-via-virtual-depth-synthesis-zijian-chen-et-al-2024>(69/97 | 108/318) Confidence-Aware RGB-D Face Recognition via Virtual Depth Synthesis (Zijian Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zijian Chen, Mei Wang, Weihong Deng, Hongzhi Shi, Dongchao Wen, Yingjie Zhang, Xingchen Cui, Jian Zhao. (2024)<br><strong>Confidence-Aware RGB-D Face Recognition via Virtual Depth Synthesis</strong><br><button class=copy-to-clipboard title="Confidence-Aware RGB-D Face Recognition via Virtual Depth Synthesis" index=108>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-108 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Face Recognition, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06529v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06529v1.pdf filename=2403.06529v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>2D <b>face</b> <b>recognition</b> encounters challenges in unconstrained environments due to varying illumination, occlusion, and pose. Recent studies focus on RGB-D <b>face</b> <b>recognition</b> to improve robustness by incorporating depth information. However, collecting sufficient paired RGB-D training data is expensive and time-consuming, hindering wide deployment. In this work, we first construct a diverse depth dataset generated by 3D Morphable Models for depth model pre-training. Then, we propose a domain-independent pre-training framework that utilizes readily available pre-trained RGB and depth models to separately perform <b>face</b> <b>recognition</b> without needing additional paired data for retraining. To seamlessly integrate the two distinct networks and harness the complementary benefits of RGB and depth information for improved accuracy, we propose an innovative Adaptive Confidence Weighting (ACW). This mechanism is designed to learn confidence estimates for each modality to achieve modality fusion at the score level. Our method is simple and lightweight, only requiring ACW training beyond the backbone models. Experiments on multiple public RGB-D <b>face</b> <b>recognition</b> <b>benchmarks</b> demonstrate state-of-the-art performance surpassing previous methods based on depth estimation and feature fusion, validating the efficacy of our approach.</p></p class="citation"></blockquote><h3 id=7097--109318-toward-robust-canine-cardiac-diagnosis-deep-prototype-alignment-network-based-few-shot-segmentation-in-veterinary-medicine-jun-young-oh-et-al-2024>(70/97 | 109/318) Toward Robust Canine Cardiac Diagnosis: Deep Prototype Alignment Network-Based Few-Shot Segmentation in Veterinary Medicine (Jun-Young Oh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jun-Young Oh, In-Gyu Lee, Tae-Eui Kam, Ji-Hoon Jeong. (2024)<br><strong>Toward Robust Canine Cardiac Diagnosis: Deep Prototype Alignment Network-Based Few-Shot Segmentation in Veterinary Medicine</strong><br><button class=copy-to-clipboard title="Toward Robust Canine Cardiac Diagnosis: Deep Prototype Alignment Network-Based Few-Shot Segmentation in Veterinary Medicine" index=109>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-109 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Few-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06471v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06471v1.pdf filename=2403.06471v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the cutting-edge domain of medical artificial intelligence (AI), remarkable advances have been achieved in areas such as diagnosis, prediction, and therapeutic interventions. Despite these advances, the technology for image segmentation faces the significant barrier of having to produce extensively annotated datasets. To address this challenge, <b>few-shot</b> segmentation (FSS) has been recognized as one of the innovative solutions. Although most of the FSS research has focused on human health care, its application in veterinary medicine, particularly for pet care, remains largely limited. This study has focused on accurate segmentation of the heart and left atrial enlargement on canine chest radiographs using the proposed deep prototype alignment network (DPANet). The PANet architecture is adopted as the backbone model, and experiments are conducted using various encoders based on VGG-19, ResNet-18, and ResNet-50 to extract features. Experimental results demonstrate that the proposed DPANet achieves the highest performance. In the 2way-1shot scenario, it achieves the highest intersection over union (IoU) value of 0.6966, and in the 2way-5shot scenario, it achieves the highest IoU value of 0.797. The DPANet not only signifies a performance improvement, but also shows an improved training speed in the 2way-5shot scenario. These results highlight our model&rsquo;s exceptional capability as a trailblazing solution for segmenting the heart and left atrial enlargement in veterinary applications through FSS, setting a new <b>benchmark</b> in veterinary AI research, and demonstrating its superior potential to veterinary medicine advances.</p></p class="citation"></blockquote><h3 id=7197--110318-see-through-their-minds-learning-transferable-neural-representation-from-cross-subject-fmri-yulong-liu-et-al-2024>(71/97 | 110/318) See Through Their Minds: Learning Transferable Neural Representation from Cross-Subject fMRI (Yulong Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yulong Liu, Yongqiang Ma, Guibo Zhu, Haodong Jing, Nanning Zheng. (2024)<br><strong>See Through Their Minds: Learning Transferable Neural Representation from Cross-Subject fMRI</strong><br><button class=copy-to-clipboard title="See Through Their Minds: Learning Transferable Neural Representation from Cross-Subject fMRI" index=110>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-110 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-HC, cs.CV<br>Keyword Score: 11<br>Keywords: Multi-modal, Representation Learning, Sample Size<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06361v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06361v1.pdf filename=2403.06361v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deciphering visual content from functional Magnetic Resonance Imaging (fMRI) helps illuminate the human vision system. However, the scarcity of fMRI data and noise hamper brain decoding model performance. Previous approaches primarily employ subject-specific models, sensitive to training <b>sample</b> <b>size.</b> In this paper, we explore a straightforward but overlooked solution to address data scarcity. We propose shallow subject-specific adapters to map cross-subject fMRI data into unified <b>representations.</b> <b>Subsequently,</b> a shared deeper decoding model decodes cross-subject features into the target feature space. During training, we leverage both visual and textual supervision for <b>multi-modal</b> brain decoding. Our model integrates a high-level perception decoding pipeline and a pixel-wise reconstruction pipeline guided by high-level perceptions, simulating bottom-up and top-down processes in neuroscience. Empirical experiments demonstrate robust neural <b>representation</b> <b>learning</b> across subjects for both pipelines. Moreover, merging high-level and low-level information improves both low-level and high-level reconstruction metrics. Additionally, we successfully transfer learned general knowledge to new subjects by training new adapters with limited training data. Compared to previous state-of-the-art methods, notably pre-training-based methods (Mind-Vis and fMRI-PTE), our approach achieves comparable or superior results across diverse tasks, showing promise as an alternative method for cross-subject fMRI data pre-training. Our code and pre-trained weights will be publicly released at <a href=https://github.com/YulongBonjour/See_Through_Their_Minds>https://github.com/YulongBonjour/See_Through_Their_Minds</a>.</p></p class="citation"></blockquote><h3 id=7297--111318-brushnet-a-plug-and-play-image-inpainting-model-with-decomposed-dual-branch-diffusion-xuan-ju-et-al-2024>(72/97 | 111/318) BrushNet: A Plug-and-Play Image Inpainting Model with Decomposed Dual-Branch Diffusion (Xuan Ju et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xuan Ju, Xian Liu, Xintao Wang, Yuxuan Bian, Ying Shan, Qiang Xu. (2024)<br><strong>BrushNet: A Plug-and-Play Image Inpainting Model with Decomposed Dual-Branch Diffusion</strong><br><button class=copy-to-clipboard title="BrushNet: A Plug-and-Play Image Inpainting Model with Decomposed Dual-Branch Diffusion" index=111>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-111 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06976v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06976v1.pdf filename=2403.06976v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Image inpainting, the process of restoring corrupted images, has seen significant advancements with the advent of <b>diffusion</b> <b>models</b> (DMs). Despite these advancements, current DM adaptations for inpainting, which involve modifications to the sampling strategy or the development of inpainting-specific DMs, frequently suffer from semantic inconsistencies and reduced image quality. Addressing these challenges, our work introduces a novel paradigm: the division of masked image features and noisy latent into separate branches. This division dramatically diminishes the model&rsquo;s learning load, facilitating a nuanced incorporation of essential masked image information in a hierarchical fashion. Herein, we present BrushNet, a novel plug-and-play dual-branch model engineered to embed pixel-level masked image features into any pre-trained DM, guaranteeing coherent and enhanced image inpainting outcomes. Additionally, we introduce BrushData and BrushBench to facilitate segmentation-based inpainting training and performance assessment. Our extensive experimental analysis demonstrates BrushNet&rsquo;s superior performance over existing models across seven key metrics, including image quality, mask region preservation, and textual coherence.</p></p class="citation"></blockquote><h3 id=7397--112318-stochastic-cortical-self-reconstruction-christian-wachinger-et-al-2024>(73/97 | 112/318) Stochastic Cortical Self-Reconstruction (Christian Wachinger et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Christian Wachinger, Dennis Hedderich, Fabian Bongratz. (2024)<br><strong>Stochastic Cortical Self-Reconstruction</strong><br><button class=copy-to-clipboard title="Stochastic Cortical Self-Reconstruction" index=112>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-112 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06837v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06837v1.pdf filename=2403.06837v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Magnetic resonance imaging (MRI) is critical for diagnosing neurodegenerative diseases, yet accurately assessing mild cortical atrophy remains a challenge due to its subtlety. Automated cortex reconstruction, paired with healthy reference ranges, aids in pinpointing pathological atrophy, yet their generalization is limited by biases from image acquisition and processing. We introduce the concept of stochastic cortical self-reconstruction (SCSR) that creates a subject-specific healthy reference by taking MRI-derived thicknesses as input and, therefore, implicitly accounting for potential confounders. SCSR randomly corrupts parts of the cortex and self-reconstructs them from the remaining information. Trained exclusively on healthy individuals, repeated self-reconstruction generates a stochastic reference cortex for assessing deviations from the norm. We present three implementations of this concept: XGBoost applied on parcels, and two <b>autoencoders</b> on vertex level &ndash; one based on a multilayer perceptron and the other using a spherical U-Net. These models were trained on healthy subjects from the UK Biobank and subsequently evaluated across four public Alzheimer&rsquo;s datasets. Finally, we deploy the model on clinical in-house data, where deviation maps&rsquo; high spatial resolution aids in discriminating between four types of dementia.</p></p class="citation"></blockquote><h3 id=7497--113318-mambamil-enhancing-long-sequence-modeling-with-sequence-reordering-in-computational-pathology-shu-yang-et-al-2024>(74/97 | 113/318) MambaMIL: Enhancing Long Sequence Modeling with Sequence Reordering in Computational Pathology (Shu Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shu Yang, Yihui Wang, Hao Chen. (2024)<br><strong>MambaMIL: Enhancing Long Sequence Modeling with Sequence Reordering in Computational Pathology</strong><br><button class=copy-to-clipboard title="MambaMIL: Enhancing Long Sequence Modeling with Sequence Reordering in Computational Pathology" index=113>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-113 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Multiple Instance Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06800v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06800v1.pdf filename=2403.06800v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multiple</b> <b>Instance</b> <b>Learning</b> (MIL) has emerged as a dominant paradigm to extract discriminative feature representations within Whole Slide Images (WSIs) in computational pathology. Despite driving notable progress, existing MIL approaches suffer from limitations in facilitating comprehensive and efficient interactions among instances, as well as challenges related to time-consuming computations and overfitting. In this paper, we incorporate the Selective Scan Space State Sequential Model (Mamba) in <b>Multiple</b> <b>Instance</b> <b>Learning</b> (MIL) for long sequence modeling with linear complexity, termed as MambaMIL. By inheriting the capability of vanilla Mamba, MambaMIL demonstrates the ability to comprehensively understand and perceive long sequences of instances. Furthermore, we propose the Sequence Reordering Mamba (SR-Mamba) aware of the order and distribution of instances, which exploits the inherent valuable information embedded within the long sequences. With the SR-Mamba as the core component, MambaMIL can effectively capture more discriminative features and mitigate the challenges associated with overfitting and high computational overhead. Extensive experiments on two public challenging tasks across nine diverse datasets demonstrate that our proposed framework performs favorably against state-of-the-art MIL methods. The code is released at <a href=https://github.com/isyangshu/MambaMIL>https://github.com/isyangshu/MambaMIL</a>.</p></p class="citation"></blockquote><h3 id=7597--114318-facechain-sude-building-derived-class-to-inherit-category-attributes-for-one-shot-subject-driven-generation-pengchong-qiao-et-al-2024>(75/97 | 114/318) FaceChain-SuDe: Building Derived Class to Inherit Category Attributes for One-shot Subject-Driven Generation (Pengchong Qiao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pengchong Qiao, Lei Shang, Chang Liu, Baigui Sun, Xiangyang Ji, Jie Chen. (2024)<br><strong>FaceChain-SuDe: Building Derived Class to Inherit Category Attributes for One-shot Subject-Driven Generation</strong><br><button class=copy-to-clipboard title="FaceChain-SuDe: Building Derived Class to Inherit Category Attributes for One-shot Subject-Driven Generation" index=114>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-114 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06775v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06775v1.pdf filename=2403.06775v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Subject-driven generation has garnered significant interest recently due to its ability to personalize <b>text-to-image</b> generation. Typical works focus on learning the new subject&rsquo;s private attributes. However, an important fact has not been taken seriously that a subject is not an isolated new concept but should be a specialization of a certain category in the pre-trained model. This results in the subject failing to comprehensively inherit the attributes in its category, causing poor attribute-related generations. In this paper, motivated by object-oriented programming, we model the subject as a derived class whose base class is its semantic category. This modeling enables the subject to inherit public attributes from its category while learning its private attributes from the user-provided example. Specifically, we propose a plug-and-play method, Subject-Derived regularization (SuDe). It constructs the base-derived class modeling by constraining the subject-driven generated images to semantically belong to the subject&rsquo;s category. Extensive experiments under three baselines and two backbones on various subjects show that our SuDe enables imaginative attribute-related generations while maintaining subject fidelity. Codes will be open sourced soon at FaceChain (<a href=https://github.com/modelscope/facechain)>https://github.com/modelscope/facechain)</a>.</p></p class="citation"></blockquote><h3 id=7697--115318-car-damage-detection-and-patch-to-patch-self-supervised-image-alignment-hanxiao-chen-2024>(76/97 | 115/318) Car Damage Detection and Patch-to-Patch Self-supervised Image Alignment (Hanxiao Chen, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hanxiao Chen. (2024)<br><strong>Car Damage Detection and Patch-to-Patch Self-supervised Image Alignment</strong><br><button class=copy-to-clipboard title="Car Damage Detection and Patch-to-Patch Self-supervised Image Alignment" index=115>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-115 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06674v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06674v1.pdf filename=2403.06674v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Most computer vision applications aim to identify pixels in a scene and use them for diverse purposes. One intriguing application is car damage detection for insurance carriers which tends to detect all car damages by comparing both pre-trip and post-trip images, even requiring two components: (i) car damage detection; (ii) image alignment. Firstly, we implemented a Mask R-CNN model to detect car damages on custom images. Whereas for the image alignment section, we especially propose a novel <b>self-supervised</b> Patch-to-Patch SimCLR inspired alignment approach to find perspective transformations between custom pre/post car rental images except for traditional computer vision methods.</p></p class="citation"></blockquote><h3 id=7797--116318-epsilon-mesh-attack-a-surface-based-adversarial-point-cloud-attack-for-facial-expression-recognition-batuhan-cengiz-et-al-2024>(77/97 | 116/318) epsilon-Mesh Attack: A Surface-based Adversarial Point Cloud Attack for Facial Expression Recognition (Batuhan Cengiz et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Batuhan Cengiz, Mert Gulsen, Yusuf H. Sahin, Gozde Unal. (2024)<br><strong>epsilon-Mesh Attack: A Surface-based Adversarial Point Cloud Attack for Facial Expression Recognition</strong><br><button class=copy-to-clipboard title="epsilon-Mesh Attack: A Surface-based Adversarial Point Cloud Attack for Facial Expression Recognition" index=116>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-116 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06661v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06661v1.pdf filename=2403.06661v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Point clouds and meshes are widely used 3D data structures for many computer vision applications. While the meshes represent the surfaces of an object, point cloud represents sampled points from the surface which is also the output of modern sensors such as LiDAR and RGB-D cameras. Due to the wide application area of point clouds and the recent advancements in deep neural networks, studies focusing on robust classification of the 3D point cloud data emerged. To evaluate the robustness of deep classifier networks, a common method is to use <b>adversarial</b> <b>attacks</b> where the gradient direction is followed to change the input slightly. The previous studies on <b>adversarial</b> <b>attacks</b> are generally evaluated on point clouds of daily objects. However, considering 3D faces, these <b>adversarial</b> <b>attacks</b> tend to affect the person&rsquo;s facial structure more than the desired amount and cause malformation. Specifically for facial expressions, even a small <b>adversarial</b> <b>attack</b> can have a significant effect on the face structure. In this paper, we suggest an <b>adversarial</b> <b>attack</b> called $\epsilon$-Mesh Attack, which operates on point cloud data via limiting perturbations to be on the mesh surface. We also parameterize our attack by $\epsilon$ to scale the perturbation mesh. Our surface-based attack has tighter perturbation bounds compared to $L_2$ and $L_\infty$ norm bounded attacks that operate on unit-ball. Even though our method has additional constraints, our experiments on CoMA, Bosphorus and FaceWarehouse datasets show that $\epsilon$-Mesh Attack (Perpendicular) successfully confuses trained DGCNN and PointNet models $99.72%$ and $97.06%$ of the time, with indistinguishable facial deformations. The code is available at <a href=https://github.com/batuceng/e-mesh-attack>https://github.com/batuceng/e-mesh-attack</a>.</p></p class="citation"></blockquote><h3 id=7897--117318-towards-zero-shot-interpretable-human-recognition-a-2d-3d-registration-framework-henrique-jesus-et-al-2024>(78/97 | 117/318) Towards Zero-Shot Interpretable Human Recognition: A 2D-3D Registration Framework (Henrique Jesus et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Henrique Jesus, Hugo Proença. (2024)<br><strong>Towards Zero-Shot Interpretable Human Recognition: A 2D-3D Registration Framework</strong><br><button class=copy-to-clipboard title="Towards Zero-Shot Interpretable Human Recognition: A 2D-3D Registration Framework" index=117>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-117 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06658v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06658v1.pdf filename=2403.06658v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Large vision models based in deep learning architectures have been consistently advancing the state-of-the-art in biometric recognition. However, three weaknesses are commonly reported for such kind of approaches: 1) their extreme demands in terms of learning data; 2) the difficulties in generalising between different domains; and 3) the lack of interpretability/explainability, with biometrics being of particular interest, as it is important to provide evidence able to be used for forensics/legal purposes (e.g., in courts). To the best of our knowledge, this paper describes the first recognition framework/strategy that aims at addressing the three weaknesses simultaneously. At first, it relies exclusively in synthetic samples for learning purposes. Instead of requiring a large amount and variety of samples for each subject, the idea is to exclusively enroll a 3D point cloud per identity. Then, using generative strategies, we synthesize a very large (potentially infinite) number of samples, containing all the desired covariates (poses, clothing, distances, perspectives, lighting, occlusions,&mldr;). Upon the synthesizing method used, it is possible to adapt precisely to different kind of domains, which accounts for generalization purposes. Such data are then used to learn a model that performs local registration between image pairs, establishing positive correspondences between body parts that are the key, not only to recognition (according to cardinality and distribution), but also to provide an interpretable description of the response (e.g.: &ldquo;both samples are from the same person, as they have similar facial shape, hair color and legs thickness&rdquo;).</p></p class="citation"></blockquote><h3 id=7997--118318-forest-inspection-dataset-for-aerial-semantic-segmentation-and-depth-estimation-bianca-cerasela-zelia-blaga-et-al-2024>(79/97 | 118/318) Forest Inspection Dataset for Aerial Semantic Segmentation and Depth Estimation (Bianca-Cerasela-Zelia Blaga et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bianca-Cerasela-Zelia Blaga, Sergiu Nedevschi. (2024)<br><strong>Forest Inspection Dataset for Aerial Semantic Segmentation and Depth Estimation</strong><br><button class=copy-to-clipboard title="Forest Inspection Dataset for Aerial Semantic Segmentation and Depth Estimation" index=118>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-118 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06621v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06621v1.pdf filename=2403.06621v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Humans use UAVs to monitor changes in forest environments since they are lightweight and provide a large variety of surveillance data. However, their information does not present enough details for understanding the scene which is needed to assess the degree of deforestation. Deep learning algorithms must be trained on large amounts of data to output accurate interpretations, but ground truth recordings of annotated forest imagery are not available. To solve this problem, we introduce a new large aerial dataset for forest inspection which contains both real-world and virtual recordings of natural environments, with densely annotated semantic segmentation labels and depth maps, taken in different illumination conditions, at various altitudes and recording angles. We test the performance of two multi-scale neural networks for solving the semantic segmentation task (HRNet and PointFlow network), studying the impact of the various acquisition conditions and the capabilities of <b>transfer</b> <b>learning</b> from virtual to real data. Our results showcase that the best results are obtained when the training is done on a dataset containing a large variety of scenarios, rather than separating the data into specific categories. We also develop a framework to assess the deforestation degree of an area.</p></p class="citation"></blockquote><h3 id=8097--119318-density-guided-label-smoothing-for-temporal-localization-of-driving-actions-tunc-alkanat-et-al-2024>(80/97 | 119/318) Density-Guided Label Smoothing for Temporal Localization of Driving Actions (Tunc Alkanat et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tunc Alkanat, Erkut Akdag, Egor Bondarev, Peter H. N. De With. (2024)<br><strong>Density-Guided Label Smoothing for Temporal Localization of Driving Actions</strong><br><button class=copy-to-clipboard title="Density-Guided Label Smoothing for Temporal Localization of Driving Actions" index=119>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-119 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Label Smoothing<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06616v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06616v1.pdf filename=2403.06616v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Temporal localization of driving actions plays a crucial role in advanced driver-assistance systems and naturalistic driving studies. However, this is a challenging task due to strict requirements for robustness, reliability and accurate localization. In this work, we focus on improving the overall performance by efficiently utilizing video action recognition networks and adapting these to the problem of action localization. To this end, we first develop a density-guided <b>label</b> <b>smoothing</b> technique based on <b>label</b> <b>probability</b> distributions to facilitate better learning from boundary video-segments that typically include multiple <b>labels.</b> <b>Second,</b> we design a post-processing step to efficiently fuse information from video-segments and multiple camera views into scene-level predictions, which facilitates elimination of false positives. Our methodology yields a competitive performance on the A2 test set of the naturalistic driving action recognition track of the 2022 NVIDIA AI City Challenge with an F1 score of 0.271.</p></p class="citation"></blockquote><h3 id=8197--120318-distributionally-generative-augmentation-for-fair-facial-attribute-classification-fengda-zhang-et-al-2024>(81/97 | 120/318) Distributionally Generative Augmentation for Fair Facial Attribute Classification (Fengda Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fengda Zhang, Qianpei He, Kun Kuang, Jiashuo Liu, Long Chen, Chao Wu, Jun Xiao, Hanwang Zhang. (2024)<br><strong>Distributionally Generative Augmentation for Fair Facial Attribute Classification</strong><br><button class=copy-to-clipboard title="Distributionally Generative Augmentation for Fair Facial Attribute Classification" index=120>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-120 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06606v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06606v1.pdf filename=2403.06606v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Facial Attribute Classification (FAC) holds substantial promise in widespread applications. However, FAC models trained by traditional methodologies can be unfair by exhibiting accuracy inconsistencies across varied data subpopulations. This unfairness is largely attributed to bias in data, where some spurious attributes (e.g., Male) statistically correlate with the target attribute (e.g., Smiling). Most of existing <b>fairness-aware</b> methods rely on the labels of spurious attributes, which may be unavailable in practice. This work proposes a novel, generation-based two-stage framework to train a fair FAC model on biased data without additional annotation. Initially, we identify the potential spurious attributes based on generative models. Notably, it enhances interpretability by explicitly showing the spurious attributes in image space. Following this, for each image, we first edit the spurious attributes with a random degree sampled from a uniform distribution, while keeping target attribute unchanged. Then we train a fair FAC model by fostering model invariance to these augmentation. Extensive experiments on three common datasets demonstrate the effectiveness of our method in promoting <b>fairness</b> in FAC without compromising accuracy. Codes are in <a href=https://github.com/heqianpei/DiGA>https://github.com/heqianpei/DiGA</a>.</p></p class="citation"></blockquote><h3 id=8297--121318-transformer-based-fusion-of-2d-pose-and-spatio-temporal-embeddings-for-distracted-driver-action-recognition-erkut-akdag-et-al-2024>(82/97 | 121/318) Transformer-based Fusion of 2D-pose and Spatio-temporal Embeddings for Distracted Driver Action Recognition (Erkut Akdag et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Erkut Akdag, Zeqi Zhu, Egor Bondarev, Peter H. N. De With. (2024)<br><strong>Transformer-based Fusion of 2D-pose and Spatio-temporal Embeddings for Distracted Driver Action Recognition</strong><br><button class=copy-to-clipboard title="Transformer-based Fusion of 2D-pose and Spatio-temporal Embeddings for Distracted Driver Action Recognition" index=121>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-121 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06577v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06577v1.pdf filename=2403.06577v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Classification and localization of driving actions over time is important for advanced driver-assistance systems and naturalistic driving studies. Temporal localization is challenging because it requires robustness, reliability, and accuracy. In this study, we aim to improve the temporal localization and classification accuracy performance by adapting video action recognition and 2D human-pose estimation networks to one model. Therefore, we design a <b>transformer-based</b> fusion architecture to effectively combine 2D-pose features and spatio-temporal features. The model uses 2D-pose features as the positional embedding of the <b>transformer</b> architecture and spatio-temporal features as the main input to the encoder of the <b>transformer.</b> The proposed solution is generic and independent of the camera numbers and positions, giving frame-based class probabilities as output. Finally, the post-processing step combines information from different camera views to obtain final predictions and eliminate false positives. The model performs well on the A2 test set of the 2023 NVIDIA AI City Challenge for naturalistic driving action recognition, achieving the overlap score of the organizer-defined distracted driver behaviour metric of 0.5079.</p></p class="citation"></blockquote><h3 id=8397--122318-query-guided-prototype-evolution-network-for-few-shot-segmentation-runmin-cong-et-al-2024>(83/97 | 122/318) Query-guided Prototype Evolution Network for Few-Shot Segmentation (Runmin Cong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Runmin Cong, Hang Xiong, Jinpeng Chen, Wei Zhang, Qingming Huang, Yao Zhao. (2024)<br><strong>Query-guided Prototype Evolution Network for Few-Shot Segmentation</strong><br><button class=copy-to-clipboard title="Query-guided Prototype Evolution Network for Few-Shot Segmentation" index=122>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-122 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Few-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06488v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06488v1.pdf filename=2403.06488v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Previous <b>Few-Shot</b> Segmentation (FSS) approaches exclusively utilize support features for prototype generation, neglecting the specific requirements of the query. To address this, we present the Query-guided Prototype Evolution Network (QPENet), a new method that integrates query features into the generation process of foreground and background prototypes, thereby yielding customized prototypes attuned to specific queries. The evolution of the foreground prototype is accomplished through a \textit{support-query-support} iterative process involving two new modules: Pseudo-prototype Generation (PPG) and Dual Prototype Evolution (DPE). The PPG module employs support features to create an initial prototype for the preliminary segmentation of the query image, resulting in a pseudo-prototype reflecting the unique needs of the current query. Subsequently, the DPE module performs reverse segmentation on support images using this pseudo-prototype, leading to the generation of evolved prototypes, which can be considered as custom solutions. As for the background prototype, the evolution begins with a global background prototype that represents the generalized features of all training images. We also design a Global Background Cleansing (GBC) module to eliminate potential adverse components mirroring the characteristics of the current foreground class. Experimental results on the PASCAL-$5^i$ and COCO-$20^i$ datasets attest to the substantial enhancements achieved by QPENet over prevailing state-of-the-art techniques, underscoring the validity of our ideas.</p></p class="citation"></blockquote><h3 id=8497--123318-point-mamba-a-novel-point-cloud-backbone-based-on-state-space-model-with-octree-based-ordering-strategy-jiuming-liu-et-al-2024>(84/97 | 123/318) Point Mamba: A Novel Point Cloud Backbone Based on State Space Model with Octree-Based Ordering Strategy (Jiuming Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiuming Liu, Ruiji Yu, Yian Wang, Yu Zheng, Tianchen Deng, Weicai Ye, Hesheng Wang. (2024)<br><strong>Point Mamba: A Novel Point Cloud Backbone Based on State Space Model with Octree-Based Ordering Strategy</strong><br><button class=copy-to-clipboard title="Point Mamba: A Novel Point Cloud Backbone Based on State Space Model with Octree-Based Ordering Strategy" index=123>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-123 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06467v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06467v1.pdf filename=2403.06467v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, state space model (SSM) has gained great attention due to its promising performance, linear complexity, and long sequence modeling ability in both language and image domains. However, it is non-trivial to extend SSM to the point cloud field, because of the causality requirement of SSM and the disorder and irregularity nature of point clouds. In this paper, we propose a novel SSM-based point cloud processing backbone, named Point Mamba, with a causality-aware ordering mechanism. To construct the causal dependency relationship, we design an octree-based ordering strategy on raw irregular points, globally sorting points in a z-order sequence and also retaining their spatial proximity. Our method achieves state-of-the-art performance compared with <b>transformer-based</b> counterparts, with 93.4% accuracy and 75.7 mIOU respectively on the ModelNet40 classification dataset and ScanNet semantic segmentation dataset. Furthermore, our Point Mamba has linear complexity, which is more efficient than <b>transformer-based</b> methods. Our method demonstrates the great potential that SSM can serve as a generic backbone in point cloud understanding. Codes are released at <a href=https://github.com/IRMVLab/Point-Mamba>https://github.com/IRMVLab/Point-Mamba</a>.</p></p class="citation"></blockquote><h3 id=8597--124318-text2qr-harmonizing-aesthetic-customization-and-scanning-robustness-for-text-guided-qr-code-generation-guangyang-wu-et-al-2024>(85/97 | 124/318) Text2QR: Harmonizing Aesthetic Customization and Scanning Robustness for Text-Guided QR Code Generation (Guangyang Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guangyang Wu, Xiaohong Liu, Jun Jia, Xuehao Cui, Guangtao Zhai. (2024)<br><strong>Text2QR: Harmonizing Aesthetic Customization and Scanning Robustness for Text-Guided QR Code Generation</strong><br><button class=copy-to-clipboard title="Text2QR: Harmonizing Aesthetic Customization and Scanning Robustness for Text-Guided QR Code Generation" index=124>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-124 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Code Generation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06452v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06452v2.pdf filename=2403.06452v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the digital era, QR <b>codes</b> <b>serve</b> as a linchpin connecting virtual and physical realms. Their pervasive integration across various applications highlights the demand for aesthetically pleasing <b>codes</b> <b>without</b> compromised scannability. However, prevailing methods grapple with the intrinsic challenge of balancing customization and scannability. Notably, stable-diffusion models have ushered in an epoch of high-quality, customizable content generation. This paper introduces Text2QR, a pioneering approach leveraging these advancements to address a fundamental challenge: concurrently achieving user-defined aesthetics and scanning robustness. To ensure stable generation of aesthetic QR <b>codes,</b> <b>we</b> introduce the QR Aesthetic Blueprint (QAB) module, generating a blueprint image exerting control over the entire generation process. Subsequently, the Scannability Enhancing Latent Refinement (SELR) process refines the output iteratively in the latent space, enhancing scanning robustness. This approach harnesses the potent generation capabilities of stable-diffusion models, navigating the trade-off between image aesthetics and QR <b>code</b> <b>scannability.</b> Our experiments demonstrate the seamless fusion of visual appeal with the practical utility of aesthetic QR <b>codes,</b> <b>markedly</b> outperforming prior methods. <b>Codes</b> <b>are</b> available at \url{https://github.com/mulns/Text2QR}</p></p class="citation"></blockquote><h3 id=8697--125318-fine-grained-pillar-feature-encoding-via-spatio-temporal-virtual-grid-for-3d-object-detection-konyul-park-et-al-2024>(86/97 | 125/318) Fine-Grained Pillar Feature Encoding Via Spatio-Temporal Virtual Grid for 3D Object Detection (Konyul Park et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Konyul Park, Yecheol Kim, Junho Koh, Byungwoo Park, Jun Won Choi. (2024)<br><strong>Fine-Grained Pillar Feature Encoding Via Spatio-Temporal Virtual Grid for 3D Object Detection</strong><br><button class=copy-to-clipboard title="Fine-Grained Pillar Feature Encoding Via Spatio-Temporal Virtual Grid for 3D Object Detection" index=125>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-125 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Object Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06433v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06433v1.pdf filename=2403.06433v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Developing high-performance, real-time architectures for LiDAR-based 3D <b>object</b> <b>detectors</b> is essential for the successful commercialization of autonomous vehicles. Pillar-based methods stand out as a practical choice for onboard deployment due to their computational efficiency. However, despite their efficiency, these methods can sometimes underperform compared to alternative point encoding techniques such as Voxel-encoding or PointNet++. We argue that current pillar-based methods have not sufficiently captured the fine-grained distributions of LiDAR points within each pillar structure. Consequently, there exists considerable room for improvement in pillar feature encoding. In this paper, we introduce a novel pillar encoding architecture referred to as Fine-Grained Pillar Feature Encoding (FG-PFE). FG-PFE utilizes Spatio-Temporal Virtual (STV) grids to capture the distribution of point clouds within each pillar across vertical, temporal, and horizontal dimensions. Through STV grids, points within each pillar are individually encoded using Vertical PFE (V-PFE), Temporal PFE (T-PFE), and Horizontal PFE (H-PFE). These encoded features are then aggregated through an Attentive Pillar Aggregation method. Our experiments conducted on the nuScenes dataset demonstrate that FG-PFE achieves significant performance improvements over baseline models such as PointPillar, CenterPoint-Pillar, and PillarNet, with only a minor increase in computational overhead.</p></p class="citation"></blockquote><h3 id=8797--126318-flowvqtalker-high-quality-emotional-talking-face-generation-through-normalizing-flow-and-quantization-shuai-tan-et-al-2024>(87/97 | 126/318) FlowVQTalker: High-Quality Emotional Talking Face Generation through Normalizing Flow and Quantization (Shuai Tan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shuai Tan, Bin Ji, Ye Pan. (2024)<br><strong>FlowVQTalker: High-Quality Emotional Talking Face Generation through Normalizing Flow and Quantization</strong><br><button class=copy-to-clipboard title="FlowVQTalker: High-Quality Emotional Talking Face Generation through Normalizing Flow and Quantization" index=126>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-126 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06375v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06375v2.pdf filename=2403.06375v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Generating emotional talking faces is a practical yet challenging endeavor. To create a lifelike avatar, we draw upon two critical insights from a human perspective: 1) The connection between audio and the non-deterministic facial dynamics, encompassing expressions, blinks, poses, should exhibit synchronous and one-to-many mapping. 2) Vibrant expressions are often accompanied by emotion-aware high-definition (HD) textures and finely detailed teeth. However, both aspects are frequently overlooked by existing methods. To this end, this paper proposes using normalizing Flow and Vector-Quantization modeling to produce emotional talking faces that satisfy both insights concurrently (FlowVQTalker). Specifically, we develop a flow-based coefficient generator that encodes the dynamics of facial emotion into a multi-emotion-class latent space represented as a mixture distribution. The generation process commences with random sampling from the modeled distribution, guided by the accompanying audio, enabling both lip-synchronization and the uncertain nonverbal facial cues generation. Furthermore, our designed vector-quantization image generator treats the creation of expressive facial images as a code query task, utilizing a learned codebook to provide rich, high-quality textures that enhance the emotional perception of the results. Extensive experiments are conducted to showcase the effectiveness of our approach.</p></p class="citation"></blockquote><h3 id=8897--127318-style2talker-high-resolution-talking-head-generation-with-emotion-style-and-art-style-shuai-tan-et-al-2024>(88/97 | 127/318) Style2Talker: High-Resolution Talking Head Generation with Emotion Style and Art Style (Shuai Tan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shuai Tan, Bin Ji, Ye Pan. (2024)<br><strong>Style2Talker: High-Resolution Talking Head Generation with Emotion Style and Art Style</strong><br><button class=copy-to-clipboard title="Style2Talker: High-Resolution Talking Head Generation with Emotion Style and Art Style" index=127>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-127 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06365v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06365v2.pdf filename=2403.06365v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Although automatically animating audio-driven talking heads has recently received growing interest, previous efforts have mainly concentrated on achieving lip synchronization with the audio, neglecting two crucial elements for generating expressive videos: emotion style and art style. In this paper, we present an innovative audio-driven talking face generation method called Style2Talker. It involves two stylized stages, namely Style-E and Style-A, which integrate text-controlled emotion style and picture-controlled art style into the final output. In order to prepare the scarce emotional text descriptions corresponding to the videos, we propose a labor-free paradigm that employs large-scale pretrained models to automatically annotate emotional text labels for existing audiovisual datasets. Incorporating the synthetic emotion texts, the Style-E stage utilizes a large-scale CLIP model to extract emotion representations, which are combined with the audio, serving as the condition for an efficient latent <b>diffusion</b> <b>model</b> designed to produce emotional motion coefficients of a 3DMM model. Moving on to the Style-A stage, we develop a coefficient-driven motion generator and an art-specific style path embedded in the well-known StyleGAN. This allows us to synthesize high-resolution artistically stylized talking head videos using the generated emotional motion coefficients and an art style source picture. Moreover, to better preserve image details and avoid artifacts, we provide StyleGAN with the multi-scale content features extracted from the identity image and refine its intermediate feature maps by the designed content encoder and refinement network, respectively. Extensive experimental results demonstrate our method outperforms existing state-of-the-art methods in terms of audio-lip synchronization and performance of both emotion style and art style.</p></p class="citation"></blockquote><h3 id=8997--128318-say-anything-with-any-style-shuai-tan-et-al-2024>(89/97 | 128/318) Say Anything with Any Style (Shuai Tan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shuai Tan, Bin Ji, Yu Ding, Ye Pan. (2024)<br><strong>Say Anything with Any Style</strong><br><button class=copy-to-clipboard title="Say Anything with Any Style" index=128>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-128 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06363v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06363v2.pdf filename=2403.06363v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Generating stylized talking head with diverse head motions is crucial for achieving natural-looking videos but still remains challenging. Previous works either adopt a regressive method to capture the speaking style, resulting in a coarse style that is averaged across all training data, or employ a universal network to synthesize videos with different styles which causes suboptimal performance. To address these, we propose a novel dynamic-weight method, namely Say Anything withAny Style (SAAS), which queries the discrete style representation via a generative model with a learned style codebook. Specifically, we develop a multi-task VQ-VAE that incorporates three closely related tasks to learn a style codebook as a prior for style extraction. This discrete prior, along with the generative model, enhances the precision and robustness when extracting the speaking styles of the given style clips. By utilizing the extracted style, a residual architecture comprising a canonical branch and style-specific branch is employed to predict the mouth shapes conditioned on any driving audio while transferring the speaking style from the source to any desired one. To adapt to different speaking styles, we steer clear of employing a universal network by exploring an elaborate HyperStyle to produce the style-specific weights offset for the style branch. Furthermore, we construct a pose generator and a pose codebook to store the <b>quantized</b> pose representation, allowing us to sample diverse head motions aligned with the audio and the extracted style. Experiments demonstrate that our approach surpasses state-of-theart methods in terms of both lip-synchronization and stylized expression. Besides, we extend our SAAS to video-driven style editing field and achieve satisfactory performance.</p></p class="citation"></blockquote><h3 id=9097--129318-exploring-hardware-friendly-bottleneck-architecture-in-cnn-for-embedded-computing-systems-xing-lei-et-al-2024>(90/97 | 129/318) Exploring Hardware Friendly Bottleneck Architecture in CNN for Embedded Computing Systems (Xing Lei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xing Lei, Longjun Liu, Zhiheng Zhou, Hongbin Sun, Nanning Zheng. (2024)<br><strong>Exploring Hardware Friendly Bottleneck Architecture in CNN for Embedded Computing Systems</strong><br><button class=copy-to-clipboard title="Exploring Hardware Friendly Bottleneck Architecture in CNN for Embedded Computing Systems" index=129>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-129 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06352v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06352v1.pdf filename=2403.06352v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we explore how to design lightweight <b>CNN</b> architecture for embedded computing systems. We propose L-Mobilenet model for ZYNQ based hardware platform. L-Mobilenet can adapt well to the hardware computing and accelerating, and its network structure is inspired by the state-of-the-art work of Inception-ResnetV1 and MobilenetV2, which can effectively reduce parameters and delay while maintaining the accuracy of inference. We deploy our L-Mobilenet model to ZYNQ embedded platform for fully evaluating the performance of our design. By measuring in cifar10 and cifar100 datasets, L-Mobilenet model is able to gain 3x speed up and 3.7x fewer parameters than MobileNetV2 while maintaining a similar accuracy. It also can obtain 2x speed up and 1.5x fewer parameters than ShufflenetV2 while maintaining the same accuracy. Experiments show that our network model can obtain better performance because of the special considerations for hardware accelerating and software-hardware co-design strategies in our L-Mobilenet bottleneck architecture.</p></p class="citation"></blockquote><h3 id=9197--130318-reliable-spatial-temporal-voxels-for-multi-modal-test-time-adaptation-haozhi-cao-et-al-2024>(91/97 | 130/318) Reliable Spatial-Temporal Voxels For Multi-Modal Test-Time Adaptation (Haozhi Cao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haozhi Cao, Yuecong Xu, Jianfei Yang, Pengyu Yin, Xingyu Ji, Shenghai Yuan, Lihua Xie. (2024)<br><strong>Reliable Spatial-Temporal Voxels For Multi-Modal Test-Time Adaptation</strong><br><button class=copy-to-clipboard title="Reliable Spatial-Temporal Voxels For Multi-Modal Test-Time Adaptation" index=130>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-130 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 6<br>Keywords: Benchmarking, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06461v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06461v2.pdf filename=2403.06461v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multi-modal</b> test-time adaptation (MM-TTA) is proposed to adapt models to an unlabeled target domain by leveraging the complementary <b>multi-modal</b> inputs in an online manner. Previous MM-TTA methods rely on predictions of cross-modal information in each input frame, while they ignore the fact that predictions of geometric neighborhoods within consecutive frames are highly correlated, leading to unstable predictions across time. To fulfill this gap, we propose ReLiable Spatial-temporal Voxels (Latte), an MM-TTA method that leverages reliable cross-modal spatial-temporal correspondences for <b>multi-modal</b> 3D segmentation. Motivated by the fact that reliable predictions should be consistent with their spatial-temporal correspondences, Latte aggregates consecutive frames in a slide window manner and constructs ST voxel to capture temporally local prediction consistency for each modality. After filtering out ST voxels with high ST entropy, Latte conducts cross-modal learning for each point and pixel by attending to those with reliable and consistent predictions among both spatial and temporal neighborhoods. Experimental results show that Latte achieves state-of-the-art performance on three different MM-TTA <b>benchmarks</b> compared to previous MM-TTA or TTA methods.</p></p class="citation"></blockquote><h3 id=9297--131318-vosh-voxel-mesh-hybrid-representation-for-real-time-view-synthesis-chenhao-zhang-et-al-2024>(92/97 | 131/318) Vosh: Voxel-Mesh Hybrid Representation for Real-Time View Synthesis (Chenhao Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chenhao Zhang, Yongyang Zhou, Lei Zhang. (2024)<br><strong>Vosh: Voxel-Mesh Hybrid Representation for Real-Time View Synthesis</strong><br><button class=copy-to-clipboard title="Vosh: Voxel-Mesh Hybrid Representation for Real-Time View Synthesis" index=131>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-131 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06505v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06505v1.pdf filename=2403.06505v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The neural radiance field (NeRF) has emerged as a prominent methodology for synthesizing realistic images of novel views. While neural radiance representations based on voxels or mesh individually offer distinct advantages, excelling in either rendering quality or speed, each has limitations in the other aspect. In response, we propose a pioneering hybrid representation named Vosh, seamlessly combining both voxel and mesh components in hybrid rendering for view synthesis. Vosh is meticulously crafted by optimizing the voxel grid of NeRF, strategically with selected voxels replaced by mesh. Therefore, it excels in fast rendering scenes with simple <b>geometry</b> and textures through its mesh component, while simultaneously enabling high-quality rendering in intricate regions by leveraging voxel component. The flexibility of Vosh is showcased through the ability to adjust hybrid ratios, providing users the ability to control the balance between rendering quality and speed based on flexible usage. Experimental results demonstrates that our method achieves commendable trade-off between rendering quality and speed, and notably has real-time performance on mobile devices.</p></p class="citation"></blockquote><h3 id=9397--132318-fregs-3d-gaussian-splatting-with-progressive-frequency-regularization-jiahui-zhang-et-al-2024>(93/97 | 132/318) FreGS: 3D Gaussian Splatting with Progressive Frequency Regularization (Jiahui Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiahui Zhang, Fangneng Zhan, Muyu Xu, Shijian Lu, Eric Xing. (2024)<br><strong>FreGS: 3D Gaussian Splatting with Progressive Frequency Regularization</strong><br><button class=copy-to-clipboard title="FreGS: 3D Gaussian Splatting with Progressive Frequency Regularization" index=132>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-132 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06908v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06908v1.pdf filename=2403.06908v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>3D Gaussian splatting has achieved very impressive performance in real-time novel view synthesis. However, it often suffers from over-reconstruction during Gaussian densification where high-variance image regions are covered by a few large Gaussians only, leading to blur and artifacts in the rendered images. We design a progressive frequency regularization (FreGS) technique to tackle the over-reconstruction issue within the frequency space. Specifically, FreGS performs coarse-to-fine Gaussian densification by exploiting low-to-high frequency components that can be easily extracted with low-pass and high-pass filters in the Fourier space. By minimizing the discrepancy between the frequency spectrum of the rendered image and the corresponding ground truth, it achieves high-quality Gaussian densification and alleviates the over-reconstruction of Gaussian splatting effectively. Experiments over multiple widely adopted <b>benchmarks</b> (e.g., Mip-NeRF360, Tanks-and-Temples and Deep Blending) show that FreGS achieves superior novel view synthesis and outperforms the state-of-the-art consistently.</p></p class="citation"></blockquote><h3 id=9497--133318-fast-text-to-3d-aware-face-generation-and-manipulation-via-direct-cross-modal-mapping-and-geometric-regularization-jinlu-zhang-et-al-2024>(94/97 | 133/318) Fast Text-to-3D-Aware Face Generation and Manipulation via Direct Cross-modal Mapping and Geometric Regularization (Jinlu Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinlu Zhang, Yiyi Zhou, Qiancheng Zheng, Xiaoxiong Du, Gen Luo, Jun Peng, Xiaoshuai Sun, Rongrong Ji. (2024)<br><strong>Fast Text-to-3D-Aware Face Generation and Manipulation via Direct Cross-modal Mapping and Geometric Regularization</strong><br><button class=copy-to-clipboard title="Fast Text-to-3D-Aware Face Generation and Manipulation via Direct Cross-modal Mapping and Geometric Regularization" index=133>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-133 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06702v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06702v1.pdf filename=2403.06702v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Text-to-3D-aware face (T3D Face) generation and manipulation is an emerging research hot spot in machine learning, which still suffers from low efficiency and poor quality. In this paper, we propose an End-to-End Efficient and Effective network for fast and accurate T3D face generation and manipulation, termed $E^3$-FaceNet. Different from existing complex generation paradigms, $E^3$-FaceNet resorts to a direct mapping from text instructions to 3D-aware visual space. We introduce a novel Style Code Enhancer to enhance cross-modal semantic alignment, alongside an innovative Geometric Regularization objective to maintain consistency across multi-view generations. Extensive experiments on three <b>benchmark</b> datasets demonstrate that $E^3$-FaceNet can not only achieve picture-like 3D face generation and manipulation, but also improve inference speed by orders of magnitudes. For instance, compared with Latent3D, $E^3$-FaceNet speeds up the five-view generations by almost 470 times, while still exceeding in generation quality. Our code are released at <a href=https://github.com/Aria-Zhangjl/E3-FaceNet>https://github.com/Aria-Zhangjl/E3-FaceNet</a>.</p></p class="citation"></blockquote><h3 id=9597--134318-ada-tracker-soft-tissue-tracking-via-inter-frame-and-adaptive-template-matching-jiaxin-guo-et-al-2024>(95/97 | 134/318) Ada-Tracker: Soft Tissue Tracking via Inter-Frame and Adaptive-Template Matching (Jiaxin Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiaxin Guo, Jiangliu Wang, Zhaoshuo Li, Tongyu Jia, Qi Dou, Yun-Hui Liu. (2024)<br><strong>Ada-Tracker: Soft Tissue Tracking via Inter-Frame and Adaptive-Template Matching</strong><br><button class=copy-to-clipboard title="Ada-Tracker: Soft Tissue Tracking via Inter-Frame and Adaptive-Template Matching" index=134>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-134 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06479v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06479v1.pdf filename=2403.06479v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Soft tissue tracking is crucial for computer-assisted interventions. Existing approaches mainly rely on extracting discriminative features from the template and videos to recover corresponding matches. However, it is difficult to adopt these techniques in surgical scenes, where tissues are changing in shape and appearance throughout the surgery. To address this problem, we exploit optical flow to naturally capture the pixel-wise tissue deformations and adaptively correct the tracked template. Specifically, we first implement an inter-frame matching mechanism to extract a coarse region of interest based on optical flow from consecutive frames. To accommodate appearance change and alleviate drift, we then propose an adaptive-template matching method, which updates the tracked template based on the reliability of the estimates. Our approach, Ada-Tracker, enjoys both short-term dynamics modeling by capturing local deformations and long-term dynamics modeling by introducing global temporal compensation. We evaluate our approach on the public SurgT <b>benchmark,</b> which is generated from Hamlyn, SCARED, and Kidney boundary datasets. The experimental results show that Ada-Tracker achieves superior accuracy and performs more robustly against prior works. Code is available at <a href=https://github.com/wrld/Ada-Tracker>https://github.com/wrld/Ada-Tracker</a>.</p></p class="citation"></blockquote><h3 id=9697--135318-put-myself-in-your-shoes-lifting-the-egocentric-perspective-from-exocentric-videos-mi-luo-et-al-2024>(96/97 | 135/318) Put Myself in Your Shoes: Lifting the Egocentric Perspective from Exocentric Videos (Mi Luo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mi Luo, Zihui Xue, Alex Dimakis, Kristen Grauman. (2024)<br><strong>Put Myself in Your Shoes: Lifting the Egocentric Perspective from Exocentric Videos</strong><br><button class=copy-to-clipboard title="Put Myself in Your Shoes: Lifting the Egocentric Perspective from Exocentric Videos" index=135>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-135 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06351v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06351v1.pdf filename=2403.06351v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We investigate exocentric-to-egocentric cross-view translation, which aims to generate a first-person (egocentric) view of an actor based on a video recording that captures the actor from a third-person (exocentric) perspective. To this end, we propose a generative framework called Exo2Ego that decouples the translation process into two stages: high-level structure transformation, which explicitly encourages cross-view correspondence between exocentric and egocentric views, and a diffusion-based pixel-level hallucination, which incorporates a hand layout prior to enhance the fidelity of the generated egocentric view. To pave the way for future advancements in this field, we curate a comprehensive exo-to-ego cross-view translation <b>benchmark.</b> It consists of a diverse collection of synchronized ego-exo tabletop activity video pairs sourced from three public datasets: H2O, Aria Pilot, and Assembly101. The experimental results validate that Exo2Ego delivers photorealistic video results with clear hand manipulation details and outperforms several baselines in terms of both synthesis quality and generalization ability to new actions.</p></p class="citation"></blockquote><h3 id=9797--136318-moab-multi-modal-outer-arithmetic-block-for-fusion-of-histopathological-images-and-genetic-data-for-brain-tumor-grading-omnia-alwazzan-et-al-2024>(97/97 | 136/318) MOAB: Multi-Modal Outer Arithmetic Block For Fusion Of Histopathological Images And Genetic Data For Brain Tumor Grading (Omnia Alwazzan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Omnia Alwazzan, Abbas Khan, Ioannis Patras, Gregory Slabaugh. (2024)<br><strong>MOAB: Multi-Modal Outer Arithmetic Block For Fusion Of Histopathological Images And Genetic Data For Brain Tumor Grading</strong><br><button class=copy-to-clipboard title="MOAB: Multi-Modal Outer Arithmetic Block For Fusion Of Histopathological Images And Genetic Data For Brain Tumor Grading" index=136>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-136 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06349v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06349v1.pdf filename=2403.06349v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Brain tumors are an abnormal growth of cells in the brain. They can be classified into distinct grades based on their growth. Often grading is performed based on a histological image and is one of the most significant predictors of a patients prognosis, the higher the grade, the more aggressive the tumor. Correct diagnosis of a tumor grade remains challenging. Though histopathological grading has been shown to be prognostic, results are subject to interobserver variability, even among experienced pathologists. Recently, the World Health Organization reported that advances in molecular genetics have led to improvements in tumor classification. This paper seeks to integrate histological images and genetic data for improved computer-aided diagnosis. We propose a novel <b>Multi-modal</b> Outer Arithmetic Block (MOAB) based on arithmetic operations to combine latent representations of the different modalities for predicting the tumor grade (Grade \rom{2}, \rom{3} and \rom{4}). Extensive experiments evaluate the effectiveness of our approach. By applying MOAB to The Cancer Genome Atlas (TCGA) glioma dataset, we show that it can improve separation between similar classes (Grade \rom{2} and \rom{3}) and outperform prior state-of-the-art grade classification techniques.</p></p class="citation"></blockquote><h2 id=csir-8>cs.IR (8)</h2><h3 id=18--137318-coral-collaborative-retrieval-augmented-large-language-models-improve-long-tail-recommendation-junda-wu-et-al-2024>(1/8 | 137/318) CoRAL: Collaborative Retrieval-Augmented Large Language Models Improve Long-tail Recommendation (Junda Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junda Wu, Cheng-Chun Chang, Tong Yu, Zhankui He, Jianing Wang, Yupeng Hou, Julian McAuley. (2024)<br><strong>CoRAL: Collaborative Retrieval-Augmented Large Language Models Improve Long-tail Recommendation</strong><br><button class=copy-to-clipboard title="CoRAL: Collaborative Retrieval-Augmented Large Language Models Improve Long-tail Recommendation" index=137>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-137 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-AI, cs-IR, cs.IR<br>Keyword Score: 80<br>Keywords: Recommendation, Recommender System, Reinforcement Learning, Reasoning, Large Language Model, Large Language Model, Prompt, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06447v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06447v1.pdf filename=2403.06447v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The long-tail <b>recommendation</b> is a challenging task for traditional <b>recommender</b> <b>systems,</b> due to data sparsity and data imbalance issues. The recent development of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> has shown their abilities in complex <b>reasoning,</b> which can help to deduce users&rsquo; preferences based on very few previous interactions. However, since most <b>LLM-based</b> systems rely on items&rsquo; semantic meaning as the sole evidence for <b>reasoning,</b> the collaborative information of user-item interactions is neglected, which can cause the <b>LLM&rsquo;s</b> <b>reasoning</b> to be misaligned with task-specific collaborative information of the dataset. To further align <b>LLMs&rsquo;</b> <b>reasoning</b> to task-specific user-item interaction knowledge, we introduce collaborative retrieval-augmented <b>LLMs,</b> CoRAL, which directly incorporate collaborative evidence into the <b>prompts.</b> Based on the retrieved user-item interactions, the <b>LLM</b> can analyze shared and distinct preferences among users, and <b>summarize</b> the patterns indicating which types of users would be attracted by certain items. The retrieved collaborative evidence <b>prompts</b> the <b>LLM</b> to align its <b>reasoning</b> with the user-item interaction patterns in the dataset. However, since the capacity of the input <b>prompt</b> is limited, finding the minimally-sufficient collaborative information for <b>recommendation</b> tasks can be challenging. We propose to find the optimal interaction set through a sequential decision-making process and develop a retrieval policy learned through a <b>reinforcement</b> <b>learning</b> (RL) framework, CoRAL. Our experimental results show that CoRAL can significantly improve <b>LLMs&rsquo;</b> <b>reasoning</b> abilities on specific <b>recommendation</b> tasks. Our analysis also reveals that CoRAL can more efficiently explore collaborative information through <b>reinforcement</b> <b>learning.</b></p></p class="citation"></blockquote><h3 id=28--138318-kellmrec-knowledge-enhanced-large-language-models-for-recommendation-weiqing-luo-et-al-2024>(2/8 | 138/318) KELLMRec: Knowledge-Enhanced Large Language Models for Recommendation (Weiqing Luo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Weiqing Luo, Chonggang Song, Lingling Yi, Gong Cheng. (2024)<br><strong>KELLMRec: Knowledge-Enhanced Large Language Models for Recommendation</strong><br><button class=copy-to-clipboard title="KELLMRec: Knowledge-Enhanced Large Language Models for Recommendation" index=138>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-138 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-AI, cs-CL, cs-IR, cs.IR<br>Keyword Score: 70<br>Keywords: Contrastive Learning, Recommendation, Recommender System, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06642v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06642v1.pdf filename=2403.06642v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The utilization of semantic information is an important research problem in the field of <b>recommender</b> <b>systems,</b> which aims to complement the missing parts of mainstream ID-based approaches. With the rise of <b>LLM,</b> its ability to act as a knowledge base and its <b>reasoning</b> capability have opened up new possibilities for this research area, making <b>LLM-based</b> <b>recommendation</b> an emerging research direction. However, directly using <b>LLM</b> to process semantic information for <b>recommendation</b> scenarios is unreliable and sub-optimal due to several problems such as hallucination. A promising way to cope with this is to use external knowledge to aid <b>LLM</b> in generating truthful and usable text. Inspired by the above motivation, we propose a Knowledge-Enhanced LLMRec method. In addition to using external knowledge in <b>prompts,</b> the proposed method also includes a knowledge-based <b>contrastive</b> <b>learning</b> scheme for training. Experiments on public datasets and in-enterprise datasets validate the effectiveness of the proposed method.</p></p class="citation"></blockquote><h3 id=38--139318-recai-leveraging-large-language-models-for-next-generation-recommender-systems-jianxun-lian-et-al-2024>(3/8 | 139/318) RecAI: Leveraging Large Language Models for Next-Generation Recommender Systems (Jianxun Lian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianxun Lian, Yuxuan Lei, Xu Huang, Jing Yao, Wei Xu, Xing Xie. (2024)<br><strong>RecAI: Leveraging Large Language Models for Next-Generation Recommender Systems</strong><br><button class=copy-to-clipboard title="RecAI: Leveraging Large Language Models for Next-Generation Recommender Systems" index=139>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-139 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: 68T50, cs-AI, cs-IR, cs.IR<br>Keyword Score: 40<br>Keywords: Recommendation, Recommender System, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06465v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06465v1.pdf filename=2403.06465v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces RecAI, a practical toolkit designed to augment or even revolutionize <b>recommender</b> <b>systems</b> with the advanced capabilities of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> RecAI provides a suite of tools, including <b>Recommender</b> <b>AI</b> Agent, <b>Recommendation-oriented</b> Language Models, Knowledge Plugin, RecExplainer, and Evaluator, to facilitate the integration of <b>LLMs</b> into <b>recommender</b> <b>systems</b> from multifaceted perspectives. The new generation of <b>recommender</b> <b>systems,</b> empowered by <b>LLMs,</b> are expected to be more versatile, explainable, conversational, and controllable, paving the way for more intelligent and user-centric <b>recommendation</b> experiences. We hope the open-source of RecAI can help accelerate evolution of new advanced <b>recommender</b> <b>systems.</b> The source code of RecAI is available at \url{https://github.com/microsoft/RecAI}.</p></p class="citation"></blockquote><h3 id=48--140318-post-training-attribute-unlearning-in-recommender-systems-chaochao-chen-et-al-2024>(4/8 | 140/318) Post-Training Attribute Unlearning in Recommender Systems (Chaochao Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chaochao Chen, Yizhao Zhang, Yuyuan Li, Dan Meng, Jun Wang, Xiaoli Zheng, Jianwei Yin. (2024)<br><strong>Post-Training Attribute Unlearning in Recommender Systems</strong><br><button class=copy-to-clipboard title="Post-Training Attribute Unlearning in Recommender Systems" index=140>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-140 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 30<br>Keywords: Recommendation, Recommender System, Stochastic Gradient Descent<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06737v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06737v1.pdf filename=2403.06737v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the growing privacy concerns in <b>recommender</b> <b>systems,</b> <b>recommendation</b> unlearning is getting increasing attention. Existing studies predominantly use training data, i.e., model inputs, as unlearning target. However, attackers can extract private information from the model even if it has not been explicitly encountered during training. We name this unseen information as \textit{attribute} and treat it as unlearning target. To protect the sensitive attribute of users, Attribute Unlearning (AU) aims to make target attributes indistinguishable. In this paper, we focus on a strict but practical setting of AU, namely Post-Training Attribute Unlearning (PoT-AU), where unlearning can only be performed after the training of the <b>recommendation</b> model is completed. To address the PoT-AU problem in <b>recommender</b> <b>systems,</b> we propose a two-component loss function. The first component is distinguishability loss, where we design a distribution-based measurement to make attribute labels indistinguishable from attackers. We further extend this measurement to handle multi-class attribute cases with efficient computational overhead. The second component is regularization loss, where we explore a function-space measurement that effectively maintains <b>recommendation</b> performance compared to parameter-space regularization. We use <b>stochastic</b> <b>gradient</b> <b>descent</b> algorithm to optimize our proposed loss. Extensive experiments on four real-world datasets demonstrate the effectiveness of our proposed methods.</p></p class="citation"></blockquote><h3 id=58--141318-toolrerank-adaptive-and-hierarchy-aware-reranking-for-tool-retrieval-yuanhang-zheng-et-al-2024>(5/8 | 141/318) ToolRerank: Adaptive and Hierarchy-Aware Reranking for Tool Retrieval (Yuanhang Zheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuanhang Zheng, Peng Li, Wei Liu, Yang Liu, Jian Luan, Bin Wang. (2024)<br><strong>ToolRerank: Adaptive and Hierarchy-Aware Reranking for Tool Retrieval</strong><br><button class=copy-to-clipboard title="ToolRerank: Adaptive and Hierarchy-Aware Reranking for Tool Retrieval" index=141>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-141 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 30<br>Keywords: Rerank, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06551v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06551v1.pdf filename=2403.06551v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Tool learning aims to extend the capabilities of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> with external tools. A major challenge in tool learning is how to support a <b>large</b> <b>number</b> <b>of</b> tools, including unseen tools. To address this challenge, previous studies have proposed retrieving suitable tools for the <b>LLM</b> based on the user query. However, previously proposed methods do not consider the differences between seen and unseen tools, nor do they take the hierarchy of the tool library into account, which may lead to suboptimal performance for tool retrieval. Therefore, to address the aforementioned issues, we propose ToolRerank, an adaptive and hierarchy-aware <b>reranking</b> method for tool retrieval to further refine the retrieval results. Specifically, our proposed ToolRerank includes Adaptive Truncation, which truncates the retrieval results related to seen and unseen tools at different positions, and Hierarchy-Aware <b>Reranking,</b> which makes retrieval results more concentrated for single-tool queries and more diverse for multi-tool queries. Experimental results show that ToolRerank can improve the quality of the retrieval results, leading to better execution results generated by the <b>LLM.</b></p></p class="citation"></blockquote><h3 id=68--142318-metasplit-meta-split-network-for-limited-stock-product-recommendation-wenhao-wu-et-al-2024>(6/8 | 142/318) MetaSplit: Meta-Split Network for Limited-Stock Product Recommendation (Wenhao Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenhao Wu, Jialiang Zhou, Ailong He, Shuguang Han, Jufeng Chen, Bo Zheng. (2024)<br><strong>MetaSplit: Meta-Split Network for Limited-Stock Product Recommendation</strong><br><button class=copy-to-clipboard title="MetaSplit: Meta-Split Network for Limited-Stock Product Recommendation" index=142>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-142 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 20<br>Keywords: Meta Learning, Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06747v3 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06747v3.pdf filename=2403.06747v3.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Compared to business-to-consumer (B2C) e-commerce systems, consumer-to-consumer (C2C) e-commerce platforms usually encounter the limited-stock problem, that is, a product can only be sold one time in a C2C system. This poses several unique challenges for click-through rate (CTR) prediction. Due to limited user interactions for each product (i.e. item), the corresponding item embedding in the CTR model may not easily converge. This makes the conventional sequence modeling based approaches cannot effectively utilize user history information since historical user behaviors contain a mixture of items with different volume of stocks. Particularly, the attention mechanism in a sequence model tends to assign higher score to products with more accumulated user interactions, making limited-stock products being ignored and contribute less to the final output. To this end, we propose the <b>Meta-Split</b> <b>Network</b> (MSNet) to split user history sequence regarding to the volume of stock for each product, and adopt differentiated modeling approaches for different sequences. As for the limited-stock products, a <b>meta-learning</b> <b>approach</b> is applied to address the problem of inconvergence, which is achieved by designing <b>meta</b> <b>scaling</b> and shifting networks with ID and side information. In addition, traditional approach can hardly update item embedding once the product is consumed. Thereby, we propose an auxiliary loss that makes the parameters updatable even when the product is no longer in distribution. To the best of our knowledge, this is the first solution addressing the <b>recommendation</b> of limited-stock product. Experimental results on the production dataset and online A/B testing demonstrate the effectiveness of our proposed method.</p></p class="citation"></blockquote><h3 id=78--143318-repeated-padding-as-data-augmentation-for-sequential-recommendation-yizhou-dang-et-al-2024>(7/8 | 143/318) Repeated Padding as Data Augmentation for Sequential Recommendation (Yizhou Dang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yizhou Dang, Yuting Liu, Enneng Yang, Guibing Guo, Linying Jiang, Xingwei Wang, Jianzhe Zhao. (2024)<br><strong>Repeated Padding as Data Augmentation for Sequential Recommendation</strong><br><button class=copy-to-clipboard title="Repeated Padding as Data Augmentation for Sequential Recommendation" index=143>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-143 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 20<br>Keywords: Data Augmentation, Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06372v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06372v1.pdf filename=2403.06372v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Sequential <b>recommendation</b> aims to provide users with personalized suggestions based on their historical interactions. When training sequential models, padding is a widely adopted technique for two main reasons: 1) The vast majority of models can only handle fixed-length sequences; 2) Batching-based training needs to ensure that the sequences in each batch have the same length. The special value \emph{0} is usually used as the padding content, which does not contain the actual information and is ignored in the model calculations. This common-sense padding strategy leads us to a problem that has never been explored before: \emph{Can we fully utilize this idle input space by padding other content to further improve model performance and training efficiency?} In this paper, we propose a simple yet effective padding method called \textbf{Rep}eated \textbf{Pad}ding (\textbf{RepPad}). Specifically, we use the original interaction sequences as the padding content and fill it to the padding positions during model training. This operation can be performed a finite number of times or repeated until the input sequences&rsquo; length reaches the maximum limit. Our RepPad can be viewed as a sequence-level <b>data</b> <b>augmentation</b> strategy. Unlike most existing works, our method contains no trainable parameters or hyperparameters and is a plug-and-play <b>data</b> <b>augmentation</b> operation. Extensive experiments on various categories of sequential models and five real-world datasets demonstrate the effectiveness and efficiency of our approach. The average <b>recommendation</b> performance improvement is up to 60.3% on GRU4Rec and 24.3% on SASRec. We also provide in-depth analysis and explanation of what makes RepPad effective from multiple perspectives. The source code will be released to ensure the reproducibility of our experiments.</p></p class="citation"></blockquote><h3 id=88--144318-splade-v3-new-baselines-for-splade-carlos-lassance-et-al-2024>(8/8 | 144/318) SPLADE-v3: New baselines for SPLADE (Carlos Lassance et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Carlos Lassance, Hervé Déjean, Thibault Formal, Stéphane Clinchant. (2024)<br><strong>SPLADE-v3: New baselines for SPLADE</strong><br><button class=copy-to-clipboard title="SPLADE-v3: New baselines for SPLADE" index=144>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-144 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-CL, cs-IR, cs.IR<br>Keyword Score: 13<br>Keywords: Benchmarking, Out-of-domain<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06789v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06789v1.pdf filename=2403.06789v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A companion to the release of the latest version of the SPLADE library. We describe changes to the training structure and present our latest series of models &ndash; SPLADE-v3. We compare this new version to BM25, SPLADE++, as well as re-rankers, and showcase its effectiveness via a meta-analysis over more than 40 query sets. SPLADE-v3 further pushes the limit of SPLADE models: it is statistically significantly more effective than both BM25 and SPLADE++, while comparing well to cross-encoder re-rankers. Specifically, it gets more than 40 MRR@10 on the MS MARCO dev set, and improves by 2% the <b>out-of-domain</b> results on the BEIR <b>benchmark.</b></p></p class="citation"></blockquote><h2 id=eesssp-3>eess.SP (3)</h2><h3 id=13--145318-zero-shot-ecg-classification-with-multimodal-learning-and-test-time-clinical-knowledge-enhancement-che-liu-et-al-2024>(1/3 | 145/318) Zero-Shot ECG Classification with Multimodal Learning and Test-time Clinical Knowledge Enhancement (Che Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Che Liu, Zhongwei Wan, Cheng Ouyang, Anand Shah, Wenjia Bai, Rossella Arcucci. (2024)<br><strong>Zero-Shot ECG Classification with Multimodal Learning and Test-time Clinical Knowledge Enhancement</strong><br><button class=copy-to-clipboard title="Zero-Shot ECG Classification with Multimodal Learning and Test-time Clinical Knowledge Enhancement" index=145>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-145 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SP<br>Categories: cs-AI, cs-LG, eess-SP, eess.SP<br>Keyword Score: 74<br>Keywords: Benchmarking, Multi-modal, Multi-modal, Representation Learning, Self-supervised Learning, Self-supervised Learning, Zero-shot, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06659v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06659v1.pdf filename=2403.06659v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Electrocardiograms (ECGs) are non-invasive diagnostic tools crucial for detecting cardiac arrhythmic diseases in clinical practice. While ECG <b>Self-supervised</b> <b>Learning</b> (eSSL) methods show promise in <b>representation</b> <b>learning</b> from unannotated ECG data, they often overlook the clinical knowledge that can be found in reports. This oversight and the requirement for annotated samples for downstream tasks limit eSSL&rsquo;s versatility. In this work, we address these issues with the <b>Multimodal</b> ECG <b>Representation</b> <b>Learning</b> (MERL}) framework. Through <b>multimodal</b> learning on ECG records and associated reports, MERL is capable of performing <b>zero-shot</b> ECG classification with text <b>prompts,</b> eliminating the need for training data in downstream tasks. At test time, we propose the Clinical Knowledge Enhanced <b>Prompt</b> Engineering (CKEPE) approach, which uses <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> to exploit external expert-verified clinical knowledge databases, generating more descriptive <b>prompts</b> and reducing hallucinations in <b>LLM-generated</b> content to boost <b>zero-shot</b> classification. Based on MERL, we perform the first <b>benchmark</b> across six public ECG datasets, showing the superior performance of MERL compared against eSSL methods. Notably, MERL achieves an average AUC score of 75.2% in <b>zero-shot</b> classification (without training data), 3.2% higher than linear probed eSSL methods with 10% annotated training data, averaged across all six datasets.</p></p class="citation"></blockquote><h3 id=23--146318-distributed-average-consensus-via-noisy-and-non-coherent-over-the-air-aggregation-huiwen-yang-et-al-2024>(2/3 | 146/318) Distributed Average Consensus via Noisy and Non-Coherent Over-the-Air Aggregation (Huiwen Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Huiwen Yang, Xiaomeng Chen, Lingying Huang, Subhrakanti Dey, Ling Shi. (2024)<br><strong>Distributed Average Consensus via Noisy and Non-Coherent Over-the-Air Aggregation</strong><br><button class=copy-to-clipboard title="Distributed Average Consensus via Noisy and Non-Coherent Over-the-Air Aggregation" index=146>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-146 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SP<br>Categories: cs-SY, eess-SP, eess-SY, eess.SP<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06920v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06920v1.pdf filename=2403.06920v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Over-the-air aggregation has attracted widespread attention for its potential advantages in task-oriented applications, such as distributed sensing, learning, and consensus. In this paper, we develop a communication-efficient distributed average consensus protocol by utilizing over-the-air aggregation, which exploits the superposition property of wireless channels rather than combat it. Noisy channels and non-coherent transmission are taken into account, and only half-duplex transceivers are required. We prove that the system can achieve average consensus in mean square and even almost surely under the proposed protocol. Furthermore, we extend the analysis to the scenarios with time-varying topology. Numerical <b>simulation</b> shows the effectiveness of the proposed protocol.</p></p class="citation"></blockquote><h3 id=33--147318-lidar-point-cloud-based-multiple-vehicle-tracking-with-probabilistic-measurement-region-association-guanhua-ding-et-al-2024>(3/3 | 147/318) LiDAR Point Cloud-based Multiple Vehicle Tracking with Probabilistic Measurement-Region Association (Guanhua Ding et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guanhua Ding, Jianan Liu, Yuxuan Xia, Tao Huang, Bing Zhu, Jinping Sun. (2024)<br><strong>LiDAR Point Cloud-based Multiple Vehicle Tracking with Probabilistic Measurement-Region Association</strong><br><button class=copy-to-clipboard title="LiDAR Point Cloud-based Multiple Vehicle Tracking with Probabilistic Measurement-Region Association" index=147>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-147 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SP<br>Categories: cs-RO, eess-SP, eess.SP<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06423v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06423v2.pdf filename=2403.06423v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multiple extended target tracking (ETT) has gained increasing attention due to the development of high-precision LiDAR and radar sensors in automotive applications. For LiDAR point cloud-based vehicle tracking, this paper presents a probabilistic measurement-region association (PMRA) ETT model, which can describe the complex measurement distribution by partitioning the target extent into different regions. The PMRA model overcomes the drawbacks of previous data-region association (DRA) models by eliminating the approximation error of constrained estimation and using continuous integrals to more reliably calculate the association probabilities. Furthermore, the PMRA model is integrated with the Poisson multi-Bernoulli mixture (PMBM) filter for tracking multiple vehicles. <b>Simulation</b> results illustrate the superior estimation accuracy of the proposed PMRA-PMBM filter in terms of both positions and extents of the vehicles comparing with PMBM filters using the gamma Gaussian inverse Wishart and DRA implementations.</p></p class="citation"></blockquote><h2 id=cslg-63>cs.LG (63)</h2><h3 id=163--148318-which-llm-to-play-convergence-aware-online-model-selection-with-time-increasing-bandits-yu-xia-et-al-2024>(1/63 | 148/318) Which LLM to Play? Convergence-Aware Online Model Selection with Time-Increasing Bandits (Yu Xia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yu Xia, Fang Kong, Tong Yu, Liya Guo, Ryan A. Rossi, Sungchul Kim, Shuai Li. (2024)<br><strong>Which LLM to Play? Convergence-Aware Online Model Selection with Time-Increasing Bandits</strong><br><button class=copy-to-clipboard title="Which LLM to Play? Convergence-Aware Online Model Selection with Time-Increasing Bandits" index=148>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-148 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 70<br>Keywords: Bandit Algorithm, Bandit Algorithm, Fine-tuning, Fine-tuning, Recommendation, Chatbot, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07213v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07213v1.pdf filename=2403.07213v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Web-based applications such as <b>chatbots,</b> search engines and news <b>recommendations</b> continue to grow in scale and complexity with the recent surge in the adoption of <b>LLMs.</b> Online model selection has thus garnered increasing attention due to the need to choose the best model among a diverse set while balancing task reward and exploration cost. Organizations faces decisions like whether to employ a costly API-based <b>LLM</b> or a locally <b>finetuned</b> small <b>LLM,</b> weighing cost against performance. Traditional selection methods often evaluate every candidate model before choosing one, which are becoming impractical given the rising costs of training and <b>finetuning</b> <b>LLMs.</b> Moreover, it is undesirable to allocate excessive resources towards exploring poor-performing models. While some recent works leverage online <b>bandit</b> <b>algorithm</b> to manage such exploration-exploitation trade-off in model selection, they tend to overlook the increasing-then-converging trend in model performances as the model is iteratively <b>finetuned,</b> leading to less accurate predictions and suboptimal model selections. In this paper, we propose a time-increasing <b>bandit</b> <b>algorithm</b> TI-UCB, which effectively predicts the increase of model performances due to <b>finetuning</b> and efficiently balances exploration and exploitation in model selection. To further capture the converging points of models, we develop a change detection mechanism by comparing consecutive increase predictions. We theoretically prove that our algorithm achieves a logarithmic regret upper bound in a typical increasing <b>bandit</b> <b>setting,</b> which implies a fast convergence rate. The advantage of our method is also empirically validated through extensive experiments on classification model selection and online selection of <b>LLMs.</b> Our results highlight the importance of utilizing increasing-then-converging pattern for more efficient and economic model selection in the deployment of <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=263--149318-comq-a-backpropagation-free-algorithm-for-post-training-quantization-aozhong-zhang-et-al-2024>(2/63 | 149/318) COMQ: A Backpropagation-Free Algorithm for Post-Training Quantization (Aozhong Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aozhong Zhang, Zi Yang, Naigang Wang, Yingyong Qin, Jack Xin, Xin Li, Penghang Yin. (2024)<br><strong>COMQ: A Backpropagation-Free Algorithm for Post-Training Quantization</strong><br><button class=copy-to-clipboard title="COMQ: A Backpropagation-Free Algorithm for Post-Training Quantization" index=149>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-149 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 70<br>Keywords: Vision Transformer, Convolution, Convolutional Neural Network, Quantization, Quantization, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07134v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07134v1.pdf filename=2403.07134v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Post-training <b>quantization</b> (PTQ) has emerged as a practical approach to compress large neural networks, making them highly efficient for deployment. However, effectively reducing these models to their low-bit counterparts without compromising the original accuracy remains a key challenge. In this paper, we propose an innovative PTQ algorithm termed COMQ, which sequentially conducts coordinate-wise minimization of the layer-wise reconstruction errors. We consider the widely used integer <b>quantization,</b> where every <b>quantized</b> weight can be decomposed into a shared floating-point scalar and an integer bit-code. Within a fixed layer, COMQ treats all the scaling factor(s) and bit-codes as the variables of the reconstruction error. Every iteration improves this error along a single coordinate while keeping all other variables constant. COMQ is easy to use and requires no hyper-parameter tuning. It instead involves only dot products and rounding operations. We update these variables in a carefully designed greedy order, significantly enhancing the accuracy. COMQ achieves remarkable results in quantizing 4-bit <b>Vision</b> <b>Transformers,</b> with a negligible loss of less than 1% in Top-1 accuracy. In 4-bit INT <b>quantization</b> of <b>convolutional</b> <b>neural</b> <b>networks,</b> COMQ maintains near-lossless accuracy with a minimal drop of merely 0.3% in Top-1 accuracy.</p></p class="citation"></blockquote><h3 id=363--150318-counterfactual-reasoning-with-knowledge-graph-embeddings-lena-zellinger-et-al-2024>(3/63 | 150/318) Counterfactual Reasoning with Knowledge Graph Embeddings (Lena Zellinger et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lena Zellinger, Andreas Stephan, Benjamin Roth. (2024)<br><strong>Counterfactual Reasoning with Knowledge Graph Embeddings</strong><br><button class=copy-to-clipboard title="Counterfactual Reasoning with Knowledge Graph Embeddings" index=150>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-150 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 66<br>Keywords: Graph, Graph Embedding, Benchmarking, Counter-factual, Knowledge Graph, Knowledge Graph, ChatGPT, Counterfactual Reasoning, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06936v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06936v1.pdf filename=2403.06936v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Knowledge</b> <b>graph</b> <b>embeddings</b> (KGEs) were originally developed to infer true but missing facts in incomplete <b>knowledge</b> <b>repositories.</b> In this paper, we link <b>knowledge</b> <b>graph</b> <b>completion</b> and <b>counterfactual</b> <b>reasoning</b> via our new task CFKGR. We model the original world state as a <b>knowledge</b> <b>graph,</b> <b>hypothetical</b> scenarios as edges added to the <b>graph,</b> <b>and</b> plausible changes to the <b>graph</b> <b>as</b> inferences from logical rules. We create corresponding <b>benchmark</b> datasets, which contain diverse hypothetical scenarios with plausible changes to the original <b>knowledge</b> <b>graph</b> <b>and</b> facts that should be retained. We develop COULDD, a general method for adapting existing <b>knowledge</b> <b>graph</b> <b>embeddings</b> given a hypothetical premise, and evaluate it on our <b>benchmark.</b> Our results indicate that KGEs learn patterns in the <b>graph</b> <b>without</b> explicit training. We further observe that KGEs adapted with COULDD solidly detect plausible <b>counterfactual</b> <b>changes</b> to the <b>graph</b> <b>that</b> follow these patterns. An evaluation on human-annotated data reveals that KGEs adapted with COULDD are mostly unable to recognize changes to the <b>graph</b> <b>that</b> do not follow learned inference rules. In contrast, <b>ChatGPT</b> mostly outperforms KGEs in detecting plausible changes to the <b>graph</b> <b>but</b> has poor <b>knowledge</b> <b>retention.</b> In summary, CFKGR connects two previously distinct areas, namely <b>KG</b> completion and <b>counterfactual</b> <b>reasoning.</b></p></p class="citation"></blockquote><h3 id=463--151318-learning-with-noisy-foundation-models-hao-chen-et-al-2024>(4/63 | 151/318) Learning with Noisy Foundation Models (Hao Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hao Chen, Jindong Wang, Zihan Wang, Ran Tao, Hongxin Wei, Xing Xie, Masashi Sugiyama, Bhiksha Raj. (2024)<br><strong>Learning with Noisy Foundation Models</strong><br><button class=copy-to-clipboard title="Learning with Noisy Foundation Models" index=151>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-151 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-CV, cs-LG, cs.LG<br>Keyword Score: 65<br>Keywords: Black Box, Foundation Model, Out-of-domain, Self-supervised Learning, Supervised Learning, Image2text, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06869v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06869v1.pdf filename=2403.06869v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Foundation</b> <b>models</b> are usually pre-trained on large-scale datasets and then adapted to downstream tasks through tuning. However, the large-scale pre-training datasets, often inaccessible or too expensive to handle, can contain label noise that may adversely affect the generalization of the model and pose unexpected risks. This paper stands out as the first work to comprehensively understand and analyze the nature of noise in pre-training datasets and then effectively mitigate its impacts on downstream tasks. Specifically, through extensive experiments of fully-supervised and <b>image-text</b> contrastive pre-training on synthetic noisy ImageNet-1K, YFCC15M, and CC12M datasets, we demonstrate that, while slight noise in pre-training can benefit in-domain (ID) performance, where the training and testing data share a similar distribution, it always deteriorates <b>out-of-domain</b> (OOD) performance, where training and testing distributions are significantly different. These observations are agnostic to scales of pre-training datasets, pre-training noise types, model architectures, pre-training objectives, downstream tuning methods, and downstream applications. We empirically ascertain that the reason behind this is that the pre-training noise shapes the feature space differently. We then propose a tuning method (NMTune) to affine the feature space to mitigate the malignant effect of noise and improve generalization, which is applicable in both parameter-efficient and <b>black-box</b> <b>tuning</b> manners. We additionally conduct extensive experiments on popular vision and language models, including APIs, which are <b>supervised</b> and <b>self-supervised</b> pre-trained on realistic noisy data for evaluation. Our analysis and results demonstrate the importance of this novel and fundamental research direction, which we term as Noisy Model Learning.</p></p class="citation"></blockquote><h3 id=563--152318-joint-embedding-masked-autoencoder-for-self-supervised-learning-of-dynamic-functional-connectivity-from-the-human-brain-jungwon-choi-et-al-2024>(5/63 | 152/318) Joint-Embedding Masked Autoencoder for Self-supervised Learning of Dynamic Functional Connectivity from the Human Brain (Jungwon Choi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jungwon Choi, Hyungi Lee, Byung-Hoon Kim, Juho Lee. (2024)<br><strong>Joint-Embedding Masked Autoencoder for Self-supervised Learning of Dynamic Functional Connectivity from the Human Brain</strong><br><button class=copy-to-clipboard title="Joint-Embedding Masked Autoencoder for Self-supervised Learning of Dynamic Functional Connectivity from the Human Brain" index=152>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-152 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, q-bio-NC<br>Keyword Score: 61<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Autoencoder, Benchmarking, Representation Learning, Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06432v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06432v1.pdf filename=2403.06432v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> have shown promise in learning dynamic functional connectivity for distinguishing phenotypes from human brain networks. However, obtaining extensive labeled clinical data for training is often resource-intensive, making practical application difficult. Leveraging unlabeled data thus becomes crucial for <b>representation</b> <b>learning</b> in a label-scarce setting. Although generative <b>self-supervised</b> <b>learning</b> techniques, especially masked <b>autoencoders,</b> have shown promising results in <b>representation</b> <b>learning</b> in various domains, their application to dynamic <b>graphs</b> <b>for</b> <b>dynamic</b> functional connectivity remains underexplored, facing challenges in capturing high-level semantic <b>representations.</b> <b>Here,</b> we introduce the Spatio-Temporal Joint Embedding Masked <b>Autoencoder</b> (ST-JEMA), drawing inspiration from the Joint Embedding Predictive Architecture (JEPA) in computer vision. ST-JEMA employs a JEPA-inspired strategy for reconstructing dynamic <b>graphs,</b> <b>which</b> <b>enables</b> the learning of higher-level semantic <b>representations</b> <b>considering</b> temporal perspectives, addressing the challenges in fMRI data <b>representation</b> <b>learning.</b> Utilizing the large-scale UK Biobank dataset for <b>self-supervised</b> <b>learning,</b> ST-JEMA shows exceptional <b>representation</b> <b>learning</b> performance on dynamic functional connectivity demonstrating superiority over previous methods in predicting phenotypes and psychiatric diagnoses across eight <b>benchmark</b> fMRI datasets even with limited samples and effectiveness of temporal reconstruction on missing data scenarios. These findings highlight the potential of our approach as a robust <b>representation</b> <b>learning</b> method for leveraging label-scarce fMRI data.</p></p class="citation"></blockquote><h3 id=663--153318-on-the-generalization-ability-of-unsupervised-pretraining-yuyang-deng-et-al-2024>(6/63 | 153/318) On the Generalization Ability of Unsupervised Pretraining (Yuyang Deng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuyang Deng, Junyuan Hong, Jiayu Zhou, Mehrdad Mahdavi. (2024)<br><strong>On the Generalization Ability of Unsupervised Pretraining</strong><br><button class=copy-to-clipboard title="On the Generalization Ability of Unsupervised Pretraining" index=153>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-153 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 60<br>Keywords: Autoencoder, Fine-tuning, Fine-tuning, Unsupervised Learning, Unsupervised Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06871v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06871v1.pdf filename=2403.06871v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advances in <b>unsupervised</b> <b>learning</b> have shown that <b>unsupervised</b> <b>pre-training,</b> followed by <b>fine-tuning,</b> can improve model generalization. However, a rigorous understanding of how the representation function learned on an unlabeled dataset affects the generalization of the <b>fine-tuned</b> model is lacking. Existing theoretical research does not adequately account for the heterogeneity of the distribution and tasks in pre-training and <b>fine-tuning</b> stage. To bridge this gap, this paper introduces a novel theoretical framework that illuminates the critical factor influencing the transferability of knowledge acquired during <b>unsupervised</b> <b>pre-training</b> to the subsequent <b>fine-tuning</b> phase, ultimately affecting the generalization capabilities of the <b>fine-tuned</b> model on downstream tasks. We apply our theoretical framework to analyze generalization bound of two distinct scenarios: Context Encoder pre-training with deep neural networks and Masked <b>Autoencoder</b> pre-training with deep <b>transformers,</b> followed by <b>fine-tuning</b> on a binary classification task. Finally, inspired by our findings, we propose a novel regularization method during pre-training to further enhances the generalization of <b>fine-tuned</b> model. Overall, our results contribute to a better understanding of <b>unsupervised</b> <b>pre-training</b> and <b>fine-tuning</b> paradigm, and can shed light on the design of more effective pre-training algorithms.</p></p class="citation"></blockquote><h3 id=763--154318-in-context-exploration-exploitation-for-reinforcement-learning-zhenwen-dai-et-al-2024>(7/63 | 154/318) In-context Exploration-Exploitation for Reinforcement Learning (Zhenwen Dai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhenwen Dai, Federico Tomasi, Sina Ghiassian. (2024)<br><strong>In-context Exploration-Exploitation for Reinforcement Learning</strong><br><button class=copy-to-clipboard title="In-context Exploration-Exploitation for Reinforcement Learning" index=154>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-154 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, stat-ML<br>Keyword Score: 60<br>Keywords: Gaussian Process, Offline Reinforcement Learning, Reinforcement Learning, Transformer, In-context Learning, In-context Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06826v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06826v1.pdf filename=2403.06826v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>In-context</b> <b>learning</b> is a promising approach for online policy learning of <b>offline</b> <b>reinforcement</b> <b>learning</b> (RL) methods, which can be achieved at inference time without gradient optimization. However, this method is hindered by significant computational costs resulting from the gathering of large training trajectory sets and the need to train large <b>Transformer</b> models. We address this challenge by introducing an <b>In-context</b> <b>Exploration-Exploitation</b> (ICEE) algorithm, designed to optimize the efficiency of <b>in-context</b> <b>policy</b> learning. Unlike existing models, ICEE performs an exploration-exploitation trade-off at inference time within a <b>Transformer</b> model, without the need for explicit Bayesian inference. Consequently, ICEE can solve Bayesian optimization problems as efficiently as <b>Gaussian</b> <b>process</b> biased methods do, but in significantly less time. Through experiments in grid world environments, we demonstrate that ICEE can learn to solve new RL tasks using only tens of episodes, marking a substantial improvement over the hundreds of episodes needed by the previous <b>in-context</b> <b>learning</b> method.</p></p class="citation"></blockquote><h3 id=863--155318-unraveling-the-mystery-of-scaling-laws-part-i-hui-su-et-al-2024>(8/63 | 155/318) Unraveling the Mystery of Scaling Laws: Part I (Hui Su et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hui Su, Zhi Tian, Xiaoyu Shen, Xunliang Cai. (2024)<br><strong>Unraveling the Mystery of Scaling Laws: Part I</strong><br><button class=copy-to-clipboard title="Unraveling the Mystery of Scaling Laws: Part I" index=155>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-155 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG<br>Keyword Score: 60<br>Keywords: GPT, GPT-4, Gemini, LLaMA, Large Language Model, Scaling Law<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06563v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06563v1.pdf filename=2403.06563v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Scaling</b> <b>law</b> principles indicate a power-law correlation between loss and variables such as model size, dataset size, and computational resources utilized during training. These principles play a vital role in optimizing various aspects of model pre-training, ultimately contributing to the success of <b>large</b> <b>language</b> <b>models</b> such as <b>GPT-4,</b> <b>Llama</b> and <b>Gemini.</b> However, the original <b>scaling</b> <b>law</b> paper by OpenAI did not disclose the complete details necessary to derive the precise <b>scaling</b> <b>law</b> formulas, and their conclusions are only based on models containing up to 1.5 billion parameters. Though some subsequent works attempt to unveil these details and scale to larger models, they often neglect the training dependency of important factors such as the learning rate, context length and batch size, leading to their failure to establish a reliable formula for predicting the test loss trajectory. In this technical report, we confirm that the <b>scaling</b> <b>law</b> formulations proposed in the original OpenAI paper remain valid when <b>scaling</b> <b>the</b> model size up to 33 billion, but the constant coefficients in these formulas vary significantly with the experiment setup. We meticulously identify influential factors and provide transparent, step-by-step instructions to estimate all constant terms in <b>scaling-law</b> <b>formulas</b> by training on models with only 1M~60M parameters. Using these estimated formulas, we showcase the capability to accurately predict various attributes for models with up to 33B parameters before their training, including (1) the minimum possible test loss; (2) the minimum required training steps and processed tokens to achieve a specific loss; (3) the critical batch size with an optimal time/computation trade-off at any loss value; and (4) the complete test loss trajectory with arbitrary batch size.</p></p class="citation"></blockquote><h3 id=963--156318-all-in-one-multi-task-prompting-for-graph-neural-networks-extended-abstract-xiangguo-sun-et-al-2024>(9/63 | 156/318) All in One: Multi-Task Prompting for Graph Neural Networks (Extended Abstract) (Xiangguo Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiangguo Sun, Hong Cheng, Jia Li, Bo Liu, Jihong Guan. (2024)<br><strong>All in One: Multi-Task Prompting for Graph Neural Networks (Extended Abstract)</strong><br><button class=copy-to-clipboard title="All in One: Multi-Task Prompting for Graph Neural Networks (Extended Abstract)" index=156>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-156 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 53<br>Keywords: Graph, Graph Neural Network, Knowledge Distillation, Meta Learning, Prompt, Prompt Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07040v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07040v1.pdf filename=2403.07040v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper is an extended abstract of our original work published in KDD23, where we won the best research paper award (Xiangguo Sun, Hong Cheng, Jia Li, Bo Liu, and Jihong Guan. All in one: Multi-task <b>prompting</b> <b>for</b> <b>graph</b> <b>neural</b> <b>networks.</b> <b>KDD</b> 23) The paper introduces a novel approach to bridging the gap between pre-trained <b>graph</b> <b>models</b> <b>and</b> the diverse tasks they&rsquo;re applied to, inspired by the success of <b>prompt</b> <b>learning</b> in NLP. Recognizing the challenge of aligning pre-trained models with varied <b>graph</b> <b>tasks</b> <b>(node</b> level, edge level, and <b>graph</b> <b>level),</b> <b>which</b> can lead to negative transfer and poor performance, we propose a multi-task <b>prompting</b> <b>method</b> for <b>graphs.</b> <b>This</b> <b>method</b> involves unifying <b>graph</b> <b>and</b> <b>language</b> <b>prompt</b> <b>formats,</b> enabling NLP&rsquo;s <b>prompting</b> <b>strategies</b> to be adapted for <b>graph</b> <b>tasks.</b> <b>By</b> analyzing the task space of <b>graph</b> <b>applications,</b> <b>we</b> reformulate problems to fit <b>graph-level</b> <b>tasks</b> <b>and</b> apply <b>meta-learning</b> <b>to</b> improve <b>prompt</b> <b>initialization</b> for multiple tasks. Experiments show our method&rsquo;s effectiveness in enhancing model performance across different <b>graph</b> <b>tasks.</b> <b>Beyond</b> the original work, in this extended abstract, we further discuss the <b>graph</b> <b>prompt</b> <b>from</b> a bigger picture and provide some of the latest work toward this area.</p></p class="citation"></blockquote><h3 id=1063--157318-advancing-graph-neural-networks-with-hl-hgat-a-hodge-laplacian-and-attention-mechanism-approach-for-heterogeneous-graph-structured-data-jinghan-huang-et-al-2024>(10/63 | 157/318) Advancing Graph Neural Networks with HL-HGAT: A Hodge-Laplacian and Attention Mechanism Approach for Heterogeneous Graph-Structured Data (Jinghan Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinghan Huang, Qiufeng Chen, Yijun Bian, Pengli Zhu, Nanguang Chen, Moo K. Chung, Anqi Qiu. (2024)<br><strong>Advancing Graph Neural Networks with HL-HGAT: A Hodge-Laplacian and Attention Mechanism Approach for Heterogeneous Graph-Structured Data</strong><br><button class=copy-to-clipboard title="Advancing Graph Neural Networks with HL-HGAT: A Hodge-Laplacian and Attention Mechanism Approach for Heterogeneous Graph-Structured Data" index=157>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-157 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 53<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Convolution, Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06687v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06687v1.pdf filename=2403.06687v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>neural</b> <b>networks</b> <b>(GNNs)</b> have proven effective in capturing relationships among nodes in a <b>graph.</b> <b>This</b> <b>study</b> introduces a novel perspective by considering a <b>graph</b> <b>as</b> <b>a</b> simplicial complex, encompassing nodes, edges, triangles, and $k$-simplices, enabling the definition of <b>graph-structured</b> <b>data</b> <b>on</b> any $k$-simplices. Our contribution is the Hodge-Laplacian heterogeneous <b>graph</b> <b>attention</b> <b>network</b> (HL-HGAT), designed to learn heterogeneous signal representations across $k$-simplices. The HL-HGAT incorporates three key components: HL <b>convolutional</b> filters (HL-filters), simplicial projection (SP), and simplicial attention pooling (SAP) operators, applied to $k$-simplices. HL-filters leverage the unique topology of $k$-simplices encoded by the Hodge-Laplacian (HL) operator, operating within the spectral domain of the $k$-th HL operator. To address computation challenges, we introduce a polynomial approximation for HL-filters, exhibiting spatial localization properties. Additionally, we propose a pooling operator to coarsen $k$-simplices, combining features through simplicial attention mechanisms of <b>self-attention</b> and cross-attention via <b>transformers</b> and SP operators, capturing topological interconnections across multiple dimensions of simplices. The HL-HGAT is comprehensively evaluated across diverse <b>graph</b> <b>applications,</b> <b>including</b> NP-hard problems, <b>graph</b> <b>multi-label</b> <b>and</b> classification challenges, and <b>graph</b> <b>regression</b> <b>tasks</b> in logistics, computer vision, biology, chemistry, and neuroscience. The results demonstrate the model&rsquo;s efficacy and versatility in handling a wide range of <b>graph-based</b> scenarios.</p></p class="citation"></blockquote><h3 id=1163--158318-uncertainty-in-graph-neural-networks-a-survey-fangxin-wang-et-al-2024>(11/63 | 158/318) Uncertainty in Graph Neural Networks: A Survey (Fangxin Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fangxin Wang, Yuqing Liu, Kay Liu, Yibo Wang, Sourav Medya, Philip S. Yu. (2024)<br><strong>Uncertainty in Graph Neural Networks: A Survey</strong><br><button class=copy-to-clipboard title="Uncertainty in Graph Neural Networks: A Survey" index=158>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-158 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 43<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Stemming, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07185v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07185v1.pdf filename=2403.07185v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> have been extensively used in various real-world applications. However, the predictive uncertainty of <b>GNNs</b> <b>stemming</b> from diverse sources such as inherent randomness in data and model training errors can lead to unstable and erroneous predictions. Therefore, identifying, quantifying, and utilizing uncertainty are essential to enhance the performance of the model for the downstream tasks as well as the reliability of the <b>GNN</b> predictions. This survey aims to provide a comprehensive overview of the <b>GNNs</b> from the perspective of uncertainty with an emphasis on its integration in <b>graph</b> <b>learning.</b> <b>We</b> compare and <b>summarize</b> existing <b>graph</b> <b>uncertainty</b> <b>theory</b> and methods, alongside the corresponding downstream tasks. Thereby, we bridge the gap between theory and practice, meanwhile connecting different <b>GNN</b> communities. Moreover, our work provides valuable insights into promising directions in this field.</p></p class="citation"></blockquote><h3 id=1263--159318-evaluating-the-energy-efficiency-of-few-shot-learning-for-object-detection-in-industrial-settings-georgios-tsoumplekas-et-al-2024>(12/63 | 159/318) Evaluating the Energy Efficiency of Few-Shot Learning for Object Detection in Industrial Settings (Georgios Tsoumplekas et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Georgios Tsoumplekas, Vladislav Li, Ilias Siniosoglou, Vasileios Argyriou, Sotirios K. Goudos, Ioannis D. Moscholios, Panagiotis Radoglou-Grammatikis, Panagiotis Sarigiannidis. (2024)<br><strong>Evaluating the Energy Efficiency of Few-Shot Learning for Object Detection in Industrial Settings</strong><br><button class=copy-to-clipboard title="Evaluating the Energy Efficiency of Few-Shot Learning for Object Detection in Industrial Settings" index=159>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-159 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs.LG<br>Keyword Score: 43<br>Keywords: Object Detection, Benchmarking, Few-shot, Few-shot Learning, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06631v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06631v1.pdf filename=2403.06631v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the ever-evolving era of Artificial Intelligence (AI), model performance has constituted a key metric driving innovation, leading to an exponential growth in model size and complexity. However, sustainability and energy efficiency have been critical requirements during deployment in contemporary industrial settings, necessitating the use of data-efficient approaches such as <b>few-shot</b> <b>learning.</b> In this paper, to alleviate the burden of lengthy model training and minimize energy consumption, a <b>finetuning</b> approach to adapt standard <b>object</b> <b>detection</b> models to downstream tasks is examined. Subsequently, a thorough case study and evaluation of the energy demands of the developed models, applied in <b>object</b> <b>detection</b> <b>benchmark</b> datasets from volatile industrial environments is presented. Specifically, different <b>finetuning</b> strategies as well as utilization of ancillary evaluation data during training are examined, and the trade-off between performance and efficiency is highlighted in this low-data regime. Finally, this paper introduces a novel way to quantify this trade-off through a customized Efficiency Factor metric.</p></p class="citation"></blockquote><h3 id=1363--160318-a-differential-geometric-view-and-explainability-of-gnn-on-evolving-graphs-yazheng-liu-et-al-2024>(13/63 | 160/318) A Differential Geometric View and Explainability of GNN on Evolving Graphs (Yazheng Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yazheng Liu, Xi Zhang, Sihong Xie. (2024)<br><strong>A Differential Geometric View and Explainability of GNN on Evolving Graphs</strong><br><button class=copy-to-clipboard title="A Differential Geometric View and Explainability of GNN on Evolving Graphs" index=160>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-160 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 43<br>Keywords: Graph Classification, Node Classification, Graph, Graph Neural Network, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06425v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06425v1.pdf filename=2403.06425v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graphs</b> <b>are</b> <b>ubiquitous</b> in social networks and biochemistry, where <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNN)</b> are the state-of-the-art models for prediction. <b>Graphs</b> <b>can</b> <b>be</b> evolving and it is vital to formally model and understand how a trained <b>GNN</b> responds to <b>graph</b> <b>evolution.</b> <b>We</b> propose a smooth parameterization of the <b>GNN</b> predicted distributions using axiomatic attribution, where the distributions are on a low-dimensional manifold within a high-dimensional embedding space. We exploit the differential geometric viewpoint to model distributional evolution as smooth curves on the manifold. We reparameterize families of curves on the manifold and design a convex optimization problem to find a unique curve that concisely approximates the distributional evolution for human interpretation. Extensive experiments on <b>node</b> <b>classification,</b> link prediction, and <b>graph</b> <b>classification</b> <b>tasks</b> with evolving <b>graphs</b> <b>demonstrate</b> <b>the</b> better sparsity, faithfulness, and intuitiveness of the proposed method over the state-of-the-art methods.</p></p class="citation"></blockquote><h3 id=1463--161318-improving-deep-learning-with-prior-knowledge-and-cognitive-models-a-survey-on-enhancing-explainability-adversarial-robustness-and-zero-shot-learning-fuseinin-mumuni-et-al-2024>(14/63 | 161/318) Improving deep learning with prior knowledge and cognitive models: A survey on enhancing explainability, adversarial robustness and zero-shot learning (Fuseinin Mumuni et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fuseinin Mumuni, Alhassan Mumuni. (2024)<br><strong>Improving deep learning with prior knowledge and cognitive models: A survey on enhancing explainability, adversarial robustness and zero-shot learning</strong><br><button class=copy-to-clipboard title="Improving deep learning with prior knowledge and cognitive models: A survey on enhancing explainability, adversarial robustness and zero-shot learning" index=161>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-161 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Few-shot, Zero-shot, Zero-shot Learning, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07078v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07078v1.pdf filename=2403.07078v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We review current and emerging knowledge-informed and brain-inspired cognitive systems for realizing <b>adversarial</b> <b>defenses,</b> eXplainable Artificial Intelligence (XAI), and <b>zero-shot</b> <b>or</b> few-short learning. Data-driven deep learning models have achieved remarkable performance and demonstrated capabilities surpassing human experts in many applications. Yet, their inability to exploit domain knowledge leads to serious performance limitations in practical applications. In particular, deep learning systems are exposed to <b>adversarial</b> <b>attacks,</b> which can trick them into making glaringly incorrect decisions. Moreover, complex data-driven models typically lack interpretability or explainability, i.e., their decisions cannot be understood by human subjects. Furthermore, models are usually trained on standard datasets with a closed-world assumption. Hence, they struggle to generalize to unseen cases during inference in practical open-world environments, thus, raising the zero- or <b>few-shot</b> generalization problem. Although many conventional solutions exist, explicit domain knowledge, brain-inspired neural network and cognitive architectures offer powerful new dimensions towards alleviating these problems. Prior knowledge is represented in appropriate forms and incorporated in deep learning frameworks to improve performance. Brain-inspired cognition methods use computational models that mimic the human mind to enhance intelligent behavior in artificial agents and autonomous robots. Ultimately, these models achieve better explainability, higher <b>adversarial</b> <b>robustness</b> and data-efficient learning, and can, in turn, provide insights for cognitive science and neuroscience-that is, to deepen human understanding on how the brain works in general, and how it handles these problems.</p></p class="citation"></blockquote><h3 id=1563--162318-simplicity-bias-of-transformers-to-learn-low-sensitivity-functions-bhavya-vasudeva-et-al-2024>(15/63 | 162/318) Simplicity Bias of Transformers to Learn Low Sensitivity Functions (Bhavya Vasudeva et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bhavya Vasudeva, Deqing Fu, Tianyi Zhou, Elliott Kau, Youqi Huang, Vatsal Sharan. (2024)<br><strong>Simplicity Bias of Transformers to Learn Low Sensitivity Functions</strong><br><button class=copy-to-clipboard title="Simplicity Bias of Transformers to Learn Low Sensitivity Functions" index=162>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-162 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG, stat-ML<br>Keyword Score: 40<br>Keywords: Convolutional Neural Network, LSTM, Transformer, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06925v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06925v1.pdf filename=2403.06925v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Transformers</b> achieve state-of-the-art accuracy and robustness across many tasks, but an understanding of the inductive biases that they have and how those biases are different from other neural network architectures remains elusive. Various neural network architectures such as fully connected networks have been found to have a simplicity bias towards simple functions of the data; one version of this simplicity bias is a spectral bias to learn simple functions in the Fourier space. In this work, we identify the notion of sensitivity of the model to random changes in the input as a notion of simplicity bias which provides a unified metric to explain the simplicity and spectral bias of <b>transformers</b> across different data modalities. We show that <b>transformers</b> have lower sensitivity than alternative architectures, such as <b>LSTMs,</b> MLPs and <b>CNNs,</b> across both vision and language tasks. We also show that low-sensitivity bias correlates with improved robustness; furthermore, it can also be used as an efficient intervention to further improve the robustness of <b>transformers.</b></p></p class="citation"></blockquote><h3 id=1663--163318-a-geospatial-approach-to-predicting-desert-locust-breeding-grounds-in-africa-ibrahim-salihu-yusuf-et-al-2024>(16/63 | 163/318) A Geospatial Approach to Predicting Desert Locust Breeding Grounds in Africa (Ibrahim Salihu Yusuf et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ibrahim Salihu Yusuf, Mukhtar Opeyemi Yusuf, Kobby Panford-Quainoo, Arnu Pretorius. (2024)<br><strong>A Geospatial Approach to Predicting Desert Locust Breeding Grounds in Africa</strong><br><button class=copy-to-clipboard title="A Geospatial Approach to Predicting Desert Locust Breeding Grounds in Africa" index=163>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-163 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Convolution, Convolutional Neural Network, Fine-tuning, LSTM<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06860v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06860v1.pdf filename=2403.06860v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Desert locust swarms present a major threat to agriculture and food security. Addressing this challenge, our study develops an operationally-ready model for predicting locust breeding grounds, which has the potential to enhance early warning systems and targeted control measures. We curated a dataset from the United Nations Food and Agriculture Organization&rsquo;s (UN-FAO) locust observation records and analyzed it using two types of spatio-temporal input features: remotely-sensed environmental and climate data as well as multi-spectral earth observation images. Our approach employed custom deep learning models (three-dimensional and <b>LSTM-based</b> recurrent <b>convolutional</b> <b>networks),</b> along with the geospatial foundational model Prithvi recently released by Jakubik et al., 2023. These models notably outperformed existing baselines, with the Prithvi-based model, <b>fine-tuned</b> on multi-spectral images from NASA&rsquo;s Harmonized Landsat and Sentinel-2 (HLS) dataset, achieving the highest accuracy, F1 and ROC-AUC scores (83.03%, 81.53% and 87.69%, respectively). A significant finding from our research is that multi-spectral earth observation images alone are sufficient for effective locust breeding ground prediction without the need to explicitly incorporate climatic or environmental features.</p></p class="citation"></blockquote><h3 id=1763--164318-contextgpt-infusing-llms-knowledge-into-neuro-symbolic-activity-recognition-models-luca-arrotta-et-al-2024>(17/63 | 164/318) ContextGPT: Infusing LLMs Knowledge into Neuro-Symbolic Activity Recognition Models (Luca Arrotta et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Luca Arrotta, Claudio Bettini, Gabriele Civitarese, Michele Fiori. (2024)<br><strong>ContextGPT: Infusing LLMs Knowledge into Neuro-Symbolic Activity Recognition Models</strong><br><button class=copy-to-clipboard title="ContextGPT: Infusing LLMs Knowledge into Neuro-Symbolic Activity Recognition Models" index=164>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-164 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Supervised Learning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06586v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06586v1.pdf filename=2403.06586v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Context-aware Human Activity Recognition (HAR) is a hot research area in mobile computing, and the most effective solutions in the literature are based on <b>supervised</b> deep learning models. However, the actual deployment of these systems is limited by the scarcity of labeled data that is required for training. Neuro-Symbolic AI (NeSy) provides an interesting research direction to mitigate this issue, by infusing common-sense knowledge about human activities and the contexts in which they can be performed into HAR deep learning classifiers. Existing NeSy methods for context-aware HAR rely on knowledge encoded in logic-based models (e.g., ontologies) whose design, implementation, and maintenance to capture new activities and contexts require significant human engineering efforts, technical knowledge, and domain expertise. Recent works show that pre-trained <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> effectively encode common-sense knowledge about human activities. In this work, we propose ContextGPT: a novel <b>prompt</b> engineering approach to retrieve from <b>LLMs</b> common-sense knowledge about the relationship between human activities and the context in which they are performed. Unlike ontologies, ContextGPT requires limited human effort and expertise. An extensive evaluation carried out on two public datasets shows how a NeSy model obtained by infusing common-sense knowledge from ContextGPT is effective in data scarcity scenarios, leading to similar (and sometimes better) recognition rates than logic-based approaches with a fraction of the effort.</p></p class="citation"></blockquote><h3 id=1863--165318-semantic-residual-prompts-for-continual-learning-martin-menabue-et-al-2024>(18/63 | 165/318) Semantic Residual Prompts for Continual Learning (Martin Menabue et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Martin Menabue, Emanuele Frascaroli, Matteo Boschini, Enver Sangineto, Lorenzo Bonicelli, Angelo Porrello, Simone Calderara. (2024)<br><strong>Semantic Residual Prompts for Continual Learning</strong><br><button class=copy-to-clipboard title="Semantic Residual Prompts for Continual Learning" index=165>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-165 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 33<br>Keywords: Benchmarking, Continual Learning, Zero-shot, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06870v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06870v2.pdf filename=2403.06870v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Prompt-tuning</b> methods for <b>Continual</b> <b>Learning</b> (CL) freeze a large pre-trained model and focus training on a few parameter vectors termed <b>prompts.</b> Most of these methods organize these vectors in a pool of key-value pairs, and use the input image as query to retrieve the <b>prompts</b> (values). However, as keys are learned while tasks progress, the <b>prompting</b> selection strategy is itself subject to catastrophic forgetting, an issue often overlooked by existing approaches. For instance, <b>prompts</b> introduced to accommodate new tasks might end up interfering with previously learned <b>prompts.</b> To make the selection strategy more stable, we ask a foundational model (CLIP) to select our <b>prompt</b> within a two-level adaptation mechanism. Specifically, the first level leverages standard textual <b>prompts</b> for the CLIP textual encoder, leading to stable class prototypes. The second level, instead, uses these prototypes along with the query image as keys to index a second pool. The retrieved <b>prompts</b> serve to adapt a pre-trained ViT, granting plasticity. In doing so, we also propose a novel residual mechanism to transfer CLIP semantics to the ViT layers. Through extensive analysis on established CL <b>benchmarks,</b> we show that our method significantly outperforms both state-of-the-art CL approaches and the <b>zero-shot</b> CLIP test. Notably, our findings hold true even for datasets with a substantial domain gap w.r.t. the pre-training knowledge of the backbone model, as showcased by experiments on satellite imagery and medical datasets.</p></p class="citation"></blockquote><h3 id=1963--166318-an-efficient-learning-based-solver-comparable-to-metaheuristics-for-the-capacitated-arc-routing-problem-runze-guo-et-al-2024>(19/63 | 166/318) An Efficient Learning-based Solver Comparable to Metaheuristics for the Capacitated Arc Routing Problem (Runze Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Runze Guo, Feng Xue, Anlong Ming, Nicu Sebe. (2024)<br><strong>An Efficient Learning-based Solver Comparable to Metaheuristics for the Capacitated Arc Routing Problem</strong><br><button class=copy-to-clipboard title="An Efficient Learning-based Solver Comparable to Metaheuristics for the Capacitated Arc Routing Problem" index=166>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-166 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, math-OC<br>Keyword Score: 33<br>Keywords: Graph, Fine-tuning, Reinforcement Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07028v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07028v1.pdf filename=2403.07028v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, neural networks (NN) have made great strides in combinatorial optimization. However, they face challenges when solving the capacitated arc routing problem (CARP) which is to find the minimum-cost tour covering all required edges on a <b>graph,</b> while within capacity constraints. In tackling CARP, NN-based approaches tend to lag behind advanced metaheuristics, since they lack directed arc modeling and efficient learning methods tailored for complex CARP. In this paper, we introduce an NN-based solver to significantly narrow the gap with advanced metaheuristics while exhibiting superior efficiency. First, we propose the direction-aware attention model (DaAM) to incorporate directionality into the embedding process, facilitating more effective one-stage decision-making. Second, we design a <b>supervised</b> <b>reinforcement</b> <b>learning</b> scheme that involves <b>supervised</b> pre-training to establish a robust initial policy for subsequent <b>reinforcement</b> <b>fine-tuning.</b> It proves particularly valuable for solving CARP that has a higher complexity than the node routing problems (NRPs). Finally, a path optimization method is proposed to adjust the depot return positions within the path generated by DaAM. Experiments illustrate that our approach surpasses heuristics and achieves decision quality comparable to state-of-the-art metaheuristics for the first time while maintaining superior efficiency.</p></p class="citation"></blockquote><h3 id=2063--167318-ups-towards-foundation-models-for-pde-solving-via-cross-modal-adaptation-junhong-shen-et-al-2024>(20/63 | 167/318) UPS: Towards Foundation Models for PDE Solving via Cross-Modal Adaptation (Junhong Shen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junhong Shen, Tanya Marwah, Ameet Talwalkar. (2024)<br><strong>UPS: Towards Foundation Models for PDE Solving via Cross-Modal Adaptation</strong><br><button class=copy-to-clipboard title="UPS: Towards Foundation Models for PDE Solving via Cross-Modal Adaptation" index=167>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-167 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Few-shot, Foundation Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07187v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07187v1.pdf filename=2403.07187v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce UPS (Unified PDE Solver), an effective and data-efficient approach to solve diverse spatiotemporal PDEs defined over various domains, dimensions, and resolutions. UPS unifies different PDEs into a consistent representation space and processes diverse collections of PDE data using a unified network architecture that combines <b>LLMs</b> with domain-specific neural operators. We train the network via a two-stage cross-modal adaptation process, leveraging ideas of modality alignment and multi-task learning. By adapting from pretrained <b>LLMs</b> and exploiting text-form meta information, we are able to use considerably fewer training samples than previous methods while obtaining strong empirical results. UPS outperforms existing baselines, often by a large margin, on a wide range of 1D and 2D datasets in PDEBench, achieving state-of-the-art results on 8 of 10 tasks considered. Meanwhile, it is capable of <b>few-shot</b> transfer to different PDE families, coefficients, and resolutions.</p></p class="citation"></blockquote><h3 id=2163--168318-acquiring-diverse-skills-using-curriculum-reinforcement-learning-with-mixture-of-experts-onur-celik-et-al-2024>(21/63 | 168/318) Acquiring Diverse Skills using Curriculum Reinforcement Learning with Mixture of Experts (Onur Celik et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Onur Celik, Aleksandar Taranovic, Gerhard Neumann. (2024)<br><strong>Acquiring Diverse Skills using Curriculum Reinforcement Learning with Mixture of Experts</strong><br><button class=copy-to-clipboard title="Acquiring Diverse Skills using Curriculum Reinforcement Learning with Mixture of Experts" index=168>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-168 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-RO, cs.LG<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06966v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06966v1.pdf filename=2403.06966v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Reinforcement</b> <b>learning</b> (RL) is a powerful approach for acquiring a good-performing policy. However, learning diverse skills is challenging in RL due to the commonly used Gaussian policy parameterization. We propose \textbf{Di}verse \textbf{Skil}l \textbf{L}earning (Di-SkilL), an RL method for learning diverse skills using Mixture of Experts, where each expert formalizes a skill as a contextual motion primitive. Di-SkilL optimizes each expert and its associate context distribution to a maximum entropy objective that incentivizes learning diverse skills in similar contexts. The per-expert context distribution enables automatic curricula learning, allowing each expert to focus on its best-performing sub-region of the context space. To overcome hard discontinuities and multi-modalities without any prior knowledge of the environment&rsquo;s unknown context probability space, we leverage energy-based models to represent the per-expert context distributions and demonstrate how we can efficiently train them using the standard policy gradient objective. We show on challenging robot <b>simulation</b> tasks that Di-SkilL can learn diverse and performant skills.</p></p class="citation"></blockquote><h3 id=2263--169318-diprompt-disentangled-prompt-tuning-for-multiple-latent-domain-generalization-in-federated-learning-sikai-bai-et-al-2024>(22/63 | 169/318) DiPrompT: Disentangled Prompt Tuning for Multiple Latent Domain Generalization in Federated Learning (Sikai Bai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sikai Bai, Jie Zhang, Shuaicheng Li, Song Guo, Jingcai Guo, Jun Hou, Tao Han, Xiaocheng Lu. (2024)<br><strong>DiPrompT: Disentangled Prompt Tuning for Multiple Latent Domain Generalization in Federated Learning</strong><br><button class=copy-to-clipboard title="DiPrompT: Disentangled Prompt Tuning for Multiple Latent Domain Generalization in Federated Learning" index=169>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-169 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Federated Learning, Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08506v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08506v1.pdf filename=2403.08506v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Federated</b> <b>learning</b> (FL) has emerged as a powerful paradigm for learning from decentralized data, and <b>federated</b> <b>domain</b> generalization further considers the test dataset (target domain) is absent from the decentralized training data (source domains). However, most existing FL methods assume that domain labels are provided during training, and their evaluation imposes explicit constraints on the number of domains, which must strictly match the number of clients. Because of the underutilization of numerous edge devices and additional cross-client domain annotations in the real world, such restrictions may be impractical and involve potential privacy leaks. In this paper, we propose an efficient and novel approach, called Disentangled <b>Prompt</b> Tuning (DiPrompT), a method that tackles the above restrictions by learning adaptive <b>prompts</b> for domain generalization in a distributed manner. Specifically, we first design two types of <b>prompts,</b> i.e., global <b>prompt</b> to capture general knowledge across all clients and domain <b>prompts</b> to capture domain-specific knowledge. They eliminate the restriction on the one-to-one mapping between source domains and local clients. Furthermore, a dynamic query metric is introduced to automatically search the suitable domain label for each sample, which includes two-substep <b>text-image</b> alignments based on <b>prompt</b> tuning without labor-intensive annotation. Extensive experiments on multiple datasets demonstrate that our DiPrompT achieves superior domain generalization performance over state-of-the-art FL methods when domain labels are not provided, and even outperforms many centralized learning methods using domain labels.</p></p class="citation"></blockquote><h3 id=2363--170318-multistep-consistency-models-jonathan-heek-et-al-2024>(23/63 | 170/318) Multistep Consistency Models (Jonathan Heek et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jonathan Heek, Emiel Hoogeboom, Tim Salimans. (2024)<br><strong>Multistep Consistency Models</strong><br><button class=copy-to-clipboard title="Multistep Consistency Models" index=170>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-170 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG, stat-ML<br>Keyword Score: 30<br>Keywords: Diffusion Model, Knowledge Distillation, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06807v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06807v1.pdf filename=2403.06807v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Diffusion</b> <b>models</b> are relatively easy to train but require many steps to generate samples. Consistency models are far more difficult to train, but generate samples in a single step. In this paper we propose Multistep Consistency Models: A unification between Consistency Models (Song et al., 2023) and TRACT (Berthelot et al., 2023) that can interpolate between a consistency model and a <b>diffusion</b> <b>model:</b> a trade-off between sampling speed and sampling quality. Specifically, a 1-step consistency model is a conventional consistency model whereas we show that a $\infty$-step consistency model is a <b>diffusion</b> <b>model.</b> Multistep Consistency Models work really well in practice. By increasing the sample budget from a single step to 2-8 steps, we can train models more easily that generate higher quality samples, while retaining much of the sampling speed benefits. Notable results are 1.4 FID on Imagenet 64 in 8 step and 2.1 FID on Imagenet128 in 8 steps with consistency <b>distillation.</b> We also show that our method scales to a <b>text-to-image</b> <b>diffusion</b> <b>model,</b> generating samples that are very close to the quality of the original model.</p></p class="citation"></blockquote><h3 id=2463--171318-on-the-global-convergence-of-policy-gradient-in-average-reward-markov-decision-processes-navdeep-kumar-et-al-2024>(24/63 | 171/318) On the Global Convergence of Policy Gradient in Average Reward Markov Decision Processes (Navdeep Kumar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Navdeep Kumar, Yashaswini Murthy, Itai Shufaro, Kfir Y. Levy, R. Srikant, Shie Mannor. (2024)<br><strong>On the Global Convergence of Policy Gradient in Average Reward Markov Decision Processes</strong><br><button class=copy-to-clipboard title="On the Global Convergence of Policy Gradient in Average Reward Markov Decision Processes" index=171>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-171 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-SY, cs.LG, eess-SY<br>Keyword Score: 30<br>Keywords: Markov Decision Process, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06806v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06806v1.pdf filename=2403.06806v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present the first finite time global convergence analysis of policy gradient in the context of infinite horizon average reward Markov decision processes <b>(MDPs).</b> Specifically, we focus on ergodic tabular <b>MDPs</b> with finite state and action spaces. Our analysis shows that the policy gradient iterates converge to the optimal policy at a sublinear rate of $O\left({\frac{1}{T}}\right),$ which translates to $O\left({\log(T)}\right)$ regret, where $T$ represents the number of iterations. Prior work on performance bounds for discounted reward <b>MDPs</b> cannot be extended to average reward <b>MDPs</b> because the bounds grow proportional to the fifth power of the effective horizon. Thus, our primary contribution is in proving that the policy gradient algorithm converges for average-reward <b>MDPs</b> and in obtaining finite-time performance guarantees. In contrast to the existing discounted reward performance bounds, our performance bounds have an explicit dependence on constants that capture the complexity of the underlying MDP. Motivated by this observation, we reexamine and improve the existing performance bounds for discounted reward <b>MDPs.</b> We also present <b>simulations</b> to empirically evaluate the performance of average reward policy gradient algorithm.</p></p class="citation"></blockquote><h3 id=2563--172318-probabilistic-contrastive-learning-for-long-tailed-visual-recognition-chaoqun-du-et-al-2024>(25/63 | 172/318) Probabilistic Contrastive Learning for Long-Tailed Visual Recognition (Chaoqun Du et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chaoqun Du, Yulin Wang, Shiji Song, Gao Huang. (2024)<br><strong>Probabilistic Contrastive Learning for Long-Tailed Visual Recognition</strong><br><button class=copy-to-clipboard title="Probabilistic Contrastive Learning for Long-Tailed Visual Recognition" index=172>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-172 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Contrastive Learning, Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06726v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06726v2.pdf filename=2403.06726v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Long-tailed distributions frequently emerge in real-world data, where a large number of minority categories contain a limited number of samples. Such imbalance issue considerably impairs the performance of standard <b>supervised</b> <b>learning</b> algorithms, which are mainly designed for balanced training sets. Recent investigations have revealed that <b>supervised</b> <b>contrastive</b> <b>learning</b> exhibits promising potential in alleviating the data imbalance. However, the performance of <b>supervised</b> <b>contrastive</b> <b>learning</b> is plagued by an inherent challenge: it necessitates sufficiently large batches of training data to construct <b>contrastive</b> <b>pairs</b> that cover all categories, yet this requirement is difficult to meet in the context of class-imbalanced data. To overcome this obstacle, we propose a novel probabilistic <b>contrastive</b> <b>(ProCo)</b> learning algorithm that estimates the data distribution of the samples from each class in the feature space, and samples <b>contrastive</b> <b>pairs</b> accordingly. In fact, estimating the distributions of all classes using features in a small batch, particularly for imbalanced data, is not feasible. Our key idea is to introduce a reasonable and simple assumption that the normalized features in <b>contrastive</b> <b>learning</b> follow a mixture of von Mises-Fisher (vMF) distributions on unit space, which brings two-fold benefits. First, the distribution parameters can be estimated using only the first sample moment, which can be efficiently computed in an online manner across different batches. Second, based on the estimated distribution, the vMF distribution allows us to sample an infinite number of <b>contrastive</b> <b>pairs</b> and derive a closed form of the expected <b>contrastive</b> <b>loss</b> for efficient optimization. Our code is available at <a href=https://github.com/LeapLabTHU/ProCo>https://github.com/LeapLabTHU/ProCo</a>.</p></p class="citation"></blockquote><h3 id=2663--173318-what-makes-quantization-for-large-language-models-hard-an-empirical-study-from-the-lens-of-perturbation-zhuocheng-gong-et-al-2024>(26/63 | 173/318) What Makes Quantization for Large Language Models Hard? An Empirical Study from the Lens of Perturbation (Zhuocheng Gong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhuocheng Gong, Jiahao Liu, Jingang Wang, Xunliang Cai, Dongyan Zhao, Rui Yan. (2024)<br><strong>What Makes Quantization for Large Language Models Hard? An Empirical Study from the Lens of Perturbation</strong><br><button class=copy-to-clipboard title="What Makes Quantization for Large Language Models Hard? An Empirical Study from the Lens of Perturbation" index=173>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-173 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Quantization, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06408v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06408v1.pdf filename=2403.06408v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Quantization</b> has emerged as a promising technique for improving the memory and computational efficiency of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> Though the trade-off between performance and efficiency is well-known, there is still much to be learned about the relationship between <b>quantization</b> and <b>LLM</b> performance. To shed light on this relationship, we propose a new perspective on <b>quantization,</b> viewing it as perturbations added to the weights and activations of <b>LLMs.</b> We call this approach &ldquo;the lens of perturbation&rdquo;. Using this lens, we conduct experiments with various artificial perturbations to explore their impact on <b>LLM</b> performance. Our findings reveal several connections between the properties of perturbations and <b>LLM</b> performance, providing insights into the failure cases of uniform <b>quantization</b> and suggesting potential solutions to improve the robustness of <b>LLM</b> <b>quantization.</b> To demonstrate the significance of our findings, we implement a simple non-uniform <b>quantization</b> approach based on our insights. Our experiments show that this approach achieves minimal performance degradation on both 4-bit weight <b>quantization</b> and 8-bit <b>quantization</b> for weights and activations. These results validate the correctness of our approach and highlight its potential to improve the efficiency of <b>LLMs</b> without sacrificing performance.</p></p class="citation"></blockquote><h3 id=2763--174318-aug-kd-anchor-based-mixup-generation-for-out-of-domain-knowledge-distillation-zihao-tang-et-al-2024>(27/63 | 174/318) AuG-KD: Anchor-Based Mixup Generation for Out-of-Domain Knowledge Distillation (Zihao Tang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zihao Tang, Zheqi Lv, Shengyu Zhang, Yifan Zhou, Xinyu Duan, Fei Wu, Kun Kuang. (2024)<br><strong>AuG-KD: Anchor-Based Mixup Generation for Out-of-Domain Knowledge Distillation</strong><br><button class=copy-to-clipboard title="AuG-KD: Anchor-Based Mixup Generation for Out-of-Domain Knowledge Distillation" index=174>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-174 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Knowledge Distillation, Knowledge Distillation, Out-of-domain<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07030v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07030v1.pdf filename=2403.07030v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Due to privacy or patent concerns, a growing number of large models are released without granting access to their training data, making transferring their <b>knowledge</b> <b>inefficient</b> and problematic. In response, Data-Free <b>Knowledge</b> <b>Distillation</b> (DFKD) methods have emerged as direct solutions. However, simply adopting models derived from DFKD for real-world applications suffers significant performance degradation, due to the discrepancy between teachers&rsquo; training data and real-world scenarios (student domain). The degradation stems from the portions of teachers&rsquo; <b>knowledge</b> <b>that</b> are not applicable to the student domain. They are specific to the teacher domain and would undermine students&rsquo; performance. Hence, selectively transferring teachers&rsquo; appropriate <b>knowledge</b> <b>becomes</b> the primary challenge in DFKD. In this work, we propose a simple but effective method AuG-KD. It utilizes an uncertainty-guided and sample-specific anchor to align student-domain data with the teacher domain and leverages a generative method to progressively trade off the learning process between OOD <b>knowledge</b> <b>distillation</b> and domain-specific information learning via mixup learning. Extensive experiments in 3 datasets and 8 settings demonstrate the stability and superiority of our approach. Code available at <a href=https://github.com/IshiKura-a/AuG-KD>https://github.com/IshiKura-a/AuG-KD</a> .</p></p class="citation"></blockquote><h3 id=2863--175318-3m-diffusion-latent-multi-modal-diffusion-for-text-guided-generation-of-molecular-graphs-huaisheng-zhu-et-al-2024>(28/63 | 175/318) 3M-Diffusion: Latent Multi-Modal Diffusion for Text-Guided Generation of Molecular Graphs (Huaisheng Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Huaisheng Zhu, Teng Xiao, Vasant G Honavar. (2024)<br><strong>3M-Diffusion: Latent Multi-Modal Diffusion for Text-Guided Generation of Molecular Graphs</strong><br><button class=copy-to-clipboard title="3M-Diffusion: Latent Multi-Modal Diffusion for Text-Guided Generation of Molecular Graphs" index=175>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-175 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG, q-bio-BM<br>Keyword Score: 26<br>Keywords: Diffusion Model, Graph, Multi-modal, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07179v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07179v1.pdf filename=2403.07179v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Generating molecules with desired properties is a critical task with broad applications in drug discovery and materials design. Inspired by recent advances in <b>large</b> <b>language</b> <b>models,</b> there is a growing interest in using natural language descriptions of molecules to generate molecules with the desired properties. Most existing methods focus on generating molecules that precisely match the text description. However, practical applications call for methods that generate diverse, and ideally novel, molecules with the desired properties. We propose 3M-Diffusion, a novel <b>multi-modal</b> molecular <b>graph</b> generation method, to address this challenge. 3M-Diffusion first encodes molecular <b>graphs</b> into a <b>graph</b> latent space aligned with text descriptions. It then reconstructs the molecular structure and atomic attributes based on the given text descriptions using the molecule decoder. It then learns a probabilistic mapping from the text space to the latent molecular <b>graph</b> space using a <b>diffusion</b> <b>model.</b> The results of our extensive experiments on several datasets demonstrate that 3M-Diffusion can generate high-quality, novel and diverse molecular <b>graphs</b> that semantically match the textual description provided.</p></p class="citation"></blockquote><h3 id=2963--176318-can-llms-separate-instructions-from-data-and-what-do-we-even-mean-by-that-egor-zverev-et-al-2024>(29/63 | 176/318) Can LLMs Separate Instructions From Data? And What Do We Even Mean By That? (Egor Zverev et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Egor Zverev, Sahar Abdelnabi, Mario Fritz, Christoph H. Lampert. (2024)<br><strong>Can LLMs Separate Instructions From Data? And What Do We Even Mean By That?</strong><br><button class=copy-to-clipboard title="Can LLMs Separate Instructions From Data? And What Do We Even Mean By That?" index=176>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-176 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG<br>Keyword Score: 25<br>Keywords: Black Box, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06833v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06833v1.pdf filename=2403.06833v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Instruction-tuned <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have achieved breakthrough results, opening countless new possibilities for many practical applications. However, <b>LLMs</b> lack elementary safety features that are established norms in other areas of computer science, such as the separation between instructions and data, causing them to malfunction or rendering them vulnerable to manipulation and interference by third parties e.g., via indirect prompt/command injection. Even worse, so far, there is not even an established definition of what precisely such a separation would mean and how its violation could be tested. In this work, we aim to close this gap. We introduce a formal measure to quantify the phenomenon of instruction-data separation as well as an empirical variant of the measure that can be computed from a model`s <b>black-box</b> <b>outputs.</b> We also introduce a new dataset, SEP (Should it be Executed or Processed?), which allows estimating the measure, and we report results on several state-of-the-art open-source and closed <b>LLMs.</b> Finally, we quantitatively demonstrate that all evaluated <b>LLMs</b> fail to achieve a high amount of separation, according to our measure. The source code and SEP dataset are openly accessible at <a href=https://github.com/egozverev/Shold-It-Be-Executed-Or-Processed>https://github.com/egozverev/Shold-It-Be-Executed-Or-Processed</a>.</p></p class="citation"></blockquote><h3 id=3063--177318-xb-maml-learning-expandable-basis-parameters-for-effective-meta-learning-with-wide-task-coverage-jae-jun-lee-et-al-2024>(30/63 | 177/318) XB-MAML: Learning Expandable Basis Parameters for Effective Meta-Learning with Wide Task Coverage (Jae-Jun Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jae-Jun Lee, Sung Whan Yoon. (2024)<br><strong>XB-MAML: Learning Expandable Basis Parameters for Effective Meta-Learning with Wide Task Coverage</strong><br><button class=copy-to-clipboard title="XB-MAML: Learning Expandable Basis Parameters for Effective Meta-Learning with Wide Task Coverage" index=177>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-177 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Benchmarking, Fine-tuning, Meta Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06768v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06768v1.pdf filename=2403.06768v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Meta-learning,</b> <b>which</b> pursues an effective initialization model, has emerged as a promising approach to handling unseen tasks. However, a limitation remains to be evident when a <b>meta-learner</b> <b>tries</b> to encompass a wide range of task distribution, e.g., learning across distinctive datasets or domains. Recently, a group of works has attempted to employ multiple model initializations to cover widely-ranging tasks, but they are limited in adaptively expanding initializations. We introduce XB-MAML, which learns expandable basis parameters, where they are linearly combined to form an effective initialization to a given task. XB-MAML observes the discrepancy between the vector space spanned by the basis and <b>fine-tuned</b> parameters to decide whether to expand the basis. Our method surpasses the existing works in the multi-domain <b>meta-learning</b> <b>benchmarks</b> and opens up new chances of <b>meta-learning</b> <b>for</b> obtaining the diverse inductive bias that can be combined to stretch toward the effective initialization for diverse unseen tasks.</p></p class="citation"></blockquote><h3 id=3163--178318-dont-forget-what-i-did-assessing-client-contributions-in-federated-learning-bishwamittra-ghosh-et-al-2024>(31/63 | 178/318) Don&rsquo;t Forget What I did?: Assessing Client Contributions in Federated Learning (Bishwamittra Ghosh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bishwamittra Ghosh, Debabrota Basu, Fu Huazhu, Wang Yuan, Renuga Kanagavelu, Jiang Jin Peng, Liu Yong, Goh Siow Mong Rick, Wei Qingsong. (2024)<br><strong>Don&rsquo;t Forget What I did?: Assessing Client Contributions in Federated Learning</strong><br><button class=copy-to-clipboard title="Don't Forget What I did?: Assessing Client Contributions in Federated Learning" index=178>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-178 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CR, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Fairness, Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07151v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07151v1.pdf filename=2403.07151v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Federated</b> <b>Learning</b> (FL) is a collaborative machine learning (ML) approach, where multiple clients participate in training an ML model without exposing the private data. Fair and accurate assessment of client contributions is an important problem in FL to facilitate incentive allocation and encouraging diverse clients to participate in a unified model training. Existing methods for assessing client contribution adopts co-operative game-theoretic concepts, such as Shapley values, but under simplified assumptions. In this paper, we propose a history-aware game-theoretic framework, called FLContrib, to assess client contributions when a subset of (potentially non-i.i.d.) clients participate in each epoch of FL training. By exploiting the FL training process and linearity of Shapley value, we develop FLContrib that yields a historical timeline of client contributions as FL training progresses over epochs. Additionally, to assess client contribution under limited computational budget, we propose a scheduling procedure that considers a two-sided <b>fairness</b> criteria to perform expensive Shapley value computation only in a subset of training epochs. In experiments, we demonstrate a controlled trade-off between the correctness and efficiency of client contributions assessed via FLContrib. To demonstrate the benefits of history-aware client contributions, we apply FLContrib to detect dishonest clients conducting data poisoning in FL training.</p></p class="citation"></blockquote><h3 id=3263--179318-falcon-flop-aware-combinatorial-optimization-for-neural-network-pruning-xiang-meng-et-al-2024>(32/63 | 179/318) FALCON: FLOP-Aware Combinatorial Optimization for Neural Network Pruning (Xiang Meng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiang Meng, Wenyu Chen, Riade Benbaki, Rahul Mazumder. (2024)<br><strong>FALCON: FLOP-Aware Combinatorial Optimization for Neural Network Pruning</strong><br><button class=copy-to-clipboard title="FALCON: FLOP-Aware Combinatorial Optimization for Neural Network Pruning" index=179>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-179 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Pruning, falcon<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07094v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07094v1.pdf filename=2403.07094v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The increasing computational demands of modern neural networks present deployment challenges on resource-constrained devices. Network <b>pruning</b> offers a solution to reduce model size and computational cost while maintaining performance. However, most current <b>pruning</b> methods focus primarily on improving sparsity by reducing the number of nonzero parameters, often neglecting other deployment costs such as inference time, which are closely related to the number of floating-point operations (FLOPs). In this paper, we propose <b>FALCON,</b> a novel combinatorial-optimization-based framework for network <b>pruning</b> that jointly takes into account model accuracy (fidelity), FLOPs, and sparsity constraints. A main building block of our approach is an integer linear program (ILP) that simultaneously handles FLOP and sparsity constraints. We present a novel algorithm to approximately solve the ILP. We propose a novel first-order method for our optimization framework which makes use of our ILP solver. Using problem structure (e.g., the low-rank structure of approx. Hessian), we can address instances with millions of parameters. Our experiments demonstrate that <b>FALCON</b> achieves superior accuracy compared to other <b>pruning</b> approaches within a fixed FLOP budget. For instance, for ResNet50 with 20% of the total FLOPs retained, our approach improves the accuracy by 48% relative to state-of-the-art. Furthermore, in gradual <b>pruning</b> settings with re-training between <b>pruning</b> steps, our framework outperforms existing <b>pruning</b> methods, emphasizing the significance of incorporating both FLOP and sparsity constraints for effective network <b>pruning.</b></p></p class="citation"></blockquote><h3 id=3363--180318-cost-sensitive-learning-to-defer-to-multiple-experts-with-workload-constraints-jean-v-alves-et-al-2024>(33/63 | 180/318) Cost-Sensitive Learning to Defer to Multiple Experts with Workload Constraints (Jean V. Alves et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jean V. Alves, Diogo Leitão, Sérgio Jesus, Marco O. P. Sampaio, Javier Liébana, Pedro Saleiro, Mário A. T. Figueiredo, Pedro Bizarro. (2024)<br><strong>Cost-Sensitive Learning to Defer to Multiple Experts with Workload Constraints</strong><br><button class=copy-to-clipboard title="Cost-Sensitive Learning to Defer to Multiple Experts with Workload Constraints" index=180>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-180 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06906v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06906v1.pdf filename=2403.06906v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Learning to defer (L2D) aims to improve human-AI collaboration systems by learning how to defer decisions to humans when they are more likely to be correct than an ML classifier. Existing research in L2D overlooks key aspects of real-world systems that impede its practical adoption, namely: i) neglecting cost-sensitive scenarios, where type 1 and type 2 errors have different costs; ii) requiring concurrent human predictions for every instance of the training dataset and iii) not dealing with human work capacity constraints. To address these issues, we propose the deferral under cost and capacity constraints framework (DeCCaF). DeCCaF is a novel L2D approach, employing <b>supervised</b> <b>learning</b> to model the probability of human error under less restrictive data requirements (only one expert prediction per instance) and using constraint programming to globally minimize the error cost subject to workload limitations. We test DeCCaF in a series of cost-sensitive fraud detection scenarios with different teams of 9 synthetic fraud analysts, with individual work capacity constraints. The results demonstrate that our approach performs significantly better than the baselines in a wide array of scenarios, achieving an average 8.4% reduction in the misclassification cost.</p></p class="citation"></blockquote><h3 id=3463--181318-ε-neural-thompson-sampling-of-deep-brain-stimulation-for-parkinson-disease-treatment-hao-lun-hsu-et-al-2024>(34/63 | 181/318) ε-Neural Thompson Sampling of Deep Brain Stimulation for Parkinson Disease Treatment (Hao-Lun Hsu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hao-Lun Hsu, Qitong Gao, Miroslav Pajic. (2024)<br><strong>ε-Neural Thompson Sampling of Deep Brain Stimulation for Parkinson Disease Treatment</strong><br><button class=copy-to-clipboard title="ε-Neural Thompson Sampling of Deep Brain Stimulation for Parkinson Disease Treatment" index=181>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-181 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, q-bio-NC<br>Keyword Score: 20<br>Keywords: Bandit Algorithm, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06814v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06814v1.pdf filename=2403.06814v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep Brain Stimulation (DBS) stands as an effective intervention for alleviating the motor symptoms of Parkinson&rsquo;s disease (PD). Traditional commercial DBS devices are only able to deliver fixed-frequency periodic pulses to the basal ganglia (BG) regions of the brain, i.e., continuous DBS (cDBS). However, they in general suffer from energy inefficiency and side effects, such as speech impairment. Recent research has focused on adaptive DBS (aDBS) to resolve the limitations of cDBS. Specifically, <b>reinforcement</b> <b>learning</b> (RL) based approaches have been developed to adapt the frequencies of the stimuli in order to achieve both energy efficiency and treatment efficacy. However, RL approaches in general require significant amount of training data and computational resources, making it intractable to integrate RL policies into real-time embedded systems as needed in aDBS. In contrast, contextual multi-armed <b>bandits</b> (CMAB) in general lead to better sample efficiency compared to RL. In this study, we propose a CMAB solution for aDBS. Specifically, we define the context as the signals capturing irregular neuronal firing activities in the BG regions (i.e., beta-band power spectral density), while each arm signifies the (discretized) pulse frequency of the stimulation. Moreover, an {\epsilon}-exploring strategy is introduced on top of the classic Thompson sampling method, leading to an algorithm called {\epsilon}-Neural Thompson sampling ({\epsilon}-NeuralTS), such that the learned CMAB policy can better balance exploration and exploitation of the BG environment. The {\epsilon}-NeuralTS algorithm is evaluated using a computation BG model that captures the neuronal activities in PD patients&rsquo; brains. The results show that our method outperforms both existing cDBS methods and CMAB baselines.</p></p class="citation"></blockquote><h3 id=3563--182318-peeraid-improving-adversarial-distillation-from-a-specialized-peer-tutor-jaewon-jung-et-al-2024>(35/63 | 182/318) PeerAiD: Improving Adversarial Distillation from a Specialized Peer Tutor (Jaewon Jung et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jaewon Jung, Hongsun Jang, Jaeyong Song, Jinho Lee. (2024)<br><strong>PeerAiD: Improving Adversarial Distillation from a Specialized Peer Tutor</strong><br><button class=copy-to-clipboard title="PeerAiD: Improving Adversarial Distillation from a Specialized Peer Tutor" index=182>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-182 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Knowledge Distillation, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06668v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06668v1.pdf filename=2403.06668v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Adversarial robustness of the neural network is a significant concern when it is applied to security-critical domains. In this situation, adversarial <b>distillation</b> is a promising option which aims to <b>distill</b> the robustness of the teacher network to improve the robustness of a small student network. Previous works pretrain the teacher network to make it robust to the adversarial examples aimed at itself. However, the adversarial examples are dependent on the parameters of the target network. The fixed teacher network inevitably degrades its robustness against the unseen transferred adversarial examples which targets the parameters of the student network in the adversarial <b>distillation</b> process. We propose PeerAiD to make a peer network learn the adversarial examples of the student network instead of adversarial examples aimed at itself. PeerAiD is an adversarial <b>distillation</b> that trains the peer network and the student network simultaneously in order to make the peer network specialized for defending the student network. We observe that such peer networks surpass the robustness of pretrained robust teacher network against student-attacked adversarial samples. With this peer network and adversarial <b>distillation,</b> PeerAiD achieves significantly higher robustness of the student network with AutoAttack (AA) accuracy up to 1.66%p and improves the natural accuracy of the student network up to 4.72%p with ResNet-18 and TinyImageNet dataset.</p></p class="citation"></blockquote><h3 id=3663--183318-elephants-never-forget-testing-language-models-for-memorization-of-tabular-data-sebastian-bordt-et-al-2024>(36/63 | 183/318) Elephants Never Forget: Testing Language Models for Memorization of Tabular Data (Sebastian Bordt et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sebastian Bordt, Harsha Nori, Rich Caruana. (2024)<br><strong>Elephants Never Forget: Testing Language Models for Memorization of Tabular Data</strong><br><button class=copy-to-clipboard title="Elephants Never Forget: Testing Language Models for Memorization of Tabular Data" index=183>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-183 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06644v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06644v1.pdf filename=2403.06644v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While many have shown how <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> can be applied to a diverse set of tasks, the critical issues of data contamination and memorization are often glossed over. In this work, we address this concern for tabular data. Starting with simple qualitative tests for whether an <b>LLM</b> knows the names and values of features, we introduce a variety of different techniques to assess the degrees of contamination, including statistical tests for conditional distribution modeling and four tests that identify memorization. Our investigation reveals that <b>LLMs</b> are pre-trained on many popular tabular datasets. This exposure can lead to invalid performance evaluation on downstream tasks because the <b>LLMs</b> have, in effect, been fit to the test set. Interestingly, we also identify a regime where the language model reproduces important statistics of the data, but fails to reproduce the dataset verbatim. On these datasets, although seen during training, good performance on downstream tasks might not be due to overfitting. Our findings underscore the need for ensuring data integrity in machine learning tasks with <b>LLMs.</b> To facilitate future research, we release an open-source tool that can perform various tests for memorization \url{https://github.com/interpretml/LLM-Tabular-Memorization-Checker}.</p></p class="citation"></blockquote><h3 id=3763--184318-scalable-online-exploration-via-coverability-philip-amortila-et-al-2024>(37/63 | 184/318) Scalable Online Exploration via Coverability (Philip Amortila et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Philip Amortila, Dylan J. Foster, Akshay Krishnamurthy. (2024)<br><strong>Scalable Online Exploration via Coverability</strong><br><button class=copy-to-clipboard title="Scalable Online Exploration via Coverability" index=184>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-184 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-OC, stat-ML<br>Keyword Score: 20<br>Keywords: Markov Decision Process, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06571v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06571v1.pdf filename=2403.06571v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Exploration is a major challenge in <b>reinforcement</b> <b>learning,</b> especially for high-dimensional domains that require function approximation. We propose exploration objectives &ndash; policy optimization objectives that enable downstream maximization of any reward function &ndash; as a conceptual framework to systematize the study of exploration. Within this framework, we introduce a new objective, $L_1$-Coverage, which generalizes previous exploration schemes and supports three fundamental desiderata: 1. Intrinsic complexity control. $L_1$-Coverage is associated with a structural parameter, $L_1$-Coverability, which reflects the intrinsic statistical difficulty of the underlying MDP, subsuming Block and Low-Rank <b>MDPs.</b> 2. Efficient planning. For a known MDP, optimizing $L_1$-Coverage efficiently reduces to standard policy optimization, allowing flexible integration with off-the-shelf methods such as policy gradient and Q-learning approaches. 3. Efficient exploration. $L_1$-Coverage enables the first computationally efficient model-based and model-free algorithms for online (reward-free or reward-driven) <b>reinforcement</b> <b>learning</b> in <b>MDPs</b> with low coverability. Empirically, we find that $L_1$-Coverage effectively drives off-the-shelf policy optimization algorithms to explore the state space.</p></p class="citation"></blockquote><h3 id=3863--185318-tactical-decision-making-for-autonomous-trucks-by-deep-reinforcement-learning-with-total-cost-of-operation-based-reward-deepthi-pathare-et-al-2024>(38/63 | 185/318) Tactical Decision Making for Autonomous Trucks by Deep Reinforcement Learning with Total Cost of Operation Based Reward (Deepthi Pathare et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Deepthi Pathare, Leo Laine, Morteza Haghir Chehreghani. (2024)<br><strong>Tactical Decision Making for Autonomous Trucks by Deep Reinforcement Learning with Total Cost of Operation Based Reward</strong><br><button class=copy-to-clipboard title="Tactical Decision Making for Autonomous Trucks by Deep Reinforcement Learning with Total Cost of Operation Based Reward" index=185>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-185 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-RO, cs.LG<br>Keyword Score: 20<br>Keywords: Curriculum Learning, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06524v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06524v1.pdf filename=2403.06524v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We develop a deep <b>reinforcement</b> <b>learning</b> framework for tactical decision making in an autonomous truck, specifically for Adaptive Cruise Control (ACC) and lane change maneuvers in a highway scenario. Our results demonstrate that it is beneficial to separate high-level decision-making processes and low-level control actions between the <b>reinforcement</b> <b>learning</b> agent and the low-level controllers based on physical models. In the following, we study optimizing the performance with a realistic and multi-objective reward function based on Total Cost of Operation (TCOP) of the truck using different approaches; by adding weights to reward components, by normalizing the reward components and by using <b>curriculum</b> <b>learning</b> techniques.</p></p class="citation"></blockquote><h3 id=3963--186318-rl-msa-a-reinforcement-learning-based-multi-line-bus-scheduling-approach-yingzhuo-liu-2024>(39/63 | 186/318) RL-MSA: a Reinforcement Learning-based Multi-line bus Scheduling Approach (Yingzhuo Liu, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yingzhuo Liu. (2024)<br><strong>RL-MSA: a Reinforcement Learning-based Multi-line bus Scheduling Approach</strong><br><button class=copy-to-clipboard title="RL-MSA: a Reinforcement Learning-based Multi-line bus Scheduling Approach" index=186>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-186 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Markov Decision Process, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06466v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06466v1.pdf filename=2403.06466v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multiple Line Bus Scheduling Problem (MLBSP) is vital to save operational cost of bus company and guarantee service quality for passengers. Existing approaches typically generate a bus scheduling scheme in an offline manner and then schedule buses according to the scheme. In practice, uncertain events such as traffic congestion occur frequently, which may make the pre-determined bus scheduling scheme infeasible. In this paper, MLBSP is modeled as a <b>Markov</b> <b>Decision</b> <b>Process</b> (MDP). A <b>Reinforcement</b> <b>Learning-based</b> Multi-line bus Scheduling Approach (RL-MSA) is proposed for bus scheduling at both the offline and online phases. At the offline phase, deadhead decision is integrated into bus selection decision for the first time to simplify the learning problem. At the online phase, deadhead decision is made through a time window mechanism based on the policy learned at the offline phase. We develop several new and useful state features including the features for control points, bus lines and buses. A bus priority screening mechanism is invented to construct bus-related features. Considering the interests of both the bus company and passengers, a reward function combining the final reward and the step-wise reward is devised. Experiments at the offline phase demonstrate that the number of buses used of RL-MSA is decreased compared with offline optimization approaches. At the online phase, RL-MSA can cover all departure times in a timetable (i.e., service quality) without increasing the number of buses used (i.e., operational cost).</p></p class="citation"></blockquote><h3 id=4063--187318-interpreting-what-typical-fault-signals-look-like-via-prototype-matching-qian-chen-et-al-2024>(40/63 | 187/318) Interpreting What Typical Fault Signals Look Like via Prototype-matching (Qian Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qian Chen, Xingjian Dong, Zhike Peng. (2024)<br><strong>Interpreting What Typical Fault Signals Look Like via Prototype-matching</strong><br><button class=copy-to-clipboard title="Interpreting What Typical Fault Signals Look Like via Prototype-matching" index=187>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-187 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Autoencoder, Black Box, Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07033v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07033v1.pdf filename=2403.07033v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neural networks, with powerful nonlinear mapping and classification capabilities, are widely applied in mechanical fault diagnosis to ensure safety. However, being typical <b>black-box</b> <b>models,</b> their application is limited in high-reliability-required scenarios. To understand the classification logic and explain what typical fault signals look like, the prototype matching network (PMN) is proposed by combining the human-inherent prototype-matching with <b>autoencoder</b> (AE). The PMN matches AE-extracted feature with each prototype and selects the most similar prototype as the prediction result. It has three interpreting paths on classification logic, fault prototypes, and matching contributions. Conventional diagnosis and domain generalization experiments demonstrate its competitive diagnostic performance and distinguished advantages in <b>representation</b> <b>learning.</b> Besides, the learned typical fault signals (i.e., sample-level prototypes) showcase the ability for denoising and extracting subtle key features that experts find challenging to capture. This ability broadens human understanding and provides a promising solution from interpretability research to AI-for-Science.</p></p class="citation"></blockquote><h3 id=4163--188318-the-cram-method-for-efficient-simultaneous-learning-and-evaluation-zeyang-jia-et-al-2024>(41/63 | 188/318) The Cram Method for Efficient Simultaneous Learning and Evaluation (Zeyang Jia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zeyang Jia, Kosuke Imai, Michael Lingzhi Li. (2024)<br><strong>The Cram Method for Efficient Simultaneous Learning and Evaluation</strong><br><button class=copy-to-clipboard title="The Cram Method for Efficient Simultaneous Learning and Evaluation" index=188>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-188 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-CO, stat-ME, stat-ML<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07031v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07031v1.pdf filename=2403.07031v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce the &ldquo;cram&rdquo; method, a general and efficient approach to simultaneous learning and evaluation using a generic machine learning (ML) algorithm. In a single pass of batched data, the proposed method repeatedly trains an ML algorithm and tests its empirical performance. Because it utilizes the entire sample for both learning and evaluation, cramming is significantly more data-efficient than sample-splitting. The cram method also naturally accommodates online learning algorithms, making its implementation computationally efficient. To demonstrate the power of the cram method, we consider the standard policy learning setting where cramming is applied to the same data to both develop an individualized treatment rule (ITR) and estimate the average outcome that would result if the learned ITR were to be deployed. We show that under a minimal set of assumptions, the resulting crammed evaluation estimator is consistent and asymptotically normal. While our asymptotic results require a relatively weak stabilization condition of ML algorithm, we develop a simple, generic method that can be used with any policy learning algorithm to satisfy this condition. Our extensive <b>simulation</b> studies show that, when compared to sample-splitting, cramming reduces the evaluation standard error by more than 40% while improving the performance of learned policy. We also apply the cram method to a randomized clinical trial to demonstrate its applicability to real-world problems. Finally, we briefly discuss future extensions of the cram method to other learning and evaluation settings.</p></p class="citation"></blockquote><h3 id=4263--189318-mathbfnk-puzzle-a-cost-efficient-testbed-for-benchmarking-reinforcement-learning-algorithms-in-generative-language-model-yufeng-zhang-et-al-2024>(42/63 | 189/318) $\mathbf{(N,K)}$-Puzzle: A Cost-Efficient Testbed for Benchmarking Reinforcement Learning Algorithms in Generative Language Model (Yufeng Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yufeng Zhang, Liyu Chen, Boyi Liu, Yingxiang Yang, Qiwen Cui, Yunzhe Tao, Hongxia Yang. (2024)<br><strong>$\mathbf{(N,K)}$-Puzzle: A Cost-Efficient Testbed for Benchmarking Reinforcement Learning Algorithms in Generative Language Model</strong><br><button class=copy-to-clipboard title="$\mathbf{(N,K)}$-Puzzle: A Cost-Efficient Testbed for Benchmarking Reinforcement Learning Algorithms in Generative Language Model" index=189>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-189 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 16<br>Keywords: Benchmarking, Benchmarking, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07191v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07191v1.pdf filename=2403.07191v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advances in <b>reinforcement</b> <b>learning</b> (RL) algorithms aim to enhance the performance of language models at scale. Yet, there is a noticeable absence of a cost-effective and standardized testbed tailored to evaluating and comparing these algorithms. To bridge this gap, we present a generalized version of the 24-Puzzle: the $(N,K)$-Puzzle, which challenges language models to reach a target value $K$ with $N$ integers. We evaluate the effectiveness of established RL algorithms such as Proximal Policy Optimization (PPO), alongside novel approaches like Identity Policy Optimization (IPO) and Direct Policy Optimization (DPO).</p></p class="citation"></blockquote><h3 id=4363--190318-a-multi-cohort-study-on-prediction-of-acute-brain-dysfunction-states-using-selective-state-space-models-brandon-silva-et-al-2024>(43/63 | 190/318) A multi-cohort study on prediction of acute brain dysfunction states using selective state space models (Brandon Silva et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Brandon Silva, Miguel Contreras, Sabyasachi Bandyopadhyay, Yuanfang Ren, Ziyuan Guan, Jeremy Balch, Kia Khezeli, Tezcan Ozrazgat Baslanti, Ben Shickel, Azra Bihorac, Parisa Rashidi. (2024)<br><strong>A multi-cohort study on prediction of acute brain dysfunction states using selective state space models</strong><br><button class=copy-to-clipboard title="A multi-cohort study on prediction of acute brain dysfunction states using selective state space models" index=190>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-190 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, stat-AP<br>Keyword Score: 13<br>Keywords: Sample Size, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07201v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07201v1.pdf filename=2403.07201v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Assessing acute brain dysfunction (ABD), including delirium and coma in the intensive care unit (ICU), is a critical challenge due to its prevalence and severe implications for patient outcomes. Current diagnostic methods rely on infrequent clinical observations, which can only determine a patient&rsquo;s ABD status after onset. Our research attempts to solve these problems by harnessing Electronic Health Records (EHR) data to develop automated methods for ABD prediction for patients in the ICU. Existing models solely predict a single state (e.g., either delirium or coma), require at least 24 hours of observation data to make predictions, do not dynamically predict fluctuating ABD conditions during ICU stay (typically a one-time prediction), and use small <b>sample</b> <b>size,</b> proprietary single-hospital datasets. Our research fills these gaps in the existing literature by dynamically predicting delirium, coma, and mortality for 12-hour intervals throughout an ICU stay and validating on two public datasets. Our research also introduces the concept of dynamically predicting critical transitions from non-ABD to ABD and between different ABD states in real time, which could be clinically more informative for the hospital staff. We compared the predictive performance of two state-of-the-art neural network models, the MAMBA selective state space model and the Longformer <b>Transformer</b> model. Using the MAMBA model, we achieved a mean area under the receiving operator characteristic curve (AUROC) of 0.95 on outcome prediction of ABD for 12-hour intervals. The model achieves a mean AUROC of 0.79 when predicting transitions between ABD states. Our study uses a curated dataset from the University of Florida Health Shands Hospital for internal validation and two publicly available datasets, MIMIC-IV and eICU, for external validation, demonstrating robustness across ICU stays from 203 hospitals and 140,945 patients.</p></p class="citation"></blockquote><h3 id=4463--191318-leveraging-graph-neural-networks-for-supporting-automatic-triage-of-patients-annamaria-defilippo-et-al-2024>(44/63 | 191/318) Leveraging graph neural networks for supporting Automatic Triage of Patients (Annamaria Defilippo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Annamaria Defilippo, Pierangelo Veltri, Pietro Lio&rsquo;, Pietro Hiram Guzzi. (2024)<br><strong>Leveraging graph neural networks for supporting Automatic Triage of Patients</strong><br><button class=copy-to-clipboard title="Leveraging graph neural networks for supporting Automatic Triage of Patients" index=191>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-191 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 13<br>Keywords: Graph, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07038v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07038v1.pdf filename=2403.07038v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Patient triage plays a crucial role in emergency departments, ensuring timely and appropriate care based on correctly evaluating the emergency grade of patient conditions. Triage methods are generally performed by human operator based on her own experience and information that are gathered from the patient management process. Thus, it is a process that can generate errors in emergency level associations. Recently, Traditional triage methods heavily rely on human decisions, which can be subjective and prone to errors. Recently, a growing interest has been focused on leveraging artificial intelligence (AI) to develop algorithms able to maximize information gathering and minimize errors in patient triage processing. We define and implement an AI based module to manage patients emergency code assignments in emergency departments. It uses emergency department historical data to train the medical decision process. Data containing relevant patient information, such as vital signs, symptoms, and medical history, are used to accurately classify patients into triage categories. Experimental results demonstrate that the proposed algorithm achieved high accuracy outperforming traditional triage methods. By using the proposed method we claim that healthcare professionals can predict severity index to guide patient management processing and resource allocation.</p></p class="citation"></blockquote><h3 id=4563--192318-graph-neural-network-with-two-uplift-estimators-for-label-scarcity-individual-uplift-modeling-dingyuan-zhu-et-al-2024>(45/63 | 192/318) Graph Neural Network with Two Uplift Estimators for Label-Scarcity Individual Uplift Modeling (Dingyuan Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dingyuan Zhu, Daixin Wang, Zhiqiang Zhang, Kun Kuang, Yan Zhang, Yulin Kang, Jun Zhou. (2024)<br><strong>Graph Neural Network with Two Uplift Estimators for Label-Scarcity Individual Uplift Modeling</strong><br><button class=copy-to-clipboard title="Graph Neural Network with Two Uplift Estimators for Label-Scarcity Individual Uplift Modeling" index=192>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-192 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 13<br>Keywords: Graph, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06489v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06489v1.pdf filename=2403.06489v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Uplift modeling aims to measure the incremental effect, which we call uplift, of a strategy or action on the users from randomized experiments or observational data. Most existing uplift methods only use individual data, which are usually not informative enough to capture the unobserved and complex hidden factors regarding the uplift. Furthermore, uplift modeling scenario usually has scarce labeled data, especially for the treatment group, which also poses a great challenge for model training. Considering that the neighbors&rsquo; features and the social relationships are very informative to characterize a user&rsquo;s uplift, we propose a <b>graph</b> <b>neural</b> <b>network-based</b> framework with two uplift estimators, called GNUM, to learn from the social <b>graph</b> <b>for</b> <b>uplift</b> estimation. Specifically, we design the first estimator based on a class-transformed target. The estimator is general for all types of outcomes, and is able to comprehensively model the treatment and control group data together to approach the uplift. When the outcome is discrete, we further design the other uplift estimator based on our defined partial labels, which is able to utilize more labeled data from both the treatment and control groups, to further alleviate the label scarcity problem. Comprehensive experiments on a public dataset and two industrial datasets show a superior performance of our proposed framework over state-of-the-art methods under various evaluation metrics. The proposed algorithms have been deployed online to serve real-world uplift estimation scenarios.</p></p class="citation"></blockquote><h3 id=4663--193318-on-the-limited-representational-power-of-value-functions-and-its-links-to-statistical-inefficiency-david-cheikhi-et-al-2024>(46/63 | 193/318) On the Limited Representational Power of Value Functions and its Links to Statistical (In)Efficiency (David Cheikhi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>David Cheikhi, Daniel Russo. (2024)<br><strong>On the Limited Representational Power of Value Functions and its Links to Statistical (In)Efficiency</strong><br><button class=copy-to-clipboard title="On the Limited Representational Power of Value Functions and its Links to Statistical (In)Efficiency" index=193>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-193 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07136v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07136v1.pdf filename=2403.07136v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Identifying the trade-offs between model-based and model-free methods is a central question in <b>reinforcement</b> <b>learning.</b> Value-based methods offer substantial computational advantages and are sometimes just as statistically efficient as model-based methods. However, focusing on the core problem of policy evaluation, we show information about the transition dynamics may be impossible to represent in the space of value functions. We explore this through a series of case studies focused on structures that arises in many important problems. In several, there is no information loss and value-based methods are as statistically efficient as model based ones. In other closely-related examples, information loss is severe and value-based methods are severely outperformed. A deeper investigation points to the limitations of the representational power as the driver of the inefficiency, as opposed to failure in algorithm design.</p></p class="citation"></blockquote><h3 id=4763--194318-explainable-learning-with-gaussian-processes-kurt-butler-et-al-2024>(47/63 | 194/318) Explainable Learning with Gaussian Processes (Kurt Butler et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kurt Butler, Guanchao Feng, Petar M. Djuric. (2024)<br><strong>Explainable Learning with Gaussian Processes</strong><br><button class=copy-to-clipboard title="Explainable Learning with Gaussian Processes" index=194>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-194 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: 60G15, cs-LG, cs.LG, eess-SP<br>Keyword Score: 10<br>Keywords: Gaussian Process<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07072v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07072v1.pdf filename=2403.07072v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The field of explainable artificial intelligence (XAI) attempts to develop methods that provide insight into how complicated machine learning methods make predictions. Many methods of explanation have focused on the concept of feature attribution, a decomposition of the model&rsquo;s prediction into individual contributions corresponding to each input feature. In this work, we explore the problem of feature attribution in the context of <b>Gaussian</b> <b>process</b> regression (GPR). We take a principled approach to defining attributions under model uncertainty, extending the existing literature. We show that although GPR is a highly flexible and non-parametric approach, we can derive interpretable, closed-form expressions for the feature attributions. When using integrated gradients as an attribution method, we show that the attributions of a GPR model also follow a <b>Gaussian</b> <b>process</b> distribution, which quantifies the uncertainty in attribution arising from uncertainty in the model. We demonstrate, both through theory and experimentation, the versatility and robustness of this approach. We also show that, when applicable, the exact expressions for GPR attributions are both more accurate and less computationally expensive than the approximations currently used in practice. The source code for this project is freely available under MIT license at <a href=https://github.com/KurtButler/2024_attributions_paper>https://github.com/KurtButler/2024_attributions_paper</a>.</p></p class="citation"></blockquote><h3 id=4863--195318-unveiling-the-significance-of-toddler-inspired-reward-transition-in-goal-oriented-reinforcement-learning-junseok-park-et-al-2024>(48/63 | 195/318) Unveiling the Significance of Toddler-Inspired Reward Transition in Goal-Oriented Reinforcement Learning (Junseok Park et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junseok Park, Yoonsung Kim, Hee Bin Yoo, Min Whoo Lee, Kibeom Kim, Won-Seok Choi, Minsu Lee, Byoung-Tak Zhang. (2024)<br><strong>Unveiling the Significance of Toddler-Inspired Reward Transition in Goal-Oriented Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Unveiling the Significance of Toddler-Inspired Reward Transition in Goal-Oriented Reinforcement Learning" index=195>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-195 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: I-2-0, cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06880v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06880v1.pdf filename=2403.06880v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Toddlers evolve from free exploration with sparse feedback to exploiting prior experiences for goal-directed learning with denser rewards. Drawing inspiration from this Toddler-Inspired Reward Transition, we set out to explore the implications of varying reward transitions when incorporated into <b>Reinforcement</b> <b>Learning</b> (RL) tasks. Central to our inquiry is the transition from sparse to potential-based dense rewards, which share optimal strategies regardless of reward changes. Through various experiments, including those in egocentric navigation and robotic arm manipulation tasks, we found that proper reward transitions significantly influence sample efficiency and success rates. Of particular note is the efficacy of the toddler-inspired Sparse-to-Dense (S2D) transition. Beyond these performance metrics, using Cross-Density Visualizer technique, we observed that transitions, especially the S2D, smooth the policy loss landscape, promoting wide minima that enhance generalization in RL models.</p></p class="citation"></blockquote><h3 id=4963--196318-quantifying-the-sensitivity-of-inverse-reinforcement-learning-to-misspecification-joar-skalse-et-al-2024>(49/63 | 196/318) Quantifying the Sensitivity of Inverse Reinforcement Learning to Misspecification (Joar Skalse et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Joar Skalse, Alessandro Abate. (2024)<br><strong>Quantifying the Sensitivity of Inverse Reinforcement Learning to Misspecification</strong><br><button class=copy-to-clipboard title="Quantifying the Sensitivity of Inverse Reinforcement Learning to Misspecification" index=196>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-196 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06854v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06854v1.pdf filename=2403.06854v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Inverse <b>reinforcement</b> <b>learning</b> (IRL) aims to infer an agent&rsquo;s preferences (represented as a reward function $R$) from their behaviour (represented as a policy $\pi$). To do this, we need a behavioural model of how $\pi$ relates to $R$. In the current literature, the most common behavioural models are optimality, Boltzmann-rationality, and causal entropy maximisation. However, the true relationship between a human&rsquo;s preferences and their behaviour is much more complex than any of these behavioural models. This means that the behavioural models are misspecified, which raises the concern that they may lead to systematic errors if applied to real data. In this paper, we analyse how sensitive the IRL problem is to misspecification of the behavioural model. Specifically, we provide necessary and sufficient conditions that completely characterise how the observed data may differ from the assumed behavioural model without incurring an error above a given threshold. In addition to this, we also characterise the conditions under which a behavioural model is robust to small perturbations of the observed policy, and we analyse how robust many behavioural models are to misspecification of their parameter values (such as e.g.\ the discount rate). Our analysis suggests that the IRL problem is highly sensitive to misspecification, in the sense that very mild misspecification can lead to very large errors in the inferred reward function.</p></p class="citation"></blockquote><h3 id=5063--197318-monotone-individual-fairness-yahav-bechavod-2024>(50/63 | 197/318) Monotone Individual Fairness (Yahav Bechavod, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yahav Bechavod. (2024)<br><strong>Monotone Individual Fairness</strong><br><button class=copy-to-clipboard title="Monotone Individual Fairness" index=197>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-197 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CY, cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06812v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06812v1.pdf filename=2403.06812v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We revisit the problem of online learning with individual <b>fairness,</b> where an online learner strives to maximize predictive accuracy while ensuring that similar individuals are treated similarly. We first extend the frameworks of Gillen et al. (2018); Bechavod et al. (2020), which rely on feedback from human auditors regarding <b>fairness</b> violations, as we consider auditing schemes that are capable of aggregating feedback from any number of auditors, using a rich class we term monotone aggregation functions. We then prove a characterization for such auditing schemes, practically reducing the analysis of auditing for individual <b>fairness</b> by multiple auditors to that of auditing by (instance-specific) single auditors. Using our generalized framework, we present an oracle-efficient algorithm achieving an upper bound frontier of $(\mathcal{O}(T^{1/2+2b}),\mathcal{O}(T^{3/4-b}))$ respectively for regret, number of <b>fairness</b> violations, for $0\leq b \leq 1/4$. We then study an online classification setting where label feedback is available for positively-predicted individuals only, and present an oracle-efficient algorithm achieving an upper bound frontier of $(\mathcal{O}(T^{2/3+2b}),\mathcal{O}(T^{5/6-b}))$ for regret, number of <b>fairness</b> violations, for $0\leq b \leq 1/6$. In both settings, our algorithms improve on the best known bounds for oracle-efficient algorithms. Furthermore, our algorithms offer significant improvements in computational efficiency, greatly reducing the number of required calls to an (offline) optimization oracle per round, to $\tilde{\mathcal{O}}(\alpha^{-2})$ in the full information setting, and $\tilde{\mathcal{O}}(\alpha^{-2} + k^2T^{1/3})$ in the partial information setting, where $\alpha$ is the sensitivity for reporting <b>fairness</b> violations, and $k$ is the number of individuals in a round.</p></p class="citation"></blockquote><h3 id=5163--198318-adaptive-federated-learning-over-the-air-chenhao-wang-et-al-2024>(51/63 | 198/318) Adaptive Federated Learning Over the Air (Chenhao Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chenhao Wang, Zihan Chen, Nikolaos Pappas, Howard H. Yang, Tony Q. S. Quek, H. Vincent Poor. (2024)<br><strong>Adaptive Federated Learning Over the Air</strong><br><button class=copy-to-clipboard title="Adaptive Federated Learning Over the Air" index=198>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-198 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-IT, cs-LG, cs-NI, cs.LG, math-IT<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06528v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06528v1.pdf filename=2403.06528v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose a <b>federated</b> <b>version</b> of adaptive gradient methods, particularly AdaGrad and Adam, within the framework of over-the-air model training. This approach capitalizes on the inherent superposition property of wireless channels, facilitating fast and scalable parameter aggregation. Meanwhile, it enhances the robustness of the model training process by dynamically adjusting the stepsize in accordance with the global gradient update. We derive the convergence rate of the training algorithms, encompassing the effects of channel fading and interference, for a broad spectrum of nonconvex loss functions. Our analysis shows that the AdaGrad-based algorithm converges to a stationary point at the rate of $\mathcal{O}( \ln{(T)} /{ T^{ 1 - \frac{1}{\alpha} } } )$, where $\alpha$ represents the tail index of the electromagnetic interference. This result indicates that the level of heavy-tailedness in interference distribution plays a crucial role in the training efficiency: the heavier the tail, the slower the algorithm converges. In contrast, an Adam-like algorithm converges at the $\mathcal{O}( 1/T )$ rate, demonstrating its advantage in expediting the model training process. We conduct extensive experiments that corroborate our theoretical findings and affirm the practical efficacy of our proposed <b>federated</b> <b>adaptive</b> gradient methods.</p></p class="citation"></blockquote><h3 id=5263--199318-a-converting-autoencoder-toward-low-latency-and-energy-efficient-dnn-inference-at-the-edge-hasanul-mahmud-et-al-2024>(52/63 | 199/318) A Converting Autoencoder Toward Low-latency and Energy-efficient DNN Inference at the Edge (Hasanul Mahmud et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hasanul Mahmud, Peng Kang, Kevin Desai, Palden Lama, Sushil Prasad. (2024)<br><strong>A Converting Autoencoder Toward Low-latency and Energy-efficient DNN Inference at the Edge</strong><br><button class=copy-to-clipboard title="A Converting Autoencoder Toward Low-latency and Energy-efficient DNN Inference at the Edge" index=199>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-199 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-DC, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07036v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07036v1.pdf filename=2403.07036v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Reducing inference time and energy usage while maintaining prediction accuracy has become a significant concern for deep neural networks (DNN) inference on resource-constrained edge devices. To address this problem, we propose a novel approach based on &ldquo;converting&rdquo; <b>autoencoder</b> and lightweight DNNs. This improves upon recent work such as early-exiting framework and DNN partitioning. Early-exiting frameworks spend different amounts of computation power for different input data depending upon their complexity. However, they can be inefficient in real-world scenarios that deal with many hard image samples. On the other hand, DNN partitioning algorithms that utilize the computation power of both the cloud and edge devices can be affected by network delays and intermittent connections between the cloud and the edge. We present CBNet, a low-latency and energy-efficient DNN inference framework tailored for edge devices. It utilizes a &ldquo;converting&rdquo; <b>autoencoder</b> to efficiently transform hard images into easy ones, which are subsequently processed by a lightweight DNN for inference. To the best of our knowledge, such <b>autoencoder</b> has not been proposed earlier. Our experimental results using three popular image-classification datasets on a Raspberry Pi 4, a Google Cloud instance, and an instance with Nvidia Tesla K80 GPU show that CBNet achieves up to 4.8x speedup in inference latency and 79% reduction in energy usage compared to competing techniques while maintaining similar or higher accuracy.</p></p class="citation"></blockquote><h3 id=5363--200318-prediction-of-wort-density-with-lstm-network-derk-rembold-et-al-2024>(53/63 | 200/318) Prediction of Wort Density with LSTM Network (Derk Rembold et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Derk Rembold, Bernd Stauss, Stefan Schwarzkopf. (2024)<br><strong>Prediction of Wort Density with LSTM Network</strong><br><button class=copy-to-clipboard title="Prediction of Wort Density with LSTM Network" index=200>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-200 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: LSTM<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06458v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06458v1.pdf filename=2403.06458v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Many physical target values in technical processes are error-prone, cumbersome, or expensive to measure automatically. One example of a physical target value is the wort density, which is an important value needed for beer production. This article introduces a system that helps the brewer measure wort density through sensors in order to reduce errors in manual data collection. Instead of a direct measurement of wort density, a method is developed that calculates the density from measured values acquired by inexpensive standard sensors such as pressure or temperature. The model behind the calculation is a neural network, known as <b>LSTM.</b></p></p class="citation"></blockquote><h3 id=5463--201318-on-the-diminishing-returns-of-width-for-continual-learning-etash-guha-et-al-2024>(54/63 | 201/318) On the Diminishing Returns of Width for Continual Learning (Etash Guha et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Etash Guha, Vihan Lakshman. (2024)<br><strong>On the Diminishing Returns of Width for Continual Learning</strong><br><button class=copy-to-clipboard title="On the Diminishing Returns of Width for Continual Learning" index=201>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-201 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Continual Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06398v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06398v2.pdf filename=2403.06398v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While deep neural networks have demonstrated groundbreaking performance in various settings, these models often suffer from \emph{catastrophic forgetting} when trained on new tasks in sequence. Several works have empirically demonstrated that increasing the width of a neural network leads to a decrease in catastrophic forgetting but have yet to characterize the exact relationship between width and <b>continual</b> <b>learning.</b> We design one of the first frameworks to analyze <b>Continual</b> <b>Learning</b> Theory and prove that width is directly related to forgetting in Feed-Forward Networks (FFN). Specifically, we demonstrate that increasing network widths to reduce forgetting yields diminishing returns. We empirically verify our claims at widths hitherto unexplored in prior studies where the diminishing returns are clearly observed as predicted by our theory.</p></p class="citation"></blockquote><h3 id=5563--202318-deepsafempc-deep-learning-based-model-predictive-control-for-safe-multi-agent-reinforcement-learning-xuefeng-wang-et-al-2024>(55/63 | 202/318) DeepSafeMPC: Deep Learning-Based Model Predictive Control for Safe Multi-Agent Reinforcement Learning (Xuefeng Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xuefeng Wang, Henglin Pu, Hyung Jun Kim, Husheng Li. (2024)<br><strong>DeepSafeMPC: Deep Learning-Based Model Predictive Control for Safe Multi-Agent Reinforcement Learning</strong><br><button class=copy-to-clipboard title="DeepSafeMPC: Deep Learning-Based Model Predictive Control for Safe Multi-Agent Reinforcement Learning" index=202>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-202 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-SY, cs.LG, eess-SY<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06397v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06397v2.pdf filename=2403.06397v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Safe Multi-agent <b>reinforcement</b> <b>learning</b> (safe MARL) has increasingly gained attention in recent years, emphasizing the need for agents to not only optimize the global return but also adhere to safety requirements through behavioral constraints. Some recent work has integrated control theory with multi-agent <b>reinforcement</b> <b>learning</b> to address the challenge of ensuring safety. However, there have been only very limited applications of Model Predictive Control (MPC) methods in this domain, primarily due to the complex and implicit dynamics characteristic of multi-agent environments. To bridge this gap, we propose a novel method called Deep Learning-Based Model Predictive Control for Safe Multi-Agent <b>Reinforcement</b> <b>Learning</b> (DeepSafeMPC). The key insight of DeepSafeMPC is leveraging a entralized deep learning model to well predict environmental dynamics. Our method applies MARL principles to search for optimal solutions. Through the employment of MPC, the actions of agents can be restricted within safe states concurrently. We demonstrate the effectiveness of our approach using the Safe Multi-agent MuJoCo environment, showcasing significant advancements in addressing safety concerns in MARL.</p></p class="citation"></blockquote><h3 id=5663--203318-towards-robust-out-of-distribution-generalization-bounds-via-sharpness-yingtian-zou-et-al-2024>(56/63 | 203/318) Towards Robust Out-of-Distribution Generalization Bounds via Sharpness (Yingtian Zou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yingtian Zou, Kenji Kawaguchi, Yingnan Liu, Jiashuo Liu, Mong-Li Lee, Wynne Hsu. (2024)<br><strong>Towards Robust Out-of-Distribution Generalization Bounds via Sharpness</strong><br><button class=copy-to-clipboard title="Towards Robust Out-of-Distribution Generalization Bounds via Sharpness" index=203>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-203 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06392v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06392v1.pdf filename=2403.06392v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Generalizing to <b>out-of-distribution</b> (OOD) data or unseen domain, termed OOD generalization, still lacks appropriate theoretical guarantees. Canonical OOD bounds focus on different distance measurements between source and target domains but fail to consider the optimization property of the learned model. As empirically shown in recent work, the sharpness of learned minima influences OOD generalization. To bridge this gap between optimization and OOD generalization, we study the effect of sharpness on how a model tolerates data change in domain shift which is usually captured by &ldquo;robustness&rdquo; in generalization. In this paper, we give a rigorous connection between sharpness and robustness, which gives better OOD guarantees for robust algorithms. It also provides a theoretical backing for &ldquo;flat minima leads to better OOD generalization&rdquo;. Overall, we propose a sharpness-based OOD generalization bound by taking robustness into consideration, resulting in a tighter bound than non-robust guarantees. Our findings are supported by the experiments on a ridge regression model, as well as the experiments on deep learning classification tasks.</p></p class="citation"></blockquote><h3 id=5763--204318-finite-time-error-analysis-of-soft-q-learning-switching-system-approach-narim-jeong-et-al-2024>(57/63 | 204/318) Finite-Time Error Analysis of Soft Q-Learning: Switching System Approach (Narim Jeong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Narim Jeong, Donghwan Lee. (2024)<br><strong>Finite-Time Error Analysis of Soft Q-Learning: Switching System Approach</strong><br><button class=copy-to-clipboard title="Finite-Time Error Analysis of Soft Q-Learning: Switching System Approach" index=204>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-204 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06366v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06366v1.pdf filename=2403.06366v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Soft Q-learning is a variation of Q-learning designed to solve entropy regularized Markov decision problems where an agent aims to maximize the entropy regularized value function. Despite its empirical success, there have been limited theoretical studies of soft Q-learning to date. This paper aims to offer a novel and unified finite-time, control-theoretic analysis of soft Q-learning algorithms. We focus on two types of soft Q-learning algorithms: one utilizing the log-sum-exp operator and the other employing the Boltzmann operator. By using dynamical switching system models, we derive novel finite-time error bounds for both soft Q-learning algorithms. We hope that our analysis will deepen the current understanding of soft Q-learning by establishing connections with switching system models and may even pave the way for new frameworks in the finite-time analysis of other <b>reinforcement</b> <b>learning</b> algorithms.</p></p class="citation"></blockquote><h3 id=5863--205318-a-representation-learning-game-for-classes-of-prediction-tasks-neria-uzan-et-al-2024>(58/63 | 205/318) A representation-learning game for classes of prediction tasks (Neria Uzan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Neria Uzan, Nir Weinberger. (2024)<br><strong>A representation-learning game for classes of prediction tasks</strong><br><button class=copy-to-clipboard title="A representation-learning game for classes of prediction tasks" index=205>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-205 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-IT, cs-LG, cs.LG, math-IT<br>Keyword Score: 5<br>Keywords: Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06971v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06971v1.pdf filename=2403.06971v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose a game-based formulation for learning dimensionality-reducing <b>representations</b> <b>of</b> feature vectors, when only a prior knowledge on future prediction tasks is available. In this game, the first player chooses a <b>representation,</b> <b>and</b> then the second player adversarially chooses a prediction task from a given class, representing the prior knowledge. The first player aims is to minimize, and the second player to maximize, the regret: The minimal prediction loss using the <b>representation,</b> <b>compared</b> to the same loss using the original features. For the canonical setting in which the <b>representation,</b> <b>the</b> response to predict and the predictors are all linear functions, and under the mean squared error loss function, we derive the theoretically optimal <b>representation</b> <b>in</b> pure strategies, which shows the effectiveness of the prior knowledge, and the optimal regret in mixed strategies, which shows the usefulness of randomizing the <b>representation.</b> <b>For</b> general <b>representations</b> <b>and</b> loss functions, we propose an efficient algorithm to optimize a randomized <b>representation.</b> <b>The</b> algorithm only requires the gradients of the loss function, and is based on incrementally adding a <b>representation</b> <b>rule</b> to a mixture of such rules.</p></p class="citation"></blockquote><h3 id=5963--206318-sliced-wasserstein-distances-and-flows-on-cartan-hadamard-manifolds-clément-bonet-et-al-2024>(59/63 | 206/318) Sliced-Wasserstein Distances and Flows on Cartan-Hadamard Manifolds (Clément Bonet et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Clément Bonet, Lucas Drumetz, Nicolas Courty. (2024)<br><strong>Sliced-Wasserstein Distances and Flows on Cartan-Hadamard Manifolds</strong><br><button class=copy-to-clipboard title="Sliced-Wasserstein Distances and Flows on Cartan-Hadamard Manifolds" index=206>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-206 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06560v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06560v1.pdf filename=2403.06560v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While many Machine Learning methods were developed or transposed on Riemannian manifolds to tackle data with known non Euclidean <b>geometry,</b> Optimal Transport (OT) methods on such spaces have not received much attention. The main OT tool on these spaces is the Wasserstein distance which suffers from a heavy computational burden. On Euclidean spaces, a popular alternative is the Sliced-Wasserstein distance, which leverages a closed-form solution of the Wasserstein distance in one dimension, but which is not readily available on manifolds. In this work, we derive general constructions of Sliced-Wasserstein distances on Cartan-Hadamard manifolds, Riemannian manifolds with non-positive curvature, which include among others Hyperbolic spaces or the space of Symmetric Positive Definite matrices. Then, we propose different applications. Additionally, we derive non-parametric schemes to minimize these new distances by approximating their Wasserstein gradient flows.</p></p class="citation"></blockquote><h3 id=6063--207318-benign-overfitting-in-leaky-relu-networks-with-moderate-input-dimension-kedar-karhadkar-et-al-2024>(60/63 | 207/318) Benign overfitting in leaky ReLU networks with moderate input dimension (Kedar Karhadkar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kedar Karhadkar, Erin George, Michael Murray, Guido Montúfar, Deanna Needell. (2024)<br><strong>Benign overfitting in leaky ReLU networks with moderate input dimension</strong><br><button class=copy-to-clipboard title="Benign overfitting in leaky ReLU networks with moderate input dimension" index=207>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-207 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 3<br>Keywords: Sample Size<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06903v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06903v1.pdf filename=2403.06903v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The problem of benign overfitting asks whether it is possible for a model to perfectly fit noisy training data and still generalize well. We study benign overfitting in two-layer leaky ReLU networks trained with the hinge loss on a binary classification task. We consider input data which can be decomposed into the sum of a common signal and a random noise component, which lie on subspaces orthogonal to one another. We characterize conditions on the signal to noise ratio (SNR) of the model parameters giving rise to benign versus non-benign, or harmful, overfitting: in particular, if the SNR is high then benign overfitting occurs, conversely if the SNR is low then harmful overfitting occurs. We attribute both benign and non-benign overfitting to an approximate margin maximization property and show that leaky ReLU networks trained on hinge loss with Gradient Descent (GD) satisfy this property. In contrast to prior work we do not require near orthogonality conditions on the training data: notably, for input dimension $d$ and training <b>sample</b> <b>size</b> $n$, while prior work shows asymptotically optimal error when $d = \Omega(n^2 \log n)$, here we require only $d = \Omega\left(n \log \frac{1}{\epsilon}\right)$ to obtain error within $\epsilon$ of optimal.</p></p class="citation"></blockquote><h3 id=6163--208318-ant-colony-sampling-with-gflownets-for-combinatorial-optimization-minsu-kim-et-al-2024>(61/63 | 208/318) Ant Colony Sampling with GFlowNets for Combinatorial Optimization (Minsu Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Minsu Kim, Sanghyeok Choi, Jiwoo Son, Hyeonah Kim, Jinkyoo Park, Yoshua Bengio. (2024)<br><strong>Ant Colony Sampling with GFlowNets for Combinatorial Optimization</strong><br><button class=copy-to-clipboard title="Ant Colony Sampling with GFlowNets for Combinatorial Optimization" index=208>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-208 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-NE, cs.LG<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07041v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07041v1.pdf filename=2403.07041v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces the Generative Flow Ant Colony Sampler (GFACS), a novel neural-guided meta-heuristic algorithm for combinatorial optimization. GFACS integrates generative flow networks (GFlowNets) with the ant colony optimization (ACO) methodology. GFlowNets, a generative model that learns a constructive policy in combinatorial spaces, enhance ACO by providing an informed prior distribution of decision variables conditioned on input <b>graph</b> instances. Furthermore, we introduce a novel combination of training tricks, including search-guided local exploration, energy normalization, and energy shaping to improve GFACS. Our experimental results demonstrate that GFACS outperforms baseline ACO algorithms in seven CO tasks and is competitive with problem-specific heuristics for vehicle routing problems. The source code is available at \url{https://github.com/ai4co/gfacs}.</p></p class="citation"></blockquote><h3 id=6263--209318-ffad-a-novel-metric-for-assessing-generated-time-series-data-utilizing-fourier-transform-and-auto-encoder-yang-chen-et-al-2024>(62/63 | 209/318) FFAD: A Novel Metric for Assessing Generated Time Series Data Utilizing Fourier Transform and Auto-encoder (Yang Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yang Chen, Dustin J. Kempton, Rafal A. Angryk. (2024)<br><strong>FFAD: A Novel Metric for Assessing Generated Time Series Data Utilizing Fourier Transform and Auto-encoder</strong><br><button class=copy-to-clipboard title="FFAD: A Novel Metric for Assessing Generated Time Series Data Utilizing Fourier Transform and Auto-encoder" index=209>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-209 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06576v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06576v1.pdf filename=2403.06576v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The success of deep learning-based generative models in producing realistic images, videos, and audios has led to a crucial consideration: how to effectively assess the quality of synthetic samples. While the Fr'{e}chet Inception Distance (FID) serves as the standard metric for evaluating generative models in image synthesis, a comparable metric for time series data is notably absent. This gap in assessment capabilities stems from the absence of a widely accepted feature vector extractor pre-trained on <b>benchmark</b> time series datasets. In addressing these challenges related to assessing the quality of time series, particularly in the context of Fr'echet Distance, this work proposes a novel solution leveraging the Fourier transform and Auto-encoder, termed the Fr'{e}chet Fourier-transform Auto-encoder Distance (FFAD). Through our experimental results, we showcase the potential of FFAD for effectively distinguishing samples from different classes. This novel metric emerges as a fundamental tool for the evaluation of generative time series data, contributing to the ongoing efforts of enhancing assessment methodologies in the realm of deep learning-based generative models.</p></p class="citation"></blockquote><h3 id=6363--210318-decentralized-and-lifelong-adaptive-multi-agent-collaborative-learning-shuo-tang-et-al-2024>(63/63 | 210/318) Decentralized and Lifelong-Adaptive Multi-Agent Collaborative Learning (Shuo Tang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shuo Tang, Rui Ye, Chenxin Xu, Xiaowen Dong, Siheng Chen, Yanfeng Wang. (2024)<br><strong>Decentralized and Lifelong-Adaptive Multi-Agent Collaborative Learning</strong><br><button class=copy-to-clipboard title="Decentralized and Lifelong-Adaptive Multi-Agent Collaborative Learning" index=210>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-210 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-MA, cs.LG<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06535v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06535v1.pdf filename=2403.06535v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Decentralized and lifelong-adaptive multi-agent collaborative learning aims to enhance collaboration among multiple agents without a central server, with each agent solving varied tasks over time. To achieve efficient collaboration, agents should: i) autonomously identify beneficial collaborative relationships in a decentralized manner; and ii) adapt to dynamically changing task observations. In this paper, we propose DeLAMA, a decentralized multi-agent lifelong collaborative learning algorithm with dynamic collaboration <b>graphs.</b> To promote autonomous collaboration relationship learning, we propose a decentralized <b>graph</b> structure learning algorithm, eliminating the need for external priors. To facilitate adaptation to dynamic tasks, we design a memory unit to capture the agents&rsquo; accumulated learning history and knowledge, while preserving finite storage consumption. To further augment the system&rsquo;s expressive capabilities and computational efficiency, we apply algorithm unrolling, leveraging the advantages of both mathematical optimization and neural networks. This allows the agents to `learn to collaborate&rsquo; through the supervision of training tasks. Our theoretical analysis verifies that inter-agent collaboration is communication efficient under a small number of communication rounds. The experimental results verify its ability to facilitate the discovery of collaboration strategies and adaptation to dynamic learning scenarios, achieving a 98.80% reduction in MSE and a 188.87% improvement in classification accuracy. We expect our work can serve as a foundational technique to facilitate future works towards an intelligent, decentralized, and dynamic multi-agent system. Code is available at <a href=https://github.com/ShuoTang123/DeLAMA>https://github.com/ShuoTang123/DeLAMA</a>.</p></p class="citation"></blockquote><h2 id=hep-ph-1>hep-ph (1)</h2><h3 id=11--211318-re-simulation-based-self-supervised-learning-for-pre-training-foundation-models-philip-harris-et-al-2024>(1/1 | 211/318) Re-Simulation-based Self-Supervised Learning for Pre-Training Foundation Models (Philip Harris et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Philip Harris, Michael Kagan, Jeffrey Krupa, Benedikt Maier, Nathaniel Woodward. (2024)<br><strong>Re-Simulation-based Self-Supervised Learning for Pre-Training Foundation Models</strong><br><button class=copy-to-clipboard title="Re-Simulation-based Self-Supervised Learning for Pre-Training Foundation Models" index=211>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-211 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: hep-ph<br>Categories: cs-LG, hep-ex, hep-ph, hep-ph<br>Keyword Score: 70<br>Keywords: Contrastive Learning, Data Augmentation, Foundation Model, Self-supervised Learning, Self-supervised Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07066v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07066v1.pdf filename=2403.07066v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Self-Supervised</b> <b>Learning</b> (SSL) is at the core of training modern large machine learning models, providing a scheme for learning powerful representations that can be used in a variety of downstream tasks. However, SSL strategies must be adapted to the type of training <b>data</b> <b>and</b> downstream tasks required. We propose RS3L, a novel <b>simulation-based</b> SSL strategy that employs a method of re-simulation to drive <b>data</b> <b>augmentation</b> for <b>contrastive</b> <b>learning.</b> By intervening in the middle of the <b>simulation</b> process and re-running <b>simulation</b> components downstream of the intervention, we generate multiple realizations of an event, thus producing a set of augmentations covering all physics-driven variations available in the simulator. Using experiments from high-energy physics, we explore how this strategy may enable the development of a <b>foundation</b> <b>model;</b> we show how R3SL pre-training enables powerful performance in downstream tasks such as discrimination of a variety of objects and uncertainty mitigation. In addition to our results, we make the RS3L dataset publicly available for further studies on how to improve SSL strategies.</p></p class="citation"></blockquote><h2 id=csdc-6>cs.DC (6)</h2><h3 id=16--212318-adding-nvme-ssds-to-enable-and-accelerate-100b-model-fine-tuning-on-a-single-gpu-changyue-liao-et-al-2024>(1/6 | 212/318) Adding NVMe SSDs to Enable and Accelerate 100B Model Fine-tuning on a Single GPU (Changyue Liao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Changyue Liao, Mo Sun, Zihan Yang, Kaiqi Chen, Binhang Yuan, Fei Wu, Zeke Wang. (2024)<br><strong>Adding NVMe SSDs to Enable and Accelerate 100B Model Fine-tuning on a Single GPU</strong><br><button class=copy-to-clipboard title="Adding NVMe SSDs to Enable and Accelerate 100B Model Fine-tuning on a Single GPU" index=212>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-212 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs.DC<br>Keyword Score: 70<br>Keywords: Fine-tuning, Fine-tuning, Stochastic Gradient Descent, GPT, GPT-3, Stemming, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06504v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06504v1.pdf filename=2403.06504v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advances in <b>large</b> <b>language</b> <b>models</b> have brought immense value to the world, with their superior capabilities <b>stemming</b> from the massive number of parameters they utilize. However, even the GPUs with the highest memory capacities, currently peaking at 80GB, are far from sufficient to accommodate these vast parameters and their associated optimizer states when conducting <b>stochastic</b> <b>gradient</b> <b>descent-based</b> optimization. One approach to hosting such huge models is to aggregate device memory from many GPUs. However, this approach introduces prohibitive costs for most academic researchers, who always have a limited budget for many high-end GPU servers. In this paper, we focus on huge model <b>fine-tuning</b> on a single, even low-end, GPU in a commodity server, which is accessible to most AI researchers. In such a scenario, the state-of-the-art work ZeRO-Infinity suffers from two severe issues when running in a commodity server: 1) low GPU utilization due to inefficient swapping, and 2) limited trainable model size due to CPU memory capacity. The underlying reason is that ZeRO-Infinity is optimized for running on high-end GPU servers. To this end, we present Fuyou, a low-cost training framework that enables efficient 100B huge model <b>fine-tuning</b> on a low-end server with a low-end GPU and limited CPU memory capacity. The key idea is to add the SSD-CPU communication as an optimization dimension and thus carefully co-optimize computation and data swapping from a systematic approach to maximize GPU utilization. The experimental results show that 1) Fuyou is able to <b>fine-tune</b> 175B <b>GPT-3</b> on a consumer GPU RTX 4090 with high GPU utilization, while ZeRO-Infinity fails to <b>fine-tune;</b> and 2) when training a small <b>GPT-3</b> 13B model, Fuyou achieves 156 TFLOPS on an RTX 4090 GPU while ZeRO-Infinity only achieves 45 TFLOPS.</p></p class="citation"></blockquote><h3 id=26--213318-dynamic-client-clustering-bandwidth-allocation-and-workload-optimization-for-semi-synchronous-federated-learning-liangkun-yu-et-al-2024>(2/6 | 213/318) Dynamic Client Clustering, Bandwidth Allocation, and Workload Optimization for Semi-synchronous Federated Learning (Liangkun Yu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Liangkun Yu, Xiang Sun, Rana Albelaihi, Chaeeun Park, Sihua Shao. (2024)<br><strong>Dynamic Client Clustering, Bandwidth Allocation, and Workload Optimization for Semi-synchronous Federated Learning</strong><br><button class=copy-to-clipboard title="Dynamic Client Clustering, Bandwidth Allocation, and Workload Optimization for Semi-synchronous Federated Learning" index=213>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-213 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs.DC<br>Keyword Score: 46<br>Keywords: MNIST, Benchmarking, Clustering, Federated Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06900v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06900v1.pdf filename=2403.06900v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Federated</b> <b>Learning</b> (FL) revolutionizes collaborative machine learning among Internet of Things (IoT) devices by enabling them to train models collectively while preserving data privacy. FL algorithms fall into two primary categories: synchronous and asynchronous. While synchronous FL efficiently handles straggler devices, it can compromise convergence speed and model accuracy. In contrast, asynchronous FL allows all devices to participate but incurs high communication overhead and potential model staleness. To overcome these limitations, the semi-synchronous FL framework introduces client tiering based on computing and communication latencies. Clients in different tiers upload their local models at distinct frequencies, striking a balance between straggler mitigation and communication costs. Enter the DecantFed algorithm (Dynamic client <b>clustering,</b> bandwidth allocation, and local training for semi-synchronous <b>Federated</b> <b>learning),</b> a dynamic solution that optimizes client <b>clustering,</b> bandwidth allocation, and local training workloads to maximize data sample processing rates. Additionally, DecantFed adapts client learning rates according to their tiers, addressing the model staleness problem. The algorithm&rsquo;s performance shines in extensive <b>simulations</b> using <b>benchmark</b> datasets, including <b>MNIST</b> and CIFAR-10, under independent and identically distributed (IID) and non-IID scenarios. DecantFed outpaces FedAvg and FedProx in terms of convergence speed and delivers a remarkable minimum 28% boost in model accuracy compared to FedProx.</p></p class="citation"></blockquote><h3 id=36--214318-accelerating-sparse-tensor-decomposition-using-adaptive-linearized-representation-jan-laukemann-et-al-2024>(3/6 | 214/318) Accelerating Sparse Tensor Decomposition Using Adaptive Linearized Representation (Jan Laukemann et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jan Laukemann, Ahmed E. Helal, S. Isaac Geronimo Anderson, Fabio Checconi, Yongseok Soh, Jesmin Jahan Tithi, Teresa Ranadive, Brian J Gravelle, Fabrizio Petrini, Jee Choi. (2024)<br><strong>Accelerating Sparse Tensor Decomposition Using Adaptive Linearized Representation</strong><br><button class=copy-to-clipboard title="Accelerating Sparse Tensor Decomposition Using Adaptive Linearized Representation" index=214>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-214 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs-DS, cs-PF, cs.DC<br>Keyword Score: 30<br>Keywords: Anomaly Detection, Tensor Decomposition, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06348v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06348v1.pdf filename=2403.06348v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>High-dimensional sparse data emerge in many critical application domains such as cybersecurity, healthcare, <b>anomaly</b> <b>detection,</b> and trend analysis. To quickly extract meaningful insights from massive volumes of these multi-dimensional data, scientists employ <b>unsupervised</b> analysis tools based on <b>tensor</b> <b>decomposition</b> (TD) methods. However, real-world sparse <b>tensors</b> <b>exhibit</b> highly irregular shapes, data distributions, and sparsity, which pose significant challenges for making efficient use of modern parallel architectures. This study breaks the prevailing assumption that compressing sparse <b>tensors</b> <b>into</b> coarse-grained structures (i.e., <b>tensor</b> <b>slices</b> or blocks) or along a particular dimension/mode (i.e., mode-specific) is more efficient than keeping them in a fine-grained, mode-agnostic form. Our novel sparse <b>tensor</b> <b>representation,</b> Adaptive Linearized <b>Tensor</b> <b>Order</b> (ALTO), encodes <b>tensors</b> <b>in</b> a compact format that can be easily streamed from memory and is amenable to both caching and parallel execution. To demonstrate the efficacy of ALTO, we accelerate popular TD methods that compute the Canonical Polyadic Decomposition (CPD) model across a range of real-world sparse <b>tensors.</b> <b>Additionally,</b> we characterize the major execution bottlenecks of TD methods on multiple generations of the latest Intel Xeon Scalable processors, including Sapphire Rapids CPUs, and introduce dynamic adaptation heuristics to automatically select the best algorithm based on the sparse <b>tensor</b> <b>characteristics.</b> Across a diverse set of real-world data sets, ALTO outperforms the state-of-the-art approaches, achieving more than an order-of-magnitude speedup over the best mode-agnostic formats. Compared to the best mode-specific formats, which require multiple <b>tensor</b> <b>copies,</b> ALTO achieves more than 5.1x geometric mean speedup at a fraction (25%) of their storage.</p></p class="citation"></blockquote><h3 id=46--215318-data-poisoning-attacks-in-gossip-learning-alexandre-pham-et-al-2024>(4/6 | 215/318) Data Poisoning Attacks in Gossip Learning (Alexandre Pham et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alexandre Pham, Maria Potop-Butucaru, Sébastien Tixeuil, Serge Fdida. (2024)<br><strong>Data Poisoning Attacks in Gossip Learning</strong><br><button class=copy-to-clipboard title="Data Poisoning Attacks in Gossip Learning" index=215>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-215 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs.DC<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06583v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06583v1.pdf filename=2403.06583v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Traditional machine learning systems were designed in a centralized manner. In such designs, the central entity maintains both the machine learning model and the data used to adjust the model&rsquo;s parameters. As data centralization yields privacy issues, <b>Federated</b> <b>Learning</b> was introduced to reduce data sharing and have a central server coordinate the learning of multiple devices. While <b>Federated</b> <b>Learning</b> is more decentralized, it still relies on a central entity that may fail or be subject to attacks, provoking the failure of the whole system. Then, Decentralized <b>Federated</b> <b>Learning</b> removes the need for a central server entirely, letting participating processes handle the coordination of the model construction. This distributed control urges studying the possibility of malicious attacks by the participants themselves. While poisoning attacks on <b>Federated</b> <b>Learning</b> have been extensively studied, their effects in Decentralized <b>Federated</b> <b>Learning</b> did not get the same level of attention. Our work is the first to propose a methodology to assess poisoning attacks in Decentralized <b>Federated</b> <b>Learning</b> in both churn free and churn prone scenarios. Furthermore, in order to evaluate our methodology on a case study representative for gossip learning we extended the gossipy simulator with an attack injector module.</p></p class="citation"></blockquote><h3 id=56--216318-comparing-task-graph-scheduling-algorithms-an-adversarial-approach-jared-coleman-et-al-2024>(5/6 | 216/318) Comparing Task Graph Scheduling Algorithms: An Adversarial Approach (Jared Coleman et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jared Coleman, Bhaskar Krishnamachari. (2024)<br><strong>Comparing Task Graph Scheduling Algorithms: An Adversarial Approach</strong><br><button class=copy-to-clipboard title="Comparing Task Graph Scheduling Algorithms: An Adversarial Approach" index=216>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-216 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs.DC<br>Keyword Score: 9<br>Keywords: Graph, Benchmarking, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07120v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07120v1.pdf filename=2403.07120v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Scheduling a task <b>graph</b> representing an application over a heterogeneous network of computers is a fundamental problem in distributed computing. It is known to be not only NP-hard but also not polynomial-time approximable within a constant factor. As a result, many heuristic algorithms have been proposed over the past few decades. Yet it remains largely unclear how these algorithms compare to each other in terms of the quality of schedules they produce. We identify gaps in the traditional <b>benchmarking</b> approach to comparing task scheduling algorithms and propose a simulated annealing-based adversarial analysis approach called PISA to help address them. We also introduce SAGA, a new open-source library for comparing task scheduling algorithms. We use SAGA to <b>benchmark</b> 15 algorithms on 16 datasets and PISA to compare the algorithms in a pairwise manner. Algorithms that appear to perform similarly on <b>benchmarking</b> datasets are shown to perform very differently on adversarially chosen problem instances. Interestingly, the results indicate that this is true even when the adversarial search is constrained to selecting among well-structured, application-specific problem instances. This work represents an important step towards a more general understanding of the performance boundaries between task scheduling algorithms on different families of problem instances.</p></p class="citation"></blockquote><h3 id=66--217318-parameterized-task-graph-scheduling-algorithm-for-comparing-algorithmic-components-jared-coleman-et-al-2024>(6/6 | 217/318) Parameterized Task Graph Scheduling Algorithm for Comparing Algorithmic Components (Jared Coleman et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jared Coleman, Ravi Vivek Agrawal, Ebrahim Hirani, Bhaskar Krishnamachari. (2024)<br><strong>Parameterized Task Graph Scheduling Algorithm for Comparing Algorithmic Components</strong><br><button class=copy-to-clipboard title="Parameterized Task Graph Scheduling Algorithm for Comparing Algorithmic Components" index=217>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-217 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs.DC<br>Keyword Score: 6<br>Keywords: Graph, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07112v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07112v1.pdf filename=2403.07112v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Scheduling distributed applications modeled as directed, acyclic task <b>graphs</b> to run on heterogeneous compute networks is a fundamental (NP-Hard) problem in distributed computing for which many heuristic algorithms have been proposed over the past decades. Many of these algorithms fall under the list-scheduling paradigm, whereby the algorithm first computes priorities for the tasks and then schedules them greedily to the compute node that minimizes some cost function. Thus, many algorithms differ from each other only in a few key components (e.g., the way they prioritize tasks, their cost functions, where the algorithms consider inserting tasks into a partially complete schedule, etc.). In this paper, we propose a generalized parametric list-scheduling algorithm that allows mixing and matching different algorithmic components to produce 72 unique algorithms. We <b>benchmark</b> these algorithms on four datasets to study the individual and combined effects of different algorithmic components on performance and runtime.</p></p class="citation"></blockquote><h2 id=cscr-7>cs.CR (7)</h2><h3 id=17--218318-stealing-part-of-a-production-language-model-nicholas-carlini-et-al-2024>(1/7 | 218/318) Stealing Part of a Production Language Model (Nicholas Carlini et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nicholas Carlini, Daniel Paleka, Krishnamurthy Dj Dvijotham, Thomas Steinke, Jonathan Hayase, A. Feder Cooper, Katherine Lee, Matthew Jagielski, Milad Nasr, Arthur Conmy, Eric Wallace, David Rolnick, Florian Tramèr. (2024)<br><strong>Stealing Part of a Production Language Model</strong><br><button class=copy-to-clipboard title="Stealing Part of a Production Language Model" index=218>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-218 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 65<br>Keywords: Black Box, ChatGPT, GPT, GPT-3, GPT-3.5, PaLM, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06634v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06634v1.pdf filename=2403.06634v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce the first model-stealing attack that extracts precise, nontrivial information from <b>black-box</b> <b>production</b> language models like OpenAI&rsquo;s <b>ChatGPT</b> or Google&rsquo;s <b>PaLM-2.</b> Specifically, our attack recovers the embedding projection layer (up to symmetries) of a <b>transformer</b> model, given typical API access. For under $20 USD, our attack extracts the entire projection matrix of OpenAI&rsquo;s Ada and Babbage language models. We thereby confirm, for the first time, that these <b>black-box</b> <b>models</b> have a hidden dimension of 1024 and 2048, respectively. We also recover the exact hidden dimension size of the <b>gpt-3.5-turbo</b> model, and estimate it would cost under $2,000 in queries to recover the entire projection matrix. We conclude with potential defenses and mitigations, and discuss the implications of possible future work that could extend our attack.</p></p class="citation"></blockquote><h3 id=27--219318-a-zero-trust-framework-for-realization-and-defense-against-generative-ai-attacks-in-power-grid-md-shirajum-munir-et-al-2024>(2/7 | 219/318) A Zero Trust Framework for Realization and Defense Against Generative AI Attacks in Power Grid (Md. Shirajum Munir et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Md. Shirajum Munir, Sravanthi Proddatoori, Manjushree Muralidhara, Walid Saad, Zhu Han, Sachin Shetty. (2024)<br><strong>A Zero Trust Framework for Realization and Defense Against Generative AI Attacks in Power Grid</strong><br><button class=copy-to-clipboard title="A Zero Trust Framework for Realization and Defense Against Generative AI Attacks in Power Grid" index=219>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-219 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-LG, cs.CR<br>Keyword Score: 40<br>Keywords: Generative AI, Generative Adversarial Network, Generative Adversarial Network, Zero Trust<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06388v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06388v1.pdf filename=2403.06388v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Understanding the potential of <b>generative</b> <b>AI</b> <b>(GenAI)-based</b> attacks on the power grid is a fundamental challenge that must be addressed in order to protect the power grid by realizing and validating risk in new attack vectors. In this paper, a novel <b>zero</b> <b>trust</b> framework for a power grid supply chain (PGSC) is proposed. This framework facilitates early detection of potential GenAI-driven attack vectors (e.g., replay and protocol-type attacks), assessment of tail risk-based stability measures, and mitigation of such threats. First, a new <b>zero</b> <b>trust</b> system model of PGSC is designed and formulated as a <b>zero-trust</b> <b>problem</b> that seeks to guarantee for a stable PGSC by realizing and defending against GenAI-driven cyber attacks. Second, in which a domain-specific <b>generative</b> <b>adversarial</b> <b>networks</b> <b>(GAN)-based</b> attack generation mechanism is developed to create a new vulnerability cyberspace for further understanding that threat. Third, tail-based risk realization metrics are developed and implemented for quantifying the extreme risk of a potential attack while leveraging a trust measurement approach for continuous validation. Fourth, an ensemble learning-based bootstrap aggregation scheme is devised to detect the attacks that are generating synthetic identities with convincing user and distributed energy resources device profiles. Experimental results show the efficacy of the proposed <b>zero</b> <b>trust</b> framework that achieves an accuracy of 95.7% on attack vector generation, a risk measure of 9.61% for a 95% stable PGSC, and a 99% confidence in defense against GenAI-driven attack.</p></p class="citation"></blockquote><h3 id=37--220318-dnnshield-embedding-identifiers-for-deep-neural-network-ownership-verification-jasper-stang-et-al-2024>(3/7 | 220/318) DNNShield: Embedding Identifiers for Deep Neural Network Ownership Verification (Jasper Stang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jasper Stang, Torsten Krauß, Alexandra Dmitrienko. (2024)<br><strong>DNNShield: Embedding Identifiers for Deep Neural Network Ownership Verification</strong><br><button class=copy-to-clipboard title="DNNShield: Embedding Identifiers for Deep Neural Network Ownership Verification" index=220>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-220 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 30<br>Keywords: Fine-tuning, Pruning, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06581v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06581v1.pdf filename=2403.06581v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The surge in popularity of machine learning (ML) has driven significant investments in training Deep Neural Networks (DNNs). However, these models that require resource-intensive training are vulnerable to theft and unauthorized use. This paper addresses this challenge by introducing DNNShield, a novel approach for DNN protection that integrates seamlessly before training. DNNShield embeds unique identifiers within the model architecture using specialized protection layers. These layers enable secure training and deployment while offering high resilience against various attacks, including <b>fine-tuning,</b> <b>pruning,</b> and adaptive <b>adversarial</b> <b>attacks.</b> Notably, our approach achieves this security with minimal performance and computational overhead (less than 5% runtime increase). We validate the effectiveness and efficiency of DNNShield through extensive evaluations across three datasets and four model architectures. This practical solution empowers developers to protect their DNNs and intellectual property rights.</p></p class="citation"></blockquote><h3 id=47--221318-poisoning-programs-by-un-repairing-code-security-concerns-of-ai-generated-code-cristina-improta-2024>(4/7 | 221/318) Poisoning Programs by Un-Repairing Code: Security Concerns of AI-generated Code (Cristina Improta, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Cristina Improta. (2024)<br><strong>Poisoning Programs by Un-Repairing Code: Security Concerns of AI-generated Code</strong><br><button class=copy-to-clipboard title="Poisoning Programs by Un-Repairing Code: Security Concerns of AI-generated Code" index=221>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-221 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs-SE, cs.CR<br>Keyword Score: 20<br>Keywords: Code Generation, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06675v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06675v1.pdf filename=2403.06675v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>AI-based <b>code</b> <b>generators</b> have gained a fundamental role in assisting developers in writing software starting from natural language (NL). However, since these <b>large</b> <b>language</b> <b>models</b> are trained on massive volumes of data collected from unreliable online sources (e.g., GitHub, Hugging Face), AI models become an easy target for data poisoning attacks, in which an attacker corrupts the training data by injecting a small amount of poison into it, i.e., astutely crafted malicious samples. In this position paper, we address the security of AI <b>code</b> <b>generators</b> by identifying a novel data poisoning attack that results in the generation of vulnerable <b>code.</b> <b>Next,</b> we devise an extensive evaluation of how these attacks impact state-of-the-art models for <b>code</b> <b>generation.</b> Lastly, we discuss potential solutions to overcome this threat.</p></p class="citation"></blockquote><h3 id=57--222318-unprotected-4g5g-control-procedures-at-low-layers-considered-dangerous-norbert-ludant-et-al-2024>(5/7 | 222/318) Unprotected 4G/5G Control Procedures at Low Layers Considered Dangerous (Norbert Ludant et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Norbert Ludant, Marinos Vomvas, Guevara Noubir. (2024)<br><strong>Unprotected 4G/5G Control Procedures at Low Layers Considered Dangerous</strong><br><button class=copy-to-clipboard title="Unprotected 4G/5G Control Procedures at Low Layers Considered Dangerous" index=222>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-222 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06717v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06717v1.pdf filename=2403.06717v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Over the years, several security vulnerabilities in the 3GPP cellular systems have been demonstrated in the literature. Most studies focus on higher layers of the cellular radio stack, such as the RRC and NAS, which are cryptographically protected. However, lower layers of the stack, such as PHY and MAC, are not as thoroughly studied, even though they are neither encrypted nor integrity protected. Furthermore, the latest releases of 5G significantly increased the number of low-layer control messages and procedures. The complexity of the cellular standards and the high degree of cross-layer operations, makes <b>reasoning</b> about security non-trivial, and requires a systematic analysis. We study the control procedures carried by each physical channel, and find that current cellular systems are susceptible to several new passive attacks due to information leakage, and active attacks by injecting MAC and PHY messages. For instance, we find that beamforming information leakage enables fingerprinting-based localization and tracking of users. We identify active attacks that reduce the users&rsquo; throughput by disabling RF front ends at the UE, disrupt user communications by tricking other connected UEs into acting as jammers, or stealthily disconnect an active user. We evaluate our attacks against COTS UEs in various scenarios and demonstrate their practicality by measuring current operators&rsquo; configurations across three countries. Our results show that an attacker can, among other things, localize users with an accuracy of 20 meters 96% of the time, track users&rsquo; moving paths with a probability of 90%, reduce throughput by more than 95% within 2 seconds (by spoofing a 39 bits DCI), and disconnect users.</p></p class="citation"></blockquote><h3 id=67--223318-real-is-not-true-backdoor-attacks-against-deepfake-detection-hong-sun-et-al-2024>(6/7 | 223/318) Real is not True: Backdoor Attacks Against Deepfake Detection (Hong Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hong Sun, Ziqiang Li, Lei Liu, Bin Li. (2024)<br><strong>Real is not True: Backdoor Attacks Against Deepfake Detection</strong><br><button class=copy-to-clipboard title="Real is not True: Backdoor Attacks Against Deepfake Detection" index=223>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-223 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 10<br>Keywords: Automatic Speech Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06610v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06610v1.pdf filename=2403.06610v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The proliferation of malicious deepfake applications has ignited substantial public apprehension, casting a shadow of doubt upon the integrity of digital media. Despite the development of proficient deepfake detection mechanisms, they persistently demonstrate pronounced vulnerability to an array of attacks. It is noteworthy that the pre-existing repertoire of attacks predominantly comprises adversarial example attack, predominantly manifesting during the testing phase. In the present study, we introduce a pioneering paradigm denominated as Bad-Deepfake, which represents a novel foray into the realm of backdoor attacks levied against deepfake detectors. Our approach hinges upon the strategic manipulation of a delimited subset of the training data, enabling us to wield disproportionate influence over the operational characteristics of a trained model. This manipulation leverages inherent frailties inherent to deepfake detectors, affording us the capacity to engineer triggers and judiciously select the most efficacious samples for the construction of the poisoned set. Through the synergistic amalgamation of these sophisticated techniques, we achieve an remarkable performance-a 100% attack success rate <b>(ASR)</b> against extensively employed deepfake detectors.</p></p class="citation"></blockquote><h3 id=77--224318-intra-section-code-cave-injection-for-adversarial-evasion-attacks-on-windows-pe-malware-file-kshitiz-aryal-et-al-2024>(7/7 | 224/318) Intra-Section Code Cave Injection for Adversarial Evasion Attacks on Windows PE Malware File (Kshitiz Aryal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kshitiz Aryal, Maanak Gupta, Mahmoud Abdelsalam, Moustafa Saleh. (2024)<br><strong>Intra-Section Code Cave Injection for Adversarial Evasion Attacks on Windows PE Malware File</strong><br><button class=copy-to-clipboard title="Intra-Section Code Cave Injection for Adversarial Evasion Attacks on Windows PE Malware File" index=224>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-224 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 10<br>Keywords: Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06428v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06428v1.pdf filename=2403.06428v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Windows malware is predominantly available in cyberspace and is a prime target for deliberate adversarial evasion attacks. Although researchers have investigated the adversarial malware attack problem, a multitude of important questions remain unanswered, including (a) Are the existing techniques to inject adversarial perturbations in Windows Portable Executable (PE) malware files effective enough for evasion purposes?; (b) Does the attack process preserve the original behavior of malware?; (c) Are there unexplored approaches/locations that can be used to carry out adversarial evasion attacks on Windows PE malware?; and (d) What are the optimal locations and sizes of adversarial perturbations required to evade an ML-based malware detector without significant structural change in the PE file? To answer some of these questions, this work proposes a novel approach that injects a code cave within the section (i.e., intra-section) of Windows PE malware files to make space for adversarial perturbations. In addition, a code loader is also injected inside the PE file, which reverts adversarial malware to its original form during the execution, preserving the malware&rsquo;s functionality and executability. To understand the effectiveness of our approach, we injected adversarial perturbations inside the .text, .data and .rdata sections, generated using the gradient descent and Fast Gradient Sign Method (FGSM), to target the two popular <b>CNN-based</b> malware detectors, MalConv and MalConv2. Our experiments yielded notable results, achieving a 92.31% evasion rate with gradient descent and 96.26% with FGSM against MalConv, compared to the 16.17% evasion rate for append attacks. Similarly, when targeting MalConv2, our approach achieved a remarkable maximum evasion rate of 97.93% with gradient descent and 94.34% with FGSM, significantly surpassing the 4.01% evasion rate observed with append attacks.</p></p class="citation"></blockquote><h2 id=eessas-2>eess.AS (2)</h2><h3 id=12--225318-the-evaluation-of-a-code-switched-sepedi-english-automatic-speech-recognition-system-amanda-phaladi-et-al-2024>(1/2 | 225/318) The evaluation of a code-switched Sepedi-English automatic speech recognition system (Amanda Phaladi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amanda Phaladi, Thipe Modipa. (2024)<br><strong>The evaluation of a code-switched Sepedi-English automatic speech recognition system</strong><br><button class=copy-to-clipboard title="The evaluation of a code-switched Sepedi-English automatic speech recognition system" index=225>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-225 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-CL, cs-LG, eess-AS, eess.AS<br>Keyword Score: 60<br>Keywords: High-Resource, Low-Resource, Automatic Speech Recognition, Automatic Speech Recognition, Automatic Speech Recognition, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07947v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07947v1.pdf filename=2403.07947v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Speech</b> <b>technology</b> is a field that encompasses various techniques and tools used to enable machines to interact with <b>speech,</b> <b>such</b> as <b>automatic</b> <b>speech</b> <b>recognition</b> <b>(ASR),</b> spoken dialog systems, and others, allowing a device to capture spoken words through a microphone from a human speaker. End-to-end approaches such as Connectionist Temporal Classification (CTC) and attention-based methods are the most used for the development of <b>ASR</b> systems. However, these techniques were commonly used for research and development for many <b>high-resourced</b> languages with large amounts of <b>speech</b> <b>data</b> for training and evaluation, leaving <b>low-resource</b> languages relatively underdeveloped. While the CTC method has been successfully used for other languages, its effectiveness for the Sepedi language remains uncertain. In this study, we present the evaluation of the Sepedi-English code-switched <b>automatic</b> <b>speech</b> <b>recognition</b> system. This end-to-end system was developed using the Sepedi <b>Prompted</b> Code Switching corpus and the CTC approach. The performance of the system was evaluated using both the NCHLT Sepedi test corpus and the Sepedi <b>Prompted</b> Code Switching corpus. The model produced the lowest WER of 41.9%, however, the model faced challenges in recognizing the Sepedi only text.</p></p class="citation"></blockquote><h3 id=22--226318-sonotracelab----a-raytracing-based-acoustic-modelling-system-for-simulating-echolocation-behavior-of-bats-wouter-jansen-et-al-2024>(2/2 | 226/318) SonoTraceLab &ndash; A Raytracing-Based Acoustic Modelling System for Simulating Echolocation Behavior of Bats (Wouter Jansen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wouter Jansen, Jan Steckel. (2024)<br><strong>SonoTraceLab &ndash; A Raytracing-Based Acoustic Modelling System for Simulating Echolocation Behavior of Bats</strong><br><button class=copy-to-clipboard title="SonoTraceLab -- A Raytracing-Based Acoustic Modelling System for Simulating Echolocation Behavior of Bats" index=226>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-226 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-SD, eess-AS, eess.AS<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06847v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06847v1.pdf filename=2403.06847v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Echolocation is the prime sensing modality for many species of bats, who show the intricate ability to perform a plethora of tasks in complex and unstructured environments. Understanding this exceptional feat of sensorimotor interaction is a key aspect into building more robust and performant man-made sonar sensors. In order to better understand the underlying perception mechanisms it is important to get a good insight into the nature of the reflected signals that the bat perceives. While ensonification experiments are in important way to better understand the nature of these signals, they are as time-consuming to perform as they are informative. In this paper we present SonoTraceLab, an open-source software package for simulating both technical as well as biological sonar systems in complex scenes. Using <b>simulation</b> approaches can drastically increase insights into the nature of biological echolocation systems, while reducing the time- and material complexity of performing them.</p></p class="citation"></blockquote><h2 id=quant-ph-4>quant-ph (4)</h2><h3 id=14--227318-application-of-quantum-tensor-networks-for-protein-classification-debarshi-kundu-et-al-2024>(1/4 | 227/318) Application of Quantum Tensor Networks for Protein Classification (Debarshi Kundu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Debarshi Kundu, Archisman Ghosh, Srinivasan Ekambaram, Jian Wang, Nikolay Dokholyan, Swaroop Ghosh. (2024)<br><strong>Application of Quantum Tensor Networks for Protein Classification</strong><br><button class=copy-to-clipboard title="Application of Quantum Tensor Networks for Protein Classification" index=227>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-227 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-LG, q-bio-BM, quant-ph, quant-ph<br>Keyword Score: 50<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, Recurrent Neural Network, Recurrent Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06890v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06890v1.pdf filename=2403.06890v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We show that protein sequences can be thought of as sentences in natural language processing and can be parsed using the existing Quantum Natural Language framework into parameterized quantum circuits of reasonable qubits, which can be trained to solve various protein-related machine-learning problems. We classify proteins based on their subcellular locations, a pivotal task in bioinformatics that is key to understanding biological processes and disease mechanisms. Leveraging the quantum-enhanced processing capabilities, we demonstrate that Quantum Tensor Networks (QTN) can effectively handle the complexity and diversity of protein sequences. We present a detailed methodology that adapts QTN architectures to the nuanced requirements of protein data, supported by comprehensive experimental results. We demonstrate two distinct QTNs, inspired by classical <b>recurrent</b> <b>neural</b> <b>networks</b> <b>(RNN)</b> and <b>convolutional</b> <b>neural</b> <b>networks</b> <b>(CNN),</b> to solve the binary classification task mentioned above. Our top-performing quantum model has achieved a 94% accuracy rate, which is comparable to the performance of a classical model that uses the ESM2 protein language model embeddings. It&rsquo;s noteworthy that the ESM2 model is extremely large, containing 8 million parameters in its smallest configuration, whereas our best quantum model requires only around 800 parameters. We demonstrate that these hybrid models exhibit promising performance, showcasing their potential to compete with classical models of similar complexity.</p></p class="citation"></blockquote><h3 id=24--228318-simulating-quantum-circuits-by-model-counting-jingyi-mei-et-al-2024>(2/4 | 228/318) Simulating Quantum Circuits by Model Counting (Jingyi Mei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jingyi Mei, Marcello Bonsangue, Alfons Laarman. (2024)<br><strong>Simulating Quantum Circuits by Model Counting</strong><br><button class=copy-to-clipboard title="Simulating Quantum Circuits by Model Counting" index=228>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-228 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-LO, quant-ph, quant-ph<br>Keyword Score: 30<br>Keywords: Simulation, Simulator, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07197v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07197v1.pdf filename=2403.07197v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Quantum circuit compilation comprises many computationally hard <b>reasoning</b> tasks that nonetheless lie inside #$\mathbf{P}$ and its decision counterpart in $\mathbf{PP}$. The classical <b>simulation</b> of general quantum circuits is a core example. We show for the first time that a strong <b>simulation</b> of universal quantum circuits can be efficiently tackled through weighted model counting by providing a linear encoding of Clifford+T circuits. To achieve this, we exploit the stabilizer formalism by Knill, Gottesmann, and Aaronson and the fact that stabilizer states form a basis for density operators. With an open-source simulator implementation, we demonstrate empirically that model counting often outperforms state-of-the-art <b>simulation</b> techniques based on the ZX calculus and decision diagrams. Our work paves the way to apply the existing array of powerful classical <b>reasoning</b> tools to realize efficient quantum circuit compilation; one of the obstacles on the road towards quantum supremacy.</p></p class="citation"></blockquote><h3 id=34--229318-better-than-classical-the-subtle-art-of-benchmarking-quantum-machine-learning-models-joseph-bowles-et-al-2024>(3/4 | 229/318) Better than classical? The subtle art of benchmarking quantum machine learning models (Joseph Bowles et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Joseph Bowles, Shahnawaz Ahmed, Maria Schuld. (2024)<br><strong>Better than classical? The subtle art of benchmarking quantum machine learning models</strong><br><button class=copy-to-clipboard title="Better than classical? The subtle art of benchmarking quantum machine learning models" index=229>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-229 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-LG, quant-ph, quant-ph<br>Keyword Score: 26<br>Keywords: Benchmarking, Benchmarking, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07059v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07059v2.pdf filename=2403.07059v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Benchmarking</b> models via classical <b>simulations</b> is one of the main ways to judge ideas in quantum machine learning before noise-free hardware is available. However, the huge impact of the experimental design on the results, the small scales within reach today, as well as narratives influenced by the commercialisation of quantum technologies make it difficult to gain robust insights. To facilitate better decision-making we develop an open-source package based on the PennyLane software framework and use it to conduct a large-scale study that systematically tests 12 popular quantum machine learning models on 6 binary classification tasks used to create 160 individual datasets. We find that overall, out-of-the-box classical machine learning models outperform the quantum classifiers. Moreover, removing entanglement from a quantum model often results in as good or better performance, suggesting that &ldquo;quantumness&rdquo; may not be the crucial ingredient for the small learning tasks considered here. Our <b>benchmarks</b> also unlock investigations beyond simplistic leaderboard comparisons, and we identify five important questions for quantum model design that follow from our results.</p></p class="citation"></blockquote><h3 id=44--230318-solving-distributed-flexible-job-shop-scheduling-problems-in-the-wool-textile-industry-with-quantum-annealing-lilia-toma-et-al-2024>(4/4 | 230/318) Solving Distributed Flexible Job Shop Scheduling Problems in the Wool Textile Industry with Quantum Annealing (Lilia Toma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lilia Toma, Markus Zajac, Uta Störl. (2024)<br><strong>Solving Distributed Flexible Job Shop Scheduling Problems in the Wool Textile Industry with Quantum Annealing</strong><br><button class=copy-to-clipboard title="Solving Distributed Flexible Job Shop Scheduling Problems in the Wool Textile Industry with Quantum Annealing" index=230>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-230 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-ET, quant-ph, quant-ph<br>Keyword Score: 10<br>Keywords: Question Answering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06699v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06699v1.pdf filename=2403.06699v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Many modern manufacturing companies have evolved from a single production site to a multi-factory production environment that must handle both geographically dispersed production orders and their multi-site production steps. The availability of a range of machines in different locations capable of performing the same operation and shipping times between factories have transformed planning systems from the classic Job Shop Scheduling Problem (JSSP) to Distributed Flexible Job Shop Scheduling Problem (DFJSP). As a result, the complexity of production planning has increased significantly. In our work, we use Quantum Annealing <b>(QA)</b> to solve the DFJSP. In addition to the assignment of production orders to production sites, the assignment of production steps to production sites also takes place. This requirement is based on a real use case of a wool textile manufacturer. To investigate the applicability of this method to large problem instances, problems ranging from 50 variables up to 250 variables, the largest problem that could be embedded into a D-Wave quantum annealer Quantum Processing Unit (QPU), are formulated and solved. Special attention is dedicated to the determination of the Lagrange parameters of the Quadratic Unconstrained Binary Optimization (QUBO) model and the QPU configuration parameters, as these factors can significantly impact solution quality. The obtained solutions are compared to solutions obtained by Simulated Annealing (SA), both in terms of solution quality and calculation time. The results demonstrate that <b>QA</b> has the potential to solve large problem instances specific to the industry.</p></p class="citation"></blockquote><h2 id=csma-2>cs.MA (2)</h2><h3 id=12--231318-generalising-multi-agent-cooperation-through-task-agnostic-communication-dulhan-jayalath-et-al-2024>(1/2 | 231/318) Generalising Multi-Agent Cooperation through Task-Agnostic Communication (Dulhan Jayalath et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dulhan Jayalath, Steven Morad, Amanda Prorok. (2024)<br><strong>Generalising Multi-Agent Cooperation through Task-Agnostic Communication</strong><br><button class=copy-to-clipboard title="Generalising Multi-Agent Cooperation through Task-Agnostic Communication" index=231>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-231 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MA<br>Categories: cs-LG, cs-MA, cs-RO, cs.MA<br>Keyword Score: 50<br>Keywords: Autoencoder, Fine-tuning, Out-of-distribution, Reinforcement Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06750v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06750v1.pdf filename=2403.06750v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing communication methods for multi-agent <b>reinforcement</b> <b>learning</b> (MARL) in cooperative multi-robot problems are almost exclusively task-specific, training new communication strategies for each unique task. We address this inefficiency by introducing a communication strategy applicable to any task within a given environment. We pre-train the communication strategy without task-specific reward guidance in a <b>self-supervised</b> manner using a set <b>autoencoder.</b> Our objective is to learn a fixed-size latent Markov state from a variable number of agent observations. Under mild assumptions, we prove that policies using our latent representations are guaranteed to converge, and upper bound the value error introduced by our Markov state approximation. Our method enables seamless adaptation to novel tasks without <b>fine-tuning</b> the communication strategy, gracefully supports scaling to more agents than present during training, and detects <b>out-of-distribution</b> events in an environment. Empirical results on diverse MARL scenarios validate the effectiveness of our approach, surpassing task-specific communication strategies in unseen tasks. Our implementation of this work is available at <a href=https://github.com/proroklab/task-agnostic-comms>https://github.com/proroklab/task-agnostic-comms</a>.</p></p class="citation"></blockquote><h3 id=22--232318-the-geometry-of-cyclical-social-trends-bernard-chazelle-et-al-2024>(2/2 | 232/318) The Geometry of Cyclical Social Trends (Bernard Chazelle et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bernard Chazelle, Kritkorn Karntikoon, Jakob Nogler. (2024)<br><strong>The Geometry of Cyclical Social Trends</strong><br><button class=copy-to-clipboard title="The Geometry of Cyclical Social Trends" index=232>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-232 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MA<br>Categories: cs-MA, cs.MA<br>Keyword Score: 15<br>Keywords: Convolution, Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06376v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06376v1.pdf filename=2403.06376v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We investigate the emergence of periodic behavior in opinion dynamics and its underlying <b>geometry.</b> For this, we use a bounded-confidence model with contrarian agents in a <b>convolution</b> social network. This means that agents adapt their opinions by interacting with their neighbors in a time-varying social network. Being contrarian, the agents are kept from reaching consensus. This is the key feature that allows the emergence of cyclical trends. We show that the systems either converge to nonconsensual equilibrium or are attracted to periodic or quasi-periodic orbits. We bound the dimension of the attractors and the period of cyclical trends. We exhibit instances where each orbit is dense and uniformly distributed within its attractor. We also investigate the case of randomly changing social networks.</p></p class="citation"></blockquote><h2 id=csai-4>cs.AI (4)</h2><h3 id=14--233318-real-time-multimodal-cognitive-assistant-for-emergency-medical-services-keshara-weerasinghe-et-al-2024>(1/4 | 233/318) Real-Time Multimodal Cognitive Assistant for Emergency Medical Services (Keshara Weerasinghe et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Keshara Weerasinghe, Saahith Janapati, Xueren Ge, Sion Kim, Sneha Iyer, John A. Stankovic, Homa Alemzadeh. (2024)<br><strong>Real-Time Multimodal Cognitive Assistant for Emergency Medical Services</strong><br><button class=copy-to-clipboard title="Real-Time Multimodal Cognitive Assistant for Emergency Medical Services" index=233>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-233 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs-CV, cs.AI<br>Keyword Score: 49<br>Keywords: Graph, Fine-tuning, Multi-modal, Multi-modal, Automatic Speech Recognition, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06734v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06734v1.pdf filename=2403.06734v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Emergency Medical Services (EMS) responders often operate under time-sensitive conditions, facing cognitive overload and inherent risks, requiring essential skills in critical thinking and rapid decision-making. This paper presents CognitiveEMS, an end-to-end wearable cognitive assistant system that can act as a collaborative virtual partner engaging in the real-time acquisition and analysis of <b>multimodal</b> data from an emergency scene and interacting with EMS responders through Augmented Reality (AR) smart glasses. CognitiveEMS processes the continuous streams of data in real-time and leverages edge computing to provide assistance in EMS protocol selection and intervention recognition. We address key technical challenges in real-time cognitive assistance by introducing three novel components: (i) a <b>Speech</b> <b>Recognition</b> model that is <b>fine-tuned</b> for real-world medical emergency conversations using simulated EMS audio recordings, augmented with synthetic data generated by <b>large</b> <b>language</b> <b>models</b> <b>(LLMs);</b> (ii) an EMS Protocol Prediction model that combines state-of-the-art (SOTA) tiny language models with EMS domain knowledge using <b>graph-based</b> attention mechanisms; (iii) an EMS Action Recognition module which leverages <b>multimodal</b> audio and video data and protocol predictions to infer the intervention/treatment actions taken by the responders at the incident scene. Our results show that for <b>speech</b> <b>recognition</b> we achieve superior performance compared to SOTA (WER of 0.290 vs. 0.618) on conversational data. Our protocol prediction component also significantly outperforms SOTA (top-3 accuracy of 0.800 vs. 0.200) and the action recognition achieves an accuracy of 0.727, while maintaining an end-to-end latency of 3.78s for protocol prediction on the edge and 0.31s on the server.</p></p class="citation"></blockquote><h3 id=24--234318-bigraph-matching-weighted-with-learnt-incentive-function-for-multi-robot-task-allocation-steve-paul-et-al-2024>(2/4 | 234/318) Bigraph Matching Weighted with Learnt Incentive Function for Multi-Robot Task Allocation (Steve Paul et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Steve Paul, Nathan Maurer, Souma Chowdhury. (2024)<br><strong>Bigraph Matching Weighted with Learnt Incentive Function for Multi-Robot Task Allocation</strong><br><button class=copy-to-clipboard title="Bigraph Matching Weighted with Learnt Incentive Function for Multi-Robot Task Allocation" index=234>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-234 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-MA, cs.AI<br>Keyword Score: 13<br>Keywords: Graph, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07131v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07131v1.pdf filename=2403.07131v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Most real-world Multi-Robot Task Allocation (MRTA) problems require fast and efficient decision-making, which is often achieved using heuristics-aided methods such as genetic algorithms, auction-based methods, and bipartite <b>graph</b> matching methods. These methods often assume a form that lends better explainability compared to an end-to-end (learnt) neural network based policy for MRTA. However, deriving suitable heuristics can be tedious, risky and in some cases impractical if problems are too complex. This raises the question: can these heuristics be learned? To this end, this paper particularly develops a <b>Graph</b> <b>Reinforcement</b> <b>Learning</b> (GRL) framework to learn the heuristics or incentives for a bipartite <b>graph</b> matching approach to MRTA. Specifically a Capsule Attention policy model is used to learn how to weight task/robot pairings (edges) in the bipartite <b>graph</b> that connects the set of tasks to the set of robots. The original capsule attention network architecture is fundamentally modified by adding encoding of robots&rsquo; state <b>graph,</b> and two Multihead Attention based decoders whose output are used to construct a LogNormal distribution matrix from which positive bigraph weights can be drawn. The performance of this new bigraph matching approach augmented with a GRL-derived incentive is found to be at par with the original bigraph matching approach that used expert-specified heuristics, with the former offering notable robustness benefits. During training, the learned incentive policy is found to get initially closer to the expert-specified incentive and then slightly deviate from its trend.</p></p class="citation"></blockquote><h3 id=34--235318-a-hybrid-intelligence-method-for-argument-mining-michiel-van-der-meer-et-al-2024>(3/4 | 235/318) A Hybrid Intelligence Method for Argument Mining (Michiel van der Meer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michiel van der Meer, Enrico Liscio, Catholijn M. Jonker, Aske Plaat, Piek Vossen, Pradeep K. Murukannaiah. (2024)<br><strong>A Hybrid Intelligence Method for Argument Mining</strong><br><button class=copy-to-clipboard title="A Hybrid Intelligence Method for Argument Mining" index=235>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-235 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs-HC, cs.AI<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09713v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09713v1.pdf filename=2403.09713v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Large-scale survey tools enable the collection of citizen feedback in opinion corpora. Extracting the key arguments from a large and noisy set of opinions helps in understanding the opinions quickly and accurately. Fully automated methods can extract arguments but (1) require large labeled datasets that induce large annotation costs and (2) work well for known viewpoints, but not for novel points of view. We propose HyEnA, a hybrid (human + AI) method for extracting arguments from opinionated texts, combining the speed of automated processing with the understanding and <b>reasoning</b> capabilities of humans. We evaluate HyEnA on three citizen feedback corpora. We find that, on the one hand, HyEnA achieves higher coverage and precision than a state-of-the-art automated method when compared to a common set of diverse opinions, justifying the need for human insight. On the other hand, HyEnA requires less human effort and does not compromise quality compared to (fully manual) expert analysis, demonstrating the benefit of combining human and artificial intelligence.</p></p class="citation"></blockquote><h3 id=44--236318-better-understandings-and-configurations-in-maxsat-local-search-solvers-via-anytime-performance-analysis-furong-ye-et-al-2024>(4/4 | 236/318) Better Understandings and Configurations in MaxSAT Local Search Solvers via Anytime Performance Analysis (Furong Ye et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Furong Ye, Chuan Luo, Shaowei Cai. (2024)<br><strong>Better Understandings and Configurations in MaxSAT Local Search Solvers via Anytime Performance Analysis</strong><br><button class=copy-to-clipboard title="Better Understandings and Configurations in MaxSAT Local Search Solvers via Anytime Performance Analysis" index=236>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-236 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06568v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06568v1.pdf filename=2403.06568v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Though numerous solvers have been proposed for the MaxSAT problem, and the <b>benchmark</b> environment such as MaxSAT Evaluations provides a platform for the comparison of the state-of-the-art solvers, existing assessments were usually evaluated based on the quality, e.g., fitness, of the best-found solutions obtained within a given running time budget. However, concerning solely the final obtained solutions regarding specific time budgets may restrict us from comprehending the behavior of the solvers along the convergence process. This paper demonstrates that Empirical Cumulative Distribution Functions can be used to compare MaxSAT local search solvers&rsquo; anytime performance across multiple problem instances and various time budgets. The assessment reveals distinctions in solvers&rsquo; performance and displays that the (dis)advantages of solvers adjust along different running times. This work also exhibits that the quantitative and high variance assessment of anytime performance can guide machines, i.e., automatic configurators, to search for better parameter settings. Our experimental results show that the hyperparameter optimization tool, i.e., SMAC, generally achieves better parameter settings of local search when using the anytime performance as the cost function, compared to using the fitness of the best-found solutions.</p></p class="citation"></blockquote><h2 id=csse-4>cs.SE (4)</h2><h3 id=14--237318-acfix-guiding-llms-with-mined-common-rbac-practices-for-context-aware-repair-of-access-control-vulnerabilities-in-smart-contracts-lyuye-zhang-et-al-2024>(1/4 | 237/318) ACFIX: Guiding LLMs with Mined Common RBAC Practices for Context-Aware Repair of Access Control Vulnerabilities in Smart Contracts (Lyuye Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lyuye Zhang, Kaixuan Li, Kairan Sun, Daoyuan Wu, Ye Liu, Haoye Tian, Yang Liu. (2024)<br><strong>ACFIX: Guiding LLMs with Mined Common RBAC Practices for Context-Aware Repair of Access Control Vulnerabilities in Smart Contracts</strong><br><button class=copy-to-clipboard title="ACFIX: Guiding LLMs with Mined Common RBAC Practices for Context-Aware Repair of Access Control Vulnerabilities in Smart Contracts" index=237>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-237 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-CR, cs-SE, cs.SE<br>Keyword Score: 43<br>Keywords: Benchmarking, GPT, GPT-4, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06838v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06838v1.pdf filename=2403.06838v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Smart contracts are susceptible to various security issues, among which access control (AC) vulnerabilities are particularly critical. While existing research has proposed multiple detection tools, the automatic and appropriate repair of AC vulnerabilities in smart contracts remains a challenge. Unlike commonly supported vulnerability types by existing repair tools, such as reentrancy, which are usually fixed by template-based approaches, the main obstacle of AC lies in identifying the appropriate roles or permissions amid a long list of non-AC-related source code to generate proper patch code, a task that demands human-level intelligence. Leveraging recent advancements in <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> we employ the state-of-the-art <b>GPT-4</b> model and enhance it with a novel approach called ACFIX. The key insight is that we can mine common AC practices for major categories of code functionality and use them to guide <b>LLMs</b> in fixing code with similar functionality. To this end, ACFIX involves both offline and online phases. First, during the offline phase, ACFIX mines a taxonomy of common Role-based Access Control (RBAC) practices from 344,251 on-chain contracts, categorizing 49 role-permission pairs from the top 1,000 pairs mined. Second, during the online phase, ACFIX tracks AC-related elements across the contract and uses this context information along with a Chain-of-Thought pipeline to guide <b>LLMs</b> in identifying the most appropriate role-permission pair for the subject contract and subsequently generating a suitable patch. This patch will then undergo a validity and effectiveness check. To evaluate ACFIX, we built the first <b>benchmark</b> dataset of 118 real-world AC vulnerabilities, and our evaluation revealed that ACFIX successfully repaired 94.92% of them. This represents a significant improvement compared to the baseline <b>GPT-4,</b> which achieved only 52.54%.</p></p class="citation"></blockquote><h3 id=24--238318-textual-analysis-of-end-user-license-agreement-for-red-flagging-potentially-malicious-software-behraj-khan-et-al-2024>(2/4 | 238/318) Textual analysis of End User License Agreement for red-flagging potentially malicious software (Behraj Khan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Behraj Khan, Tahir Syed, Zeshan Khan, Muhammad Rafi. (2024)<br><strong>Textual analysis of End User License Agreement for red-flagging potentially malicious software</strong><br><button class=copy-to-clipboard title="Textual analysis of End User License Agreement for red-flagging potentially malicious software" index=238>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-238 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-CL, cs-CR, cs-LG, cs-SE, cs.SE<br>Keyword Score: 40<br>Keywords: Supervised Learning, Text Summarization, Summarization, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09715v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09715v1.pdf filename=2403.09715v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>New software and updates are downloaded by end users every day. Each dowloaded software has associated with it an End Users License Agreements (EULA), but this is rarely read. An EULA includes information to avoid legal repercussions. However,this proposes a host of potential problems such as spyware or producing an unwanted affect in the target system. End users do not read these EULA&rsquo;s because of length of the document and users find it extremely difficult to understand. <b>Text</b> <b>summarization</b> is one of the relevant solution to these kind of problems. This require a solution which can <b>summarize</b> the EULA and classify the EULA as &ldquo;Benign&rdquo; or &ldquo;Malicious&rdquo;. We propose a solution in which we have <b>summarize</b> the EULA and classify the EULA as &ldquo;Benign&rdquo; or &ldquo;Malicious&rdquo;. We extract EULA <b>text</b> <b>of</b> different sofware&rsquo;s then we classify the <b>text</b> <b>using</b> eight different <b>supervised</b> classifiers. we use ensemble learning to classify the EULA as benign or malicious using five different <b>text</b> <b>summarization</b> methods. An accuracy of $95.8$% shows the effectiveness of the presented approach.</p></p class="citation"></blockquote><h3 id=34--239318-knowledge-aware-alert-aggregation-in-large-scale-cloud-systems-a-hybrid-approach-jinxi-kuang-et-al-2024>(3/4 | 239/318) Knowledge-aware Alert Aggregation in Large-scale Cloud Systems: a Hybrid Approach (Jinxi Kuang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinxi Kuang, Jinyang Liu, Junjie Huang, Renyi Zhong, Jiazhen Gu, Lan Yu, Rui Tan, Zengyin Yang, Michael R. Lyu. (2024)<br><strong>Knowledge-aware Alert Aggregation in Large-scale Cloud Systems: a Hybrid Approach</strong><br><button class=copy-to-clipboard title="Knowledge-aware Alert Aggregation in Large-scale Cloud Systems: a Hybrid Approach" index=239>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-239 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-CL, cs-LG, cs-SE, cs.SE<br>Keyword Score: 30<br>Keywords: Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06485v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06485v1.pdf filename=2403.06485v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Due to the scale and complexity of cloud systems, a system failure would trigger an &ldquo;alert storm&rdquo;, i.e., massive correlated alerts. Although these alerts can be traced back to a few root causes, the overwhelming number makes it infeasible for manual handling. Alert aggregation is thus critical to help engineers concentrate on the root cause and facilitate failure resolution. Existing methods typically utilize semantic similarity-based methods or statistical methods to aggregate alerts. However, semantic similarity-based methods overlook the causal rationale of alerts, while statistical methods can hardly handle infrequent alerts. To tackle these limitations, we introduce leveraging external knowledge, i.e., Standard Operation Procedure (SOP) of alerts as a supplement. We propose COLA, a novel hybrid approach based on correlation mining and <b>LLM</b> <b>(Large</b> <b>Language</b> <b>Model)</b> <b>reasoning</b> for online alert aggregation. The correlation mining module effectively captures the temporal and spatial relations between alerts, measuring their correlations in an efficient manner. Subsequently, only uncertain pairs with low confidence are forwarded to the <b>LLM</b> <b>reasoning</b> module for detailed analysis. This hybrid design harnesses both statistical evidence for frequent alerts and the <b>reasoning</b> capabilities of computationally intensive <b>LLMs,</b> ensuring the overall efficiency of COLA in handling <b>large</b> <b>volumes</b> <b>of</b> alerts in practical scenarios. We evaluate COLA on three datasets collected from the production environment of a <b>large-scale</b> <b>cloud</b> <b>platform.</b> The experimental results show COLA achieves F1-scores from 0.901 to 0.930, outperforming state-of-the-art methods and achieving comparable efficiency. We also share our experience in deploying COLA in our real-world cloud system, Cloud X.</p></p class="citation"></blockquote><h3 id=44--240318-technical-debt-management-the-road-ahead-for-successful-software-delivery-paris-avgeriou-et-al-2024>(4/4 | 240/318) Technical Debt Management: The Road Ahead for Successful Software Delivery (Paris Avgeriou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Paris Avgeriou, Ipek Ozkaya, Alexander Chatzigeorgiou, Marcus Ciolkowski, Neil A. Ernst, Ronald J. Koontz, Eltjo Poort, Forrest Shull. (2024)<br><strong>Technical Debt Management: The Road Ahead for Successful Software Delivery</strong><br><button class=copy-to-clipboard title="Technical Debt Management: The Road Ahead for Successful Software Delivery" index=240>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-240 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: D-2-11; D-2-10; D-2-7; D-2-5, cs-SE, cs.SE<br>Keyword Score: 10<br>Keywords: Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06484v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06484v1.pdf filename=2403.06484v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Technical Debt, considered by many to be the &lsquo;silent killer&rsquo; of software projects, has undeniably become part of the everyday vocabulary of software engineers. We know it compromises the internal quality of a system, either deliberately or inadvertently. We understand Technical Debt is not all derogatory, often serving the purpose of expediency. But, it is associated with a clear risk, especially for large and complex systems with extended service life: if we do not properly manage Technical Debt, it threatens to &ldquo;bankrupt&rdquo; those systems. Software engineers and organizations that develop software-intensive systems are facing an increasingly more dire future state of those systems if they do not start incorporating Technical Debt management into their day to day practice. But how? What have the wins and losses of the past decade of research and practice in managing Technical Debt taught us and where should we focus next? In this paper, we examine the state of the art in both industry and research communities in managing Technical Debt; we subsequently <b>distill</b> the gaps in industrial practice and the research shortcomings, and synthesize them to define and articulate a vision for what Technical Debt management looks like five years hence.</p></p class="citation"></blockquote><h2 id=statme-1>stat.ME (1)</h2><h3 id=11--241318-stochastic-gradient-descent-based-inference-for-dynamic-network-models-with-attractors-hancong-pan-et-al-2024>(1/1 | 241/318) Stochastic gradient descent-based inference for dynamic network models with attractors (Hancong Pan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hancong Pan, Xiaojing Zhu, Cantay Caliskan, Dino P. Christenson, Konstantinos Spiliopoulos, Dylan Walker, Eric D. Kolaczyk. (2024)<br><strong>Stochastic gradient descent-based inference for dynamic network models with attractors</strong><br><button class=copy-to-clipboard title="Stochastic gradient descent-based inference for dynamic network models with attractors" index=241>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-241 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ME<br>Categories: cs-SI, stat-ME, stat.ME<br>Keyword Score: 40<br>Keywords: Simulation, Simulator, Stochastic Gradient Descent, Stochastic Gradient Descent<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07124v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07124v1.pdf filename=2403.07124v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In Coevolving Latent Space Networks with Attractors (CLSNA) models, nodes in a latent space represent social actors, and edges indicate their dynamic interactions. Attractors are added at the latent level to capture the notion of attractive and repulsive forces between nodes, borrowing from dynamical systems theory. However, CLSNA reliance on MCMC estimation makes scaling difficult, and the requirement for nodes to be present throughout the study period limit practical applications. We address these issues by (i) introducing a <b>Stochastic</b> <b>gradient</b> <b>descent</b> <b>(SGD)</b> parameter estimation method, (ii) developing a novel approach for uncertainty quantification using <b>SGD,</b> and (iii) extending the model to allow nodes to join and leave over time. <b>Simulation</b> results show that our extensions result in little loss of accuracy compared to MCMC, but can scale to much larger networks. We apply our approach to the longitudinal social networks of members of US Congress on the social media platform X. Accounting for node dynamics overcomes selection bias in the network and uncovers uniquely and increasingly repulsive forces within the Republican Party.</p></p class="citation"></blockquote><h2 id=eessiv-11>eess.IV (11)</h2><h3 id=111--242318-dynamic-perturbation-adaptive-adversarial-training-on-medical-image-classification-shuai-li-et-al-2024>(1/11 | 242/318) Dynamic Perturbation-Adaptive Adversarial Training on Medical Image Classification (Shuai Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shuai Li, Xiaoguang Ma, Shancheng Jiang, Lu Meng. (2024)<br><strong>Dynamic Perturbation-Adaptive Adversarial Training on Medical Image Classification</strong><br><button class=copy-to-clipboard title="Dynamic Perturbation-Adaptive Adversarial Training on Medical Image Classification" index=242>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-242 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 40<br>Keywords: Adversarial Learning, Convolution, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06798v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06798v1.pdf filename=2403.06798v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Remarkable successes were made in Medical Image Classification (MIC) recently, mainly due to wide applications of <b>convolutional</b> <b>neural</b> <b>networks</b> <b>(CNNs).</b> However, <b>adversarial</b> <b>examples</b> (AEs) exhibited imperceptible similarity with raw data, raising serious concerns on network robustness. Although <b>adversarial</b> <b>training</b> (AT), in responding to malevolent AEs, was recognized as an effective approach to improve robustness, it was challenging to overcome generalization decline of networks caused by the AT. In this paper, in order to reserve high generalization while improving robustness, we proposed a dynamic perturbation-adaptive <b>adversarial</b> <b>training</b> (DPAAT) method, which placed AT in a dynamic learning environment to generate adaptive data-level perturbations and provided a dynamically updated criterion by loss information collections to handle the disadvantage of fixed perturbation sizes in conventional AT methods and the dependence on external transference. Comprehensive testing on dermatology HAM10000 dataset showed that the DPAAT not only achieved better robustness improvement and generalization preservation but also significantly enhanced mean average precision and interpretability on various <b>CNNs,</b> indicating its great potential as a generic <b>adversarial</b> <b>training</b> method on the MIC.</p></p class="citation"></blockquote><h3 id=211--243318-a-segmentation-foundation-model-for-diverse-type-tumors-jianhao-xie-et-al-2024>(2/11 | 243/318) A Segmentation Foundation Model for Diverse-type Tumors (Jianhao Xie et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianhao Xie, Ziang Zhang, Guibo Luo, Yuesheng Zhu. (2024)<br><strong>A Segmentation Foundation Model for Diverse-type Tumors</strong><br><button class=copy-to-clipboard title="A Segmentation Foundation Model for Diverse-type Tumors" index=243>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-243 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: I-4-6, cs-CV, eess-IV, eess.IV<br>Keyword Score: 40<br>Keywords: Fine-tuning, Foundation Model, Transfer Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06396v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06396v1.pdf filename=2403.06396v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Large pre-trained models with their numerous model parameters and extensive training datasets have shown excellent performance in various tasks. Many publicly available medical image datasets do not have a sufficient amount of data so there are few large-scale models in medical imaging. We propose a large-scale Tumor Segmentation <b>Foundation</b> <b>Model</b> (TSFM) with 1.6 billion parameters using Resblock-backbone and <b>Transformer-bottleneck,which</b> has good <b>transfer</b> <b>ability</b> for downstream tasks. To make TSFM exhibit good performance in tumor segmentation, we make full use of the strong spatial correlation between tumors and organs in the medical image, innovatively fuse 7 tumor datasets and 3 multi-organ datasets to build a 3D medical dataset pool, including 2779 cases with totally 300k medical images, whose size currently exceeds many other single publicly available datasets. TSFM is the pre-trained model for medical image segmentation, which also can be transferred to multiple downstream tasks for <b>fine-tuning</b> learning. The average performance of our pre-trained model is 2% higher than that of nnU-Net across various tumor types. In the <b>transfer</b> <b>learning</b> task, TSFM only needs 5% training epochs of nnU-Net to achieve similar performance and can surpass nnU-Net by 2% on average with 10% training epoch. Pre-trained TSFM and its code will be released soon.</p></p class="citation"></blockquote><h3 id=311--244318-libr-improving-intraoperative-liver-registration-by-learning-the-residual-of-biomechanics-based-deformable-registration-dingrong-wang-et-al-2024>(3/11 | 244/318) LIBR+: Improving Intraoperative Liver Registration by Learning the Residual of Biomechanics-Based Deformable Registration (Dingrong Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dingrong Wang, Soheil Azadvar, Jon Heiselman, Xiajun Jiang, Michael Miga, Linwei Wang. (2024)<br><strong>LIBR+: Improving Intraoperative Liver Registration by Learning the Residual of Biomechanics-Based Deformable Registration</strong><br><button class=copy-to-clipboard title="LIBR+: Improving Intraoperative Liver Registration by Learning the Residual of Biomechanics-Based Deformable Registration" index=244>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-244 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-AI, cs-LG, eess-IV, eess.IV<br>Keyword Score: 28<br>Keywords: Graph, Convolution, Convolutional Neural Network, Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06901v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06901v1.pdf filename=2403.06901v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The surgical environment imposes unique challenges to the intraoperative registration of organ shapes to their preoperatively-imaged <b>geometry.</b> Biomechanical model-based registration remains popular, while deep learning solutions remain limited due to the sparsity and variability of intraoperative measurements and the limited ground-truth deformation of an organ that can be obtained during the surgery. In this paper, we propose a novel \textit{hybrid} registration approach that leverage a linearized iterative boundary reconstruction (LIBR) method based on linear elastic biomechanics, and use deep neural networks to learn its residual to the ground-truth deformation (LIBR+). We further formulate a dual-branch spline-residual <b>graph</b> <b>convolutional</b> <b>neural</b> <b>network</b> (SR-GCN) to assimilate information from sparse and variable intraoperative measurements and effectively propagate it through the <b>geometry</b> of the 3D organ. Experiments on a large intraoperative liver registration dataset demonstrated the consistent improvements achieved by LIBR+ in comparison to existing rigid, biomechnical model-based non-rigid, and deep-learning based non-rigid approaches to intraoperative liver registration.</p></p class="citation"></blockquote><h3 id=411--245318-simulation-based-segmentation-of-blood-vessels-in-cerebral-3d-octa-images-bastian-wittmann-et-al-2024>(4/11 | 245/318) Simulation-Based Segmentation of Blood Vessels in Cerebral 3D OCTA Images (Bastian Wittmann et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bastian Wittmann, Lukas Glandorf, Johannes C. Paetzold, Tamaz Amiranashvili, Thomas Wälchli, Daniel Razansky, Bjoern Menze. (2024)<br><strong>Simulation-Based Segmentation of Blood Vessels in Cerebral 3D OCTA Images</strong><br><button class=copy-to-clipboard title="Simulation-Based Segmentation of Blood Vessels in Cerebral 3D OCTA Images" index=245>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-245 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 23<br>Keywords: Graph, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07116v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07116v1.pdf filename=2403.07116v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Segmentation of blood vessels in murine cerebral 3D OCTA images is foundational for in vivo quantitative analysis of the effects of neurovascular disorders, such as stroke or Alzheimer&rsquo;s, on the vascular network. However, to accurately segment blood vessels with state-of-the-art deep learning methods, a vast amount of voxel-level annotations is required. Since cerebral 3D OCTA images are typically plagued by artifacts and generally have a low signal-to-noise ratio, acquiring manual annotations poses an especially cumbersome and time-consuming task. To alleviate the need for manual annotations, we propose utilizing synthetic data to supervise segmentation algorithms. To this end, we extract patches from vessel <b>graphs</b> and transform them into synthetic cerebral 3D OCTA images paired with their matching ground truth labels by simulating the most dominant 3D OCTA artifacts. In extensive experiments, we demonstrate that our approach achieves competitive results, enabling annotation-free blood vessel segmentation in cerebral 3D OCTA images.</p></p class="citation"></blockquote><h3 id=511--246318-ct2rep-automated-radiology-report-generation-for-3d-medical-imaging-ibrahim-ethem-hamamci-et-al-2024>(5/11 | 246/318) CT2Rep: Automated Radiology Report Generation for 3D Medical Imaging (Ibrahim Ethem Hamamci et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ibrahim Ethem Hamamci, Sezgin Er, Bjoern Menze. (2024)<br><strong>CT2Rep: Automated Radiology Report Generation for 3D Medical Imaging</strong><br><button class=copy-to-clipboard title="CT2Rep: Automated Radiology Report Generation for 3D Medical Imaging" index=246>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-246 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 16<br>Keywords: Multi-modal, Multi-modal, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06801v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06801v1.pdf filename=2403.06801v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Medical imaging plays a crucial role in diagnosis, with radiology reports serving as vital documentation. Automating report generation has emerged as a critical need to alleviate the workload of radiologists. While machine learning has facilitated report generation for 2D medical imaging, extending this to 3D has been unexplored due to computational complexity and data scarcity. We introduce the first method to generate radiology reports for 3D medical imaging, specifically targeting chest CT volumes. Given the absence of comparable methods, we establish a baseline using an advanced 3D vision encoder in medical imaging to demonstrate our method&rsquo;s effectiveness, which leverages a novel auto-regressive causal <b>transformer.</b> Furthermore, recognizing the benefits of leveraging information from previous visits, we augment CT2Rep with a cross-attention-based <b>multi-modal</b> fusion module and hierarchical memory, enabling the incorporation of longitudinal <b>multimodal</b> data. Access our code at: <a href=https://github.com/ibrahimethemhamamci/CT2Rep>https://github.com/ibrahimethemhamamci/CT2Rep</a></p></p class="citation"></blockquote><h3 id=611--247318-conditional-score-based-diffusion-model-for-cortical-thickness-trajectory-prediction-qing-xiao-et-al-2024>(6/11 | 247/318) Conditional Score-Based Diffusion Model for Cortical Thickness Trajectory Prediction (Qing Xiao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qing Xiao, Siyeop Yoon, Hui Ren, Matthew Tivnan, Lichao Sun, Quanzheng Li, Tianming Liu, Yu Zhang, Xiang Li. (2024)<br><strong>Conditional Score-Based Diffusion Model for Cortical Thickness Trajectory Prediction</strong><br><button class=copy-to-clipboard title="Conditional Score-Based Diffusion Model for Cortical Thickness Trajectory Prediction" index=247>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-247 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-LG, eess-IV, eess.IV, q-bio-QM<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06940v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06940v1.pdf filename=2403.06940v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Alzheimer&rsquo;s Disease (AD) is a neurodegenerative condition characterized by diverse progression rates among individuals, with changes in cortical thickness (CTh) closely linked to its progression. Accurately forecasting CTh trajectories can significantly enhance early diagnosis and intervention strategies, providing timely care. However, the longitudinal data essential for these studies often suffer from temporal sparsity and incompleteness, presenting substantial challenges in modeling the disease&rsquo;s progression accurately. Existing methods are limited, focusing primarily on datasets without missing entries or requiring predefined assumptions about CTh progression. To overcome these obstacles, we propose a conditional score-based <b>diffusion</b> <b>model</b> specifically designed to generate CTh trajectories with the given baseline information, such as age, sex, and initial diagnosis. Our conditional <b>diffusion</b> <b>model</b> utilizes all available data during the training phase to make predictions based solely on baseline information during inference without needing prior history about CTh progression. The prediction accuracy of the proposed CTh prediction pipeline using a conditional score-based model was compared for sub-groups consisting of cognitively normal, mild cognitive impairment, and AD subjects. The Bland-Altman analysis shows our <b>diffusion-based</b> <b>prediction</b> model has a near-zero bias with narrow 95% confidential interval compared to the ground-truth CTh in 6-36 months. In addition, our conditional <b>diffusion</b> <b>model</b> has a stochastic generative nature, therefore, we demonstrated an uncertainty analysis of patient-specific CTh prediction through multiple realizations.</p></p class="citation"></blockquote><h3 id=711--248318-shortcut-learning-in-medical-image-segmentation-manxi-lin-et-al-2024>(7/11 | 248/318) Shortcut Learning in Medical Image Segmentation (Manxi Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Manxi Lin, Nina Weng, Kamil Mikolaj, Zahra Bashir, Morten Bo Søndergaard Svendsen, Martin Tolsgaard, Anders Nymark Christensen, Aasa Feragen. (2024)<br><strong>Shortcut Learning in Medical Image Segmentation</strong><br><button class=copy-to-clipboard title="Shortcut Learning in Medical Image Segmentation" index=248>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-248 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06748v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06748v1.pdf filename=2403.06748v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Shortcut learning is a phenomenon where machine learning models prioritize learning simple, potentially misleading cues from data that do not generalize well beyond the training set. While existing research primarily investigates this in the realm of image classification, this study extends the exploration of shortcut learning into medical image segmentation. We demonstrate that clinical annotations such as calipers, and the combination of zero-padded <b>convolutions</b> and center-cropped training sets in the dataset can inadvertently serve as shortcuts, impacting segmentation accuracy. We identify and evaluate the shortcut learning on two different but common medical image segmentation tasks. In addition, we suggest strategies to mitigate the influence of shortcut learning and improve the generalizability of the segmentation models. By uncovering the presence and implications of shortcuts in medical image segmentation, we provide insights and methodologies for evaluating and overcoming this pervasive challenge and call for attention in the community for shortcuts in segmentation.</p></p class="citation"></blockquote><h3 id=811--249318-restaingan-leveraging-ihc-to-if-stain-domain-translation-for-in-silico-data-generation-dominik-winter-et-al-2024>(8/11 | 249/318) ReStainGAN: Leveraging IHC to IF Stain Domain Translation for in-silico Data Generation (Dominik Winter et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dominik Winter, Nicolas Triltsch, Philipp Plewa, Marco Rosati, Thomas Padel, Ross Hill, Markus Schick, Nicolas Brieu. (2024)<br><strong>ReStainGAN: Leveraging IHC to IF Stain Domain Translation for in-silico Data Generation</strong><br><button class=copy-to-clipboard title="ReStainGAN: Leveraging IHC to IF Stain Domain Translation for in-silico Data Generation" index=249>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-249 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: I-2-10, J-3, I-4-6, cs-AI, cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06545v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06545v1.pdf filename=2403.06545v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The creation of in-silico datasets can expand the utility of existing annotations to new domains with different staining patterns in computational pathology. As such, it has the potential to significantly lower the cost associated with building large and pixel precise datasets needed to train <b>supervised</b> deep learning models. We propose a novel approach for the generation of in-silico immunohistochemistry (IHC) images by disentangling morphology specific IHC stains into separate image channels in immunofluorescence (IF) images. The proposed approach qualitatively and quantitatively outperforms baseline methods as proven by training nucleus segmentation models on the created in-silico datasets.</p></p class="citation"></blockquote><h3 id=911--250318-incorporating-improved-sinusoidal-threshold-based-semi-supervised-method-and-diffusion-models-for-osteoporosis-diagnosis-wenchi-ke-2024>(9/11 | 250/318) Incorporating Improved Sinusoidal Threshold-based Semi-supervised Method and Diffusion Models for Osteoporosis Diagnosis (Wenchi Ke, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenchi Ke. (2024)<br><strong>Incorporating Improved Sinusoidal Threshold-based Semi-supervised Method and Diffusion Models for Osteoporosis Diagnosis</strong><br><button class=copy-to-clipboard title="Incorporating Improved Sinusoidal Threshold-based Semi-supervised Method and Diffusion Models for Osteoporosis Diagnosis" index=250>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-250 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06498v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06498v1.pdf filename=2403.06498v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Osteoporosis is a common skeletal disease that seriously affects patients&rsquo; quality of life. Traditional osteoporosis diagnosis methods are expensive and complex. The semi-supervised model based on <b>diffusion</b> <b>model</b> and class threshold sinusoidal decay proposed in this paper can automatically diagnose osteoporosis based on patient&rsquo;s imaging data, which has the advantages of convenience, accuracy, and low cost. Unlike previous semi-supervised models, all the unlabeled data used in this paper are generated by the <b>diffusion</b> <b>model.</b> Compared with real unlabeled data, synthetic data generated by the <b>diffusion</b> <b>model</b> show better performance. In addition, this paper proposes a novel pseudo-label threshold adjustment mechanism, Sinusoidal Threshold Decay, which can make the semi-supervised model converge more quickly and improve its performance. Specifically, the method is tested on a dataset including 749 dental panoramic images, and its achieved leading detect performance and produces a 80.10% accuracy.</p></p class="citation"></blockquote><h3 id=1011--251318-exploring-cluster-analysis-in-nelore-cattle-visual-score-attribution-alexandre-de-oliveira-bezerra-et-al-2024>(10/11 | 251/318) Exploring Cluster Analysis in Nelore Cattle Visual Score Attribution (Alexandre de Oliveira Bezerra et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alexandre de Oliveira Bezerra, Rodrigo Goncalves Mateus, Vanessa Ap. de Moraes Weber, Fabricio de Lima Weber, Yasmin Alves de Arruda, Rodrigo da Costa Gomes, Gabriel Toshio Hirokawa Higa, Hemerson Pistori. (2024)<br><strong>Exploring Cluster Analysis in Nelore Cattle Visual Score Attribution</strong><br><button class=copy-to-clipboard title="Exploring Cluster Analysis in Nelore Cattle Visual Score Attribution" index=251>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-251 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 3<br>Keywords: Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07137v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07137v1.pdf filename=2403.07137v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Assessing the biotype of cattle through human visual inspection is a very common and important practice in precision cattle breeding. This paper presents the results of a correlation analysis between scores produced by humans for Nelore cattle and a variety of measurements that can be derived from images or other instruments. It also presents a study using the k-means algorithm to generate new ways of <b>clustering</b> a batch of cattle using the measurements that most correlate with the animal&rsquo;s body weight and visual scores.</p></p class="citation"></blockquote><h3 id=1111--252318-from-pixel-to-cancer-cellular-automata-in-computed-tomography-yuxiang-lai-et-al-2024>(11/11 | 252/318) From Pixel to Cancer: Cellular Automata in Computed Tomography (Yuxiang Lai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuxiang Lai, Xiaoxi Chen, Angtian Wang, Alan Yuille, Zongwei Zhou. (2024)<br><strong>From Pixel to Cancer: Cellular Automata in Computed Tomography</strong><br><button class=copy-to-clipboard title="From Pixel to Cancer: Cellular Automata in Computed Tomography" index=252>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-252 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06459v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06459v1.pdf filename=2403.06459v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>AI for cancer detection encounters the bottleneck of data scarcity, annotation difficulty, and low prevalence of early tumors. Tumor synthesis seeks to create artificial tumors in medical images, which can greatly diversify the data and annotations for AI training. However, current tumor synthesis approaches are not applicable across different organs due to their need for specific expertise and design. This paper establishes a set of generic rules to simulate tumor development. Each cell (pixel) is initially assigned a state between zero and ten to represent the tumor population, and a tumor can be developed based on three rules to describe the process of growth, invasion, and death. We apply these three generic rules to simulate tumor development&ndash;from pixel to cancer&ndash;using cellular automata. We then integrate the tumor state into the original computed tomography (CT) images to generate synthetic tumors across different organs. This tumor synthesis approach allows for sampling tumors at multiple stages and analyzing tumor-organ interaction. Clinically, a reader study involving three expert radiologists reveals that the synthetic tumors and their developing trajectories are convincingly realistic. Technically, we generate tumors at varied stages in 9,262 raw, unlabeled CT images sourced from 68 hospitals worldwide. The performance in segmenting tumors in the liver, pancreas, and kidneys exceeds prevailing literature <b>benchmarks,</b> underlining the immense potential of tumor synthesis, especially for earlier cancer detection. The code and models are available at <a href=https://github.com/MrGiovanni/Pixel2Cancer>https://github.com/MrGiovanni/Pixel2Cancer</a></p></p class="citation"></blockquote><h2 id=cscy-3>cs.CY (3)</h2><h3 id=13--253318-improving-low-resource-knowledge-tracing-tasks-by-supervised-pre-training-and-importance-mechanism-fine-tuning-hengyuan-zhang-et-al-2024>(1/3 | 253/318) Improving Low-Resource Knowledge Tracing Tasks by Supervised Pre-training and Importance Mechanism Fine-tuning (Hengyuan Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hengyuan Zhang, Zitao Liu, Shuyan Huang, Chenming Shang, Bojun Zhan, Yong Jiang. (2024)<br><strong>Improving Low-Resource Knowledge Tracing Tasks by Supervised Pre-training and Importance Mechanism Fine-tuning</strong><br><button class=copy-to-clipboard title="Improving Low-Resource Knowledge Tracing Tasks by Supervised Pre-training and Importance Mechanism Fine-tuning" index=253>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-253 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-AI, cs-CL, cs-CY, cs-LG, cs.CY<br>Keyword Score: 40<br>Keywords: Fine-tuning, Low-Resource, Supervised Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06725v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06725v1.pdf filename=2403.06725v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Knowledge tracing (KT) aims to estimate student&rsquo;s knowledge mastery based on their historical interactions. Recently, the deep learning based KT (DLKT) approaches have achieved impressive performance in the KT task. These DLKT models heavily rely on the large number of available student interactions. However, due to various reasons such as budget constraints and privacy concerns, observed interactions are very limited in many real-world scenarios, a.k.a, <b>low-resource</b> KT datasets. Directly training a DLKT model on a <b>low-resource</b> KT dataset may lead to overfitting and it is difficult to choose the appropriate deep neural architecture. Therefore, in this paper, we propose a <b>low-resource</b> KT framework called LoReKT to address above challenges. Inspired by the prevalent &ldquo;pre-training and <b>fine-tuning&rdquo;</b> paradigm, we aim to learn transferable parameters and representations from rich-resource KT datasets during the pre-training stage and subsequently facilitate effective adaptation to <b>low-resource</b> KT datasets. Specifically, we simplify existing sophisticated DLKT model architectures with purely a stack of <b>transformer</b> decoders. We design an encoding mechanism to incorporate student interactions from multiple KT data sources and develop an importance mechanism to prioritize updating parameters with high importance while constraining less important ones during the <b>fine-tuning</b> stage. We evaluate LoReKT on six public KT datasets and experimental results demonstrate the superiority of our approach in terms of AUC and Accuracy. To encourage reproducible research, we make our data and code publicly available at <a href=https://anonymous.4open.science/r/LoReKT-C619>https://anonymous.4open.science/r/LoReKT-C619</a>.</p></p class="citation"></blockquote><h3 id=23--254318-exploring-the-impact-of-chatgpt-on-student-interactions-in-computer-supported-collaborative-learning-han-kyul-kim-et-al-2024>(2/3 | 254/318) Exploring the Impact of ChatGPT on Student Interactions in Computer-Supported Collaborative Learning (Han Kyul Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Han Kyul Kim, Shriniwas Nayak, Aleyeh Roknaldin, Xiaoci Zhang, Marlon Twyman, Stephen Lu. (2024)<br><strong>Exploring the Impact of ChatGPT on Student Interactions in Computer-Supported Collaborative Learning</strong><br><button class=copy-to-clipboard title="Exploring the Impact of ChatGPT on Student Interactions in Computer-Supported Collaborative Learning" index=254>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-254 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CY, cs.CY<br>Keyword Score: 30<br>Keywords: Generative AI, ChatGPT, Question Answering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07082v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07082v1.pdf filename=2403.07082v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The growing popularity of <b>generative</b> <b>AI,</b> particularly <b>ChatGPT,</b> has sparked both enthusiasm and caution among practitioners and researchers in education. To effectively harness the full potential of <b>ChatGPT</b> in educational contexts, it is crucial to analyze its impact and suitability for different educational purposes. This paper takes an initial step in exploring the applicability of <b>ChatGPT</b> in a computer-supported collaborative learning (CSCL) environment. Using statistical analysis, we validate the shifts in student interactions during an asynchronous group brainstorming session by introducing <b>ChatGPT</b> as an instantaneous <b>question-answering</b> <b>agent.</b></p></p class="citation"></blockquote><h3 id=33--255318-authorship-and-the-politics-and-ethics-of-llm-watermarks-tim-räz-2024>(3/3 | 255/318) Authorship and the Politics and Ethics of LLM Watermarks (Tim Räz, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tim Räz. (2024)<br><strong>Authorship and the Politics and Ethics of LLM Watermarks</strong><br><button class=copy-to-clipboard title="Authorship and the Politics and Ethics of LLM Watermarks" index=255>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-255 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CY, cs.CY<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06593v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06593v1.pdf filename=2403.06593v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, watermarking schemes for <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have been proposed to distinguish text generated by machines and by humans. The present paper explores philosophical, political, and ethical ramifications of implementing and using watermarking schemes. A definition of authorship that includes both machines <b>(LLMs)</b> and humans is proposed to serve as a backdrop. It is argued that private watermarks may provide private companies with sweeping rights to determine authorship, which is incompatible with traditional standards of authorship determination. Then, possible ramifications of the so-called entropy dependence of watermarking mechanisms are explored. It is argued that entropy may vary for different, socially salient groups. This could lead to group dependent rates at which machine generated text is detected. Specifically, groups more interested in low entropy text may face the challenge that it is harder to detect machine generated text that is of interest to them.</p></p class="citation"></blockquote><h2 id=csdb-3>cs.DB (3)</h2><h3 id=13--256318-booster-leveraging-large-language-models-for-enhancing-entity-resolution-huahang-li-et-al-2024>(1/3 | 256/318) BoostER: Leveraging Large Language Models for Enhancing Entity Resolution (Huahang Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Huahang Li, Shuangyin Li, Fei Hao, Chen Jason Zhang, Yuanfeng Song, Lei Chen. (2024)<br><strong>BoostER: Leveraging Large Language Models for Enhancing Entity Resolution</strong><br><button class=copy-to-clipboard title="BoostER: Leveraging Large Language Models for Enhancing Entity Resolution" index=256>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-256 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DB<br>Categories: cs-DB, cs.DB<br>Keyword Score: 40<br>Keywords: GPT, GPT-4, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06434v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06434v1.pdf filename=2403.06434v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Entity resolution, which involves identifying and merging records that refer to the same real-world entity, is a crucial task in areas like Web data integration. This importance is underscored by the presence of numerous duplicated and multi-version data resources on the Web. However, achieving high-quality entity resolution typically demands significant effort. The advent of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> like <b>GPT-4</b> has demonstrated advanced linguistic capabilities, which can be a new paradigm for this task. In this paper, we propose a demonstration system named BoostER that examines the possibility of leveraging <b>LLMs</b> in the entity resolution process, revealing advantages in both easy deployment and low cost. Our approach optimally selects a set of matching questions and poses them to <b>LLMs</b> for verification, then refines the distribution of entity resolution results with the response of <b>LLMs.</b> This offers promising prospects to achieve a high-quality entity resolution result for real-world applications, especially to individuals or small companies without the need for extensive model training or significant financial investment.</p></p class="citation"></blockquote><h3 id=23--257318-evaluating-large-language-models-in-process-mining-capabilities-benchmarks-evaluation-strategies-and-future-challenges-alessandro-berti-et-al-2024>(2/3 | 257/318) Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, Evaluation Strategies, and Future Challenges (Alessandro Berti et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alessandro Berti, Humam Kourani, Hannes Hafke, Chiao-Yun Li, Daniel Schuster. (2024)<br><strong>Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, Evaluation Strategies, and Future Challenges</strong><br><button class=copy-to-clipboard title="Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, Evaluation Strategies, and Future Challenges" index=257>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-257 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DB<br>Categories: cs-DB, cs.DB<br>Keyword Score: 26<br>Keywords: Benchmarking, Benchmarking, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06749v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06749v1.pdf filename=2403.06749v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Using <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> for Process Mining (PM) tasks is becoming increasingly essential, and initial approaches yield promising results. However, little attention has been given to developing strategies for evaluating and <b>benchmarking</b> the utility of incorporating <b>LLMs</b> into PM tasks. This paper reviews the current implementations of <b>LLMs</b> in PM and reflects on three different questions. 1) What is the minimal set of capabilities required for PM on LLMs? 2) Which <b>benchmark</b> strategies help choose optimal <b>LLMs</b> for PM? 3) How do we evaluate the output of <b>LLMs</b> on specific PM tasks? The answer to these questions is fundamental to the development of comprehensive process mining <b>benchmarks</b> on <b>LLMs</b> covering different tasks and implementation paradigms.</p></p class="citation"></blockquote><h3 id=33--258318-sfvint-simple-fast-and-generic-variable-length-integer-decoding-using-bit-manipulation-instructions-gang-liao-et-al-2024>(3/3 | 258/318) SFVInt: Simple, Fast and Generic Variable-Length Integer Decoding using Bit Manipulation Instructions (Gang Liao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gang Liao, Ye Liu, Yonghua Ding, Le Cai, Jianjun Chen. (2024)<br><strong>SFVInt: Simple, Fast and Generic Variable-Length Integer Decoding using Bit Manipulation Instructions</strong><br><button class=copy-to-clipboard title="SFVInt: Simple, Fast and Generic Variable-Length Integer Decoding using Bit Manipulation Instructions" index=258>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-258 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DB<br>Categories: cs-DB, cs-DC, cs.DB<br>Keyword Score: 10<br>Keywords: Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06898v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06898v1.pdf filename=2403.06898v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The ubiquity of variable-length integers in data storage and communication necessitates efficient decoding techniques. In this paper, we present SFVInt, a simple and fast approach to decode the prevalent Little Endian Base-128 (LEB128) varints. Our approach, <b>distilled</b> into a mere 500 lines of code, effectively utilizes the Bit Manipulation Instruction Set 2 (BMI2) in modern Intel and AMD processors, achieving significant performance improvement while maintaining simplicity and avoiding overengineering. SFVInt, with its generic design, effectively processes both 32-bit and 64-bit unsigned integers using a unified code template, marking a significant leap forward in varint decoding efficiency. We thoroughly evaluate SFVInt&rsquo;s performance across various datasets and scenarios, demonstrating that it achieves up to a 2x increase in decoding speed when compared to varint decoding methods used in established frameworks like Facebook Folly and Google Protobuf.</p></p class="citation"></blockquote><h2 id=csro-12>cs.RO (12)</h2><h3 id=112--259318-rlingua-improving-reinforcement-learning-sample-efficiency-in-robotic-manipulations-with-large-language-models-liangliang-chen-et-al-2024>(1/12 | 259/318) RLingua: Improving Reinforcement Learning Sample Efficiency in Robotic Manipulations With Large Language Models (Liangliang Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Liangliang Chen, Yutian Lei, Shiyu Jin, Ying Zhang, Liangjun Zhang. (2024)<br><strong>RLingua: Improving Reinforcement Learning Sample Efficiency in Robotic Manipulations With Large Language Models</strong><br><button class=copy-to-clipboard title="RLingua: Improving Reinforcement Learning Sample Efficiency in Robotic Manipulations With Large Language Models" index=259>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-259 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-HC, cs-LG, cs-RO, cs.RO<br>Keyword Score: 40<br>Keywords: Reinforcement Learning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06420v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06420v1.pdf filename=2403.06420v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Reinforcement</b> <b>learning</b> (RL) has demonstrated its capability in solving various tasks but is notorious for its low sample efficiency. In this paper, we propose RLingua, a framework that can leverage the internal knowledge of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to reduce the sample complexity of RL in robotic manipulations. To this end, we first present how to extract the prior knowledge of <b>LLMs</b> by <b>prompt</b> engineering so that a preliminary rule-based robot controller for a specific task can be generated. Despite being imperfect, the <b>LLM-generated</b> robot controller is utilized to produce action samples during rollouts with a decaying probability, thereby improving RL&rsquo;s sample efficiency. We employ the actor-critic framework and modify the actor loss to regularize the policy learning towards the <b>LLM-generated</b> controller. RLingua also provides a novel method of improving the imperfect <b>LLM-generated</b> robot controllers by RL. We demonstrated that RLingua can significantly reduce the sample complexity of TD3 in the robot tasks of panda_gym and achieve high success rates in sparsely rewarded robot tasks in RLBench, where the standard TD3 fails. Additionally, We validated RLingua&rsquo;s effectiveness in real-world robot experiments through Sim2Real, demonstrating that the learned policies are effectively transferable to real robot tasks. Further details and videos about our work are available at our project website <a href=https://rlingua.github.io>https://rlingua.github.io</a>.</p></p class="citation"></blockquote><h3 id=212--260318-landerai-adaptive-landing-behavior-agent-for-expertise-in-3d-dynamic-platform-landings-robinroy-peter-et-al-2024>(2/12 | 260/318) Lander.AI: Adaptive Landing Behavior Agent for Expertise in 3D Dynamic Platform Landings (Robinroy Peter et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Robinroy Peter, Lavanya Ratnabala, Demetros Aschu, Aleksey Fedoseev, Dzmitry Tsetserukou. (2024)<br><strong>Lander.AI: Adaptive Landing Behavior Agent for Expertise in 3D Dynamic Platform Landings</strong><br><button class=copy-to-clipboard title="Lander.AI: Adaptive Landing Behavior Agent for Expertise in 3D Dynamic Platform Landings" index=260>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-260 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-NE, cs-RO, cs.RO<br>Keyword Score: 33<br>Keywords: Benchmarking, Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06572v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06572v2.pdf filename=2403.06572v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Mastering autonomous drone landing on dynamic platforms presents formidable challenges due to unpredictable velocities and external disturbances caused by the wind, ground effect, turbines or propellers of the docking platform. This study introduces an advanced Deep <b>Reinforcement</b> <b>Learning</b> (DRL) agent, Lander:AI, designed to navigate and land on platforms in the presence of windy conditions, thereby enhancing drone autonomy and safety. Lander:AI is rigorously trained within the gym-pybullet-drone <b>simulation,</b> an environment that mirrors real-world complexities, including wind turbulence, to ensure the agent&rsquo;s robustness and adaptability. The agent&rsquo;s capabilities were empirically validated with Crazyflie 2.1 drones across various test scenarios, encompassing both simulated environments and real-world conditions. The experimental results showcased Lander:AI&rsquo;s high-precision landing and its ability to adapt to moving platforms, even under wind-induced disturbances. Furthermore, the system performance was <b>benchmarked</b> against a baseline PID controller augmented with an Extended Kalman Filter, illustrating significant improvements in landing precision and error recovery. Lander:AI leverages bio-inspired learning to adapt to external forces like birds, enhancing drone adaptability without knowing force magnitudes.This research not only advances drone landing technologies, essential for inspection and emergency applications, but also highlights the potential of DRL in addressing intricate aerodynamic challenges.</p></p class="citation"></blockquote><h3 id=312--261318-accelerating-interface-adaptation-with-user-friendly-priors-benjamin-a-christie-et-al-2024>(3/12 | 261/318) Accelerating Interface Adaptation with User-Friendly Priors (Benjamin A. Christie et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Benjamin A. Christie, Heramb Nemlekar, Dylan P. Losey. (2024)<br><strong>Accelerating Interface Adaptation with User-Friendly Priors</strong><br><button class=copy-to-clipboard title="Accelerating Interface Adaptation with User-Friendly Priors" index=261>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-261 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07192v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07192v1.pdf filename=2403.07192v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Robots often need to convey information to human users. For example, robots can leverage visual, auditory, and haptic interfaces to display their intent or express their internal state. In some scenarios there are socially agreed upon conventions for what these signals mean: e.g., a red light indicates an autonomous car is slowing down. But as robots develop new capabilities and seek to convey more complex data, the meaning behind their signals is not always mutually understood: one user might think a flashing light indicates the autonomous car is an aggressive driver, while another user might think the same signal means the autonomous car is defensive. In this paper we enable robots to adapt their interfaces to the current user so that the human&rsquo;s personalized interpretation is aligned with the robot&rsquo;s meaning. We start with an information theoretic end-to-end approach, which automatically tunes the interface policy to optimize the correlation between human and robot. But to ensure that this learning policy is intuitive &ndash; and to accelerate how quickly the interface adapts to the human &ndash; we recognize that humans have priors over how interfaces should function. For instance, humans expect interface signals to be proportional and convex. Our approach biases the robot&rsquo;s interface towards these priors, resulting in signals that are adapted to the current user while still following social expectations. Our <b>simulations</b> and user study results across $15$ participants suggest that these priors improve robot-to-human communication. See videos here: <a href=https://youtu.be/Re3OLg57hp8>https://youtu.be/Re3OLg57hp8</a></p></p class="citation"></blockquote><h3 id=412--262318-sim-to-real-gap-in-rl-use-case-with-tiago-and-isaac-simgym-jaume-albardaner-et-al-2024>(4/12 | 262/318) Sim-to-Real gap in RL: Use Case with TIAGo and Isaac Sim/Gym (Jaume Albardaner et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jaume Albardaner, Alberto San Miguel, Néstor García, Magí Dalmau. (2024)<br><strong>Sim-to-Real gap in RL: Use Case with TIAGo and Isaac Sim/Gym</strong><br><button class=copy-to-clipboard title="Sim-to-Real gap in RL: Use Case with TIAGo and Isaac Sim/Gym" index=262>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-262 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07091v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07091v1.pdf filename=2403.07091v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper explores policy-learning approaches in the context of sim-to-real transfer for robotic manipulation using a TIAGo mobile manipulator, focusing on two state-of-art simulators, Isaac Gym and Isaac Sim, both developed by Nvidia. Control architectures are discussed, with a particular emphasis on achieving collision-less movement in both <b>simulation</b> and the real environment. Presented results demonstrate successful sim-to-real transfer, showcasing similar movements executed by an RL-trained model in both simulated and real setups.</p></p class="citation"></blockquote><h3 id=512--263318-a-collision-cone-approach-for-control-barrier-functions-manan-tayal-et-al-2024>(5/12 | 263/318) A Collision Cone Approach for Control Barrier Functions (Manan Tayal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Manan Tayal, Bhavya Giri Goswami, Karthik Rajgopal, Rajpal Singh, Tejas Rao, Jishnu Keshavan, Pushpak Jagtap, Shishir Kolathaya. (2024)<br><strong>A Collision Cone Approach for Control Barrier Functions</strong><br><button class=copy-to-clipboard title="A Collision Cone Approach for Control Barrier Functions" index=263>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-263 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07043v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07043v1.pdf filename=2403.07043v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work presents a unified approach for collision avoidance using Collision-Cone Control Barrier Functions (CBFs) in both ground (UGV) and aerial (UAV) unmanned vehicles. We propose a novel CBF formulation inspired by collision cones, to ensure safety by constraining the relative velocity between the vehicle and the obstacle to always point away from each other. The efficacy of this approach is demonstrated through <b>simulations</b> and hardware implementations on the TurtleBot, Stoch-Jeep, and Crazyflie 2.1 quadrotor robot, showcasing its effectiveness in avoiding collisions with dynamic obstacles in both ground and aerial settings. The real-time controller is developed using CBF Quadratic Programs (CBF-QPs). Comparative analysis with the state-of-the-art CBFs highlights the less conservative nature of the proposed approach. Overall, this research contributes to a novel control formation that can give a guarantee for collision avoidance in unmanned vehicles by modifying the control inputs from existing path-planning controllers.</p></p class="citation"></blockquote><h3 id=612--264318-quadruped-frog-rapid-online-optimization-of-continuous-quadruped-jumping-guillaume-bellegarda-et-al-2024>(6/12 | 264/318) Quadruped-Frog: Rapid Online Optimization of Continuous Quadruped Jumping (Guillaume Bellegarda et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guillaume Bellegarda, Milad Shafiee, Merih Ekin Özberk, Auke Ijspeert. (2024)<br><strong>Quadruped-Frog: Rapid Online Optimization of Continuous Quadruped Jumping</strong><br><button class=copy-to-clipboard title="Quadruped-Frog: Rapid Online Optimization of Continuous Quadruped Jumping" index=264>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-264 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06954v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06954v1.pdf filename=2403.06954v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Legged robots are becoming increasingly agile in exhibiting dynamic behaviors such as running and jumping. Usually, such behaviors are either optimized and engineered offline (i.e. the behavior is designed for before it is needed), either through model-based trajectory optimization, or through deep learning-based methods involving millions of timesteps of <b>simulation</b> interactions. Notably, such offline-designed locomotion controllers cannot perfectly model the true dynamics of the system, such as the motor dynamics. In contrast, in this paper, we consider a quadruped jumping task that we rapidly optimize online. We design foot force profiles parameterized by only a few parameters which we optimize for directly on hardware with Bayesian Optimization. The force profiles are tracked at the joint level, and added to Cartesian PD impedance control and Virtual Model Control to stabilize the jumping motions. After optimization, which takes only a handful of jumps, we show that this control architecture is capable of diverse and omnidirectional jumps including forward, lateral, and twist (turning) jumps, even on uneven terrain, enabling the Unitree Go1 quadruped to jump 0.5 m high, 0.5 m forward, and jump-turn over 2 rad. Video results can be found at <a href=https://youtu.be/SvfVNQ90k_w>https://youtu.be/SvfVNQ90k_w</a>.</p></p class="citation"></blockquote><h3 id=712--265318-design-and-performance-comparison-of-fuzzypid-and-non-linear-model-predictive-controller-for-4-wheel-omni-drive-robot-love-panta-2024>(7/12 | 265/318) Design and Performance Comparison of FuzzyPID and Non-linear Model Predictive Controller for 4-Wheel Omni-drive Robot (Love Panta, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Love Panta. (2024)<br><strong>Design and Performance Comparison of FuzzyPID and Non-linear Model Predictive Controller for 4-Wheel Omni-drive Robot</strong><br><button class=copy-to-clipboard title="Design and Performance Comparison of FuzzyPID and Non-linear Model Predictive Controller for 4-Wheel Omni-drive Robot" index=265>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-265 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs-SY, cs.RO, eess-SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06744v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06744v1.pdf filename=2403.06744v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Trajectory tracking for an Omni-drive robot presents a challenging task that demands an efficient controller design. To address the limitations of manual tuning, we introduce a self-optimizing controller named fuzzyPID, leveraging the analysis of responses from various dynamic and static systems. The rule-based controller design is implemented using Matlab/Simulink, and trajectory tracking <b>simulations</b> are conducted within the CoppeliaSim environment. Similarly, a non-linear model predictive controller(NMPC) is proposed to compare tracking performance with fuzzyPID. We also assess the impact of tunable parameters of NMPC on its tracking accuracy. <b>Simulation</b> results validate the precision and effectiveness of NMPC over fuzzyPID controller while trading computational complexity.</p></p class="citation"></blockquote><h3 id=812--266318-multimodal-transformers-for-real-time-surgical-activity-prediction-keshara-weerasinghe-et-al-2024>(8/12 | 266/318) Multimodal Transformers for Real-Time Surgical Activity Prediction (Keshara Weerasinghe et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Keshara Weerasinghe, Seyed Hamid Reza Roodabeh, Kay Hutchinson, Homa Alemzadeh. (2024)<br><strong>Multimodal Transformers for Real-Time Surgical Activity Prediction</strong><br><button class=copy-to-clipboard title="Multimodal Transformers for Real-Time Surgical Activity Prediction" index=266>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-266 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 16<br>Keywords: Multi-modal, Multi-modal, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06705v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06705v1.pdf filename=2403.06705v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Real-time recognition and prediction of surgical activities are fundamental to advancing safety and autonomy in robot-assisted surgery. This paper presents a <b>multimodal</b> <b>transformer</b> architecture for real-time recognition and prediction of surgical gestures and trajectories based on short segments of kinematic and video data. We conduct an ablation study to evaluate the impact of fusing different input modalities and their representations on gesture recognition and prediction performance. We perform an end-to-end assessment of the proposed architecture using the JHU-ISI Gesture and Skill Assessment Working Set (JIGSAWS) dataset. Our model outperforms the state-of-the-art (SOTA) with 89.5% accuracy for gesture prediction through effective fusion of kinematic features with spatial and contextual video features. It achieves the real-time performance of 1.1-1.3ms for processing a 1-second input window by relying on a computationally efficient model.</p></p class="citation"></blockquote><h3 id=912--267318-autonomous-overhead-powerline-recharging-for-uninterrupted-drone-operations-viet-duong-hoang-et-al-2024>(9/12 | 267/318) Autonomous Overhead Powerline Recharging for Uninterrupted Drone Operations (Viet Duong Hoang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Viet Duong Hoang, Frederik Falk Nyboe, Nicolaj Haarhøj Malle, Emad Ebeid. (2024)<br><strong>Autonomous Overhead Powerline Recharging for Uninterrupted Drone Operations</strong><br><button class=copy-to-clipboard title="Autonomous Overhead Powerline Recharging for Uninterrupted Drone Operations" index=267>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-267 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06533v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06533v1.pdf filename=2403.06533v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a fully autonomous self-recharging drone system capable of long-duration sustained operations near powerlines. The drone is equipped with a robust onboard perception and navigation system that enables it to locate powerlines and approach them for landing. A passively actuated gripping mechanism grasps the powerline cable during landing after which a control circuit regulates the magnetic field inside a split-core current <b>transformer</b> to provide sufficient holding force as well as battery recharging. The system is evaluated in an active outdoor three-phase powerline environment. We demonstrate multiple contiguous hours of fully autonomous uninterrupted drone operations composed of several cycles of flying, landing, recharging, and takeoff, validating the capability of extended, essentially unlimited, operational endurance.</p></p class="citation"></blockquote><h3 id=1012--268318-3dref-3d-dataset-and-benchmark-for-reflection-detection-in-rgb-and-lidar-data-xiting-zhao-et-al-2024>(10/12 | 268/318) 3DRef: 3D Dataset and Benchmark for Reflection Detection in RGB and Lidar Data (Xiting Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiting Zhao, Sören Schwertfeger. (2024)<br><strong>3DRef: 3D Dataset and Benchmark for Reflection Detection in RGB and Lidar Data</strong><br><button class=copy-to-clipboard title="3DRef: 3D Dataset and Benchmark for Reflection Detection in RGB and Lidar Data" index=268>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-268 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-CV, cs-RO, cs.RO, eess-IV<br>Keyword Score: 6<br>Keywords: Benchmarking, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06538v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06538v1.pdf filename=2403.06538v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Reflective surfaces present a persistent challenge for reliable 3D mapping and perception in robotics and autonomous systems. However, existing reflection datasets and <b>benchmarks</b> remain limited to sparse 2D data. This paper introduces the first large-scale 3D reflection detection dataset containing more than 50,000 aligned samples of multi-return Lidar, RGB images, and 2D/3D semantic labels across diverse indoor environments with various reflections. Textured 3D ground truth meshes enable automatic point cloud labeling to provide precise ground truth annotations. Detailed <b>benchmarks</b> evaluate three Lidar point cloud segmentation methods, as well as current state-of-the-art image segmentation networks for glass and mirror detection. The proposed dataset advances reflection detection by providing a comprehensive testbed with precise global alignment, <b>multi-modal</b> data, and diverse reflective objects and materials. It will drive future research towards reliable reflection detection. The dataset is publicly available at <a href=http://3dref.github.io>http://3dref.github.io</a></p></p class="citation"></blockquote><h3 id=1112--269318-mapping-high-level-semantic-regions-in-indoor-environments-without-object-recognition-roberto-bigazzi-et-al-2024>(11/12 | 269/318) Mapping High-level Semantic Regions in Indoor Environments without Object Recognition (Roberto Bigazzi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Roberto Bigazzi, Lorenzo Baraldi, Shreyas Kousik, Rita Cucchiara, Marco Pavone. (2024)<br><strong>Mapping High-level Semantic Regions in Indoor Environments without Object Recognition</strong><br><button class=copy-to-clipboard title="Mapping High-level Semantic Regions in Indoor Environments without Object Recognition" index=269>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-269 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-CV, cs-RO, cs.RO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07076v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07076v1.pdf filename=2403.07076v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Robots require a semantic understanding of their surroundings to operate in an efficient and explainable way in human environments. In the literature, there has been an extensive focus on object labeling and exhaustive scene <b>graph</b> generation; less effort has been focused on the task of purely identifying and mapping large semantic regions. The present work proposes a method for semantic region mapping via embodied navigation in indoor environments, generating a high-level representation of the knowledge of the agent. To enable region identification, the method uses a vision-to-language model to provide scene information for mapping. By projecting egocentric scene understanding into the global frame, the proposed method generates a semantic map as a distribution over possible region labels at each location. This mapping procedure is paired with a trained navigation policy to enable autonomous map generation. The proposed method significantly outperforms a variety of baselines, including an object-based system and a pretrained scene classifier, in experiments in a photorealistic simulator.</p></p class="citation"></blockquote><h3 id=1212--270318-neupan-direct-point-robot-navigation-with-end-to-end-model-based-learning-ruihua-han-et-al-2024>(12/12 | 270/318) NeuPAN: Direct Point Robot Navigation with End-to-End Model-based Learning (Ruihua Han et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruihua Han, Shuai Wang, Shuaijun Wang, Zeqing Zhang, Jianjun Chen, Shijie Lin, Chengyang Li, Chengzhong Xu, Yonina C. Eldar, Qi Hao, Jia Pan. (2024)<br><strong>NeuPAN: Direct Point Robot Navigation with End-to-End Model-based Learning</strong><br><button class=copy-to-clipboard title="NeuPAN: Direct Point Robot Navigation with End-to-End Model-based Learning" index=270>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-270 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs.RO<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06828v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06828v1.pdf filename=2403.06828v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Navigating a nonholonomic robot in a cluttered environment requires extremely accurate perception and locomotion for collision avoidance. This paper presents NeuPAN: a real-time, highly-accurate, map-free, robot-agnostic, and environment-invariant robot navigation solution. Leveraging a tightly-coupled perception-locomotion framework, NeuPAN has two key innovations compared to existing approaches: 1) it directly maps raw points to a learned multi-frame distance space, avoiding error propagation from perception to control; 2) it is interpretable from an end-to-end model-based learning perspective, enabling provable convergence. The crux of NeuPAN is to solve a high-dimensional end-to-end mathematical model with various point-level constraints using the plug-and-play (PnP) proximal alternating-minimization network (PAN) with neurons in the loop. This allows NeuPAN to generate real-time, end-to-end, physically-interpretable motions directly from point clouds, which seamlessly integrates data- and knowledge-engines, where its network parameters are adjusted via back propagation. We evaluate NeuPAN on car-like robot, wheel-legged robot, and passenger autonomous vehicle, in both simulated and real-world environments. Experiments demonstrate that NeuPAN outperforms various <b>benchmarks,</b> in terms of accuracy, efficiency, robustness, and generalization capability across various environments, including the cluttered sandbox, office, corridor, and parking lot. We show that NeuPAN works well in unstructured environments with arbitrary-shape undetectable objects, making impassable ways passable.</p></p class="citation"></blockquote><h2 id=cslo-1>cs.LO (1)</h2><h3 id=11--271318-are-targeted-messages-more-effective-martin-grohe-et-al-2024>(1/1 | 271/318) Are Targeted Messages More Effective? (Martin Grohe et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Martin Grohe, Eran Rosenbluth. (2024)<br><strong>Are Targeted Messages More Effective?</strong><br><button class=copy-to-clipboard title="Are Targeted Messages More Effective?" index=271>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-271 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LO<br>Categories: 68T05, 68T07, I-2-6, cs-AI, cs-LG, cs-LO, cs.LO<br>Keyword Score: 33<br>Keywords: Message-Passing, Graph, Graph Neural Network, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06817v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06817v1.pdf filename=2403.06817v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>neural</b> <b>networks</b> <b>(GNN)</b> are deep learning architectures for <b>graphs.</b> <b>Essentially,</b> <b>a</b> <b>GNN</b> is a distributed message passing algorithm, which is controlled by parameters learned from data. It operates on the vertices of a <b>graph:</b> <b>in</b> <b>each</b> iteration, vertices receive a message on each incoming edge, aggregate these messages, and then update their state based on their current state and the aggregated messages. The expressivity of <b>GNNs</b> can be characterised in terms of certain fragments of first-order logic with counting and the Weisfeiler-Lehman algorithm. The core <b>GNN</b> architecture comes in two different versions. In the first version, a message only depends on the state of the source vertex, whereas in the second version it depends on the states of the source and target vertices. In practice, both of these versions are used, but the theory of <b>GNNs</b> so far mostly focused on the first one. On the logical side, the two versions correspond to two fragments of first-order logic with counting that we call modal and guarded. The question whether the two versions differ in their expressivity has been mostly overlooked in the <b>GNN</b> literature and has only been asked recently (Grohe, LICS'23). We answer this question here. It turns out that the answer is not as straightforward as one might expect. By proving that the modal and guarded fragment of first-order logic with counting have the same expressivity over labelled undirected <b>graphs,</b> <b>we</b> <b>show</b> that in a non-uniform setting the two <b>GNN</b> versions have the same expressivity. However, we also prove that in a uniform setting the second version is strictly more expressive.</p></p class="citation"></blockquote><h2 id=q-finrm-1>q-fin.RM (1)</h2><h3 id=11--272318-financial-default-prediction-via-motif-preserving-graph-neural-network-with-curriculum-learning-daixin-wang-et-al-2024>(1/1 | 272/318) Financial Default Prediction via Motif-preserving Graph Neural Network with Curriculum Learning (Daixin Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daixin Wang, Zhiqiang Zhang, Yeyu Zhao, Kai Huang, Yulin Kang, Jun Zhou. (2024)<br><strong>Financial Default Prediction via Motif-preserving Graph Neural Network with Curriculum Learning</strong><br><button class=copy-to-clipboard title="Financial Default Prediction via Motif-preserving Graph Neural Network with Curriculum Learning" index=272>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-272 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-fin.RM<br>Categories: cs-LG, q-fin-RM, q-fin.RM<br>Keyword Score: 33<br>Keywords: Graph Attention Networks, Graph, Graph Neural Network, Curriculum Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06482v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06482v1.pdf filename=2403.06482v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>User financial default prediction plays a critical role in credit risk forecasting and management. It aims at predicting the probability that the user will fail to make the repayments in the future. Previous methods mainly extract a set of user individual features regarding his own profiles and behaviors and build a binary-classification model to make default predictions. However, these methods cannot get satisfied results, especially for users with limited information. Although recent efforts suggest that default prediction can be improved by social relations, they fail to capture the higher-order topology structure at the level of small subgraph patterns. In this paper, we fill in this gap by proposing a motif-preserving <b>Graph</b> <b>Neural</b> <b>Network</b> with <b>curriculum</b> <b>learning</b> (MotifGNN) to jointly learn the lower-order structures from the original <b>graph</b> <b>and</b> <b>higherorder</b> structures from multi-view motif-based <b>graphs</b> <b>for</b> <b>financial</b> default prediction. Specifically, to solve the problem of weak connectivity in motif-based <b>graphs,</b> <b>we</b> <b>design</b> the motif-based <b>gating</b> mechanism. It utilizes the information learned from the original <b>graph</b> <b>with</b> <b>good</b> connectivity to strengthen the learning of the higher-order structure. And considering that the motif patterns of different samples are highly unbalanced, we propose a <b>curriculum</b> <b>learning</b> mechanism on the whole learning process to more focus on the samples with uncommon motif distributions. Extensive experiments on one public dataset and two industrial datasets all demonstrate the effectiveness of our proposed method.</p></p class="citation"></blockquote><h2 id=eesssy-5>eess.SY (5)</h2><h3 id=15--273318-learning-aided-control-of-robotic-tether-net-with-maneuverable-nodes-to-capture-large-space-debris-achira-boonrath-et-al-2024>(1/5 | 273/318) Learning-Aided Control of Robotic Tether-Net with Maneuverable Nodes to Capture Large Space Debris (Achira Boonrath et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Achira Boonrath, Feng Liu, Elenora M. Botta, Souma Chowdhury. (2024)<br><strong>Learning-Aided Control of Robotic Tether-Net with Maneuverable Nodes to Capture Large Space Debris</strong><br><button class=copy-to-clipboard title="Learning-Aided Control of Robotic Tether-Net with Maneuverable Nodes to Capture Large Space Debris" index=273>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-273 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07125v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07125v1.pdf filename=2403.07125v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Maneuverable tether-net systems launched from an unmanned spacecraft offer a promising solution for the active removal of large space debris. Guaranteeing the successful capture of such space debris is dependent on the ability to reliably maneuver the tether-net system &ndash; a flexible, many-DoF (thus complex) system &ndash; for a wide range of launch scenarios. Here, scenarios are defined by the relative location of the debris with respect to the chaser spacecraft. This paper represents and solves this problem as a hierarchically decentralized implementation of robotic trajectory planning and control and demonstrates the effectiveness of the approach when applied to two different tether-net systems, with 4 and 8 maneuverable units (MUs), respectively. <b>Reinforcement</b> <b>learning</b> (policy gradient) is used to design the centralized trajectory planner that, based on the relative location of the target debris at the launch of the net, computes the final aiming positions of each MU, from which their trajectory can be derived. Each MU then seeks to follow its assigned trajectory by using a decentralized PID controller that outputs the MU&rsquo;s thrust vector and is informed by noisy sensor feedback (for realism) of its relative location. System performance is assessed in terms of capture success and overall fuel consumption by the MUs. Reward shaping and surrogate models are used to respectively guide and speed up the RL process. <b>Simulation-based</b> experiments show that this approach allows the successful capture of debris at fuel costs that are notably lower than nominal baselines, including in scenarios where the debris is significantly off-centered compared to the approaching chaser spacecraft.</p></p class="citation"></blockquote><h3 id=25--274318-grid-monitoring-and-protection-with-continuous-point-on-wave-measurements-and-generative-ai-lang-tong-et-al-2024>(2/5 | 274/318) Grid Monitoring and Protection with Continuous Point-on-Wave Measurements and Generative AI (Lang Tong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lang Tong, Xinyi Wang, Qing Zhao. (2024)<br><strong>Grid Monitoring and Protection with Continuous Point-on-Wave Measurements and Generative AI</strong><br><button class=copy-to-clipboard title="Grid Monitoring and Protection with Continuous Point-on-Wave Measurements and Generative AI" index=274>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-274 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-LG, cs-SY, eess-SY, eess.SY, stat-ML<br>Keyword Score: 20<br>Keywords: Autoencoder, Generative AI<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06942v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06942v1.pdf filename=2403.06942v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Purpose This article presents a case for a next-generation grid monitoring and control system, leveraging recent advances in <b>generative</b> <b>artificial</b> intelligence (AI), machine learning, and statistical inference. Advancing beyond earlier generations of wide-area monitoring systems built upon supervisory control and data acquisition (SCADA) and synchrophasor technologies, we argue for a monitoring and control framework based on the streaming of continuous point-on-wave (CPOW) measurements with AI-powered data compression and fault detection. Methods and Results: The architecture of the proposed design originates from the Wiener-Kallianpur innovation representation of a random process that transforms causally a stationary random process into an innovation sequence with independent and identically distributed random variables. This work presents a <b>generative</b> <b>AI</b> approach that (i) learns an innovation <b>autoencoder</b> that extracts innovation sequence from CPOW time series, (ii) compresses the CPOW streaming data with innovation <b>autoencoder</b> and subband coding, and (iii) detects unknown faults and novel trends via nonparametric sequential hypothesis testing. Conclusion: This work argues that conventional monitoring using SCADA and phasor measurement unit (PMU) technologies is ill-suited for a future grid with deep penetration of inverter-based renewable generations and distributed energy resources. A monitoring system based on CPOW data streaming and AI data analytics should be the basic building blocks for situational awareness of a highly dynamic future grid.</p></p class="citation"></blockquote><h3 id=35--275318-efficient-dual-scale-generalized-radon-fourier-transform-detector-family-for-long-time-coherent-integration-suqi-li-et-al-2024>(3/5 | 275/318) Efficient dual-scale generalized Radon-Fourier transform detector family for long time coherent integration (Suqi Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Suqi Li, Yihan Wang, Bailu Wang, Giorgio Battistelli, Luigi Chisci, Guolong Cui. (2024)<br><strong>Efficient dual-scale generalized Radon-Fourier transform detector family for long time coherent integration</strong><br><button class=copy-to-clipboard title="Efficient dual-scale generalized Radon-Fourier transform detector family for long time coherent integration" index=275>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-275 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06788v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06788v1.pdf filename=2403.06788v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Long Time Coherent Integration (LTCI) aims to accumulate target energy through long time integration, which is an effective method for the detection of a weak target. However, for a moving target, defocusing can occur due to range migration (RM) and Doppler frequency migration (DFM). To address this issue, RM and DFM corrections are required in order to achieve a well-focused image for the subsequent detection. Since RM and DFM are induced by the same motion parameters, existing approaches such as the generalized Radon-Fourier transform (GRFT) or the keystone transform (KT)-matching filter process (MFP) adopt the same search space for the motion parameters in order to eliminate both effects, thus leading to large redundancy in computation. To this end, this paper first proposes a dual-scale decomposition of the target motion parameters, consisting of well designed coarse and fine motion parameters. Then, utilizing this decomposition, the joint correction of the RM and DFM effects is decoupled into a cascade procedure, first RM correction on the coarse search space and then DFM correction on the fine search spaces. As such, step size of the search space can be tailored to RM and DFM corrections, respectively, thus avoiding large redundant computation effectively. The resulting algorithms are called dual-scale GRFT (DS-GRFT) or dual-scale GRFT (DS-KTMFP) which provide comparable performance while achieving significant improvement in computational efficiency compared to standard GRFT (KT-MFP). <b>Simulation</b> experiments verify their effectiveness and efficiency.</p></p class="citation"></blockquote><h3 id=45--276318-edge-information-hub-orchestrating-satellites-uavs-mec-sensing-and-communications-for-6g-closed-loop-controls-chengleyang-lei-et-al-2024>(4/5 | 276/318) Edge Information Hub: Orchestrating Satellites, UAVs, MEC, Sensing and Communications for 6G Closed-Loop Controls (Chengleyang Lei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chengleyang Lei, Wei Feng, Peng Wei, Yunfei Chen, Ning Ge, Shiwen Mao. (2024)<br><strong>Edge Information Hub: Orchestrating Satellites, UAVs, MEC, Sensing and Communications for 6G Closed-Loop Controls</strong><br><button class=copy-to-clipboard title="Edge Information Hub: Orchestrating Satellites, UAVs, MEC, Sensing and Communications for 6G Closed-Loop Controls" index=276>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-276 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06579v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06579v1.pdf filename=2403.06579v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>An increasing number of field robots would be used for mission-critical tasks in remote or post-disaster areas. Due to usually-limited individual abilities, these robots require an edge information hub (EIH), which is capable of not only communications but also sensing and computing. Such EIH could be deployed on a flexibly-dispatched unmanned aerial vehicle (UAV). Different from traditional aerial base stations or mobile edge computing (MEC), the EIH would direct the operations of robots via sensing-communication-computing-control ($\textbf{SC}^3$) closed-loop orchestration. This paper aims to optimize the closed-loop control performance of multiple $\textbf{SC}^3$ loops, under the constraints of satellite-backhaul rate, computing capability, and on-board energy. Specifically, the linear quadratic regulator (LQR) control cost is used to measure the closed-loop utility, and a sum LQR cost minimization problem is formulated to jointly optimize the splitting of sensor data and allocation of communication and computing resources. We first derive the optimal splitting ratio of sensor data, and then recast the problem to a more tractable form. An iterative algorithm is finally proposed to provide a sub-optimal solution. <b>Simulation</b> results demonstrate the superiority of the proposed algorithm. We also uncover the influence of $\textbf{SC}^3$ parameters on closed-loop controls, highlighting more systematic understanding.</p></p class="citation"></blockquote><h3 id=55--277318-a-prediction-based-forward-looking-vehicle-dispatching-strategy-for-dynamic-ride-pooling-xiaolei-wang-et-al-2024>(5/5 | 277/318) A prediction-based forward-looking vehicle dispatching strategy for dynamic ride-pooling (Xiaolei Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaolei Wang, Chen Yang, Yuzhen Feng, Luohan Hu, Zhengbing He. (2024)<br><strong>A prediction-based forward-looking vehicle dispatching strategy for dynamic ride-pooling</strong><br><button class=copy-to-clipboard title="A prediction-based forward-looking vehicle dispatching strategy for dynamic ride-pooling" index=277>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-277 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06463v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06463v1.pdf filename=2403.06463v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>For on-demand dynamic ride-pooling services, e.g., Uber Pool and Didi Pinche, a well-designed vehicle dispatching strategy is crucial for platform profitability and passenger experience. Most existing dispatching strategies overlook incoming pairing opportunities, therefore suffer from short-sighted limitations. In this paper, we propose a forward-looking vehicle dispatching strategy, which first predicts the expected distance saving that could be brought about by future orders and then solves a bipartite matching problem based on the prediction to match passengers with partially occupied or vacant vehicles or keep passengers waiting for next rounds of matching. To demonstrate the performance of the proposed strategy, a number of <b>simulation</b> experiments and comparisons are conducted based on the real-world road network and historical trip data from Haikou, China. Results show that the proposed strategy outperform the baseline strategies by generating approximately 31% more distance saving and 18% less average passenger detour distance. It indicates the significant benefits of considering future pairing opportunities in dispatching, and highlights the effectiveness of our innovative forward-looking vehicle dispatching strategy in improving system efficiency and user experience for dynamic ride-pooling services.</p></p class="citation"></blockquote><h2 id=cond-matmtrl-sci-1>cond-mat.mtrl-sci (1)</h2><h3 id=11--278318-materials-science-in-the-era-of-large-language-models-a-perspective-ge-lei-et-al-2024>(1/1 | 278/318) Materials science in the era of large language models: a perspective (Ge Lei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ge Lei, Ronan Docherty, Samuel J. Cooper. (2024)<br><strong>Materials science in the era of large language models: a perspective</strong><br><button class=copy-to-clipboard title="Materials science in the era of large language models: a perspective" index=278>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-278 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cond-mat.mtrl-sci<br>Categories: cond-mat-mtrl-sci, cond-mat.mtrl-sci, cs-CL<br>Keyword Score: 30<br>Keywords: Code Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06949v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06949v1.pdf filename=2403.06949v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have garnered considerable interest due to their impressive natural language capabilities, which in conjunction with various emergent properties make them versatile tools in workflows ranging from complex <b>code</b> <b>generation</b> to heuristic finding for combinatorial problems. In this paper we offer a perspective on their applicability to materials science research, arguing their ability to handle ambiguous requirements across a range of tasks and disciplines mean they could be a powerful tool to aid researchers. We qualitatively examine basic <b>LLM</b> theory, connecting it to relevant properties and techniques in the literature before providing two case studies that demonstrate their use in task automation and knowledge extraction at-scale. At their current stage of development, we argue <b>LLMs</b> should be viewed less as oracles of novel insight, and more as tireless workers that can accelerate and unify exploration across domains. It is our hope that this paper can familiarise material science researchers with the concepts needed to leverage these tools in their own research.</p></p class="citation"></blockquote><h2 id=csgr-2>cs.GR (2)</h2><h3 id=12--279318-surface-aware-mesh-texture-synthesis-with-pre-trained-2d-cnns-áron-samuel-kovács-et-al-2024>(1/2 | 279/318) Surface-aware Mesh Texture Synthesis with Pre-trained 2D CNNs (Áron Samuel Kovács et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Áron Samuel Kovács, Pedro Hermosilla, Renata G. Raidou. (2024)<br><strong>Surface-aware Mesh Texture Synthesis with Pre-trained 2D CNNs</strong><br><button class=copy-to-clipboard title="Surface-aware Mesh Texture Synthesis with Pre-trained 2D CNNs" index=279>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-279 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GR<br>Categories: cs-GR, cs.GR<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06855v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06855v1.pdf filename=2403.06855v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Mesh texture synthesis is a key component in the automatic generation of 3D content. Existing learning-based methods have drawbacks &ndash; either by disregarding the shape manifold during texture generation or by requiring a large number of different views to mitigate occlusion-related inconsistencies. In this paper, we present a novel surface-aware approach for mesh texture synthesis that overcomes these drawbacks by leveraging the pre-trained weights of 2D <b>Convolutional</b> <b>Neural</b> <b>Networks</b> <b>(CNNs)</b> with the same architecture, but with <b>convolutions</b> designed for 3D meshes. Our proposed network keeps track of the oriented patches surrounding each texel, enabling seamless texture synthesis and retaining local similarity to classical 2D <b>convolutions</b> with square kernels. Our approach allows us to synthesize textures that account for the geometric content of mesh surfaces, eliminating discontinuities and achieving comparable quality to 2D image synthesis algorithms. We compare our approach with state-of-the-art methods where, through qualitative and quantitative evaluations, we demonstrate that our approach is more effective for a variety of meshes and styles, while also producing visually appealing and consistent textures on meshes.</p></p class="citation"></blockquote><h3 id=22--280318-inverse-garment-and-pattern-modeling-with-a-differentiable-simulator-boyang-yu-et-al-2024>(2/2 | 280/318) Inverse Garment and Pattern Modeling with a Differentiable Simulator (Boyang Yu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Boyang Yu, Frederic Cordier, Hyewon Seo. (2024)<br><strong>Inverse Garment and Pattern Modeling with a Differentiable Simulator</strong><br><button class=copy-to-clipboard title="Inverse Garment and Pattern Modeling with a Differentiable Simulator" index=280>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-280 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GR<br>Categories: cs-GR, cs.GR<br>Keyword Score: 25<br>Keywords: Geometry, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06841v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06841v2.pdf filename=2403.06841v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The capability to generate <b>simulation-ready</b> garment models from 3D shapes of clothed humans will significantly enhance the interpretability of captured <b>geometry</b> of real garments, as well as their faithful reproduction in the virtual world. This will have notable impact on fields like shape capture in social VR, and virtual try-on in the fashion industry. To align with the garment modeling process standardized by the fashion industry as well as cloth <b>simulation</b> softwares, it is required to recover 2D patterns. This involves an inverse garment design problem, which is the focus of our work here: Starting with an arbitrary target garment <b>geometry,</b> our system estimates an animatable garment model by automatically adjusting its corresponding 2D template pattern, along with the material parameters of the physics-based <b>simulation</b> (PBS). Built upon a differentiable cloth simulator, the optimization process is directed towards minimizing the deviation of the simulated garment shape from the target <b>geometry.</b> Moreover, our produced patterns meet manufacturing requirements such as left-to-right-symmetry, making them suited for reverse garment fabrication. We validate our approach on examples of different garment types, and show that our method faithfully reproduces both the draped garment shape and the sewing pattern.</p></p class="citation"></blockquote><h2 id=cset-2>cs.ET (2)</h2><h3 id=12--281318-integration-of-physics-derived-memristor-models-with-machine-learning-frameworks-zhenming-yu-et-al-2024>(1/2 | 281/318) Integration of Physics-Derived Memristor Models with Machine Learning Frameworks (Zhenming Yu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhenming Yu, Stephan Menzel, John Paul Strachan, Emre Neftci. (2024)<br><strong>Integration of Physics-Derived Memristor Models with Machine Learning Frameworks</strong><br><button class=copy-to-clipboard title="Integration of Physics-Derived Memristor Models with Machine Learning Frameworks" index=281>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-281 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.ET<br>Categories: cs-ET, cs.ET<br>Keyword Score: 30<br>Keywords: MNIST, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06746v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06746v1.pdf filename=2403.06746v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Simulation</b> frameworks such MemTorch, DNN+NeuroSim, and aihwkit are commonly used to facilitate the end-to-end co-design of memristive machine learning (ML) accelerators. These simulators can take device nonidealities into account and are integrated with modern ML frameworks. However, memristors in these simulators are modeled with either lookup tables or simple analytic models with basic nonlinearities. These simple models are unable to capture certain performance-critical aspects of device nonidealities. For example, they ignore the physical cause of switching, which induces errors in switching timings and thus incorrect estimations of conductance states. This work aims at bringing physical dynamics into consideration to model nonidealities while being compatible with GPU accelerators. We focus on Valence Change Memory (VCM) cells, where the switching nonlinearity and SET/RESET asymmetry relate tightly with the thermal resistance, ion mobility, Schottky barrier height, parasitic resistance, and other effects. The resulting dynamics require solving an ODE that captures changes in oxygen vacancies. We modified a physics-derived SPICE-level VCM model, integrated it with the aihwkit simulator and tested the performance with the <b>MNIST</b> dataset. Results show that noise that disrupts the SET/RESET matching affects network performance the most. This work serves as a tool for evaluating how physical dynamics in memristive devices affect neural network accuracy and can be used to guide the development of future integrated devices.</p></p class="citation"></blockquote><h3 id=22--282318-designing-a-k-state-p-bit-engine-mohammad-khairul-bashar-et-al-2024>(2/2 | 282/318) Designing a K-state P-bit Engine (Mohammad Khairul Bashar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohammad Khairul Bashar, Abir Hasan, Nikhil Shukla. (2024)<br><strong>Designing a K-state P-bit Engine</strong><br><button class=copy-to-clipboard title="Designing a K-state P-bit Engine" index=282>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-282 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.ET<br>Categories: cs-ET, cs.ET, math-OC, physics-app-ph<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06436v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06436v1.pdf filename=2403.06436v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Probabilistic bit (p-bit)-based compute engines utilize the unique capability of a p-bit to probabilistically switch between two states to solve computationally challenging problems. However, when solving problems that require more than two states (e.g., problems such as Max-3-Cut, verifying if a <b>graph</b> is K-partite (K>2) etc.), additional pre-processing steps such as <b>graph</b> reduction are required to make the problem compatible with a two-state p-bit platform. Moreover, this not only increases the problem size by entailing the use of auxiliary variables but can also degrade the solution quality. In this work, we develop a unique framework for implementing a K-state (K>2) p-bit engine. Furthermore, from an implementation standpoint, we show that such a K-state p-bit engine can be implemented using N traditional (2-state) p-bits, and one multi-state p-bit &ndash; a novel concept proposed here. Augmenting traditional p-bit platforms, our approach enables us to solve an archetypal combinatoric problem class requiring multiple states, namely Max-K-Cut (K=3, 4 shown here), without using any additional auxiliary variables. Thus, our work fundamentally advances the functional capability of p-bit engines, enabling them to solve a broader class of computationally challenging problems more efficiently.</p></p class="citation"></blockquote><h2 id=cshc-5>cs.HC (5)</h2><h3 id=15--283318-hill-a-hallucination-identifier-for-large-language-models-florian-leiser-et-al-2024>(1/5 | 283/318) HILL: A Hallucination Identifier for Large Language Models (Florian Leiser et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Florian Leiser, Sven Eckhardt, Valentin Leuthe, Merlin Knaeble, Alexander Maedche, Gerhard Schwabe, Ali Sunyaev. (2024)<br><strong>HILL: A Hallucination Identifier for Large Language Models</strong><br><button class=copy-to-clipboard title="HILL: A Hallucination Identifier for Large Language Models" index=283>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-283 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 30<br>Keywords: Question Answering, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06710v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06710v1.pdf filename=2403.06710v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> are prone to hallucinations, i.e., nonsensical, unfaithful, and undesirable text. Users tend to overrely on <b>LLMs</b> and corresponding hallucinations which can lead to misinterpretations and errors. To tackle the problem of overreliance, we propose HILL, the &ldquo;Hallucination Identifier for <b>Large</b> <b>Language</b> <b>Models&rdquo;.</b> First, we identified design features for HILL with a Wizard of Oz approach with nine participants. Subsequently, we implemented HILL based on the identified design features and evaluated HILL&rsquo;s interface design by surveying 17 participants. Further, we investigated HILL&rsquo;s functionality to identify hallucinations based on an existing <b>question-answering</b> <b>dataset</b> and five user interviews. We find that HILL can correctly identify and highlight hallucinations in <b>LLM</b> responses which enables users to handle <b>LLM</b> responses with more caution. With that, we propose an easy-to-implement adaptation to existing <b>LLMs</b> and demonstrate the relevance of user-centered designs of AI artifacts.</p></p class="citation"></blockquote><h3 id=25--284318-designing-for-projection-based-communication-between-autonomous-vehicles-and-pedestrians-trung-thanh-nguyen-et-al-2024>(2/5 | 284/318) Designing for Projection-based Communication between Autonomous Vehicles and Pedestrians (Trung Thanh Nguyen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Trung Thanh Nguyen, Kai Hollander, Marius Hoggenmueller, Callum Parker, Martin Tomitsch. (2024)<br><strong>Designing for Projection-based Communication between Autonomous Vehicles and Pedestrians</strong><br><button class=copy-to-clipboard title="Designing for Projection-based Communication between Autonomous Vehicles and Pedestrians" index=284>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-284 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 30<br>Keywords: Recommendation, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06429v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06429v1.pdf filename=2403.06429v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent studies have investigated new approaches for communicating an autonomous vehicle&rsquo;s (AV) intent and awareness to pedestrians. This paper adds to this body of work by presenting the design and evaluation of in-situ projections on the road. Our design combines common traffic light patterns with aesthetic visual elements. We describe the iterative design process and the prototyping methods used in each stage. The final design concept was represented as a virtual reality <b>simulation</b> and evaluated with 18 participants in four different street crossing scenarios, which included three scenarios that simulated various degrees of system errors. We found that different design elements were able to support participants&rsquo; confidence in their decision even when the AV failed to correctly detect their presence. We also identified elements in our design that needed to be more clearly communicated. Based on these findings, the paper presents a series of design <b>recommendations</b> for projection-based communication between AVs and pedestrians.</p></p class="citation"></blockquote><h3 id=35--285318-people-attribute-purpose-to-autonomous-vehicles-when-explaining-their-behavior-balint-gyevnar-et-al-2024>(3/5 | 285/318) People Attribute Purpose to Autonomous Vehicles When Explaining Their Behavior (Balint Gyevnar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Balint Gyevnar, Stephanie Droop, Tadeg Quillien. (2024)<br><strong>People Attribute Purpose to Autonomous Vehicles When Explaining Their Behavior</strong><br><button class=copy-to-clipboard title="People Attribute Purpose to Autonomous Vehicles When Explaining Their Behavior" index=285>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-285 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-AI, cs-HC, cs-RO, cs.HC<br>Keyword Score: 20<br>Keywords: Counter-factual, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08828v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08828v1.pdf filename=2403.08828v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A hallmark of a good XAI system is explanations that users can understand and act on. In many cases, this requires a system to offer causal or <b>counterfactual</b> explanations that are intelligible. Cognitive science can help us understand what kinds of explanations users might expect, and in which format to frame these explanations. We briefly review relevant literature from the cognitive science of explanation, particularly as it concerns teleology, the tendency to explain a decision in terms of the purpose it was meant to achieve. We then report empirical data on how people generate explanations for the behavior of autonomous vehicles, and how they evaluate these explanations. In a first survey, participants (n=54) were shown videos of a road scene and asked to generate either mechanistic, <b>counterfactual,</b> or teleological verbal explanations for a vehicle&rsquo;s actions. In the second survey, a different set of participants (n=356) rated these explanations along various metrics including quality, trustworthiness, and how much each explanatory mode was emphasized in the explanation. Participants deemed mechanistic and teleological explanations as significantly higher quality than <b>counterfactual</b> explanations. In addition, perceived teleology was the best predictor of perceived quality and trustworthiness. Neither perceived teleology nor quality ratings were affected by whether the car whose actions were being explained was an autonomous vehicle or was being driven by a person. The results show people use and value teleological concepts to evaluate information about both other people and autonomous vehicles, indicating they find the &lsquo;intentional stance&rsquo; a convenient abstraction. We make our dataset of annotated video situations with explanations, called Human Explanations for Autonomous Driving Decisions (HEADD), publicly available, which we hope will <b>prompt</b> further research.</p></p class="citation"></blockquote><h3 id=45--286318-decoding-complexity-exploring-human-ai-concordance-in-qualitative-coding-elisabeth-kirsten-et-al-2024>(4/5 | 286/318) Decoding Complexity: Exploring Human-AI Concordance in Qualitative Coding (Elisabeth Kirsten et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Elisabeth Kirsten, Annalina Buckmann, Abraham Mhaidli, Steffen Becker. (2024)<br><strong>Decoding Complexity: Exploring Human-AI Concordance in Qualitative Coding</strong><br><button class=copy-to-clipboard title="Decoding Complexity: Exploring Human-AI Concordance in Qualitative Coding" index=286>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-286 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06607v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06607v1.pdf filename=2403.06607v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Qualitative data analysis provides insight into the underlying perceptions and experiences within unstructured data. However, the time-consuming nature of the coding process, especially for larger datasets, calls for innovative approaches, such as the integration of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> This short paper presents initial findings from a study investigating the integration of <b>LLMs</b> for coding tasks of varying complexity in a real-world dataset. Our results highlight the challenges inherent in coding with extensive codebooks and contexts, both for human coders and <b>LLMs,</b> and suggest that the integration of <b>LLMs</b> into the coding process requires a task-by-task evaluation. We examine factors influencing the complexity of coding tasks and initiate a discussion on the usefulness and limitations of incorporating <b>LLMs</b> in qualitative research.</p></p class="citation"></blockquote><h3 id=55--287318-mitigating-biases-in-collective-decision-making-enhancing-performance-in-the-face-of-fake-news-axel-abels-et-al-2024>(5/5 | 287/318) Mitigating Biases in Collective Decision-Making: Enhancing Performance in the Face of Fake News (Axel Abels et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Axel Abels, Elias Fernandez Domingos, Ann Nowé, Tom Lenaerts. (2024)<br><strong>Mitigating Biases in Collective Decision-Making: Enhancing Performance in the Face of Fake News</strong><br><button class=copy-to-clipboard title="Mitigating Biases in Collective Decision-Making: Enhancing Performance in the Face of Fake News" index=287>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-287 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs-LG, cs-SI, cs.HC<br>Keyword Score: 13<br>Keywords: Benchmarking, Fake News Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.08829v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.08829v1.pdf filename=2403.08829v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Individual and social biases undermine the effectiveness of human advisers by inducing judgment errors which can disadvantage protected groups. In this paper, we study the influence these biases can have in the pervasive problem of <b>fake</b> <b>news</b> by evaluating human participants&rsquo; capacity to identify false headlines. By focusing on headlines involving sensitive characteristics, we gather a comprehensive dataset to explore how human responses are shaped by their biases. Our analysis reveals recurring individual biases and their permeation into collective decisions. We show that demographic factors, headline categories, and the manner in which information is presented significantly influence errors in human judgment. We then use our collected data as a <b>benchmark</b> problem on which we evaluate the efficacy of adaptive aggregation algorithms. In addition to their improved accuracy, our results highlight the interactions between the emergence of collective intelligence and the mitigation of participant biases.</p></p class="citation"></blockquote><h2 id=csmm-1>cs.MM (1)</h2><h3 id=11--288318-fashionregen-llm-empowered-fashion-report-generation-yujuan-ding-et-al-2024>(1/1 | 288/318) FashionReGen: LLM-Empowered Fashion Report Generation (Yujuan Ding et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yujuan Ding, Yunshan Ma, Wenqi Fan, Yige Yao, Tat-Seng Chua, Qing Li. (2024)<br><strong>FashionReGen: LLM-Empowered Fashion Report Generation</strong><br><button class=copy-to-clipboard title="FashionReGen: LLM-Empowered Fashion Report Generation" index=288>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-288 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MM<br>Categories: cs-AI, cs-MA, cs-MM, cs.MM<br>Keyword Score: 30<br>Keywords: GPT, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06660v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06660v1.pdf filename=2403.06660v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Fashion analysis refers to the process of examining and evaluating trends, styles, and elements within the fashion industry to understand and interpret its current state, generating fashion reports. It is traditionally performed by fashion professionals based on their expertise and experience, which requires high labour cost and may also produce biased results for relying heavily on a small group of people. In this paper, to tackle the Fashion Report Generation (FashionReGen) task, we propose an intelligent Fashion Analyzing and Reporting system based the advanced <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> debbed as <b>GPT-FAR.</b> Specifically, it tries to deliver FashionReGen based on effective catwalk analysis, which is equipped with several key procedures, namely, catwalk understanding, collective organization and analysis, and report generation. By posing and exploring such an open-ended, complex and domain-specific task of FashionReGen, it is able to test the general capability of <b>LLMs</b> in fashion domain. It also inspires the explorations of more high-level tasks with industrial significance in other domains. Video illustration and more materials of <b>GPT-FAR</b> can be found in <a href=https://github.com/CompFashion/FashionReGen>https://github.com/CompFashion/FashionReGen</a>.</p></p class="citation"></blockquote><h2 id=csar-3>cs.AR (3)</h2><h3 id=13--289318-from-english-to-asic-hardware-implementation-with-large-language-model-emil-goh-et-al-2024>(1/3 | 289/318) From English to ASIC: Hardware Implementation with Large Language Model (Emil Goh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Emil Goh, Maoyang Xiang, I-Chyn Wey, T. Hui Teo. (2024)<br><strong>From English to ASIC: Hardware Implementation with Large Language Model</strong><br><button class=copy-to-clipboard title="From English to ASIC: Hardware Implementation with Large Language Model" index=289>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-289 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AR<br>Categories: cs-AI, cs-AR, cs-CL, cs-PL, cs.AR<br>Keyword Score: 30<br>Keywords: Fine-tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07039v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07039v1.pdf filename=2403.07039v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the realm of ASIC engineering, the landscape has been significantly reshaped by the rapid development of <b>LLM,</b> paralleled by an increase in the complexity of modern digital circuits. This complexity has escalated the requirements for HDL coding, necessitating a higher degree of precision and sophistication. However, challenges have been faced due to the less-than-optimal performance of modern language models in generating hardware description code, a situation further exacerbated by the scarcity of the corresponding high-quality code datasets. These challenges have highlighted the gap between the potential of <b>LLMs</b> to revolutionize digital circuit design and their current capabilities in accurately interpreting and implementing hardware specifications. To address these challenges, a strategy focusing on the <b>fine-tuning</b> of the leading-edge nature language model and the reshuffling of the HDL code dataset has been developed. The <b>fine-tuning</b> aims to enhance models&rsquo; proficiency in generating precise and efficient ASIC design, while the dataset reshuffling is intended to broaden the scope and improve the quality of training material. The model demonstrated significant improvements compared to the base model, with approximately 10% to 20% increase in accuracy across a wide range of temperature for the pass@1 metric. This approach is expected to facilitate a simplified and more efficient <b>LLM-assisted</b> framework for complex circuit design, leveraging their capabilities to meet the sophisticated demands of HDL coding and thus streamlining the ASIC development process.</p></p class="citation"></blockquote><h3 id=23--290318-smart-infinity-fast-large-language-model-training-using-near-storage-processing-on-a-real-system-hongsun-jang-et-al-2024>(2/3 | 290/318) Smart-Infinity: Fast Large Language Model Training using Near-Storage Processing on a Real System (Hongsun Jang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hongsun Jang, Jaeyong Song, Jaewon Jung, Jaeyoung Park, Youngsok Kim, Jinho Lee. (2024)<br><strong>Smart-Infinity: Fast Large Language Model Training using Near-Storage Processing on a Real System</strong><br><button class=copy-to-clipboard title="Smart-Infinity: Fast Large Language Model Training using Near-Storage Processing on a Real System" index=290>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-290 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AR<br>Categories: cs-AR, cs-LG, cs.AR<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06664v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06664v1.pdf filename=2403.06664v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The recent huge advance of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> is mainly driven by the increase in the number of parameters. This has led to substantial memory capacity requirements, necessitating the use of dozens of GPUs just to meet the capacity. One popular solution to this is storage-offloaded training, which uses host memory and storage as an extended memory hierarchy. However, this obviously comes at the cost of storage bandwidth bottleneck because storage devices have orders of magnitude lower bandwidth compared to that of GPU device memories. Our work, Smart-Infinity, addresses the storage bandwidth bottleneck of storage-offloaded <b>LLM</b> training using near-storage processing devices on a real system. The main component of Smart-Infinity is SmartUpdate, which performs parameter updates on custom near-storage accelerators. We identify that moving parameter updates to the storage side removes most of the storage traffic. In addition, we propose an efficient data transfer handler structure to address the system integration issues for Smart-Infinity. The handler allows overlapping data transfers with fixed memory consumption by reusing the device buffer. Lastly, we propose accelerator-assisted gradient compression/decompression to enhance the scalability of Smart-Infinity. When scaling to multiple near-storage processing devices, the write traffic on the shared channel becomes the bottleneck. To alleviate this, we compress the gradients on the GPU and decompress them on the accelerators. It provides further acceleration from reduced traffic. As a result, Smart-Infinity achieves a significant speedup compared to the baseline. Notably, Smart-Infinity is a ready-to-use approach that is fully integrated into PyTorch on a real system. We will open-source Smart-Infinity to facilitate its use.</p></p class="citation"></blockquote><h3 id=33--291318-tcam-ssd-a-framework-for-search-based-computing-in-solid-state-drives-ryan-wong-et-al-2024>(3/3 | 291/318) TCAM-SSD: A Framework for Search-Based Computing in Solid-State Drives (Ryan Wong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ryan Wong, Nikita Kim, Kevin Higgs, Sapan Agarwal, Engin Ipek, Saugata Ghose, Ben Feinberg. (2024)<br><strong>TCAM-SSD: A Framework for Search-Based Computing in Solid-State Drives</strong><br><button class=copy-to-clipboard title="TCAM-SSD: A Framework for Search-Based Computing in Solid-State Drives" index=291>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-291 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AR<br>Categories: cs-AR, cs.AR<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06938v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06938v1.pdf filename=2403.06938v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As the amount of data produced in society continues to grow at an exponential rate, modern applications are incurring significant performance and energy penalties due to high data movement between the CPU and memory/storage. While processing in main memory can alleviate these penalties, it is becoming increasingly difficult to keep large datasets entirely in main memory. This has led to a recent push for in-storage computation, where processing is performed inside the storage device. We propose TCAM-SSD, a new framework for search-based computation inside the NAND flash memory arrays of a conventional solid-state drive (SSD), which requires lightweight modifications to only the array periphery and firmware. TCAM-SSD introduces a search manager and link table, which can logically partition the NAND flash memory&rsquo;s contents into search-enabled regions and standard storage regions. Together, these light firmware changes enable TCAM-SSD to seamlessly handle block I/O operations, in addition to new search operations, thereby reducing end-to-end execution time and total data movement. We provide an NVMe-compatible interface that provides programmers with the ability to dynamically allocate data on and make use of TCAM-SSD, allowing the system to be leveraged by a wide variety of applications. We evaluate three example use cases of TCAM-SSD to demonstrate its benefits. For transactional databases, TCAM-SSD can mitigate the performance penalties for applications with large datasets, achieving a 60.9% speedup over a conventional system that retrieves data from the SSD and computes using the CPU. For database analytics, TCAM-SSD provides an average speedup of 17.7x over a conventional system for a collection of analytical queries. For <b>graph</b> analytics, we combine TCAM-SSD&rsquo;s associative search with a sparse data structure, speeding up <b>graph</b> computing for larger-than-memory datasets by 14.5%.</p></p class="citation"></blockquote><h2 id=cssd-1>cs.SD (1)</h2><h3 id=11--292318-towards-decoupling-frontend-enhancement-and-backend-recognition-in-monaural-robust-asr-yufeng-yang-et-al-2024>(1/1 | 292/318) Towards Decoupling Frontend Enhancement and Backend Recognition in Monaural Robust ASR (Yufeng Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yufeng Yang, Ashutosh Pandey, DeLiang Wang. (2024)<br><strong>Towards Decoupling Frontend Enhancement and Backend Recognition in Monaural Robust ASR</strong><br><button class=copy-to-clipboard title="Towards Decoupling Frontend Enhancement and Backend Recognition in Monaural Robust ASR" index=292>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-292 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-SD, cs.SD, eess-AS<br>Keyword Score: 30<br>Keywords: Automatic Speech Recognition, Automatic Speech Recognition, Automatic Speech Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06387v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06387v1.pdf filename=2403.06387v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>It has been shown that the intelligibility of noisy <b>speech</b> <b>can</b> be improved by <b>speech</b> <b>enhancement</b> (SE) algorithms. However, monaural SE has not been established as an effective frontend for <b>automatic</b> <b>speech</b> <b>recognition</b> <b>(ASR)</b> in noisy conditions compared to an <b>ASR</b> model trained on noisy <b>speech</b> <b>directly.</b> The divide between SE and <b>ASR</b> impedes the progress of robust <b>ASR</b> systems, especially as SE has made major advances in recent years. This paper focuses on eliminating this divide with an ARN (attentive recurrent network) time-domain and a CrossNet time-frequency domain enhancement models. The proposed systems fully decouple frontend enhancement and backend <b>ASR</b> trained only on clean <b>speech.</b> <b>Results</b> on the WSJ, CHiME-2, LibriSpeech, and CHiME-4 corpora demonstrate that ARN and CrossNet enhanced <b>speech</b> <b>both</b> translate to improved <b>ASR</b> results in noisy and reverberant environments, and generalize well to real acoustic scenarios. The proposed system outperforms the baselines trained on corrupted <b>speech</b> <b>directly.</b> Furthermore, it cuts the previous best word error rate (WER) on CHiME-2 by $28.4%$ relatively with a $5.57%$ WER, and achieves $3.32/4.44%$ WER on single-channel CHiME-4 simulated/real test data without training on CHiME-4.</p></p class="citation"></blockquote><h2 id=csne-1>cs.NE (1)</h2><h3 id=11--293318-map-elites-with-transverse-assessment-for-multimodal-problems-in-creative-domains-marvin-zammit-et-al-2024>(1/1 | 293/318) MAP-Elites with Transverse Assessment for Multimodal Problems in Creative Domains (Marvin Zammit et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marvin Zammit, Antonios Liapis, Georgios N. Yannakakis. (2024)<br><strong>MAP-Elites with Transverse Assessment for Multimodal Problems in Creative Domains</strong><br><button class=copy-to-clipboard title="MAP-Elites with Transverse Assessment for Multimodal Problems in Creative Domains" index=293>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-293 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-NE, cs.NE<br>Keyword Score: 26<br>Keywords: Multi-modal, Multi-modal, Image2text, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07182v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07182v1.pdf filename=2403.07182v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The recent advances in language-based generative models have paved the way for the orchestration of multiple generators of different artefact types (text, image, audio, etc.) into one system. Presently, many open-source pre-trained models combine text with other modalities, thus enabling shared vector embeddings to be compared across different generators. Within this context we propose a novel approach to handle <b>multimodal</b> creative tasks using Quality Diversity evolution. Our contribution is a variation of the MAP-Elites algorithm, MAP-Elites with Transverse Assessment (MEliTA), which is tailored for <b>multimodal</b> creative tasks and leverages deep learned models that assess coherence across modalities. MEliTA decouples the artefacts&rsquo; modalities and promotes cross-pollination between elites. As a test bed for this algorithm, we generate text descriptions and cover images for a hypothetical video game and assign each artefact a unique modality-specific behavioural characteristic. Results indicate that MEliTA can improve <b>text-to-image</b> mappings within the solution space, compared to a baseline MAP-Elites algorithm that strictly treats each <b>image-text</b> pair as one solution. Our approach represents a significant step forward in <b>multimodal</b> bottom-up orchestration and lays the groundwork for more complex systems coordinating <b>multimodal</b> creative agents in the future.</p></p class="citation"></blockquote><h2 id=physicscomp-ph-1>physics.comp-ph (1)</h2><h3 id=11--294318-an-alternative-to-stride-based-rng-for-monte-carlo-transport-braxton-s-cuneo-et-al-2024>(1/1 | 294/318) An Alternative to Stride-Based RNG for Monte Carlo Transport (Braxton S. Cuneo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Braxton S. Cuneo, Ilham Variansyah. (2024)<br><strong>An Alternative to Stride-Based RNG for Monte Carlo Transport</strong><br><button class=copy-to-clipboard title="An Alternative to Stride-Based RNG for Monte Carlo Transport" index=294>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-294 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.comp-ph<br>Categories: cs-CE, physics-comp-ph, physics.comp-ph<br>Keyword Score: 23<br>Keywords: Benchmarking, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06362v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06362v1.pdf filename=2403.06362v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The techniques used to generate pseudo-random numbers for Monte Carlo (MC) applications bear many implications on the quality and speed of that programs work. As a random number generator (RNG) slows, the production of random numbers begins to dominate runtime. As RNG output grows in correlation, the final product becomes less reliable. These difficulties are further compounded by the need for reproducibility and parallelism. For reproducibility, the numbers generated to determine any outcome must be the same each time a <b>simulation</b> is run. However, the concurrency that comes with most parallelism introduces race conditions. To have both reproducibility and concurrency, separate RNG states must be tracked for each independently schedulable unit of <b>simulation,</b> forming independent random number streams. We propose an alternative to the stride-based parallel LCG seeding approach that scales more practically with increased concurrency and workload by generating seeds through hashing and allowing for repeated outputs. Data gathered from normality tests of tally results from simple MC transport <b>benchmark</b> calculations indicates that the proposed hash-based RNG does not significantly affect the tally result normality property as compared to the conventional stride-based RNG.</p></p class="citation"></blockquote><h2 id=mathna-2>math.NA (2)</h2><h3 id=12--295318-a-method-for-accelerating-low-precision-operations-by-sparse-matrix-multiplication-hongyaoxing-gu-2024>(1/2 | 295/318) A method for accelerating low precision operations by sparse matrix multiplication (Hongyaoxing Gu, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hongyaoxing Gu. (2024)<br><strong>A method for accelerating low precision operations by sparse matrix multiplication</strong><br><button class=copy-to-clipboard title="A method for accelerating low precision operations by sparse matrix multiplication" index=295>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-295 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA<br>Keyword Score: 20<br>Keywords: Quantization, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06924v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06924v1.pdf filename=2403.06924v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, the fervent demand for computational power across various domains has <b>prompted</b> hardware manufacturers to introduce specialized computing hardware aimed at enhancing computational capabilities. Particularly, the utilization of tensor hardware supporting low precision has gained increasing prominence in scientific research. However, the use of low-precision tensor hardware for computational acceleration often introduces errors, posing a fundamental challenge of simultaneously achieving effective acceleration while maintaining computational accuracy. This paper proposes improvements in the methodology by incorporating low-precision <b>quantization</b> and employing a residual matrix for error correction and combines vector-wise <b>quantization</b> method.. The key innovation lies in the use of sparse matrices instead of dense matrices when compensating for errors with a residual matrix. By focusing solely on values that may significantly impact relative errors under a specified threshold, this approach aims to control <b>quantization</b> errors while reducing computational complexity. Experimental results demonstrate that this method can effectively control the <b>quantization</b> error while maintaining high acceleration effect.The improved algorithm on the CPU can achieve up to 15% accuracy improvement while 1.46 times speed improvement.</p></p class="citation"></blockquote><h3 id=22--296318-a-functionally-connected-element-method-for-solving-boundary-value-problems-jielin-yang-et-al-2024>(2/2 | 296/318) A Functionally Connected Element Method for Solving Boundary Value Problems (Jielin Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jielin Yang, Suchuan Dong. (2024)<br><strong>A Functionally Connected Element Method for Solving Boundary Value Problems</strong><br><button class=copy-to-clipboard title="A Functionally Connected Element Method for Solving Boundary Value Problems" index=296>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-296 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA, physics-flu-dyn<br>Keyword Score: 10<br>Keywords: Stemming<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06393v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06393v1.pdf filename=2403.06393v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present the general forms of piece-wise functions on partitioned domains satisfying an intrinsic $C^0$ or $C^1$ continuity across the sub-domain boundaries. These general forms are constructed based on a strategy <b>stemming</b> from the theory of functional connections, and we refer to partitioned domains endowed with these general forms as functionally connected elements (FCE). We further present a method, incorporating functionally connected elements and a least squares collocation approach, for solving boundary and initial value problems. This method exhibits a spectral-like accuracy, with the free functions involved in the FCE form represented by polynomial bases or by non-polynomial bases of quasi-random sinusoidal functions. The FCE method offers a unique advantage over traditional element-based methods for boundary value problems involving relative boundary conditions. A number of linear and nonlinear numerical examples in one and two dimensions are presented to demonstrate the performance of the FCE method developed herein.</p></p class="citation"></blockquote><h2 id=csce-2>cs.CE (2)</h2><h3 id=12--297318-numerical-simulation-of-individual-coil-placement----a-proof-of-concept-study-for-the-prediction-of-recurrence-after-aneurysm-coiling-julian-schwarting-et-al-2024>(1/2 | 297/318) Numerical simulation of individual coil placement &ndash; A proof-of-concept study for the prediction of recurrence after aneurysm coiling (Julian Schwarting et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Julian Schwarting, Fabian Holzberger, Markus Muhr, Martin Renz, Tobias Boeckh-Behrens, Barbara Wohlmuth, Jan Kirschke. (2024)<br><strong>Numerical simulation of individual coil placement &ndash; A proof-of-concept study for the prediction of recurrence after aneurysm coiling</strong><br><button class=copy-to-clipboard title="Numerical simulation of individual coil placement -- A proof-of-concept study for the prediction of recurrence after aneurysm coiling" index=297>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-297 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CE<br>Categories: cs-CE, cs.CE<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06889v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06889v1.pdf filename=2403.06889v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Rupture of intracranial aneurysms results in severe subarachnoidal hemorrhage, which is associated with high morbidity and mortality. Neurointerventional occlusion of the aneurysm through coiling has evolved to a therapeutical standard. The choice of the specific coil has an important influence on secondary regrowth requiring retreatment. Aneurysm occlusion was simulated either through virtual implantation of a preshaped 3D coil or with a porous media approach. In this study, we used a recently developed numerical approach to simulate aneurysm shapes in specific challenging aneurysm anatomies and correlated these with aneurysm recurrence 6 months after treatment. The <b>simulation</b> showed a great variety of coil shapes depending on the variability in possible microcatheter positions. Aneurysms with a later recurrence showed a tendency for more successful coiling attempts. Results revealed further trends suggesting lower simulated packing densities in aneurysms with reoccurrence. Simulated packing densities did not correlate with those calculated by conventional software, indicating the potential for our approach to offer additional predictive value. Our study, therefore, pioneers a comprehensive numerical model for simulating aneurysm coiling, providing insights into individualized treatment strategies and outcome prediction. Future directions involve expanding the model&rsquo;s capabilities to simulate intraprocedural outcomes and long-term predictions, aiming to refine occlusion quality criteria and validate prediction parameters in larger patient cohorts. This <b>simulation</b> framework holds promise for enhancing clinical decision-making and optimizing patient outcomes in endovascular aneurysm treatment.</p></p class="citation"></blockquote><h3 id=22--298318-when-crypto-economics-meet-graph-analytics-and-learning-bingqiao-luo-2024>(2/2 | 298/318) When Crypto Economics Meet Graph Analytics and Learning (Bingqiao Luo, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bingqiao Luo. (2024)<br><strong>When Crypto Economics Meet Graph Analytics and Learning</strong><br><button class=copy-to-clipboard title="When Crypto Economics Meet Graph Analytics and Learning" index=298>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-298 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CE<br>Categories: cs-CE, cs.CE<br>Keyword Score: 13<br>Keywords: Graph, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06454v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06454v1.pdf filename=2403.06454v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Utilizing <b>graph</b> analytics and learning has proven to be an effective method for exploring aspects of crypto economics such as network effects, decentralization, tokenomics, and fraud detection. However, the majority of existing research predominantly focuses on leading cryptocurrencies, namely Bitcoin (BTC) and Ethereum (ETH), overlooking the vast diversity among the more than 10,000 cryptocurrency projects. This oversight may result in skewed insights. In our paper, we aim to broaden the scope of investigation to encompass the entire spectrum of cryptocurrencies, examining various coins across their entire life cycles. Furthermore, we intend to pioneer advanced methodologies, including <b>graph</b> <b>transfer</b> <b>learning</b> and the innovative concept of <b>&ldquo;graph</b> of <b>graphs&rdquo;.</b> By extending our research beyond the confines of BTC and ETH, our goal is to enhance the depth of our understanding of crypto economics and to advance the development of more intricate <b>graph-based</b> techniques.</p></p class="citation"></blockquote><h2 id=csit-2>cs.IT (2)</h2><h3 id=12--299318-on-the-secrecy-rate-of-in-band-full-duplex-two-way-wiretap-channel-navneet-garg-et-al-2024>(1/2 | 299/318) On the Secrecy Rate of In-Band Full-duplex Two-way Wiretap Channel (Navneet Garg et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Navneet Garg, Haifeng Luo, Tharmalingam Ratnarajah. (2024)<br><strong>On the Secrecy Rate of In-Band Full-duplex Two-way Wiretap Channel</strong><br><button class=copy-to-clipboard title="On the Secrecy Rate of In-Band Full-duplex Two-way Wiretap Channel" index=299>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-299 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06720v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06720v1.pdf filename=2403.06720v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we consider a two-way wiretap Multi-Input Multi-Output Multi-antenna Eve (MIMOME) channel, where both nodes (Alice and Bob) transmit and receive in an in-band full-duplex (IBFD) manner. For this system with keyless security, we provide a novel artificial noise (AN) based signal design, where the AN is injected in both signal and null spaces. We present an ergodic secrecy rate approximation to derive the power allocation algorithm. We consider scenarios where AN is known and unknown to legitimate users and include imperfect channel information effects. To maximize secrecy rates subject to the transmit power constraint, a two-step power allocation solution is proposed, where the first step is known at Eve, and the second step helps to improve the secrecy further. We also consider scenarios where partial information is known by Eve and the effects of non-ideal self-interference cancellation. The usefulness and limitations of the resulting power allocation solution are analyzed and verified via <b>simulations.</b> Results show that secrecy rates are less when AN is unknown to receivers or Eve has more information about legitimate users. Since the ergodic approximation only considers Eves distance, the resulting power allocation provides secrecy rates close to the actual ones.</p></p class="citation"></blockquote><h3 id=22--300318-a-multiscale-cavity-method-for-sublinear-rank-symmetric-matrix-factorization-jean-barbier-et-al-2024>(2/2 | 300/318) A multiscale cavity method for sublinear-rank symmetric matrix factorization (Jean Barbier et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jean Barbier, Justin Ko, Anas A. Rahman. (2024)<br><strong>A multiscale cavity method for sublinear-rank symmetric matrix factorization</strong><br><button class=copy-to-clipboard title="A multiscale cavity method for sublinear-rank symmetric matrix factorization" index=300>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-300 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cond-mat-dis-nn, cs-IT, cs.IT, math-IT, math-MP, math-ST, math-ph, stat-TH<br>Keyword Score: 10<br>Keywords: Mutual Information<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07189v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07189v1.pdf filename=2403.07189v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider a statistical model for symmetric matrix factorization with additive Gaussian noise in the high-dimensional regime where the rank $M$ of the signal matrix to infer scales with its size $N$ as $M = o(N^{1/10})$. Allowing for a $N$-dependent rank offers new challenges and requires new methods. Working in the Bayesian-optimal setting, we show that whenever the signal has i.i.d. entries the limiting <b>mutual</b> <b>information</b> between signal and data is given by a variational formula involving a rank-one replica symmetric potential. In other words, from the information-theoretic perspective, the case of a (slowly) growing rank is the same as when $M = 1$ (namely, the standard spiked Wigner model). The proof is primarily based on a novel multiscale cavity method allowing for growing rank along with some information-theoretic identities on worst noise for the Gaussian vector channel. We believe that the cavity method developed here will play a role in the analysis of a broader class of inference and spin models where the degrees of freedom are large arrays instead of vectors.</p></p class="citation"></blockquote><h2 id=mathdg-2>math.DG (2)</h2><h3 id=12--301318-asymptotic-behavior-of-unstable-perturbations-of-the-fubini-study-metric-in-ricci-flow-david-garfinkle-et-al-2024>(1/2 | 301/318) Asymptotic behavior of unstable perturbations of the Fubini-Study metric in Ricci flow (David Garfinkle et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>David Garfinkle, James Isenberg, Dan Knopf, Haotian Wu. (2024)<br><strong>Asymptotic behavior of unstable perturbations of the Fubini-Study metric in Ricci flow</strong><br><button class=copy-to-clipboard title="Asymptotic behavior of unstable perturbations of the Fubini-Study metric in Ricci flow" index=301>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-301 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.DG<br>Categories: cs-NA, math-AP, math-DG, math-NA, math.DG<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06427v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06427v1.pdf filename=2403.06427v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Kr"oncke has shown that the Fubini-Study metric is an unstable generalized stationary solution of Ricci flow [Kr"o20]. In this paper, we carry out numerical <b>simulations</b> which indicate that Ricci flow solutions originating at unstable perturbations of the Fubini-Study metric develop local singularities modeled by the blowdown soliton discovered in [FIK03].</p></p class="citation"></blockquote><h3 id=22--302318-pulling-back-symmetric-riemannian-geometry-for-data-analysis-willem-diepeveen-2024>(2/2 | 302/318) Pulling back symmetric Riemannian geometry for data analysis (Willem Diepeveen, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Willem Diepeveen. (2024)<br><strong>Pulling back symmetric Riemannian geometry for data analysis</strong><br><button class=copy-to-clipboard title="Pulling back symmetric Riemannian geometry for data analysis" index=302>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-302 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.DG<br>Categories: 53Z50, 53C35, 53C22, cs-LG, math-DG, math.DG<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06612v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06612v1.pdf filename=2403.06612v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Data sets tend to live in low-dimensional non-linear subspaces. Ideal data analysis tools for such data sets should therefore account for such non-linear <b>geometry.</b> The symmetric Riemannian <b>geometry</b> setting can be suitable for a variety of reasons. First, it comes with a rich mathematical structure to account for a wide range of non-linear geometries that has been shown to be able to capture the data <b>geometry</b> through empirical evidence from classical non-linear embedding. Second, many standard data analysis tools initially developed for data in Euclidean space can also be generalised efficiently to data on a symmetric Riemannian manifold. A conceptual challenge comes from the lack of guidelines for constructing a symmetric Riemannian structure on the data space itself and the lack of guidelines for modifying successful algorithms on symmetric Riemannian manifolds for data analysis to this setting. This work considers these challenges in the setting of pullback Riemannian <b>geometry</b> through a diffeomorphism. The first part of the paper characterises diffeomorphisms that result in proper, stable and efficient data analysis. The second part then uses these best practices to guide construction of such diffeomorphisms through deep learning. As a proof of concept, different types of pullback geometries &ndash; among which the proposed construction &ndash; are tested on several data analysis tasks and on several toy data sets. The numerical experiments confirm the predictions from theory, i.e., that the diffeomorphisms generating the pullback <b>geometry</b> need to map the data manifold into a geodesic subspace of the pulled back Riemannian manifold while preserving local isometry around the data manifold for proper, stable and efficient data analysis, and that pulling back positive curvature can be problematic in terms of stability.</p></p class="citation"></blockquote><h2 id=statml-2>stat.ML (2)</h2><h3 id=12--303318-bridging-domains-with-approximately-shared-features-ziliang-samuel-zhong-et-al-2024>(1/2 | 303/318) Bridging Domains with Approximately Shared Features (Ziliang Samuel Zhong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ziliang Samuel Zhong, Xiang Pan, Qi Lei. (2024)<br><strong>Bridging Domains with Approximately Shared Features</strong><br><button class=copy-to-clipboard title="Bridging Domains with Approximately Shared Features" index=303>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-303 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-CV, cs-LG, stat-ML, stat.ML<br>Keyword Score: 20<br>Keywords: Fine-tuning, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06424v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06424v1.pdf filename=2403.06424v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multi-source <b>domain</b> <b>adaptation</b> aims to reduce performance degradation when applying machine learning models to unseen <b>domains.</b> <b>A</b> fundamental challenge is devising the optimal strategy for feature selection. Existing literature is somewhat paradoxical: some advocate for learning invariant features from source <b>domains,</b> <b>while</b> others favor more diverse features. To address the challenge, we propose a statistical framework that distinguishes the utilities of features based on the variance of their correlation to label $y$ across <b>domains.</b> <b>Under</b> our framework, we design and analyze a learning procedure consisting of learning approximately shared feature representation from source tasks and <b>fine-tuning</b> it on the target task. Our theoretical analysis necessitates the importance of learning approximately shared features instead of only the strictly invariant features and yields an improved population risk compared to previous results on both source and target tasks, thus partly resolving the paradox mentioned above. Inspired by our theory, we proposed a more practical way to isolate the content (invariant+approximately shared) from environmental features and further consolidate our theoretical findings.</p></p class="citation"></blockquote><h3 id=22--304318-provable-mutual-benefits-from-federated-learning-in-privacy-sensitive-domains-nikita-tsoy-et-al-2024>(2/2 | 304/318) Provable Mutual Benefits from Federated Learning in Privacy-Sensitive Domains (Nikita Tsoy et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nikita Tsoy, Anna Mihalkova, Teodora Todorova, Nikola Konstantinov. (2024)<br><strong>Provable Mutual Benefits from Federated Learning in Privacy-Sensitive Domains</strong><br><button class=copy-to-clipboard title="Provable Mutual Benefits from Federated Learning in Privacy-Sensitive Domains" index=304>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-304 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-CR, cs-GT, cs-LG, stat-ML, stat.ML<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06672v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06672v1.pdf filename=2403.06672v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Cross-silo <b>federated</b> <b>learning</b> (FL) allows data owners to train accurate machine learning models by benefiting from each others private datasets. Unfortunately, the model accuracy benefits of collaboration are often undermined by privacy defenses. Therefore, to incentivize client participation in privacy-sensitive domains, a FL protocol should strike a delicate balance between privacy guarantees and end-model accuracy. In this paper, we study the question of when and how a server could design a FL protocol provably beneficial for all participants. First, we provide necessary and sufficient conditions for the existence of mutually beneficial protocols in the context of mean estimation and convex stochastic optimization. We also derive protocols that maximize the total clients&rsquo; utility, given symmetric privacy preferences. Finally, we design protocols maximizing end-model accuracy and demonstrate their benefits in synthetic experiments.</p></p class="citation"></blockquote><h2 id=physicsdata-an-1>physics.data-an (1)</h2><h3 id=11--305318-process-signature-driven-high-spatio-temporal-resolution-alignment-of-multimodal-data-abhishek-hanchate-et-al-2024>(1/1 | 305/318) Process signature-driven high spatio-temporal resolution alignment of multimodal data (Abhishek Hanchate et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Abhishek Hanchate, Himanshu Balhara, Vishal S. Chindepalli, Satish T. S. Bukkapatnam. (2024)<br><strong>Process signature-driven high spatio-temporal resolution alignment of multimodal data</strong><br><button class=copy-to-clipboard title="Process signature-driven high spatio-temporal resolution alignment of multimodal data" index=305>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-305 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.data-an<br>Categories: cs-LG, physics-app-ph, physics-data-an, physics.data-an<br>Keyword Score: 16<br>Keywords: Multi-modal, Multi-modal, Neural Machine Translation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06888v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06888v2.pdf filename=2403.06888v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present HiRA-Pro, a novel procedure to align, at high spatio-temporal resolutions, <b>multimodal</b> signals from real-world processes and systems that exhibit diverse transient, nonlinear stochastic dynamics, such as manufacturing machines. It is based on discerning and synchronizing the process signatures of salient kinematic and dynamic events in these disparate signals. HiRA-Pro addresses the challenge of aligning data with sub-millisecond phenomena, where traditional timestamp, external trigger, or clock-based alignment methods fall short. The effectiveness of HiRA-Pro is demonstrated in a smart manufacturing context, where it aligns data from 13+ channels acquired during 3D-printing and milling operations on an Optomec-LENS <b>MTS</b> 500 hybrid machine. The aligned data is then voxelized to generate 0.25 second aligned data chunks that correspond to physical voxels on the produced part. The superiority of HiRA-Pro is further showcased through case studies in additive manufacturing, demonstrating improved machine learning-based predictive performance due to precise <b>multimodal</b> data alignment. Specifically, testing classification accuracies improved by almost 35% with the application of HiRA-Pro, even with limited data, allowing for precise localization of artifacts. The paper also provides a comprehensive discussion on the proposed method, its applications, and comparative qualitative analysis with a few other alignment methods. HiRA-Pro achieves temporal-spatial resolutions of 10-1000 us and 100 um in order to generate datasets that register with physical voxels on the 3D-printed and milled part. These resolutions are at least an order of magnitude finer than the existing alignment methods that employ individual timestamps, statistical correlations, or common clocks, which achieve precision of hundreds of milliseconds.</p></p class="citation"></blockquote><h2 id=cscg-1>cs.CG (1)</h2><h3 id=11--306318-an-algorithm-for-correct-computation-of-reeb-spaces-for-pl-bivariate-fields-amit-chattopadhyay-et-al-2024>(1/1 | 306/318) An Algorithm for Correct Computation of Reeb Spaces for PL Bivariate Fields (Amit Chattopadhyay et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amit Chattopadhyay, Yashwanth Ramamurthi, Osamu Saeki. (2024)<br><strong>An Algorithm for Correct Computation of Reeb Spaces for PL Bivariate Fields</strong><br><button class=copy-to-clipboard title="An Algorithm for Correct Computation of Reeb Spaces for PL Bivariate Fields" index=306>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-306 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CG<br>Categories: cs-CG, cs-DS, cs.CG<br>Keyword Score: 13<br>Keywords: Graph, Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06564v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06564v1.pdf filename=2403.06564v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Reeb space is a topological structure which is a generalization of the notion of the Reeb <b>graph</b> to multi-fields. Its effectiveness has been established in revealing topological features in data across diverse computational domains which cannot be identified using the Reeb <b>graph</b> or other scalar-topology-based methods. Approximations of Reeb spaces such as the Mapper and the Joint Contour Net have been developed based on <b>quantization</b> of the range. However, computing the topologically correct Reeb space dispensing the range-quantization is a challenging problem. In the current paper, we develop an algorithm for computing a correct net-like approximation corresponding to the Reeb space of a generic piecewise-linear (PL) bivariate field based on a multi-dimensional Reeb <b>graph</b> (MDRG). First, we prove that the Reeb space is homeomorphic to its MDRG. Subsequently, we introduce an algorithm for computing the MDRG of a generic PL bivariate field through the computation of its Jacobi set and Jacobi structure, a projection of the Jacobi set into the Reeb space. This marks the first algorithm for MDRG computation without requiring the <b>quantization</b> of bivariate fields. Following this, we compute a net-like structure embedded in the corresponding Reeb space using the MDRG and the Jacobi structure. We provide the proof of correctness and complexity analysis of our algorithm.</p></p class="citation"></blockquote><h2 id=csgt-1>cs.GT (1)</h2><h3 id=11--307318-new-perspectives-in-online-contract-design-heterogeneous-homogeneous-non-myopic-agents-and-team-production-shiliang-zuo-2024>(1/1 | 307/318) New Perspectives in Online Contract Design: Heterogeneous, Homogeneous, Non-myopic Agents and Team Production (Shiliang Zuo, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shiliang Zuo. (2024)<br><strong>New Perspectives in Online Contract Design: Heterogeneous, Homogeneous, Non-myopic Agents and Team Production</strong><br><button class=copy-to-clipboard title="New Perspectives in Online Contract Design: Heterogeneous, Homogeneous, Non-myopic Agents and Team Production" index=307>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-307 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-GT, cs-LG, cs.GT<br>Keyword Score: 10<br>Keywords: Bandit Algorithm<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07143v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07143v1.pdf filename=2403.07143v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work studies the repeated principal-agent problem from an online learning perspective. The principal&rsquo;s goal is to learn the optimal contract that maximizes her utility through repeated interactions, without prior knowledge of the agent&rsquo;s type (i.e., the agent&rsquo;s cost and production functions). I study three different settings when the principal contracts with a $\textit{single}$ agent each round: 1. The agents are heterogeneous; 2. the agents are homogenous; 3. the principal interacts with the same agent and the agent is non-myopic. I present different approaches and techniques for designing learning algorithms in each setting. For heterogeneous agent types, I identify a condition that allows the problem to be reduced to Lipschitz <b>bandits</b> directly. For identical agents, I give a polynomial sample complexity scheme to learn the optimal contract based on inverse game theory. For strategic non-myopic agents, I design a low strategic-regret mechanism. Also, I identify a connection between linear contracts and posted-price auctions, showing the two can be reduced to one another, and give a regret lower bound on learning the optimal linear contract based on this observation. I also study a $\textit{team production}$ model. I identify a condition under which the principal&rsquo;s learning problem can be reformulated as solving a family of convex programs, thereby showing the optimal contract can be found efficiently.</p></p class="citation"></blockquote><h2 id=mathlo-1>math.LO (1)</h2><h3 id=11--308318-a-study-on-actions-for-atomic-logics-raül-espejo-boix-2024>(1/1 | 308/318) A Study on Actions for Atomic Logics (Raül Espejo-Boix, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Raül Espejo-Boix. (2024)<br><strong>A Study on Actions for Atomic Logics</strong><br><button class=copy-to-clipboard title="A Study on Actions for Atomic Logics" index=308>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-308 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.LO<br>Categories: cs-LO, math-LO, math.LO<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07948v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07948v1.pdf filename=2403.07948v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Nowadays there is a large number of non-classical logics, each one best suited for <b>reasoning</b> about some issues in abstract fields, such as linguistics or epistemology, among others. Proving interesting properties for each one of them supposes a big workload for logicians and computer scientists. We want an approach into this problematic that is modular. To adress this issue, the report shows new insights in the construction of Atomic Logics introduced by Guillaume Aucher. Atomic Logics let us represent very general left and right introduction rules and they come along a new kind of rules based on display logics and residuation. A new approach is taken into the definition of Atomic Logics, which is now built on a class of actions for which we prove cut-elimination. We show that some of them are equivalent to Aucher&rsquo;s Atomic Logics and we prove cut-elimination and Craig Interpolation for a class of them. The introduced theory is applied to the non-associative Lambek Calculus throughout the report. It is accompanied by a computer-checked formalisation of the original syntax in the proof assistant Coq.</p></p class="citation"></blockquote><h2 id=mathoc-1>math.OC (1)</h2><h3 id=11--309318-last-iterate-convergence-of-incremental-methods-and-applications-in-continual-learning-xufeng-cai-et-al-2024>(1/1 | 309/318) Last Iterate Convergence of Incremental Methods and Applications in Continual Learning (Xufeng Cai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xufeng Cai, Jelena Diakonikolas. (2024)<br><strong>Last Iterate Convergence of Incremental Methods and Applications in Continual Learning</strong><br><button class=copy-to-clipboard title="Last Iterate Convergence of Incremental Methods and Applications in Continual Learning" index=309>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-309 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-LG, math-OC, math.OC<br>Keyword Score: 10<br>Keywords: Continual Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06873v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06873v1.pdf filename=2403.06873v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Incremental gradient methods and incremental proximal methods are a fundamental class of optimization algorithms used for solving finite sum problems, broadly studied in the literature. Yet, when it comes to their convergence guarantees, nonasymptotic (first-order or proximal) oracle complexity bounds have been obtained fairly recently, almost exclusively applying to the average iterate. Motivated by applications in <b>continual</b> <b>learning,</b> we obtain the first convergence guarantees for the last iterate of both incremental gradient and incremental proximal methods, in general convex smooth (for both) and convex Lipschitz (for the proximal variants) settings. Our oracle complexity bounds for the last iterate nearly match (i.e., match up to a square-root-log or a log factor) the best known oracle complexity bounds for the average iterate, for both classes of methods. We further obtain generalizations of our results to weighted averaging of the iterates with increasing weights, which can be seen as interpolating between the last iterate and the average iterate guarantees. Additionally, we discuss how our results can be generalized to variants of studied incremental methods with permuted ordering of updates. Our results generalize last iterate guarantees for incremental methods compared to state of the art, as such results were previously known only for overparameterized linear models, which correspond to convex quadratic problems with infinitely many solutions.</p></p class="citation"></blockquote><h2 id=cspl-1>cs.PL (1)</h2><h3 id=11--310318-automatic-generation-of-python-programs-using-context-free-grammars-kamel-yamani-et-al-2024>(1/1 | 310/318) Automatic Generation of Python Programs Using Context-Free Grammars (Kamel Yamani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kamel Yamani, Marwa Naïr, Riyadh Baghdadi. (2024)<br><strong>Automatic Generation of Python Programs Using Context-Free Grammars</strong><br><button class=copy-to-clipboard title="Automatic Generation of Python Programs Using Context-Free Grammars" index=310>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-310 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.PL<br>Categories: cs-CL, cs-LG, cs-PL, cs.PL<br>Keyword Score: 10<br>Keywords: Code Generation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06503v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06503v1.pdf filename=2403.06503v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, data has emerged as the new gold, serving as a powerful tool for creating intelligent systems. However, procuring high-quality data remains challenging, especially for <b>code.</b> <b>To</b> address this, we developed TinyPy Generator, a tool that generates random Python programs using a context-free grammar. The generated programs are guaranteed to be correct by construction. Our system uses custom production rules (in the Backus-Naur Form (BNF) format) to recursively generate <b>code.</b> <b>This</b> allows us to generate <b>code</b> <b>with</b> different levels of complexity, ranging from <b>code</b> <b>containing</b> only assignments to more complex <b>code</b> <b>containing</b> conditionals and loops. Our proposed tool enables effortless large-scale Python <b>code</b> <b>generation,</b> beneficial for a wide range of applications. TinyPy Generator is particularly useful in the field of machine learning, where it can generate substantial amounts of Python <b>code</b> <b>for</b> training Python language models. Additionally, researchers who are studying programming languages can utilize this tool to create datasets for their experiments, which can help validate the robustness of <b>code</b> <b>interpreters</b> or compilers. Unlike existing research, we have open-sourced our implementation. This allows customization according to user needs and extends potential usage to other languages.</p></p class="citation"></blockquote><h2 id=cssi-1>cs.SI (1)</h2><h3 id=11--311318-hierarchical-cutting-of-complex-networks-performed-by-random-walks-alexandre-benatti-et-al-2024>(1/1 | 311/318) Hierarchical Cutting of Complex Networks Performed by Random Walks (Alexandre Benatti et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alexandre Benatti, Luciano da F. Costa. (2024)<br><strong>Hierarchical Cutting of Complex Networks Performed by Random Walks</strong><br><button class=copy-to-clipboard title="Hierarchical Cutting of Complex Networks Performed by Random Walks" index=311>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-311 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-SI, cs.SI, physics-soc-ph<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06876v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06876v1.pdf filename=2403.06876v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Several interesting approaches have been reported in the literature on complex networks, random walks, and hierarchy of <b>graphs.</b> While many of these works perform random walks on stable, fixed networks, in the present work we address the situation in which the connections traversed by each step of a uniformly random walks are progressively removed, yielding a successively less interconnected structure that may break into two components, therefore establishing a respective hierarchy. The sizes of each of these pairs of sliced networks, as well as the permanence of each connected component, are studied in the present work. Several interesting results are reported, including the tendency of geometrical networks sometimes to be broken into two components with comparable large sizes.</p></p class="citation"></blockquote><h2 id=mathco-3>math.CO (3)</h2><h3 id=13--312318-c_2k1-coloring-of-bounded-diameter-graphs-marta-piecyk-2024>(1/3 | 312/318) $C_{2k+1}$-coloring of bounded-diameter graphs (Marta Piecyk, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marta Piecyk. (2024)<br><strong>$C_{2k+1}$-coloring of bounded-diameter graphs</strong><br><button class=copy-to-clipboard title="$C_{2k+1}$-coloring of bounded-diameter graphs" index=312>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-312 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.CO<br>Categories: cs-CC, math-CO, math.CO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06694v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06694v1.pdf filename=2403.06694v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>For a fixed <b>graph</b> $H$, in the <b>graph</b> homomorphism problem, denoted by $Hom(H)$, we are given a <b>graph</b> $G$ and we have to determine whether there exists an edge-preserving mapping $\varphi: V(G) \to V(H)$. Note that $Hom(C_3)$, where $C_3$ is the cycle of length $3$, is equivalent to $3$-Coloring. The question whether $3$-Coloring is polynomial-time solvable on diameter-$2$ <b>graphs</b> is a well-known open problem. In this paper we study the $Hom(C_{2k+1})$ problem on bounded-diameter <b>graphs</b> for $k\geq 2$, so we consider all other odd cycles than $C_3$. We prove that for $k\geq 2$, the $Hom(C_{2k+1})$ problem is polynomial-time solvable on diameter-$(k+1)$ <b>graphs</b> &ndash; note that such a result for $k=1$ would be precisely a polynomial-time algorithm for $3$-Coloring of diameter-$2$ <b>graphs.</b> Furthermore, we give subexponential-time algorithms for diameter-$(k+2)$ and -$(k+3)$ <b>graphs.</b> We complement these results with a lower bound for diameter-$(2k+2)$ <b>graphs</b> &ndash; in this class of <b>graphs</b> the $Hom(C_{2k+1})$ problem is NP-hard and cannot be solved in subexponential-time, unless the ETH fails. Finally, we consider another direction of generalizing $3$-Coloring on diameter-$2$ <b>graphs.</b> We consider other target <b>graphs</b> $H$ than odd cycles but we restrict ourselves to diameter $2$. We show that if $H$ is triangle-free, then $Hom(H)$ is polynomial-time solvable on diameter-$2$ <b>graphs.</b></p></p class="citation"></blockquote><h3 id=23--313318-a-lower-bound-for-secure-domination-number-of-an-outerplanar-graph-toru-araki-2024>(2/3 | 313/318) A Lower bound for Secure Domination Number of an Outerplanar Graph (Toru Araki, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Toru Araki. (2024)<br><strong>A Lower bound for Secure Domination Number of an Outerplanar Graph</strong><br><button class=copy-to-clipboard title="A Lower bound for Secure Domination Number of an Outerplanar Graph" index=313>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-313 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.CO<br>Categories: 05C69, 05C10, G-2-2, cs-DM, math-CO, math.CO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06493v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06493v2.pdf filename=2403.06493v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A subset $S$ of vertices in a <b>graph</b> $G$ is a secure dominating set of $G$ if $S$ is a dominating set of $G$ and, for each vertex $u \not\in S$, there is a vertex $v \in S$ such that $uv$ is an edge and $(S \setminus {v}) \cup {u}$ is also a dominating set of $G$. The secure domination number of $G$, denoted by $\gamma_{s}(G)$, is the cardinality of a smallest secure dominating sets of $G$. In this paper, we prove that for any outerplanar <b>graph</b> with $n \geq 4$ vertices, $\gamma_{s}(G) \geq (n+4)/5$ and the bound is tight.</p></p class="citation"></blockquote><h3 id=33--314318-tight-bound-for-the-erdős-pósa-property-of-tree-minors-vida-dujmović-et-al-2024>(3/3 | 314/318) Tight bound for the Erdős-Pósa property of tree minors (Vida Dujmović et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vida Dujmović, Gwenaël Joret, Piotr Micek, Pat Morin. (2024)<br><strong>Tight bound for the Erdős-Pósa property of tree minors</strong><br><button class=copy-to-clipboard title="Tight bound for the Erdős-Pósa property of tree minors" index=314>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-314 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.CO<br>Categories: cs-DM, math-CO, math.CO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06370v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06370v1.pdf filename=2403.06370v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Let $T$ be a tree on $t$ vertices. We prove that for every positive integer $k$ and every <b>graph</b> $G$, either $G$ contains $k$ pairwise vertex-disjoint subgraphs each having a $T$ minor, or there exists a set $X$ of at most $t(k-1)$ vertices of $G$ such that $G-X$ has no $T$ minor. The bound on the size of $X$ is best possible and improves on an earlier $f(t)k$ bound proved by Fiorini, Joret, and Wood (2013) with some very fast growing function $f(t)$. Moreover, our proof is very short and simple.</p></p class="citation"></blockquote><h2 id=csdm-1>cs.DM (1)</h2><h3 id=11--315318-approximating-maximum-edge-2-coloring-by-normalizing-graphs-tobias-mömke-et-al-2024>(1/1 | 315/318) Approximating Maximum Edge 2-Coloring by Normalizing Graphs (Tobias Mömke et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tobias Mömke, Alexandru Popa, Aida Roshany-Tabrizi, Michael Ruderer, Roland Vincze. (2024)<br><strong>Approximating Maximum Edge 2-Coloring by Normalizing Graphs</strong><br><button class=copy-to-clipboard title="Approximating Maximum Edge 2-Coloring by Normalizing Graphs" index=315>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-315 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DM<br>Categories: cs-DM, cs-DS, cs.DM<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06691v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06691v1.pdf filename=2403.06691v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In a simple, undirected <b>graph</b> G, an edge 2-coloring is a coloring of the edges such that no vertex is incident to edges with more than 2 distinct colors. The problem maximum edge 2-coloring (ME2C) is to find an edge 2-coloring in a <b>graph</b> G with the goal to maximize the number of colors. For a relevant <b>graph</b> class, ME2C models anti-Ramsey numbers and it was considered in network applications. For the problem a 2-approximation algorithm is known, and if the input <b>graph</b> has a perfect matching, the same algorithm has been shown to have a performance guarantee of 5/3. It is known that ME2C is APX-hard and that it is UG-hard to obtain an approximation ratio better than 1.5. We show that if the input <b>graph</b> has a perfect matching, there is a polynomial time 1.625-approximation and if the <b>graph</b> is claw-free or if the maximum degree of the input <b>graph</b> is at most three (i.e., the <b>graph</b> is subcubic), there is a polynomial time 1.5-approximation algorithm for ME2C</p></p class="citation"></blockquote><h2 id=mathst-1>math.ST (1)</h2><h3 id=11--316318-untangling-gaussian-mixtures-eva-fluck-et-al-2024>(1/1 | 316/318) Untangling Gaussian Mixtures (Eva Fluck et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Eva Fluck, Sandra Kiefer, Christoph Standke. (2024)<br><strong>Untangling Gaussian Mixtures</strong><br><button class=copy-to-clipboard title="Untangling Gaussian Mixtures" index=316>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-316 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.ST<br>Categories: 05C40, 62H30, 68R10, cs-DM, cs-LG, math-CO, math-ST, math.ST, stat-TH<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06671v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06671v1.pdf filename=2403.06671v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Tangles were originally introduced as a concept to formalize regions of high connectivity in <b>graphs.</b> In recent years, they have also been discovered as a link between structural <b>graph</b> theory and data science: when interpreting similarity in data sets as connectivity between points, finding clusters in the data essentially amounts to finding tangles in the underlying <b>graphs.</b> This paper further explores the potential of tangles in data sets as a means for a formal study of clusters. Real-world data often follow a normal distribution. Accounting for this, we develop a quantitative theory of tangles in data sets drawn from Gaussian mixtures. To this end, we equip the data with a <b>graph</b> structure that models similarity between the points and allows us to apply tangle theory to the data. We provide explicit conditions under which tangles associated with the marginal Gaussian distributions exist asymptotically almost surely. This can be considered as a sufficient formal criterion for the separabability of clusters in the data.</p></p class="citation"></blockquote><h2 id=csds-2>cs.DS (2)</h2><h3 id=12--317318-balanced-substructures-in-bicolored-graphs-p-s-ardra-et-al-2024>(1/2 | 317/318) Balanced Substructures in Bicolored Graphs (P. S. Ardra et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>P. S. Ardra, R. Krithika, Saket Saurabh, Roohani Sharma. (2024)<br><strong>Balanced Substructures in Bicolored Graphs</strong><br><button class=copy-to-clipboard title="Balanced Substructures in Bicolored Graphs" index=317>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-317 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: G-2-2, cs-CC, cs-DS, cs.DS<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06608v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06608v1.pdf filename=2403.06608v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>An edge-colored <b>graph</b> is said to be balanced if it has an equal number of edges of each color. Given a <b>graph</b> $G$ whose edges are colored using two colors and a positive integer $k$, the objective in the Edge Balanced Connected Subgraph problem is to determine if $G$ has a balanced connected subgraph containing at least $k$ edges. We first show that this problem is NP-complete and remains so even if the solution is required to be a tree or a path. Then, we focus on the parameterized complexity of Edge Balanced Connected Subgraph and its variants (where the balanced subgraph is required to be a path/tree) with respect to $k$ as the parameter. Towards this, we show that if a <b>graph</b> has a balanced connected subgraph/tree/path of size at least $k$, then it has one of size at least $k$ and at most $f(k)$ where $f$ is a linear function. We use this result combined with dynamic programming algorithms based on color coding and representative sets to show that Edge Balanced Connected Subgraph and its variants are FPT. Further, using polynomial-time reductions to the Multilinear Monomial Detection problem, we give faster randomized FPT algorithms for the problems. In order to describe these reductions, we define a combinatorial object called relaxed-subgraph. We define this object in such a way that balanced connected subgraphs, trees and paths are relaxed-subgraphs with certain properties. This object is defined in the spirit of branching walks known for the Steiner Tree problem and may be of independent interest.</p></p class="citation"></blockquote><h3 id=22--318318-arborescences-and-shortest-path-trees-when-colors-matter-p-s-ardra-et-al-2024>(2/2 | 318/318) Arborescences and Shortest Path Trees when Colors Matter (P. S. Ardra et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>P. S. Ardra, Jasine Babu, Kritika Kashyap, R. Krithika, Sreejith K. Pallathumadam, Deepak Rajendraprasad. (2024)<br><strong>Arborescences and Shortest Path Trees when Colors Matter</strong><br><button class=copy-to-clipboard title="Arborescences and Shortest Path Trees when Colors Matter" index=318>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-318 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: G-2-2, cs-CC, cs-DM, cs-DS, cs.DS<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06580v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06580v1.pdf filename=2403.06580v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Color-constrained subgraph problems are those where we are given an edge-colored (directed or undirected) <b>graph</b> and the task is to find a specific type of subgraph, like a spanning tree, an arborescence, a single-source shortest path tree, a perfect matching etc., with constraints on the number of edges of each color. Some of these problems, like color-constrained spanning tree, have elegant solutions and some of them, like color-constrained perfect matching, are longstanding open questions. In this work, we study color-constrained arborescences and shortest path trees. Computing a color-constrained shortest path tree on weighted digraphs turns out to be NP-hard in general but polynomial-time solvable when all cycles have positive weight. This polynomial-time solvability is due to the fact that the solution space is essentially the set of all color-constrained arborescences of a directed acyclic subgraph of the original <b>graph.</b> While finding color-constrained arborescence of digraphs is NP-hard in general, we give efficient algorithms when the input <b>graph</b> is acyclic. Consequently, a color-constrained shortest path tree on weighted digraphs having only positive weight cycles can be efficiently computed. Our algorithms also generalize to the problem of finding a color-constrained shortest path tree with minimum total weight. En route, we sight nice connections to colored matroids and color-constrained bases.</p></p class="citation"></blockquote></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/ title="arXiv @ 2024.03.12" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Prev</div><div class=next-prev-text>arXiv @ 2024.03.12</div></a></div><div class="col-md-6 next-article"><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/ title="arXiv @ 2024.03.14" class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>arXiv @ 2024.03.14</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#primary-categories>Primary Categories</a></li><li><a href=#keywords>Keywords</a></li><li><a href=#cscl-39>cs.CL (39)</a><ul><li><a href=#139--1318-mend-meta-demonstration-distillation-for-efficient-and-effective-in-context-learning-yichuan-li-et-al-2024>(1/39 | 1/318) MEND: Meta dEmonstratioN Distillation for Efficient and Effective In-Context Learning (Yichuan Li et al., 2024)</a></li><li><a href=#239--2318-development-of-a-reliable-and-accessible-caregiving-language-model-calm-bambang-parmanto-et-al-2024>(2/39 | 2/318) Development of a Reliable and Accessible Caregiving Language Model (CaLM) (Bambang Parmanto et al., 2024)</a></li><li><a href=#339--3318-alarm-align-language-models-via-hierarchical-rewards-modeling-yuhang-lai-et-al-2024>(3/39 | 3/318) ALaRM: Align Language Models via Hierarchical Rewards Modeling (Yuhang Lai et al., 2024)</a></li><li><a href=#439--4318-ra-isf-learning-to-answer-and-understand-from-retrieval-augmentation-via-iterative-self-feedback-yanming-liu-et-al-2024>(4/39 | 4/318) RA-ISF: Learning to Answer and Understand from Retrieval Augmentation via Iterative Self-Feedback (Yanming Liu et al., 2024)</a></li><li><a href=#539--5318-amharic-llama-and-llava-multimodal-llms-for-low-resource-languages-michael-andersland-2024>(5/39 | 5/318) Amharic LLaMA and LLaVA: Multimodal LLMs for Low Resource Languages (Michael Andersland, 2024)</a></li><li><a href=#639--6318-evolving-knowledge-distillation-with-large-language-models-and-active-learning-chengyuan-liu-et-al-2024>(6/39 | 6/318) Evolving Knowledge Distillation with Large Language Models and Active Learning (Chengyuan Liu et al., 2024)</a></li><li><a href=#739--7318-narrating-causal-graphs-with-large-language-models-atharva-phatak-et-al-2024>(7/39 | 7/318) Narrating Causal Graphs with Large Language Models (Atharva Phatak et al., 2024)</a></li><li><a href=#839--8318-exploring-large-language-models-and-hierarchical-frameworks-for-classification-of-large-unstructured-legal-documents-nishchal-prasad-et-al-2024>(8/39 | 8/318) Exploring Large Language Models and Hierarchical Frameworks for Classification of Large Unstructured Legal Documents (Nishchal Prasad et al., 2024)</a></li><li><a href=#939--9318-hybrid-human-llm-corpus-construction-and-llm-evaluation-for-rare-linguistic-phenomena-leonie-weissweiler-et-al-2024>(9/39 | 9/318) Hybrid Human-LLM Corpus Construction and LLM Evaluation for Rare Linguistic Phenomena (Leonie Weissweiler et al., 2024)</a></li><li><a href=#1039--10318-a-knowledge-injected-curriculum-pretraining-framework-for-question-answering-xin-lin-et-al-2024>(10/39 | 10/318) A Knowledge-Injected Curriculum Pretraining Framework for Question Answering (Xin Lin et al., 2024)</a></li><li><a href=#1139--11318-era-cot-improving-chain-of-thought-through-entity-relationship-analysis-yanming-liu-et-al-2024>(11/39 | 11/318) ERA-CoT: Improving Chain-of-Thought through Entity Relationship Analysis (Yanming Liu et al., 2024)</a></li><li><a href=#1239--12318-act-mnmt-auto-constriction-turning-for-multilingual-neural-machine-translation-shaojie-dai-et-al-2024>(12/39 | 12/318) ACT-MNMT Auto-Constriction Turning for Multilingual Neural Machine Translation (Shaojie Dai et al., 2024)</a></li><li><a href=#1339--13318-guiding-clinical-reasoning-with-large-language-models-via-knowledge-seeds-jiageng-wu-et-al-2024>(13/39 | 13/318) Guiding Clinical Reasoning with Large Language Models via Knowledge Seeds (Jiageng WU et al., 2024)</a></li><li><a href=#1439--14318-one-size-doesnt-fit-all-learning-how-many-examples-to-use-for-in-context-learning-for-improved-text-classification-manish-chandra-et-al-2024>(14/39 | 14/318) &lsquo;One size doesn&rsquo;t fit all&rsquo;: Learning how many Examples to use for In-Context Learning for Improved Text Classification (Manish Chandra et al., 2024)</a></li><li><a href=#1539--15318-the-power-of-noise-toward-a-unified-multi-modal-knowledge-graph-representation-framework-zhuo-chen-et-al-2024>(15/39 | 15/318) The Power of Noise: Toward a Unified Multi-modal Knowledge Graph Representation Framework (Zhuo Chen et al., 2024)</a></li><li><a href=#1639--16318-mrl-parsing-without-tears-the-case-of-hebrew-shaltiel-shmidman-et-al-2024>(16/39 | 16/318) MRL Parsing Without Tears: The Case of Hebrew (Shaltiel Shmidman et al., 2024)</a></li><li><a href=#1739--17318-conspemollm-conspiracy-theory-detection-using-an-emotion-based-large-language-model-zhiwei-liu-et-al-2024>(17/39 | 17/318) ConspEmoLLM: Conspiracy Theory Detection Using an Emotion-Based Large Language Model (Zhiwei Liu et al., 2024)</a></li><li><a href=#1839--18318-ac-eval-evaluating-ancient-chinese-language-understanding-in-large-language-models-yuting-wei-et-al-2024>(18/39 | 18/318) AC-EVAL: Evaluating Ancient Chinese Language Understanding in Large Language Models (Yuting Wei et al., 2024)</a></li><li><a href=#1939--19318-unsupervised-real-time-hallucination-detection-based-on-the-internal-states-of-large-language-models-weihang-su-et-al-2024>(19/39 | 19/318) Unsupervised Real-Time Hallucination Detection based on the Internal States of Large Language Models (Weihang Su et al., 2024)</a></li><li><a href=#2039--20318-click-a-benchmark-dataset-of-cultural-and-linguistic-intelligence-in-korean-eunsu-kim-et-al-2024>(20/39 | 20/318) CLIcK: A Benchmark Dataset of Cultural and Linguistic Intelligence in Korean (Eunsu Kim et al., 2024)</a></li><li><a href=#2139--21318-lstm-based-text-generation-a-study-on-historical-datasets-mustafa-abbas-hussein-hussein-et-al-2024>(21/39 | 21/318) LSTM-Based Text Generation: A Study on Historical Datasets (Mustafa Abbas Hussein Hussein et al., 2024)</a></li><li><a href=#2239--22318-monitoring-ai-modified-content-at-scale-a-case-study-on-the-impact-of-chatgpt-on-ai-conference-peer-reviews-weixin-liang-et-al-2024>(22/39 | 22/318) Monitoring AI-Modified Content at Scale: A Case Study on the Impact of ChatGPT on AI Conference Peer Reviews (Weixin Liang et al., 2024)</a></li><li><a href=#2339--23318-spa-towards-a-computational-friendly-cloud-base-and-on-devices-collaboration-seq2seq-personalized-generation-yanming-liu-et-al-2024>(23/39 | 23/318) SPA: Towards A Computational Friendly Cloud-Base and On-Devices Collaboration Seq2seq Personalized Generation (Yanming Liu et al., 2024)</a></li><li><a href=#2439--24318-naming-describing-and-quantifying-visual-objects-in-humans-and-llms-alberto-testoni-et-al-2024>(24/39 | 24/318) Naming, Describing, and Quantifying Visual Objects in Humans and LLMs (Alberto Testoni et al., 2024)</a></li><li><a href=#2539--25318-indicllmsuite-a-blueprint-for-creating-pre-training-and-fine-tuning-datasets-for-indian-languages-mohammed-safi-ur-rahman-khan-et-al-2024>(25/39 | 25/318) IndicLLMSuite: A Blueprint for Creating Pre-training and Fine-Tuning Datasets for Indian Languages (Mohammed Safi Ur Rahman Khan et al., 2024)</a></li><li><a href=#2639--26318-medkp-medical-dialogue-with-knowledge-enhancement-and-clinical-pathway-encoding-jiageng-wu-et-al-2024>(26/39 | 26/318) MedKP: Medical Dialogue with Knowledge Enhancement and Clinical Pathway Encoding (Jiageng Wu et al., 2024)</a></li><li><a href=#2739--27318-linguistic-structure-induction-from-language-models-omar-momen-2024>(27/39 | 27/318) Linguistic Structure Induction from Language Models (Omar Momen, 2024)</a></li><li><a href=#2839--28318-restoring-ancient-ideograph-a-multimodal-multitask-neural-network-approach-siyu-duan-et-al-2024>(28/39 | 28/318) Restoring Ancient Ideograph: A Multimodal Multitask Neural Network Approach (Siyu Duan et al., 2024)</a></li><li><a href=#2939--29318-thought-graph-generating-thought-process-for-biological-reasoning-chi-yang-hsu-et-al-2024>(29/39 | 29/318) Thought Graph: Generating Thought Process for Biological Reasoning (Chi-Yang Hsu et al., 2024)</a></li><li><a href=#3039--30318-multi-modal-semantic-understanding-with-contrastive-cross-modal-feature-alignment-ming-zhang-et-al-2024>(30/39 | 30/318) Multi-modal Semantic Understanding with Contrastive Cross-modal Feature Alignment (Ming Zhang et al., 2024)</a></li><li><a href=#3139--31318-academically-intelligent-llms-are-not-necessarily-socially-intelligent-ruoxi-xu-et-al-2024>(31/39 | 31/318) Academically intelligent LLMs are not necessarily socially intelligent (Ruoxi Xu et al., 2024)</a></li><li><a href=#3239--32318-improving-speaker-assignment-in-speaker-attributed-asr-for-real-meeting-applications-can-cui-et-al-2024>(32/39 | 32/318) Improving Speaker Assignment in Speaker-Attributed ASR for Real Meeting Applications (Can Cui et al., 2024)</a></li><li><a href=#3339--33318-glosslm-multilingual-pretraining-for-low-resource-interlinear-glossing-michael-ginn-et-al-2024>(33/39 | 33/318) GlossLM: Multilingual Pretraining for Low-Resource Interlinear Glossing (Michael Ginn et al., 2024)</a></li><li><a href=#3439--34318-cuentosie-can-a-chatbot-about-tales-with-a-message-help-to-teach-emotional-intelligence-antonio-ferrández-et-al-2024>(34/39 | 34/318) CuentosIE: can a chatbot about &rsquo;tales with a message&rsquo; help to teach emotional intelligence? (Antonio Ferrández et al., 2024)</a></li><li><a href=#3539--35318-the-pitfalls-of-next-token-prediction-gregor-bachmann-et-al-2024>(35/39 | 35/318) The pitfalls of next-token prediction (Gregor Bachmann et al., 2024)</a></li><li><a href=#3639--36318-on-the-consideration-of-ai-openness-can-good-intent-be-abused-yeeun-kim-et-al-2024>(36/39 | 36/318) On the Consideration of AI Openness: Can Good Intent Be Abused? (Yeeun Kim et al., 2024)</a></li><li><a href=#3739--37318-multilingual-turn-taking-prediction-using-voice-activity-projection-koji-inoue-et-al-2024>(37/39 | 37/318) Multilingual Turn-taking Prediction Using Voice Activity Projection (Koji Inoue et al., 2024)</a></li><li><a href=#3839--38318-a-logical-pattern-memory-pre-trained-model-for-entailment-tree-generation-li-yuan-et-al-2024>(38/39 | 38/318) A Logical Pattern Memory Pre-trained Model for Entailment Tree Generation (Li Yuan et al., 2024)</a></li><li><a href=#3939--39318-strength-lies-in-differences-towards-effective-non-collaborative-dialogues-via-tailored-strategy-planning-tong-zhang-et-al-2024>(39/39 | 39/318) Strength Lies in Differences! Towards Effective Non-collaborative Dialogues via Tailored Strategy Planning (Tong Zhang et al., 2024)</a></li></ul></li><li><a href=#cscv-97>cs.CV (97)</a><ul><li><a href=#197--40318-cam-back-again-large-kernel-cnns-from-a-weakly-supervised-object-localization-perspective-shunsuke-yasuki-et-al-2024>(1/97 | 40/318) CAM Back Again: Large Kernel CNNs from a Weakly Supervised Object Localization Perspective (Shunsuke Yasuki et al., 2024)</a></li><li><a href=#297--41318-selma-learning-and-merging-skill-specific-text-to-image-experts-with-auto-generated-data-jialu-li-et-al-2024>(2/97 | 41/318) SELMA: Learning and Merging Skill-Specific Text-to-Image Experts with Auto-Generated Data (Jialu Li et al., 2024)</a></li><li><a href=#397--42318-one-category-one-prompt-dataset-distillation-using-diffusion-models-ali-abbasi-et-al-2024>(3/97 | 42/318) One Category One Prompt: Dataset Distillation using Diffusion Models (Ali Abbasi et al., 2024)</a></li><li><a href=#497--43318-can-llms-tuning-methods-work-in-medical-multimodal-domain-jiawei-chen-et-al-2024>(4/97 | 43/318) Can LLMs&rsquo; Tuning Methods Work in Medical Multimodal Domain? (Jiawei Chen et al., 2024)</a></li><li><a href=#597--44318-leoclr-leveraging-original-images-for-contrastive-learning-of-visual-representations-mohammad-alkhalefi-et-al-2024>(5/97 | 44/318) LeOCLR: Leveraging Original Images for Contrastive Learning of Visual Representations (Mohammad Alkhalefi et al., 2024)</a></li><li><a href=#697--45318-enhanced-sparsification-via-stimulative-training-shengji-tang-et-al-2024>(6/97 | 45/318) Enhanced Sparsification via Stimulative Training (Shengji Tang et al., 2024)</a></li><li><a href=#797--46318-deep-learning-approaches-for-human-action-recognition-in-video-data-yufei-xie-2024>(7/97 | 46/318) Deep Learning Approaches for Human Action Recognition in Video Data (Yufei Xie, 2024)</a></li><li><a href=#897--47318-quanttune-optimizing-model-quantization-with-adaptive-outlier-driven-fine-tuning-jiun-man-chen-et-al-2024>(8/97 | 47/318) QuantTune: Optimizing Model Quantization with Adaptive Outlier-Driven Fine Tuning (Jiun-Man Chen et al., 2024)</a></li><li><a href=#997--48318-split-to-merge-unifying-separated-modalities-for-unsupervised-domain-adaptation-xinyao-li-et-al-2024>(9/97 | 48/318) Split to Merge: Unifying Separated Modalities for Unsupervised Domain Adaptation (Xinyao Li et al., 2024)</a></li><li><a href=#1097--49318-real-time-transformer-based-open-vocabulary-detection-with-efficient-fusion-head-tiancheng-zhao-et-al-2024>(10/97 | 49/318) Real-time Transformer-based Open-Vocabulary Detection with Efficient Fusion Head (Tiancheng Zhao et al., 2024)</a></li><li><a href=#1197--50318-divcon-divide-and-conquer-for-progressive-text-to-image-generation-yuhao-jia-et-al-2024>(11/97 | 50/318) DivCon: Divide and Conquer for Progressive Text-to-Image Generation (Yuhao Jia et al., 2024)</a></li><li><a href=#1297--51318-enhancing-image-caption-generation-using-reinforcement-learning-with-human-feedback-adarsh-n-l-et-al-2024>(12/97 | 51/318) Enhancing Image Caption Generation Using Reinforcement Learning with Human Feedback (Adarsh N L et al., 2024)</a></li><li><a href=#1397--52318-fsviewfusion-few-shots-view-generation-of-novel-objects-rukhshanda-hussain-et-al-2024>(13/97 | 52/318) FSViewFusion: Few-Shots View Generation of Novel Objects (Rukhshanda Hussain et al., 2024)</a></li><li><a href=#1497--53318-focusclip-multimodal-subject-level-guidance-for-zero-shot-transfer-in-human-centric-tasks-muhammad-saif-ullah-khan-et-al-2024>(14/97 | 53/318) FocusCLIP: Multimodal Subject-Level Guidance for Zero-Shot Transfer in Human-Centric Tasks (Muhammad Saif Ullah Khan et al., 2024)</a></li><li><a href=#1597--54318-gritv2-efficient-and-light-weight-social-relation-recognition-n-k-sagar-reddy-et-al-2024>(15/97 | 54/318) GRITv2: Efficient and Light-weight Social Relation Recognition (N K Sagar Reddy et al., 2024)</a></li><li><a href=#1697--55318-answering-diverse-questions-via-text-attached-with-key-audio-visual-clues-qilang-ye-et-al-2024>(16/97 | 55/318) Answering Diverse Questions via Text Attached with Key Audio-Visual Clues (Qilang Ye et al., 2024)</a></li><li><a href=#1797--56318-cross-domain-and-cross-dimension-learning-for-image-to-graph-transformers-alexander-h-berger-et-al-2024>(17/97 | 56/318) Cross-domain and Cross-dimension Learning for Image-to-Graph Transformers (Alexander H. Berger et al., 2024)</a></li><li><a href=#1897--57318-toward-generalist-anomaly-detection-via-in-context-residual-learning-with-few-shot-sample-prompts-jiawen-zhu-et-al-2024>(18/97 | 57/318) Toward Generalist Anomaly Detection via In-context Residual Learning with Few-shot Sample Prompts (Jiawen Zhu et al., 2024)</a></li><li><a href=#1997--58318-ensemble-quadratic-assignment-network-for-graph-matching-haoru-tan-et-al-2024>(19/97 | 58/318) Ensemble Quadratic Assignment Network for Graph Matching (Haoru Tan et al., 2024)</a></li><li><a href=#2097--59318-action-reimagined-text-to-pose-video-editing-for-dynamic-human-actions-lan-wang-et-al-2024>(20/97 | 59/318) Action Reimagined: Text-to-Pose Video Editing for Dynamic Human Actions (Lan Wang et al., 2024)</a></li><li><a href=#2197--60318-v3d-video-diffusion-models-are-effective-3d-generators-zilong-chen-et-al-2024>(21/97 | 60/318) V3D: Video Diffusion Models are Effective 3D Generators (Zilong Chen et al., 2024)</a></li><li><a href=#2297--61318-fontclip-a-semantic-typography-visual-language-model-for-multilingual-font-applications-yuki-tatsukawa-et-al-2024>(22/97 | 61/318) FontCLIP: A Semantic Typography Visual-Language Model for Multilingual Font Applications (Yuki Tatsukawa et al., 2024)</a></li><li><a href=#2397--62318-enhancing-semantic-fidelity-in-text-to-image-synthesis-attention-regulation-in-diffusion-models-yang-zhang-et-al-2024>(23/97 | 62/318) Enhancing Semantic Fidelity in Text-to-Image Synthesis: Attention Regulation in Diffusion Models (Yang Zhang et al., 2024)</a></li><li><a href=#2497--63318-videomamba-state-space-model-for-efficient-video-understanding-kunchang-li-et-al-2024>(24/97 | 63/318) VideoMamba: State Space Model for Efficient Video Understanding (Kunchang Li et al., 2024)</a></li><li><a href=#2597--64318-dialoc-an-iterative-approach-to-embodied-dialog-localization-chao-zhang-et-al-2024>(25/97 | 64/318) DiaLoc: An Iterative Approach to Embodied Dialog Localization (Chao Zhang et al., 2024)</a></li><li><a href=#2697--65318-large-model-driven-radiology-report-generation-with-clinical-quality-reinforcement-learning-zijian-zhou-et-al-2024>(26/97 | 65/318) Large Model driven Radiology Report Generation with Clinical Quality Reinforcement Learning (Zijian Zhou et al., 2024)</a></li><li><a href=#2797--66318-leveraging-foundation-models-for-content-based-medical-image-retrieval-in-radiology-stefan-denner-et-al-2024>(27/97 | 66/318) Leveraging Foundation Models for Content-Based Medical Image Retrieval in Radiology (Stefan Denner et al., 2024)</a></li><li><a href=#2897--67318-quasar-quality-and-aesthetics-scoring-with-advanced-representations-sergey-kastryulin-et-al-2024>(28/97 | 67/318) QUASAR: QUality and Aesthetics Scoring with Advanced Representations (Sergey Kastryulin et al., 2024)</a></li><li><a href=#2997--68318-3d-aware-image-generation-and-editing-with-multi-modal-conditions-bo-li-et-al-2024>(29/97 | 68/318) 3D-aware Image Generation and Editing with Multi-modal Conditions (Bo Li et al., 2024)</a></li><li><a href=#3097--69318-pre-trained-model-recommendation-for-downstream-fine-tuning-jiameng-bai-et-al-2024>(30/97 | 69/318) Pre-Trained Model Recommendation for Downstream Fine-tuning (Jiameng Bai et al., 2024)</a></li><li><a href=#3197--70318-structure-your-data-towards-semantic-graph-counterfactuals-angeliki-dimitriou-et-al-2024>(31/97 | 70/318) Structure Your Data: Towards Semantic Graph Counterfactuals (Angeliki Dimitriou et al., 2024)</a></li><li><a href=#3297--71318-attention-prompt-tuning-parameter-efficient-adaptation-of-pre-trained-models-for-spatiotemporal-modeling-wele-gedara-chaminda-bandara-et-al-2024>(32/97 | 71/318) Attention Prompt Tuning: Parameter-efficient Adaptation of Pre-trained Models for Spatiotemporal Modeling (Wele Gedara Chaminda Bandara et al., 2024)</a></li><li><a href=#3397--72318-trustworthy-partial-label-learning-with-out-of-distribution-detection-jintao-huang-et-al-2024>(33/97 | 72/318) Trustworthy Partial Label Learning with Out-of-distribution Detection (Jintao Huang et al., 2024)</a></li><li><a href=#3497--73318-active-generation-for-image-classification-tao-huang-et-al-2024>(34/97 | 73/318) Active Generation for Image Classification (Tao Huang et al., 2024)</a></li><li><a href=#3597--74318-pointseg-a-training-free-paradigm-for-3d-scene-segmentation-via-foundation-models-qingdong-he-et-al-2024>(35/97 | 74/318) PointSeg: A Training-Free Paradigm for 3D Scene Segmentation via Foundation Models (Qingdong He et al., 2024)</a></li><li><a href=#3697--75318-class-imbalance-in-object-detection-an-experimental-diagnosis-and-study-of-mitigation-strategies-nieves-crasto-2024>(36/97 | 75/318) Class Imbalance in Object Detection: An Experimental Diagnosis and Study of Mitigation Strategies (Nieves Crasto, 2024)</a></li><li><a href=#3797--76318-a-holistic-framework-towards-vision-based-traffic-signal-control-with-microscopic-simulation-pan-he-et-al-2024>(37/97 | 76/318) A Holistic Framework Towards Vision-based Traffic Signal Control with Microscopic Simulation (Pan He et al., 2024)</a></li><li><a href=#3897--77318-bayesian-diffusion-models-for-3d-shape-reconstruction-haiyang-xu-et-al-2024>(38/97 | 77/318) Bayesian Diffusion Models for 3D Shape Reconstruction (Haiyang Xu et al., 2024)</a></li><li><a href=#3997--78318-shape-non-rigid-kinematics-snk-a-zero-shot-method-for-non-rigid-shape-matching-via-unsupervised-functional-map-regularized-reconstruction-souhaib-attaiki-et-al-2024>(39/97 | 78/318) Shape Non-rigid Kinematics (SNK): A Zero-Shot Method for Non-Rigid Shape Matching via Unsupervised Functional Map Regularized Reconstruction (Souhaib Attaiki et al., 2024)</a></li><li><a href=#4097--79318-omh-structured-sparsity-via-optimally-matched-hierarchy-for-unsupervised-semantic-segmentation-baran-ozaydin-et-al-2024>(40/97 | 79/318) OMH: Structured Sparsity via Optimally Matched Hierarchy for Unsupervised Semantic Segmentation (Baran Ozaydin et al., 2024)</a></li><li><a href=#4197--80318-sardet-100k-towards-open-source-benchmark-and-toolkit-for-large-scale-sar-object-detection-yuxuan-li-et-al-2024>(41/97 | 80/318) SARDet-100K: Towards Open-Source Benchmark and ToolKit for Large-Scale SAR Object Detection (Yuxuan Li et al., 2024)</a></li><li><a href=#4297--81318-eliminating-warping-shakes-for-unsupervised-online-video-stitching-lang-nie-et-al-2024>(42/97 | 81/318) Eliminating Warping Shakes for Unsupervised Online Video Stitching (Lang Nie et al., 2024)</a></li><li><a href=#4397--82318-optimizing-latent-graph-representations-of-surgical-scenes-for-zero-shot-domain-transfer-siddhant-satyanaik-et-al-2024>(43/97 | 82/318) Optimizing Latent Graph Representations of Surgical Scenes for Zero-Shot Domain Transfer (Siddhant Satyanaik et al., 2024)</a></li><li><a href=#4497--83318-liso-lidar-only-self-supervised-3d-object-detection-stefan-baur-et-al-2024>(44/97 | 83/318) LISO: Lidar-only Self-Supervised 3D Object Detection (Stefan Baur et al., 2024)</a></li><li><a href=#4597--84318-explainable-transformer-prototypes-for-medical-diagnoses-ugur-demir-et-al-2024>(45/97 | 84/318) Explainable Transformer Prototypes for Medical Diagnoses (Ugur Demir et al., 2024)</a></li><li><a href=#4697--85318-deadiff-an-efficient-stylization-diffusion-model-with-disentangled-representations-tianhao-qi-et-al-2024>(46/97 | 85/318) DEADiff: An Efficient Stylization Diffusion Model with Disentangled Representations (Tianhao Qi et al., 2024)</a></li><li><a href=#4797--86318-cood-combined-out-of-distribution-detection-using-multiple-measures-for-anomaly--novel-class-detection-in-large-scale-hierarchical-classification-l-e-hogeweg-et-al-2024>(47/97 | 86/318) COOD: Combined out-of-distribution detection using multiple measures for anomaly & novel class detection in large-scale hierarchical classification (L. E. Hogeweg et al., 2024)</a></li><li><a href=#4897--87318-drivedreamer-2-llm-enhanced-world-models-for-diverse-driving-video-generation-guosheng-zhao-et-al-2024>(48/97 | 87/318) DriveDreamer-2: LLM-Enhanced World Models for Diverse Driving Video Generation (Guosheng Zhao et al., 2024)</a></li><li><a href=#4997--88318-medical-image-synthesis-via-fine-grained-image-text-alignment-and-anatomy-pathology-prompting-wenting-chen-et-al-2024>(49/97 | 88/318) Medical Image Synthesis via Fine-Grained Image-Text Alignment and Anatomy-Pathology Prompting (Wenting Chen et al., 2024)</a></li><li><a href=#5097--89318-hdrtransdc-high-dynamic-range-image-reconstruction-with-transformer-deformation-convolution-shuaikang-shang-et-al-2024>(50/97 | 89/318) HDRTransDC: High Dynamic Range Image Reconstruction with Transformer Deformation Convolution (Shuaikang Shang et al., 2024)</a></li><li><a href=#5197--90318-data-independent-operator-a-training-free-artifact-representation-extractor-for-generalizable-deepfake-detection-chuangchuang-tan-et-al-2024>(51/97 | 90/318) Data-Independent Operator: A Training-Free Artifact Representation Extractor for Generalizable Deepfake Detection (Chuangchuang Tan et al., 2024)</a></li><li><a href=#5297--91318-genetic-learning-for-designing-sim-to-real-data-augmentations-bram-vanherle-et-al-2024>(52/97 | 91/318) Genetic Learning for Designing Sim-to-Real Data Augmentations (Bram Vanherle et al., 2024)</a></li><li><a href=#5397--92318-an-image-is-worth-12-tokens-after-layer-2-plug-and-play-inference-acceleration-for-large-vision-language-models-liang-chen-et-al-2024>(53/97 | 92/318) An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference Acceleration for Large Vision-Language Models (Liang Chen et al., 2024)</a></li><li><a href=#5497--93318-distribution-aware-data-expansion-with-diffusion-models-haowei-zhu-et-al-2024>(54/97 | 93/318) Distribution-Aware Data Expansion with Diffusion Models (Haowei Zhu et al., 2024)</a></li><li><a href=#5597--94318-pcld-point-cloud-layerwise-diffusion-for-adversarial-purification-mert-gulsen-et-al-2024>(55/97 | 94/318) PCLD: Point Cloud Layerwise Diffusion for Adversarial Purification (Mert Gulsen et al., 2024)</a></li><li><a href=#5697--95318-detection-of-object-throwing-behavior-in-surveillance-videos-ivo-p-c-kersten-et-al-2024>(56/97 | 95/318) Detection of Object Throwing Behavior in Surveillance Videos (Ivo P. C. Kersten et al., 2024)</a></li><li><a href=#5797--96318-multi-scale-implicit-transformer-with-re-parameterize-for-arbitrary-scale-super-resolution-jinchen-zhu-et-al-2024>(57/97 | 96/318) Multi-Scale Implicit Transformer with Re-parameterize for Arbitrary-Scale Super-Resolution (Jinchen Zhu et al., 2024)</a></li><li><a href=#5897--97318-advancing-text-driven-chest-x-ray-generation-with-policy-based-reinforcement-learning-woojung-han-et-al-2024>(58/97 | 97/318) Advancing Text-Driven Chest X-Ray Generation with Policy-Based Reinforcement Learning (Woojung Han et al., 2024)</a></li><li><a href=#5997--98318-towards-the-uncharted-density-descending-feature-perturbation-for-semi-supervised-semantic-segmentation-xiaoyang-wang-et-al-2024>(59/97 | 98/318) Towards the Uncharted: Density-Descending Feature Perturbation for Semi-supervised Semantic Segmentation (Xiaoyang Wang et al., 2024)</a></li><li><a href=#6097--99318-refining-segmentation-on-the-fly-an-interactive-framework-for-point-cloud-semantic-segmentation-peng-zhang-et-al-2024>(60/97 | 99/318) Refining Segmentation On-the-Fly: An Interactive Framework for Point Cloud Semantic Segmentation (Peng Zhang et al., 2024)</a></li><li><a href=#6197--100318-3d-semantic-segmentation-driven-representations-for-3d-object-detection-hayeon-o-et-al-2024>(61/97 | 100/318) 3D Semantic Segmentation-Driven Representations for 3D Object Detection (Hayeon O et al., 2024)</a></li><li><a href=#6297--101318-memory-based-adapters-for-online-3d-scene-perception-xiuwei-xu-et-al-2024>(62/97 | 101/318) Memory-based Adapters for Online 3D Scene Perception (Xiuwei Xu et al., 2024)</a></li><li><a href=#6397--102318-dngaussian-optimizing-sparse-view-3d-gaussian-radiance-fields-with-global-local-depth-normalization-jiahe-li-et-al-2024>(63/97 | 102/318) DNGaussian: Optimizing Sparse-View 3D Gaussian Radiance Fields with Global-Local Depth Normalization (Jiahe Li et al., 2024)</a></li><li><a href=#6497--103318-skeleton-supervised-airway-segmentation-mingyue-zhao-et-al-2024>(64/97 | 103/318) Skeleton Supervised Airway Segmentation (Mingyue Zhao et al., 2024)</a></li><li><a href=#6597--104318-earthloc-astronaut-photography-localization-by-indexing-earth-from-space-gabriele-berton-et-al-2024>(65/97 | 104/318) EarthLoc: Astronaut Photography Localization by Indexing Earth from Space (Gabriele Berton et al., 2024)</a></li><li><a href=#6697--105318-transferring-relative-monocular-depth-to-surgical-vision-with-temporal-consistency-charlie-budd-et-al-2024>(66/97 | 105/318) Transferring Relative Monocular Depth to Surgical Vision with Temporal Consistency (Charlie Budd et al., 2024)</a></li><li><a href=#6797--106318-ceat-continual-expansion-and-absorption-transformer-for-non-exemplar-class-incremental-learning-xinyuan-gao-et-al-2024>(67/97 | 106/318) CEAT: Continual Expansion and Absorption Transformer for Non-Exemplar Class-Incremental Learning (Xinyuan Gao et al., 2024)</a></li><li><a href=#6897--107318-exploiting-style-latent-flows-for-generalizing-deepfake-detection-video-detection-jongwook-choi-et-al-2024>(68/97 | 107/318) Exploiting Style Latent Flows for Generalizing Deepfake Detection Video Detection (Jongwook Choi et al., 2024)</a></li><li><a href=#6997--108318-confidence-aware-rgb-d-face-recognition-via-virtual-depth-synthesis-zijian-chen-et-al-2024>(69/97 | 108/318) Confidence-Aware RGB-D Face Recognition via Virtual Depth Synthesis (Zijian Chen et al., 2024)</a></li><li><a href=#7097--109318-toward-robust-canine-cardiac-diagnosis-deep-prototype-alignment-network-based-few-shot-segmentation-in-veterinary-medicine-jun-young-oh-et-al-2024>(70/97 | 109/318) Toward Robust Canine Cardiac Diagnosis: Deep Prototype Alignment Network-Based Few-Shot Segmentation in Veterinary Medicine (Jun-Young Oh et al., 2024)</a></li><li><a href=#7197--110318-see-through-their-minds-learning-transferable-neural-representation-from-cross-subject-fmri-yulong-liu-et-al-2024>(71/97 | 110/318) See Through Their Minds: Learning Transferable Neural Representation from Cross-Subject fMRI (Yulong Liu et al., 2024)</a></li><li><a href=#7297--111318-brushnet-a-plug-and-play-image-inpainting-model-with-decomposed-dual-branch-diffusion-xuan-ju-et-al-2024>(72/97 | 111/318) BrushNet: A Plug-and-Play Image Inpainting Model with Decomposed Dual-Branch Diffusion (Xuan Ju et al., 2024)</a></li><li><a href=#7397--112318-stochastic-cortical-self-reconstruction-christian-wachinger-et-al-2024>(73/97 | 112/318) Stochastic Cortical Self-Reconstruction (Christian Wachinger et al., 2024)</a></li><li><a href=#7497--113318-mambamil-enhancing-long-sequence-modeling-with-sequence-reordering-in-computational-pathology-shu-yang-et-al-2024>(74/97 | 113/318) MambaMIL: Enhancing Long Sequence Modeling with Sequence Reordering in Computational Pathology (Shu Yang et al., 2024)</a></li><li><a href=#7597--114318-facechain-sude-building-derived-class-to-inherit-category-attributes-for-one-shot-subject-driven-generation-pengchong-qiao-et-al-2024>(75/97 | 114/318) FaceChain-SuDe: Building Derived Class to Inherit Category Attributes for One-shot Subject-Driven Generation (Pengchong Qiao et al., 2024)</a></li><li><a href=#7697--115318-car-damage-detection-and-patch-to-patch-self-supervised-image-alignment-hanxiao-chen-2024>(76/97 | 115/318) Car Damage Detection and Patch-to-Patch Self-supervised Image Alignment (Hanxiao Chen, 2024)</a></li><li><a href=#7797--116318-epsilon-mesh-attack-a-surface-based-adversarial-point-cloud-attack-for-facial-expression-recognition-batuhan-cengiz-et-al-2024>(77/97 | 116/318) epsilon-Mesh Attack: A Surface-based Adversarial Point Cloud Attack for Facial Expression Recognition (Batuhan Cengiz et al., 2024)</a></li><li><a href=#7897--117318-towards-zero-shot-interpretable-human-recognition-a-2d-3d-registration-framework-henrique-jesus-et-al-2024>(78/97 | 117/318) Towards Zero-Shot Interpretable Human Recognition: A 2D-3D Registration Framework (Henrique Jesus et al., 2024)</a></li><li><a href=#7997--118318-forest-inspection-dataset-for-aerial-semantic-segmentation-and-depth-estimation-bianca-cerasela-zelia-blaga-et-al-2024>(79/97 | 118/318) Forest Inspection Dataset for Aerial Semantic Segmentation and Depth Estimation (Bianca-Cerasela-Zelia Blaga et al., 2024)</a></li><li><a href=#8097--119318-density-guided-label-smoothing-for-temporal-localization-of-driving-actions-tunc-alkanat-et-al-2024>(80/97 | 119/318) Density-Guided Label Smoothing for Temporal Localization of Driving Actions (Tunc Alkanat et al., 2024)</a></li><li><a href=#8197--120318-distributionally-generative-augmentation-for-fair-facial-attribute-classification-fengda-zhang-et-al-2024>(81/97 | 120/318) Distributionally Generative Augmentation for Fair Facial Attribute Classification (Fengda Zhang et al., 2024)</a></li><li><a href=#8297--121318-transformer-based-fusion-of-2d-pose-and-spatio-temporal-embeddings-for-distracted-driver-action-recognition-erkut-akdag-et-al-2024>(82/97 | 121/318) Transformer-based Fusion of 2D-pose and Spatio-temporal Embeddings for Distracted Driver Action Recognition (Erkut Akdag et al., 2024)</a></li><li><a href=#8397--122318-query-guided-prototype-evolution-network-for-few-shot-segmentation-runmin-cong-et-al-2024>(83/97 | 122/318) Query-guided Prototype Evolution Network for Few-Shot Segmentation (Runmin Cong et al., 2024)</a></li><li><a href=#8497--123318-point-mamba-a-novel-point-cloud-backbone-based-on-state-space-model-with-octree-based-ordering-strategy-jiuming-liu-et-al-2024>(84/97 | 123/318) Point Mamba: A Novel Point Cloud Backbone Based on State Space Model with Octree-Based Ordering Strategy (Jiuming Liu et al., 2024)</a></li><li><a href=#8597--124318-text2qr-harmonizing-aesthetic-customization-and-scanning-robustness-for-text-guided-qr-code-generation-guangyang-wu-et-al-2024>(85/97 | 124/318) Text2QR: Harmonizing Aesthetic Customization and Scanning Robustness for Text-Guided QR Code Generation (Guangyang Wu et al., 2024)</a></li><li><a href=#8697--125318-fine-grained-pillar-feature-encoding-via-spatio-temporal-virtual-grid-for-3d-object-detection-konyul-park-et-al-2024>(86/97 | 125/318) Fine-Grained Pillar Feature Encoding Via Spatio-Temporal Virtual Grid for 3D Object Detection (Konyul Park et al., 2024)</a></li><li><a href=#8797--126318-flowvqtalker-high-quality-emotional-talking-face-generation-through-normalizing-flow-and-quantization-shuai-tan-et-al-2024>(87/97 | 126/318) FlowVQTalker: High-Quality Emotional Talking Face Generation through Normalizing Flow and Quantization (Shuai Tan et al., 2024)</a></li><li><a href=#8897--127318-style2talker-high-resolution-talking-head-generation-with-emotion-style-and-art-style-shuai-tan-et-al-2024>(88/97 | 127/318) Style2Talker: High-Resolution Talking Head Generation with Emotion Style and Art Style (Shuai Tan et al., 2024)</a></li><li><a href=#8997--128318-say-anything-with-any-style-shuai-tan-et-al-2024>(89/97 | 128/318) Say Anything with Any Style (Shuai Tan et al., 2024)</a></li><li><a href=#9097--129318-exploring-hardware-friendly-bottleneck-architecture-in-cnn-for-embedded-computing-systems-xing-lei-et-al-2024>(90/97 | 129/318) Exploring Hardware Friendly Bottleneck Architecture in CNN for Embedded Computing Systems (Xing Lei et al., 2024)</a></li><li><a href=#9197--130318-reliable-spatial-temporal-voxels-for-multi-modal-test-time-adaptation-haozhi-cao-et-al-2024>(91/97 | 130/318) Reliable Spatial-Temporal Voxels For Multi-Modal Test-Time Adaptation (Haozhi Cao et al., 2024)</a></li><li><a href=#9297--131318-vosh-voxel-mesh-hybrid-representation-for-real-time-view-synthesis-chenhao-zhang-et-al-2024>(92/97 | 131/318) Vosh: Voxel-Mesh Hybrid Representation for Real-Time View Synthesis (Chenhao Zhang et al., 2024)</a></li><li><a href=#9397--132318-fregs-3d-gaussian-splatting-with-progressive-frequency-regularization-jiahui-zhang-et-al-2024>(93/97 | 132/318) FreGS: 3D Gaussian Splatting with Progressive Frequency Regularization (Jiahui Zhang et al., 2024)</a></li><li><a href=#9497--133318-fast-text-to-3d-aware-face-generation-and-manipulation-via-direct-cross-modal-mapping-and-geometric-regularization-jinlu-zhang-et-al-2024>(94/97 | 133/318) Fast Text-to-3D-Aware Face Generation and Manipulation via Direct Cross-modal Mapping and Geometric Regularization (Jinlu Zhang et al., 2024)</a></li><li><a href=#9597--134318-ada-tracker-soft-tissue-tracking-via-inter-frame-and-adaptive-template-matching-jiaxin-guo-et-al-2024>(95/97 | 134/318) Ada-Tracker: Soft Tissue Tracking via Inter-Frame and Adaptive-Template Matching (Jiaxin Guo et al., 2024)</a></li><li><a href=#9697--135318-put-myself-in-your-shoes-lifting-the-egocentric-perspective-from-exocentric-videos-mi-luo-et-al-2024>(96/97 | 135/318) Put Myself in Your Shoes: Lifting the Egocentric Perspective from Exocentric Videos (Mi Luo et al., 2024)</a></li><li><a href=#9797--136318-moab-multi-modal-outer-arithmetic-block-for-fusion-of-histopathological-images-and-genetic-data-for-brain-tumor-grading-omnia-alwazzan-et-al-2024>(97/97 | 136/318) MOAB: Multi-Modal Outer Arithmetic Block For Fusion Of Histopathological Images And Genetic Data For Brain Tumor Grading (Omnia Alwazzan et al., 2024)</a></li></ul></li><li><a href=#csir-8>cs.IR (8)</a><ul><li><a href=#18--137318-coral-collaborative-retrieval-augmented-large-language-models-improve-long-tail-recommendation-junda-wu-et-al-2024>(1/8 | 137/318) CoRAL: Collaborative Retrieval-Augmented Large Language Models Improve Long-tail Recommendation (Junda Wu et al., 2024)</a></li><li><a href=#28--138318-kellmrec-knowledge-enhanced-large-language-models-for-recommendation-weiqing-luo-et-al-2024>(2/8 | 138/318) KELLMRec: Knowledge-Enhanced Large Language Models for Recommendation (Weiqing Luo et al., 2024)</a></li><li><a href=#38--139318-recai-leveraging-large-language-models-for-next-generation-recommender-systems-jianxun-lian-et-al-2024>(3/8 | 139/318) RecAI: Leveraging Large Language Models for Next-Generation Recommender Systems (Jianxun Lian et al., 2024)</a></li><li><a href=#48--140318-post-training-attribute-unlearning-in-recommender-systems-chaochao-chen-et-al-2024>(4/8 | 140/318) Post-Training Attribute Unlearning in Recommender Systems (Chaochao Chen et al., 2024)</a></li><li><a href=#58--141318-toolrerank-adaptive-and-hierarchy-aware-reranking-for-tool-retrieval-yuanhang-zheng-et-al-2024>(5/8 | 141/318) ToolRerank: Adaptive and Hierarchy-Aware Reranking for Tool Retrieval (Yuanhang Zheng et al., 2024)</a></li><li><a href=#68--142318-metasplit-meta-split-network-for-limited-stock-product-recommendation-wenhao-wu-et-al-2024>(6/8 | 142/318) MetaSplit: Meta-Split Network for Limited-Stock Product Recommendation (Wenhao Wu et al., 2024)</a></li><li><a href=#78--143318-repeated-padding-as-data-augmentation-for-sequential-recommendation-yizhou-dang-et-al-2024>(7/8 | 143/318) Repeated Padding as Data Augmentation for Sequential Recommendation (Yizhou Dang et al., 2024)</a></li><li><a href=#88--144318-splade-v3-new-baselines-for-splade-carlos-lassance-et-al-2024>(8/8 | 144/318) SPLADE-v3: New baselines for SPLADE (Carlos Lassance et al., 2024)</a></li></ul></li><li><a href=#eesssp-3>eess.SP (3)</a><ul><li><a href=#13--145318-zero-shot-ecg-classification-with-multimodal-learning-and-test-time-clinical-knowledge-enhancement-che-liu-et-al-2024>(1/3 | 145/318) Zero-Shot ECG Classification with Multimodal Learning and Test-time Clinical Knowledge Enhancement (Che Liu et al., 2024)</a></li><li><a href=#23--146318-distributed-average-consensus-via-noisy-and-non-coherent-over-the-air-aggregation-huiwen-yang-et-al-2024>(2/3 | 146/318) Distributed Average Consensus via Noisy and Non-Coherent Over-the-Air Aggregation (Huiwen Yang et al., 2024)</a></li><li><a href=#33--147318-lidar-point-cloud-based-multiple-vehicle-tracking-with-probabilistic-measurement-region-association-guanhua-ding-et-al-2024>(3/3 | 147/318) LiDAR Point Cloud-based Multiple Vehicle Tracking with Probabilistic Measurement-Region Association (Guanhua Ding et al., 2024)</a></li></ul></li><li><a href=#cslg-63>cs.LG (63)</a><ul><li><a href=#163--148318-which-llm-to-play-convergence-aware-online-model-selection-with-time-increasing-bandits-yu-xia-et-al-2024>(1/63 | 148/318) Which LLM to Play? Convergence-Aware Online Model Selection with Time-Increasing Bandits (Yu Xia et al., 2024)</a></li><li><a href=#263--149318-comq-a-backpropagation-free-algorithm-for-post-training-quantization-aozhong-zhang-et-al-2024>(2/63 | 149/318) COMQ: A Backpropagation-Free Algorithm for Post-Training Quantization (Aozhong Zhang et al., 2024)</a></li><li><a href=#363--150318-counterfactual-reasoning-with-knowledge-graph-embeddings-lena-zellinger-et-al-2024>(3/63 | 150/318) Counterfactual Reasoning with Knowledge Graph Embeddings (Lena Zellinger et al., 2024)</a></li><li><a href=#463--151318-learning-with-noisy-foundation-models-hao-chen-et-al-2024>(4/63 | 151/318) Learning with Noisy Foundation Models (Hao Chen et al., 2024)</a></li><li><a href=#563--152318-joint-embedding-masked-autoencoder-for-self-supervised-learning-of-dynamic-functional-connectivity-from-the-human-brain-jungwon-choi-et-al-2024>(5/63 | 152/318) Joint-Embedding Masked Autoencoder for Self-supervised Learning of Dynamic Functional Connectivity from the Human Brain (Jungwon Choi et al., 2024)</a></li><li><a href=#663--153318-on-the-generalization-ability-of-unsupervised-pretraining-yuyang-deng-et-al-2024>(6/63 | 153/318) On the Generalization Ability of Unsupervised Pretraining (Yuyang Deng et al., 2024)</a></li><li><a href=#763--154318-in-context-exploration-exploitation-for-reinforcement-learning-zhenwen-dai-et-al-2024>(7/63 | 154/318) In-context Exploration-Exploitation for Reinforcement Learning (Zhenwen Dai et al., 2024)</a></li><li><a href=#863--155318-unraveling-the-mystery-of-scaling-laws-part-i-hui-su-et-al-2024>(8/63 | 155/318) Unraveling the Mystery of Scaling Laws: Part I (Hui Su et al., 2024)</a></li><li><a href=#963--156318-all-in-one-multi-task-prompting-for-graph-neural-networks-extended-abstract-xiangguo-sun-et-al-2024>(9/63 | 156/318) All in One: Multi-Task Prompting for Graph Neural Networks (Extended Abstract) (Xiangguo Sun et al., 2024)</a></li><li><a href=#1063--157318-advancing-graph-neural-networks-with-hl-hgat-a-hodge-laplacian-and-attention-mechanism-approach-for-heterogeneous-graph-structured-data-jinghan-huang-et-al-2024>(10/63 | 157/318) Advancing Graph Neural Networks with HL-HGAT: A Hodge-Laplacian and Attention Mechanism Approach for Heterogeneous Graph-Structured Data (Jinghan Huang et al., 2024)</a></li><li><a href=#1163--158318-uncertainty-in-graph-neural-networks-a-survey-fangxin-wang-et-al-2024>(11/63 | 158/318) Uncertainty in Graph Neural Networks: A Survey (Fangxin Wang et al., 2024)</a></li><li><a href=#1263--159318-evaluating-the-energy-efficiency-of-few-shot-learning-for-object-detection-in-industrial-settings-georgios-tsoumplekas-et-al-2024>(12/63 | 159/318) Evaluating the Energy Efficiency of Few-Shot Learning for Object Detection in Industrial Settings (Georgios Tsoumplekas et al., 2024)</a></li><li><a href=#1363--160318-a-differential-geometric-view-and-explainability-of-gnn-on-evolving-graphs-yazheng-liu-et-al-2024>(13/63 | 160/318) A Differential Geometric View and Explainability of GNN on Evolving Graphs (Yazheng Liu et al., 2024)</a></li><li><a href=#1463--161318-improving-deep-learning-with-prior-knowledge-and-cognitive-models-a-survey-on-enhancing-explainability-adversarial-robustness-and-zero-shot-learning-fuseinin-mumuni-et-al-2024>(14/63 | 161/318) Improving deep learning with prior knowledge and cognitive models: A survey on enhancing explainability, adversarial robustness and zero-shot learning (Fuseinin Mumuni et al., 2024)</a></li><li><a href=#1563--162318-simplicity-bias-of-transformers-to-learn-low-sensitivity-functions-bhavya-vasudeva-et-al-2024>(15/63 | 162/318) Simplicity Bias of Transformers to Learn Low Sensitivity Functions (Bhavya Vasudeva et al., 2024)</a></li><li><a href=#1663--163318-a-geospatial-approach-to-predicting-desert-locust-breeding-grounds-in-africa-ibrahim-salihu-yusuf-et-al-2024>(16/63 | 163/318) A Geospatial Approach to Predicting Desert Locust Breeding Grounds in Africa (Ibrahim Salihu Yusuf et al., 2024)</a></li><li><a href=#1763--164318-contextgpt-infusing-llms-knowledge-into-neuro-symbolic-activity-recognition-models-luca-arrotta-et-al-2024>(17/63 | 164/318) ContextGPT: Infusing LLMs Knowledge into Neuro-Symbolic Activity Recognition Models (Luca Arrotta et al., 2024)</a></li><li><a href=#1863--165318-semantic-residual-prompts-for-continual-learning-martin-menabue-et-al-2024>(18/63 | 165/318) Semantic Residual Prompts for Continual Learning (Martin Menabue et al., 2024)</a></li><li><a href=#1963--166318-an-efficient-learning-based-solver-comparable-to-metaheuristics-for-the-capacitated-arc-routing-problem-runze-guo-et-al-2024>(19/63 | 166/318) An Efficient Learning-based Solver Comparable to Metaheuristics for the Capacitated Arc Routing Problem (Runze Guo et al., 2024)</a></li><li><a href=#2063--167318-ups-towards-foundation-models-for-pde-solving-via-cross-modal-adaptation-junhong-shen-et-al-2024>(20/63 | 167/318) UPS: Towards Foundation Models for PDE Solving via Cross-Modal Adaptation (Junhong Shen et al., 2024)</a></li><li><a href=#2163--168318-acquiring-diverse-skills-using-curriculum-reinforcement-learning-with-mixture-of-experts-onur-celik-et-al-2024>(21/63 | 168/318) Acquiring Diverse Skills using Curriculum Reinforcement Learning with Mixture of Experts (Onur Celik et al., 2024)</a></li><li><a href=#2263--169318-diprompt-disentangled-prompt-tuning-for-multiple-latent-domain-generalization-in-federated-learning-sikai-bai-et-al-2024>(22/63 | 169/318) DiPrompT: Disentangled Prompt Tuning for Multiple Latent Domain Generalization in Federated Learning (Sikai Bai et al., 2024)</a></li><li><a href=#2363--170318-multistep-consistency-models-jonathan-heek-et-al-2024>(23/63 | 170/318) Multistep Consistency Models (Jonathan Heek et al., 2024)</a></li><li><a href=#2463--171318-on-the-global-convergence-of-policy-gradient-in-average-reward-markov-decision-processes-navdeep-kumar-et-al-2024>(24/63 | 171/318) On the Global Convergence of Policy Gradient in Average Reward Markov Decision Processes (Navdeep Kumar et al., 2024)</a></li><li><a href=#2563--172318-probabilistic-contrastive-learning-for-long-tailed-visual-recognition-chaoqun-du-et-al-2024>(25/63 | 172/318) Probabilistic Contrastive Learning for Long-Tailed Visual Recognition (Chaoqun Du et al., 2024)</a></li><li><a href=#2663--173318-what-makes-quantization-for-large-language-models-hard-an-empirical-study-from-the-lens-of-perturbation-zhuocheng-gong-et-al-2024>(26/63 | 173/318) What Makes Quantization for Large Language Models Hard? An Empirical Study from the Lens of Perturbation (Zhuocheng Gong et al., 2024)</a></li><li><a href=#2763--174318-aug-kd-anchor-based-mixup-generation-for-out-of-domain-knowledge-distillation-zihao-tang-et-al-2024>(27/63 | 174/318) AuG-KD: Anchor-Based Mixup Generation for Out-of-Domain Knowledge Distillation (Zihao Tang et al., 2024)</a></li><li><a href=#2863--175318-3m-diffusion-latent-multi-modal-diffusion-for-text-guided-generation-of-molecular-graphs-huaisheng-zhu-et-al-2024>(28/63 | 175/318) 3M-Diffusion: Latent Multi-Modal Diffusion for Text-Guided Generation of Molecular Graphs (Huaisheng Zhu et al., 2024)</a></li><li><a href=#2963--176318-can-llms-separate-instructions-from-data-and-what-do-we-even-mean-by-that-egor-zverev-et-al-2024>(29/63 | 176/318) Can LLMs Separate Instructions From Data? And What Do We Even Mean By That? (Egor Zverev et al., 2024)</a></li><li><a href=#3063--177318-xb-maml-learning-expandable-basis-parameters-for-effective-meta-learning-with-wide-task-coverage-jae-jun-lee-et-al-2024>(30/63 | 177/318) XB-MAML: Learning Expandable Basis Parameters for Effective Meta-Learning with Wide Task Coverage (Jae-Jun Lee et al., 2024)</a></li><li><a href=#3163--178318-dont-forget-what-i-did-assessing-client-contributions-in-federated-learning-bishwamittra-ghosh-et-al-2024>(31/63 | 178/318) Don&rsquo;t Forget What I did?: Assessing Client Contributions in Federated Learning (Bishwamittra Ghosh et al., 2024)</a></li><li><a href=#3263--179318-falcon-flop-aware-combinatorial-optimization-for-neural-network-pruning-xiang-meng-et-al-2024>(32/63 | 179/318) FALCON: FLOP-Aware Combinatorial Optimization for Neural Network Pruning (Xiang Meng et al., 2024)</a></li><li><a href=#3363--180318-cost-sensitive-learning-to-defer-to-multiple-experts-with-workload-constraints-jean-v-alves-et-al-2024>(33/63 | 180/318) Cost-Sensitive Learning to Defer to Multiple Experts with Workload Constraints (Jean V. Alves et al., 2024)</a></li><li><a href=#3463--181318-ε-neural-thompson-sampling-of-deep-brain-stimulation-for-parkinson-disease-treatment-hao-lun-hsu-et-al-2024>(34/63 | 181/318) ε-Neural Thompson Sampling of Deep Brain Stimulation for Parkinson Disease Treatment (Hao-Lun Hsu et al., 2024)</a></li><li><a href=#3563--182318-peeraid-improving-adversarial-distillation-from-a-specialized-peer-tutor-jaewon-jung-et-al-2024>(35/63 | 182/318) PeerAiD: Improving Adversarial Distillation from a Specialized Peer Tutor (Jaewon Jung et al., 2024)</a></li><li><a href=#3663--183318-elephants-never-forget-testing-language-models-for-memorization-of-tabular-data-sebastian-bordt-et-al-2024>(36/63 | 183/318) Elephants Never Forget: Testing Language Models for Memorization of Tabular Data (Sebastian Bordt et al., 2024)</a></li><li><a href=#3763--184318-scalable-online-exploration-via-coverability-philip-amortila-et-al-2024>(37/63 | 184/318) Scalable Online Exploration via Coverability (Philip Amortila et al., 2024)</a></li><li><a href=#3863--185318-tactical-decision-making-for-autonomous-trucks-by-deep-reinforcement-learning-with-total-cost-of-operation-based-reward-deepthi-pathare-et-al-2024>(38/63 | 185/318) Tactical Decision Making for Autonomous Trucks by Deep Reinforcement Learning with Total Cost of Operation Based Reward (Deepthi Pathare et al., 2024)</a></li><li><a href=#3963--186318-rl-msa-a-reinforcement-learning-based-multi-line-bus-scheduling-approach-yingzhuo-liu-2024>(39/63 | 186/318) RL-MSA: a Reinforcement Learning-based Multi-line bus Scheduling Approach (Yingzhuo Liu, 2024)</a></li><li><a href=#4063--187318-interpreting-what-typical-fault-signals-look-like-via-prototype-matching-qian-chen-et-al-2024>(40/63 | 187/318) Interpreting What Typical Fault Signals Look Like via Prototype-matching (Qian Chen et al., 2024)</a></li><li><a href=#4163--188318-the-cram-method-for-efficient-simultaneous-learning-and-evaluation-zeyang-jia-et-al-2024>(41/63 | 188/318) The Cram Method for Efficient Simultaneous Learning and Evaluation (Zeyang Jia et al., 2024)</a></li><li><a href=#4263--189318-mathbfnk-puzzle-a-cost-efficient-testbed-for-benchmarking-reinforcement-learning-algorithms-in-generative-language-model-yufeng-zhang-et-al-2024>(42/63 | 189/318) $\mathbf{(N,K)}$-Puzzle: A Cost-Efficient Testbed for Benchmarking Reinforcement Learning Algorithms in Generative Language Model (Yufeng Zhang et al., 2024)</a></li><li><a href=#4363--190318-a-multi-cohort-study-on-prediction-of-acute-brain-dysfunction-states-using-selective-state-space-models-brandon-silva-et-al-2024>(43/63 | 190/318) A multi-cohort study on prediction of acute brain dysfunction states using selective state space models (Brandon Silva et al., 2024)</a></li><li><a href=#4463--191318-leveraging-graph-neural-networks-for-supporting-automatic-triage-of-patients-annamaria-defilippo-et-al-2024>(44/63 | 191/318) Leveraging graph neural networks for supporting Automatic Triage of Patients (Annamaria Defilippo et al., 2024)</a></li><li><a href=#4563--192318-graph-neural-network-with-two-uplift-estimators-for-label-scarcity-individual-uplift-modeling-dingyuan-zhu-et-al-2024>(45/63 | 192/318) Graph Neural Network with Two Uplift Estimators for Label-Scarcity Individual Uplift Modeling (Dingyuan Zhu et al., 2024)</a></li><li><a href=#4663--193318-on-the-limited-representational-power-of-value-functions-and-its-links-to-statistical-inefficiency-david-cheikhi-et-al-2024>(46/63 | 193/318) On the Limited Representational Power of Value Functions and its Links to Statistical (In)Efficiency (David Cheikhi et al., 2024)</a></li><li><a href=#4763--194318-explainable-learning-with-gaussian-processes-kurt-butler-et-al-2024>(47/63 | 194/318) Explainable Learning with Gaussian Processes (Kurt Butler et al., 2024)</a></li><li><a href=#4863--195318-unveiling-the-significance-of-toddler-inspired-reward-transition-in-goal-oriented-reinforcement-learning-junseok-park-et-al-2024>(48/63 | 195/318) Unveiling the Significance of Toddler-Inspired Reward Transition in Goal-Oriented Reinforcement Learning (Junseok Park et al., 2024)</a></li><li><a href=#4963--196318-quantifying-the-sensitivity-of-inverse-reinforcement-learning-to-misspecification-joar-skalse-et-al-2024>(49/63 | 196/318) Quantifying the Sensitivity of Inverse Reinforcement Learning to Misspecification (Joar Skalse et al., 2024)</a></li><li><a href=#5063--197318-monotone-individual-fairness-yahav-bechavod-2024>(50/63 | 197/318) Monotone Individual Fairness (Yahav Bechavod, 2024)</a></li><li><a href=#5163--198318-adaptive-federated-learning-over-the-air-chenhao-wang-et-al-2024>(51/63 | 198/318) Adaptive Federated Learning Over the Air (Chenhao Wang et al., 2024)</a></li><li><a href=#5263--199318-a-converting-autoencoder-toward-low-latency-and-energy-efficient-dnn-inference-at-the-edge-hasanul-mahmud-et-al-2024>(52/63 | 199/318) A Converting Autoencoder Toward Low-latency and Energy-efficient DNN Inference at the Edge (Hasanul Mahmud et al., 2024)</a></li><li><a href=#5363--200318-prediction-of-wort-density-with-lstm-network-derk-rembold-et-al-2024>(53/63 | 200/318) Prediction of Wort Density with LSTM Network (Derk Rembold et al., 2024)</a></li><li><a href=#5463--201318-on-the-diminishing-returns-of-width-for-continual-learning-etash-guha-et-al-2024>(54/63 | 201/318) On the Diminishing Returns of Width for Continual Learning (Etash Guha et al., 2024)</a></li><li><a href=#5563--202318-deepsafempc-deep-learning-based-model-predictive-control-for-safe-multi-agent-reinforcement-learning-xuefeng-wang-et-al-2024>(55/63 | 202/318) DeepSafeMPC: Deep Learning-Based Model Predictive Control for Safe Multi-Agent Reinforcement Learning (Xuefeng Wang et al., 2024)</a></li><li><a href=#5663--203318-towards-robust-out-of-distribution-generalization-bounds-via-sharpness-yingtian-zou-et-al-2024>(56/63 | 203/318) Towards Robust Out-of-Distribution Generalization Bounds via Sharpness (Yingtian Zou et al., 2024)</a></li><li><a href=#5763--204318-finite-time-error-analysis-of-soft-q-learning-switching-system-approach-narim-jeong-et-al-2024>(57/63 | 204/318) Finite-Time Error Analysis of Soft Q-Learning: Switching System Approach (Narim Jeong et al., 2024)</a></li><li><a href=#5863--205318-a-representation-learning-game-for-classes-of-prediction-tasks-neria-uzan-et-al-2024>(58/63 | 205/318) A representation-learning game for classes of prediction tasks (Neria Uzan et al., 2024)</a></li><li><a href=#5963--206318-sliced-wasserstein-distances-and-flows-on-cartan-hadamard-manifolds-clément-bonet-et-al-2024>(59/63 | 206/318) Sliced-Wasserstein Distances and Flows on Cartan-Hadamard Manifolds (Clément Bonet et al., 2024)</a></li><li><a href=#6063--207318-benign-overfitting-in-leaky-relu-networks-with-moderate-input-dimension-kedar-karhadkar-et-al-2024>(60/63 | 207/318) Benign overfitting in leaky ReLU networks with moderate input dimension (Kedar Karhadkar et al., 2024)</a></li><li><a href=#6163--208318-ant-colony-sampling-with-gflownets-for-combinatorial-optimization-minsu-kim-et-al-2024>(61/63 | 208/318) Ant Colony Sampling with GFlowNets for Combinatorial Optimization (Minsu Kim et al., 2024)</a></li><li><a href=#6263--209318-ffad-a-novel-metric-for-assessing-generated-time-series-data-utilizing-fourier-transform-and-auto-encoder-yang-chen-et-al-2024>(62/63 | 209/318) FFAD: A Novel Metric for Assessing Generated Time Series Data Utilizing Fourier Transform and Auto-encoder (Yang Chen et al., 2024)</a></li><li><a href=#6363--210318-decentralized-and-lifelong-adaptive-multi-agent-collaborative-learning-shuo-tang-et-al-2024>(63/63 | 210/318) Decentralized and Lifelong-Adaptive Multi-Agent Collaborative Learning (Shuo Tang et al., 2024)</a></li></ul></li><li><a href=#hep-ph-1>hep-ph (1)</a><ul><li><a href=#11--211318-re-simulation-based-self-supervised-learning-for-pre-training-foundation-models-philip-harris-et-al-2024>(1/1 | 211/318) Re-Simulation-based Self-Supervised Learning for Pre-Training Foundation Models (Philip Harris et al., 2024)</a></li></ul></li><li><a href=#csdc-6>cs.DC (6)</a><ul><li><a href=#16--212318-adding-nvme-ssds-to-enable-and-accelerate-100b-model-fine-tuning-on-a-single-gpu-changyue-liao-et-al-2024>(1/6 | 212/318) Adding NVMe SSDs to Enable and Accelerate 100B Model Fine-tuning on a Single GPU (Changyue Liao et al., 2024)</a></li><li><a href=#26--213318-dynamic-client-clustering-bandwidth-allocation-and-workload-optimization-for-semi-synchronous-federated-learning-liangkun-yu-et-al-2024>(2/6 | 213/318) Dynamic Client Clustering, Bandwidth Allocation, and Workload Optimization for Semi-synchronous Federated Learning (Liangkun Yu et al., 2024)</a></li><li><a href=#36--214318-accelerating-sparse-tensor-decomposition-using-adaptive-linearized-representation-jan-laukemann-et-al-2024>(3/6 | 214/318) Accelerating Sparse Tensor Decomposition Using Adaptive Linearized Representation (Jan Laukemann et al., 2024)</a></li><li><a href=#46--215318-data-poisoning-attacks-in-gossip-learning-alexandre-pham-et-al-2024>(4/6 | 215/318) Data Poisoning Attacks in Gossip Learning (Alexandre Pham et al., 2024)</a></li><li><a href=#56--216318-comparing-task-graph-scheduling-algorithms-an-adversarial-approach-jared-coleman-et-al-2024>(5/6 | 216/318) Comparing Task Graph Scheduling Algorithms: An Adversarial Approach (Jared Coleman et al., 2024)</a></li><li><a href=#66--217318-parameterized-task-graph-scheduling-algorithm-for-comparing-algorithmic-components-jared-coleman-et-al-2024>(6/6 | 217/318) Parameterized Task Graph Scheduling Algorithm for Comparing Algorithmic Components (Jared Coleman et al., 2024)</a></li></ul></li><li><a href=#cscr-7>cs.CR (7)</a><ul><li><a href=#17--218318-stealing-part-of-a-production-language-model-nicholas-carlini-et-al-2024>(1/7 | 218/318) Stealing Part of a Production Language Model (Nicholas Carlini et al., 2024)</a></li><li><a href=#27--219318-a-zero-trust-framework-for-realization-and-defense-against-generative-ai-attacks-in-power-grid-md-shirajum-munir-et-al-2024>(2/7 | 219/318) A Zero Trust Framework for Realization and Defense Against Generative AI Attacks in Power Grid (Md. Shirajum Munir et al., 2024)</a></li><li><a href=#37--220318-dnnshield-embedding-identifiers-for-deep-neural-network-ownership-verification-jasper-stang-et-al-2024>(3/7 | 220/318) DNNShield: Embedding Identifiers for Deep Neural Network Ownership Verification (Jasper Stang et al., 2024)</a></li><li><a href=#47--221318-poisoning-programs-by-un-repairing-code-security-concerns-of-ai-generated-code-cristina-improta-2024>(4/7 | 221/318) Poisoning Programs by Un-Repairing Code: Security Concerns of AI-generated Code (Cristina Improta, 2024)</a></li><li><a href=#57--222318-unprotected-4g5g-control-procedures-at-low-layers-considered-dangerous-norbert-ludant-et-al-2024>(5/7 | 222/318) Unprotected 4G/5G Control Procedures at Low Layers Considered Dangerous (Norbert Ludant et al., 2024)</a></li><li><a href=#67--223318-real-is-not-true-backdoor-attacks-against-deepfake-detection-hong-sun-et-al-2024>(6/7 | 223/318) Real is not True: Backdoor Attacks Against Deepfake Detection (Hong Sun et al., 2024)</a></li><li><a href=#77--224318-intra-section-code-cave-injection-for-adversarial-evasion-attacks-on-windows-pe-malware-file-kshitiz-aryal-et-al-2024>(7/7 | 224/318) Intra-Section Code Cave Injection for Adversarial Evasion Attacks on Windows PE Malware File (Kshitiz Aryal et al., 2024)</a></li></ul></li><li><a href=#eessas-2>eess.AS (2)</a><ul><li><a href=#12--225318-the-evaluation-of-a-code-switched-sepedi-english-automatic-speech-recognition-system-amanda-phaladi-et-al-2024>(1/2 | 225/318) The evaluation of a code-switched Sepedi-English automatic speech recognition system (Amanda Phaladi et al., 2024)</a></li><li><a href=#22--226318-sonotracelab----a-raytracing-based-acoustic-modelling-system-for-simulating-echolocation-behavior-of-bats-wouter-jansen-et-al-2024>(2/2 | 226/318) SonoTraceLab &ndash; A Raytracing-Based Acoustic Modelling System for Simulating Echolocation Behavior of Bats (Wouter Jansen et al., 2024)</a></li></ul></li><li><a href=#quant-ph-4>quant-ph (4)</a><ul><li><a href=#14--227318-application-of-quantum-tensor-networks-for-protein-classification-debarshi-kundu-et-al-2024>(1/4 | 227/318) Application of Quantum Tensor Networks for Protein Classification (Debarshi Kundu et al., 2024)</a></li><li><a href=#24--228318-simulating-quantum-circuits-by-model-counting-jingyi-mei-et-al-2024>(2/4 | 228/318) Simulating Quantum Circuits by Model Counting (Jingyi Mei et al., 2024)</a></li><li><a href=#34--229318-better-than-classical-the-subtle-art-of-benchmarking-quantum-machine-learning-models-joseph-bowles-et-al-2024>(3/4 | 229/318) Better than classical? The subtle art of benchmarking quantum machine learning models (Joseph Bowles et al., 2024)</a></li><li><a href=#44--230318-solving-distributed-flexible-job-shop-scheduling-problems-in-the-wool-textile-industry-with-quantum-annealing-lilia-toma-et-al-2024>(4/4 | 230/318) Solving Distributed Flexible Job Shop Scheduling Problems in the Wool Textile Industry with Quantum Annealing (Lilia Toma et al., 2024)</a></li></ul></li><li><a href=#csma-2>cs.MA (2)</a><ul><li><a href=#12--231318-generalising-multi-agent-cooperation-through-task-agnostic-communication-dulhan-jayalath-et-al-2024>(1/2 | 231/318) Generalising Multi-Agent Cooperation through Task-Agnostic Communication (Dulhan Jayalath et al., 2024)</a></li><li><a href=#22--232318-the-geometry-of-cyclical-social-trends-bernard-chazelle-et-al-2024>(2/2 | 232/318) The Geometry of Cyclical Social Trends (Bernard Chazelle et al., 2024)</a></li></ul></li><li><a href=#csai-4>cs.AI (4)</a><ul><li><a href=#14--233318-real-time-multimodal-cognitive-assistant-for-emergency-medical-services-keshara-weerasinghe-et-al-2024>(1/4 | 233/318) Real-Time Multimodal Cognitive Assistant for Emergency Medical Services (Keshara Weerasinghe et al., 2024)</a></li><li><a href=#24--234318-bigraph-matching-weighted-with-learnt-incentive-function-for-multi-robot-task-allocation-steve-paul-et-al-2024>(2/4 | 234/318) Bigraph Matching Weighted with Learnt Incentive Function for Multi-Robot Task Allocation (Steve Paul et al., 2024)</a></li><li><a href=#34--235318-a-hybrid-intelligence-method-for-argument-mining-michiel-van-der-meer-et-al-2024>(3/4 | 235/318) A Hybrid Intelligence Method for Argument Mining (Michiel van der Meer et al., 2024)</a></li><li><a href=#44--236318-better-understandings-and-configurations-in-maxsat-local-search-solvers-via-anytime-performance-analysis-furong-ye-et-al-2024>(4/4 | 236/318) Better Understandings and Configurations in MaxSAT Local Search Solvers via Anytime Performance Analysis (Furong Ye et al., 2024)</a></li></ul></li><li><a href=#csse-4>cs.SE (4)</a><ul><li><a href=#14--237318-acfix-guiding-llms-with-mined-common-rbac-practices-for-context-aware-repair-of-access-control-vulnerabilities-in-smart-contracts-lyuye-zhang-et-al-2024>(1/4 | 237/318) ACFIX: Guiding LLMs with Mined Common RBAC Practices for Context-Aware Repair of Access Control Vulnerabilities in Smart Contracts (Lyuye Zhang et al., 2024)</a></li><li><a href=#24--238318-textual-analysis-of-end-user-license-agreement-for-red-flagging-potentially-malicious-software-behraj-khan-et-al-2024>(2/4 | 238/318) Textual analysis of End User License Agreement for red-flagging potentially malicious software (Behraj Khan et al., 2024)</a></li><li><a href=#34--239318-knowledge-aware-alert-aggregation-in-large-scale-cloud-systems-a-hybrid-approach-jinxi-kuang-et-al-2024>(3/4 | 239/318) Knowledge-aware Alert Aggregation in Large-scale Cloud Systems: a Hybrid Approach (Jinxi Kuang et al., 2024)</a></li><li><a href=#44--240318-technical-debt-management-the-road-ahead-for-successful-software-delivery-paris-avgeriou-et-al-2024>(4/4 | 240/318) Technical Debt Management: The Road Ahead for Successful Software Delivery (Paris Avgeriou et al., 2024)</a></li></ul></li><li><a href=#statme-1>stat.ME (1)</a><ul><li><a href=#11--241318-stochastic-gradient-descent-based-inference-for-dynamic-network-models-with-attractors-hancong-pan-et-al-2024>(1/1 | 241/318) Stochastic gradient descent-based inference for dynamic network models with attractors (Hancong Pan et al., 2024)</a></li></ul></li><li><a href=#eessiv-11>eess.IV (11)</a><ul><li><a href=#111--242318-dynamic-perturbation-adaptive-adversarial-training-on-medical-image-classification-shuai-li-et-al-2024>(1/11 | 242/318) Dynamic Perturbation-Adaptive Adversarial Training on Medical Image Classification (Shuai Li et al., 2024)</a></li><li><a href=#211--243318-a-segmentation-foundation-model-for-diverse-type-tumors-jianhao-xie-et-al-2024>(2/11 | 243/318) A Segmentation Foundation Model for Diverse-type Tumors (Jianhao Xie et al., 2024)</a></li><li><a href=#311--244318-libr-improving-intraoperative-liver-registration-by-learning-the-residual-of-biomechanics-based-deformable-registration-dingrong-wang-et-al-2024>(3/11 | 244/318) LIBR+: Improving Intraoperative Liver Registration by Learning the Residual of Biomechanics-Based Deformable Registration (Dingrong Wang et al., 2024)</a></li><li><a href=#411--245318-simulation-based-segmentation-of-blood-vessels-in-cerebral-3d-octa-images-bastian-wittmann-et-al-2024>(4/11 | 245/318) Simulation-Based Segmentation of Blood Vessels in Cerebral 3D OCTA Images (Bastian Wittmann et al., 2024)</a></li><li><a href=#511--246318-ct2rep-automated-radiology-report-generation-for-3d-medical-imaging-ibrahim-ethem-hamamci-et-al-2024>(5/11 | 246/318) CT2Rep: Automated Radiology Report Generation for 3D Medical Imaging (Ibrahim Ethem Hamamci et al., 2024)</a></li><li><a href=#611--247318-conditional-score-based-diffusion-model-for-cortical-thickness-trajectory-prediction-qing-xiao-et-al-2024>(6/11 | 247/318) Conditional Score-Based Diffusion Model for Cortical Thickness Trajectory Prediction (Qing Xiao et al., 2024)</a></li><li><a href=#711--248318-shortcut-learning-in-medical-image-segmentation-manxi-lin-et-al-2024>(7/11 | 248/318) Shortcut Learning in Medical Image Segmentation (Manxi Lin et al., 2024)</a></li><li><a href=#811--249318-restaingan-leveraging-ihc-to-if-stain-domain-translation-for-in-silico-data-generation-dominik-winter-et-al-2024>(8/11 | 249/318) ReStainGAN: Leveraging IHC to IF Stain Domain Translation for in-silico Data Generation (Dominik Winter et al., 2024)</a></li><li><a href=#911--250318-incorporating-improved-sinusoidal-threshold-based-semi-supervised-method-and-diffusion-models-for-osteoporosis-diagnosis-wenchi-ke-2024>(9/11 | 250/318) Incorporating Improved Sinusoidal Threshold-based Semi-supervised Method and Diffusion Models for Osteoporosis Diagnosis (Wenchi Ke, 2024)</a></li><li><a href=#1011--251318-exploring-cluster-analysis-in-nelore-cattle-visual-score-attribution-alexandre-de-oliveira-bezerra-et-al-2024>(10/11 | 251/318) Exploring Cluster Analysis in Nelore Cattle Visual Score Attribution (Alexandre de Oliveira Bezerra et al., 2024)</a></li><li><a href=#1111--252318-from-pixel-to-cancer-cellular-automata-in-computed-tomography-yuxiang-lai-et-al-2024>(11/11 | 252/318) From Pixel to Cancer: Cellular Automata in Computed Tomography (Yuxiang Lai et al., 2024)</a></li></ul></li><li><a href=#cscy-3>cs.CY (3)</a><ul><li><a href=#13--253318-improving-low-resource-knowledge-tracing-tasks-by-supervised-pre-training-and-importance-mechanism-fine-tuning-hengyuan-zhang-et-al-2024>(1/3 | 253/318) Improving Low-Resource Knowledge Tracing Tasks by Supervised Pre-training and Importance Mechanism Fine-tuning (Hengyuan Zhang et al., 2024)</a></li><li><a href=#23--254318-exploring-the-impact-of-chatgpt-on-student-interactions-in-computer-supported-collaborative-learning-han-kyul-kim-et-al-2024>(2/3 | 254/318) Exploring the Impact of ChatGPT on Student Interactions in Computer-Supported Collaborative Learning (Han Kyul Kim et al., 2024)</a></li><li><a href=#33--255318-authorship-and-the-politics-and-ethics-of-llm-watermarks-tim-räz-2024>(3/3 | 255/318) Authorship and the Politics and Ethics of LLM Watermarks (Tim Räz, 2024)</a></li></ul></li><li><a href=#csdb-3>cs.DB (3)</a><ul><li><a href=#13--256318-booster-leveraging-large-language-models-for-enhancing-entity-resolution-huahang-li-et-al-2024>(1/3 | 256/318) BoostER: Leveraging Large Language Models for Enhancing Entity Resolution (Huahang Li et al., 2024)</a></li><li><a href=#23--257318-evaluating-large-language-models-in-process-mining-capabilities-benchmarks-evaluation-strategies-and-future-challenges-alessandro-berti-et-al-2024>(2/3 | 257/318) Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, Evaluation Strategies, and Future Challenges (Alessandro Berti et al., 2024)</a></li><li><a href=#33--258318-sfvint-simple-fast-and-generic-variable-length-integer-decoding-using-bit-manipulation-instructions-gang-liao-et-al-2024>(3/3 | 258/318) SFVInt: Simple, Fast and Generic Variable-Length Integer Decoding using Bit Manipulation Instructions (Gang Liao et al., 2024)</a></li></ul></li><li><a href=#csro-12>cs.RO (12)</a><ul><li><a href=#112--259318-rlingua-improving-reinforcement-learning-sample-efficiency-in-robotic-manipulations-with-large-language-models-liangliang-chen-et-al-2024>(1/12 | 259/318) RLingua: Improving Reinforcement Learning Sample Efficiency in Robotic Manipulations With Large Language Models (Liangliang Chen et al., 2024)</a></li><li><a href=#212--260318-landerai-adaptive-landing-behavior-agent-for-expertise-in-3d-dynamic-platform-landings-robinroy-peter-et-al-2024>(2/12 | 260/318) Lander.AI: Adaptive Landing Behavior Agent for Expertise in 3D Dynamic Platform Landings (Robinroy Peter et al., 2024)</a></li><li><a href=#312--261318-accelerating-interface-adaptation-with-user-friendly-priors-benjamin-a-christie-et-al-2024>(3/12 | 261/318) Accelerating Interface Adaptation with User-Friendly Priors (Benjamin A. Christie et al., 2024)</a></li><li><a href=#412--262318-sim-to-real-gap-in-rl-use-case-with-tiago-and-isaac-simgym-jaume-albardaner-et-al-2024>(4/12 | 262/318) Sim-to-Real gap in RL: Use Case with TIAGo and Isaac Sim/Gym (Jaume Albardaner et al., 2024)</a></li><li><a href=#512--263318-a-collision-cone-approach-for-control-barrier-functions-manan-tayal-et-al-2024>(5/12 | 263/318) A Collision Cone Approach for Control Barrier Functions (Manan Tayal et al., 2024)</a></li><li><a href=#612--264318-quadruped-frog-rapid-online-optimization-of-continuous-quadruped-jumping-guillaume-bellegarda-et-al-2024>(6/12 | 264/318) Quadruped-Frog: Rapid Online Optimization of Continuous Quadruped Jumping (Guillaume Bellegarda et al., 2024)</a></li><li><a href=#712--265318-design-and-performance-comparison-of-fuzzypid-and-non-linear-model-predictive-controller-for-4-wheel-omni-drive-robot-love-panta-2024>(7/12 | 265/318) Design and Performance Comparison of FuzzyPID and Non-linear Model Predictive Controller for 4-Wheel Omni-drive Robot (Love Panta, 2024)</a></li><li><a href=#812--266318-multimodal-transformers-for-real-time-surgical-activity-prediction-keshara-weerasinghe-et-al-2024>(8/12 | 266/318) Multimodal Transformers for Real-Time Surgical Activity Prediction (Keshara Weerasinghe et al., 2024)</a></li><li><a href=#912--267318-autonomous-overhead-powerline-recharging-for-uninterrupted-drone-operations-viet-duong-hoang-et-al-2024>(9/12 | 267/318) Autonomous Overhead Powerline Recharging for Uninterrupted Drone Operations (Viet Duong Hoang et al., 2024)</a></li><li><a href=#1012--268318-3dref-3d-dataset-and-benchmark-for-reflection-detection-in-rgb-and-lidar-data-xiting-zhao-et-al-2024>(10/12 | 268/318) 3DRef: 3D Dataset and Benchmark for Reflection Detection in RGB and Lidar Data (Xiting Zhao et al., 2024)</a></li><li><a href=#1112--269318-mapping-high-level-semantic-regions-in-indoor-environments-without-object-recognition-roberto-bigazzi-et-al-2024>(11/12 | 269/318) Mapping High-level Semantic Regions in Indoor Environments without Object Recognition (Roberto Bigazzi et al., 2024)</a></li><li><a href=#1212--270318-neupan-direct-point-robot-navigation-with-end-to-end-model-based-learning-ruihua-han-et-al-2024>(12/12 | 270/318) NeuPAN: Direct Point Robot Navigation with End-to-End Model-based Learning (Ruihua Han et al., 2024)</a></li></ul></li><li><a href=#cslo-1>cs.LO (1)</a><ul><li><a href=#11--271318-are-targeted-messages-more-effective-martin-grohe-et-al-2024>(1/1 | 271/318) Are Targeted Messages More Effective? (Martin Grohe et al., 2024)</a></li></ul></li><li><a href=#q-finrm-1>q-fin.RM (1)</a><ul><li><a href=#11--272318-financial-default-prediction-via-motif-preserving-graph-neural-network-with-curriculum-learning-daixin-wang-et-al-2024>(1/1 | 272/318) Financial Default Prediction via Motif-preserving Graph Neural Network with Curriculum Learning (Daixin Wang et al., 2024)</a></li></ul></li><li><a href=#eesssy-5>eess.SY (5)</a><ul><li><a href=#15--273318-learning-aided-control-of-robotic-tether-net-with-maneuverable-nodes-to-capture-large-space-debris-achira-boonrath-et-al-2024>(1/5 | 273/318) Learning-Aided Control of Robotic Tether-Net with Maneuverable Nodes to Capture Large Space Debris (Achira Boonrath et al., 2024)</a></li><li><a href=#25--274318-grid-monitoring-and-protection-with-continuous-point-on-wave-measurements-and-generative-ai-lang-tong-et-al-2024>(2/5 | 274/318) Grid Monitoring and Protection with Continuous Point-on-Wave Measurements and Generative AI (Lang Tong et al., 2024)</a></li><li><a href=#35--275318-efficient-dual-scale-generalized-radon-fourier-transform-detector-family-for-long-time-coherent-integration-suqi-li-et-al-2024>(3/5 | 275/318) Efficient dual-scale generalized Radon-Fourier transform detector family for long time coherent integration (Suqi Li et al., 2024)</a></li><li><a href=#45--276318-edge-information-hub-orchestrating-satellites-uavs-mec-sensing-and-communications-for-6g-closed-loop-controls-chengleyang-lei-et-al-2024>(4/5 | 276/318) Edge Information Hub: Orchestrating Satellites, UAVs, MEC, Sensing and Communications for 6G Closed-Loop Controls (Chengleyang Lei et al., 2024)</a></li><li><a href=#55--277318-a-prediction-based-forward-looking-vehicle-dispatching-strategy-for-dynamic-ride-pooling-xiaolei-wang-et-al-2024>(5/5 | 277/318) A prediction-based forward-looking vehicle dispatching strategy for dynamic ride-pooling (Xiaolei Wang et al., 2024)</a></li></ul></li><li><a href=#cond-matmtrl-sci-1>cond-mat.mtrl-sci (1)</a><ul><li><a href=#11--278318-materials-science-in-the-era-of-large-language-models-a-perspective-ge-lei-et-al-2024>(1/1 | 278/318) Materials science in the era of large language models: a perspective (Ge Lei et al., 2024)</a></li></ul></li><li><a href=#csgr-2>cs.GR (2)</a><ul><li><a href=#12--279318-surface-aware-mesh-texture-synthesis-with-pre-trained-2d-cnns-áron-samuel-kovács-et-al-2024>(1/2 | 279/318) Surface-aware Mesh Texture Synthesis with Pre-trained 2D CNNs (Áron Samuel Kovács et al., 2024)</a></li><li><a href=#22--280318-inverse-garment-and-pattern-modeling-with-a-differentiable-simulator-boyang-yu-et-al-2024>(2/2 | 280/318) Inverse Garment and Pattern Modeling with a Differentiable Simulator (Boyang Yu et al., 2024)</a></li></ul></li><li><a href=#cset-2>cs.ET (2)</a><ul><li><a href=#12--281318-integration-of-physics-derived-memristor-models-with-machine-learning-frameworks-zhenming-yu-et-al-2024>(1/2 | 281/318) Integration of Physics-Derived Memristor Models with Machine Learning Frameworks (Zhenming Yu et al., 2024)</a></li><li><a href=#22--282318-designing-a-k-state-p-bit-engine-mohammad-khairul-bashar-et-al-2024>(2/2 | 282/318) Designing a K-state P-bit Engine (Mohammad Khairul Bashar et al., 2024)</a></li></ul></li><li><a href=#cshc-5>cs.HC (5)</a><ul><li><a href=#15--283318-hill-a-hallucination-identifier-for-large-language-models-florian-leiser-et-al-2024>(1/5 | 283/318) HILL: A Hallucination Identifier for Large Language Models (Florian Leiser et al., 2024)</a></li><li><a href=#25--284318-designing-for-projection-based-communication-between-autonomous-vehicles-and-pedestrians-trung-thanh-nguyen-et-al-2024>(2/5 | 284/318) Designing for Projection-based Communication between Autonomous Vehicles and Pedestrians (Trung Thanh Nguyen et al., 2024)</a></li><li><a href=#35--285318-people-attribute-purpose-to-autonomous-vehicles-when-explaining-their-behavior-balint-gyevnar-et-al-2024>(3/5 | 285/318) People Attribute Purpose to Autonomous Vehicles When Explaining Their Behavior (Balint Gyevnar et al., 2024)</a></li><li><a href=#45--286318-decoding-complexity-exploring-human-ai-concordance-in-qualitative-coding-elisabeth-kirsten-et-al-2024>(4/5 | 286/318) Decoding Complexity: Exploring Human-AI Concordance in Qualitative Coding (Elisabeth Kirsten et al., 2024)</a></li><li><a href=#55--287318-mitigating-biases-in-collective-decision-making-enhancing-performance-in-the-face-of-fake-news-axel-abels-et-al-2024>(5/5 | 287/318) Mitigating Biases in Collective Decision-Making: Enhancing Performance in the Face of Fake News (Axel Abels et al., 2024)</a></li></ul></li><li><a href=#csmm-1>cs.MM (1)</a><ul><li><a href=#11--288318-fashionregen-llm-empowered-fashion-report-generation-yujuan-ding-et-al-2024>(1/1 | 288/318) FashionReGen: LLM-Empowered Fashion Report Generation (Yujuan Ding et al., 2024)</a></li></ul></li><li><a href=#csar-3>cs.AR (3)</a><ul><li><a href=#13--289318-from-english-to-asic-hardware-implementation-with-large-language-model-emil-goh-et-al-2024>(1/3 | 289/318) From English to ASIC: Hardware Implementation with Large Language Model (Emil Goh et al., 2024)</a></li><li><a href=#23--290318-smart-infinity-fast-large-language-model-training-using-near-storage-processing-on-a-real-system-hongsun-jang-et-al-2024>(2/3 | 290/318) Smart-Infinity: Fast Large Language Model Training using Near-Storage Processing on a Real System (Hongsun Jang et al., 2024)</a></li><li><a href=#33--291318-tcam-ssd-a-framework-for-search-based-computing-in-solid-state-drives-ryan-wong-et-al-2024>(3/3 | 291/318) TCAM-SSD: A Framework for Search-Based Computing in Solid-State Drives (Ryan Wong et al., 2024)</a></li></ul></li><li><a href=#cssd-1>cs.SD (1)</a><ul><li><a href=#11--292318-towards-decoupling-frontend-enhancement-and-backend-recognition-in-monaural-robust-asr-yufeng-yang-et-al-2024>(1/1 | 292/318) Towards Decoupling Frontend Enhancement and Backend Recognition in Monaural Robust ASR (Yufeng Yang et al., 2024)</a></li></ul></li><li><a href=#csne-1>cs.NE (1)</a><ul><li><a href=#11--293318-map-elites-with-transverse-assessment-for-multimodal-problems-in-creative-domains-marvin-zammit-et-al-2024>(1/1 | 293/318) MAP-Elites with Transverse Assessment for Multimodal Problems in Creative Domains (Marvin Zammit et al., 2024)</a></li></ul></li><li><a href=#physicscomp-ph-1>physics.comp-ph (1)</a><ul><li><a href=#11--294318-an-alternative-to-stride-based-rng-for-monte-carlo-transport-braxton-s-cuneo-et-al-2024>(1/1 | 294/318) An Alternative to Stride-Based RNG for Monte Carlo Transport (Braxton S. Cuneo et al., 2024)</a></li></ul></li><li><a href=#mathna-2>math.NA (2)</a><ul><li><a href=#12--295318-a-method-for-accelerating-low-precision-operations-by-sparse-matrix-multiplication-hongyaoxing-gu-2024>(1/2 | 295/318) A method for accelerating low precision operations by sparse matrix multiplication (Hongyaoxing Gu, 2024)</a></li><li><a href=#22--296318-a-functionally-connected-element-method-for-solving-boundary-value-problems-jielin-yang-et-al-2024>(2/2 | 296/318) A Functionally Connected Element Method for Solving Boundary Value Problems (Jielin Yang et al., 2024)</a></li></ul></li><li><a href=#csce-2>cs.CE (2)</a><ul><li><a href=#12--297318-numerical-simulation-of-individual-coil-placement----a-proof-of-concept-study-for-the-prediction-of-recurrence-after-aneurysm-coiling-julian-schwarting-et-al-2024>(1/2 | 297/318) Numerical simulation of individual coil placement &ndash; A proof-of-concept study for the prediction of recurrence after aneurysm coiling (Julian Schwarting et al., 2024)</a></li><li><a href=#22--298318-when-crypto-economics-meet-graph-analytics-and-learning-bingqiao-luo-2024>(2/2 | 298/318) When Crypto Economics Meet Graph Analytics and Learning (Bingqiao Luo, 2024)</a></li></ul></li><li><a href=#csit-2>cs.IT (2)</a><ul><li><a href=#12--299318-on-the-secrecy-rate-of-in-band-full-duplex-two-way-wiretap-channel-navneet-garg-et-al-2024>(1/2 | 299/318) On the Secrecy Rate of In-Band Full-duplex Two-way Wiretap Channel (Navneet Garg et al., 2024)</a></li><li><a href=#22--300318-a-multiscale-cavity-method-for-sublinear-rank-symmetric-matrix-factorization-jean-barbier-et-al-2024>(2/2 | 300/318) A multiscale cavity method for sublinear-rank symmetric matrix factorization (Jean Barbier et al., 2024)</a></li></ul></li><li><a href=#mathdg-2>math.DG (2)</a><ul><li><a href=#12--301318-asymptotic-behavior-of-unstable-perturbations-of-the-fubini-study-metric-in-ricci-flow-david-garfinkle-et-al-2024>(1/2 | 301/318) Asymptotic behavior of unstable perturbations of the Fubini-Study metric in Ricci flow (David Garfinkle et al., 2024)</a></li><li><a href=#22--302318-pulling-back-symmetric-riemannian-geometry-for-data-analysis-willem-diepeveen-2024>(2/2 | 302/318) Pulling back symmetric Riemannian geometry for data analysis (Willem Diepeveen, 2024)</a></li></ul></li><li><a href=#statml-2>stat.ML (2)</a><ul><li><a href=#12--303318-bridging-domains-with-approximately-shared-features-ziliang-samuel-zhong-et-al-2024>(1/2 | 303/318) Bridging Domains with Approximately Shared Features (Ziliang Samuel Zhong et al., 2024)</a></li><li><a href=#22--304318-provable-mutual-benefits-from-federated-learning-in-privacy-sensitive-domains-nikita-tsoy-et-al-2024>(2/2 | 304/318) Provable Mutual Benefits from Federated Learning in Privacy-Sensitive Domains (Nikita Tsoy et al., 2024)</a></li></ul></li><li><a href=#physicsdata-an-1>physics.data-an (1)</a><ul><li><a href=#11--305318-process-signature-driven-high-spatio-temporal-resolution-alignment-of-multimodal-data-abhishek-hanchate-et-al-2024>(1/1 | 305/318) Process signature-driven high spatio-temporal resolution alignment of multimodal data (Abhishek Hanchate et al., 2024)</a></li></ul></li><li><a href=#cscg-1>cs.CG (1)</a><ul><li><a href=#11--306318-an-algorithm-for-correct-computation-of-reeb-spaces-for-pl-bivariate-fields-amit-chattopadhyay-et-al-2024>(1/1 | 306/318) An Algorithm for Correct Computation of Reeb Spaces for PL Bivariate Fields (Amit Chattopadhyay et al., 2024)</a></li></ul></li><li><a href=#csgt-1>cs.GT (1)</a><ul><li><a href=#11--307318-new-perspectives-in-online-contract-design-heterogeneous-homogeneous-non-myopic-agents-and-team-production-shiliang-zuo-2024>(1/1 | 307/318) New Perspectives in Online Contract Design: Heterogeneous, Homogeneous, Non-myopic Agents and Team Production (Shiliang Zuo, 2024)</a></li></ul></li><li><a href=#mathlo-1>math.LO (1)</a><ul><li><a href=#11--308318-a-study-on-actions-for-atomic-logics-raül-espejo-boix-2024>(1/1 | 308/318) A Study on Actions for Atomic Logics (Raül Espejo-Boix, 2024)</a></li></ul></li><li><a href=#mathoc-1>math.OC (1)</a><ul><li><a href=#11--309318-last-iterate-convergence-of-incremental-methods-and-applications-in-continual-learning-xufeng-cai-et-al-2024>(1/1 | 309/318) Last Iterate Convergence of Incremental Methods and Applications in Continual Learning (Xufeng Cai et al., 2024)</a></li></ul></li><li><a href=#cspl-1>cs.PL (1)</a><ul><li><a href=#11--310318-automatic-generation-of-python-programs-using-context-free-grammars-kamel-yamani-et-al-2024>(1/1 | 310/318) Automatic Generation of Python Programs Using Context-Free Grammars (Kamel Yamani et al., 2024)</a></li></ul></li><li><a href=#cssi-1>cs.SI (1)</a><ul><li><a href=#11--311318-hierarchical-cutting-of-complex-networks-performed-by-random-walks-alexandre-benatti-et-al-2024>(1/1 | 311/318) Hierarchical Cutting of Complex Networks Performed by Random Walks (Alexandre Benatti et al., 2024)</a></li></ul></li><li><a href=#mathco-3>math.CO (3)</a><ul><li><a href=#13--312318-c_2k1-coloring-of-bounded-diameter-graphs-marta-piecyk-2024>(1/3 | 312/318) $C_{2k+1}$-coloring of bounded-diameter graphs (Marta Piecyk, 2024)</a></li><li><a href=#23--313318-a-lower-bound-for-secure-domination-number-of-an-outerplanar-graph-toru-araki-2024>(2/3 | 313/318) A Lower bound for Secure Domination Number of an Outerplanar Graph (Toru Araki, 2024)</a></li><li><a href=#33--314318-tight-bound-for-the-erdős-pósa-property-of-tree-minors-vida-dujmović-et-al-2024>(3/3 | 314/318) Tight bound for the Erdős-Pósa property of tree minors (Vida Dujmović et al., 2024)</a></li></ul></li><li><a href=#csdm-1>cs.DM (1)</a><ul><li><a href=#11--315318-approximating-maximum-edge-2-coloring-by-normalizing-graphs-tobias-mömke-et-al-2024>(1/1 | 315/318) Approximating Maximum Edge 2-Coloring by Normalizing Graphs (Tobias Mömke et al., 2024)</a></li></ul></li><li><a href=#mathst-1>math.ST (1)</a><ul><li><a href=#11--316318-untangling-gaussian-mixtures-eva-fluck-et-al-2024>(1/1 | 316/318) Untangling Gaussian Mixtures (Eva Fluck et al., 2024)</a></li></ul></li><li><a href=#csds-2>cs.DS (2)</a><ul><li><a href=#12--317318-balanced-substructures-in-bicolored-graphs-p-s-ardra-et-al-2024>(1/2 | 317/318) Balanced Substructures in Bicolored Graphs (P. S. Ardra et al., 2024)</a></li><li><a href=#22--318318-arborescences-and-shortest-path-trees-when-colors-matter-p-s-ardra-et-al-2024>(2/2 | 318/318) Arborescences and Shortest Path Trees when Colors Matter (P. S. Ardra et al., 2024)</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:contact.to.akitenkrad@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>contact.to.akitenkrad@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/akitenkrad-blog/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2020 Akitenkrad.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/akitenkrad-blog/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/jquery-3.4.1.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/popper.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/bootstrap.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/navbar.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/plyr.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=https://akitenkrad.github.io/akitenkrad-blog/js/single.js></script><script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.css><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.js></script><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!0})})</script></body></html>