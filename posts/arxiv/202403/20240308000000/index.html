<!doctype html><html><head><title>arXiv @ 2024.03.08</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/bootstrap.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/main.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/navbar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/plyr.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/flag-icon.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/custom/style.css><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM crossorigin=anonymous><script async src=https://cdn.jsdelivr.net/npm/es-module-shims@1/dist/es-module-shims.min.js crossorigin=anonymous></script><script type=importmap>
{
  "imports": {
    "@popperjs/core": "https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.8/dist/esm/popper.min.js",
    "bootstrap": "https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.esm.min.js"
  }
}
</script><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js integrity=sha384-geWF76RCwLtnZ8qwWowPQNguL3RmwHVBC9FhGdlKrxdiJJigb/j/68SIy3Te4Bkz crossorigin=anonymous></script><script type=module src=https://akitenkrad.github.io/akitenkrad-blog/js/custom/custom.js></script><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/fontawesome/css/all.min.css><link rel=icon type=image/png href=https://akitenkrad.github.io/akitenkrad-blog/images/favicons/favicon-96x96_huf1ee13f0caf27d1547f91fb46207d708_13005_42x0_resize_box_3.png><meta property="og:title" content="arXiv @ 2024.03.08"><meta property="og:description" content="Primary Categories astro-ph.EP (1) cs.AI (18) cs.AR (1) cs.CE (3) cs.CL (39) cs.CR (7) cs.CV (43) cs.CY (1) cs.DB (1) cs.DC (2) cs.DS (5) cs.GL (1) cs.GT (7) cs.HC (5) cs.IR (7) cs.IT (1) cs.LG (54) cs.LO (1) cs.MA (1) cs.NE (3) cs.NI (1) cs.PL (1) cs.RO (15) cs.SD (7) cs.SE (6) cs.SI (2) eess.AS (1) eess.IV (7) eess.SP (2) eess.SY (8) math.AT (1) math.CO (2) math.NA (7) math.OC (1) physics."><meta property="og:type" content="article"><meta property="og:url" content="https://akitenkrad.github.io/akitenkrad-blog/posts/arxiv/202403/20240308000000/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-03-08T00:00:00+00:00"><meta property="article:modified_time" content="2024-03-08T00:00:00+00:00"><meta name=description content="arXiv @ 2024.03.08"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/single.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/sidebar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/style.css><script type=text/javascript src=//ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js></script><script language=JavaScript>$(document).ready(function(){$("a[href^='http']:not([href*='"+location.hostname+"'])").attr("target","_blank")})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-1MYYZQG0WE"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-1MYYZQG0WE",{anonymize_ip:!1})}</script></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow" style=position:fixed><div class=container><button class="navbar-toggler navbar-light navbar-for-sidebar" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span>
</button>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav mr-auto"><li class=nav-item><a class=navbar-brand href=/akitenkrad-blog><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png alt=Logo>
Akitenkrad's Blog</a></li></ul><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=/akitenkrad-blog#home>Home</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#about>About</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#recent-posts>Recent Posts</a></li><li class=nav-item><a class=nav-link id=blog-link href=https://akitenkrad.github.io/akitenkrad-blog/posts>Posts</a></li><li class=nav-item><a class=nav-link id=tag-link href=https://akitenkrad.github.io/akitenkrad-blog/tags>Tags</a></li></ul></div></div><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/akitenkrad-blog/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/>Papers</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202205/>2022.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202205/20220518224923/ title="A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks">A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220508162318/ title="A Deep Cascade Model for Multi-Document Reading Comprehension">A Deep Cascade Model for Multi-Document Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220506021208/ title="A Primer in BERTology: What We Know About How BERT Works">A Primer in BERTology: What We Know About How BERT Works</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220529131339/ title="Attention Is All You Need">Attention Is All You Need</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220503010000/ title="DeBERTa: Decoding-Enhanced BERT with Disentangled Attention">DeBERTa: Decoding-Enhanced BERT with Disentangled Attention</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220505222900/ title="Dense Passage Retrieval for Open-Domain Question Answering">Dense Passage Retrieval for Open-Domain Question Answering</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220511010217/ title="Multi-Style Generative Reading Comprehension">Multi-Style Generative Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220530102936/ title="Neural Machine Translation of Rare Words with Subword Units">Neural Machine Translation of Rare Words with Subword Units</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220523223206/ title="RoBERTa: A Robustly Optimized BERT Pretraining Approach">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220520124748/ title="Semi-Supervised Classification with Graph Convolutional Networks">Semi-Supervised Classification with Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220509110738/ title="Survey on graph embeddings and their applications to machine learning problems on graphs">Survey on graph embeddings and their applications to machine learning problems on graphs</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220514151839/ title="UnitedQA: A Hybrid Approach for Open Domain Question Answering">UnitedQA: A Hybrid Approach for Open Domain Question Answering</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202206/>2022.06</a><ul><li><a href=/akitenkrad-blog/posts/papers/202206/20220612105422/ title="Attributed Network Embedding for Learning in a Dynamic Environment">Attributed Network Embedding for Learning in a Dynamic Environment</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220608085622/ title="CodeBERT: A Pre-Trained Model for Programming and Natural Languages">CodeBERT: A Pre-Trained Model for Programming and Natural Languages</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220618223844/ title="High-order Proximity Preserved Embedding for Dynamic Networks">High-order Proximity Preserved Embedding for Dynamic Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220602171700/ title="S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension">S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202207/>2022.07</a><ul><li><a href=/akitenkrad-blog/posts/papers/202207/20220727145036/ title="Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions">Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions</a></li><li><a href=/akitenkrad-blog/posts/papers/202207/20220726163444/ title="Dynamic Network Embedding Survey">Dynamic Network Embedding Survey</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/papers/202208/20220802103319/ title="Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN">Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/papers/202209/20220909180827/ title="Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting">Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202305/>2023.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202305/20230513094804/ title="Improving Language Understanding by Generative Pre-Training">Improving Language Understanding by Generative Pre-Training</a></li><li><a href=/akitenkrad-blog/posts/papers/202305/20230514200241/ title="Language Models are Unsupervised Multitask Learners">Language Models are Unsupervised Multitask Learners</a></li></ul></li><li><a href=/akitenkrad-blog/posts/papers/202306/ title=2023.06>2023.06</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/>arXiv</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202307/>2023.07</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230701000000/ title="arXiv @ 2023.07.01">arXiv @ 2023.07.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230702000000/ title="arXiv @ 2023.07.02">arXiv @ 2023.07.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230703000000/ title="arXiv @ 2023.07.03">arXiv @ 2023.07.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230704000000/ title="arXiv @ 2023.07.04">arXiv @ 2023.07.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230705000000/ title="arXiv @ 2023.07.05">arXiv @ 2023.07.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230706000000/ title="arXiv @ 2023.07.06">arXiv @ 2023.07.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230707000000/ title="arXiv @ 2023.07.07">arXiv @ 2023.07.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230708000000/ title="arXiv @ 2023.07.08">arXiv @ 2023.07.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230709000000/ title="arXiv @ 2023.07.09">arXiv @ 2023.07.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230710000000/ title="arXiv @ 2023.07.10">arXiv @ 2023.07.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230711000000/ title="arXiv @ 2023.07.11">arXiv @ 2023.07.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230712000000/ title="arXiv @ 2023.07.12">arXiv @ 2023.07.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230713000000/ title="arXiv @ 2023.07.13">arXiv @ 2023.07.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230714000000/ title="arXiv @ 2023.07.14">arXiv @ 2023.07.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230715000000/ title="arXiv @ 2023.07.15">arXiv @ 2023.07.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230716000000/ title="arXiv @ 2023.07.16">arXiv @ 2023.07.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230717000000/ title="arXiv @ 2023.07.17">arXiv @ 2023.07.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230718000000/ title="arXiv @ 2023.07.18">arXiv @ 2023.07.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230719000000/ title="arXiv @ 2023.07.19">arXiv @ 2023.07.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230720000000/ title="arXiv @ 2023.07.20">arXiv @ 2023.07.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230721000000/ title="arXiv @ 2023.07.21">arXiv @ 2023.07.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230722000000/ title="arXiv @ 2023.07.22">arXiv @ 2023.07.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230723000000/ title="arXiv @ 2023.07.23">arXiv @ 2023.07.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230724000000/ title="arXiv @ 2023.07.24">arXiv @ 2023.07.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230725000000/ title="arXiv @ 2023.07.25">arXiv @ 2023.07.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230726000000/ title="arXiv @ 2023.07.26">arXiv @ 2023.07.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230727000000/ title="arXiv @ 2023.07.27">arXiv @ 2023.07.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230728000000/ title="arXiv @ 2023.07.28">arXiv @ 2023.07.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230729000000/ title="arXiv @ 2023.07.29">arXiv @ 2023.07.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230730000000/ title="arXiv @ 2023.07.30">arXiv @ 2023.07.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230731000000/ title="arXiv @ 2023.07.31">arXiv @ 2023.07.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202308/>2023.08</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230801000000/ title="arXiv @ 2023.08.01">arXiv @ 2023.08.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230802000000/ title="arXiv @ 2023.08.02">arXiv @ 2023.08.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230803000000/ title="arXiv @ 2023.08.03">arXiv @ 2023.08.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230804000000/ title="arXiv @ 2023.08.04">arXiv @ 2023.08.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230805000000/ title="arXiv @ 2023.08.05">arXiv @ 2023.08.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230806000000/ title="arXiv @ 2023.08.06">arXiv @ 2023.08.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230807000000/ title="arXiv @ 2023.08.07">arXiv @ 2023.08.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230808000000/ title="arXiv @ 2023.08.08">arXiv @ 2023.08.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230809000000/ title="arXiv @ 2023.08.09">arXiv @ 2023.08.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230810000000/ title="arXiv @ 2023.08.10">arXiv @ 2023.08.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230811000000/ title="arXiv @ 2023.08.11">arXiv @ 2023.08.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230812000000/ title="arXiv @ 2023.08.12">arXiv @ 2023.08.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230813000000/ title="arXiv @ 2023.08.13">arXiv @ 2023.08.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230814000000/ title="arXiv @ 2023.08.14">arXiv @ 2023.08.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230815000000/ title="arXiv @ 2023.08.15">arXiv @ 2023.08.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230816000000/ title="arXiv @ 2023.08.16">arXiv @ 2023.08.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230817000000/ title="arXiv @ 2023.08.17">arXiv @ 2023.08.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230818000000/ title="arXiv @ 2023.08.18">arXiv @ 2023.08.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230819000000/ title="arXiv @ 2023.08.19">arXiv @ 2023.08.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230820000000/ title="arXiv @ 2023.08.20">arXiv @ 2023.08.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230821000000/ title="arXiv @ 2023.08.21">arXiv @ 2023.08.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230822000000/ title="arXiv @ 2023.08.22">arXiv @ 2023.08.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230823000000/ title="arXiv @ 2023.08.23">arXiv @ 2023.08.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230824000000/ title="arXiv @ 2023.08.24">arXiv @ 2023.08.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230825000000/ title="arXiv @ 2023.08.25">arXiv @ 2023.08.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230826000000/ title="arXiv @ 2023.08.26">arXiv @ 2023.08.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230827000000/ title="arXiv @ 2023.08.27">arXiv @ 2023.08.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230828000000/ title="arXiv @ 2023.08.28">arXiv @ 2023.08.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230829000000/ title="arXiv @ 2023.08.29">arXiv @ 2023.08.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230830000000/ title="arXiv @ 2023.08.30">arXiv @ 2023.08.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230831000000/ title="arXiv @ 2023.08.31">arXiv @ 2023.08.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202309/>2023.09</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230901000000/ title="arXiv @ 2023.09.01">arXiv @ 2023.09.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230902000000/ title="arXiv @ 2023.09.02">arXiv @ 2023.09.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230903000000/ title="arXiv @ 2023.09.03">arXiv @ 2023.09.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230904000000/ title="arXiv @ 2023.09.04">arXiv @ 2023.09.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230905000000/ title="arXiv @ 2023.09.05">arXiv @ 2023.09.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230906000000/ title="arXiv @ 2023.09.06">arXiv @ 2023.09.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230907000000/ title="arXiv @ 2023.09.07">arXiv @ 2023.09.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230908000000/ title="arXiv @ 2023.09.08">arXiv @ 2023.09.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230909000000/ title="arXiv @ 2023.09.09">arXiv @ 2023.09.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230910000000/ title="arXiv @ 2023.09.10">arXiv @ 2023.09.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230911000000/ title="arXiv @ 2023.09.11">arXiv @ 2023.09.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230912000000/ title="arXiv @ 2023.09.12">arXiv @ 2023.09.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230913000000/ title="arXiv @ 2023.09.13">arXiv @ 2023.09.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230914000000/ title="arXiv @ 2023.09.14">arXiv @ 2023.09.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230915000000/ title="arXiv @ 2023.09.15">arXiv @ 2023.09.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230916000000/ title="arXiv @ 2023.09.16">arXiv @ 2023.09.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230917000000/ title="arXiv @ 2023.09.17">arXiv @ 2023.09.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230918000000/ title="arXiv @ 2023.09.18">arXiv @ 2023.09.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230919000000/ title="arXiv @ 2023.09.19">arXiv @ 2023.09.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230920000000/ title="arXiv @ 2023.09.20">arXiv @ 2023.09.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230921000000/ title="arXiv @ 2023.09.21">arXiv @ 2023.09.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230922000000/ title="arXiv @ 2023.09.22">arXiv @ 2023.09.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230923000000/ title="arXiv @ 2023.09.23">arXiv @ 2023.09.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230924000000/ title="arXiv @ 2023.09.24">arXiv @ 2023.09.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230925000000/ title="arXiv @ 2023.09.25">arXiv @ 2023.09.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230926000000/ title="arXiv @ 2023.09.26">arXiv @ 2023.09.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230927000000/ title="arXiv @ 2023.09.27">arXiv @ 2023.09.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230928000000/ title="arXiv @ 2023.09.28">arXiv @ 2023.09.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230929000000/ title="arXiv @ 2023.09.29">arXiv @ 2023.09.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230930000000/ title="arXiv @ 2023.09.30">arXiv @ 2023.09.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202310/>2023.1</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231001000000/ title="arXiv @ 2023.10.01">arXiv @ 2023.10.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231002000000/ title="arXiv @ 2023.10.02">arXiv @ 2023.10.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231003000000/ title="arXiv @ 2023.10.03">arXiv @ 2023.10.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231004000000/ title="arXiv @ 2023.10.04">arXiv @ 2023.10.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231005000000/ title="arXiv @ 2023.10.05">arXiv @ 2023.10.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231006000000/ title="arXiv @ 2023.10.06">arXiv @ 2023.10.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231007000000/ title="arXiv @ 2023.10.07">arXiv @ 2023.10.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231008000000/ title="arXiv @ 2023.10.08">arXiv @ 2023.10.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231009000000/ title="arXiv @ 2023.10.09">arXiv @ 2023.10.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231010000000/ title="arXiv @ 2023.10.10">arXiv @ 2023.10.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231011000000/ title="arXiv @ 2023.10.11">arXiv @ 2023.10.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231012000000/ title="arXiv @ 2023.10.12">arXiv @ 2023.10.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231013000000/ title="arXiv @ 2023.10.13">arXiv @ 2023.10.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231014000000/ title="arXiv @ 2023.10.14">arXiv @ 2023.10.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231015000000/ title="arXiv @ 2023.10.15">arXiv @ 2023.10.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231016000000/ title="arXiv @ 2023.10.16">arXiv @ 2023.10.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231017000000/ title="arXiv @ 2023.10.17">arXiv @ 2023.10.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231018000000/ title="arXiv @ 2023.10.18">arXiv @ 2023.10.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231019000000/ title="arXiv @ 2023.10.19">arXiv @ 2023.10.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231020000000/ title="arXiv @ 2023.10.20">arXiv @ 2023.10.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231021000000/ title="arXiv @ 2023.10.21">arXiv @ 2023.10.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231022000000/ title="arXiv @ 2023.10.22">arXiv @ 2023.10.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231023000000/ title="arXiv @ 2023.10.23">arXiv @ 2023.10.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231024000000/ title="arXiv @ 2023.10.24">arXiv @ 2023.10.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231025000000/ title="arXiv @ 2023.10.25">arXiv @ 2023.10.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231026000000/ title="arXiv @ 2023.10.26">arXiv @ 2023.10.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231027000000/ title="arXiv @ 2023.10.27">arXiv @ 2023.10.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231028000000/ title="arXiv @ 2023.10.28">arXiv @ 2023.10.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231029000000/ title="arXiv @ 2023.10.29">arXiv @ 2023.10.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231030000000/ title="arXiv @ 2023.10.30">arXiv @ 2023.10.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231031000000/ title="arXiv @ 2023.10.31">arXiv @ 2023.10.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202311/>2023.11</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231101000000/ title="arXiv @ 2023.11.01">arXiv @ 2023.11.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231102000000/ title="arXiv @ 2023.11.02">arXiv @ 2023.11.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231103000000/ title="arXiv @ 2023.11.03">arXiv @ 2023.11.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231104000000/ title="arXiv @ 2023.11.04">arXiv @ 2023.11.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231105000000/ title="arXiv @ 2023.11.05">arXiv @ 2023.11.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231106000000/ title="arXiv @ 2023.11.06">arXiv @ 2023.11.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231107000000/ title="arXiv @ 2023.11.07">arXiv @ 2023.11.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231108000000/ title="arXiv @ 2023.11.08">arXiv @ 2023.11.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231109000000/ title="arXiv @ 2023.11.09">arXiv @ 2023.11.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231110000000/ title="arXiv @ 2023.11.10">arXiv @ 2023.11.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231111000000/ title="arXiv @ 2023.11.11">arXiv @ 2023.11.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231112000000/ title="arXiv @ 2023.11.12">arXiv @ 2023.11.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231113000000/ title="arXiv @ 2023.11.13">arXiv @ 2023.11.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231114000000/ title="arXiv @ 2023.11.14">arXiv @ 2023.11.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231115000000/ title="arXiv @ 2023.11.15">arXiv @ 2023.11.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231116000000/ title="arXiv @ 2023.11.16">arXiv @ 2023.11.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231117000000/ title="arXiv @ 2023.11.17">arXiv @ 2023.11.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231118000000/ title="arXiv @ 2023.11.18">arXiv @ 2023.11.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231119000000/ title="arXiv @ 2023.11.19">arXiv @ 2023.11.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231120000000/ title="arXiv @ 2023.11.20">arXiv @ 2023.11.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231121000000/ title="arXiv @ 2023.11.21">arXiv @ 2023.11.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231122000000/ title="arXiv @ 2023.11.22">arXiv @ 2023.11.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231123000000/ title="arXiv @ 2023.11.23">arXiv @ 2023.11.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231124000000/ title="arXiv @ 2023.11.24">arXiv @ 2023.11.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231125000000/ title="arXiv @ 2023.11.25">arXiv @ 2023.11.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231126000000/ title="arXiv @ 2023.11.26">arXiv @ 2023.11.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231127000000/ title="arXiv @ 2023.11.27">arXiv @ 2023.11.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231128000000/ title="arXiv @ 2023.11.28">arXiv @ 2023.11.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231129000000/ title="arXiv @ 2023.11.29">arXiv @ 2023.11.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231130000000/ title="arXiv @ 2023.11.30">arXiv @ 2023.11.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202312/>2023.12</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231201000000/ title="arXiv @ 2023.12.01">arXiv @ 2023.12.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231202000000/ title="arXiv @ 2023.12.02">arXiv @ 2023.12.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231203000000/ title="arXiv @ 2023.12.03">arXiv @ 2023.12.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231204000000/ title="arXiv @ 2023.12.04">arXiv @ 2023.12.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231205000000/ title="arXiv @ 2023.12.05">arXiv @ 2023.12.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231206000000/ title="arXiv @ 2023.12.06">arXiv @ 2023.12.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231207000000/ title="arXiv @ 2023.12.07">arXiv @ 2023.12.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231208000000/ title="arXiv @ 2023.12.08">arXiv @ 2023.12.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231209000000/ title="arXiv @ 2023.12.09">arXiv @ 2023.12.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231210000000/ title="arXiv @ 2023.12.10">arXiv @ 2023.12.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231211000000/ title="arXiv @ 2023.12.11">arXiv @ 2023.12.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231212000000/ title="arXiv @ 2023.12.12">arXiv @ 2023.12.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231213000000/ title="arXiv @ 2023.12.13">arXiv @ 2023.12.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231214000000/ title="arXiv @ 2023.12.14">arXiv @ 2023.12.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231215000000/ title="arXiv @ 2023.12.15">arXiv @ 2023.12.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231216000000/ title="arXiv @ 2023.12.16">arXiv @ 2023.12.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231217000000/ title="arXiv @ 2023.12.17">arXiv @ 2023.12.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231218000000/ title="arXiv @ 2023.12.18">arXiv @ 2023.12.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231219000000/ title="arXiv @ 2023.12.19">arXiv @ 2023.12.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231220000000/ title="arXiv @ 2023.12.20">arXiv @ 2023.12.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231221000000/ title="arXiv @ 2023.12.21">arXiv @ 2023.12.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231222000000/ title="arXiv @ 2023.12.22">arXiv @ 2023.12.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231223000000/ title="arXiv @ 2023.12.23">arXiv @ 2023.12.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231224000000/ title="arXiv @ 2023.12.24">arXiv @ 2023.12.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231225000000/ title="arXiv @ 2023.12.25">arXiv @ 2023.12.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231226000000/ title="arXiv @ 2023.12.26">arXiv @ 2023.12.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231227000000/ title="arXiv @ 2023.12.27">arXiv @ 2023.12.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231228000000/ title="arXiv @ 2023.12.28">arXiv @ 2023.12.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231229000000/ title="arXiv @ 2023.12.29">arXiv @ 2023.12.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231230000000/ title="arXiv @ 2023.12.30">arXiv @ 2023.12.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231231000000/ title="arXiv @ 2023.12.31">arXiv @ 2023.12.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202401/>2024.01</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240101000000/ title="arXiv @ 2024.01.01">arXiv @ 2024.01.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240102000000/ title="arXiv @ 2024.01.02">arXiv @ 2024.01.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240103000000/ title="arXiv @ 2024.01.03">arXiv @ 2024.01.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240104000000/ title="arXiv @ 2024.01.04">arXiv @ 2024.01.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240105000000/ title="arXiv @ 2024.01.05">arXiv @ 2024.01.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240106000000/ title="arXiv @ 2024.01.06">arXiv @ 2024.01.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240107000000/ title="arXiv @ 2024.01.07">arXiv @ 2024.01.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240108000000/ title="arXiv @ 2024.01.08">arXiv @ 2024.01.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240109000000/ title="arXiv @ 2024.01.09">arXiv @ 2024.01.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240110000000/ title="arXiv @ 2024.01.10">arXiv @ 2024.01.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240111000000/ title="arXiv @ 2024.01.11">arXiv @ 2024.01.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240112000000/ title="arXiv @ 2024.01.12">arXiv @ 2024.01.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240113000000/ title="arXiv @ 2024.01.13">arXiv @ 2024.01.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240114000000/ title="arXiv @ 2024.01.14">arXiv @ 2024.01.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240115000000/ title="arXiv @ 2024.01.15">arXiv @ 2024.01.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240116000000/ title="arXiv @ 2024.01.16">arXiv @ 2024.01.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240117000000/ title="arXiv @ 2024.01.17">arXiv @ 2024.01.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240118000000/ title="arXiv @ 2024.01.18">arXiv @ 2024.01.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240119000000/ title="arXiv @ 2024.01.19">arXiv @ 2024.01.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240120000000/ title="arXiv @ 2024.01.20">arXiv @ 2024.01.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240121000000/ title="arXiv @ 2024.01.21">arXiv @ 2024.01.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240122000000/ title="arXiv @ 2024.01.22">arXiv @ 2024.01.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240123000000/ title="arXiv @ 2024.01.23">arXiv @ 2024.01.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240124000000/ title="arXiv @ 2024.01.24">arXiv @ 2024.01.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240125000000/ title="arXiv @ 2024.01.25">arXiv @ 2024.01.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240126000000/ title="arXiv @ 2024.01.26">arXiv @ 2024.01.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240127000000/ title="arXiv @ 2024.01.27">arXiv @ 2024.01.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240128000000/ title="arXiv @ 2024.01.28">arXiv @ 2024.01.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240129000000/ title="arXiv @ 2024.01.29">arXiv @ 2024.01.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240130000000/ title="arXiv @ 2024.01.30">arXiv @ 2024.01.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240131000000/ title="arXiv @ 2024.01.31">arXiv @ 2024.01.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202402/>2024.02</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240201000000/ title="arXiv @ 2024.02.01">arXiv @ 2024.02.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240202000000/ title="arXiv @ 2024.02.02">arXiv @ 2024.02.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240203000000/ title="arXiv @ 2024.02.03">arXiv @ 2024.02.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240204000000/ title="arXiv @ 2024.02.04">arXiv @ 2024.02.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240205000000/ title="arXiv @ 2024.02.05">arXiv @ 2024.02.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240206000000/ title="arXiv @ 2024.02.06">arXiv @ 2024.02.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/ title="arXiv @ 2024.02.07">arXiv @ 2024.02.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/ title="arXiv @ 2024.02.08">arXiv @ 2024.02.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/ title="arXiv @ 2024.02.09">arXiv @ 2024.02.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/ title="arXiv @ 2024.02.10">arXiv @ 2024.02.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240211000000/ title="arXiv @ 2024.02.11">arXiv @ 2024.02.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/ title="arXiv @ 2024.02.12">arXiv @ 2024.02.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/ title="arXiv @ 2024.02.13">arXiv @ 2024.02.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/ title="arXiv @ 2024.02.14">arXiv @ 2024.02.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/ title="arXiv @ 2024.02.15">arXiv @ 2024.02.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240216000000/ title="arXiv @ 2024.02.16">arXiv @ 2024.02.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/ title="arXiv @ 2024.02.17">arXiv @ 2024.02.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240218000000/ title="arXiv @ 2024.02.18">arXiv @ 2024.02.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/ title="arXiv @ 2024.02.19">arXiv @ 2024.02.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/ title="arXiv @ 2024.02.20">arXiv @ 2024.02.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/ title="arXiv @ 2024.02.21">arXiv @ 2024.02.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/ title="arXiv @ 2024.02.22">arXiv @ 2024.02.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/ title="arXiv @ 2024.02.23">arXiv @ 2024.02.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/ title="arXiv @ 2024.02.24">arXiv @ 2024.02.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/ title="arXiv @ 2024.02.25">arXiv @ 2024.02.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/ title="arXiv @ 2024.02.26">arXiv @ 2024.02.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/ title="arXiv @ 2024.02.27">arXiv @ 2024.02.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/ title="arXiv @ 2024.02.28">arXiv @ 2024.02.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/ title="arXiv @ 2024.02.29">arXiv @ 2024.02.29</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/202403/>2024.03</a><ul class=active><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/ title="arXiv @ 2024.03.01">arXiv @ 2024.03.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/ title="arXiv @ 2024.03.02">arXiv @ 2024.03.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/ title="arXiv @ 2024.03.03">arXiv @ 2024.03.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240304000000/ title="arXiv @ 2024.03.04">arXiv @ 2024.03.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/ title="arXiv @ 2024.03.05">arXiv @ 2024.03.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/ title="arXiv @ 2024.03.06">arXiv @ 2024.03.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/ title="arXiv @ 2024.03.07">arXiv @ 2024.03.07</a></li><li><a class=active href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/ title="arXiv @ 2024.03.08">arXiv @ 2024.03.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/ title="arXiv @ 2024.03.09">arXiv @ 2024.03.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/ title="arXiv @ 2024.03.10">arXiv @ 2024.03.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/ title="arXiv @ 2024.03.11">arXiv @ 2024.03.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/ title="arXiv @ 2024.03.12">arXiv @ 2024.03.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/ title="arXiv @ 2024.03.13">arXiv @ 2024.03.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/ title="arXiv @ 2024.03.14">arXiv @ 2024.03.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/ title="arXiv @ 2024.03.15">arXiv @ 2024.03.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/ title="arXiv @ 2024.03.16">arXiv @ 2024.03.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/ title="arXiv @ 2024.03.17">arXiv @ 2024.03.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/ title="arXiv @ 2024.03.18">arXiv @ 2024.03.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240319000000/ title="arXiv @ 2024.03.19">arXiv @ 2024.03.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/ title="arXiv @ 2024.03.20">arXiv @ 2024.03.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/ title="arXiv @ 2024.03.21">arXiv @ 2024.03.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/ title="arXiv @ 2024.03.22">arXiv @ 2024.03.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240323000000/ title="arXiv @ 2024.03.23">arXiv @ 2024.03.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/ title="arXiv @ 2024.03.24">arXiv @ 2024.03.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240325000000/ title="arXiv @ 2024.03.25">arXiv @ 2024.03.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240326000000/ title="arXiv @ 2024.03.26">arXiv @ 2024.03.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240327000000/ title="arXiv @ 2024.03.27">arXiv @ 2024.03.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240328000000/ title="arXiv @ 2024.03.28">arXiv @ 2024.03.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240329000000/ title="arXiv @ 2024.03.29">arXiv @ 2024.03.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240330000000/ title="arXiv @ 2024.03.30">arXiv @ 2024.03.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240331000000/ title="arXiv @ 2024.03.31">arXiv @ 2024.03.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202404/>2024.04</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240401000000/ title="arXiv @ 2024.04.01">arXiv @ 2024.04.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240402000000/ title="arXiv @ 2024.04.02">arXiv @ 2024.04.02</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/algorithms/>Algorithms</a><ul><li><a href=/akitenkrad-blog/posts/algorithms/bandit-algorithm-basic/ title="Bandit Algorithm Basic">Bandit Algorithm Basic</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/conference/>Conference</a><ul><li><a href=/akitenkrad-blog/posts/conference/acl/ title="Annual Meeting of the Association for Computational Linguistics">Annual Meeting of the Association for Computational Linguistics</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/>Figures</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/figures/202208/20220822092748/ title=1ヶ月当たりの実労働時間の推移>1ヶ月当たりの実労働時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220813115200/ title=コンビニエンスストアの店舗数の推移>コンビニエンスストアの店舗数の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220821113322/ title=地域別1世帯当たり1ヶ月間の支出額の推移>地域別1世帯当たり1ヶ月間の支出額の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220828111836/ title="夫婦別子供の有無別生活時間 (2016)">夫婦別子供の有無別生活時間 (2016)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220827103615/ title=家族類型の変遷>家族類型の変遷</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220815131016/ title="年齢別大学院入学者数 (2021)">年齢別大学院入学者数 (2021)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220818122628/ title=情報サービス業における企業特殊的人的資本（名目）の推移>情報サービス業における企業特殊的人的資本（名目）の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220824095127/ title=有業者の社会生活における活動の変化>有業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220814175022/ title=東京23区の乗用車保有台数>東京23区の乗用車保有台数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220817121117/ title="業種別企業特殊的人的資本 (2018)">業種別企業特殊的人的資本 (2018)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220825094111/ title=無業者の社会生活における活動の変化>無業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220823104100/ title=睡眠時間の推移>睡眠時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220826102652/ title=総人口の推移>総人口の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220820225528/ title=都道府県別国公立別学校数・学生数>都道府県別国公立別学校数・学生数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220816120845/ title="都道府県別消費者物価指数/教育（全国平均=100）">都道府県別消費者物価指数/教育（全国平均=100）</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220812173814/ title=金沢市の夏の月平均気温の遷移>金沢市の夏の月平均気温の遷移</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/figures/202209/20220907122906/ title="ACL-2022 WordCloud">ACL-2022 WordCloud</a></li></ul></li></ul></li><li><a href=/akitenkrad-blog/posts/latex/ title="Latex Mathematics Syntax Guide">Latex Mathematics Syntax Guide</a></li><li><a href=/akitenkrad-blog/posts/markdown/ title="Markdown Sample">Markdown Sample</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/akitenkrad-blog/posts/arxiv/202403/20240308000000/hero.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_120x120_fit_box_3.png alt="Author Image"><h5 class=author-name></h5><p>Friday, Mar 8, 2024</p></div><div class=title><h1>arXiv @ 2024.03.08</h1></div><div class=taxonomy-terms><ul><li class=rounded><a href=/akitenkrad-blog/tags/arxiv class="btn, btn-sm">arXiv</a></li><li class=rounded><a href=/akitenkrad-blog/tags/published2024 class="btn, btn-sm">2024</a></li></ul></div><div class=post-content id=post-content><figure style=border:none;width:100%;display:flex;justify-content:center><iframe src=pie.html width=900 height=620 style=border:none></iframe></figure><h2 id=primary-categories>Primary Categories</h2><ul><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/#astro-phep-1>astro-ph.EP (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/#csai-18>cs.AI (18)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/#csar-1>cs.AR (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/#csce-3>cs.CE (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/#cscl-39>cs.CL (39)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/#cscr-7>cs.CR (7)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/#cscv-43>cs.CV (43)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/#cscy-1>cs.CY (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/#csdb-1>cs.DB (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/#csdc-2>cs.DC (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/#csds-5>cs.DS (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/#csgl-1>cs.GL (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/#csgt-7>cs.GT (7)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/#cshc-5>cs.HC (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/#csir-7>cs.IR (7)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/#csit-1>cs.IT (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/#cslg-54>cs.LG (54)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/#cslo-1>cs.LO (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/#csma-1>cs.MA (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/#csne-3>cs.NE (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/#csni-1>cs.NI (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/#cspl-1>cs.PL (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/#csro-15>cs.RO (15)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/#cssd-7>cs.SD (7)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/#csse-6>cs.SE (6)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/#cssi-2>cs.SI (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/#eessas-1>eess.AS (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/#eessiv-7>eess.IV (7)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/#eesssp-2>eess.SP (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/#eesssy-8>eess.SY (8)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/#mathat-1>math.AT (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/#mathco-2>math.CO (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/#mathna-7>math.NA (7)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/#mathoc-1>math.OC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/#physicschem-ph-1>physics.chem-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/#physicsoptics-1>physics.optics (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/#physicssoc-ph-1>physics.soc-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/#q-bioto-1>q-bio.TO (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/#q-fincp-1>q-fin.CP (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/#quant-ph-1>quant-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/#statml-3>stat.ML (3)</a></li></ul><h2 id=keywords>Keywords</h2><table border=1 class=dataframe><thead><tr style=text-align:right><th>keyword</th><th>cs.AI</th><th>cs.CL</th><th>cs.CV</th><th>cs.LG</th><th>cs.RO</th></tr></thead><tbody><tr><td>AI-generated Text Detection</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Active Learning</td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><td>Adversarial Attack</td><td></td><td></td><td>1</td><td>3</td><td></td></tr><tr><td>Adversarial Learning</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Anomaly Detection</td><td></td><td></td><td>1</td><td>1</td><td>1</td></tr><tr><td>Autoencoder</td><td></td><td></td><td>1</td><td>1</td><td>1</td></tr><tr><td>BERT</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>BLEU</td><td></td><td>1</td><td></td><td>1</td><td></td></tr><tr><td>Benchmarking</td><td>4</td><td>13</td><td>9</td><td>10</td><td>1</td></tr><tr><td>Black Box</td><td></td><td></td><td>1</td><td>1</td><td></td></tr><tr><td>Causal Intervention</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Claude</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Clustering</td><td>1</td><td></td><td>1</td><td>5</td><td>1</td></tr><tr><td>Code Generation</td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>Continual Learning</td><td></td><td></td><td>1</td><td>1</td><td>1</td></tr><tr><td>Contrastive Learning</td><td>1</td><td></td><td>4</td><td>2</td><td></td></tr><tr><td>ControlNet</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Convolution</td><td></td><td></td><td>1</td><td>1</td><td>1</td></tr><tr><td>Convolutional Neural Network</td><td></td><td></td><td>2</td><td>1</td><td>2</td></tr><tr><td>Counter-factual</td><td></td><td></td><td>1</td><td>4</td><td></td></tr><tr><td>Data Augmentation</td><td>1</td><td></td><td>3</td><td></td><td></td></tr><tr><td>Dense Retrieval</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Differential Privacy</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Diffusion Model</td><td></td><td></td><td>4</td><td>3</td><td>1</td></tr><tr><td>Distribution Shift</td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><td>Domain Adaptation</td><td></td><td></td><td>3</td><td></td><td></td></tr><tr><td>Fact Verification</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Federated Learning</td><td></td><td></td><td></td><td>4</td><td></td></tr><tr><td>Few-shot</td><td></td><td>3</td><td>1</td><td>2</td><td></td></tr><tr><td>Few-shot Learning</td><td></td><td>2</td><td>1</td><td>2</td><td></td></tr><tr><td>Fine-tuning</td><td>1</td><td>8</td><td>4</td><td>6</td><td>2</td></tr><tr><td>Foundation Model</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>GLUE</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>GPT</td><td>3</td><td>8</td><td></td><td></td><td></td></tr><tr><td>GPT-3</td><td>2</td><td>4</td><td></td><td></td><td></td></tr><tr><td>GPT-3.5</td><td>2</td><td>3</td><td></td><td></td><td></td></tr><tr><td>GPT-4</td><td>2</td><td>5</td><td></td><td></td><td></td></tr><tr><td>Gemini</td><td></td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>Generative Adversarial Network</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Geometry</td><td></td><td></td><td>2</td><td>1</td><td>1</td></tr><tr><td>Graph</td><td>2</td><td>1</td><td>3</td><td>10</td><td></td></tr><tr><td>Graph Anomaly Detection</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Graph Convolutional Network</td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><td>Graph Neural Network</td><td>3</td><td></td><td></td><td>12</td><td></td></tr><tr><td>Grounding</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Hallucination Detection</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>High-Resource</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Human Intervention</td><td>1</td><td></td><td></td><td>1</td><td>2</td></tr><tr><td>Image2text</td><td></td><td></td><td>3</td><td></td><td></td></tr><tr><td>In-context Learning</td><td></td><td>6</td><td></td><td></td><td></td></tr><tr><td>Information Compression</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Information Retrieval</td><td></td><td>1</td><td></td><td>1</td><td></td></tr><tr><td>InstructGPT</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Instruction Following</td><td>1</td><td>3</td><td></td><td></td><td></td></tr><tr><td>Instruction Tuning</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Knowledge Distillation</td><td></td><td></td><td>3</td><td>6</td><td>2</td></tr><tr><td>Knowledge Graph</td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><td>Knowledge Transfer</td><td>1</td><td></td><td>1</td><td></td><td></td></tr><tr><td>LLaMA</td><td></td><td>5</td><td></td><td>1</td><td></td></tr><tr><td>LSTM</td><td>1</td><td>1</td><td></td><td>3</td><td></td></tr><tr><td>Large Language Model</td><td>13</td><td>39</td><td>5</td><td>5</td><td></td></tr><tr><td>Logistic Regression</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Low-Resource</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Markov Decision Process</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Message-Passing</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Meta Learning</td><td></td><td></td><td>1</td><td>1</td><td></td></tr><tr><td>Mistral</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Model Pruning</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Multi-modal</td><td>1</td><td>4</td><td>4</td><td>2</td><td>5</td></tr><tr><td>Named Entity Recognition</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Natural Language Understanding</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Neural Machine Translation</td><td>1</td><td>13</td><td></td><td>1</td><td></td></tr><tr><td>Node Embedding</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Object Detection</td><td></td><td></td><td>6</td><td></td><td></td></tr><tr><td>Open-Domain Dialogue</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Optical Character Recognition</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Out-of-distribution</td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><td>Pre-trained Language Model</td><td></td><td>3</td><td>1</td><td></td><td></td></tr><tr><td>Prompt</td><td>5</td><td>5</td><td>1</td><td></td><td></td></tr><tr><td>Pruning</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Quantization</td><td></td><td>1</td><td></td><td>1</td><td></td></tr><tr><td>Question Answering</td><td></td><td>4</td><td>1</td><td></td><td></td></tr><tr><td>Reasoning</td><td>2</td><td>4</td><td>4</td><td></td><td>1</td></tr><tr><td>Recommendation</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Recommender System</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Reconstruction Loss</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Recurrent Neural Network</td><td></td><td>2</td><td>1</td><td></td><td></td></tr><tr><td>Reinforcement Learning</td><td></td><td>1</td><td></td><td>5</td><td>3</td></tr><tr><td>Reinforcement Learning from Human Feedback</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Representation Learning</td><td>2</td><td></td><td>4</td><td>2</td><td></td></tr><tr><td>Retrieval-Augmented Generation</td><td></td><td>3</td><td></td><td></td><td></td></tr><tr><td>RoBERTa</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Rouge</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Sample Size</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Self-Attention</td><td></td><td></td><td>1</td><td>1</td><td></td></tr><tr><td>Self-Distillation</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Self-supervised Learning</td><td></td><td></td><td>5</td><td>3</td><td>2</td></tr><tr><td>Self-supervised Pre-training</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Semi-Supervised Learning</td><td></td><td>1</td><td>2</td><td>1</td><td></td></tr><tr><td>Simulation</td><td>1</td><td></td><td>2</td><td>2</td><td>8</td></tr><tr><td>Simulator</td><td>1</td><td></td><td>2</td><td>2</td><td>8</td></tr><tr><td>Summarization</td><td></td><td>4</td><td></td><td>1</td><td></td></tr><tr><td>Supervised Learning</td><td></td><td>2</td><td>4</td><td>7</td><td>2</td></tr><tr><td>Text Generation</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Text Segmentation</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Text Summarization</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Text Understanding</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Text2image</td><td></td><td></td><td>2</td><td></td><td></td></tr><tr><td>Textual Entailment</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Tokenization</td><td></td><td>1</td><td>1</td><td>1</td><td></td></tr><tr><td>Topic Model</td><td>1</td><td>1</td><td></td><td>1</td><td></td></tr><tr><td>Topic Modeling</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Transfer Learning</td><td></td><td></td><td>1</td><td>1</td><td></td></tr><tr><td>Transformer</td><td>1</td><td>2</td><td>5</td><td>10</td><td></td></tr><tr><td>Unsupervised Learning</td><td></td><td>1</td><td>6</td><td>4</td><td></td></tr><tr><td>Variational Autoencoder</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Vision Transformer</td><td></td><td></td><td>2</td><td></td><td></td></tr><tr><td>Vision-and-Language</td><td></td><td></td><td>4</td><td></td><td></td></tr><tr><td>Visual Question Answering</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Weakly Supervised Learning</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Zero-shot</td><td></td><td>2</td><td>3</td><td></td><td></td></tr><tr><td>Zero-shot Learning</td><td></td><td>1</td><td></td><td></td><td></td></tr></tbody></table><script>$(function(){$("table").addClass("keyword-table table-bordered border-success"),$("table thead").addClass("sticky-top"),$("table tbody td").css("text-align","")})</script><h2 id=cscl-39>cs.CL (39)</h2><h3 id=139--1271-benchmarking-hallucination-in-large-language-models-based-on-unanswerable-math-word-problem-yuhong-sun-et-al-2024>(1/39 | 1/271) Benchmarking Hallucination in Large Language Models based on Unanswerable Math Word Problem (Yuhong Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuhong Sun, Zhangyue Yin, Qipeng Guo, Jiawen Wu, Xipeng Qiu, Hui Zhao. (2024)<br><strong>Benchmarking Hallucination in Large Language Models based on Unanswerable Math Word Problem</strong><br><button class=copy-to-clipboard title="Benchmarking Hallucination in Large Language Models based on Unanswerable Math Word Problem" index=1>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-1 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 136<br>Keywords: Benchmarking, Benchmarking, Reinforcement Learning, Reinforcement Learning from Human Feedback, Claude, GPT, GPT-3, InstructGPT, LLaMA, Question Answering, Question Answering, In-context Learning, In-context Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03558v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03558v1.pdf filename=2403.03558v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> are highly effective in various natural language processing (NLP) tasks. However, they are susceptible to producing unreliable conjectures in ambiguous contexts called hallucination. This paper presents a new method for evaluating <b>LLM</b> hallucination in <b>Question</b> <b>Answering</b> <b>(QA)</b> based on the unanswerable math word problem (MWP). To support this approach, we innovatively develop a dataset called Unanswerable Math Word Problem (UMWP) which comprises 5200 <b>questions</b> <b>across</b> five categories. We developed an evaluation methodology combining text similarity and mathematical expression detection to determine whether <b>LLM</b> considers the <b>question</b> <b>unanswerable.</b> The results of extensive experiments conducted on 31 <b>LLMs,</b> including <b>GPT-3,</b> <b>InstructGPT,</b> <b>LLaMA,</b> and <b>Claude,</b> demonstrate that <b>in-context</b> <b>learning</b> and <b>reinforcement</b> <b>learning</b> with human feedback <b>(RLHF)</b> training significantly enhance the model&rsquo;s ability to avoid hallucination. We show that utilizing MWP is a reliable and effective approach to assess hallucination. Our code and data are available at <a href=https://github.com/Yuki-Asuuna/UMWP>https://github.com/Yuki-Asuuna/UMWP</a>.</p></p class="citation"></blockquote><h3 id=239--2271-general2specialized-llms-translation-for-e-commerce-kaidi-chen-et-al-2024>(2/39 | 2/271) General2Specialized LLMs Translation for E-commerce (Kaidi Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kaidi Chen, Ben Chen, Dehong Gao, Huangyu Dai, Wen Jiang, Wei Ning, Shanqing Yu, Libin Yang, Xiaoyan Cai. (2024)<br><strong>General2Specialized LLMs Translation for E-commerce</strong><br><button class=copy-to-clipboard title="General2Specialized LLMs Translation for E-commerce" index=2>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-2 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 110<br>Keywords: Fine-tuning, GPT, GPT-3, GPT-3.5, GPT-4, LLaMA, Neural Machine Translation, Neural Machine Translation, Neural Machine Translation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03689v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03689v1.pdf filename=2403.03689v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing <b>Neural</b> <b>Machine</b> <b>Translation</b> <b>(NMT)</b> models mainly handle translation in the general domain, while overlooking domains with special writing formulas, such as e-commerce and legal documents. Taking e-commerce as an example, the texts usually include amounts of domain-related words and have more grammar problems, which leads to inferior performances of current <b>NMT</b> methods. To address these problems, we collect two domain-related resources, including a set of term pairs (aligned Chinese-English bilingual terms) and a parallel corpus annotated for the e-commerce domain. Furthermore, we propose a two-step <b>fine-tuning</b> paradigm (named G2ST) with self-contrastive semantic enhancement to transfer one general <b>NMT</b> model to the specialized <b>NMT</b> model for e-commerce. The paradigm can be used for the <b>NMT</b> models based on <b>Large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> Extensive evaluations on real e-commerce titles demonstrate the superior translation quality and robustness of our G2ST approach, as compared with state-of-the-art <b>NMT</b> models such as <b>LLaMA,</b> Qwen, <b>GPT-3.5,</b> and even <b>GPT-4.</b></p></p class="citation"></blockquote><h3 id=339--3271-can-large-language-models-do-analytical-reasoning-yebowen-hu-et-al-2024>(3/39 | 3/271) Can Large Language Models do Analytical Reasoning? (Yebowen Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yebowen Hu, Kaiqiang Song, Sangwoo Cho, Xiaoyang Wang, Hassan Foroosh, Dong Yu, Fei Liu. (2024)<br><strong>Can Large Language Models do Analytical Reasoning?</strong><br><button class=copy-to-clipboard title="Can Large Language Models do Analytical Reasoning?" index=3>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-3 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 100<br>Keywords: Claude, GPT, GPT-3, GPT-3.5, GPT-4, Gemini, LLaMA, Reasoning, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04031v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04031v1.pdf filename=2403.04031v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper explores the cutting-edge <b>Large</b> <b>Language</b> <b>Model</b> with analytical <b>reasoning</b> on sports. Our analytical <b>reasoning</b> embodies the tasks of letting <b>large</b> <b>language</b> <b>models</b> count how many points each team scores in a quarter in the NBA and NFL games. Our major discoveries are in two folds. Firstly, we find among all the models we employed, <b>GPT-4</b> stands out in effectiveness, followed by <b>Claude-2.1,</b> with <b>GPT-3.5,</b> <b>Gemini-Pro,</b> and <b>Llama-2-70b</b> lagging behind. Specifically, we compare three different <b>prompting</b> techniques and a divide-and-conquer approach, we find that the latter was the most effective. Our divide-and-conquer approach breaks down play-by-play data into smaller, more manageable segments, solves each piece individually, and then aggregates them together. Besides the divide-and-conquer approach, we also explore the Chain of Thought (CoT) strategy, which markedly improves outcomes for certain models, notably <b>GPT-4</b> and <b>Claude-2.1,</b> with their accuracy rates increasing significantly. However, the CoT strategy has negligible or even detrimental effects on the performance of other models like <b>GPT-3.5</b> and <b>Gemini-Pro.</b> Secondly, to our surprise, we observe that most models, including <b>GPT-4,</b> struggle to accurately count the total scores for NBA quarters despite showing strong performance in counting NFL quarter scores. This leads us to further investigate the factors that impact the complexity of analytical <b>reasoning</b> tasks with extensive experiments, through which we conclude that task complexity depends on the length of context, the information density, and the presence of related information. Our research provides valuable insights into the complexity of analytical <b>reasoning</b> tasks and potential directions for developing future <b>large</b> <b>language</b> <b>models.</b></p></p class="citation"></blockquote><h3 id=439--4271-rapidly-developing-high-quality-instruction-data-and-evaluation-benchmark-for-large-language-models-with-minimal-human-effort-a-case-study-on-japanese-yikun-sun-et-al-2024>(4/39 | 4/271) Rapidly Developing High-quality Instruction Data and Evaluation Benchmark for Large Language Models with Minimal Human Effort: A Case Study on Japanese (Yikun Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yikun Sun, Zhen Wan, Nobuhiro Ueda, Sakiko Yahata, Fei Cheng, Chenhui Chu, Sadao Kurohashi. (2024)<br><strong>Rapidly Developing High-quality Instruction Data and Evaluation Benchmark for Large Language Models with Minimal Human Effort: A Case Study on Japanese</strong><br><button class=copy-to-clipboard title="Rapidly Developing High-quality Instruction Data and Evaluation Benchmark for Large Language Models with Minimal Human Effort: A Case Study on Japanese" index=4>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-4 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 83<br>Keywords: Benchmarking, Fine-tuning, GPT, GPT-3, GPT-3.5, GPT-4, LLaMA, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03690v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03690v1.pdf filename=2403.03690v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The creation of instruction data and evaluation <b>benchmarks</b> for serving <b>Large</b> <b>language</b> <b>models</b> often involves enormous human annotation. This issue becomes particularly pronounced when rapidly developing such resources for a non-English language like Japanese. Instead of following the popular practice of directly translating existing English resources into Japanese (e.g., Japanese-Alpaca), we propose an efficient self-instruct method based on <b>GPT-4.</b> We first translate a small amount of English instructions into Japanese and post-edit them to obtain native-level quality. <b>GPT-4</b> then utilizes them as demonstrations to automatically generate Japanese instruction data. We also construct an evaluation <b>benchmark</b> containing 80 questions across 8 categories, using <b>GPT-4</b> to automatically assess the response quality of <b>LLMs</b> without human references. The empirical results suggest that the models <b>fine-tuned</b> on our <b>GPT-4</b> self-instruct data significantly outperformed the Japanese-Alpaca across all three base pre-trained models. Our <b>GPT-4</b> self-instruct data allowed the <b>LLaMA</b> 13B model to defeat <b>GPT-3.5</b> (Davinci-003) with a 54.37% win-rate. The human evaluation exhibits the consistency between <b>GPT-4&rsquo;s</b> assessments and human preference. Our high-quality instruction data and evaluation <b>benchmark</b> have been released here.</p></p class="citation"></blockquote><h3 id=539--5271-designing-informative-metrics-for-few-shot-example-selection-rishabh-adiga-et-al-2024>(5/39 | 5/271) Designing Informative Metrics for Few-Shot Example Selection (Rishabh Adiga et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rishabh Adiga, Lakshminarayanan Subramanian, Varun Chandrasekaran. (2024)<br><strong>Designing Informative Metrics for Few-Shot Example Selection</strong><br><button class=copy-to-clipboard title="Designing Informative Metrics for Few-Shot Example Selection" index=5>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-5 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 80<br>Keywords: Few-shot, Few-shot Learning, GPT, GPT-4, Named Entity Recognition, Pre-trained Language Model, Pre-trained Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03861v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03861v1.pdf filename=2403.03861v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Pretrained</b> <b>language</b> <b>models</b> <b>(PLMs)</b> have shown remarkable <b>few-shot</b> <b>learning</b> capabilities when provided with properly formatted examples. However, selecting the &ldquo;best&rdquo; examples remains an open challenge. We propose a complexity-based <b>prompt</b> selection approach for sequence tagging tasks. This approach avoids the training of a dedicated model for selection of examples, and instead uses certain metrics to align the syntactico-semantic complexity of test sentences and examples. We use both sentence- and word-level metrics to match the complexity of examples to the (test) sentence being considered. Our results demonstrate that our approach extracts greater performance from <b>PLMs:</b> it achieves state-of-the-art performance on <b>few-shot</b> <b>NER,</b> achieving a 5% absolute improvement in F1 score on the CoNLL2003 dataset for <b>GPT-4.</b> We also see large gains of upto 28.85 points (F1/Acc.) in smaller models like <b>GPT-j-6B.</b></p></p class="citation"></blockquote><h3 id=639--6271-german-also-hallucinates-inconsistency-detection-in-news-summaries-with-the-absinth-dataset-laura-mascarell-et-al-2024>(6/39 | 6/271) German also Hallucinates! Inconsistency Detection in News Summaries with the Absinth Dataset (Laura Mascarell et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Laura Mascarell, Ribin Chalumattu, Annette Rios. (2024)<br><strong>German also Hallucinates! Inconsistency Detection in News Summaries with the Absinth Dataset</strong><br><button class=copy-to-clipboard title="German also Hallucinates! Inconsistency Detection in News Summaries with the Absinth Dataset" index=6>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-6 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: I-2-7, cs-AI, cs-CL, cs.CL<br>Keyword Score: 80<br>Keywords: Fine-tuning, Hallucination Detection, Text Summarization, In-context Learning, In-context Learning, Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03750v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03750v1.pdf filename=2403.03750v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The advent of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> has led to remarkable progress on a wide range of natural language processing tasks. Despite the advances, these <b>large-sized</b> <b>models</b> <b>still</b> suffer from hallucinating information in their output, which poses a major issue in automatic <b>text</b> <b>summarization,</b> as we must guarantee that the generated summary is consistent with the content of the source document. Previous research addresses the challenging task of detecting <b>hallucinations</b> <b>in</b> the output (i.e. inconsistency detection) in order to evaluate the faithfulness of the generated summaries. However, these works primarily focus on English and recent multilingual approaches lack German data. This work presents absinth, a manually annotated dataset for <b>hallucination</b> <b>detection</b> in German news <b>summarization</b> and explores the capabilities of novel open-source <b>LLMs</b> on this task in both <b>fine-tuning</b> and <b>in-context</b> <b>learning</b> settings. We open-source and release the absinth dataset to foster further research on <b>hallucination</b> <b>detection</b> in German.</p></p class="citation"></blockquote><h3 id=739--7271-japanese-english-sentence-translation-exercises-dataset-for-automatic-grading-naoki-miura-et-al-2024>(7/39 | 7/271) Japanese-English Sentence Translation Exercises Dataset for Automatic Grading (Naoki Miura et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Naoki Miura, Hiroaki Funayama, Seiya Kikuchi, Yuichiroh Matsubayashi, Yuya Iwase, Kentaro Inui. (2024)<br><strong>Japanese-English Sentence Translation Exercises Dataset for Automatic Grading</strong><br><button class=copy-to-clipboard title="Japanese-English Sentence Translation Exercises Dataset for Automatic Grading" index=7>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-7 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 80<br>Keywords: Few-shot, Few-shot Learning, Fine-tuning, BERT, GPT, In-context Learning, In-context Learning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03396v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03396v1.pdf filename=2403.03396v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper proposes the task of automatic assessment of Sentence Translation Exercises (STEs), that have been used in the early stage of L2 language learning. We formalize the task as grading student responses for each rubric criterion pre-specified by the educators. We then create a dataset for STE between Japanese and English including 21 questions, along with a total of 3, 498 student responses (167 on average). The answer responses were collected from students and crowd workers. Using this dataset, we demonstrate the performance of baselines including <b>finetuned</b> <b>BERT</b> and <b>GPT</b> models with <b>few-shot</b> <b>in-context</b> <b>learning.</b> Experimental results show that the baseline model with <b>finetuned</b> <b>BERT</b> was able to classify correct responses with approximately 90% in F1, but only less than 80% for incorrect responses. Furthermore, the <b>GPT</b> models with <b>few-shot</b> <b>learning</b> show poorer results than <b>finetuned</b> <b>BERT,</b> indicating that our newly proposed task presents a challenging issue, even for the stateof-the-art <b>large</b> <b>language</b> <b>models.</b></p></p class="citation"></blockquote><h3 id=839--8271-evaluating-the-elementary-multilingual-capabilities-of-large-language-models-with-multiq-carolin-holtermann-et-al-2024>(8/39 | 8/271) Evaluating the Elementary Multilingual Capabilities of Large Language Models with MultiQ (Carolin Holtermann et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Carolin Holtermann, Paul Röttger, Timm Dill, Anne Lauscher. (2024)<br><strong>Evaluating the Elementary Multilingual Capabilities of Large Language Models with MultiQ</strong><br><button class=copy-to-clipboard title="Evaluating the Elementary Multilingual Capabilities of Large Language Models with MultiQ" index=8>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-8 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 73<br>Keywords: Benchmarking, High-Resource, Mistral, Question Answering, Tokenization, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03814v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03814v1.pdf filename=2403.03814v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> need to serve everyone, including a global majority of non-English speakers. However, most <b>LLMs</b> today, and open <b>LLMs</b> in particular, are often intended for use in just English (e.g. Llama2, <b>Mistral)</b> or a small handful of <b>high-resource</b> languages (e.g. Mixtral, Qwen). Recent research shows that, despite limits in their intended use, people <b>prompt</b> <b>LLMs</b> in many different languages. Therefore, in this paper, we investigate the basic multilingual capabilities of state-of-the-art open <b>LLMs</b> beyond their intended use. For this purpose, we introduce MultiQ, a new silver standard <b>benchmark</b> for basic open-ended <b>question</b> <b>answering</b> with 27.4k test <b>questions</b> <b>across</b> a typologically diverse set of 137 languages. With MultiQ, we evaluate language fidelity, i.e.\ whether models respond in the <b>prompted</b> language, and <b>question</b> <b>answering</b> accuracy. All <b>LLMs</b> we test respond faithfully and/or accurately for at least some languages beyond their intended use. Most models are more accurate when they respond faithfully. However, differences across models are <b>large,</b> <b>and</b> <b>there</b> is a long tail of languages where models are neither accurate nor faithful. We explore differences in <b>tokenization</b> as a potential explanation for our findings, identifying possible correlations that warrant further investigation.</p></p class="citation"></blockquote><h3 id=939--9271-x-shot-a-unified-system-to-handle-frequent-few-shot-and-zero-shot-learning-simultaneously-in-classification-hanzi-xu-et-al-2024>(9/39 | 9/271) X-Shot: A Unified System to Handle Frequent, Few-shot and Zero-shot Learning Simultaneously in Classification (Hanzi Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hanzi Xu, Muhao Chen, Lifu Huang, Slobodan Vucetic, Wenpeng Yin. (2024)<br><strong>X-Shot: A Unified System to Handle Frequent, Few-shot and Zero-shot Learning Simultaneously in Classification</strong><br><button class=copy-to-clipboard title="X-Shot: A Unified System to Handle Frequent, Few-shot and Zero-shot Learning Simultaneously in Classification" index=9>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-9 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 63<br>Keywords: Benchmarking, Few-shot, Zero-shot, Instruction Following, Large Language Model, Weakly Supervised Learning, Zero-shot Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03863v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03863v1.pdf filename=2403.03863v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, <b>few-shot</b> and <b>zero-shot</b> <b>learning,</b> which learn to predict labels with limited annotated instances, have garnered significant attention. Traditional approaches often treat frequent-shot (freq-shot; labels with abundant instances), <b>few-shot,</b> and <b>zero-shot</b> <b>learning</b> as distinct challenges, optimizing systems for just one of these scenarios. Yet, in real-world settings, label occurrences vary greatly. Some of them might appear thousands of times, while others might only appear sporadically or not at all. For practical deployment, it is crucial that a system can adapt to any label occurrence. We introduce a novel classification challenge: X-shot, reflecting a real-world context where freq-shot, <b>few-shot,</b> and <b>zero-shot</b> <b>labels</b> co-occur without predefined limits. Here, X can span from 0 to positive infinity. The crux of X-shot centers on open-domain generalization and devising a system versatile enough to manage various label scenarios. To solve X-shot, we propose BinBin (Binary INference Based on <b>INstruction</b> <b>following)</b> that leverages the Indirect Supervision from a <b>large</b> <b>collection</b> <b>of</b> NLP tasks via <b>instruction</b> <b>following,</b> bolstered by <b>Weak</b> <b>Supervision</b> provided by <b>large</b> <b>language</b> <b>models.</b> BinBin surpasses previous state-of-the-art techniques on three <b>benchmark</b> datasets across multiple domains. To our knowledge, this is the first work addressing X-shot learning, where X remains variable.</p></p class="citation"></blockquote><h3 id=1039--10271-multimodal-large-language-models-to-support-real-world-fact-checking-jiahui-geng-et-al-2024>(10/39 | 10/271) Multimodal Large Language Models to Support Real-World Fact-Checking (Jiahui Geng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiahui Geng, Yova Kementchedjhieva, Preslav Nakov, Iryna Gurevych. (2024)<br><strong>Multimodal Large Language Models to Support Real-World Fact-Checking</strong><br><button class=copy-to-clipboard title="Multimodal Large Language Models to Support Real-World Fact-Checking" index=10>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-10 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 56<br>Keywords: Multi-modal, Multi-modal, GPT, Fact Verification, Reasoning, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03627v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03627v1.pdf filename=2403.03627v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multimodal</b> <b>large</b> <b>language</b> <b>models</b> (MLLMs) carry the potential to support humans in processing vast amounts of information. While MLLMs are already being used as a <b>fact-checking</b> <b>tool,</b> their abilities and limitations in this regard are understudied. Here is aim to bridge this gap. In particular, we propose a framework for systematically assessing the capacity of current <b>multimodal</b> models to facilitate real-world <b>fact-checking.</b> <b>Our</b> methodology is evidence-free, leveraging only these models&rsquo; intrinsic knowledge and <b>reasoning</b> capabilities. By designing <b>prompts</b> that extract models&rsquo; predictions, explanations, and confidence levels, we delve into research questions concerning model accuracy, robustness, and reasons for failure. We empirically find that (1) <b>GPT-4V</b> exhibits superior performance in identifying malicious and misleading <b>multimodal</b> claims, with the ability to explain the unreasonable aspects and underlying motives, and (2) existing open-source models exhibit strong biases and are highly sensitive to the <b>prompt.</b> Our study offers insights into combating false <b>multimodal</b> information and building secure, trustworthy <b>multimodal</b> models. To the best of our knowledge, we are the first to evaluate MLLMs for real-world fact-checking.</p></p class="citation"></blockquote><h3 id=1139--11271-did-translation-models-get-more-robust-without-anyone-even-noticing-ben-peters-et-al-2024>(11/39 | 11/271) Did Translation Models Get More Robust Without Anyone Even Noticing? (Ben Peters et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ben Peters, André F. T. Martins. (2024)<br><strong>Did Translation Models Get More Robust Without Anyone Even Noticing?</strong><br><button class=copy-to-clipboard title="Did Translation Models Get More Robust Without Anyone Even Noticing?" index=11>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-11 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Neural Machine Translation, Neural Machine Translation, Neural Machine Translation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03923v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03923v1.pdf filename=2403.03923v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Neural</b> <b>machine</b> <b>translation</b> <b>(MT)</b> models achieve strong results across a variety of settings, but it is widely believed that they are highly sensitive to &ldquo;noisy&rdquo; inputs, such as spelling errors, abbreviations, and other formatting issues. In this paper, we revisit this insight in light of recent multilingual <b>MT</b> models and <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> applied to <b>machine</b> <b>translation.</b> Somewhat surprisingly, we show through controlled experiments that these models are far more robust to many kinds of noise than previous models, even when they perform similarly on clean data. This is notable because, even though <b>LLMs</b> have more parameters and more complex training processes than past models, none of the open ones we consider use any techniques specifically designed to encourage robustness. Next, we show that similar trends hold for social media translation experiments &ndash; <b>LLMs</b> are more robust to social media text. We include an analysis of the circumstances in which source correction techniques can be used to mitigate the effects of noise. Altogether, we show that robustness to many types of noise has increased.</p></p class="citation"></blockquote><h3 id=1239--12271-faaf-facts-as-a-function-for-the-evaluation-of-rag-systems-vasileios-katranidis-et-al-2024>(12/39 | 12/271) FaaF: Facts as a Function for the evaluation of RAG systems (Vasileios Katranidis et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vasileios Katranidis, Gabor Barany. (2024)<br><strong>FaaF: Facts as a Function for the evaluation of RAG systems</strong><br><button class=copy-to-clipboard title="FaaF: Facts as a Function for the evaluation of RAG systems" index=12>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-12 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Fact Verification, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03888v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03888v1.pdf filename=2403.03888v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Factual recall from a reference source is crucial for evaluating the performance of <b>Retrieval</b> <b>Augmented</b> <b>Generation</b> <b>(RAG)</b> systems, as it directly probes into the quality of both <b>retrieval</b> <b>and</b> <b>generation.</b> However, it still remains a challenge to perform this evaluation reliably and efficiently. Recent work has focused on <b>fact</b> <b>verification</b> via <b>prompting</b> language model (LM) evaluators, however we demonstrate that these methods are unreliable in the presence of incomplete or inaccurate information. We introduce <b>Facts</b> <b>as</b> a Function (FaaF), a new approach to <b>fact</b> <b>verification</b> that utilizes the function calling abilities of LMs and a framework for <b>RAG</b> factual recall evaluation. FaaF substantially improves the ability of LMs to identify unsupported <b>facts</b> <b>in</b> text with incomplete information whilst improving efficiency and lowering cost by several times, compared to <b>prompt-based</b> approaches.</p></p class="citation"></blockquote><h3 id=1339--13271-learning-to-decode-collaboratively-with-multiple-language-models-shannon-zejiang-shen-et-al-2024>(13/39 | 13/271) Learning to Decode Collaboratively with Multiple Language Models (Shannon Zejiang Shen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shannon Zejiang Shen, Hunter Lang, Bailin Wang, Yoon Kim, David Sontag. (2024)<br><strong>Learning to Decode Collaboratively with Multiple Language Models</strong><br><button class=copy-to-clipboard title="Learning to Decode Collaboratively with Multiple Language Models" index=13>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-13 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 50<br>Keywords: Instruction Following, Question Answering, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03870v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03870v1.pdf filename=2403.03870v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose a method to teach multiple <b>large</b> <b>language</b> <b>models</b> <b>(LLM)</b> to collaborate by interleaving their generations at the token level. We model the decision of which <b>LLM</b> generates the next token as a latent variable. By optimizing the marginal likelihood of a training set under our latent variable model, the base <b>LLM</b> automatically learns when to generate itself and when to call on one of the ``assistant&rsquo;&rsquo; language models to generate, all without direct supervision. Token-level collaboration during decoding allows for a fusion of each model&rsquo;s expertise in a manner tailored to the specific task at hand. Our collaborative decoding is especially useful in cross-domain settings where a generalist base <b>LLM</b> learns to invoke domain expert models. On <b>instruction-following,</b> <b>domain-specific</b> <b>QA,</b> and <b>reasoning</b> tasks, we show that the performance of the joint system exceeds that of the individual models. Through qualitative analysis of the learned latent decisions, we show models trained with our method exhibit several interesting collaboration patterns, e.g., template-filling. Our code is available at <a href=https://github.com/clinicalml/co-llm>https://github.com/clinicalml/co-llm</a>.</p></p class="citation"></blockquote><h3 id=1439--14271-shortgpt-layers-in-large-language-models-are-more-redundant-than-you-expect-xin-men-et-al-2024>(14/39 | 14/271) ShortGPT: Layers in Large Language Models are More Redundant Than You Expect (Xin Men et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xin Men, Mingyu Xu, Qingyu Zhang, Bingning Wang, Hongyu Lin, Yaojie Lu, Xianpei Han, Weipeng Chen. (2024)<br><strong>ShortGPT: Layers in Large Language Models are More Redundant Than You Expect</strong><br><button class=copy-to-clipboard title="ShortGPT: Layers in Large Language Models are More Redundant Than You Expect" index=14>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-14 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Model Pruning, Pruning, Quantization, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03853v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03853v2.pdf filename=2403.03853v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> continue to advance in performance, their size has escalated significantly, with current <b>LLMs</b> containing billions or even trillions of parameters. However, in this study, we discovered that many layers of <b>LLMs</b> exhibit high similarity, and some layers play a negligible role in network functionality. Based on this observation, we define a metric called Block Influence (BI) to gauge the significance of each layer in <b>LLMs.</b> We then propose a straightforward <b>pruning</b> approach: layer removal, in which we directly delete the redundant layers in <b>LLMs</b> based on their BI scores. Experiments demonstrate that our method, which we call ShortGPT, significantly outperforms previous state-of-the-art (SOTA) methods in <b>model</b> <b>pruning.</b> Moreover, ShortGPT is orthogonal to <b>quantization-like</b> methods, enabling further reduction in parameters and computation. The ability to achieve better results through simple layer removal, as opposed to more complex <b>pruning</b> techniques, suggests a high degree of redundancy in the <b>model</b> <b>architecture.</b></p></p class="citation"></blockquote><h3 id=1539--15271-pptc-r-benchmark-towards-evaluating-the-robustness-of-large-language-models-for-powerpoint-task-completion-zekai-zhang-et-al-2024>(15/39 | 15/271) PPTC-R benchmark: Towards Evaluating the Robustness of Large Language Models for PowerPoint Task Completion (Zekai Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zekai Zhang, Yiduo Guo, Yaobo Liang, Dongyan Zhao, Nan Duan. (2024)<br><strong>PPTC-R benchmark: Towards Evaluating the Robustness of Large Language Models for PowerPoint Task Completion</strong><br><button class=copy-to-clipboard title="PPTC-R benchmark: Towards Evaluating the Robustness of Large Language Models for PowerPoint Task Completion" index=15>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-15 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Benchmarking, GPT, GPT-4, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03788v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03788v1.pdf filename=2403.03788v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The growing dependence on <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> for finishing user instructions necessitates a comprehensive understanding of their robustness to complex task completion in real-world situations. To address this critical need, we propose the PowerPoint Task Completion Robustness <b>benchmark</b> (PPTC-R) to measure <b>LLMs&rsquo;</b> robustness to the user PPT task instruction and software version. Specifically, we construct adversarial user instructions by attacking user instructions at sentence, semantic, and multi-language levels. To assess the robustness of Language Models to software versions, we vary the number of provided APIs to simulate both the newest version and earlier version settings. Subsequently, we test 3 closed-source and 4 open-source <b>LLMs</b> using a <b>benchmark</b> that incorporates these robustness settings, aiming to evaluate how deviations impact <b>LLMs&rsquo;</b> API calls for task completion. We find that <b>GPT-4</b> exhibits the highest performance and strong robustness in our <b>benchmark,</b> particularly in the version update and the multilingual settings. However, we find that all <b>LLMs</b> lose their robustness when confronted with multiple challenges (e.g., multi-turn) simultaneously, leading to significant performance drops. We further analyze the robustness behavior and error reasons of <b>LLMs</b> in our <b>benchmark,</b> which provide valuable insights for researchers to understand the <b>LLM&rsquo;s</b> robustness in task completion and develop more robust <b>LLMs</b> and agents. We release the code and data at \url{https://github.com/ZekaiGalaxy/PPTCR}.</p></p class="citation"></blockquote><h3 id=1639--16271-design-of-an-open-source-architecture-for-neural-machine-translation-séamus-lankford-et-al-2024>(16/39 | 16/271) Design of an Open-Source Architecture for Neural Machine Translation (Séamus Lankford et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Séamus Lankford, Haithem Afli, Andy Way. (2024)<br><strong>Design of an Open-Source Architecture for Neural Machine Translation</strong><br><button class=copy-to-clipboard title="Design of an Open-Source Architecture for Neural Machine Translation" index=16>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-16 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Graph, Recurrent Neural Network, Transformer, Neural Machine Translation, Neural Machine Translation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03582v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03582v1.pdf filename=2403.03582v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>adaptNMT is an open-source application that offers a streamlined approach to the development and deployment of <b>Recurrent</b> <b>Neural</b> <b>Networks</b> <b>and</b> <b>Transformer</b> models. This application is built upon the widely-adopted OpenNMT ecosystem, and is particularly useful for new entrants to the field, as it simplifies the setup of the development environment and creation of train, validation, and test splits. The application offers a <b>graphing</b> feature that illustrates the progress of model training, and employs SentencePiece for creating subword segmentation models. Furthermore, the application provides an intuitive user interface that facilitates hyperparameter customization. Notably, a single-click model development approach has been implemented, and models developed by adaptNMT can be evaluated using a range of metrics. To encourage eco-friendly research, adaptNMT incorporates a green report that flags the power consumption and kgCO${_2}$ emissions generated during model development. The application is freely available.</p></p class="citation"></blockquote><h3 id=1739--17271-unsupervised-multilingual-dense-retrieval-via-generative-pseudo-labeling-chao-wei-huang-et-al-2024>(17/39 | 17/271) Unsupervised Multilingual Dense Retrieval via Generative Pseudo Labeling (Chao-Wei Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chao-Wei Huang, Chen-An Li, Tsu-Yuan Hsu, Chen-Yu Hsu, Yun-Nung Chen. (2024)<br><strong>Unsupervised Multilingual Dense Retrieval via Generative Pseudo Labeling</strong><br><button class=copy-to-clipboard title="Unsupervised Multilingual Dense Retrieval via Generative Pseudo Labeling" index=17>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-17 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-IR, cs.CL<br>Keyword Score: 43<br>Keywords: Benchmarking, Dense Retrieval, Supervised Learning, Unsupervised Learning, Information Retrieval<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03516v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03516v1.pdf filename=2403.03516v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Dense</b> <b>retrieval</b> methods have demonstrated promising performance in multilingual <b>information</b> <b>retrieval,</b> where queries and documents can be in different languages. However, <b>dense</b> <b>retrievers</b> typically require a substantial amount of paired data, which poses even greater challenges in multilingual scenarios. This paper introduces UMR, an <b>Unsupervised</b> Multilingual <b>dense</b> <b>Retriever</b> trained without any paired data. Our approach leverages the sequence likelihood estimation capabilities of multilingual language models to acquire pseudo labels for training <b>dense</b> <b>retrievers.</b> We propose a two-stage framework which iteratively improves the performance of multilingual <b>dense</b> <b>retrievers.</b> Experimental results on two <b>benchmark</b> datasets show that UMR outperforms <b>supervised</b> baselines, showcasing the potential of training multilingual retrievers without paired data, thereby enhancing their practicality. Our source code, data, and models are publicly available at <a href=https://github.com/MiuLab/UMR>https://github.com/MiuLab/UMR</a></p></p class="citation"></blockquote><h3 id=1839--18271-semi-supervised-dialogue-abstractive-summarization-via-high-quality-pseudolabel-selection-jianfeng-he-et-al-2024>(18/39 | 18/271) Semi-Supervised Dialogue Abstractive Summarization via High-Quality Pseudolabel Selection (Jianfeng He et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianfeng He, Hang Su, Jason Cai, Igor Shalyminov, Hwanjun Song, Saab Mansour. (2024)<br><strong>Semi-Supervised Dialogue Abstractive Summarization via High-Quality Pseudolabel Selection</strong><br><button class=copy-to-clipboard title="Semi-Supervised Dialogue Abstractive Summarization via High-Quality Pseudolabel Selection" index=18>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-18 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Semi-Supervised Learning, Natural Language Understanding, Summarization, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04073v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04073v1.pdf filename=2403.04073v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Semi-supervised</b> <b>dialogue</b> <b>summarization</b> (SSDS) leverages model-generated summaries to reduce reliance on human-labeled data and improve the performance of <b>summarization</b> models. While addressing label noise, previous works on <b>semi-supervised</b> <b>learning</b> primarily focus on <b>natural</b> <b>language</b> <b>understanding</b> tasks, assuming each sample has a unique label. However, these methods are not directly applicable to SSDS, as it is a generative task, and each dialogue can be <b>summarized</b> in different ways. In this work, we propose a novel scoring approach, SiCF, which encapsulates three primary dimensions of <b>summarization</b> model quality: Semantic invariance (indicative of model confidence), Coverage (factual recall), and Faithfulness (factual precision). Using the SiCF score, we select unlabeled dialogues with high-quality generated summaries to train <b>summarization</b> models. Comprehensive experiments on three public datasets demonstrate the effectiveness of SiCF scores in uncertainty estimation and <b>semi-supervised</b> <b>learning</b> for dialogue <b>summarization</b> tasks. Our code is available at \url{https://github.com/amazon-science/summarization-sicf-score}.</p></p class="citation"></blockquote><h3 id=1939--19271-saullm-7b-a-pioneering-large-language-model-for-law-pierre-colombo-et-al-2024>(19/39 | 19/271) SaulLM-7B: A pioneering Large Language Model for Law (Pierre Colombo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pierre Colombo, Telmo Pessoa Pires, Malik Boudiaf, Dominic Culver, Rui Melo, Caio Corro, Andre F. T. Martins, Fabrizio Esposito, Vera Lúcia Raposo, Sofia Morgado, Michael Desa. (2024)<br><strong>SaulLM-7B: A pioneering Large Language Model for Law</strong><br><button class=copy-to-clipboard title="SaulLM-7B: A pioneering Large Language Model for Law" index=19>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-19 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Fine-tuning, Mistral, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03883v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03883v2.pdf filename=2403.03883v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we introduce SaulLM-7B, a <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> tailored for the legal domain. With 7 billion parameters, SaulLM-7B is the first <b>LLM</b> designed explicitly for legal text comprehension and generation. Leveraging the <b>Mistral</b> 7B architecture as its foundation, SaulLM-7B is trained on an English legal corpus of over 30 billion tokens. SaulLM-7B exhibits state-of-the-art proficiency in understanding and processing legal documents. Additionally, we present a novel instructional <b>fine-tuning</b> method that leverages legal datasets to further enhance SaulLM-7B&rsquo;s performance in legal tasks. SaulLM-7B is released under the MIT License.</p></p class="citation"></blockquote><h3 id=2039--20271-gptopic-dynamic-and-interactive-topic-representations-arik-reuter-et-al-2024>(20/39 | 20/271) GPTopic: Dynamic and Interactive Topic Representations (Arik Reuter et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Arik Reuter, Anton Thielmann, Christoph Weisser, Sebastian Fischer, Benjamin Säfken. (2024)<br><strong>GPTopic: Dynamic and Interactive Topic Representations</strong><br><button class=copy-to-clipboard title="GPTopic: Dynamic and Interactive Topic Representations" index=20>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-20 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Topic Model, Large Language Model, Large Language Model, Topic Modeling<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03628v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03628v1.pdf filename=2403.03628v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Topic</b> <b>modeling</b> seems to be almost synonymous with generating lists of top words to represent <b>topics</b> <b>within</b> <b>large</b> <b>text</b> <b>corpora.</b> However, deducing a <b>topic</b> <b>from</b> such list of individual terms can require substantial expertise and experience, making <b>topic</b> <b>modelling</b> less accessible to people unfamiliar with the particularities and pitfalls of top-word interpretation. A <b>topic</b> <b>representation</b> limited to top-words might further fall short of offering a comprehensive and easily accessible characterization of the various aspects, facets and nuances a <b>topic</b> <b>might</b> have. To address these challenges, we introduce GPTopic, a software package that leverages <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> to create dynamic, interactive <b>topic</b> <b>representations.</b> GPTopic provides an intuitive chat interface for users to explore, analyze, and refine <b>topics</b> <b>interactively,</b> making <b>topic</b> <b>modeling</b> more accessible and comprehensive. The corresponding code is available here: https://github. com/05ec6602be/GPTopic.</p></p class="citation"></blockquote><h3 id=2139--21271-gahealth-an-english-irish-bilingual-corpus-of-health-data-séamus-lankford-et-al-2024>(21/39 | 21/271) gaHealth: An English-Irish Bilingual Corpus of Health Data (Séamus Lankford et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Séamus Lankford, Haithem Afli, Órla Ní Loinsigh, Andy Way. (2024)<br><strong>gaHealth: An English-Irish Bilingual Corpus of Health Data</strong><br><button class=copy-to-clipboard title="gaHealth: An English-Irish Bilingual Corpus of Health Data" index=21>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-21 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: High-Resource, Low-Resource, Neural Machine Translation, BLEU<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03575v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03575v1.pdf filename=2403.03575v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Machine</b> <b>Translation</b> is a mature technology for many <b>high-resource</b> language pairs. However in the context of <b>low-resource</b> languages, there is a paucity of parallel data datasets available for developing translation models. Furthermore, the development of datasets for <b>low-resource</b> languages often focuses on simply creating the largest possible dataset for generic translation. The benefits and development of smaller in-domain datasets can easily be overlooked. To assess the merits of using in-domain data, a dataset for the specific domain of health was developed for the <b>low-resource</b> English to Irish language pair. Our study outlines the process used in developing the corpus and empirically demonstrates the benefits of using an in-domain dataset for the health domain. In the context of translating health-related data, models developed using the gaHealth corpus demonstrated a maximum <b>BLEU</b> score improvement of 22.2 points (40%) when compared with top performing models from the LoResMT2021 Shared Task. Furthermore, we define linguistic guidelines for developing gaHealth, the first bilingual corpus of health data for the Irish language, which we hope will be of use to other creators of <b>low-resource</b> data sets. gaHealth is now freely available online and is ready to be explored for further research.</p></p class="citation"></blockquote><h3 id=2239--22271-bivert-bidirectional-vocabulary-evaluation-using-relations-for-machine-translation-carinne-cherf-et-al-2024>(22/39 | 22/271) BiVert: Bidirectional Vocabulary Evaluation using Relations for Machine Translation (Carinne Cherf et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Carinne Cherf, Yuval Pinter. (2024)<br><strong>BiVert: Bidirectional Vocabulary Evaluation using Relations for Machine Translation</strong><br><button class=copy-to-clipboard title="BiVert: Bidirectional Vocabulary Evaluation using Relations for Machine Translation" index=22>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-22 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Neural Machine Translation, Neural Machine Translation, Neural Machine Translation, Neural Machine Translation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03521v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03521v1.pdf filename=2403.03521v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Neural</b> <b>machine</b> <b>translation</b> <b>(NMT)</b> has progressed rapidly in the past few years, promising improvements and quality translations for different languages. Evaluation of this task is crucial to determine the quality of the translation. Overall, insufficient emphasis is placed on the actual sense of the translation in traditional methods. We propose a bidirectional semantic-based evaluation method designed to assess the sense distance of the translation from the source text. This approach employs the comprehensive multilingual encyclopedic dictionary BabelNet. Through the calculation of the semantic distance between the source and its back translation of the output, our method introduces a quantifiable approach that empowers sentence comparison on the same linguistic level. Factual analysis shows a strong correlation between the average evaluation scores generated by our method and the human assessments across various <b>machine</b> <b>translation</b> systems for English-German language pair. Finally, our method proposes a new multilingual approach to rank <b>MT</b> systems without the need for parallel corpora.</p></p class="citation"></blockquote><h3 id=2339--23271-mixture-of-loras-an-efficient-multitask-tuning-for-large-language-models-wenfeng-feng-et-al-2024>(23/39 | 23/271) Mixture-of-LoRAs: An Efficient Multitask Tuning for Large Language Models (Wenfeng Feng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenfeng Feng, Chuzhan Hao, Yuewei Zhang, Yu Han, Hao Wang. (2024)<br><strong>Mixture-of-LoRAs: An Efficient Multitask Tuning for Large Language Models</strong><br><button class=copy-to-clipboard title="Mixture-of-LoRAs: An Efficient Multitask Tuning for Large Language Models" index=23>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-23 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Supervised Learning, Instruction Tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03432v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03432v1.pdf filename=2403.03432v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Instruction</b> <b>Tuning</b> has the potential to stimulate or enhance specific capabilities of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> However, achieving the right balance of data is crucial to prevent catastrophic forgetting and interference between tasks. To address these limitations and enhance training flexibility, we propose the Mixture-of-LoRAs (MoA) architecture which is a novel and parameter-efficient tuning method designed for multi-task learning with <b>LLMs.</b> In this paper, we start by individually training multiple domain-specific LoRA modules using corresponding <b>supervised</b> corpus data. These LoRA modules can be aligned with the expert design principles observed in Mixture-of-Experts (MoE). Subsequently, we combine the multiple LoRAs using an explicit routing strategy and introduce domain labels to facilitate multi-task learning, which help prevent interference between tasks and ultimately enhances the performance of each individual task. Furthermore, each LoRA model can be iteratively adapted to a new domain, allowing for quick domain-specific adaptation. Experiments on diverse tasks demonstrate superior and robust performance, which can further promote the wide application of domain-specific <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=2439--24271-a-modular-approach-for-multimodal-summarization-of-tv-shows-louis-mahon-et-al-2024>(24/39 | 24/271) A Modular Approach for Multimodal Summarization of TV Shows (Louis Mahon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Louis Mahon, Mirella Lapata. (2024)<br><strong>A Modular Approach for Multimodal Summarization of TV Shows</strong><br><button class=copy-to-clipboard title="A Modular Approach for Multimodal Summarization of TV Shows" index=24>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-24 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 36<br>Keywords: Multi-modal, Multi-modal, Reasoning, Rouge, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03823v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03823v2.pdf filename=2403.03823v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper we address the task of summarizing television shows, which touches key areas in AI research: complex <b>reasoning,</b> multiple modalities, and long narratives. We present a modular approach where separate components perform specialized sub-tasks which we argue affords greater flexibility compared to end-to-end methods. Our modules involve detecting scene boundaries, reordering scenes so as to minimize the number of cuts between different events, converting visual information to text, summarizing the dialogue in each scene, and fusing the scene summaries into a final summary for the entire episode. We also present a new metric, PREFS (Precision and Recall Evaluation of Summary FactS), to measure both precision and recall of generated summaries, which we decompose into atomic facts. Tested on the recently released SummScreen3D dataset Papalampidi and Lapata (2023), our method produces higher quality summaries than comparison models, as measured with <b>ROUGE</b> and our new fact-based metric.</p></p class="citation"></blockquote><h3 id=2539--25271-a-knowledge-plug-and-play-test-bed-for-open-domain-dialogue-generation-xiangci-li-et-al-2024>(25/39 | 25/271) A Knowledge Plug-and-Play Test Bed for Open-domain Dialogue Generation (Xiangci Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiangci Li, Linfeng Song, Lifeng Jin, Haitao Mi, Jessica Ouyang, Dong Yu. (2024)<br><strong>A Knowledge Plug-and-Play Test Bed for Open-domain Dialogue Generation</strong><br><button class=copy-to-clipboard title="A Knowledge Plug-and-Play Test Bed for Open-domain Dialogue Generation" index=25>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-25 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, Zero-shot, Open-Domain Dialogue, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03496v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03496v1.pdf filename=2403.03496v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Knowledge-based, <b>open-domain</b> <b>dialogue</b> generation aims to build chit-chat systems that talk to humans using mined support knowledge. Many types and sources of knowledge have previously been shown to be useful as support knowledge. Even in the era of <b>large</b> <b>language</b> <b>models,</b> response generation grounded in knowledge retrieved from additional up-to-date sources remains a practically important approach. While prior work using single-source knowledge has shown a clear positive correlation between the performances of knowledge selection and response generation, there are no existing multi-source datasets for evaluating support knowledge retrieval. Further, prior work has assumed that the knowledge sources available at test time are the same as during training. This unrealistic assumption unnecessarily handicaps models, as new knowledge sources can become available after a model is trained. In this paper, we present a high-quality <b>benchmark</b> named multi-source Wizard of Wikipedia (Ms.WoW) for evaluating multi-source dialogue knowledge selection and response generation. Unlike existing datasets, it contains clean support knowledge, grounded at the utterance level and partitioned into multiple knowledge sources. We further propose a new challenge, dialogue knowledge plug-and-play, which aims to test an already trained dialogue model on using new support knowledge from previously unseen sources in a <b>zero-shot</b> fashion.</p></p class="citation"></blockquote><h3 id=2639--26271-kiwi-a-dataset-of-knowledge-intensive-writing-instructions-for-answering-research-questions-fangyuan-xu-et-al-2024>(26/39 | 26/271) KIWI: A Dataset of Knowledge-Intensive Writing Instructions for Answering Research Questions (Fangyuan Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fangyuan Xu, Kyle Lo, Luca Soldaini, Bailey Kuehl, Eunsol Choi, David Wadden. (2024)<br><strong>KIWI: A Dataset of Knowledge-Intensive Writing Instructions for Answering Research Questions</strong><br><button class=copy-to-clipboard title="KIWI: A Dataset of Knowledge-Intensive Writing Instructions for Answering Research Questions" index=26>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-26 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Instruction Following, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03866v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03866v1.pdf filename=2403.03866v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> adapted to follow user <b>instructions</b> <b>are</b> now widely deployed as conversational agents. In this work, we examine one increasingly common <b>instruction-following</b> <b>task:</b> providing writing assistance to compose a long-form answer. To evaluate the capabilities of current <b>LLMs</b> on this task, we construct KIWI, a dataset of knowledge-intensive writing <b>instructions</b> <b>in</b> the scientific domain. Given a research question, an initial model-generated answer and a set of relevant papers, an expert annotator iteratively issues <b>instructions</b> <b>for</b> the model to revise and improve its answer. We collect 1,260 interaction turns from 234 interaction sessions with three state-of-the-art <b>LLMs.</b> Each turn includes a user <b>instruction,</b> <b>a</b> model response, and a human evaluation of the model response. Through a detailed analysis of the collected responses, we find that all models struggle to incorporate new information into an existing answer, and to perform precise and unambiguous edits. Further, we find that models struggle to judge whether their outputs successfully followed user <b>instructions,</b> <b>with</b> accuracy at least 10 points short of human agreement. Our findings indicate that KIWI will be a valuable resource to measure progress and improve <b>LLMs&rsquo;</b> <b>instruction-following</b> <b>capabilities</b> for knowledge intensive writing tasks.</p></p class="citation"></blockquote><h3 id=2739--27271-enhancing-asd-detection-accuracy-a-combined-approach-of-machine-learning-and-deep-learning-models-with-natural-language-processing-sergio-rubio-martín-et-al-2024>(27/39 | 27/271) Enhancing ASD detection accuracy: a combined approach of machine learning and deep learning models with natural language processing (Sergio Rubio-Martín et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sergio Rubio-Martín, María Teresa García-Ordás, Martín Bayón-Gutiérrez, Natalia Prieto-Fernández, José Alberto Benítez-Andrades. (2024)<br><strong>Enhancing ASD detection accuracy: a combined approach of machine learning and deep learning models with natural language processing</strong><br><button class=copy-to-clipboard title="Enhancing ASD detection accuracy: a combined approach of machine learning and deep learning models with natural language processing" index=27>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-27 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 30<br>Keywords: BERT, LSTM, Recurrent Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03581v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03581v1.pdf filename=2403.03581v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Purpose: Our study explored the use of artificial intelligence (AI) to diagnose autism spectrum disorder (ASD). It focused on machine learning (ML) and deep learning (DL) to detect ASD from text inputs on social media, addressing challenges in traditional ASD diagnosis. Methods: We used natural language processing (NLP), ML, and DL models (including decision trees, XGB, KNN, <b>RNN,</b> <b>LSTM,</b> Bi-LSTM, <b>BERT,</b> and BERTweet) to analyze 404,627 tweets, classifying them based on ASD or non-ASD authors. A subset of 90,000 tweets was used for model training and testing. Results: Our AI models showed high accuracy, with an 88% success rate in identifying texts from individuals with ASD. Conclusion: The study demonstrates AI&rsquo;s potential in improving ASD diagnosis, especially in children, highlighting the importance of early detection.</p></p class="citation"></blockquote><h3 id=2839--28271-apollo-an-lightweight-multilingual-medical-llm-towards-democratizing-medical-ai-to-6b-people-xidong-wang-et-al-2024>(28/39 | 28/271) Apollo: An Lightweight Multilingual Medical LLM towards Democratizing Medical AI to 6B People (Xidong Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xidong Wang, Nuo Chen, Junyin Chen, Yan Hu, Yidong Wang, Xiangbo Wu, Anningzhe Gao, Xiang Wan, Haizhou Li, Benyou Wang. (2024)<br><strong>Apollo: An Lightweight Multilingual Medical LLM towards Democratizing Medical AI to 6B People</strong><br><button class=copy-to-clipboard title="Apollo: An Lightweight Multilingual Medical LLM towards Democratizing Medical AI to 6B People" index=28>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-28 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 23<br>Keywords: Benchmarking, Fine-tuning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03640v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03640v2.pdf filename=2403.03640v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite the vast repository of global medical knowledge predominantly being in English, local languages are crucial for delivering tailored healthcare services, particularly in areas with limited medical resources. To extend the reach of medical AI advancements to a broader population, we aim to develop medical <b>LLMs</b> across the six most widely spoken languages, encompassing a global population of 6.1 billion. This effort culminates in the creation of the ApolloCorpora multilingual medical dataset and the XMedBench <b>benchmark.</b> In the multilingual medical <b>benchmark,</b> the released Apollo models, at various relatively-small sizes (i.e., 0.5B, 1.8B, 2B, 6B, and 7B), achieve the best performance among models of equivalent size. Especially, Apollo-7B is the state-of-the-art multilingual medical <b>LLMs</b> up to 70B. Additionally, these lite models could be used to improve the multi-lingual medical capabilities of larger models without <b>fine-tuning</b> in a proxy-tuning fashion. We will open-source training corpora, code, model weights and evaluation <b>benchmark.</b></p></p class="citation"></blockquote><h3 id=2939--29271-clongeval-a-chinese-benchmark-for-evaluating-long-context-large-language-models-zexuan-qiu-et-al-2024>(29/39 | 29/271) CLongEval: A Chinese Benchmark for Evaluating Long-Context Large Language Models (Zexuan Qiu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zexuan Qiu, Jingjing Li, Shijue Huang, Wanjun Zhong, Irwin King. (2024)<br><strong>CLongEval: A Chinese Benchmark for Evaluating Long-Context Large Language Models</strong><br><button class=copy-to-clipboard title="CLongEval: A Chinese Benchmark for Evaluating Long-Context Large Language Models" index=29>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-29 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 23<br>Keywords: Benchmarking, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03514v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03514v1.pdf filename=2403.03514v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Developing <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> with robust long-context capabilities has been the recent research focus, resulting in the emergence of long-context <b>LLMs</b> proficient in Chinese. However, the evaluation of these models remains underdeveloped due to a lack of <b>benchmarks.</b> To address this gap, we present CLongEval, a comprehensive Chinese <b>benchmark</b> for evaluating long-context <b>LLMs.</b> CLongEval is characterized by three key features: (1) Sufficient data volume, comprising 7 distinct tasks and 7,267 examples; (2) Broad applicability, accommodating to models with context windows size from 1K to 100K; (3) High quality, with over 2,000 manually annotated question-answer pairs in addition to the automatically constructed labels. With CLongEval, we undertake a comprehensive assessment of 6 open-source long-context <b>LLMs</b> and 2 leading commercial counterparts that feature both long-context abilities and proficiency in Chinese. We also provide in-depth analysis based on the empirical results, trying to shed light on the critical capabilities that present challenges in long-context settings. The dataset, evaluation scripts, and model outputs will be released.</p></p class="citation"></blockquote><h3 id=3039--30271-the-heuristic-core-understanding-subnetwork-generalization-in-pretrained-language-models-adithya-bhaskar-et-al-2024>(30/39 | 30/271) The Heuristic Core: Understanding Subnetwork Generalization in Pretrained Language Models (Adithya Bhaskar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Adithya Bhaskar, Dan Friedman, Danqi Chen. (2024)<br><strong>The Heuristic Core: Understanding Subnetwork Generalization in Pretrained Language Models</strong><br><button class=copy-to-clipboard title="The Heuristic Core: Understanding Subnetwork Generalization in Pretrained Language Models" index=30>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-30 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 20<br>Keywords: Fine-tuning, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03942v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03942v1.pdf filename=2403.03942v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Prior work has found that <b>pretrained</b> <b>language</b> <b>models</b> (LMs) <b>fine-tuned</b> with different random seeds can achieve similar in-domain performance but generalize differently on tests of syntactic generalization. In this work, we show that, even within a single model, we can find multiple subnetworks that perform similarly in-domain, but generalize vastly differently. To better understand these phenomena, we investigate if they can be understood in terms of &ldquo;competing subnetworks&rdquo;: the model initially represents a variety of distinct algorithms, corresponding to different subnetworks, and generalization occurs when it ultimately converges to one. This explanation has been used to account for generalization in simple algorithmic tasks. Instead of finding competing subnetworks, we find that all subnetworks &ndash; whether they generalize or not &ndash; share a set of attention heads, which we refer to as the heuristic core. Further analysis suggests that these attention heads emerge early in training and compute shallow, non-generalizing features. The model learns to generalize by incorporating additional attention heads, which depend on the outputs of the &ldquo;heuristic&rdquo; heads to compute higher-level features. Overall, our results offer a more detailed picture of the mechanisms for syntactic generalization in pretrained LMs.</p></p class="citation"></blockquote><h3 id=3139--31271-on-the-origins-of-linear-representations-in-large-language-models-yibo-jiang-et-al-2024>(31/39 | 31/271) On the Origins of Linear Representations in Large Language Models (Yibo Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yibo Jiang, Goutham Rajendran, Pradeep Ravikumar, Bryon Aragam, Victor Veitch. (2024)<br><strong>On the Origins of Linear Representations in Large Language Models</strong><br><button class=copy-to-clipboard title="On the Origins of Linear Representations in Large Language Models" index=31>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-31 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL, stat-ML<br>Keyword Score: 20<br>Keywords: LLaMA, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03867v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03867v1.pdf filename=2403.03867v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent works have argued that high-level semantic concepts are encoded &ldquo;linearly&rdquo; in the representation space of <b>large</b> <b>language</b> <b>models.</b> In this work, we study the origins of such linear representations. To that end, we introduce a simple latent variable model to abstract and formalize the concept dynamics of the next token prediction. We use this formalism to show that the next token prediction objective (softmax with cross-entropy) and the implicit bias of gradient descent together promote the linear representation of concepts. Experiments show that linear representations emerge when learning from data matching the latent variable model, confirming that this simple structure already suffices to yield linear representations. We additionally confirm some predictions of the theory using the <b>LLaMA-2</b> <b>large</b> <b>language</b> <b>model,</b> giving evidence that the simplified model yields generalizable insights.</p></p class="citation"></blockquote><h3 id=3239--32271-emojinize-enriching-any-text-with-emoji-translations-lars-henning-klein-et-al-2024>(32/39 | 32/271) Emojinize: Enriching Any Text with Emoji Translations (Lars Henning Klein et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lars Henning Klein, Roland Aydin, Robert West. (2024)<br><strong>Emojinize: Enriching Any Text with Emoji Translations</strong><br><button class=copy-to-clipboard title="Emojinize: Enriching Any Text with Emoji Translations" index=32>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-32 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-HC, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Text Understanding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03857v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03857v2.pdf filename=2403.03857v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Emoji have become ubiquitous in written communication, on the Web and beyond. They can emphasize or clarify emotions, add details to conversations, or simply serve decorative purposes. This casual use, however, barely scratches the surface of the expressive power of emoji. To further unleash this power, we present Emojinize, a method for translating arbitrary <b>text</b> <b>phrases</b> into sequences of one or more emoji without requiring human input. By leveraging the power of <b>large</b> <b>language</b> <b>models,</b> Emojinize can choose appropriate emoji by disambiguating based on context (eg, cricket-bat vs bat) and can express complex concepts compositionally by combining multiple emoji (eq, &ldquo;Emojinize&rdquo; is translated to input-latin-letters right-arrow grinning-face). In a cloze test&ndash;based user study, we show that Emojinize&rsquo;s emoji translations increase the human guessability of masked words by 55%, whereas human-picked emoji translations do so by only 29%. These results suggest that emoji provide a sufficiently rich vocabulary to accurately translate a wide variety of words. Moreover, annotating words and phrases with Emojinize&rsquo;s emoji translations opens the door to numerous downstream applications, including children learning how to read, adults learning foreign languages, and <b>text</b> <b>understanding</b> for people with learning disabilities.</p></p class="citation"></blockquote><h3 id=3339--33271-towards-detecting-ai-generated-text-within-human-ai-collaborative-hybrid-texts-zijie-zeng-et-al-2024>(33/39 | 33/271) Towards Detecting AI-Generated Text within Human-AI Collaborative Hybrid Texts (Zijie Zeng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zijie Zeng, Shiqi Liu, Lele Sha, Zhuang Li, Kaixun Yang, Sannyuya Liu, Dragan Gašević, Guanliang Chen. (2024)<br><strong>Towards Detecting AI-Generated Text within Human-AI Collaborative Hybrid Texts</strong><br><button class=copy-to-clipboard title="Towards Detecting AI-Generated Text within Human-AI Collaborative Hybrid Texts" index=33>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-33 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: AI-generated Text Detection, Text Segmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03506v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03506v1.pdf filename=2403.03506v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study explores the challenge of sentence-level <b>AI-generated</b> <b>text</b> <b>detection</b> within human-AI collaborative hybrid <b>texts.</b> <b>Existing</b> studies of <b>AI-generated</b> <b>text</b> <b>detection</b> for hybrid <b>texts</b> <b>often</b> rely on synthetic datasets. These typically involve hybrid <b>texts</b> <b>with</b> a limited number of boundaries. We contend that studies of detecting <b>AI-generated</b> <b>content</b> <b>within</b> hybrid <b>texts</b> <b>should</b> cover different types of hybrid <b>texts</b> <b>generated</b> in realistic settings to better inform real-world applications. Therefore, our study utilizes the CoAuthor dataset, which includes diverse, realistic hybrid <b>texts</b> <b>generated</b> through the collaboration between human writers and an intelligent writing system in multi-turn interactions. We adopt a two-step, segmentation-based pipeline: (i) detect segments within a given hybrid <b>text</b> <b>where</b> each segment contains sentences of consistent authorship, and (ii) classify the authorship of each identified segment. Our empirical findings highlight (1) detecting <b>AI-generated</b> <b>sentences</b> <b>in</b> hybrid <b>texts</b> <b>is</b> overall a challenging task because (1.1) human writers&rsquo; selecting and even editing <b>AI-generated</b> <b>sentences</b> <b>based</b> on personal preferences adds difficulty in identifying the authorship of segments; (1.2) the frequent change of authorship between neighboring sentences within the hybrid <b>text</b> <b>creates</b> difficulties for segment detectors in identifying authorship-consistent segments; (1.3) the short length of <b>text</b> <b>segments</b> within hybrid <b>texts</b> <b>provides</b> limited stylistic cues for reliable authorship determination; (2) before embarking on the detection process, it is beneficial to assess the average length of segments within the hybrid <b>text.</b> <b>This</b> assessment aids in deciding whether (2.1) to employ a <b>text</b> <b>segmentation-based</b> strategy for hybrid <b>texts</b> <b>with</b> longer segments, or (2.2) to adopt a direct sentence-by-sentence classification strategy for those with shorter segments.</p></p class="citation"></blockquote><h3 id=3439--34271-negating-negatives-alignment-without-human-positive-samples-via-distributional-dispreference-optimization-shitong-duan-et-al-2024>(34/39 | 34/271) Negating Negatives: Alignment without Human Positive Samples via Distributional Dispreference Optimization (Shitong Duan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shitong Duan, Xiaoyuan Yi, Peng Zhang, Tun Lu, Xing Xie, Ning Gu. (2024)<br><strong>Negating Negatives: Alignment without Human Positive Samples via Distributional Dispreference Optimization</strong><br><button class=copy-to-clipboard title="Negating Negatives: Alignment without Human Positive Samples via Distributional Dispreference Optimization" index=34>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-34 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03419v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03419v1.pdf filename=2403.03419v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have revolutionized the role of AI, yet also pose potential risks of propagating unethical content. Alignment technologies have been introduced to steer <b>LLMs</b> towards human preference, gaining increasing attention. Despite notable breakthroughs in this direction, existing methods heavily rely on high-quality positive-negative training pairs, suffering from noisy labels and the marginal distinction between preferred and dispreferred response data. Given recent <b>LLMs&rsquo;</b> proficiency in generating helpful responses, this work pivots towards a new research focus: achieving alignment using solely human-annotated negative samples, preserving helpfulness while reducing harmfulness. For this purpose, we propose Distributional Dispreference Optimization (D$^2$O), which maximizes the discrepancy between the generated responses and the dispreferred ones to effectively eschew harmful information. We theoretically demonstrate that D$^2$O is equivalent to learning a distributional instead of instance-level preference model reflecting human dispreference against the distribution of negative responses. Besides, D$^2$O integrates an implicit Jeffrey Divergence regularization to balance the exploitation and exploration of reference policies and converges to a non-negative one during training. Extensive experiments demonstrate that our method achieves comparable generation quality and surpasses the latest baselines in producing less harmful and more informative responses with better training stability and faster convergence.</p></p class="citation"></blockquote><h3 id=3539--35271-transformers-and-language-models-in-form-understanding-a-comprehensive-review-of-scanned-document-analysis-abdelrahman-abdallah-et-al-2024>(35/39 | 35/271) Transformers and Language Models in Form Understanding: A Comprehensive Review of Scanned Document Analysis (Abdelrahman Abdallah et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Abdelrahman Abdallah, Daniel Eberharter, Zoe Pfister, Adam Jatowt. (2024)<br><strong>Transformers and Language Models in Form Understanding: A Comprehensive Review of Scanned Document Analysis</strong><br><button class=copy-to-clipboard title="Transformers and Language Models in Form Understanding: A Comprehensive Review of Scanned Document Analysis" index=35>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-35 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CV, cs.CL<br>Keyword Score: 13<br>Keywords: Benchmarking, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04080v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04080v1.pdf filename=2403.04080v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a comprehensive survey of research works on the topic of form understanding in the context of scanned documents. We delve into recent advancements and breakthroughs in the field, highlighting the significance of language models and <b>transformers</b> in solving this challenging task. Our research methodology involves an in-depth analysis of popular documents and forms of understanding of trends over the last decade, enabling us to offer valuable insights into the evolution of this domain. Focusing on cutting-edge models, we showcase how <b>transformers</b> have propelled the field forward, revolutionizing form-understanding techniques. Our exploration includes an extensive examination of state-of-the-art language models designed to effectively tackle the complexities of noisy scanned documents. Furthermore, we present an overview of the latest and most relevant datasets, which serve as essential <b>benchmarks</b> for evaluating the performance of selected models. By comparing and contrasting the capabilities of these models, we aim to provide researchers and practitioners with useful guidance in choosing the most suitable solutions for their specific form understanding tasks.</p></p class="citation"></blockquote><h3 id=3639--36271-magic-markup-maintaining-document-external-markup-with-an-llm-edward-misback-et-al-2024>(36/39 | 36/271) Magic Markup: Maintaining Document-External Markup with an LLM (Edward Misback et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Edward Misback, Zachary Tatlock, Steven L. Tanimoto. (2024)<br><strong>Magic Markup: Maintaining Document-External Markup with an LLM</strong><br><button class=copy-to-clipboard title="Magic Markup: Maintaining Document-External Markup with an LLM" index=36>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-36 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 13<br>Keywords: Benchmarking, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03481v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03481v1.pdf filename=2403.03481v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Text documents, including programs, typically have human-readable semantic structure. Historically, programmatic access to these semantics has required explicit in-document tagging. Especially in systems where the text has an execution semantics, this means it is an opt-in feature that is hard to support properly. Today, language models offer a new method: metadata can be bound to entities in changing text using a model&rsquo;s human-like understanding of semantics, with no requirements on the document structure. This method expands the applications of document annotation, a fundamental operation in program writing, debugging, maintenance, and presentation. We contribute a system that employs an intelligent agent to re-tag modified programs, enabling rich annotations to automatically follow code as it evolves. We also contribute a formal problem definition, an empirical synthetic <b>benchmark</b> suite, and our <b>benchmark</b> generator. Our system achieves an accuracy of 90% on our <b>benchmarks</b> and can replace a document&rsquo;s tags in parallel at a rate of 5 seconds per tag. While there remains significant room for improvement, we find performance reliable enough to justify further exploration of applications.</p></p class="citation"></blockquote><h3 id=3739--37271-from-one-to-many-expanding-the-scope-of-toxicity-mitigation-in-language-models-luiza-pozzobon-et-al-2024>(37/39 | 37/271) From One to Many: Expanding the Scope of Toxicity Mitigation in Language Models (Luiza Pozzobon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Luiza Pozzobon, Patrick Lewis, Sara Hooker, Beyza Ermis. (2024)<br><strong>From One to Many: Expanding the Scope of Toxicity Mitigation in Language Models</strong><br><button class=copy-to-clipboard title="From One to Many: Expanding the Scope of Toxicity Mitigation in Language Models" index=37>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-37 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03893v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03893v1.pdf filename=2403.03893v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To date, toxicity mitigation in language models has almost entirely been focused on single-language settings. As language models embrace multilingual capabilities, it&rsquo;s crucial our safety measures keep pace. Recognizing this research gap, our approach expands the scope of conventional toxicity mitigation to address the complexities presented by multiple languages. In the absence of sufficient annotated datasets across languages, we employ translated data to evaluate and enhance our mitigation techniques. We also compare <b>finetuning</b> mitigation approaches against retrieval-augmented techniques under both static and continual toxicity mitigation scenarios. This allows us to examine the effects of translation quality and the cross-lingual transfer on toxicity mitigation. We also explore how model size and data quantity affect the success of these mitigation efforts. Covering nine languages, our study represents a broad array of linguistic families and levels of resource availability, ranging from high to mid-resource languages. Through comprehensive experiments, we provide insights into the complexities of multilingual toxicity mitigation, offering valuable insights and paving the way for future research in this increasingly important field. Code and data are available at <a href=https://github.com/for-ai/goodtriever>https://github.com/for-ai/goodtriever</a>.</p></p class="citation"></blockquote><h3 id=3839--38271-vlsp-2023----lter-a-summary-of-the-challenge-on-legal-textual-entailment-recognition-vu-tran-et-al-2024>(38/39 | 38/271) VLSP 2023 &ndash; LTER: A Summary of the Challenge on Legal Textual Entailment Recognition (Vu Tran et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vu Tran, Ha-Thanh Nguyen, Trung Vo, Son T. Luu, Hoang-Anh Dang, Ngoc-Cam Le, Thi-Thuy Le, Minh-Tien Nguyen, Truong-Son Nguyen, Le-Minh Nguyen. (2024)<br><strong>VLSP 2023 &ndash; LTER: A Summary of the Challenge on Legal Textual Entailment Recognition</strong><br><button class=copy-to-clipboard title="VLSP 2023 -- LTER: A Summary of the Challenge on Legal Textual Entailment Recognition" index=38>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-38 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Textual Entailment<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03435v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03435v1.pdf filename=2403.03435v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this new era of rapid AI development, especially in language processing, the demand for AI in the legal domain is increasingly critical. In the context where research in other languages such as English, Japanese, and Chinese has been well-established, we introduce the first fundamental research for the Vietnamese language in the legal domain: legal <b>textual</b> <b>entailment</b> recognition through the Vietnamese Language and Speech Processing workshop. In analyzing participants&rsquo; results, we discuss certain linguistic aspects critical in the legal domain that pose challenges that need to be addressed.</p></p class="citation"></blockquote><h3 id=3939--39271-a-measure-for-transparent-comparison-of-linguistic-diversity-in-multilingual-nlp-data-sets-tanja-samardzic-et-al-2024>(39/39 | 39/271) A Measure for Transparent Comparison of Linguistic Diversity in Multilingual NLP Data Sets (Tanja Samardzic et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tanja Samardzic, Ximena Gutierrez, Christian Bentz, Steven Moran, Olga Pelloni. (2024)<br><strong>A Measure for Transparent Comparison of Linguistic Diversity in Multilingual NLP Data Sets</strong><br><button class=copy-to-clipboard title="A Measure for Transparent Comparison of Linguistic Diversity in Multilingual NLP Data Sets" index=39>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-39 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03909v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03909v1.pdf filename=2403.03909v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Typologically diverse <b>benchmarks</b> are increasingly created to track the progress achieved in multilingual NLP. Linguistic diversity of these data sets is typically measured as the number of languages or language families included in the sample, but such measures do not consider structural properties of the included languages. In this paper, we propose assessing linguistic diversity of a data set against a reference language sample as a means of maximising linguistic diversity in the long run. We represent languages as sets of features and apply a version of the Jaccard index suitable for comparing sets of measures. In addition to the features extracted from typological data bases, we propose an automatic text-based measure, which can be used as a means of overcoming the well-known problem of data sparsity in manually collected features. Our diversity score is interpretable in terms of linguistic features and can identify the types of languages that are not represented in a data set. Using our method, we analyse a range of popular multilingual data sets (UD, Bible100, mBERT, XTREME, XGLUE, XNLI, XCOPA, TyDiQA, XQuAD). In addition to ranking these data sets, we find, for example, that (poly)synthetic languages are missing in almost all of them.</p></p class="citation"></blockquote><h2 id=cslg-54>cs.LG (54)</h2><h3 id=154--40271-self-attention-empowered-graph-convolutional-network-for-structure-learning-and-node-embedding-mengying-jiang-et-al-2024>(1/54 | 40/271) Self-Attention Empowered Graph Convolutional Network for Structure Learning and Node Embedding (Mengying Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mengying Jiang, Guizhong Liu, Yuanchao Su, Xinliang Wu. (2024)<br><strong>Self-Attention Empowered Graph Convolutional Network for Structure Learning and Node Embedding</strong><br><button class=copy-to-clipboard title="Self-Attention Empowered Graph Convolutional Network for Structure Learning and Node Embedding" index=40>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-40 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-SI, cs.LG<br>Keyword Score: 101<br>Keywords: Graph Convolutional Network, Graph Convolutional Network, Graph, Graph Neural Network, Graph Neural Network, Node Embedding, Benchmarking, Convolution, Convolutional Neural Network, Representation Learning, Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03465v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03465v1.pdf filename=2403.03465v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In <b>representation</b> <b>learning</b> on <b>graph-structured</b> <b>data,</b> <b>many</b> popular <b>graph</b> <b>neural</b> <b>networks</b> <b>(GNNs)</b> fail to capture long-range dependencies, leading to performance degradation. Furthermore, this weakness is magnified when the concerned <b>graph</b> <b>is</b> <b>characterized</b> by heterophily (low homophily). To solve this issue, this paper proposes a novel <b>graph</b> <b>learning</b> <b>framework</b> called the <b>graph</b> <b>convolutional</b> <b>network</b> with <b>self-attention</b> <b>(GCN-SA).</b> The proposed scheme exhibits an exceptional generalization capability in <b>node-level</b> <b>representation</b> <b>learning.</b> The proposed <b>GCN-SA</b> contains two enhancements corresponding to edges and <b>node</b> <b>features.</b> For edges, we utilize a <b>self-attention</b> mechanism to design a stable and effective <b>graph-structure-learning</b> <b>module</b> <b>that</b> can capture the internal correlation between any pair of <b>nodes.</b> <b>This</b> <b>graph-structure-learning</b> <b>module</b> <b>can</b> identify reliable neighbors for each <b>node</b> <b>from</b> the entire <b>graph.</b> <b>Regarding</b> <b>the</b> <b>node</b> <b>features,</b> we modify the <b>transformer</b> block to make it more applicable to enable <b>GCN</b> to fuse valuable information from the entire <b>graph.</b> <b>These</b> <b>two</b> enhancements work in distinct ways to help our <b>GCN-SA</b> capture long-range dependencies, enabling it to perform <b>representation</b> <b>learning</b> on <b>graphs</b> <b>with</b> <b>varying</b> levels of homophily. The experimental results on <b>benchmark</b> datasets demonstrate the effectiveness of the proposed <b>GCN-SA.</b> Compared to other outstanding <b>GNN</b> counterparts, the proposed <b>GCN-SA</b> is competitive.</p></p class="citation"></blockquote><h3 id=254--41271-a-teacher-free-graph-knowledge-distillation-framework-with-dual-self-distillation-lirong-wu-et-al-2024>(2/54 | 41/271) A Teacher-Free Graph Knowledge Distillation Framework with Dual Self-Distillation (Lirong Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lirong Wu, Haitao Lin, Zhangyang Gao, Guojiang Zhao, Stan Z. Li. (2024)<br><strong>A Teacher-Free Graph Knowledge Distillation Framework with Dual Self-Distillation</strong><br><button class=copy-to-clipboard title="A Teacher-Free Graph Knowledge Distillation Framework with Dual Self-Distillation" index=41>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-41 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 63<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Knowledge Distillation, Knowledge Distillation, Knowledge Distillation, Self-Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03483v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03483v1.pdf filename=2403.03483v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent years have witnessed great success in handling <b>graph-related</b> <b>tasks</b> <b>with</b> <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs).</b> Despite their great academic success, Multi-Layer Perceptrons (MLPs) remain the primary workhorse for practical industrial applications. One reason for such an academic-industry gap is the neighborhood-fetching latency incurred by data dependency in <b>GNNs.</b> To reduce their gaps, <b>Graph</b> <b>Knowledge</b> <b>Distillation</b> (GKD) is proposed, usually based on a standard teacher-student architecture, to <b>distill</b> <b>knowledge</b> <b>from</b> a large teacher <b>GNN</b> into a lightweight student <b>GNN</b> or MLP. However, we found in this paper that neither teachers nor <b>GNNs</b> are necessary for <b>graph</b> <b>knowledge</b> <b>distillation.</b> We propose a Teacher-Free <b>Graph</b> <b>Self-Distillation</b> <b>(TGS)</b> framework that does not require any teacher model or <b>GNNs</b> during both training and inference. More importantly, the proposed TGS framework is purely based on MLPs, where structural information is only implicitly used to guide dual <b>knowledge</b> <b>self-distillation</b> between the target node and its neighborhood. As a result, TGS enjoys the benefits of <b>graph</b> <b>topology</b> <b>awareness</b> in training but is free from data dependency in inference. Extensive experiments have shown that the performance of vanilla MLPs can be greatly improved with dual <b>self-distillation,</b> e.g., TGS improves over vanilla MLPs by 15.54% on average and outperforms state-of-the-art GKD algorithms on six real-world datasets. In terms of inference speed, TGS infers 75X-89X faster than existing <b>GNNs</b> and 16X-25X faster than classical inference acceleration methods.</p></p class="citation"></blockquote><h3 id=354--42271-galore-memory-efficient-llm-training-by-gradient-low-rank-projection-jiawei-zhao-et-al-2024>(3/54 | 42/271) GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection (Jiawei Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiawei Zhao, Zhenyu Zhang, Beidi Chen, Zhangyang Wang, Anima Anandkumar, Yuandong Tian. (2024)<br><strong>GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection</strong><br><button class=copy-to-clipboard title="GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection" index=42>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-42 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 60<br>Keywords: Fine-tuning, LLaMA, RoBERTa, GLUE, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03507v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03507v1.pdf filename=2403.03507v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Training <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> presents significant memory challenges, predominantly due to the growing size of weights and optimizer states. Common memory-reduction approaches, such as low-rank adaptation (LoRA), add a trainable low-rank matrix to the frozen pre-trained weight in each layer, reducing trainable parameters and optimizer states. However, such approaches typically underperform training with full-rank weights in both pre-training and <b>fine-tuning</b> stages since they limit the parameter search to a low-rank subspace and alter the training dynamics, and further, may require full-rank warm start. In this work, we propose Gradient Low-Rank Projection (GaLore), a training strategy that allows full-parameter learning but is more memory-efficient than common low-rank adaptation methods such as LoRA. Our approach reduces memory usage by up to 65.5% in optimizer states while maintaining both efficiency and performance for pre-training on <b>LLaMA</b> 1B and 7B architectures with C4 dataset with up to 19.7B tokens, and on <b>fine-tuning</b> <b>RoBERTa</b> on <b>GLUE</b> tasks. Our 8-bit GaLore further reduces optimizer memory by up to 82.5% and total training memory by 63.3%, compared to a BF16 baseline. Notably, we demonstrate, for the first time, the feasibility of pre-training a 7B model on consumer GPUs with 24GB memory (e.g., NVIDIA RTX 4090) without model parallel, checkpointing, or offloading strategies.</p></p class="citation"></blockquote><h3 id=454--43271-on-the-effectiveness-of-distillation-in-mitigating-backdoors-in-pre-trained-encoder-tingxu-han-et-al-2024>(4/54 | 43/271) On the Effectiveness of Distillation in Mitigating Backdoors in Pre-trained Encoder (Tingxu Han et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tingxu Han, Shenghan Huang, Ziqi Ding, Weisong Sun, Yebo Feng, Chunrong Fang, Jun Li, Hanwei Qian, Cong Wu, Quanjun Zhang, Yang Liu, Zhenyu Chen. (2024)<br><strong>On the Effectiveness of Distillation in Mitigating Backdoors in Pre-trained Encoder</strong><br><button class=copy-to-clipboard title="On the Effectiveness of Distillation in Mitigating Backdoors in Pre-trained Encoder" index=43>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-43 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 50<br>Keywords: Fine-tuning, Knowledge Distillation, Knowledge Distillation, Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03846v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03846v1.pdf filename=2403.03846v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we study a defense against poisoned encoders in SSL called <b>distillation,</b> which is a defense used in <b>supervised</b> <b>learning</b> originally. <b>Distillation</b> aims to <b>distill</b> knowledge from a given model (a.k.a the teacher net) and transfer it to another (a.k.a the student net). Now, we use it to <b>distill</b> benign knowledge from poisoned pre-trained encoders and transfer it to a new encoder, resulting in a clean pre-trained encoder. In particular, we conduct an empirical study on the effectiveness and performance of <b>distillation</b> against poisoned encoders. Using two state-of-the-art backdoor attacks against pre-trained image encoders and four commonly used image classification datasets, our experimental results show that <b>distillation</b> can reduce attack success rate from 80.87% to 27.51% while suffering a 6.35% loss in accuracy. Moreover, we investigate the impact of three core components of <b>distillation</b> on performance: teacher net, student net, and <b>distillation</b> loss. By comparing 4 different teacher nets, 3 student nets, and 6 <b>distillation</b> losses, we find that <b>fine-tuned</b> teacher nets, warm-up-training-based student nets, and attention-based <b>distillation</b> loss perform best, respectively.</p></p class="citation"></blockquote><h3 id=554--44271-three-revisits-to-node-level-graph-anomaly-detection-outliers-message-passing-and-hyperbolic-neural-networks-jing-gu-et-al-2024>(5/54 | 44/271) Three Revisits to Node-Level Graph Anomaly Detection: Outliers, Message Passing and Hyperbolic Neural Networks (Jing Gu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jing Gu, Dongmian Zou. (2024)<br><strong>Three Revisits to Node-Level Graph Anomaly Detection: Outliers, Message Passing and Hyperbolic Neural Networks</strong><br><button class=copy-to-clipboard title="Three Revisits to Node-Level Graph Anomaly Detection: Outliers, Message Passing and Hyperbolic Neural Networks" index=44>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-44 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 49<br>Keywords: Message-Passing, Graph Anomaly Detection, Graph, Anomaly Detection, Benchmarking, Benchmarking, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04010v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04010v1.pdf filename=2403.04010v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>anomaly</b> <b>detection</b> plays a vital role for identifying abnormal instances in complex networks. Despite advancements of methodology based on deep learning in recent years, existing <b>benchmarking</b> approaches exhibit limitations that hinder a comprehensive comparison. In this paper, we revisit datasets and approaches for <b>unsupervised</b> node-level <b>graph</b> <b>anomaly</b> <b>detection</b> tasks from three aspects. Firstly, we introduce outlier injection methods that create more diverse and <b>graph-based</b> <b>anomalies</b> <b>in</b> <b>graph</b> <b>datasets.</b> <b>Secondly,</b> we compare methods employing message passing against those without, uncovering the unexpected decline in performance associated with message passing. Thirdly, we explore the use of hyperbolic neural networks, specifying crucial architecture and loss design that contribute to enhanced performance. Through rigorous experiments and evaluations, our study sheds light on general strategies for improving node-level <b>graph</b> <b>anomaly</b> <b>detection</b> methods.</p></p class="citation"></blockquote><h3 id=654--45271-probabilistic-topic-modelling-with-transformer-representations-arik-reuter-et-al-2024>(6/54 | 45/271) Probabilistic Topic Modelling with Transformer Representations (Arik Reuter et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Arik Reuter, Anton Thielmann, Christoph Weisser, Benjamin Säfken, Thomas Kneib. (2024)<br><strong>Probabilistic Topic Modelling with Transformer Representations</strong><br><button class=copy-to-clipboard title="Probabilistic Topic Modelling with Transformer Representations" index=45>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-45 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG<br>Keyword Score: 43<br>Keywords: Autoencoder, Clustering, Topic Model, Variational Autoencoder, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03737v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03737v1.pdf filename=2403.03737v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Topic</b> <b>modelling</b> was mostly dominated by Bayesian graphical models during the last decade. With the rise of <b>transformers</b> in Natural Language Processing, however, several successful models that rely on straightforward <b>clustering</b> approaches in <b>transformer-based</b> embedding spaces have emerged and consolidated the notion of <b>topics</b> <b>as</b> clusters of embedding vectors. We propose the <b>Transformer-Representation</b> Neural <b>Topic</b> <b>Model</b> (TNTM), which combines the benefits of <b>topic</b> <b>representations</b> in <b>transformer-based</b> embedding spaces and probabilistic modelling. Therefore, this approach unifies the powerful and versatile notion of <b>topics</b> <b>based</b> on <b>transformer</b> embeddings with fully probabilistic modelling, as in models such as Latent Dirichlet Allocation (LDA). We utilize the <b>variational</b> <b>autoencoder</b> (VAE) framework for improved inference speed and modelling flexibility. Experimental results show that our proposed model achieves results on par with various state-of-the-art approaches in terms of embedding coherence while maintaining almost perfect <b>topic</b> <b>diversity.</b> The corresponding source code is available at <a href=https://github.com/ArikReuter/TNTM>https://github.com/ArikReuter/TNTM</a>.</p></p class="citation"></blockquote><h3 id=754--46271-simplified-pcnet-with-robustness-bingheng-li-et-al-2024>(7/54 | 46/271) Simplified PCNet with Robustness (Bingheng Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bingheng Li, Xuanting Xie, Haoxiang Lei, Ruiyi Fang, Zhao Kang. (2024)<br><strong>Simplified PCNet with Robustness</strong><br><button class=copy-to-clipboard title="Simplified PCNet with Robustness" index=46>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-46 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 43<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Semi-Supervised Learning, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03676v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03676v1.pdf filename=2403.03676v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> have garnered significant attention for their success in learning the representation of homophilic or heterophilic <b>graphs.</b> <b>However,</b> <b>they</b> cannot generalize well to real-world <b>graphs</b> <b>with</b> <b>different</b> levels of homophily. In response, the Possion-Charlier Network (PCNet) \cite{li2024pc}, the previous work, allows <b>graph</b> <b>representation</b> <b>to</b> be learned from heterophily to homophily. Although PCNet alleviates the heterophily issue, there remain some challenges in further improving the efficacy and efficiency. In this paper, we simplify PCNet and enhance its robustness. We first extend the filter order to continuous values and reduce its parameters. Two variants with adaptive neighborhood sizes are implemented. Theoretical analysis shows our model&rsquo;s robustness to <b>graph</b> <b>structure</b> <b>perturbations</b> or <b>adversarial</b> <b>attacks.</b> We validate our approach through <b>semi-supervised</b> <b>learning</b> tasks on various datasets representing both homophilic and heterophilic <b>graphs.</b></p></p class="citation"></blockquote><h3 id=854--47271-unsupervised-contrastive-learning-for-robust-rf-device-fingerprinting-under-time-domain-shift-jun-chen-et-al-2024>(8/54 | 47/271) Unsupervised Contrastive Learning for Robust RF Device Fingerprinting Under Time-Domain Shift (Jun Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jun Chen, Weng-Keen Wong, Bechir Hamdaoui. (2024)<br><strong>Unsupervised Contrastive Learning for Robust RF Device Fingerprinting Under Time-Domain Shift</strong><br><button class=copy-to-clipboard title="Unsupervised Contrastive Learning for Robust RF Device Fingerprinting Under Time-Domain Shift" index=47>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-47 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, eess-SP<br>Keyword Score: 40<br>Keywords: Contrastive Learning, Self-supervised Learning, Self-supervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04036v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04036v1.pdf filename=2403.04036v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Radio Frequency (RF) device fingerprinting has been recognized as a potential technology for enabling automated wireless device identification and classification. However, it faces a key challenge due to the domain shift that could arise from variations in the channel conditions and environmental settings, potentially degrading the accuracy of RF-based device classification when testing and training data is collected in different domains. This paper introduces a novel solution that leverages <b>contrastive</b> <b>learning</b> to mitigate this domain shift problem. <b>Contrastive</b> <b>learning,</b> a state-of-the-art <b>self-supervised</b> <b>learning</b> approach from deep learning, learns a distance metric such that positive pairs are closer (i.e. more similar) in the learned metric space than negative pairs. When applied to RF fingerprinting, our model treats RF signals from the same transmission as positive pairs and those from different transmissions as negative pairs. Through experiments on wireless and wired RF datasets collected over several days, we demonstrate that our <b>contrastive</b> <b>learning</b> approach captures domain-invariant features, diminishing the effects of domain-specific variations. Our results show large and consistent improvements in accuracy (10.8% to 27.8%) over baseline models, thus underscoring the effectiveness of <b>contrastive</b> <b>learning</b> in improving device classification under domain shift.</p></p class="citation"></blockquote><h3 id=954--48271-knockoff-guided-feature-selection-via-a-single-pre-trained-reinforced-agent-xinyuan-wang-et-al-2024>(9/54 | 48/271) Knockoff-Guided Feature Selection via A Single Pre-trained Reinforced Agent (Xinyuan Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinyuan Wang, Dongjie Wang, Wangyang Ying, Rui Xie, Haifeng Chen, Yanjie Fu. (2024)<br><strong>Knockoff-Guided Feature Selection via A Single Pre-trained Reinforced Agent</strong><br><button class=copy-to-clipboard title="Knockoff-Guided Feature Selection via A Single Pre-trained Reinforced Agent" index=48>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-48 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, stat-ML<br>Keyword Score: 40<br>Keywords: Reconstruction Loss, Reinforcement Learning, Supervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04015v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04015v1.pdf filename=2403.04015v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Feature selection prepares the AI-readiness of data by eliminating redundant features. Prior research falls into two primary categories: i) <b>Supervised</b> Feature Selection, which identifies the optimal feature subset based on their relevance to the target variable; ii) <b>Unsupervised</b> Feature Selection, which reduces the feature space dimensionality by capturing the essential information within the feature set instead of using target variable. However, SFS approaches suffer from time-consuming processes and limited generalizability due to the dependence on the target variable and downstream ML tasks. UFS methods are constrained by the deducted feature space is latent and untraceable. To address these challenges, we introduce an innovative framework for feature selection, which is guided by knockoff features and optimized through <b>reinforcement</b> <b>learning,</b> to identify the optimal and effective feature subset. In detail, our method involves generating &ldquo;knockoff&rdquo; features that replicate the distribution and characteristics of the original features but are independent of the target variable. Each feature is then assigned a pseudo label based on its correlation with all the knockoff features, serving as a novel metric for feature evaluation. Our approach utilizes these pseudo labels to guide the feature selection process in 3 novel ways, optimized by a single reinforced agent: 1). A deep Q-network, pre-trained with the original features and their corresponding pseudo labels, is employed to improve the efficacy of the exploration process in feature selection. 2). We introduce <b>unsupervised</b> rewards to evaluate the feature subset quality based on the pseudo labels and the feature space <b>reconstruction</b> <b>loss</b> to reduce dependencies on the target variable. 3). A new {\epsilon}-greedy strategy is used, incorporating insights from the pseudo labels to make the feature selection process more effective.</p></p class="citation"></blockquote><h3 id=1054--49271-stop-regressing-training-value-functions-via-classification-for-scalable-deep-rl-jesse-farebrother-et-al-2024>(10/54 | 49/271) Stop Regressing: Training Value Functions via Classification for Scalable Deep RL (Jesse Farebrother et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jesse Farebrother, Jordi Orbay, Quan Vuong, Adrien Ali Taïga, Yevgen Chebotar, Ted Xiao, Alex Irpan, Sergey Levine, Pablo Samuel Castro, Aleksandra Faust, Aviral Kumar, Rishabh Agarwal. (2024)<br><strong>Stop Regressing: Training Value Functions via Classification for Scalable Deep RL</strong><br><button class=copy-to-clipboard title="Stop Regressing: Training Value Functions via Classification for Scalable Deep RL" index=49>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-49 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, stat-ML<br>Keyword Score: 40<br>Keywords: Reinforcement Learning, Supervised Learning, Supervised Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03950v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03950v1.pdf filename=2403.03950v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Value functions are a central component of deep <b>reinforcement</b> <b>learning</b> (RL). These functions, parameterized by neural networks, are trained using a mean squared error regression objective to match bootstrapped target values. However, scaling value-based RL methods that use regression to large networks, such as high-capacity <b>Transformers,</b> has proven challenging. This difficulty is in stark contrast to <b>supervised</b> <b>learning:</b> by leveraging a cross-entropy classification loss, <b>supervised</b> <b>methods</b> have scaled reliably to massive networks. Observing this discrepancy, in this paper, we investigate whether the scalability of deep RL can also be improved simply by using classification in place of regression for training value functions. We demonstrate that value functions trained with categorical cross-entropy significantly improves performance and scalability in a variety of domains. These include: single-task RL on Atari 2600 games with SoftMoEs, multi-task RL on Atari with large-scale ResNets, robotic manipulation with Q-transformers, playing Chess without search, and a language-agent Wordle task with high-capacity <b>Transformers,</b> achieving state-of-the-art results on these domains. Through careful analysis, we show that the benefits of categorical cross-entropy primarily stem from its ability to mitigate issues inherent to value-based RL, such as noisy targets and non-stationarity. Overall, we argue that a simple shift to training value functions with categorical cross-entropy can yield substantial improvements in the scalability of deep RL at little-to-no cost.</p></p class="citation"></blockquote><h3 id=1154--50271-on-transfer-in-classification-how-well-do-subsets-of-classes-generalize-raphael-baena-et-al-2024>(11/54 | 50/271) On Transfer in Classification: How Well do Subsets of Classes Generalize? (Raphael Baena et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Raphael Baena, Lucas Drumetz, Vincent Gripon. (2024)<br><strong>On Transfer in Classification: How Well do Subsets of Classes Generalize?</strong><br><button class=copy-to-clipboard title="On Transfer in Classification: How Well do Subsets of Classes Generalize?" index=50>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-50 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Few-shot, Few-shot Learning, Fine-tuning, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03569v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03569v1.pdf filename=2403.03569v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In classification, it is usual to observe that models trained on a given set of classes can generalize to previously unseen ones, suggesting the ability to learn beyond the initial task. This ability is often leveraged in the context of <b>transfer</b> <b>learning</b> where a pretrained model can be used to process new classes, with or without fine tuning. Surprisingly, there are a few papers looking at the theoretical roots beyond this phenomenon. In this work, we are interested in laying the foundations of such a theoretical framework for transferability between sets of classes. Namely, we establish a partially ordered set of subsets of classes. This tool allows to represent which subset of classes can generalize to others. In a more practical setting, we explore the ability of our framework to predict which subset of classes can lead to the best performance when testing on all of them. We also explore <b>few-shot</b> <b>learning,</b> where <b>transfer</b> <b>is</b> the golden standard. Our work contributes to better understanding of <b>transfer</b> <b>mechanics</b> and model generalization.</p></p class="citation"></blockquote><h3 id=1254--51271-provable-filter-for-real-world-graph-clustering-xuanting-xie-et-al-2024>(12/54 | 51/271) Provable Filter for Real-world Graph Clustering (Xuanting Xie et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xuanting Xie, Erlin Pan, Zhao Kang, Wenyu Chen, Bingheng Li. (2024)<br><strong>Provable Filter for Real-world Graph Clustering</strong><br><button class=copy-to-clipboard title="Provable Filter for Real-world Graph Clustering" index=51>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-51 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 36<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Clustering, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03666v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03666v1.pdf filename=2403.03666v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>clustering,</b> <b>an</b> important <b>unsupervised</b> problem, has been shown to be more resistant to advances in <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs).</b> In addition, almost all <b>clustering</b> methods focus on homophilic <b>graphs</b> <b>and</b> <b>ignore</b> heterophily. This significantly limits their applicability in practice, since real-world <b>graphs</b> <b>exhibit</b> <b>a</b> structural disparity and cannot simply be classified as homophily and heterophily. Thus, a principled way to handle practical <b>graphs</b> <b>is</b> <b>urgently</b> needed. To fill this gap, we provide a novel solution with theoretical support. Interestingly, we find that most homophilic and heterophilic edges can be correctly identified on the basis of neighbor information. Motivated by this finding, we construct two <b>graphs</b> <b>that</b> <b>are</b> highly homophilic and heterophilic, respectively. They are used to build low-pass and high-pass filters to capture holistic information. Important features are further enhanced by the squeeze-and-excitation block. We validate our approach through extensive experiments on both homophilic and heterophilic <b>graphs.</b> <b>Empirical</b> <b>results</b> demonstrate the superiority of our method compared to state-of-the-art <b>clustering</b> methods.</p></p class="citation"></blockquote><h3 id=1354--52271-sampling-based-safe-reinforcement-learning-for-nonlinear-dynamical-systems-wesley-a-suttle-et-al-2024>(13/54 | 52/271) Sampling-based Safe Reinforcement Learning for Nonlinear Dynamical Systems (Wesley A. Suttle et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wesley A. Suttle, Vipul K. Sharma, Krishna C. Kosaraju, S. Sivaranjani, Ji Liu, Vijay Gupta, Brian M. Sadler. (2024)<br><strong>Sampling-based Safe Reinforcement Learning for Nonlinear Dynamical Systems</strong><br><button class=copy-to-clipboard title="Sampling-based Safe Reinforcement Learning for Nonlinear Dynamical Systems" index=52>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-52 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-OC<br>Keyword Score: 33<br>Keywords: Benchmarking, Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04007v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04007v1.pdf filename=2403.04007v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We develop provably safe and convergent <b>reinforcement</b> <b>learning</b> (RL) algorithms for control of nonlinear dynamical systems, bridging the gap between the hard safety guarantees of control theory and the convergence guarantees of RL theory. Recent advances at the intersection of control and RL follow a two-stage, safety filter approach to enforcing hard safety constraints: model-free RL is used to learn a potentially unsafe controller, whose actions are projected onto safe sets prescribed, for example, by a control barrier function. Though safe, such approaches lose any convergence guarantees enjoyed by the underlying RL methods. In this paper, we develop a single-stage, sampling-based approach to hard constraint satisfaction that learns RL controllers enjoying classical convergence guarantees while satisfying hard safety constraints throughout training and deployment. We validate the efficacy of our approach in <b>simulation,</b> including safe control of a quadcopter in a challenging obstacle avoidance problem, and demonstrate that it outperforms existing <b>benchmarks.</b></p></p class="citation"></blockquote><h3 id=1454--53271-graph-neural-network-outputs-are-almost-surely-asymptotically-constant-sam-adam-day-et-al-2024>(14/54 | 53/271) Graph neural network outputs are almost surely asymptotically constant (Sam Adam-Day et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sam Adam-Day, Michael Benedikt, İsmail İlkan Ceylan, Ben Finkelshtein. (2024)<br><strong>Graph neural network outputs are almost surely asymptotically constant</strong><br><button class=copy-to-clipboard title="Graph neural network outputs are almost surely asymptotically constant" index=53>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-53 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-LO, cs.LG<br>Keyword Score: 33<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03880v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03880v1.pdf filename=2403.03880v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>neural</b> <b>networks</b> <b>(GNNs)</b> are the predominant architectures for a variety of learning tasks on <b>graphs.</b> <b>We</b> <b>present</b> a new angle on the expressive power of <b>GNNs</b> by studying how the predictions of a <b>GNN</b> probabilistic classifier evolve as we apply it on larger <b>graphs</b> <b>drawn</b> <b>from</b> some random <b>graph</b> <b>model.</b> <b>We</b> show that the output converges to a constant function, which upper-bounds what these classifiers can express uniformly. This convergence phenomenon applies to a very wide class of <b>GNNs,</b> including state of the art models, with aggregates including mean and the attention-based mechanism of <b>graph</b> <b>transformers.</b> <b>Our</b> results apply to a broad class of random <b>graph</b> <b>models,</b> <b>including</b> the (sparse) Erd\H{o}s-R'enyi model and the stochastic block model. We empirically validate these findings, observing that the convergence phenomenon already manifests itself on <b>graphs</b> <b>of</b> <b>relatively</b> modest size.</p></p class="citation"></blockquote><h3 id=1554--54271-kg-treat-pre-training-for-treatment-effect-estimation-by-synergizing-patient-data-with-knowledge-graphs-ruoqi-liu-et-al-2024>(15/54 | 54/271) KG-TREAT: Pre-training for Treatment Effect Estimation by Synergizing Patient Data with Knowledge Graphs (Ruoqi Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruoqi Liu, Lingfei Wu, Ping Zhang. (2024)<br><strong>KG-TREAT: Pre-training for Treatment Effect Estimation by Synergizing Patient Data with Knowledge Graphs</strong><br><button class=copy-to-clipboard title="KG-TREAT: Pre-training for Treatment Effect Estimation by Synergizing Patient Data with Knowledge Graphs" index=54>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-54 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 33<br>Keywords: Graph, Fine-tuning, Knowledge Graph, Knowledge Graph, Grounding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03791v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03791v1.pdf filename=2403.03791v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Treatment effect estimation (TEE) is the task of determining the impact of various treatments on patient outcomes. Current TEE methods fall short due to reliance on limited labeled data and challenges posed by sparse and high-dimensional observational patient data. To address the challenges, we introduce a novel pre-training and <b>fine-tuning</b> framework, <b>KG-TREAT,</b> which synergizes large-scale observational patient data with biomedical <b>knowledge</b> <b>graphs</b> <b>(KGs)</b> to enhance TEE. Unlike previous approaches, <b>KG-TREAT</b> constructs dual-focus <b>KGs</b> and integrates a deep bi-level attention synergy method for in-depth information fusion, enabling distinct encoding of treatment-covariate and outcome-covariate relationships. <b>KG-TREAT</b> also incorporates two pre-training tasks to ensure a thorough <b>grounding</b> and contextualization of patient data and <b>KGs.</b> Evaluation on four downstream TEE tasks shows <b>KG-TREAT&rsquo;s</b> superiority over existing methods, with an average improvement of 7% in Area under the ROC Curve (AUC) and 9% in Influence Function-based Precision of Estimating Heterogeneous Effects (IF-PEHE). The effectiveness of our estimated treatment effects is further affirmed by alignment with established randomized clinical trial findings.</p></p class="citation"></blockquote><h3 id=1654--55271-prediction-of-cryptocurrency-prices-using-lstm-svm-and-polynomial-regression-novan-fauzi-al-giffary-et-al-2024>(16/54 | 55/271) Prediction Of Cryptocurrency Prices Using LSTM, SVM And Polynomial Regression (Novan Fauzi Al Giffary et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Novan Fauzi Al Giffary, Feri Sulianta. (2024)<br><strong>Prediction Of Cryptocurrency Prices Using LSTM, SVM And Polynomial Regression</strong><br><button class=copy-to-clipboard title="Prediction Of Cryptocurrency Prices Using LSTM, SVM And Polynomial Regression" index=55>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-55 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, q-fin-ST<br>Keyword Score: 33<br>Keywords: Benchmarking, LSTM, LSTM, LSTM<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03410v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03410v1.pdf filename=2403.03410v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rapid development of information technology, especially the Internet, has facilitated users with a quick and easy way to seek information. With these convenience offered by internet services, many individuals who initially invested in gold and precious metals are now shifting into digital investments in form of cryptocurrencies. However, investments in crypto coins are filled with uncertainties and fluctuation in daily basis. This risk posed as significant challenges for coin investors that could result in substantial investment losses. The uncertainty of the value of these crypto coins is a critical issue in the field of coin investment. Forecasting, is one of the methods used to predict the future value of these crypto coins. By utilizing the models of <b>Long</b> <b>Short</b> <b>Term</b> <b>Memory,</b> Support Vector Machine, and Polynomial Regression algorithm for forecasting, a performance comparison is conducted to determine which algorithm model is most suitable for predicting crypto currency prices. The mean square error is employed as a <b>benchmark</b> for the comparison. By applying those three constructed algorithm models, the Support Vector Machine uses a linear kernel to produce the smallest mean square error compared to the <b>Long</b> <b>Short</b> <b>Term</b> <b>Memory</b> and Polynomial Regression algorithm models, with a mean square error value of 0.02. Keywords: Cryptocurrency, Forecasting, <b>Long</b> <b>Short</b> <b>Term</b> <b>Memory,</b> Mean Square Error, Polynomial Regression, Support Vector Machine</p></p class="citation"></blockquote><h3 id=1754--56271-inference-via-interpolation-contrastive-representations-provably-enable-planning-and-inference-benjamin-eysenbach-et-al-2024>(17/54 | 56/271) Inference via Interpolation: Contrastive Representations Provably Enable Planning and Inference (Benjamin Eysenbach et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Benjamin Eysenbach, Vivek Myers, Ruslan Salakhutdinov, Sergey Levine. (2024)<br><strong>Inference via Interpolation: Contrastive Representations Provably Enable Planning and Inference</strong><br><button class=copy-to-clipboard title="Inference via Interpolation: Contrastive Representations Provably Enable Planning and Inference" index=56>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-56 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 30<br>Keywords: Contrastive Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04082v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04082v1.pdf filename=2403.04082v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Given time series data, how can we answer questions like &ldquo;what will happen in the future?&rdquo; and &ldquo;how did we get here?&rdquo; These sorts of probabilistic inference questions are challenging when observations are high-dimensional. In this paper, we show how these questions can have compact, closed form solutions in terms of learned representations. The key idea is to apply a variant of <b>contrastive</b> <b>learning</b> to time series data. Prior work already shows that the representations learned by <b>contrastive</b> <b>learning</b> encode a probability ratio. By extending prior work to show that the marginal distribution over representations is Gaussian, we can then prove that joint distribution of representations is also Gaussian. Taken together, these results show that representations learned via temporal <b>contrastive</b> <b>learning</b> follow a Gauss-Markov chain, a graphical model where inference (e.g., prediction, planning) over representations corresponds to inverting a low-dimensional matrix. In one special case, inferring intermediate representations will be equivalent to interpolating between the learned representations. We validate our theory using numerical <b>simulations</b> on tasks up to 46-dimensions.</p></p class="citation"></blockquote><h3 id=1854--57271-bridging-diversity-and-uncertainty-in-active-learning-with-self-supervised-pre-training-paul-doucet-et-al-2024>(18/54 | 57/271) Bridging Diversity and Uncertainty in Active learning with Self-Supervised Pre-Training (Paul Doucet et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Paul Doucet, Benjamin Estermann, Till Aczel, Roger Wattenhofer. (2024)<br><strong>Bridging Diversity and Uncertainty in Active learning with Self-Supervised Pre-Training</strong><br><button class=copy-to-clipboard title="Bridging Diversity and Uncertainty in Active learning with Self-Supervised Pre-Training" index=57>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-57 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Active Learning, Self-supervised Learning, Self-supervised Pre-training<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03728v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03728v1.pdf filename=2403.03728v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study addresses the integration of diversity-based and uncertainty-based sampling strategies in <b>active</b> <b>learning,</b> particularly within the context of <b>self-supervised</b> <b>pre-trained</b> models. We introduce a straightforward heuristic called TCM that mitigates the cold start problem while maintaining strong performance across various data levels. By initially applying TypiClust for diversity sampling and subsequently transitioning to uncertainty sampling with Margin, our approach effectively combines the strengths of both strategies. Our experiments demonstrate that TCM consistently outperforms existing methods across various datasets in both low and high data regimes.</p></p class="citation"></blockquote><h3 id=1954--58271-routeexplainer-an-explanation-framework-for-vehicle-routing-problem-daisuke-kikuta-et-al-2024>(19/54 | 58/271) RouteExplainer: An Explanation Framework for Vehicle Routing Problem (Daisuke Kikuta et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daisuke Kikuta, Hiroki Ikeuchi, Kengo Tajiri, Yuusuke Nakano. (2024)<br><strong>RouteExplainer: An Explanation Framework for Vehicle Routing Problem</strong><br><button class=copy-to-clipboard title="RouteExplainer: An Explanation Framework for Vehicle Routing Problem" index=58>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-58 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, math-OC<br>Keyword Score: 30<br>Keywords: Counter-factual, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03585v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03585v1.pdf filename=2403.03585v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Vehicle Routing Problem (VRP) is a widely studied combinatorial optimization problem and has been applied to various practical problems. While the explainability for VRP is significant for improving the reliability and interactivity in practical VRP applications, it remains unexplored. In this paper, we propose RouteExplainer, a post-hoc explanation framework that explains the influence of each edge in a generated route. Our framework realizes this by rethinking a route as the sequence of actions and extending <b>counterfactual</b> explanations based on the action influence model to VRP. To enhance the explanation, we additionally propose an edge classifier that infers the intentions of each edge, a loss function to train the edge classifier, and explanation-text generation by <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> We quantitatively evaluate our edge classifier on four different VRPs. The results demonstrate its rapid computation while maintaining reasonable accuracy, thereby highlighting its potential for deployment in practical applications. Moreover, on the subject of a tourist route, we qualitatively evaluate explanations generated by our framework. This evaluation not only validates our framework but also shows the synergy between explanation frameworks and <b>LLMs.</b> See <a href=https://ntt-dkiku.github.io/xai-vrp>https://ntt-dkiku.github.io/xai-vrp</a> for our code, datasets, models, and demo.</p></p class="citation"></blockquote><h3 id=2054--59271-inverse-free-fast-natural-gradient-descent-method-for-deep-learning-xinwei-ou-et-al-2024>(20/54 | 59/271) Inverse-Free Fast Natural Gradient Descent Method for Deep Learning (Xinwei Ou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinwei Ou, Ce Zhu, Xiaolin Huang, Yipeng Liu. (2024)<br><strong>Inverse-Free Fast Natural Gradient Descent Method for Deep Learning</strong><br><button class=copy-to-clipboard title="Inverse-Free Fast Natural Gradient Descent Method for Deep Learning" index=59>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-59 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Transformer, Neural Machine Translation, BLEU<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03473v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03473v1.pdf filename=2403.03473v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Second-order methods can converge much faster than first-order methods by incorporating second-order derivates or statistics, but they are far less prevalent in deep learning due to their computational inefficiency. To handle this, many of the existing solutions focus on reducing the size of the matrix to be inverted. However, it is still needed to perform the inverse operator in each iteration. In this paper, we present a fast natural gradient descent (FNGD) method, which only requires computing the inverse during the first epoch. Firstly, we reformulate the gradient preconditioning formula in the natural gradient descent (NGD) as a weighted sum of per-sample gradients using the Sherman-Morrison-Woodbury formula. Building upon this, to avoid the iterative inverse operation involved in computing coefficients, the weighted coefficients are shared across epochs without affecting the empirical performance. FNGD approximates the NGD as a fixed-coefficient weighted sum, akin to the average sum in first-order methods. Consequently, the computational complexity of FNGD can approach that of first-order methods. To demonstrate the efficiency of the proposed FNGD, we perform empirical evaluations on image classification and <b>machine</b> <b>translation</b> tasks. For training ResNet-18 on the CIFAR-100 dataset, FNGD can achieve a speedup of 2.05$\times$ compared with KFAC. For training <b>Transformer</b> on Multi30K, FNGD outperforms AdamW by 24 <b>BLEU</b> score while requiring almost the same training time.</p></p class="citation"></blockquote><h3 id=2154--60271-boosting-meta-training-with-base-class-information-for-few-shot-learning-weihao-jiang-et-al-2024>(21/54 | 60/271) Boosting Meta-Training with Base Class Information for Few-Shot Learning (Weihao Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Weihao Jiang, Guodong Liu, Di He, Kun He. (2024)<br><strong>Boosting Meta-Training with Base Class Information for Few-Shot Learning</strong><br><button class=copy-to-clipboard title="Boosting Meta-Training with Base Class Information for Few-Shot Learning" index=60>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-60 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Few-shot, Few-shot Learning, Meta Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03472v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03472v1.pdf filename=2403.03472v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Few-shot</b> <b>learning,</b> a challenging task in machine learning, aims to learn a classifier adaptable to recognize new, unseen classes with limited labeled examples. <b>Meta-learning</b> <b>has</b> emerged as a prominent framework for <b>few-shot</b> <b>learning.</b> Its training framework is originally a task-level learning method, such as Model-Agnostic <b>Meta-Learning</b> <b>(MAML)</b> and Prototypical Networks. And a recently proposed training paradigm called <b>Meta-Baseline,</b> <b>which</b> consists of sequential pre-training and <b>meta-training</b> <b>stages,</b> gains state-of-the-art performance. However, as a non-end-to-end training method, indicating the <b>meta-training</b> <b>stage</b> can only begin after the completion of pre-training, <b>Meta-Baseline</b> <b>suffers</b> from higher training cost and suboptimal performance due to the inherent conflicts of the two training stages. To address these limitations, we propose an end-to-end training paradigm consisting of two alternative loops. In the outer loop, we calculate cross entropy loss on the entire training set while updating only the final linear layer. In the inner loop, we employ the original <b>meta-learning</b> <b>training</b> mode to calculate the loss and incorporate gradients from the outer loss to guide the parameter updates. This training paradigm not only converges quickly but also outperforms existing baselines, indicating that information from the overall training set and the <b>meta-learning</b> <b>training</b> paradigm could mutually reinforce one another. Moreover, being model-agnostic, our framework achieves significant performance gains, surpassing the baseline systems by approximate 1%.</p></p class="citation"></blockquote><h3 id=2254--61271-temporal-cross-attention-for-dynamic-embedding-and-tokenization-of-multimodal-electronic-health-records-yingbo-ma-et-al-2024>(22/54 | 61/271) Temporal Cross-Attention for Dynamic Embedding and Tokenization of Multimodal Electronic Health Records (Yingbo Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yingbo Ma, Suraj Kolla, Dhruv Kaliraman, Victoria Nolan, Zhenhong Hu, Ziyuan Guan, Yuanfang Ren, Brooke Armfield, Tezcan Ozrazgat-Baslanti, Tyler J. Loftus, Parisa Rashidi, Azra Bihorac, Benjamin Shickel. (2024)<br><strong>Temporal Cross-Attention for Dynamic Embedding and Tokenization of Multimodal Electronic Health Records</strong><br><button class=copy-to-clipboard title="Temporal Cross-Attention for Dynamic Embedding and Tokenization of Multimodal Electronic Health Records" index=61>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-61 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 26<br>Keywords: Multi-modal, Multi-modal, Transformer, Tokenization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04012v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04012v1.pdf filename=2403.04012v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The breadth, scale, and temporal granularity of modern electronic health records (EHR) systems offers great potential for estimating personalized and contextual patient health trajectories using sequential deep learning. However, learning useful representations of EHR data is challenging due to its high dimensionality, sparsity, multimodality, irregular and variable-specific recording frequency, and timestamp duplication when multiple measurements are recorded simultaneously. Although recent efforts to fuse structured EHR and unstructured clinical notes suggest the potential for more accurate prediction of clinical outcomes, less focus has been placed on EHR embedding approaches that directly address temporal EHR challenges by learning time-aware representations from <b>multimodal</b> patient time series. In this paper, we introduce a dynamic embedding and <b>tokenization</b> framework for precise representation of <b>multimodal</b> clinical time series that combines novel methods for encoding time and sequential position with temporal cross-attention. Our embedding and <b>tokenization</b> framework, when integrated into a multitask <b>transformer</b> classifier with sliding window attention, outperformed baseline approaches on the exemplar task of predicting the occurrence of nine postoperative complications of more than 120,000 major inpatient surgeries using <b>multimodal</b> data from three hospitals and two academic health centers in the United States.</p></p class="citation"></blockquote><h3 id=2354--62271-learning-invariant-representations-of-graph-neural-networks-via-cluster-generalization-donglin-xia-et-al-2024>(23/54 | 62/271) Learning Invariant Representations of Graph Neural Networks via Cluster Generalization (Donglin Xia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Donglin Xia, Xiao Wang, Nian Liu, Chuan Shi. (2024)<br><strong>Learning Invariant Representations of Graph Neural Networks via Cluster Generalization</strong><br><button class=copy-to-clipboard title="Learning Invariant Representations of Graph Neural Networks via Cluster Generalization" index=62>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-62 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Graph, Graph Neural Network, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03599v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03599v1.pdf filename=2403.03599v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>neural</b> <b>networks</b> <b>(GNNs)</b> have become increasingly popular in modeling <b>graph-structured</b> <b>data</b> <b>due</b> to their ability to learn node representations by aggregating local structure information. However, it is widely acknowledged that the test <b>graph</b> <b>structure</b> <b>may</b> differ from the training <b>graph</b> <b>structure,</b> <b>resulting</b> in a structure shift. In this paper, we experimentally find that the performance of <b>GNNs</b> drops significantly when the structure shift happens, suggesting that the learned models may be biased towards specific structure patterns. To address this challenge, we propose the Cluster Information Transfer (CIT) mechanism (Code available at <a href=https://github.com/BUPT-GAMMA/CITGNN)>https://github.com/BUPT-GAMMA/CITGNN)</a>, which can learn invariant representations for <b>GNNs,</b> thereby improving their generalization ability to various and unknown test <b>graphs</b> <b>with</b> <b>structure</b> shift. The CIT mechanism achieves this by combining different cluster information with the nodes while preserving their cluster-independent information. By generating nodes across different clusters, the mechanism significantly enhances the diversity of the nodes and helps <b>GNNs</b> learn the invariant representations. We provide a theoretical analysis of the CIT mechanism, showing that the impact of changing clusters during structure shift can be mitigated after transfer. Additionally, the proposed mechanism is a plug-in that can be easily used to improve existing <b>GNNs.</b> We comprehensively evaluate our proposed method on three typical structure shift scenarios, demonstrating its effectiveness in enhancing <b>GNNs&rsquo;</b> performance.</p></p class="citation"></blockquote><h3 id=2454--63271-dpot-auto-regressive-denoising-operator-transformer-for-large-scale-pde-pre-training-zhongkai-hao-et-al-2024>(24/54 | 63/271) DPOT: Auto-Regressive Denoising Operator Transformer for Large-Scale PDE Pre-Training (Zhongkai Hao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhongkai Hao, Chang Su, Songming Liu, Julius Berner, Chengyang Ying, Hang Su, Anima Anandkumar, Jian Song, Jun Zhu. (2024)<br><strong>DPOT: Auto-Regressive Denoising Operator Transformer for Large-Scale PDE Pre-Training</strong><br><button class=copy-to-clipboard title="DPOT: Auto-Regressive Denoising Operator Transformer for Large-Scale PDE Pre-Training" index=63>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-63 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-NA, cs.LG, math-NA<br>Keyword Score: 23<br>Keywords: Benchmarking, Foundation Model, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03542v3 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03542v3.pdf filename=2403.03542v3.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Pre-training has been investigated to improve the efficiency and performance of training neural operators in data-scarce settings. However, it is largely in its infancy due to the inherent complexity and diversity, such as long trajectories, multiple scales and varying dimensions of partial differential equations (PDEs) data. In this paper, we present a new auto-regressive denoising pre-training strategy, which allows for more stable and efficient pre-training on PDE data and generalizes to various downstream tasks. Moreover, by designing a flexible and scalable model architecture based on Fourier attention, we can easily scale up the model for large-scale pre-training. We train our PDE <b>foundation</b> <b>model</b> with up to 0.5B parameters on 10+ PDE datasets with more than 100k trajectories. Extensive experiments show that we achieve SOTA on these <b>benchmarks</b> and validate the strong generalizability of our model to significantly enhance performance on diverse downstream PDE tasks like 3D data. Code is available at \url{https://github.com/thu-ml/DPOT}.</p></p class="citation"></blockquote><h3 id=2554--64271-many-objective-multi-solution-transport-ziyue-li-et-al-2024>(25/54 | 64/271) Many-Objective Multi-Solution Transport (Ziyue Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ziyue Li, Tian Li, Virginia Smith, Jeff Bilmes, Tianyi Zhou. (2024)<br><strong>Many-Objective Multi-Solution Transport</strong><br><button class=copy-to-clipboard title="Many-Objective Multi-Solution Transport" index=64>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-64 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Federated Learning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04099v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04099v1.pdf filename=2403.04099v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Optimizing the performance of many objectives (instantiated by tasks or clients) jointly with a few Pareto stationary solutions (models) is critical in machine learning. However, previous multi-objective optimization methods often focus on a few number of objectives and cannot scale to many objectives that outnumber the solutions, leading to either subpar performance or ignored objectives. We introduce Many-objective multi-solution Transport (MosT), a framework that finds multiple diverse solutions in the Pareto front of many objectives. Our insight is to seek multiple solutions, each performing as a domain expert and focusing on a specific subset of objectives while collectively covering all of them. MosT formulates the problem as a bi-level optimization of weighted objectives for each solution, where the weights are defined by an optimal transport between the objectives and solutions. Our algorithm ensures convergence to Pareto stationary solutions for complementary subsets of objectives. On a range of applications in <b>federated</b> <b>learning,</b> multi-task learning, and mixture-of-prompt learning for <b>LLMs,</b> MosT distinctly outperforms strong baselines, delivering high-quality, diverse solutions that profile the entire Pareto frontier, thus ensuring balanced trade-offs across many objectives.</p></p class="citation"></blockquote><h3 id=2654--65271-improving-adversarial-training-using-vulnerability-aware-perturbation-budget-olukorede-fakorede-et-al-2024>(26/54 | 65/271) Improving Adversarial Training using Vulnerability-Aware Perturbation Budget (Olukorede Fakorede et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Olukorede Fakorede, Modeste Atsague, Jin Tian. (2024)<br><strong>Improving Adversarial Training using Vulnerability-Aware Perturbation Budget</strong><br><button class=copy-to-clipboard title="Improving Adversarial Training using Vulnerability-Aware Perturbation Budget" index=65>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-65 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CR, cs-CV, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Adversarial Learning, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04070v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04070v1.pdf filename=2403.04070v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Adversarial</b> <b>Training</b> (AT) effectively improves the robustness of Deep Neural Networks (DNNs) to <b>adversarial</b> <b>attacks.</b> Generally, AT involves training DNN models with <b>adversarial</b> <b>examples</b> obtained within a pre-defined, fixed perturbation bound. Notably, individual natural examples from which these <b>adversarial</b> <b>examples</b> are crafted exhibit varying degrees of intrinsic vulnerabilities, and as such, crafting <b>adversarial</b> <b>examples</b> with fixed perturbation radius for all instances may not sufficiently unleash the potency of AT. Motivated by this observation, we propose two simple, computationally cheap vulnerability-aware reweighting functions for assigning perturbation bounds to <b>adversarial</b> <b>examples</b> used for AT, named Margin-Weighted Perturbation Budget (MWPB) and Standard-Deviation-Weighted Perturbation Budget (SDWPB). The proposed methods assign perturbation radii to individual <b>adversarial</b> <b>samples</b> based on the vulnerability of their corresponding natural examples. Experimental results show that the proposed methods yield genuine improvements in the robustness of AT algorithms against various <b>adversarial</b> <b>attacks.</b></p></p class="citation"></blockquote><h3 id=2754--66271-guide-guidance-based-incremental-learning-with-diffusion-models-bartosz-cywiński-et-al-2024>(27/54 | 66/271) GUIDE: Guidance-based Incremental Learning with Diffusion Models (Bartosz Cywiński et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bartosz Cywiński, Kamil Deja, Tomasz Trzciński, Bartłomiej Twardowski, Łukasz Kuciński. (2024)<br><strong>GUIDE: Guidance-based Incremental Learning with Diffusion Models</strong><br><button class=copy-to-clipboard title="GUIDE: Guidance-based Incremental Learning with Diffusion Models" index=66>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-66 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Diffusion Model, Continual Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03938v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03938v1.pdf filename=2403.03938v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce GUIDE, a novel <b>continual</b> <b>learning</b> approach that directs <b>diffusion</b> <b>models</b> to rehearse samples at risk of being forgotten. Existing generative strategies combat catastrophic forgetting by randomly sampling rehearsal examples from a generative model. Such an approach contradicts buffer-based approaches where sampling strategy plays an important role. We propose to bridge this gap by integrating <b>diffusion</b> <b>models</b> with classifier guidance techniques to produce rehearsal examples specifically targeting information forgotten by a continuously trained model. This approach enables the generation of samples from preceding task distributions, which are more likely to be misclassified in the context of recently encountered classes. Our experimental results show that GUIDE significantly reduces catastrophic forgetting, outperforming conventional random sampling approaches and surpassing recent state-of-the-art methods in <b>continual</b> <b>learning</b> with generative replay.</p></p class="citation"></blockquote><h3 id=2854--67271-public-data-assisted-private-stochastic-optimization-power-and-limitations-enayat-ullah-et-al-2024>(28/54 | 67/271) Public-data Assisted Private Stochastic Optimization: Power and Limitations (Enayat Ullah et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Enayat Ullah, Michael Menart, Raef Bassily, Cristóbal Guzmán, Raman Arora. (2024)<br><strong>Public-data Assisted Private Stochastic Optimization: Power and Limitations</strong><br><button class=copy-to-clipboard title="Public-data Assisted Private Stochastic Optimization: Power and Limitations" index=67>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-67 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-LG, cs.LG, math-OC, stat-ML<br>Keyword Score: 20<br>Keywords: Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03856v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03856v1.pdf filename=2403.03856v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the limits and capability of public-data assisted differentially private (PA-DP) algorithms. Specifically, we focus on the problem of stochastic convex optimization (SCO) with either labeled or unlabeled public data. For complete/labeled public data, we show that any $(\epsilon,\delta)$-PA-DP has excess risk $\tilde{\Omega}\big(\min\big{\frac{1}{\sqrt{n_{\text{pub}}}},\frac{1}{\sqrt{n}}+\frac{\sqrt{d}}{n\epsilon} \big} \big)$, where $d$ is the dimension, ${n_{\text{pub}}}$ is the number of public samples, ${n_{\text{priv}}}$ is the number of private samples, and $n={n_{\text{pub}}}+{n_{\text{priv}}}$. These lower bounds are established via our new lower bounds for PA-DP mean estimation, which are of a similar form. Up to constant factors, these lower bounds show that the simple strategy of either treating all data as private or discarding the private data, is optimal. We also study PA-DP <b>supervised</b> <b>learning</b> with \textit{unlabeled} public samples. In contrast to our previous result, we here show novel methods for leveraging public data in private <b>supervised</b> <b>learning.</b> For generalized linear models (GLM) with unlabeled public data, we show an efficient algorithm which, given $\tilde{O}({n_{\text{priv}}}\epsilon)$ unlabeled public samples, achieves the dimension independent rate $\tilde{O}\big(\frac{1}{\sqrt{{n_{\text{priv}}}}} + \frac{1}{\sqrt{{n_{\text{priv}}}\epsilon}}\big)$. We develop new lower bounds for this setting which shows that this rate cannot be improved with more public samples, and any fewer public samples leads to a worse rate. Finally, we provide extensions of this result to general hypothesis classes with finite fat-shattering dimension with applications to neural networks and non-Euclidean geometries.</p></p class="citation"></blockquote><h3 id=2954--68271-feature-selection-as-deep-sequential-generative-learning-wangyang-ying-et-al-2024>(29/54 | 68/271) Feature Selection as Deep Sequential Generative Learning (Wangyang Ying et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wangyang Ying, Dongjie Wang, Haifeng Chen, Yanjie Fu. (2024)<br><strong>Feature Selection as Deep Sequential Generative Learning</strong><br><button class=copy-to-clipboard title="Feature Selection as Deep Sequential Generative Learning" index=68>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-68 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Knowledge Distillation, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03838v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03838v1.pdf filename=2403.03838v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Feature selection aims to identify the most pattern-discriminative feature subset. In prior literature, filter (e.g., backward elimination) and embedded (e.g., Lasso) methods have hyperparameters (e.g., top-K, score thresholding) and tie to specific models, thus, hard to generalize; wrapper methods search a feature subset in a huge discrete space and is computationally costly. To transform the way of feature selection, we regard a selected feature subset as a selection decision token sequence and reformulate feature selection as a deep sequential generative learning task that <b>distills</b> feature knowledge and generates decision sequences. Our method includes three steps: (1) We develop a deep variational <b>transformer</b> model over a joint of sequential reconstruction, variational, and performance evaluator losses. Our model can <b>distill</b> feature selection knowledge and learn a continuous embedding space to map feature selection decision sequences into embedding vectors associated with utility scores. (2) We leverage the trained feature subset utility evaluator as a gradient provider to guide the identification of the optimal feature subset embedding;(3) We decode the optimal feature subset embedding to autoregressively generate the best feature selection decision sequence with autostop. Extensive experimental results show this generative perspective is effective and generic, without large discrete search space and expert-specific hyperparameters.</p></p class="citation"></blockquote><h3 id=3054--69271-verified-training-for-counterfactual-explanation-robustness-under-data-shift-anna-p-meyer-et-al-2024>(30/54 | 69/271) Verified Training for Counterfactual Explanation Robustness under Data Shift (Anna P. Meyer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anna P. Meyer, Yuhao Zhang, Aws Albarghouthi, Loris D&rsquo;Antoni. (2024)<br><strong>Verified Training for Counterfactual Explanation Robustness under Data Shift</strong><br><button class=copy-to-clipboard title="Verified Training for Counterfactual Explanation Robustness under Data Shift" index=69>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-69 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Counter-factual, Distribution Shift, Distribution Shift<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03773v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03773v1.pdf filename=2403.03773v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Counterfactual</b> explanations (CEs) enhance the interpretability of machine learning models by describing what changes to an input are necessary to change its prediction to a desired class. These explanations are commonly used to guide users&rsquo; actions, e.g., by describing how a user whose loan application was denied can be approved for a loan in the future. Existing approaches generate CEs by focusing on a single, fixed model, and do not provide any formal guarantees on the CEs&rsquo; future validity. When models are updated periodically to account for data shift, if the generated CEs are not robust to the shifts, users&rsquo; actions may no longer have the desired impacts on their predictions. This paper introduces VeriTraCER, an approach that jointly trains a classifier and an explainer to explicitly consider the robustness of the generated CEs to small model shifts. VeriTraCER optimizes over a carefully designed loss function that ensures the verifiable robustness of CEs to local model updates, thus providing deterministic guarantees to CE validity. Our empirical evaluation demonstrates that VeriTraCER generates CEs that (1) are verifiably robust to small model updates and (2) display competitive robustness to state-of-the-art approaches in handling empirical model updates including random initialization, leave-one-out, and <b>distribution</b> <b>shifts.</b></p></p class="citation"></blockquote><h3 id=3154--70271-diffusion-on-language-model-embeddings-for-protein-sequence-generation-viacheslav-meshchaninov-et-al-2024>(31/54 | 70/271) Diffusion on language model embeddings for protein sequence generation (Viacheslav Meshchaninov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Viacheslav Meshchaninov, Pavel Strashnov, Andrey Shevtsov, Fedor Nikolaev, Nikita Ivanisenko, Olga Kardymon, Dmitry Vetrov. (2024)<br><strong>Diffusion on language model embeddings for protein sequence generation</strong><br><button class=copy-to-clipboard title="Diffusion on language model embeddings for protein sequence generation" index=70>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-70 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, q-bio-BM<br>Keyword Score: 20<br>Keywords: Diffusion Model, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03726v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03726v1.pdf filename=2403.03726v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Protein design requires a deep understanding of the inherent complexities of the protein universe. While many efforts lean towards conditional generation or focus on specific families of proteins, the foundational task of unconditional generation remains underexplored and undervalued. Here, we explore this pivotal domain, introducing DiMA, a model that leverages continuous <b>diffusion</b> <b>on</b> embeddings derived from the protein language model, ESM-2, to generate amino acid sequences. DiMA surpasses leading solutions, including autoregressive <b>transformer-based</b> and discrete <b>diffusion</b> <b>models,</b> and we quantitatively illustrate the impact of the design choices that lead to its superior performance. We extensively evaluate the quality, diversity, distribution similarity, and biological relevance of the generated sequences using multiple metrics across various modalities. Our approach consistently produces novel, diverse protein sequences that accurately reflect the inherent structural and functional diversity of the protein space. This work advances the field of protein design and sets the stage for conditional models by providing a robust framework for scalable and high-quality protein sequence generation.</p></p class="citation"></blockquote><h3 id=3254--71271-learning-adversarial-mdps-with-stochastic-hard-constraints-francesco-emanuele-stradi-et-al-2024>(32/54 | 71/271) Learning Adversarial MDPs with Stochastic Hard Constraints (Francesco Emanuele Stradi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Francesco Emanuele Stradi, Matteo Castiglioni, Alberto Marchesi, Nicola Gatti. (2024)<br><strong>Learning Adversarial MDPs with Stochastic Hard Constraints</strong><br><button class=copy-to-clipboard title="Learning Adversarial MDPs with Stochastic Hard Constraints" index=71>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-71 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Markov Decision Process, Recommender System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03672v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03672v1.pdf filename=2403.03672v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study online learning problems in constrained Markov decision processes (CMDPs) with adversarial losses and stochastic hard constraints. We consider two different scenarios. In the first one, we address general CMDPs, where we design an algorithm that attains sublinear regret and cumulative positive constraints violation. In the second scenario, under the mild assumption that a policy strictly satisfying the constraints exists and is known to the learner, we design an algorithm that achieves sublinear regret while ensuring that the constraints are satisfied at every episode with high probability. To the best of our knowledge, our work is the first to study CMDPs involving both adversarial losses and hard constraints. Indeed, previous works either focus on much weaker soft constraints&ndash;allowing for positive violation to cancel out negative ones&ndash;or are restricted to stochastic losses. Thus, our algorithms can deal with general non-stationary environments subject to requirements much stricter than those manageable with state-of-the-art algorithms. This enables their adoption in a much wider range of real-world applications, ranging from autonomous driving to online advertising and <b>recommender</b> <b>systems.</b></p></p class="citation"></blockquote><h3 id=3354--72271-a-survey-on-applications-of-reinforcement-learning-in-spatial-resource-allocation-di-zhang-et-al-2024>(33/54 | 72/271) A Survey on Applications of Reinforcement Learning in Spatial Resource Allocation (Di Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Di Zhang, Moyang Wang, Joseph Mango, Xiang Li, Xianrui Xu. (2024)<br><strong>A Survey on Applications of Reinforcement Learning in Spatial Resource Allocation</strong><br><button class=copy-to-clipboard title="A Survey on Applications of Reinforcement Learning in Spatial Resource Allocation" index=72>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-72 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Reinforcement Learning, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03643v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03643v2.pdf filename=2403.03643v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The challenge of spatial resource allocation is pervasive across various domains such as transportation, industry, and daily life. As the scale of real-world issues continues to expand and demands for real-time solutions increase, traditional algorithms face significant computational pressures, struggling to achieve optimal efficiency and real-time capabilities. In recent years, with the escalating computational power of computers, the remarkable achievements of <b>reinforcement</b> <b>learning</b> in domains like Go and robotics have demonstrated its robust learning and sequential decision-making capabilities. Given these advancements, there has been a surge in novel methods employing <b>reinforcement</b> <b>learning</b> to tackle spatial resource allocation problems. These methods exhibit advantages such as rapid solution convergence and strong model generalization abilities, offering a new perspective on resolving spatial resource allocation problems. Therefore, this paper aims to <b>summarize</b> and review recent theoretical methods and applied research utilizing <b>reinforcement</b> <b>learning</b> to address spatial resource allocation problems. It provides a summary and comprehensive overview of its fundamental principles, related methodologies, and applied research. Additionally, it highlights several unresolved issues that urgently require attention in this direction for the future.</p></p class="citation"></blockquote><h3 id=3454--73271-probing-the-robustness-of-time-series-forecasting-models-with-counterfacts-håkon-hanisch-kjærnli-et-al-2024>(34/54 | 73/271) Probing the Robustness of Time-series Forecasting Models with CounterfacTS (Håkon Hanisch Kjærnli et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Håkon Hanisch Kjærnli, Lluis Mas-Ribas, Aida Ashrafi, Gleb Sizov, Helge Langseth, Odd Erik Gundersen. (2024)<br><strong>Probing the Robustness of Time-series Forecasting Models with CounterfacTS</strong><br><button class=copy-to-clipboard title="Probing the Robustness of Time-series Forecasting Models with CounterfacTS" index=73>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-73 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Counter-factual, Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03508v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03508v1.pdf filename=2403.03508v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A common issue for machine learning models applied to time-series forecasting is the temporal evolution of the data distributions (i.e., concept drift). Because most of the training data does not reflect such changes, the models present poor performance on the new <b>out-of-distribution</b> scenarios and, therefore, the impact of such events cannot be reliably anticipated ahead of time. We present and publicly release CounterfacTS, a tool to probe the robustness of deep learning models in time-series forecasting tasks via <b>counterfactuals.</b> CounterfacTS has a user-friendly interface that allows the user to visualize, compare and quantify time series data and their forecasts, for a number of datasets and deep learning models. Furthermore, the user can apply various transformations to the time series and explore the resulting changes in the forecasts in an interpretable manner. Through example cases, we illustrate how CounterfacTS can be used to i) identify the main features characterizing and differentiating sets of time series, ii) assess how the model performance depends on these characateristics, and iii) guide transformations of the original time series to create <b>counterfactuals</b> with desired properties for training and increasing the forecasting performance in new regions of the data distribution. We discuss the importance of visualizing and considering the location of the data in a projected feature space to transform time-series and create effective <b>counterfactuals</b> for training the models. Overall, CounterfacTS aids at creating <b>counterfactuals</b> to efficiently explore the impact of hypothetical scenarios not covered by the original data in time-series forecasting tasks.</p></p class="citation"></blockquote><h3 id=3554--74271-advancing-out-of-distribution-detection-through-data-purification-and-dynamic-activation-function-design-yingrui-ji-et-al-2024>(35/54 | 74/271) Advancing Out-of-Distribution Detection through Data Purification and Dynamic Activation Function Design (Yingrui Ji et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yingrui Ji, Yao Zhu, Zhigang Li, Jiansheng Chen, Yunlong Kong, Jingbo Chen. (2024)<br><strong>Advancing Out-of-Distribution Detection through Data Purification and Dynamic Activation Function Design</strong><br><button class=copy-to-clipboard title="Advancing Out-of-Distribution Detection through Data Purification and Dynamic Activation Function Design" index=74>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-74 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Fine-tuning, Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03412v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03412v1.pdf filename=2403.03412v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the dynamic realms of machine learning and deep learning, the robustness and reliability of models are paramount, especially in critical real-world applications. A fundamental challenge in this sphere is managing <b>Out-of-Distribution</b> (OOD) samples, significantly increasing the risks of model misclassification and uncertainty. Our work addresses this challenge by enhancing the detection and management of OOD samples in neural networks. We introduce OOD-R <b>(Out-of-Distribution-Rectified),</b> a meticulously curated collection of open-source datasets with enhanced noise reduction properties. In-Distribution (ID) noise in existing OOD datasets can lead to inaccurate evaluation of detection algorithms. Recognizing this, OOD-R incorporates noise filtering technologies to refine the datasets, ensuring a more accurate and reliable evaluation of OOD detection algorithms. This approach not only improves the overall quality of data but also aids in better distinguishing between OOD and ID samples, resulting in up to a 2.5% improvement in model accuracy and a minimum 3.2% reduction in false positives. Furthermore, we present ActFun, an innovative method that <b>fine-tunes</b> the model&rsquo;s response to diverse inputs, thereby improving the stability of feature extraction and minimizing specificity issues. ActFun addresses the common problem of model overconfidence in OOD detection by strategically reducing the influence of hidden units, which enhances the model&rsquo;s capability to estimate OOD uncertainty more accurately. Implementing ActFun in the OOD-R dataset has led to significant performance enhancements, including an 18.42% increase in AUROC of the GradNorm method and a 16.93% decrease in FPR95 of the Energy method. Overall, our research not only advances the methodologies in OOD detection but also emphasizes the importance of dataset integrity for accurate algorithm evaluation.</p></p class="citation"></blockquote><h3 id=3654--75271-kernel-correlation-dissimilarity-for-multiple-kernel-k-means-clustering-rina-su-et-al-2024>(36/54 | 75/271) Kernel Correlation-Dissimilarity for Multiple Kernel k-Means Clustering (Rina Su et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rina Su, Yu Guo, Caiying Wu, Qiyu Jin, Tieyong Zeng. (2024)<br><strong>Kernel Correlation-Dissimilarity for Multiple Kernel k-Means Clustering</strong><br><button class=copy-to-clipboard title="Kernel Correlation-Dissimilarity for Multiple Kernel k-Means Clustering" index=75>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-75 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 16<br>Keywords: Benchmarking, Clustering, Information Retrieval<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03448v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03448v1.pdf filename=2403.03448v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The main objective of the Multiple Kernel k-Means (MKKM) algorithm is to extract non-linear <b>information</b> <b>and</b> achieve optimal <b>clustering</b> by optimizing base kernel matrices. Current methods enhance <b>information</b> <b>diversity</b> and reduce redundancy by exploiting interdependencies among multiple kernels based on correlations or dissimilarities. Nevertheless, relying solely on a single metric, such as correlation or dissimilarity, to define kernel relationships introduces bias and incomplete characterization. Consequently, this limitation hinders efficient <b>information</b> <b>extraction,</b> ultimately compromising <b>clustering</b> performance. To tackle this challenge, we introduce a novel method that systematically integrates both kernel correlation and dissimilarity. Our approach comprehensively captures kernel relationships, facilitating more efficient classification <b>information</b> <b>extraction</b> and improving <b>clustering</b> performance. By emphasizing the coherence between kernel correlation and dissimilarity, our method offers a more objective and transparent strategy for extracting non-linear <b>information</b> <b>and</b> significantly improving <b>clustering</b> precision, supported by theoretical rationale. We assess the performance of our algorithm on 13 challenging <b>benchmark</b> datasets, demonstrating its superiority over contemporary state-of-the-art MKKM techniques.</p></p class="citation"></blockquote><h3 id=3754--76271-sample-size-planning-for-conditional-counterfactual-mean-estimation-with-a-k-armed-randomized-experiment-gabriel-ruiz-2024>(37/54 | 76/271) Sample size planning for conditional counterfactual mean estimation with a K-armed randomized experiment (Gabriel Ruiz, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gabriel Ruiz. (2024)<br><strong>Sample size planning for conditional counterfactual mean estimation with a K-armed randomized experiment</strong><br><button class=copy-to-clipboard title="Sample size planning for conditional counterfactual mean estimation with a K-armed randomized experiment" index=76>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-76 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ME, stat-ML<br>Keyword Score: 13<br>Keywords: Counter-factual, Sample Size<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04039v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04039v1.pdf filename=2403.04039v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We cover how to determine a sufficiently large <b>sample</b> <b>size</b> for a $K$-armed randomized experiment in order to estimate conditional <b>counterfactual</b> expectations in data-driven subgroups. The sub-groups can be output by any feature space partitioning algorithm, including as defined by binning users having similar predictive scores or as defined by a learned policy tree. After carefully specifying the inference target, a minimum confidence level, and a maximum margin of error, the key is to turn the original goal into a simultaneous inference problem where the recommended <b>sample</b> <b>size</b> to offset an increased possibility of estimation error is directly related to the number of inferences to be conducted. Given a fixed <b>sample</b> <b>size</b> budget, our result allows us to invert the question to one about the feasible number of treatment arms or partition complexity (e.g. number of decision tree leaves). Using policy trees to learn sub-groups, we evaluate our nominal guarantees on a large publicly-available randomized experiment test data set.</p></p class="citation"></blockquote><h3 id=3854--77271-enot-expectile-regularization-for-fast-and-accurate-training-of-neural-optimal-transport-nazar-buzun-et-al-2024>(38/54 | 77/271) ENOT: Expectile Regularization for Fast and Accurate Training of Neural Optimal Transport (Nazar Buzun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nazar Buzun, Maksim Bobrin, Dmitry V. Dylov. (2024)<br><strong>ENOT: Expectile Regularization for Fast and Accurate Training of Neural Optimal Transport</strong><br><button class=copy-to-clipboard title="ENOT: Expectile Regularization for Fast and Accurate Training of Neural Optimal Transport" index=77>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-77 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 13<br>Keywords: Benchmarking, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03777v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03777v1.pdf filename=2403.03777v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a new extension for Neural Optimal Transport (NOT) training procedure, capable of accurately and efficiently estimating optimal transportation plan via specific regularisation on conjugate potentials. The main bottleneck of existing NOT solvers is associated with the procedure of finding a near-exact approximation of the conjugate operator (i.e., the c-transform), which is done either by optimizing over maximin objectives or by the computationally-intensive <b>fine-tuning</b> of the initial approximated prediction. We resolve both issues by proposing a new, theoretically justified loss in the form of expectile regularization that enforces binding conditions on the learning dual potentials. Such a regularization provides the upper bound estimation over the distribution of possible conjugate potentials and makes the learning stable, eliminating the need for additional extensive <b>finetuning.</b> We formally justify the efficiency of our method, called Expectile-Regularised Neural Optimal Transport (ENOT). ENOT outperforms previous state-of-the-art approaches on the Wasserstein-2 <b>benchmark</b> tasks by a large margin (up to a 3-fold improvement in quality and up to a 10-fold improvement in runtime).</p></p class="citation"></blockquote><h3 id=3954--78271-cdc-a-simple-framework-for-complex-data-clustering-zhao-kang-et-al-2024>(39/54 | 78/271) CDC: A Simple Framework for Complex Data Clustering (Zhao Kang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhao Kang, Xuanting Xie, Bingheng Li, Erlin Pan. (2024)<br><strong>CDC: A Simple Framework for Complex Data Clustering</strong><br><button class=copy-to-clipboard title="CDC: A Simple Framework for Complex Data Clustering" index=78>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-78 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 11<br>Keywords: Graph, Clustering, Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03670v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03670v1.pdf filename=2403.03670v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In today&rsquo;s data-driven digital era, the amount as well as complexity, such as multi-view, non-Euclidean, and multi-relational, of the collected data are growing exponentially or even faster. <b>Clustering,</b> which unsupervisely extracts valid knowledge from data, is extremely useful in practice. However, existing methods are independently developed to handle one particular challenge at the expense of the others. In this work, we propose a simple but effective framework for complex data <b>clustering</b> (CDC) that can efficiently process different types of data with linear complexity. We first utilize <b>graph</b> filtering to fuse <b>geometry</b> structure and attribute information. We then reduce the complexity with high-quality anchors that are adaptively learned via a novel similarity-preserving regularizer. We illustrate the cluster-ability of our proposed method theoretically and experimentally. In particular, we deploy CDC to <b>graph</b> data of size 111M.</p></p class="citation"></blockquote><h3 id=4054--79271-robust-graph-structure-learning-under-heterophily-xuanting-xie-et-al-2024>(40/54 | 79/271) Robust Graph Structure Learning under Heterophily (Xuanting Xie et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xuanting Xie, Zhao Kang, Wenyu Chen. (2024)<br><strong>Robust Graph Structure Learning under Heterophily</strong><br><button class=copy-to-clipboard title="Robust Graph Structure Learning under Heterophily" index=79>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-79 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 11<br>Keywords: Graph, Clustering, Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03659v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03659v1.pdf filename=2403.03659v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> is a fundamental mathematical structure in characterizing relations between different objects and has been widely used on various learning tasks. Most methods implicitly assume a given <b>graph</b> to be accurate and complete. However, real data is inevitably noisy and sparse, which will lead to inferior results. Despite the remarkable success of recent <b>graph</b> <b>representation</b> <b>learning</b> methods, they inherently presume that the <b>graph</b> is homophilic, and largely overlook heterophily, where most connected nodes are from different classes. In this regard, we propose a novel robust <b>graph</b> structure learning method to achieve a high-quality <b>graph</b> from heterophilic data for downstream tasks. We first apply a high-pass filter to make each node more distinctive from its neighbors by encoding structure information into the node features. Then, we learn a robust <b>graph</b> with an adaptive norm characterizing different levels of noise. Afterwards, we propose a novel regularizer to further refine the <b>graph</b> structure. <b>Clustering</b> and semi-supervised classification experiments on heterophilic <b>graphs</b> verify the effectiveness of our method.</p></p class="citation"></blockquote><h3 id=4154--80271-automated-multi-task-learning-for-joint-disease-prediction-on-electronic-health-records-suhan-cui-et-al-2024>(41/54 | 80/271) Automated Multi-Task Learning for Joint Disease Prediction on Electronic Health Records (Suhan Cui et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Suhan Cui, Prasenjit Mitra. (2024)<br><strong>Automated Multi-Task Learning for Joint Disease Prediction on Electronic Health Records</strong><br><button class=copy-to-clipboard title="Automated Multi-Task Learning for Joint Disease Prediction on Electronic Health Records" index=80>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-80 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Human Intervention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04086v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04086v1.pdf filename=2403.04086v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the realm of big data and digital healthcare, Electronic Health Records (EHR) have become a rich source of information with the potential to improve patient care and medical research. In recent years, machine learning models have proliferated for analyzing EHR data to predict patients future health conditions. Among them, some studies advocate for multi-task learning (MTL) to jointly predict multiple target diseases for improving the prediction performance over single task learning. Nevertheless, current MTL frameworks for EHR data have significant limitations due to their heavy reliance on <b>human</b> <b>experts</b> to identify task groups for joint training and design model architectures. To reduce <b>human</b> <b>intervention</b> and improve the framework design, we propose an automated approach named AutoDP, which can search for the optimal configuration of task grouping and architectures simultaneously. To tackle the vast joint search space encompassing task combinations and architectures, we employ surrogate model-based optimization, enabling us to efficiently discover the optimal solution. Experimental results on real-world EHR data demonstrate the efficacy of the proposed AutoDP framework. It achieves significant performance improvements over both hand-crafted and automated state-of-the-art methods, also maintains a feasible search cost at the same time.</p></p class="citation"></blockquote><h3 id=4254--81271-directional-smoothness-and-gradient-methods-convergence-and-adaptivity-aaron-mishkin-et-al-2024>(42/54 | 81/271) Directional Smoothness and Gradient Methods: Convergence and Adaptivity (Aaron Mishkin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aaron Mishkin, Ahmed Khaled, Yuanhao Wang, Aaron Defazio, Robert M. Gower. (2024)<br><strong>Directional Smoothness and Gradient Methods: Convergence and Adaptivity</strong><br><button class=copy-to-clipboard title="Directional Smoothness and Gradient Methods: Convergence and Adaptivity" index=81>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-81 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-OC<br>Keyword Score: 10<br>Keywords: Logistic Regression<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04081v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04081v1.pdf filename=2403.04081v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We develop new sub-optimality bounds for gradient descent (GD) that depend on the conditioning of the objective along the path of optimization, rather than on global, worst-case constants. Key to our proofs is directional smoothness, a measure of gradient variation that we use to develop upper-bounds on the objective. Minimizing these upper-bounds requires solving implicit equations to obtain a sequence of strongly adapted step-sizes; we show that these equations are straightforward to solve for convex quadratics and lead to new guarantees for two classical step-sizes. For general functions, we prove that the Polyak step-size and normalized GD obtain fast, path-dependent rates despite using no knowledge of the directional smoothness. Experiments on <b>logistic</b> <b>regression</b> show our convergence guarantees are tighter than the classical theory based on L-smoothness.</p></p class="citation"></blockquote><h3 id=4354--82271-belief-enriched-pessimistic-q-learning-against-adversarial-state-perturbations-xiaolin-sun-et-al-2024>(43/54 | 82/271) Belief-Enriched Pessimistic Q-Learning against Adversarial State Perturbations (Xiaolin Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaolin Sun, Zizhan Zheng. (2024)<br><strong>Belief-Enriched Pessimistic Q-Learning against Adversarial State Perturbations</strong><br><button class=copy-to-clipboard title="Belief-Enriched Pessimistic Q-Learning against Adversarial State Perturbations" index=82>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-82 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04050v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04050v1.pdf filename=2403.04050v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Reinforcement</b> <b>learning</b> (RL) has achieved phenomenal success in various domains. However, its data-driven nature also introduces new vulnerabilities that can be exploited by malicious opponents. Recent work shows that a well-trained RL agent can be easily manipulated by strategically perturbing its state observations at the test stage. Existing solutions either introduce a regularization term to improve the smoothness of the trained policy against perturbations or alternatively train the agent&rsquo;s policy and the attacker&rsquo;s policy. However, the former does not provide sufficient protection against strong attacks, while the latter is computationally prohibitive for large environments. In this work, we propose a new robust RL algorithm for deriving a pessimistic policy to safeguard against an agent&rsquo;s uncertainty about true states. This approach is further enhanced with belief state inference and diffusion-based state purification to reduce uncertainty. Empirical results show that our approach obtains superb performance under strong attacks and has a comparable training overhead with regularization-based methods. Our code is available at <a href=https://github.com/SliencerX/Belief-enriched-robust-Q-learning>https://github.com/SliencerX/Belief-enriched-robust-Q-learning</a>.</p></p class="citation"></blockquote><h3 id=4454--83271-ocd-fl-a-novel-communication-efficient-peer-selection-based-decentralized-federated-learning-nizar-masmoudi-et-al-2024>(44/54 | 83/271) OCD-FL: A Novel Communication-Efficient Peer Selection-based Decentralized Federated Learning (Nizar Masmoudi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nizar Masmoudi, Wael Jaafar. (2024)<br><strong>OCD-FL: A Novel Communication-Efficient Peer Selection-based Decentralized Federated Learning</strong><br><button class=copy-to-clipboard title="OCD-FL: A Novel Communication-Efficient Peer Selection-based Decentralized Federated Learning" index=83>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-83 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-DC, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04037v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04037v1.pdf filename=2403.04037v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The conjunction of edge intelligence and the ever-growing Internet-of-Things (IoT) network heralds a new era of collaborative machine learning, with <b>federated</b> <b>learning</b> (FL) emerging as the most prominent paradigm. With the growing interest in these learning schemes, researchers started addressing some of their most fundamental limitations. Indeed, conventional FL with a central aggregator presents a single point of failure and a network bottleneck. To bypass this issue, decentralized FL where nodes collaborate in a peer-to-peer network has been proposed. Despite the latter&rsquo;s efficiency, communication costs and data heterogeneity remain key challenges in decentralized FL. In this context, we propose a novel scheme, called opportunistic communication-efficient decentralized <b>federated</b> <b>learning,</b> a.k.a., OCD-FL, consisting of a systematic FL peer selection for collaboration, aiming to achieve maximum FL knowledge gain while reducing energy consumption. Experimental results demonstrate the capability of OCD-FL to achieve similar or better performances than the fully collaborative FL, while significantly reducing consumed energy by at least 30% and up to 80%.</p></p class="citation"></blockquote><h3 id=4554--84271-spearexact-gradient-inversion-of-batches-in-federated-learning-dimitar-i-dimitrov-et-al-2024>(45/54 | 84/271) SPEAR:Exact Gradient Inversion of Batches in Federated Learning (Dimitar I. Dimitrov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dimitar I. Dimitrov, Maximilian Baader, Mark Niklas Müller, Martin Vechev. (2024)<br><strong>SPEAR:Exact Gradient Inversion of Batches in Federated Learning</strong><br><button class=copy-to-clipboard title="SPEAR:Exact Gradient Inversion of Batches in Federated Learning" index=84>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-84 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: I-2-11, cs-CR, cs-DC, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03945v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03945v1.pdf filename=2403.03945v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Federated</b> <b>learning</b> is a popular framework for collaborative machine learning where multiple clients only share gradient updates on their local data with the server and not the actual data. Unfortunately, it was recently shown that gradient inversion attacks can reconstruct this data from these shared gradients. Existing attacks enable exact reconstruction only for a batch size of $b=1$ in the important honest-but-curious setting, with larger batches permitting only approximate reconstruction. In this work, we propose \emph{the first algorithm reconstructing whole batches with $b >1$ exactly}. This approach combines mathematical insights into the explicit low-rank structure of gradients with a sampling-based algorithm. Crucially, we leverage ReLU-induced gradient sparsity to precisely filter out large numbers of incorrect samples, making a final reconstruction step tractable. We provide an efficient GPU implementation for fully connected networks and show that it recovers batches of $b \lesssim 25$ elements exactly while being tractable for large network widths and depths.</p></p class="citation"></blockquote><h3 id=4654--85271-extreme-precipitation-nowcasting-using-transformer-based-generative-models-cristian-meo-et-al-2024>(46/54 | 85/271) Extreme Precipitation Nowcasting using Transformer-based Generative Models (Cristian Meo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Cristian Meo, Ankush Roy, Mircea Lică, Junzhe Yin, Zeineb Bou Che, Yanbo Wang, Ruben Imhoff, Remko Uijlenhoet, Justin Dauwels. (2024)<br><strong>Extreme Precipitation Nowcasting using Transformer-based Generative Models</strong><br><button class=copy-to-clipboard title="Extreme Precipitation Nowcasting using Transformer-based Generative Models" index=85>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-85 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03929v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03929v1.pdf filename=2403.03929v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents an innovative approach to extreme precipitation nowcasting by employing <b>Transformer-based</b> generative models, namely NowcastingGPT with Extreme Value Loss (EVL) regularization. Leveraging a comprehensive dataset from the Royal Netherlands Meteorological Institute (KNMI), our study focuses on predicting short-term precipitation with high accuracy. We introduce a novel method for computing EVL without assuming fixed extreme representations, addressing the limitations of current models in capturing extreme weather events. We present both qualitative and quantitative analyses, demonstrating the superior performance of the proposed NowcastingGPT-EVL in generating accurate precipitation forecasts, especially when dealing with extreme precipitation events. The code is available at \url{https://github.com/Cmeo97/NowcastingGPT}.</p></p class="citation"></blockquote><h3 id=4754--86271-decoupled-vertical-federated-learning-for-practical-training-on-vertically-partitioned-data-avi-amalanshu-et-al-2024>(47/54 | 86/271) Decoupled Vertical Federated Learning for Practical Training on Vertically Partitioned Data (Avi Amalanshu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Avi Amalanshu, Yash Sirvi, David I. Inouye. (2024)<br><strong>Decoupled Vertical Federated Learning for Practical Training on Vertically Partitioned Data</strong><br><button class=copy-to-clipboard title="Decoupled Vertical Federated Learning for Practical Training on Vertically Partitioned Data" index=86>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-86 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-DC, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03871v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03871v1.pdf filename=2403.03871v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Vertical <b>Federated</b> <b>Learning</b> (VFL) is an emergent distributed machine learning paradigm wherein owners of disjoint features of a common set of entities collaborate to learn a global model without sharing data. In VFL, a host client owns data labels for each entity and learns a final representation based on intermediate local representations from all guest clients. Therefore, the host is a single point of failure and label feedback can be used by malicious guest clients to infer private features. Requiring all participants to remain active and trustworthy throughout the entire training process is generally impractical and altogether infeasible outside of controlled environments. We propose Decoupled VFL (DVFL), a blockwise learning approach to VFL. By training each model on its own objective, DVFL allows for decentralized aggregation and isolation between feature learning and label supervision. With these properties, DVFL is fault tolerant and secure. We implement DVFL to train split neural networks and show that model performance is comparable to VFL on a variety of classification datasets.</p></p class="citation"></blockquote><h3 id=4854--87271-accelerating-convergence-of-score-based-diffusion-models-provably-gen-li-et-al-2024>(48/54 | 87/271) Accelerating Convergence of Score-Based Diffusion Models, Provably (Gen Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gen Li, Yu Huang, Timofey Efimov, Yuting Wei, Yuejie Chi, Yuxin Chen. (2024)<br><strong>Accelerating Convergence of Score-Based Diffusion Models, Provably</strong><br><button class=copy-to-clipboard title="Accelerating Convergence of Score-Based Diffusion Models, Provably" index=87>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-87 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-IT, cs-LG, cs.LG, math-IT, math-OC, stat-ML<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03852v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03852v1.pdf filename=2403.03852v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Score-based <b>diffusion</b> <b>models,</b> while achieving remarkable empirical performance, often suffer from low sampling speed, due to extensive function evaluations needed during the sampling phase. Despite a flurry of recent activities towards speeding up <b>diffusion</b> <b>generative</b> modeling in practice, theoretical underpinnings for acceleration techniques remain severely limited. In this paper, we design novel training-free algorithms to accelerate popular deterministic (i.e., DDIM) and stochastic (i.e., DDPM) samplers. Our accelerated deterministic sampler converges at a rate $O(1/{T}^2)$ with $T$ the number of steps, improving upon the $O(1/T)$ rate for the DDIM sampler; and our accelerated stochastic sampler converges at a rate $O(1/T)$, outperforming the rate $O(1/\sqrt{T})$ for the DDPM sampler. The design of our algorithms leverages insights from higher-order approximation, and shares similar intuitions as popular high-order ODE solvers like the DPM-Solver-2. Our theory accommodates $\ell_2$-accurate score estimates, and does not require log-concavity or smoothness on the target distribution.</p></p class="citation"></blockquote><h3 id=4954--88271-effect-of-ambient-intrinsic-dimension-gap-on-adversarial-vulnerability-rajdeep-haldar-et-al-2024>(49/54 | 88/271) Effect of Ambient-Intrinsic Dimension Gap on Adversarial Vulnerability (Rajdeep Haldar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rajdeep Haldar, Yue Xing, Qifan Song. (2024)<br><strong>Effect of Ambient-Intrinsic Dimension Gap on Adversarial Vulnerability</strong><br><button class=copy-to-clipboard title="Effect of Ambient-Intrinsic Dimension Gap on Adversarial Vulnerability" index=88>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-88 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03967v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03967v1.pdf filename=2403.03967v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The existence of <b>adversarial</b> <b>attacks</b> on machine learning models imperceptible to a human is still quite a mystery from a theoretical perspective. In this work, we introduce two notions of <b>adversarial</b> <b>attacks:</b> natural or on-manifold attacks, which are perceptible by a human/oracle, and unnatural or off-manifold attacks, which are not. We argue that the existence of the off-manifold attacks is a natural consequence of the dimension gap between the intrinsic and ambient dimensions of the data. For 2-layer ReLU networks, we prove that even though the dimension gap does not affect generalization performance on samples drawn from the observed data space, it makes the clean-trained model more vulnerable to <b>adversarial</b> <b>perturbations</b> in the off-manifold direction of the data space. Our main results provide an explicit relationship between the $\ell_2,\ell_{\infty}$ attack strength of the on/off-manifold attack and the dimension gap.</p></p class="citation"></blockquote><h3 id=5054--89271-supclust-active-learning-at-the-boundaries-yuta-ono-et-al-2024>(50/54 | 89/271) SUPClust: Active Learning at the Boundaries (Yuta Ono et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuta Ono, Till Aczel, Benjamin Estermann, Roger Wattenhofer. (2024)<br><strong>SUPClust: Active Learning at the Boundaries</strong><br><button class=copy-to-clipboard title="SUPClust: Active Learning at the Boundaries" index=89>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-89 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Active Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03741v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03741v1.pdf filename=2403.03741v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Active</b> <b>learning</b> is a machine learning paradigm designed to optimize model performance in a setting where labeled data is expensive to acquire. In this work, we propose a novel <b>active</b> <b>learning</b> method called SUPClust that seeks to identify points at the decision boundary between classes. By targeting these points, SUPClust aims to gather information that is most informative for refining the model&rsquo;s prediction of complex decision regions. We demonstrate experimentally that labeling these points leads to strong model performance. This improvement is observed even in scenarios characterized by strong class imbalance.</p></p class="citation"></blockquote><h3 id=5154--90271-ab-bnn-addbit-operation-only-hardware-friendly-binary-neural-network-ruichen-ma-et-al-2024>(51/54 | 90/271) A&amp;B BNN: Add&amp;Bit-Operation-Only Hardware-Friendly Binary Neural Network (Ruichen Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruichen Ma, Guanchao Qiao, Yian Liu, Liwei Meng, Ning Ning, Yang Liu, Shaogang Hu. (2024)<br><strong>A&amp;B BNN: Add&amp;Bit-Operation-Only Hardware-Friendly Binary Neural Network</strong><br><button class=copy-to-clipboard title="A&B BNN: Add&Bit-Operation-Only Hardware-Friendly Binary Neural Network" index=90>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-90 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03739v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03739v1.pdf filename=2403.03739v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Binary neural networks utilize 1-bit <b>quantized</b> weights and activations to reduce both the model&rsquo;s storage demands and computational burden. However, advanced binary architectures still incorporate millions of inefficient and nonhardware-friendly full-precision multiplication operations. A&amp;B BNN is proposed to directly remove part of the multiplication operations in a traditional BNN and replace the rest with an equal number of bit operations, introducing the mask layer and the <b>quantized</b> RPReLU structure based on the normalizer-free network architecture. The mask layer can be removed during inference by leveraging the intrinsic characteristics of BNN with straightforward mathematical transformations to avoid the associated multiplication operations. The <b>quantized</b> RPReLU structure enables more efficient bit operations by constraining its slope to be integer powers of 2. Experimental results achieved 92.30%, 69.35%, and 66.89% on the CIFAR-10, CIFAR-100, and ImageNet datasets, respectively, which are competitive with the state-of-the-art. Ablation studies have verified the efficacy of the <b>quantized</b> RPReLU structure, leading to a 1.14% enhancement on the ImageNet compared to using a fixed slope RLeakyReLU. The proposed add&amp;bit-operation-only BNN offers an innovative approach for hardware-friendly network architecture.</p></p class="citation"></blockquote><h3 id=5254--91271-restricted-bayesian-neural-network-sourav-ganguly-2024>(52/54 | 91/271) Restricted Bayesian Neural Network (Sourav Ganguly, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sourav Ganguly. (2024)<br><strong>Restricted Bayesian Neural Network</strong><br><button class=copy-to-clipboard title="Restricted Bayesian Neural Network" index=91>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-91 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-NE, cs.LG<br>Keyword Score: 5<br>Keywords: Black Box<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04810v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04810v1.pdf filename=2403.04810v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Modern deep learning tools are remarkably effective in addressing intricate problems. However, their operation as <b>black-box</b> <b>models</b> introduces increased uncertainty in predictions. Additionally, they contend with various challenges, including the need for substantial storage space in large networks, issues of overfitting, underfitting, vanishing gradients, and more. This study explores the concept of Bayesian Neural Networks, presenting a novel architecture designed to significantly alleviate the storage space complexity of a network. Furthermore, we introduce an algorithm adept at efficiently handling uncertainties, ensuring robust convergence values without becoming trapped in local optima, particularly when the objective function lacks perfect convexity.</p></p class="citation"></blockquote><h3 id=5354--92271-acceleratedlingam-learning-causal-dags-at-the-speed-of-gpus-victor-akinwande-et-al-2024>(53/54 | 92/271) AcceleratedLiNGAM: Learning Causal DAGs at the speed of GPUs (Victor Akinwande et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Victor Akinwande, J. Zico Kolter. (2024)<br><strong>AcceleratedLiNGAM: Learning Causal DAGs at the speed of GPUs</strong><br><button class=copy-to-clipboard title="AcceleratedLiNGAM: Learning Causal DAGs at the speed of GPUs" index=92>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-92 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-DC, cs-LG, cs.LG, stat-ML<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03772v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03772v1.pdf filename=2403.03772v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing causal discovery methods based on combinatorial optimization or search are slow, prohibiting their application on large-scale datasets. In response, more recent methods attempt to address this limitation by formulating causal discovery as structure learning with continuous optimization but such approaches thus far provide no statistical guarantees. In this paper, we show that by efficiently parallelizing existing causal discovery methods, we can in fact scale them to thousands of dimensions, making them practical for substantially larger-scale problems. In particular, we parallelize the LiNGAM method, which is quadratic in the number of variables, obtaining up to a 32-fold speed-up on <b>benchmark</b> datasets when compared with existing sequential implementations. Specifically, we focus on the causal ordering subprocedure in DirectLiNGAM and implement GPU kernels to accelerate it. This allows us to apply DirectLiNGAM to causal inference on large-scale gene expression data with genetic interventions yielding competitive results compared with specialized continuous optimization methods, and Var-LiNGAM for causal discovery on U.S. stock data.</p></p class="citation"></blockquote><h3 id=5454--93271-uncertainty-quantification-for-deeponets-with-ensemble-kalman-inversion-andrew-pensoneault-et-al-2024>(54/54 | 93/271) Uncertainty quantification for deeponets with ensemble kalman inversion (Andrew Pensoneault et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andrew Pensoneault, Xueyu Zhu. (2024)<br><strong>Uncertainty quantification for deeponets with ensemble kalman inversion</strong><br><button class=copy-to-clipboard title="Uncertainty quantification for deeponets with ensemble kalman inversion" index=93>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-93 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: 65, cs-AI, cs-LG, cs-NA, cs.LG, math-NA, stat-ML<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03444v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03444v1.pdf filename=2403.03444v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, operator learning, particularly the DeepONet, has received much attention for efficiently learning complex mappings between input and output functions across diverse fields. However, in practical scenarios with limited and noisy data, accessing the uncertainty in DeepONet predictions becomes essential, especially in mission-critical or safety-critical applications. Existing methods, either computationally intensive or yielding unsatisfactory uncertainty quantification, leave room for developing efficient and informative uncertainty quantification (UQ) techniques tailored for DeepONets. In this work, we proposed a novel inference approach for efficient UQ for operator learning by harnessing the power of the Ensemble Kalman Inversion (EKI) approach. EKI, known for its derivative-free, noise-robust, and highly parallelizable feature, has demonstrated its advantages for UQ for physics-informed neural networks [28]. Our innovative application of EKI enables us to efficiently train ensembles of DeepONets while obtaining informative uncertainty estimates for the output of interest. We deploy a mini-batch variant of EKI to accommodate larger datasets, mitigating the computational demand due to large datasets during the training stage. Furthermore, we introduce a heuristic method to estimate the artificial dynamics covariance, thereby improving our uncertainty estimates. Finally, we demonstrate the effectiveness and versatility of our proposed methodology across various <b>benchmark</b> problems, showcasing its potential to address the pressing challenges of uncertainty quantification in DeepONets, especially for practical applications with limited and noisy data.</p></p class="citation"></blockquote><h2 id=csai-18>cs.AI (18)</h2><h3 id=118--94271-emotional-manipulation-through-prompt-engineering-amplifies-disinformation-generation-in-ai-large-language-models-rasita-vinay-et-al-2024>(1/18 | 94/271) Emotional Manipulation Through Prompt Engineering Amplifies Disinformation Generation in AI Large Language Models (Rasita Vinay et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rasita Vinay, Giovanni Spitale, Nikola Biller-Andorno, Federico Germani. (2024)<br><strong>Emotional Manipulation Through Prompt Engineering Amplifies Disinformation Generation in AI Large Language Models</strong><br><button class=copy-to-clipboard title="Emotional Manipulation Through Prompt Engineering Amplifies Disinformation Generation in AI Large Language Models" index=94>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-94 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CY, cs-HC, cs.AI<br>Keyword Score: 80<br>Keywords: GPT, GPT-3, GPT-3.5, GPT-4, Text Generation, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03550v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03550v1.pdf filename=2403.03550v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study investigates the generation of synthetic disinformation by OpenAI&rsquo;s <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> through <b>prompt</b> engineering and explores their responsiveness to emotional <b>prompting.</b> Leveraging various <b>LLM</b> iterations using davinci-002, davinci-003, <b>gpt-3.5-turbo</b> and <b>gpt-4,</b> we designed experiments to assess their success in producing disinformation. Our findings, based on a corpus of 19,800 synthetic disinformation social media posts, reveal that all <b>LLMs</b> by OpenAI can successfully produce disinformation, and that they effectively respond to emotional <b>prompting,</b> indicating their nuanced understanding of emotional cues in <b>text</b> <b>generation.</b> When <b>prompted</b> politely, all examined <b>LLMs</b> consistently generate disinformation at a high frequency. Conversely, when <b>prompted</b> impolitely, the frequency of disinformation production diminishes, as the models often refuse to generate disinformation and instead caution users that the tool is not intended for such purposes. This research contributes to the ongoing discourse surrounding responsible development and application of AI technologies, particularly in mitigating the spread of disinformation and promoting transparency in AI-generated content.</p></p class="citation"></blockquote><h3 id=218--95271-guiding-enumerative-program-synthesis-with-large-language-models-yixuan-li-et-al-2024>(2/18 | 95/271) Guiding Enumerative Program Synthesis with Large Language Models (Yixuan Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yixuan Li, Julian Parsert, Elizabeth Polgreen. (2024)<br><strong>Guiding Enumerative Program Synthesis with Large Language Models</strong><br><button class=copy-to-clipboard title="Guiding Enumerative Program Synthesis with Large Language Models" index=95>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-95 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 73<br>Keywords: Benchmarking, GPT, GPT-3, GPT-3.5, Code Generation, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03997v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03997v1.pdf filename=2403.03997v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Pre-trained <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> are beginning to dominate the discourse around automatic <b>code</b> <b>generation</b> with natural language specifications. In contrast, the best-performing synthesizers in the domain of formal synthesis with precise logical specifications are still based on enumerative algorithms. In this paper, we evaluate the abilities of <b>LLMs</b> to solve formal synthesis <b>benchmarks</b> by carefully crafting a library of <b>prompts</b> for the domain. When one-shot synthesis fails, we propose a novel enumerative synthesis algorithm, which integrates calls to an <b>LLM</b> into a weighted probabilistic search. This allows the synthesizer to provide the <b>LLM</b> with information about the progress of the enumerator, and the <b>LLM</b> to provide the enumerator with syntactic guidance in an iterative loop. We evaluate our techniques on <b>benchmarks</b> from the Syntax-Guided Synthesis (SyGuS) competition. We find that <b>GPT-3.5</b> as a stand-alone tool for formal synthesis is easily outperformed by state-of-the-art formal synthesis algorithms, but our approach integrating the <b>LLM</b> into an enumerative synthesis algorithm shows significant performance gains over both the <b>LLM</b> and the enumerative synthesizer alone and the winning SyGuS competition tool.</p></p class="citation"></blockquote><h3 id=318--96271-k-link-knowledge-link-graph-from-llms-for-enhanced-representation-learning-in-multivariate-time-series-data-yucheng-wang-et-al-2024>(3/18 | 96/271) K-Link: Knowledge-Link Graph from LLMs for Enhanced Representation Learning in Multivariate Time-Series Data (Yucheng Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yucheng Wang, Ruibing Jin, Min Wu, Xiaoli Li, Lihua Xie, Zhenghua Chen. (2024)<br><strong>K-Link: Knowledge-Link Graph from LLMs for Enhanced Representation Learning in Multivariate Time-Series Data</strong><br><button class=copy-to-clipboard title="K-Link: Knowledge-Link Graph from LLMs for Enhanced Representation Learning in Multivariate Time-Series Data" index=96>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-96 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 58<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Representation Learning, Neural Machine Translation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03645v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03645v1.pdf filename=2403.03645v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Sourced from various sensors and organized chronologically, Multivariate Time-Series <b>(MTS)</b> data involves crucial spatial-temporal dependencies, e.g., correlations among sensors. To capture these dependencies, <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> have emerged as powerful tools, yet their effectiveness is restricted by the quality of <b>graph</b> <b>construction</b> <b>from</b> <b>MTS</b> data. Typically, existing approaches construct <b>graphs</b> <b>solely</b> <b>from</b> <b>MTS</b> signals, which may introduce bias due to a small training dataset and may not accurately represent underlying dependencies. To address this challenge, we propose a novel framework named K-Link, leveraging <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> to encode extensive general knowledge and thereby providing effective solutions to reduce the bias. Leveraging the knowledge embedded in <b>LLMs,</b> such as physical principles, we extract a \textit{Knowledge-Link graph}, capturing vast semantic knowledge of sensors and the linkage of the sensor-level knowledge. To harness the potential of the knowledge-link <b>graph</b> <b>in</b> <b>enhancing</b> the <b>graph</b> <b>derived</b> <b>from</b> <b>MTS</b> data, we propose a <b>graph</b> <b>alignment</b> <b>module,</b> facilitating the transfer of semantic knowledge within the knowledge-link <b>graph</b> <b>into</b> <b>the</b> <b>MTS-derived</b> <b>graph.</b> <b>By</b> <b>doing</b> so, we can improve the <b>graph</b> <b>quality,</b> <b>ensuring</b> effective <b>representation</b> <b>learning</b> with <b>GNNs</b> for <b>MTS</b> data. Extensive experiments demonstrate the efficacy of our approach for superior performance across various <b>MTS-related</b> downstream tasks.</p></p class="citation"></blockquote><h3 id=418--97271-a-privacy-preserving-framework-with-multi-modal-data-for-cross-domain-recommendation-li-wang-et-al-2024>(4/18 | 97/271) A Privacy-Preserving Framework with Multi-Modal Data for Cross-Domain Recommendation (Li Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Li Wang, Lei Sang, Quangui Zhang, Qiang Wu, Min Xu. (2024)<br><strong>A Privacy-Preserving Framework with Multi-Modal Data for Cross-Domain Recommendation</strong><br><button class=copy-to-clipboard title="A Privacy-Preserving Framework with Multi-Modal Data for Cross-Domain Recommendation" index=97>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-97 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 43<br>Keywords: Contrastive Learning, Knowledge Transfer, Multi-modal, Recommendation, Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03600v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03600v1.pdf filename=2403.03600v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Cross-domain <b>recommendation</b> (CDR) aims to enhance <b>recommendation</b> accuracy in a target domain with sparse data by leveraging rich information in a source domain, thereby addressing the data-sparsity problem. Some existing CDR methods highlight the advantages of extracting domain-common and domain-specific features to learn comprehensive user and item representations. However, these methods can&rsquo;t effectively disentangle these components as they often rely on simple user-item historical interaction information (such as ratings, clicks, and browsing), neglecting the rich <b>multi-modal</b> features. Additionally, they don&rsquo;t protect user-sensitive data from potential leakage during <b>knowledge</b> <b>transfer</b> between domains. To address these challenges, we propose a Privacy-Preserving Framework with <b>Multi-Modal</b> Data for Cross-Domain <b>Recommendation,</b> called P2M2-CDR. Specifically, we first design a <b>multi-modal</b> disentangled encoder that utilizes <b>multi-modal</b> information to disentangle more informative domain-common and domain-specific embeddings. Furthermore, we introduce a privacy-preserving decoder to mitigate user privacy leakage during <b>knowledge</b> <b>transfer.</b> Local <b>differential</b> <b>privacy</b> (LDP) is utilized to obfuscate the disentangled embeddings before inter-domain exchange, thereby enhancing privacy protection. To ensure both consistency and differentiation among these obfuscated disentangled embeddings, we incorporate <b>contrastive</b> <b>learning-based</b> domain-inter and domain-intra losses. Extensive Experiments conducted on four real-world datasets demonstrate that P2M2-CDR outperforms other state-of-the-art single-domain and cross-domain baselines.</p></p class="citation"></blockquote><h3 id=518--98271-ircoder-intermediate-representations-make-language-models-robust-multilingual-code-generators-indraneil-paul-et-al-2024>(5/18 | 98/271) IRCoder: Intermediate Representations Make Language Models Robust Multilingual Code Generators (Indraneil Paul et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Indraneil Paul, Jun Luo, Goran Glavaš, Iryna Gurevych. (2024)<br><strong>IRCoder: Intermediate Representations Make Language Models Robust Multilingual Code Generators</strong><br><button class=copy-to-clipboard title="IRCoder: Intermediate Representations Make Language Models Robust Multilingual Code Generators" index=98>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-98 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs-PL, cs.AI<br>Keyword Score: 40<br>Keywords: Data Augmentation, Code Generation, Instruction Following, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03894v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03894v1.pdf filename=2403.03894v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Code</b> <b>understanding</b> and generation have fast become some of the most popular applications of language models (LMs). Nonetheless, research on multilingual aspects of <b>Code-LMs</b> <b>(i.e.,</b> LMs for <b>code</b> <b>generation)</b> such as cross-lingual transfer between different programming languages, language-specific <b>data</b> <b>augmentation,</b> and post-hoc LM adaptation, alongside exploitation of <b>data</b> <b>sources</b> other than the original textual content, has been much sparser than for their natural language counterparts. In particular, most mainstream <b>Code-LMs</b> <b>have</b> been pre-trained on source <b>code</b> <b>files</b> alone. In this work, we investigate the prospect of leveraging readily available compiler intermediate representations - shared across programming languages - to improve the multilingual capabilities of <b>Code-LMs</b> <b>and</b> facilitate cross-lingual transfer. To this end, we first compile SLTrans, a parallel dataset consisting of nearly 4M self-contained source <b>code</b> <b>files</b> coupled with respective intermediate representations. Next, starting from various base <b>Code-LMs</b> <b>(ranging</b> in size from 1.1B to 7.3B parameters), we carry out continued causal language modelling training on SLTrans, forcing the <b>Code-LMs</b> <b>to</b> (1) learn the IR language and (2) align the IR constructs with respective constructs of various programming languages. Our resulting models, dubbed IRCoder, display sizeable and consistent gains across a wide variety of <b>code</b> <b>generation</b> tasks and metrics, including <b>prompt</b> robustness, multilingual <b>code</b> <b>completion,</b> <b>code</b> <b>understanding,</b> and <b>instruction</b> <b>following.</b></p></p class="citation"></blockquote><h3 id=618--99271-assessing-the-aesthetic-evaluation-capabilities-of-gpt-4-with-vision-insights-from-group-and-individual-assessments-yoshia-abe-et-al-2024>(6/18 | 99/271) Assessing the Aesthetic Evaluation Capabilities of GPT-4 with Vision: Insights from Group and Individual Assessments (Yoshia Abe et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yoshia Abe, Tatsuya Daikoku, Yasuo Kuniyoshi. (2024)<br><strong>Assessing the Aesthetic Evaluation Capabilities of GPT-4 with Vision: Insights from Group and Individual Assessments</strong><br><button class=copy-to-clipboard title="Assessing the Aesthetic Evaluation Capabilities of GPT-4 with Vision: Insights from Group and Individual Assessments" index=99>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-99 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 40<br>Keywords: GPT, GPT-4, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03594v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03594v1.pdf filename=2403.03594v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, it has been recognized that <b>large</b> <b>language</b> <b>models</b> demonstrate high performance on various intellectual tasks. However, few studies have investigated alignment with humans in behaviors that involve sensibility, such as aesthetic evaluation. This study investigates the performance of <b>GPT-4</b> with Vision, a state-of-the-art language model that can handle image input, on the task of aesthetic evaluation of images. We employ two tasks, prediction of the average evaluation values of a group and an individual&rsquo;s evaluation values. We investigate the performance of <b>GPT-4</b> with Vision by exploring <b>prompts</b> and analyzing prediction behaviors. Experimental results reveal <b>GPT-4</b> with Vision&rsquo;s superior performance in predicting aesthetic evaluations and the nature of different responses to beauty and ugliness. Finally, we discuss developing an AI system for aesthetic evaluation based on scientific knowledge of the human perception of beauty, employing agent technologies that integrate traditional deep learning models with <b>large</b> <b>language</b> <b>models.</b></p></p class="citation"></blockquote><h3 id=718--100271-sheetagent-a-generalist-agent-for-spreadsheet-reasoning-and-manipulation-via-large-language-models-yibin-chen-et-al-2024>(7/18 | 100/271) SheetAgent: A Generalist Agent for Spreadsheet Reasoning and Manipulation via Large Language Models (Yibin Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yibin Chen, Yifu Yuan, Zeyu Zhang, Yan Zheng, Jinyi Liu, Fei Ni, Jianye Hao. (2024)<br><strong>SheetAgent: A Generalist Agent for Spreadsheet Reasoning and Manipulation via Large Language Models</strong><br><button class=copy-to-clipboard title="SheetAgent: A Generalist Agent for Spreadsheet Reasoning and Manipulation via Large Language Models" index=100>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-100 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LG, cs.AI<br>Keyword Score: 33<br>Keywords: Benchmarking, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03636v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03636v1.pdf filename=2403.03636v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Spreadsheet manipulation is widely existing in most daily works and significantly improves working efficiency. <b>Large</b> <b>language</b> <b>model</b> <b>(LLM)</b> has been recently attempted for automatic spreadsheet manipulation but has not yet been investigated in complicated and realistic tasks where <b>reasoning</b> challenges exist (e.g., long horizon manipulation with multi-step <b>reasoning</b> and ambiguous requirements). To bridge the gap with the real-world requirements, we introduce $\textbf{SheetRM}$, a <b>benchmark</b> featuring long-horizon and multi-category tasks with <b>reasoning-dependent</b> manipulation caused by real-life challenges. To mitigate the above challenges, we further propose $\textbf{SheetAgent}$, a novel autonomous agent that utilizes the power of <b>LLMs.</b> SheetAgent consists of three collaborative modules: $\textit{Planner}$, $\textit{Informer}$, and $\textit{Retriever}$, achieving both advanced <b>reasoning</b> and accurate manipulation over spreadsheets without human interaction through iterative task <b>reasoning</b> and reflection. Extensive experiments demonstrate that SheetAgent delivers 20-30% pass rate improvements on multiple <b>benchmarks</b> over baselines, achieving enhanced precision in spreadsheet manipulation and demonstrating superior table <b>reasoning</b> abilities. More details and visualizations are available at <a href=https://sheetagent.github.io>https://sheetagent.github.io</a>.</p></p class="citation"></blockquote><h3 id=818--101271-towards-safe-and-aligned-large-language-models-for-medicine-tessa-han-et-al-2024>(8/18 | 101/271) Towards Safe and Aligned Large Language Models for Medicine (Tessa Han et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tessa Han, Aounon Kumar, Chirag Agarwal, Himabindu Lakkaraju. (2024)<br><strong>Towards Safe and Aligned Large Language Models for Medicine</strong><br><button class=copy-to-clipboard title="Towards Safe and Aligned Large Language Models for Medicine" index=101>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-101 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 30<br>Keywords: Fine-tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03744v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03744v1.pdf filename=2403.03744v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The capabilities of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have been progressing at a breathtaking speed, leaving even their own developers grappling with the depth of their potential and risks. While initial steps have been taken to evaluate the safety and alignment of general-knowledge <b>LLMs,</b> exposing some weaknesses, to our knowledge, the safety and alignment of medical <b>LLMs</b> has not been evaluated despite their risks for personal health and safety, public health and safety, and human rights. To this end, we carry out the first safety evaluation for medical <b>LLMs.</b> Specifically, we set forth a definition of medical safety and alignment for medical artificial intelligence systems, develop a dataset of harmful medical questions to evaluate the medical safety and alignment of an <b>LLM,</b> evaluate both general and medical safety and alignment of medical <b>LLMs,</b> demonstrate <b>fine-tuning</b> as an effective mitigation strategy, and discuss broader, <b>large-scale</b> <b>approaches</b> <b>used</b> by the machine learning community to develop safe and aligned <b>LLMs.</b> We hope that this work casts light on the safety and alignment of medical <b>LLMs</b> and motivates future work to study it and develop additional mitigation strategies, minimizing the risks of harm of <b>LLMs</b> in medicine.</p></p class="citation"></blockquote><h3 id=918--102271-an-enkf-lstm-assimilation-algorithm-for-crop-growth-model-siqi-zhou-et-al-2024>(9/18 | 102/271) An EnKF-LSTM Assimilation Algorithm for Crop Growth Model (Siqi Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Siqi Zhou, Ling Wang, Jie Liu, Jinshan Tang. (2024)<br><strong>An EnKF-LSTM Assimilation Algorithm for Crop Growth Model</strong><br><button class=copy-to-clipboard title="An EnKF-LSTM Assimilation Algorithm for Crop Growth Model" index=102>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-102 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LG, cs.AI<br>Keyword Score: 30<br>Keywords: Simulation, Simulator, LSTM<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03406v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03406v1.pdf filename=2403.03406v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Accurate and timely prediction of crop growth is of great significance to ensure crop yields and researchers have developed several crop models for the prediction of crop growth. However, there are large difference between the <b>simulation</b> results obtained by the crop models and the actual results, thus in this paper, we proposed to combine the <b>simulation</b> results with the collected crop data for data assimilation so that the accuracy of prediction will be improved. In this paper, an EnKF-LSTM data assimilation method for various crops is proposed by combining ensemble Kalman filter and <b>LSTM</b> neural network, which effectively avoids the overfitting problem of existing data assimilation methods and eliminates the uncertainty of the measured data. The verification of the proposed EnKF-LSTM method and the comparison of the proposed method with other data assimilation methods were performed using datasets collected by sensor equipment deployed on a farm.</p></p class="citation"></blockquote><h3 id=1018--103271-prompt-mining-for-language-based-human-mobility-forecasting-hao-xue-et-al-2024>(10/18 | 103/271) Prompt Mining for Language-based Human Mobility Forecasting (Hao Xue et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hao Xue, Tianye Tang, Ali Payani, Flora D. Salim. (2024)<br><strong>Prompt Mining for Language-based Human Mobility Forecasting</strong><br><button class=copy-to-clipboard title="Prompt Mining for Language-based Human Mobility Forecasting" index=103>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-103 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs.AI<br>Keyword Score: 20<br>Keywords: Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03544v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03544v1.pdf filename=2403.03544v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the advancement of <b>large</b> <b>language</b> <b>models,</b> language-based forecasting has recently emerged as an innovative approach for predicting human mobility patterns. The core idea is to use <b>prompts</b> to transform the raw mobility data given as numerical values into natural language sentences so that the language models can be leveraged to generate the description for future observations. However, previous studies have only employed fixed and manually designed templates to transform numerical values into sentences. Since the forecasting performance of language models heavily relies on <b>prompts,</b> using fixed templates for <b>prompting</b> may limit the forecasting capability of language models. In this paper, we propose a novel framework for <b>prompt</b> mining in language-based mobility forecasting, aiming to explore diverse <b>prompt</b> design strategies. Specifically, the framework includes a <b>prompt</b> generation stage based on the information entropy of <b>prompts</b> and a <b>prompt</b> refinement stage to integrate mechanisms such as the chain of thought. Experimental results on real-world <b>large-scale</b> <b>data</b> <b>demonstrate</b> the superiority of generated <b>prompts</b> from our <b>prompt</b> mining pipeline. Additionally, the comparison of different <b>prompt</b> variants shows that the proposed <b>prompt</b> refinement process is effective. Our study presents a promising direction for further advancing language-based mobility forecasting.</p></p class="citation"></blockquote><h3 id=1118--104271-bait-benchmarking-embedding-architectures-for-interactive-theorem-proving-sean-lamont-et-al-2024>(11/18 | 104/271) BAIT: Benchmarking (Embedding) Architectures for Interactive Theorem-Proving (Sean Lamont et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sean Lamont, Michael Norrish, Amir Dezfouli, Christian Walder, Paul Montague. (2024)<br><strong>BAIT: Benchmarking (Embedding) Architectures for Interactive Theorem-Proving</strong><br><button class=copy-to-clipboard title="BAIT: Benchmarking (Embedding) Architectures for Interactive Theorem-Proving" index=104>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-104 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LG, cs-LO, cs.AI<br>Keyword Score: 16<br>Keywords: Benchmarking, Benchmarking, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03401v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03401v1.pdf filename=2403.03401v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Artificial Intelligence for Theorem Proving has given rise to a plethora of <b>benchmarks</b> and methodologies, particularly in Interactive Theorem Proving (ITP). Research in the area is fragmented, with a diverse set of approaches being spread across several ITP systems. This presents a significant challenge to the comparison of methods, which are often complex and difficult to replicate. Addressing this, we present BAIT, a framework for fair and streamlined comparison of learning approaches in ITP. We demonstrate BAIT&rsquo;s capabilities with an in-depth comparison, across several ITP <b>benchmarks,</b> of state-of-the-art architectures applied to the problem of formula embedding. We find that Structure Aware <b>Transformers</b> perform particularly well, improving on techniques associated with the original problem sets. BAIT also allows us to assess the end-to-end proving performance of systems built on interactive environments. This unified perspective reveals a novel end-to-end system that improves on prior work. We also provide a qualitative analysis, illustrating that improved performance is associated with more semantically-aware embeddings. By streamlining the implementation and comparison of Machine Learning algorithms in the ITP context, we anticipate BAIT will be a springboard for future research.</p></p class="citation"></blockquote><h3 id=1218--105271-the-geometric-structure-of-topic-models-johannes-hirth-et-al-2024>(12/18 | 105/271) The Geometric Structure of Topic Models (Johannes Hirth et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Johannes Hirth, Tom Hanika. (2024)<br><strong>The Geometric Structure of Topic Models</strong><br><button class=copy-to-clipboard title="The Geometric Structure of Topic Models" index=105>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-105 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 13<br>Keywords: Clustering, Topic Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03607v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03607v1.pdf filename=2403.03607v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Topic</b> <b>models</b> are a popular tool for <b>clustering</b> and analyzing textual data. They allow texts to be classified on the basis of their affiliation to the previously calculated <b>topics.</b> <b>Despite</b> their widespread use in research and application, an in-depth analysis of <b>topic</b> <b>models</b> is still an open research <b>topic.</b> <b>State-of-the-art</b> methods for interpreting <b>topic</b> <b>models</b> are based on simple visualizations, such as similarity matrices, top-term lists or embeddings, which are limited to a maximum of three dimensions. In this paper, we propose an incidence-geometric method for deriving an ordinal structure from flat <b>topic</b> <b>models,</b> such as non-negative matrix factorization. These enable the analysis of the <b>topic</b> <b>model</b> in a higher (order) dimension and the possibility of extracting conceptual relationships between several <b>topics</b> <b>at</b> once. Due to the use of conceptual scaling, our approach does not introduce any artificial <b>topical</b> <b>relationships,</b> such as artifacts of feature compression. Based on our findings, we present a new visualization paradigm for concept hierarchies based on ordinal motifs. These allow for a top-down view on <b>topic</b> <b>spaces.</b> We introduce and demonstrate the applicability of our approach based on a <b>topic</b> <b>model</b> derived from a corpus of scientific papers taken from 32 top machine learning venues.</p></p class="citation"></blockquote><h3 id=1318--106271-ib-net-initial-branch-network-for-variable-decision-in-boolean-satisfiability-tsz-ho-chan-et-al-2024>(13/18 | 106/271) IB-Net: Initial Branch Network for Variable Decision in Boolean Satisfiability (Tsz Ho Chan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tsz Ho Chan, Wenyi Xiao, Junhua Huang, Huiling Zhen, Guangji Tian, Mingxuan Yuan. (2024)<br><strong>IB-Net: Initial Branch Network for Variable Decision in Boolean Satisfiability</strong><br><button class=copy-to-clipboard title="IB-Net: Initial Branch Network for Variable Decision in Boolean Satisfiability" index=106>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-106 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 13<br>Keywords: Graph, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03517v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03517v1.pdf filename=2403.03517v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Boolean Satisfiability problems are vital components in Electronic Design Automation, particularly within the Logic Equivalence Checking process. Currently, SAT solvers are employed for these problems and neural network is tried as assistance to solvers. However, as SAT problems in the LEC context are distinctive due to their predominantly unsatisfiability nature and a substantial proportion of UNSAT-core variables, existing neural network assistance has proven unsuccessful in this specialized domain. To tackle this challenge, we propose IB-Net, an innovative framework utilizing <b>graph</b> <b>neural</b> <b>networks</b> and novel <b>graph</b> <b>encoding</b> <b>techniques</b> to model unsatisfiable problems and interact with state-of-the-art solvers. Extensive evaluations across solvers and datasets demonstrate IB-Net&rsquo;s acceleration, achieving an average runtime speedup of 5.0% on industrial data and 8.3% on SAT competition data empirically. This breakthrough advances efficient solving in LEC workflows.</p></p class="citation"></blockquote><h3 id=1418--107271-understanding-biology-in-the-age-of-artificial-intelligence-elsa-lawrence-et-al-2024>(14/18 | 107/271) Understanding Biology in the Age of Artificial Intelligence (Elsa Lawrence et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Elsa Lawrence, Adham El-Shazly, Srijit Seal, Chaitanya K Joshi, Pietro Liò, Shantanu Singh, Andreas Bender, Pietro Sormanni, Matthew Greenig. (2024)<br><strong>Understanding Biology in the Age of Artificial Intelligence</strong><br><button class=copy-to-clipboard title="Understanding Biology in the Age of Artificial Intelligence" index=107>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-107 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 10<br>Keywords: Information Compression<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04106v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04106v1.pdf filename=2403.04106v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Modern life sciences research is increasingly relying on artificial intelligence approaches to model biological systems, primarily centered around the use of machine learning (ML) models. Although ML is undeniably useful for identifying patterns in large, complex data sets, its widespread application in biological sciences represents a significant deviation from traditional methods of scientific inquiry. As such, the interplay between these models and scientific understanding in biology is a topic with important implications for the future of scientific research, yet it is a subject that has received little attention. Here, we draw from an epistemological toolkit to contextualize recent applications of ML in biological sciences under modern philosophical theories of understanding, identifying general principles that can guide the design and application of ML systems to model biological phenomena and advance scientific knowledge. We propose that conceptions of scientific understanding as <b>information</b> <b>compression,</b> qualitative intelligibility, and dependency relation modelling provide a useful framework for interpreting ML-mediated understanding of biological systems. Through a detailed analysis of two key application areas of ML in modern biological research - protein structure prediction and single cell RNA-sequencing - we explore how these features have thus far enabled ML systems to advance scientific understanding of their target phenomena, how they may guide the development of future ML models, and the key obstacles that remain in preventing ML from achieving its potential as a tool for biological discovery. Consideration of the epistemological features of ML applications in biology will improve the prospects of these methods to solve important problems and advance scientific understanding of living systems.</p></p class="citation"></blockquote><h3 id=1518--108271-artificial-intelligence-exploring-the-patent-field-lekang-jiang-et-al-2024>(15/18 | 108/271) Artificial Intelligence Exploring the Patent Field (Lekang Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lekang Jiang, Stephan Goetz. (2024)<br><strong>Artificial Intelligence Exploring the Patent Field</strong><br><button class=copy-to-clipboard title="Artificial Intelligence Exploring the Patent Field" index=108>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-108 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04105v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04105v1.pdf filename=2403.04105v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Advanced language-processing and machine-learning techniques promise massive efficiency improvements in the previously widely manual field of patent and technical knowledge management. This field presents <b>large-scale</b> <b>and</b> <b>complex</b> data with very precise contents and language representation of those contents. Particularly, patent texts can differ from mundane texts in various aspects, which entails significant opportunities and challenges. This paper presents a systematic overview of patent-related tasks and popular methodologies with a special focus on evolving and promising techniques. Language processing and particularly <b>large</b> <b>language</b> <b>models</b> as well as the recent boost of general generative methods promise to become game changers in the patent field. The patent literature and the fact-based argumentative procedures around patents appear almost as an ideal use case. However, patents entail a number of difficulties with which existing models struggle. The paper introduces fundamental aspects of patents and patent-related data that affect technology that wants to explore or manage them. It further reviews existing methods and approaches and points out how important reliable and unbiased evaluation metrics become. Although research has made substantial progress on certain tasks, the performance across many others remains suboptimal, sometimes because of either the special nature of patents and their language or inconsistencies between legal terms and the everyday meaning of terms. Moreover, yet few methods have demonstrated the ability to produce satisfactory text for specific sections of patents. By pointing out key developments, opportunities, and gaps, we aim to encourage further research and accelerate the advancement of this field.</p></p class="citation"></blockquote><h3 id=1618--109271-learning-guided-automated-reasoning-a-brief-survey-lasse-blaauwbroek-et-al-2024>(16/18 | 109/271) Learning Guided Automated Reasoning: A Brief Survey (Lasse Blaauwbroek et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lasse Blaauwbroek, David Cerna, Thibault Gauthier, Jan Jakubův, Cezary Kaliszyk, Martin Suda, Josef Urban. (2024)<br><strong>Learning Guided Automated Reasoning: A Brief Survey</strong><br><button class=copy-to-clipboard title="Learning Guided Automated Reasoning: A Brief Survey" index=109>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-109 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LG, cs-LO, cs-NE, cs-SC, cs.AI<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04017v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04017v1.pdf filename=2403.04017v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Automated theorem provers and formal proof assistants are general <b>reasoning</b> systems that are in theory capable of proving arbitrarily hard theorems, thus solving arbitrary problems reducible to mathematics and logical <b>reasoning.</b> In practice, such systems however face large combinatorial explosion, and therefore include many heuristics and choice points that considerably influence their performance. This is an opportunity for trained machine learning predictors, which can guide the work of such <b>reasoning</b> systems. Conversely, deductive search supported by the notion of logically valid proof allows one to train machine learning systems on large <b>reasoning</b> corpora. Such bodies of proof are usually correct by construction and when combined with more and more precise trained guidance they can be boostrapped into very large corpora, with increasingly long <b>reasoning</b> chains and possibly novel proof ideas. In this paper we provide an overview of several automated <b>reasoning</b> and theorem proving domains and the learning and AI methods that have been so far developed for them. These include premise selection, proof guidance in several settings, AI systems and feedback loops iterating between <b>reasoning</b> and learning, and symbolic classification problems.</p></p class="citation"></blockquote><h3 id=1718--110271-rethinking-urban-flood-risk-assessment-by-adapting-health-domain-perspective-zhewei-liu-et-al-2024>(17/18 | 110/271) Rethinking Urban Flood Risk Assessment By Adapting Health Domain Perspective (Zhewei Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhewei Liu, Kai Yin, Ali Mostafavi. (2024)<br><strong>Rethinking Urban Flood Risk Assessment By Adapting Health Domain Perspective</strong><br><button class=copy-to-clipboard title="Rethinking Urban Flood Risk Assessment By Adapting Health Domain Perspective" index=110>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-110 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 10<br>Keywords: Human Intervention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03996v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03996v1.pdf filename=2403.03996v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Inspired by ideas from health risk assessment, this paper presents a new perspective for flood risk assessment. The proposed perspective focuses on three pillars for examining flood risk: (1) inherent susceptibility, (2) mitigation strategies, and (3) external stressors. These pillars collectively encompass the physical and environmental characteristics of urban areas, the effectiveness of <b>human-intervention</b> <b>measures,</b> and the influence of uncontrollable external factors, offering a fresh point of view for decoding flood risks. For each pillar, we delineate its individual contributions to flood risk and illustrate their interactive and overall impact. The three-pillars model embodies a shift in focus from the quest to precisely model and quantify flood risk to evaluating pathways to high flood risk. The shift in perspective is intended to alleviate the quest for quantifying and predicting flood risk at fine resolutions as a panacea for enhanced flood risk management. The decomposition of flood risk pathways into the three intertwined pillars (i.e., inherent factors, mitigation factors, and external factors) enables evaluation of changes in factors within each pillar enhance and exacerbate flood risk, creating a platform from which to inform plans, decisions, and actions. Building on this foundation, we argue that a flood risk pathway analysis approach, which examines the individual and collective impacts of inherent factors, mitigation strategies, and external stressors, is essential for a nuanced evaluation of flood risk. Accordingly, the proposed perspective could complement the existing frameworks and approaches for flood risk assessment.</p></p class="citation"></blockquote><h3 id=1818--111271-adaptive-discovering-and-merging-for-incremental-novel-class-discovery-guangyao-chen-et-al-2024>(18/18 | 111/271) Adaptive Discovering and Merging for Incremental Novel Class Discovery (Guangyao Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guangyao Chen, Peixi Peng, Yangru Huang, Mengyue Geng, Yonghong Tian. (2024)<br><strong>Adaptive Discovering and Merging for Incremental Novel Class Discovery</strong><br><button class=copy-to-clipboard title="Adaptive Discovering and Merging for Incremental Novel Class Discovery" index=111>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-111 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 5<br>Keywords: Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03382v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03382v1.pdf filename=2403.03382v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>One important desideratum of lifelong learning aims to discover novel classes from unlabelled data in a continuous manner. The central challenge is twofold: discovering and learning novel classes while mitigating the issue of catastrophic forgetting of established knowledge. To this end, we introduce a new paradigm called Adaptive Discovering and Merging (ADM) to discover novel categories adaptively in the incremental stage and integrate novel knowledge into the model without affecting the original knowledge. To discover novel classes adaptively, we decouple <b>representation</b> <b>learning</b> and novel class discovery, and use Triple Comparison (TC) and Probability Regularization (PR) to constrain the probability discrepancy and diversity for adaptive category assignment. To merge the learned novel knowledge adaptively, we propose a hybrid structure with base and novel branches named Adaptive Model Merging (AMM), which reduces the interference of the novel branch on the old classes to preserve the previous knowledge, and merges the novel branch to the base model without performance loss and parameter growth. Extensive experiments on several datasets show that ADM significantly outperforms existing class-incremental Novel Class Discovery (class-iNCD) approaches. Moreover, our AMM also benefits the class-incremental Learning (class-IL) task by alleviating the catastrophic forgetting problem.</p></p class="citation"></blockquote><h2 id=csma-1>cs.MA (1)</h2><h3 id=11--112271-an-ai-enabled-agent-based-model-and-its-application-in-measles-outbreak-simulation-for-new-zealand-sijin-zhang-et-al-2024>(1/1 | 112/271) An AI-enabled Agent-Based Model and Its Application in Measles Outbreak Simulation for New Zealand (Sijin Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sijin Zhang, Alvaro Orsi, Lei Chen. (2024)<br><strong>An AI-enabled Agent-Based Model and Its Application in Measles Outbreak Simulation for New Zealand</strong><br><button class=copy-to-clipboard title="An AI-enabled Agent-Based Model and Its Application in Measles Outbreak Simulation for New Zealand" index=112>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-112 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MA<br>Categories: cs-CY, cs-LG, cs-MA, cs.MA<br>Keyword Score: 73<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Simulation, Simulator, LSTM, LSTM, LSTM<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03434v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03434v2.pdf filename=2403.03434v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Agent Based Models (ABMs) have emerged as a powerful tool for investigating complex social interactions, particularly in the context of public health and infectious disease investigation. In an effort to enhance the conventional ABM, enabling automated model calibration and reducing the computational resources needed for scaling up the model, we have developed a tensorized and differentiable agent-based model by coupling <b>Graph</b> <b>Neural</b> <b>Network</b> <b>(GNN)</b> and <b>Long</b> <b>Short-Term</b> <b>Memory</b> <b>(LSTM)</b> network. The model was employed to investigate the 2019 measles outbreak occurred in New Zealand, demonstrating a promising ability to accurately simulate the outbreak dynamics, particularly during the peak period of repeated cases. This paper shows that by leveraging the latest Artificial Intelligence (AI) technology and the capabilities of traditional ABMs, we gain deeper insights into the dynamics of infectious disease outbreaks. This, in turn, helps us make more informed decision when developing effective strategies that strike a balance between managing outbreaks and minimizing disruptions to everyday life.</p></p class="citation"></blockquote><h2 id=csse-6>cs.SE (6)</h2><h3 id=16--113271-automatic-bi-modal-question-title-generation-for-stack-overflow-with-prompt-learning-shaoyu-yang-et-al-2024>(1/6 | 113/271) Automatic Bi-modal Question Title Generation for Stack Overflow with Prompt Learning (Shaoyu Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shaoyu Yang, Xiang Chen, Ke Liu, Guang Yang, Chi Yu. (2024)<br><strong>Automatic Bi-modal Question Title Generation for Stack Overflow with Prompt Learning</strong><br><button class=copy-to-clipboard title="Automatic Bi-modal Question Title Generation for Stack Overflow with Prompt Learning" index=113>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-113 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 70<br>Keywords: Automatic Evaluation, Fine-tuning, Fine-tuning, Pre-trained Language Model, Prompt, Prompt Learning, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03677v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03677v1.pdf filename=2403.03677v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>When drafting question posts for Stack Overflow, developers may not accurately <b>summarize</b> the core problems in the question titles, which can cause these questions to not get timely help. Therefore, improving the quality of question titles has attracted the wide attention of researchers. An initial study aimed to automatically generate the titles by only analyzing the code snippets in the question body. However, this study ignored the helpful information in their corresponding problem descriptions. Therefore, we propose an approach SOTitle+ by considering bi-modal information (i.e., the code snippets and the problem descriptions) in the question body. Then we formalize the title generation for different programming languages as separate but related tasks and utilize multi-task learning to solve these tasks. Later we <b>fine-tune</b> the <b>pre-trained</b> <b>language</b> <b>model</b> CodeT5 to automatically generate the titles. Unfortunately, the inconsistent inputs and optimization objectives between the pre-training task and our investigated task may make <b>fine-tuning</b> hard to fully explore the knowledge of the <b>pre-trained</b> <b>model.</b> <b>To</b> solve this issue, SOTitle+ further <b>prompt-tunes</b> <b>CodeT5</b> with hybrid <b>prompts</b> <b>(i.e.,</b> mixture of hard and soft <b>prompts).</b> <b>To</b> verify the effectiveness of SOTitle+, we construct a large-scale high-quality corpus from recent data dumps shared by Stack Overflow. Our corpus includes 179,119 high-quality question posts for six popular programming languages. Experimental results show that SOTitle+ can significantly outperform four state-of-the-art baselines in both <b>automatic</b> <b>evaluation</b> and human evaluation. Our work indicates that considering bi-modal information and <b>prompt</b> <b>learning</b> in Stack Overflow title generation is a promising exploration direction.</p></p class="citation"></blockquote><h3 id=26--114271-quantifying-contamination-in-evaluating-code-generation-capabilities-of-language-models-martin-riddell-et-al-2024>(2/6 | 114/271) Quantifying Contamination in Evaluating Code Generation Capabilities of Language Models (Martin Riddell et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Martin Riddell, Ansong Ni, Arman Cohan. (2024)<br><strong>Quantifying Contamination in Evaluating Code Generation Capabilities of Language Models</strong><br><button class=copy-to-clipboard title="Quantifying Contamination in Evaluating Code Generation Capabilities of Language Models" index=114>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-114 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-CL, cs-LG, cs-SE, cs.SE<br>Keyword Score: 63<br>Keywords: Benchmarking, Fine-tuning, Code Generation, Language Generation, Natural Language Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04811v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04811v1.pdf filename=2403.04811v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While <b>large</b> <b>language</b> <b>models</b> have achieved remarkable performance on various <b>code</b> <b>generation</b> <b>benchmarks,</b> there have been growing concerns regarding potential contamination of these <b>benchmarks</b> as they may be leaked into pretraining and <b>finetuning</b> data. While recent work has investigated contamination in <b>natural</b> <b>language</b> <b>generation</b> and understanding tasks, there has been less extensive research into how data contamination impacts the evaluation of <b>code</b> <b>generation,</b> which is critical for understanding the robustness and reliability of <b>LLMs</b> in programming contexts. In this work, we perform a comprehensive study of data contamination of popular <b>code</b> <b>generation</b> <b>benchmarks,</b> and precisely quantify their overlap with pretraining corpus through both surface-level and semantic-level matching. In our experiments, we show that there are substantial overlap between popular <b>code</b> <b>generation</b> <b>benchmarks</b> and open training corpus, and models perform significantly better on the subset of the <b>benchmarks</b> where similar solutions are seen during training. We also conduct extensive analysis on the factors that affects model memorization and generalization, such as model size, problem difficulty, and question length. We release all resulting files from our matching pipeline for future research.</p></p class="citation"></blockquote><h3 id=36--115271-whodunit-classifying-code-as-human-authored-or-gpt-4-generated----a-case-study-on-codechef-problems-oseremen-joy-idialu-et-al-2024>(3/6 | 115/271) Whodunit: Classifying Code as Human Authored or GPT-4 Generated &ndash; A case study on CodeChef problems (Oseremen Joy Idialu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Oseremen Joy Idialu, Noble Saji Mathews, Rungroj Maipradit, Joanne M. Atlee, Mei Nagappan. (2024)<br><strong>Whodunit: Classifying Code as Human Authored or GPT-4 Generated &ndash; A case study on CodeChef problems</strong><br><button class=copy-to-clipboard title="Whodunit: Classifying Code as Human Authored or GPT-4 Generated -- A case study on CodeChef problems" index=115>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-115 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 50<br>Keywords: Generative AI, ChatGPT, GPT, GPT-4, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04013v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04013v1.pdf filename=2403.04013v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Artificial intelligence (AI) assistants such as GitHub Copilot and <b>ChatGPT,</b> built on <b>large</b> <b>language</b> <b>models</b> like <b>GPT-4,</b> are revolutionizing how programming tasks are performed, raising questions about whether code is authored by <b>generative</b> <b>AI</b> models. Such questions are of particular interest to educators, who worry that these tools enable a new form of academic dishonesty, in which students submit AI generated code as their own work. Our research explores the viability of using code stylometry and machine learning to distinguish between <b>GPT-4</b> generated and human-authored code. Our dataset comprises human-authored solutions from CodeChef and AI-authored solutions generated by <b>GPT-4.</b> Our classifier outperforms baselines, with an F1-score and AUC-ROC score of 0.91. A variant of our classifier that excludes gameable features (e.g., empty lines, whitespace) still performs well with an F1-score and AUC-ROC score of 0.89. We also evaluated our classifier with respect to the difficulty of the programming problem and found that there was almost no difference between easier and intermediate problems, and the classifier performed only slightly worse on harder problems. Our study shows that code stylometry is a promising approach for distinguishing between <b>GPT-4</b> generated code and human-authored code.</p></p class="citation"></blockquote><h3 id=46--116271-fuzzing-busybox-leveraging-llm-and-crash-reuse-for-embedded-bug-unearthing-asmita-et-al-2024>(4/6 | 116/271) Fuzzing BusyBox: Leveraging LLM and Crash Reuse for Embedded Bug Unearthing (Asmita et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Asmita, Yaroslav Oliinyk, Michael Scott, Ryan Tsang, Chongzhou Fang, Houman Homayoun. (2024)<br><strong>Fuzzing BusyBox: Leveraging LLM and Crash Reuse for Embedded Bug Unearthing</strong><br><button class=copy-to-clipboard title="Fuzzing BusyBox: Leveraging LLM and Crash Reuse for Embedded Bug Unearthing" index=116>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-116 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-CR, cs-SE, cs.SE<br>Keyword Score: 30<br>Keywords: Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03897v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03897v1.pdf filename=2403.03897v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>BusyBox, an open-source software bundling over 300 essential Linux commands into a single executable, is ubiquitous in Linux-based embedded devices. Vulnerabilities in BusyBox can have far-reaching consequences, affecting a wide array of devices. This research, driven by the extensive use of BusyBox, delved into its analysis. The study revealed the prevalence of older BusyBox versions in real-world embedded products, <b>prompting</b> us to conduct fuzz testing on BusyBox. Fuzzing, a pivotal software testing method, aims to induce crashes that are subsequently scrutinized to uncover vulnerabilities. Within this study, we introduce two techniques to fortify software testing. The first technique enhances fuzzing by leveraging <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLM)</b> to generate target-specific initial seeds. Our study showed a substantial increase in crashes when using <b>LLM-generated</b> initial seeds, highlighting the potential of <b>LLM</b> to efficiently tackle the typically labor-intensive task of generating target-specific initial seeds. The second technique involves repurposing previously acquired crash data from similar fuzzed targets before initiating fuzzing on a new target. This approach streamlines the time-consuming fuzz testing process by providing crash data directly to the new target before commencing fuzzing. We successfully identified crashes in the latest BusyBox target without conducting traditional fuzzing, emphasizing the effectiveness of <b>LLM</b> and crash reuse techniques in enhancing software testing and improving vulnerability detection in embedded systems. Additionally, manual triaging was performed to identify the nature of crashes in the latest BusyBox.</p></p class="citation"></blockquote><h3 id=56--117271-does-documentation-matter-an-empirical-study-of-practitioners-perspective-on-open-source-software-adoption-aaron-imani-et-al-2024>(5/6 | 117/271) Does Documentation Matter? An Empirical Study of Practitioners&rsquo; Perspective on Open-Source Software Adoption (Aaron Imani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aaron Imani, Shiva Radmanesh, Iftekhar Ahmed, Mohammad Moshirpour. (2024)<br><strong>Does Documentation Matter? An Empirical Study of Practitioners&rsquo; Perspective on Open-Source Software Adoption</strong><br><button class=copy-to-clipboard title="Does Documentation Matter? An Empirical Study of Practitioners' Perspective on Open-Source Software Adoption" index=117>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-117 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 30<br>Keywords: Topic Model, ChatGPT, TF-IDF<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03819v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03819v1.pdf filename=2403.03819v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, open-source software (OSS) has become increasingly prevalent in developing software products. While OSS documentation is the primary source of information provided by the developers&rsquo; community about a product, its role in the industry&rsquo;s adoption process has yet to be examined. We conducted semi-structured interviews and an online survey to provide insight into this area. Based on interviews and survey insights, we developed a <b>topic</b> <b>model</b> to collect relevant information from OSS documentation automatically. Additionally, according to our survey responses regarding challenges associated with OSS documentation, we propose a novel information augmentation approach, DocMentor, by combining OSS documentation corpus <b>TF-IDF</b> scores and <b>ChatGPT.</b> Through explaining technical terms and providing examples and references, our approach enhances the documentation context and improves practitioners&rsquo; understanding. Our tool&rsquo;s effectiveness is assessed by surveying practitioners.</p></p class="citation"></blockquote><h3 id=66--118271-an-ide-plugin-for-gamified-continuous-integration-philipp-straubinger-et-al-2024>(6/6 | 118/271) An IDE Plugin for Gamified Continuous Integration (Philipp Straubinger et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Philipp Straubinger, Gordon Fraser. (2024)<br><strong>An IDE Plugin for Gamified Continuous Integration</strong><br><button class=copy-to-clipboard title="An IDE Plugin for Gamified Continuous Integration" index=118>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-118 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 10<br>Keywords: Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03557v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03557v1.pdf filename=2403.03557v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Interruptions and context switches resulting from meetings, urgent tasks, emails, and queries from colleagues contribute to productivity losses in developers&rsquo; daily routines. This is particularly challenging for tasks like software testing, which are already perceived as less enjoyable, <b>prompting</b> developers to seek distractions. To mitigate this, applying gamification to testing activities can enhance motivation for test writing. One such gamification tool is Gamekins, which integrates challenges, quests, achievements, and leaderboards into the Jenkins CI (continuous integration) platform. However, as Gamekins is typically accessed through a browser, it introduces a context switch. This paper presents an IntelliJ plugin designed to seamlessly integrate Gamekins&rsquo; gamification elements into the IDE, aiming to minimize context switches and boost developer motivation for test writing.</p></p class="citation"></blockquote><h2 id=csne-3>cs.NE (3)</h2><h3 id=13--119271-explaining-genetic-programming-trees-using-large-language-models-paula-maddigan-et-al-2024>(1/3 | 119/271) Explaining Genetic Programming Trees using Large Language Models (Paula Maddigan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Paula Maddigan, Andrew Lensen, Bing Xue. (2024)<br><strong>Explaining Genetic Programming Trees using Large Language Models</strong><br><button class=copy-to-clipboard title="Explaining Genetic Programming Trees using Large Language Models" index=119>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-119 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-NE, cs.NE<br>Keyword Score: 70<br>Keywords: Explainable AI, Generative AI, ChatGPT, Chatbot, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03397v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03397v1.pdf filename=2403.03397v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Genetic programming (GP) has the potential to generate <b>explainable</b> <b>results,</b> especially when used for dimensionality reduction. In this research, we investigate the potential of leveraging <b>eXplainable</b> <b>AI</b> (XAI) and <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> like <b>ChatGPT</b> to improve the interpretability of GP-based non-linear dimensionality reduction. Our study introduces a novel XAI dashboard named GP4NLDR, the first approach to combine state-of-the-art GP with an <b>LLM-powered</b> <b>chatbot</b> to provide comprehensive, user-centred explanations. We showcase the system&rsquo;s ability to provide intuitive and insightful narratives on high-dimensional data reduction processes through case studies. Our study highlights the importance of <b>prompt</b> engineering in eliciting accurate and pertinent responses from <b>LLMs.</b> We also address important considerations around data privacy, hallucinatory outputs, and the rapid advancements in <b>generative</b> <b>AI.</b> Our findings demonstrate its potential in advancing the explainability of GP algorithms. This opens the door for future research into explaining GP models with <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=23--120271-neural-architecture-search-using-particle-swarm-and-ant-colony-optimization-séamus-lankford-et-al-2024>(2/3 | 120/271) Neural Architecture Search using Particle Swarm and Ant Colony Optimization (Séamus Lankford et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Séamus Lankford, Diarmuid Grimes. (2024)<br><strong>Neural Architecture Search using Particle Swarm and Ant Colony Optimization</strong><br><button class=copy-to-clipboard title="Neural Architecture Search using Particle Swarm and Ant Colony Optimization" index=120>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-120 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-AI, cs-LG, cs-NE, cs.NE<br>Keyword Score: 50<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, Fine-tuning, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03781v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03781v1.pdf filename=2403.03781v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neural network models have a number of hyperparameters that must be chosen along with their architecture. This can be a heavy burden on a novice user, choosing which architecture and what values to assign to parameters. In most cases, default hyperparameters and architectures are used. Significant improvements to model accuracy can be achieved through the evaluation of multiple architectures. A process known as Neural Architecture Search (NAS) may be applied to automatically evaluate a large number of such architectures. A system integrating open source tools for Neural Architecture Search (OpenNAS), in the classification of images, has been developed as part of this research. OpenNAS takes any dataset of grayscale, or RBG images, and generates <b>Convolutional</b> <b>Neural</b> <b>Network</b> <b>(CNN)</b> architectures based on a range of metaheuristics using either an AutoKeras, a <b>transfer</b> <b>learning</b> or a Swarm Intelligence (SI) approach. Particle Swarm Optimization (PSO) and Ant Colony Optimization (ACO) are used as the SI algorithms. Furthermore, models developed through such metaheuristics may be combined using stacking ensembles. In the context of this paper, we focus on training and optimizing <b>CNNs</b> using the Swarm Intelligence (SI) components of OpenNAS. Two major types of SI algorithms, namely PSO and ACO, are compared to see which is more effective in generating higher model accuracies. It is shown, with our experimental design, that the PSO algorithm performs better than ACO. The performance improvement of PSO is most notable with a more complex dataset. As a baseline, the performance of <b>fine-tuned</b> pre-trained models is also evaluated.</p></p class="citation"></blockquote><h3 id=33--121271-sparse-spiking-neural-network-exploiting-heterogeneity-in-timescales-for-pruning-recurrent-snn-biswadeep-chakraborty-et-al-2024>(3/3 | 121/271) Sparse Spiking Neural Network: Exploiting Heterogeneity in Timescales for Pruning Recurrent SNN (Biswadeep Chakraborty et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Biswadeep Chakraborty, Beomseok Kang, Harshit Kumar, Saibal Mukhopadhyay. (2024)<br><strong>Sparse Spiking Neural Network: Exploiting Heterogeneity in Timescales for Pruning Recurrent SNN</strong><br><button class=copy-to-clipboard title="Sparse Spiking Neural Network: Exploiting Heterogeneity in Timescales for Pruning Recurrent SNN" index=121>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-121 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-AI, cs-NE, cs.NE<br>Keyword Score: 13<br>Keywords: Graph, Pruning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03409v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03409v1.pdf filename=2403.03409v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recurrent Spiking Neural Networks (RSNNs) have emerged as a computationally efficient and brain-inspired learning model. The design of sparse RSNNs with fewer neurons and synapses helps reduce the computational complexity of RSNNs. Traditionally, sparse SNNs are obtained by first training a dense and complex SNN for a target task, and, then, <b>pruning</b> neurons with low activity (activity-based <b>pruning)</b> while maintaining task performance. In contrast, this paper presents a task-agnostic methodology for designing sparse RSNNs by <b>pruning</b> a large randomly initialized model. We introduce a novel Lyapunov Noise <b>Pruning</b> (LNP) algorithm that uses <b>graph</b> sparsification methods and utilizes Lyapunov exponents to design a stable sparse RSNN from a randomly initialized RSNN. We show that the LNP can leverage diversity in neuronal timescales to design a sparse Heterogeneous RSNN (HRSNN). Further, we show that the same sparse HRSNN model can be trained for different tasks, such as image classification and temporal prediction. We experimentally show that, in spite of being task-agnostic, LNP increases computational efficiency (fewer neurons and synapses) and prediction performance of RSNNs compared to traditional activity-based <b>pruning</b> of trained dense models.</p></p class="citation"></blockquote><h2 id=cscv-43>cs.CV (43)</h2><h3 id=143--122271-are-language-models-puzzle-prodigies-algorithmic-puzzles-unveil-serious-challenges-in-multimodal-reasoning-deepanway-ghosal-et-al-2024>(1/43 | 122/271) Are Language Models Puzzle Prodigies? Algorithmic Puzzles Unveil Serious Challenges in Multimodal Reasoning (Deepanway Ghosal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Deepanway Ghosal, Vernon Toh Yan Han, Chia Yew Ken, Soujanya Poria. (2024)<br><strong>Are Language Models Puzzle Prodigies? Algorithmic Puzzles Unveil Serious Challenges in Multimodal Reasoning</strong><br><button class=copy-to-clipboard title="Are Language Models Puzzle Prodigies? Algorithmic Puzzles Unveil Serious Challenges in Multimodal Reasoning" index=122>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-122 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 69<br>Keywords: Graph, Multi-modal, Multi-modal, Gemini, Question Answering, Reasoning, Visual Question Answering, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03864v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03864v2.pdf filename=2403.03864v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces the novel task of <b>multimodal</b> puzzle solving, framed within the context of <b>visual</b> <b>question-answering.</b> <b>We</b> present a new dataset, AlgoPuzzleVQA designed to challenge and evaluate the capabilities of <b>multimodal</b> language models in solving algorithmic puzzles that necessitate both <b>visual</b> <b>understanding,</b> <b>language</b> understanding, and complex algorithmic <b>reasoning.</b> We create the puzzles to encompass a diverse array of mathematical and algorithmic topics such as boolean logic, combinatorics, <b>graph</b> theory, optimization, search, etc., aiming to evaluate the gap between <b>visual</b> <b>data</b> <b>interpretation</b> and algorithmic problem-solving skills. The dataset is generated automatically from code authored by humans. All our puzzles have exact solutions that can be found from the algorithm without tedious human calculations. It ensures that our dataset can be scaled up arbitrarily in terms of <b>reasoning</b> complexity and dataset size. Our investigation reveals that <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> such as GPT4V and <b>Gemini</b> exhibit limited performance in puzzle-solving tasks. We find that their performance is near random in a multi-choice <b>question-answering</b> <b>setup</b> for a significant number of puzzles. The findings emphasize the challenges of integrating <b>visual,</b> <b>language,</b> <b>and</b> algorithmic knowledge for solving complex <b>reasoning</b> problems.</p></p class="citation"></blockquote><h3 id=243--123271-multimodal-transformer-for-comics-text-cloze-emanuele-vivoli-et-al-2024>(2/43 | 123/271) Multimodal Transformer for Comics Text-Cloze (Emanuele Vivoli et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Emanuele Vivoli, Joan Lafuente Baeza, Ernest Valveny Llobet, Dimosthenis Karatzas. (2024)<br><strong>Multimodal Transformer for Comics Text-Cloze</strong><br><button class=copy-to-clipboard title="Multimodal Transformer for Comics Text-Cloze" index=123>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-123 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 66<br>Keywords: Optical Character Recognition, Fine-tuning, Multi-modal, Multi-modal, Self-supervised Learning, Recurrent Neural Network, Transformer, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03719v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03719v1.pdf filename=2403.03719v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work explores a closure task in comics, a medium where visual and textual elements are intricately intertwined. Specifically, Text-cloze refers to the task of selecting the correct text to use in a comic panel, given its neighboring panels. Traditional methods based on <b>recurrent</b> <b>neural</b> <b>networks</b> have struggled with this task due to limited <b>OCR</b> accuracy and inherent model limitations. We introduce a novel <b>Multimodal</b> <b>Large</b> <b>Language</b> <b>Model</b> <b>(Multimodal-LLM)</b> architecture, specifically designed for Text-cloze, achieving a 10% improvement over existing state-of-the-art models in both its easy and hard variants. Central to our approach is a Domain-Adapted ResNet-50 based visual encoder, <b>fine-tuned</b> to the comics domain in a <b>self-supervised</b> manner using SimCLR. This encoder delivers comparable results to more complex models with just one-fifth of the parameters. Additionally, we release new <b>OCR</b> annotations for this dataset, enhancing model input quality and resulting in another 1% improvement. Finally, we extend the task to a generative format, establishing new baselines and expanding the research possibilities in the field of comics analysis.</p></p class="citation"></blockquote><h3 id=343--124271-molnextr-a-generalized-deep-learning-model-for-molecular-image-recognition-yufan-chen-et-al-2024>(3/43 | 124/271) MolNexTR: A Generalized Deep Learning Model for Molecular Image Recognition (Yufan Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yufan Chen, Ching Ting Leung, Yong Huang, Jianwei Sun, Hao Chen, Hanyu Gao. (2024)<br><strong>MolNexTR: A Generalized Deep Learning Model for Molecular Image Recognition</strong><br><button class=copy-to-clipboard title="MolNexTR: A Generalized Deep Learning Model for Molecular Image Recognition" index=124>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-124 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 53<br>Keywords: Vision Transformer, Graph, Convolution, Convolutional Neural Network, Data Augmentation, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03691v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03691v2.pdf filename=2403.03691v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the field of chemical structure recognition, the task of converting molecular images into <b>graph</b> structures and SMILES string stands as a significant challenge, primarily due to the varied drawing styles and conventions prevalent in chemical literature. To bridge this gap, we proposed MolNexTR, a novel image-to-graph deep learning model that collaborates to fuse the strengths of ConvNext, a powerful <b>Convolutional</b> <b>Neural</b> <b>Network</b> variant, and <b>Vision-TRansformer.</b> <b>This</b> integration facilitates a more nuanced extraction of both local and global features from molecular images. MolNexTR can predict atoms and bonds simultaneously and understand their layout rules. It also excels at flexibly integrating symbolic chemistry principles to discern chirality and decipher abbreviated structures. We further incorporate a series of advanced algorithms, including improved <b>data</b> <b>augmentation</b> module, image contamination module, and a post-processing module to get the final SMILES output. These modules synergistically enhance the model&rsquo;s robustness against the diverse styles of molecular imagery found in real literature. In our test sets, MolNexTR has demonstrated superior performance, achieving an accuracy rate of 81-97%, marking a significant advancement in the domain of molecular structure recognition. Scientific contribution: MolNexTR is a novel image-to-graph model that incorporates a unique dual-stream encoder to extract complex molecular image features, and combines chemical rules to predict atoms and bonds while understanding atom and bond layout rules. In addition, it employs a series of novel augmentation algorithms to significantly enhance the robustness and performance of the model.</p></p class="citation"></blockquote><h3 id=443--125271-self-supervised-photographic-image-layout-representation-learning-zhaoran-zhao-et-al-2024>(4/43 | 125/271) Self-supervised Photographic Image Layout Representation Learning (Zhaoran Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhaoran Zhao, Peng Lu, Xujun Peng, Wenhao Guo. (2024)<br><strong>Self-supervised Photographic Image Layout Representation Learning</strong><br><button class=copy-to-clipboard title="Self-supervised Photographic Image Layout Representation Learning" index=125>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-125 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-MM, cs.CV<br>Keyword Score: 41<br>Keywords: Graph, Autoencoder, Benchmarking, Representation Learning, Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03740v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03740v1.pdf filename=2403.03740v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the domain of image layout <b>representation</b> <b>learning,</b> the critical process of translating image layouts into succinct vector forms is increasingly significant across diverse applications, such as image retrieval, manipulation, and generation. Most approaches in this area heavily rely on costly labeled datasets and notably lack in adapting their modeling and learning methods to the specific nuances of photographic image layouts. This shortfall makes the learning process for photographic image layouts suboptimal. In our research, we directly address these challenges. We innovate by defining basic layout primitives that encapsulate various levels of layout information and by mapping these, along with their interconnections, onto a heterogeneous <b>graph</b> structure. This <b>graph</b> is meticulously engineered to capture the intricate layout information within the pixel domain explicitly. Advancing further, we introduce novel pretext tasks coupled with customized loss functions, strategically designed for effective <b>self-supervised</b> <b>learning</b> of these layout <b>graphs.</b> Building on this foundation, we develop an <b>autoencoder-based</b> network architecture skilled in compressing these heterogeneous layout <b>graphs</b> into precise, dimensionally-reduced layout <b>representations.</b> <b>Additionally,</b> we introduce the LODB dataset, which features a broader range of layout categories and richer semantics, serving as a comprehensive <b>benchmark</b> for evaluating the effectiveness of layout <b>representation</b> <b>learning</b> methods. Our extensive experimentation on this dataset demonstrates the superior performance of our approach in the realm of photographic image layout <b>representation</b> <b>learning.</b></p></p class="citation"></blockquote><h3 id=543--126271-meacap-memory-augmented-zero-shot-image-captioning-zequn-zeng-et-al-2024>(5/43 | 126/271) MeaCap: Memory-Augmented Zero-shot Image Captioning (Zequn Zeng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zequn Zeng, Yan Xie, Hao Zhang, Chiyu Chen, Zhengjue Wang, Bo Chen. (2024)<br><strong>MeaCap: Memory-Augmented Zero-shot Image Captioning</strong><br><button class=copy-to-clipboard title="MeaCap: Memory-Augmented Zero-shot Image Captioning" index=126>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-126 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Zero-shot, Image2text, Pre-trained Language Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03715v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03715v1.pdf filename=2403.03715v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Zero-shot</b> image captioning (IC) without well-paired <b>image-text</b> data can be divided into two categories, training-free and text-only-training. Generally, these two types of methods realize <b>zero-shot</b> IC by integrating <b>pretrained</b> <b>vision-language</b> <b>models</b> like CLIP for <b>image-text</b> similarity evaluation and a <b>pre-trained</b> <b>language</b> <b>model</b> (LM) for caption generation. The main difference between them is whether using a textual corpus to train the LM. Though achieving attractive performance w.r.t. some metrics, existing methods often exhibit some common drawbacks. Training-free methods tend to produce hallucinations, while text-only-training often lose generalization capability. To move forward, in this paper, we propose a novel Memory-Augmented <b>zero-shot</b> image Captioning framework (MeaCap). Specifically, equipped with a textual memory, we introduce a retrieve-then-filter module to get key concepts that are highly related to the image. By deploying our proposed memory-augmented visual-related fusion score in a keywords-to-sentence LM, MeaCap can generate concept-centered captions that keep high consistency with the image with fewer hallucinations and more world-knowledge. The framework of MeaCap achieves the state-of-the-art performance on a series of <b>zero-shot</b> IC settings. Our code is available at <a href=https://github.com/joeyz0z/MeaCap>https://github.com/joeyz0z/MeaCap</a>.</p></p class="citation"></blockquote><h3 id=643--127271-performance-evaluation-of-semi-supervised-learning-frameworks-for-multi-class-weed-detection-jiajia-li-et-al-2024>(6/43 | 127/271) Performance Evaluation of Semi-supervised Learning Frameworks for Multi-Class Weed Detection (Jiajia Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiajia Li, Dong Chen, Xunyuan Yin, Zhaojian Li. (2024)<br><strong>Performance Evaluation of Semi-supervised Learning Frameworks for Multi-Class Weed Detection</strong><br><button class=copy-to-clipboard title="Performance Evaluation of Semi-supervised Learning Frameworks for Multi-Class Weed Detection" index=127>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-127 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV, eess-IV<br>Keyword Score: 40<br>Keywords: Object Detection, Semi-Supervised Learning, Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03390v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03390v1.pdf filename=2403.03390v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Effective weed control plays a crucial role in optimizing crop yield and enhancing agricultural product quality. However, the reliance on herbicide application not only poses a critical threat to the environment but also promotes the emergence of resistant weeds. Fortunately, recent advances in precision weed management enabled by ML and DL provide a sustainable alternative. Despite great progress, existing algorithms are mainly developed based on <b>supervised</b> <b>learning</b> approaches, which typically demand large-scale datasets with manual-labeled annotations, which is time-consuming and labor-intensive. As such, label-efficient learning methods, especially <b>semi-supervised</b> <b>learning,</b> have gained increased attention in the broader domain of computer vision and have demonstrated promising performance. These methods aim to utilize a small number of labeled data samples along with a great number of unlabeled samples to develop high-performing models comparable to the <b>supervised</b> <b>learning</b> counterpart trained on a large amount of labeled data samples. In this study, we assess the effectiveness of a <b>semi-supervised</b> <b>learning</b> framework for multi-class weed detection, employing two well-known <b>object</b> <b>detection</b> frameworks, namely FCOS and Faster-RCNN. Specifically, we evaluate a generalized student-teacher framework with an improved pseudo-label generation module to produce reliable pseudo-labels for the unlabeled data. To enhance generalization, an ensemble student network is employed to facilitate the training process. Experimental results show that the proposed approach is able to achieve approximately 76% and 96% detection accuracy as the <b>supervised</b> <b>methods</b> with only 10% of labeled data in CottenWeedDet3 and CottonWeedDet12, respectively. We offer access to the source code, contributing a valuable resource for ongoing <b>semi-supervised</b> <b>learning</b> research in weed detection and beyond.</p></p class="citation"></blockquote><h3 id=743--128271-contrastive-learning-of-person-independent-representations-for-facial-action-unit-detection-yong-li-et-al-2024>(7/43 | 128/271) Contrastive Learning of Person-independent Representations for Facial Action Unit Detection (Yong Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yong Li, Shiguang Shan. (2024)<br><strong>Contrastive Learning of Person-independent Representations for Facial Action Unit Detection</strong><br><button class=copy-to-clipboard title="Contrastive Learning of Person-independent Representations for Facial Action Unit Detection" index=128>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-128 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 35<br>Keywords: Contrastive Learning, Representation Learning, Self-supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03400v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03400v1.pdf filename=2403.03400v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Facial action unit (AU) detection, aiming to classify AU present in the facial image, has long suffered from insufficient AU annotations. In this paper, we aim to mitigate this data scarcity issue by learning AU <b>representations</b> <b>from</b> a large number of unlabelled facial videos in a <b>contrastive</b> <b>learning</b> paradigm. We formulate the <b>self-supervised</b> AU <b>representation</b> <b>learning</b> signals in two-fold: (1) AU <b>representation</b> <b>should</b> be frame-wisely discriminative within a short video clip; (2) Facial frames sampled from different identities but show analogous facial AUs should have consistent AU <b>representations.</b> <b>As</b> to achieve these goals, we propose to contrastively learn the AU <b>representation</b> <b>within</b> a video clip and devise a cross-identity reconstruction mechanism to learn the person-independent <b>representations.</b> <b>Specially,</b> we adopt a margin-based temporal <b>contrastive</b> <b>learning</b> paradigm to perceive the temporal AU coherence and evolution characteristics within a clip that consists of consecutive input facial frames. Moreover, the cross-identity reconstruction mechanism facilitates pushing the faces from different identities but show analogous AUs close in the latent embedding space. Experimental results on three public AU datasets demonstrate that the learned AU <b>representation</b> <b>is</b> discriminative for AU detection. Our method outperforms other <b>contrastive</b> <b>learning</b> methods and significantly closes the performance gap between the <b>self-supervised</b> and <b>supervised</b> AU detection approaches.</p></p class="citation"></blockquote><h3 id=843--129271-ecap-extensive-cut-and-paste-augmentation-for-unsupervised-domain-adaptive-semantic-segmentation-erik-brorsson-et-al-2024>(8/43 | 129/271) ECAP: Extensive Cut-and-Paste Augmentation for Unsupervised Domain Adaptive Semantic Segmentation (Erik Brorsson et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Erik Brorsson, Knut Åkesson, Lennart Svensson, Kristofer Bengtsson. (2024)<br><strong>ECAP: Extensive Cut-and-Paste Augmentation for Unsupervised Domain Adaptive Semantic Segmentation</strong><br><button class=copy-to-clipboard title="ECAP: Extensive Cut-and-Paste Augmentation for Unsupervised Domain Adaptive Semantic Segmentation" index=129>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-129 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Benchmarking, Data Augmentation, Unsupervised Learning, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03854v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03854v1.pdf filename=2403.03854v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider <b>unsupervised</b> <b>domain</b> <b>adaptation</b> (UDA) for semantic segmentation in which the model is trained on a labeled source dataset and adapted to an unlabeled target dataset. Unfortunately, current self-training methods are susceptible to misclassified pseudo-labels resulting from erroneous predictions. Since certain classes are typically associated with less reliable predictions in UDA, reducing the impact of such pseudo-labels without skewing the training towards some classes is notoriously difficult. To this end, we propose an extensive cut-and-paste strategy (ECAP) to leverage reliable pseudo-labels through <b>data</b> <b>augmentation.</b> Specifically, ECAP maintains a memory bank of pseudo-labeled target samples throughout training and cut-and-pastes the most confident ones onto the current training batch. We implement ECAP on top of the recent method MIC and boost its performance on two synthetic-to-real <b>domain</b> <b>adaptation</b> <b>benchmarks.</b> Notably, MIC+ECAP reaches an unprecedented performance of 69.1 mIoU on the Synthia->Cityscapes <b>benchmark.</b> Our code is available at <a href=https://github.com/ErikBrorsson/ECAP>https://github.com/ErikBrorsson/ECAP</a>.</p></p class="citation"></blockquote><h3 id=943--130271-cmda-cross-modal-and-domain-adversarial-adaptation-for-lidar-based-3d-object-detection-gyusam-chang-et-al-2024>(9/43 | 130/271) CMDA: Cross-Modal and Domain Adversarial Adaptation for LiDAR-Based 3D Object Detection (Gyusam Chang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gyusam Chang, Wonseok Roh, Sujin Jang, Dongwook Lee, Daehyun Ji, Gyeongrok Oh, Jinsun Park, Jinkyu Kim, Sangpil Kim. (2024)<br><strong>CMDA: Cross-Modal and Domain Adversarial Adaptation for LiDAR-Based 3D Object Detection</strong><br><button class=copy-to-clipboard title="CMDA: Cross-Modal and Domain Adversarial Adaptation for LiDAR-Based 3D Object Detection" index=130>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-130 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Object Detection, Benchmarking, Unsupervised Learning, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03721v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03721v2.pdf filename=2403.03721v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent LiDAR-based 3D <b>Object</b> <b>Detection</b> (3DOD) methods show promising results, but they often do not generalize well to target <b>domains</b> <b>outside</b> the source (or training) data distribution. To reduce such <b>domain</b> <b>gaps</b> and thus to make 3DOD models more generalizable, we introduce a novel <b>unsupervised</b> <b>domain</b> <b>adaptation</b> (UDA) method, called CMDA, which (i) leverages visual semantic cues from an image modality (i.e., camera images) as an effective semantic bridge to close the <b>domain</b> <b>gap</b> in the cross-modal Bird&rsquo;s Eye View (BEV) representations. Further, (ii) we also introduce a self-training-based learning strategy, wherein a model is adversarially trained to generate <b>domain-invariant</b> <b>features,</b> which disrupt the discrimination of whether a feature instance comes from a source or an unseen target <b>domain.</b> <b>Overall,</b> our CMDA framework guides the 3DOD model to generate highly informative and <b>domain-adaptive</b> <b>features</b> for novel data distributions. In our extensive experiments with large-scale <b>benchmarks,</b> such as nuScenes, Waymo, and KITTI, those mentioned above provide significant performance gains for UDA tasks, achieving state-of-the-art performance.</p></p class="citation"></blockquote><h3 id=1043--131271-task-attribute-distance-for-few-shot-learning-theoretical-analysis-and-applications-minyang-hu-et-al-2024>(10/43 | 131/271) Task Attribute Distance for Few-Shot Learning: Theoretical Analysis and Applications (Minyang Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Minyang Hu, Hong Chang, Zong Guo, Bingpeng Ma, Shiguan Shan, Xilin Chen. (2024)<br><strong>Task Attribute Distance for Few-Shot Learning: Theoretical Analysis and Applications</strong><br><button class=copy-to-clipboard title="Task Attribute Distance for Few-Shot Learning: Theoretical Analysis and Applications" index=131>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-131 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 33<br>Keywords: Benchmarking, Data Augmentation, Few-shot, Few-shot Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03535v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03535v1.pdf filename=2403.03535v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Few-shot</b> <b>learning</b> (FSL) aims to learn novel tasks with very few labeled samples by leveraging experience from \emph{related} training tasks. In this paper, we try to understand FSL by delving into two key questions: (1) How to quantify the relationship between \emph{training} and \emph{novel} tasks? (2) How does the relationship affect the \emph{adaptation difficulty} on novel tasks for different models? To answer the two questions, we introduce Task Attribute Distance (TAD) built upon attributes as a metric to quantify the task relatedness. Unlike many existing metrics, TAD is model-agnostic, making it applicable to different FSL models. Then, we utilize TAD metric to establish a theoretical connection between task relatedness and task adaptation difficulty. By deriving the generalization error bound on a novel task, we discover how TAD measures the adaptation difficulty on novel tasks for FSL models. To validate our TAD metric and theoretical findings, we conduct experiments on three <b>benchmarks.</b> Our experimental results confirm that TAD metric effectively quantifies the task relatedness and reflects the adaptation difficulty on novel tasks for various FSL methods, even if some of them do not learn attributes explicitly or human-annotated attributes are not available. Finally, we present two applications of the proposed TAD metric: <b>data</b> <b>augmentation</b> and test-time intervention, which further verify its effectiveness and general applicability. The source code is available at <a href=https://github.com/hu-my/TaskAttributeDistance>https://github.com/hu-my/TaskAttributeDistance</a>.</p></p class="citation"></blockquote><h3 id=1143--132271-dlp-gan-learning-to-draw-modern-chinese-landscape-photos-with-generative-adversarial-network-xiangquan-gui-et-al-2024>(11/43 | 132/271) DLP-GAN: learning to draw modern Chinese landscape photos with generative adversarial network (Xiangquan Gui et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiangquan Gui, Binxuan Zhang, Li Li, Yi Yang. (2024)<br><strong>DLP-GAN: learning to draw modern Chinese landscape photos with generative adversarial network</strong><br><button class=copy-to-clipboard title="DLP-GAN: learning to draw modern Chinese landscape photos with generative adversarial network" index=132>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-132 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Benchmarking, Generative Adversarial Network, Unsupervised Learning, Image2text<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03456v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03456v2.pdf filename=2403.03456v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Chinese landscape painting has a unique and artistic style, and its drawing technique is highly abstract in both the use of color and the realistic representation of objects. Previous methods focus on transferring from modern photos to ancient ink paintings. However, little attention has been paid to translating landscape paintings into modern photos. To solve such problems, in this paper, we (1) propose DLP-GAN (Draw Modern Chinese Landscape Photos with <b>Generative</b> <b>Adversarial</b> <b>Network),</b> an <b>unsupervised</b> cross-domain <b>image</b> <b>translation</b> framework with a novel asymmetric cycle mapping, and (2) introduce a generator based on a dense-fusion module to match different translation directions. Moreover, a dual-consistency loss is proposed to balance the realism and abstraction of model painting. In this way, our model can draw landscape photos and sketches in the modern sense. Finally, based on our collection of modern landscape and sketch datasets, we compare the <b>images</b> <b>generated</b> by our model with other <b>benchmarks.</b> Extensive experiments including user studies show that our model outperforms state-of-the-art methods.</p></p class="citation"></blockquote><h3 id=1243--133271-self-and-mixed-supervision-to-improve-training-labels-for-multi-class-medical-image-segmentation-jianfei-liu-et-al-2024>(12/43 | 133/271) Self and Mixed Supervision to Improve Training Labels for Multi-Class Medical Image Segmentation (Jianfei Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianfei Liu, Christopher Parnell, Ronald M. Summers. (2024)<br><strong>Self and Mixed Supervision to Improve Training Labels for Multi-Class Medical Image Segmentation</strong><br><button class=copy-to-clipboard title="Self and Mixed Supervision to Improve Training Labels for Multi-Class Medical Image Segmentation" index=133>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-133 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Fine-tuning, Fine-tuning, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03882v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03882v1.pdf filename=2403.03882v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Accurate training labels are a key component for multi-class medical image segmentation. Their annotation is costly and time-consuming because it requires domain expertise. This work aims to develop a dual-branch network and automatically improve training labels for multi-class image segmentation. <b>Transfer</b> <b>learning</b> is used to train the network and improve inaccurate weak labels sequentially. The dual-branch network is first trained by weak labels alone to initialize model parameters. After the network is stabilized, the shared encoder is frozen, and strong and weak decoders are <b>fine-tuned</b> by strong and weak labels together. The accuracy of weak labels is iteratively improved in the <b>fine-tuning</b> process. The proposed method was applied to a three-class segmentation of muscle, subcutaneous and visceral adipose tissue on abdominal CT scans. Validation results on 11 patients showed that the accuracy of training labels was statistically significantly improved, with the Dice similarity coefficient of muscle, subcutaneous and visceral adipose tissue increased from 74.2% to 91.5%, 91.2% to 95.6%, and 77.6% to 88.5%, respectively (p&lt;0.05). In comparison with our earlier method, the label accuracy was also significantly improved (p&lt;0.05). These experimental results suggested that the combination of the dual-branch network and <b>transfer</b> <b>learning</b> is an efficient means to improve training labels for multi-class segmentation.</p></p class="citation"></blockquote><h3 id=1343--134271-latent-dataset-distillation-with-diffusion-models-brian-b-moser-et-al-2024>(13/43 | 134/271) Latent Dataset Distillation with Diffusion Models (Brian B. Moser et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Brian B. Moser, Federico Raue, Sebastian Palacio, Stanislav Frolov, Andreas Dengel. (2024)<br><strong>Latent Dataset Distillation with Diffusion Models</strong><br><button class=copy-to-clipboard title="Latent Dataset Distillation with Diffusion Models" index=134>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-134 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 30<br>Keywords: Diffusion Model, Knowledge Distillation, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03881v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03881v1.pdf filename=2403.03881v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The efficacy of machine learning has traditionally relied on the availability of increasingly larger datasets. However, large datasets pose storage challenges and contain non-influential samples, which could be ignored during training without impacting the final accuracy of the model. In response to these limitations, the concept of <b>distilling</b> the information on a dataset into a condensed set of (synthetic) samples, namely a <b>distilled</b> dataset, emerged. One crucial aspect is the selected architecture (usually ConvNet) for linking the original and synthetic datasets. However, the final accuracy is lower if the employed model architecture differs from the model used during <b>distillation.</b> Another challenge is the generation of high-resolution images, e.g., 128x128 and higher. In this paper, we propose Latent Dataset <b>Distillation</b> with <b>Diffusion</b> <b>Models</b> (LD3M) that combine <b>diffusion</b> <b>in</b> latent space with dataset <b>distillation</b> to tackle both challenges. LD3M incorporates a novel <b>diffusion</b> <b>process</b> tailored for dataset <b>distillation,</b> which improves the gradient norms for learning synthetic images. By adjusting the number of <b>diffusion</b> <b>steps,</b> LD3M also offers a straightforward way of controlling the trade-off between speed and accuracy. We evaluate our approach in several ImageNet subsets and for high-resolution images (128x128 and 256x256). As a result, LD3M consistently outperforms state-of-the-art <b>distillation</b> techniques by up to 4.8 p.p. and 4.2 p.p. for 1 and 10 images per class, respectively.</p></p class="citation"></blockquote><h3 id=1443--135271-popeye-a-unified-visual-language-model-for-multi-source-ship-detection-from-remote-sensing-imagery-wei-zhang-et-al-2024>(14/43 | 135/271) Popeye: A Unified Visual-Language Model for Multi-Source Ship Detection from Remote Sensing Imagery (Wei Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wei Zhang, Miaoxin Cai, Tong Zhang, Guoqiang Lei, Yin Zhuang, Xuerui Mao. (2024)<br><strong>Popeye: A Unified Visual-Language Model for Multi-Source Ship Detection from Remote Sensing Imagery</strong><br><button class=copy-to-clipboard title="Popeye: A Unified Visual-Language Model for Multi-Source Ship Detection from Remote Sensing Imagery" index=135>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-135 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Zero-shot, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03790v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03790v1.pdf filename=2403.03790v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Ship detection needs to identify ship locations from remote sensing (RS) scenes. However, due to different imaging payloads, various appearances of ships, and complicated background interference from the bird&rsquo;s eye view, it is difficult to set up a unified paradigm for achieving multi-source ship detection. Therefore, in this article, considering that the <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> emerge the powerful generalization ability, a novel unified visual-language model called Popeye is proposed for multi-source ship detection from RS imagery. First, to bridge the interpretation gap between multi-source images for ship detection, a novel image-instruction-answer way is designed to integrate the various ship detection ways (e.g., horizontal bounding box (HBB), oriented bounding box (OBB)) into a unified labeling paradigm. Then, in view of this, a cross-modal image interpretation method is developed for the proposed Popeye to enhance interactive comprehension ability between visual and language content, which can be easily migrated into any multi-source ship detection task. Subsequently, owing to objective domain differences, a knowledge adaption mechanism is designed to adapt the pre-trained visual-language knowledge from the nature scene into the RS domain for multi-source ship detection. In addition, the segment anything model (SAM) is also seamlessly integrated into the proposed Popeye to achieve pixel-level ship segmentation without additional training costs. Finally, extensive experiments are conducted on the newly constructed instruction dataset named MMShip, and the results indicate that the proposed Popeye outperforms current specialist, open-vocabulary, and other visual-language models for <b>zero-shot</b> multi-source ship detection.</p></p class="citation"></blockquote><h3 id=1543--136271-multi-grained-cross-modal-alignment-for-learning-open-vocabulary-semantic-segmentation-from-text-supervision-yajie-liu-et-al-2024>(15/43 | 136/271) Multi-Grained Cross-modal Alignment for Learning Open-vocabulary Semantic Segmentation from Text Supervision (Yajie Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yajie Liu, Pu Ge, Qingjie Liu, Di Huang. (2024)<br><strong>Multi-Grained Cross-modal Alignment for Learning Open-vocabulary Semantic Segmentation from Text Supervision</strong><br><button class=copy-to-clipboard title="Multi-Grained Cross-modal Alignment for Learning Open-vocabulary Semantic Segmentation from Text Supervision" index=136>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-136 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Contrastive Learning, Zero-shot, Image2text<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03707v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03707v1.pdf filename=2403.03707v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, learning open-vocabulary semantic segmentation from text supervision has achieved promising downstream performance. Nevertheless, current approaches encounter an alignment granularity gap owing to the absence of dense annotations, wherein they learn coarse image/region-text alignment during training yet perform group/pixel-level predictions at inference. Such discrepancy leads to suboptimal learning efficiency and inferior <b>zero-shot</b> segmentation results. In this paper, we introduce a Multi-Grained Cross-modal Alignment (MGCA) framework, which explicitly learns pixel-level alignment along with object- and region-level alignment to bridge the granularity gap without any dense annotations. Specifically, MGCA ingeniously constructs pseudo multi-granular semantic correspondences upon <b>image-text</b> pairs and collaborates with hard sampling strategies to facilitate fine-grained cross-modal <b>contrastive</b> <b>learning.</b> Further, we point out the defects of existing group and pixel prediction units in downstream segmentation and develop an adaptive semantic unit which effectively mitigates their dilemmas including under- and over-segmentation. Training solely on CC3M, our method achieves significant advancements over state-of-the-art methods, demonstrating its effectiveness and efficiency.</p></p class="citation"></blockquote><h3 id=1643--137271-causal-prototype-inspired-contrast-adaptation-for-unsupervised-domain-adaptive-semantic-segmentation-of-high-resolution-remote-sensing-imagery-jingru-zhu-et-al-2024>(16/43 | 137/271) Causal Prototype-inspired Contrast Adaptation for Unsupervised Domain Adaptive Semantic Segmentation of High-resolution Remote Sensing Imagery (Jingru Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jingru Zhu, Ya Guo, Geng Sun, Liang Hong, Jie Chen. (2024)<br><strong>Causal Prototype-inspired Contrast Adaptation for Unsupervised Domain Adaptive Semantic Segmentation of High-resolution Remote Sensing Imagery</strong><br><button class=copy-to-clipboard title="Causal Prototype-inspired Contrast Adaptation for Unsupervised Domain Adaptive Semantic Segmentation of High-resolution Remote Sensing Imagery" index=137>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-137 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Causal Intervention, Counter-factual, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03704v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03704v1.pdf filename=2403.03704v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Semantic segmentation of high-resolution remote sensing imagery (HRSI) suffers from the domain shift, resulting in poor performance of the model in another unseen domain. <b>Unsupervised</b> domain adaptive (UDA) semantic segmentation aims to adapt the semantic segmentation model trained on the labeled source domain to an unlabeled target domain. However, the existing UDA semantic segmentation models tend to align pixels or features based on statistical information related to labels in source and target domain data, and make predictions accordingly, which leads to uncertainty and fragility of prediction results. In this paper, we propose a <b>causal</b> <b>prototype-inspired</b> contrast adaptation (CPCA) method to explore the invariant <b>causal</b> <b>mechanisms</b> between different HRSIs domains and their semantic labels. It firstly disentangles <b>causal</b> <b>features</b> and bias features from the source and target domain images through a <b>causal</b> <b>feature</b> disentanglement module. Then, a <b>causal</b> <b>prototypical</b> contrast module is used to learn domain invariant <b>causal</b> <b>features.</b> To further de-correlate <b>causal</b> <b>and</b> bias features, a <b>causal</b> <b>intervention</b> module is introduced to intervene on the bias features to generate <b>counterfactual</b> unbiased samples. By forcing the <b>causal</b> <b>features</b> to meet the principles of separability, invariance and intervention, CPCA can simulate the <b>causal</b> <b>factors</b> of source and target domains, and make decisions on the target domain based on the <b>causal</b> <b>features,</b> which can observe improved generalization ability. Extensive experiments under three cross-domain tasks indicate that CPCA is remarkably superior to the state-of-the-art methods.</p></p class="citation"></blockquote><h3 id=1743--138271-adversarial-infrared-geometry-using-geometry-to-perform-adversarial-attack-against-infrared-pedestrian-detectors-kalibinuer-tiliwalidi-2024>(17/43 | 138/271) Adversarial Infrared Geometry: Using Geometry to Perform Adversarial Attack against Infrared Pedestrian Detectors (Kalibinuer Tiliwalidi, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kalibinuer Tiliwalidi. (2024)<br><strong>Adversarial Infrared Geometry: Using Geometry to Perform Adversarial Attack against Infrared Pedestrian Detectors</strong><br><button class=copy-to-clipboard title="Adversarial Infrared Geometry: Using Geometry to Perform Adversarial Attack against Infrared Pedestrian Detectors" index=138>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-138 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Object Detection, Black Box, Geometry, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03674v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03674v1.pdf filename=2403.03674v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Currently, infrared imaging technology enjoys widespread usage, with infrared <b>object</b> <b>detection</b> technology experiencing a surge in prominence. While previous studies have delved into physical attacks on infrared <b>object</b> <b>detectors,</b> the implementation of these techniques remains complex. For instance, some approaches entail the use of bulb boards or infrared QR suits as perturbations to execute attacks, which entail costly optimization and cumbersome deployment processes. Other methodologies involve the utilization of irregular aerogel as physical perturbations for infrared attacks, albeit at the expense of optimization expenses and perceptibility issues. In this study, we propose a novel infrared physical attack termed <b>Adversarial</b> <b>Infrared</b> <b>Geometry</b> (\textbf{AdvIG}), which facilitates efficient <b>black-box</b> <b>query</b> attacks by modeling diverse geometric shapes (lines, triangles, ellipses) and optimizing their physical parameters using Particle Swarm Optimization (PSO). Extensive experiments are conducted to evaluate the effectiveness, stealthiness, and robustness of AdvIG. In digital attack experiments, line, triangle, and ellipse patterns achieve attack success rates of 93.1%, 86.8%, and 100.0%, respectively, with average query times of 71.7, 113.1, and 2.57, respectively, thereby confirming the efficiency of AdvIG. Physical attack experiments are conducted to assess the attack success rate of AdvIG at different distances. On average, the line, triangle, and ellipse achieve attack success rates of 61.1%, 61.2%, and 96.2%, respectively. Further experiments are conducted to comprehensively analyze AdvIG, including ablation experiments, transfer attack experiments, and <b>adversarial</b> <b>defense</b> mechanisms. Given the superior performance of our method as a simple and efficient <b>black-box</b> <b>adversarial</b> <b>attack</b> in both digital and physical environments, we advocate for widespread attention to AdvIG.</p></p class="citation"></blockquote><h3 id=1843--139271-noisecollage-a-layout-aware-text-to-image-diffusion-model-based-on-noise-cropping-and-merging-takahiro-shirakawa-et-al-2024>(18/43 | 139/271) NoiseCollage: A Layout-Aware Text-to-Image Diffusion Model Based on Noise Cropping and Merging (Takahiro Shirakawa et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Takahiro Shirakawa, Seiichi Uchida. (2024)<br><strong>NoiseCollage: A Layout-Aware Text-to-Image Diffusion Model Based on Noise Cropping and Merging</strong><br><button class=copy-to-clipboard title="NoiseCollage: A Layout-Aware Text-to-Image Diffusion Model Based on Noise Cropping and Merging" index=139>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-139 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: ControlNet, Diffusion Model, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03485v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03485v1.pdf filename=2403.03485v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Layout-aware <b>text-to-image</b> generation is a task to generate multi-object images that reflect layout conditions in addition to text conditions. The current layout-aware <b>text-to-image</b> <b>diffusion</b> <b>models</b> still have several issues, including mismatches between the text and layout conditions and quality degradation of generated images. This paper proposes a novel layout-aware <b>text-to-image</b> <b>diffusion</b> <b>model</b> called NoiseCollage to tackle these issues. During the denoising process, NoiseCollage independently estimates noises for individual objects and then crops and merges them into a single noise. This operation helps avoid condition mismatches; in other words, it can put the right objects in the right places. Qualitative and quantitative evaluations show that NoiseCollage outperforms several state-of-the-art models. These successful results indicate that the crop-and-merge operation of noises is a reasonable strategy to control image generation. We also show that NoiseCollage can be integrated with <b>ControlNet</b> to use edges, sketches, and pose skeletons as additional conditions. Experimental results show that this integration boosts the layout accuracy of <b>ControlNet.</b> The code is available at <a href=https://github.com/univ-esuty/noisecollage>https://github.com/univ-esuty/noisecollage</a>.</p></p class="citation"></blockquote><h3 id=1943--140271-flame-diffuser-grounded-wildfire-image-synthesis-using-mask-guided-diffusion-hao-wang-et-al-2024>(19/43 | 140/271) FLAME Diffuser: Grounded Wildfire Image Synthesis using Mask Guided Diffusion (Hao Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hao Wang, Sayed Pedram Haeri Boroujeni, Xiwen Chen, Ashish Bastola, Huayu Li, Abolfazl Razi. (2024)<br><strong>FLAME Diffuser: Grounded Wildfire Image Synthesis using Mask Guided Diffusion</strong><br><button class=copy-to-clipboard title="FLAME Diffuser: Grounded Wildfire Image Synthesis using Mask Guided Diffusion" index=140>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-140 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Diffusion Model, Object Detection, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03463v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03463v1.pdf filename=2403.03463v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rise of machine learning in recent years has brought benefits to various research fields such as wide fire detection. Nevertheless, small <b>object</b> <b>detection</b> and rare <b>object</b> <b>detection</b> remain a challenge. To address this problem, we present a dataset automata that can generate ground truth paired datasets using <b>diffusion</b> <b>models.</b> Specifically, we introduce a mask-guided <b>diffusion</b> <b>framework</b> that can fusion the wildfire into the existing images while the flame position and size can be precisely controlled. In advance, to fill the gap that the dataset of wildfire images in specific scenarios is missing, we vary the background of synthesized images by controlling both the text <b>prompt</b> and input image. Furthermore, to solve the color tint problem or the well-known domain shift issue, we apply the CLIP model to filter the generated massive dataset to preserve quality. Thus, our proposed framework can generate a massive dataset of that images are high-quality and ground truth-paired, which well addresses the needs of the annotated datasets in specific tasks.</p></p class="citation"></blockquote><h3 id=2043--141271-towards-understanding-cross-and-self-attention-in-stable-diffusion-for-text-guided-image-editing-bingyan-liu-et-al-2024>(20/43 | 141/271) Towards Understanding Cross and Self-Attention in Stable Diffusion for Text-Guided Image Editing (Bingyan Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bingyan Liu, Chengyu Wang, Tingfeng Cao, Kui Jia, Jun Huang. (2024)<br><strong>Towards Understanding Cross and Self-Attention in Stable Diffusion for Text-Guided Image Editing</strong><br><button class=copy-to-clipboard title="Towards Understanding Cross and Self-Attention in Stable Diffusion for Text-Guided Image Editing" index=141>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-141 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Diffusion Model, Text2image, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03431v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03431v1.pdf filename=2403.03431v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep <b>Text-to-Image</b> Synthesis (TIS) models such as Stable <b>Diffusion</b> <b>have</b> recently gained significant popularity for creative <b>Text-to-image</b> generation. Yet, for domain-specific scenarios, tuning-free Text-guided Image Editing (TIE) is of greater importance for application developers, which modify objects or object properties in images by manipulating feature components in attention layers during the generation process. However, little is known about what semantic meanings these attention layers have learned and which parts of the attention maps contribute to the success of image editing. In this paper, we conduct an in-depth probing analysis and demonstrate that cross-attention maps in Stable <b>Diffusion</b> <b>often</b> contain object attribution information that can result in editing failures. In contrast, <b>self-attention</b> maps play a crucial role in preserving the geometric and shape details of the source image during the transformation to the target image. Our analysis offers valuable insights into understanding cross and <b>self-attention</b> maps in <b>diffusion</b> <b>models.</b> Moreover, based on our findings, we simplify popular image editing methods and propose a more straightforward yet more stable and efficient tuning-free procedure that only modifies <b>self-attention</b> maps of the specified attention layers during the denoising process. Experimental results show that our simplified method consistently surpasses the performance of popular approaches on multiple datasets.</p></p class="citation"></blockquote><h3 id=2143--142271-lodisc-learning-global-local-discriminative-features-for-self-supervised-fine-grained-visual-recognition-jialu-shi-et-al-2024>(21/43 | 142/271) LoDisc: Learning Global-Local Discriminative Features for Self-Supervised Fine-Grained Visual Recognition (Jialu Shi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jialu Shi, Zhiqiang Wei, Jie Nie, Lei Huang. (2024)<br><strong>LoDisc: Learning Global-Local Discriminative Features for Self-Supervised Fine-Grained Visual Recognition</strong><br><button class=copy-to-clipboard title="LoDisc: Learning Global-Local Discriminative Features for Self-Supervised Fine-Grained Visual Recognition" index=142>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-142 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: 68U10, I-4, cs-CV, cs.CV<br>Keyword Score: 25<br>Keywords: Contrastive Learning, Representation Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04066v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04066v1.pdf filename=2403.04066v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Self-supervised</b> <b>contrastive</b> <b>learning</b> strategy has attracted remarkable attention due to its exceptional ability in <b>representation</b> <b>learning.</b> However, current <b>contrastive</b> <b>learning</b> tends to learn global coarse-grained <b>representations</b> <b>of</b> the image that benefit generic object recognition, whereas such coarse-grained features are insufficient for fine-grained visual recognition. In this paper, we present to incorporate the subtle local fine-grained feature learning into global <b>self-supervised</b> <b>contrastive</b> <b>learning</b> through a pure <b>self-supervised</b> global-local fine-grained <b>contrastive</b> <b>learning</b> framework. Specifically, a novel pretext task called Local Discrimination (LoDisc) is proposed to explicitly supervise <b>self-supervised</b> model&rsquo;s focus towards local pivotal regions which are captured by a simple-but-effective location-wise mask sampling strategy. We show that Local Discrimination pretext task can effectively enhance fine-grained clues in important local regions, and the global-local framework further refines the fine-grained feature <b>representations</b> <b>of</b> images. Extensive experimental results on different fine-grained object recognition tasks demonstrate that the proposed method can lead to a decent improvement in different evaluation settings. Meanwhile, the proposed method is also effective in general object recognition tasks.</p></p class="citation"></blockquote><h3 id=2243--143271-lead-learning-decomposition-for-source-free-universal-domain-adaptation-sanqing-qu-et-al-2024>(22/43 | 143/271) LEAD: Learning Decomposition for Source-free Universal Domain Adaptation (Sanqing Qu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sanqing Qu, Tianpei Zou, Lianghua He, Florian Röhrbein, Alois Knoll, Guang Chen, Changjun Jiang. (2024)<br><strong>LEAD: Learning Decomposition for Source-free Universal Domain Adaptation</strong><br><button class=copy-to-clipboard title="LEAD: Learning Decomposition for Source-free Universal Domain Adaptation" index=143>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-143 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 23<br>Keywords: Clustering, Knowledge Transfer, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03421v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03421v1.pdf filename=2403.03421v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Universal <b>Domain</b> <b>Adaptation</b> (UniDA) targets <b>knowledge</b> <b>transfer</b> in the presence of both covariate and label shifts. Recently, Source-free Universal <b>Domain</b> <b>Adaptation</b> (SF-UniDA) has emerged to achieve UniDA without access to source data, which tends to be more practical due to data protection policies. The main challenge lies in determining whether covariate-shifted samples belong to target-private unknown categories. Existing methods tackle this either through hand-crafted thresholding or by developing time-consuming iterative <b>clustering</b> strategies. In this paper, we propose a new idea of LEArning Decomposition (LEAD), which decouples features into source-known and -unknown components to identify target-private data. Technically, LEAD initially leverages the orthogonal decomposition analysis for feature decomposition. Then, LEAD builds instance-level decision boundaries to adaptively identify target-private data. Extensive experiments across various UniDA scenarios have demonstrated the effectiveness and superiority of LEAD. Notably, in the OPDA scenario on VisDA dataset, LEAD outperforms GLC by 3.5% overall H-score and reduces 75% time to derive pseudo-labeling decision boundaries. Besides, LEAD is also appealing in that it is complementary to most existing methods. The code is available at <a href=https://github.com/ispc-lab/LEAD>https://github.com/ispc-lab/LEAD</a>.</p></p class="citation"></blockquote><h3 id=2343--144271-dart-implicit-doppler-tomography-for-radar-novel-view-synthesis-tianshu-huang-et-al-2024>(23/43 | 144/271) DART: Implicit Doppler Tomography for Radar Novel View Synthesis (Tianshu Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianshu Huang, John Miller, Akarsh Prabhakara, Tao Jin, Tarana Laroia, Zico Kolter, Anthony Rowe. (2024)<br><strong>DART: Implicit Doppler Tomography for Radar Novel View Synthesis</strong><br><button class=copy-to-clipboard title="DART: Implicit Doppler Tomography for Radar Novel View Synthesis" index=144>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-144 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03896v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03896v1.pdf filename=2403.03896v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Simulation</b> is an invaluable tool for radio-frequency system designers that enables rapid prototyping of various algorithms for imaging, target detection, classification, and tracking. However, simulating realistic radar scans is a challenging task that requires an accurate model of the scene, radio frequency material properties, and a corresponding radar synthesis function. Rather than specifying these models explicitly, we propose DART - Doppler Aided Radar Tomography, a Neural Radiance Field-inspired method which uses radar-specific physics to create a reflectance and transmittance-based rendering pipeline for range-Doppler images. We then evaluate DART by constructing a custom data collection platform and collecting a novel radar dataset together with accurate position and instantaneous velocity measurements from lidar-based localization. In comparison to state-of-the-art baselines, DART synthesizes superior radar range-Doppler images from novel views across all datasets and additionally can be used to generate high quality tomographic images.</p></p class="citation"></blockquote><h3 id=2443--145271-redefining-cystoscopy-with-ai-bladder-cancer-diagnosis-using-an-efficient-hybrid-cnn-transformer-model-meryem-amaouche-et-al-2024>(24/43 | 145/271) Redefining cystoscopy with ai: bladder cancer diagnosis using an efficient hybrid cnn-transformer model (Meryem Amaouche et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Meryem Amaouche, Ouassim Karrakchou, Mounir Ghogho, Anouar El Ghazzaly, Mohamed Alami, Ahmed Ameur. (2024)<br><strong>Redefining cystoscopy with ai: bladder cancer diagnosis using an efficient hybrid cnn-transformer model</strong><br><button class=copy-to-clipboard title="Redefining cystoscopy with ai: bladder cancer diagnosis using an efficient hybrid cnn-transformer model" index=145>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-145 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Convolutional Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03879v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03879v1.pdf filename=2403.03879v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Bladder cancer ranks within the top 10 most diagnosed cancers worldwide and is among the most expensive cancers to treat due to the high recurrence rates which require lifetime follow-ups. The primary tool for diagnosis is cystoscopy, which heavily relies on doctors&rsquo; expertise and interpretation. Therefore, annually, numerous cases are either undiagnosed or misdiagnosed and treated as urinary infections. To address this, we suggest a deep learning approach for bladder cancer detection and segmentation which combines <b>CNNs</b> with a lightweight positional-encoding-free <b>transformer</b> and dual attention gates that fuse self and spatial attention for feature enhancement. The architecture suggested in this paper is efficient making it suitable for medical scenarios that require real time inference. Experiments have proven that this model addresses the critical need for a balance between computational efficiency and diagnostic accuracy in cystoscopic imaging as despite its small size it rivals large models in performance.</p></p class="citation"></blockquote><h3 id=2543--146271-temporal-enhanced-floating-car-observers-jeremias-gerner-et-al-2024>(25/43 | 146/271) Temporal Enhanced Floating Car Observers (Jeremias Gerner et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jeremias Gerner, Klaus Bogenberger, Stefanie Schmidtner. (2024)<br><strong>Temporal Enhanced Floating Car Observers</strong><br><button class=copy-to-clipboard title="Temporal Enhanced Floating Car Observers" index=146>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-146 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03825v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03825v1.pdf filename=2403.03825v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Floating Car Observers (FCOs) are an innovative method to collect traffic data by deploying sensor-equipped vehicles to detect and locate other vehicles. We demonstrate that even a small penetration rate of FCOs can identify a significant amount of vehicles at a given intersection. This is achieved through the emulation of detection within a microscopic traffic <b>simulation.</b> Additionally, leveraging data from previous moments can enhance the detection of vehicles in the current frame. Our findings indicate that, with a 20-second observation window, it is possible to recover up to 20% of vehicles that are not visible by FCOs in the current timestep. To exploit this, we developed a data-driven strategy, utilizing sequences of Bird&rsquo;s Eye View (BEV) representations of detected vehicles and deep learning models. This approach aims to bring currently undetected vehicles into view in the present moment, enhancing the currently detected vehicles. Results of different spatiotemporal architectures show that up to 41% of the vehicles can be recovered into the current timestep at their current position. This enhancement enriches the information initially available by the FCO, allowing an improved estimation of traffic states and metrics (e.g. density and queue length) for improved implementation of traffic management strategies.</p></p class="citation"></blockquote><h3 id=2643--147271-unifying-generation-and-compression-ultra-low-bitrate-image-coding-via-multi-stage-transformer-naifu-xue-et-al-2024>(26/43 | 147/271) Unifying Generation and Compression: Ultra-low bitrate Image Coding Via Multi-stage Transformer (Naifu Xue et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Naifu Xue, Qi Mao, Zijian Wang, Yuan Zhang, Siwei Ma. (2024)<br><strong>Unifying Generation and Compression: Ultra-low bitrate Image Coding Via Multi-stage Transformer</strong><br><button class=copy-to-clipboard title="Unifying Generation and Compression: Ultra-low bitrate Image Coding Via Multi-stage Transformer" index=147>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-147 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV, eess-IV<br>Keyword Score: 20<br>Keywords: Transformer, Tokenization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03736v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03736v1.pdf filename=2403.03736v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent progress in generative compression technology has significantly improved the perceptual quality of compressed data. However, these advancements primarily focus on producing high-frequency details, often overlooking the ability of generative models to capture the prior distribution of image content, thus impeding further bitrate reduction in extreme compression scenarios (&lt;0.05 bpp). Motivated by the capabilities of predictive language models for lossless compression, this paper introduces a novel Unified Image Generation-Compression (UIGC) paradigm, merging the processes of generation and compression. A key feature of the UIGC framework is the adoption of vector-quantized (VQ) image models for <b>tokenization,</b> alongside a multi-stage <b>transformer</b> designed to exploit spatial contextual information for modeling the prior distribution. As such, the dual-purpose framework effectively utilizes the learned prior for entropy estimation and assists in the regeneration of lost tokens. Extensive experiments demonstrate the superiority of the proposed UIGC framework over existing codecs in perceptual quality and human perception, particularly in ultra-low bitrate scenarios (&lt;=0.03 bpp), pioneering a new direction in generative compression.</p></p class="citation"></blockquote><h3 id=2743--148271-harnessing-meta-learning-for-improving-full-frame-video-stabilization-muhammad-kashif-ali-et-al-2024>(27/43 | 148/271) Harnessing Meta-Learning for Improving Full-Frame Video Stabilization (Muhammad Kashif Ali et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Muhammad Kashif Ali, Eun Woo Im, Dongjin Kim, Tae Hyun Kim. (2024)<br><strong>Harnessing Meta-Learning for Improving Full-Frame Video Stabilization</strong><br><button class=copy-to-clipboard title="Harnessing Meta-Learning for Improving Full-Frame Video Stabilization" index=148>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-148 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Fine-tuning, Meta Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03662v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03662v1.pdf filename=2403.03662v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Video stabilization is a longstanding computer vision problem, particularly pixel-level synthesis solutions for video stabilization which synthesize full frames add to the complexity of this task. These techniques aim to stabilize videos by synthesizing full frames while enhancing the stability of the considered video. This intensifies the complexity of the task due to the distinct mix of unique motion profiles and visual content present in each video sequence, making robust generalization with fixed parameters difficult. In our study, we introduce a novel approach to enhance the performance of pixel-level synthesis solutions for video stabilization by adapting these models to individual input video sequences. The proposed adaptation exploits low-level visual cues accessible during test-time to improve both the stability and quality of resulting videos. We highlight the efficacy of our methodology of &ldquo;test-time adaptation&rdquo; through simple <b>fine-tuning</b> of one of these models, followed by significant stability gain via the integration of <b>meta-learning</b> <b>techniques.</b> Notably, significant improvement is achieved with only a single adaptation step. The versatility of the proposed algorithm is demonstrated by consistently improving the performance of various pixel-level synthesis models for video stabilization in real-world scenarios.</p></p class="citation"></blockquote><h3 id=2843--149271-extend-your-own-correspondences-unsupervised-distant-point-cloud-registration-by-progressive-distance-extension-quan-liu-et-al-2024>(28/43 | 149/271) Extend Your Own Correspondences: Unsupervised Distant Point Cloud Registration by Progressive Distance Extension (Quan Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Quan Liu, Hongzi Zhu, Zhenxi Wang, Yunsong Zhou, Shan Chang, Minyi Guo. (2024)<br><strong>Extend Your Own Correspondences: Unsupervised Distant Point Cloud Registration by Progressive Distance Extension</strong><br><button class=copy-to-clipboard title="Extend Your Own Correspondences: Unsupervised Distant Point Cloud Registration by Progressive Distance Extension" index=149>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-149 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Supervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03532v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03532v1.pdf filename=2403.03532v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Registration of point clouds collected from a pair of distant vehicles provides a comprehensive and accurate 3D view of the driving scenario, which is vital for driving safety related applications, yet existing literature suffers from the expensive pose label acquisition and the deficiency to generalize to new data distributions. In this paper, we propose EYOC, an <b>unsupervised</b> distant point cloud registration method that adapts to new point cloud distributions on the fly, requiring no global pose labels. The core idea of EYOC is to train a feature extractor in a progressive fashion, where in each round, the feature extractor, trained with near point cloud pairs, can label slightly farther point cloud pairs, enabling self-supervision on such far point cloud pairs. This process continues until the derived extractor can be used to register distant point clouds. Particularly, to enable high-fidelity correspondence label generation, we devise an effective spatial filtering scheme to select the most representative correspondences to register a point cloud pair, and then utilize the aligned point clouds to discover more correct correspondences. Experiments show that EYOC can achieve comparable performance with state-of-the-art <b>supervised</b> methods at a lower training cost. Moreover, it outwits <b>supervised</b> methods regarding generalization performance on new data distributions.</p></p class="citation"></blockquote><h3 id=2943--150271-dcl-net-dual-contrastive-learning-network-for-semi-supervised-multi-organ-segmentation-lu-wen-et-al-2024>(29/43 | 150/271) Dcl-Net: Dual Contrastive Learning Network for Semi-Supervised Multi-Organ Segmentation (Lu Wen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lu Wen, Zhenghao Feng, Yun Hou, Peng Wang, Xi Wu, Jiliu Zhou, Yan Wang. (2024)<br><strong>Dcl-Net: Dual Contrastive Learning Network for Semi-Supervised Multi-Organ Segmentation</strong><br><button class=copy-to-clipboard title="Dcl-Net: Dual Contrastive Learning Network for Semi-Supervised Multi-Organ Segmentation" index=150>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-150 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Contrastive Learning, Semi-Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03512v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03512v1.pdf filename=2403.03512v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Semi-supervised</b> <b>learning</b> is a sound measure to relieve the strict demand of abundant annotated datasets, especially for challenging multi-organ segmentation . However, most existing SSL methods predict pixels in a single image independently, ignoring the relations among images and categories. In this paper, we propose a two-stage Dual <b>Contrastive</b> <b>Learning</b> Network for <b>semi-supervised</b> <b>MoS,</b> which utilizes global and local <b>contrastive</b> <b>learning</b> to strengthen the relations among images and classes. Concretely, in Stage 1, we develop a similarity-guided global <b>contrastive</b> <b>learning</b> to explore the implicit continuity and similarity among images and learn global context. Then, in Stage 2, we present an organ-aware local <b>contrastive</b> <b>learning</b> to further attract the class representations. To ease the computation burden, we introduce a mask center computation algorithm to compress the category representations for local <b>contrastive</b> <b>learning.</b> Experiments conducted on the public 2017 ACDC dataset and an in-house RC-OARs dataset has demonstrated the superior performance of our method.</p></p class="citation"></blockquote><h3 id=3043--151271-continual-segmentation-with-disentangled-objectness-learning-and-class-recognition-yizheng-gong-et-al-2024>(30/43 | 151/271) Continual Segmentation with Disentangled Objectness Learning and Class Recognition (Yizheng Gong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yizheng Gong, Siyue Yu, Xiaoyang Wang, Jimin Xiao. (2024)<br><strong>Continual Segmentation with Disentangled Objectness Learning and Class Recognition</strong><br><button class=copy-to-clipboard title="Continual Segmentation with Disentangled Objectness Learning and Class Recognition" index=151>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-151 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Continual Learning, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03477v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03477v1.pdf filename=2403.03477v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Most <b>continual</b> <b>segmentation</b> methods tackle the problem as a per-pixel classification task. However, such a paradigm is very challenging, and we find query-based segmenters with built-in objectness have inherent advantages compared with per-pixel ones, as objectness has strong transfer ability and forgetting resistance. Based on these findings, we propose CoMasTRe by disentangling <b>continual</b> <b>segmentation</b> into two stages: forgetting-resistant <b>continual</b> <b>objectness</b> learning and well-researched <b>continual</b> <b>classification.</b> CoMasTRe uses a two-stage segmenter learning class-agnostic mask proposals at the first stage and leaving recognition to the second stage. During <b>continual</b> <b>learning,</b> a simple but effective <b>distillation</b> is adopted to strengthen objectness. To further mitigate the forgetting of old classes, we design a multi-label class <b>distillation</b> strategy suited for segmentation. We assess the effectiveness of CoMasTRe on PASCAL VOC and ADE20K. Extensive experiments show that our method outperforms per-pixel and query-based methods on both datasets. Code will be available at <a href=https://github.com/jordangong/CoMasTRe>https://github.com/jordangong/CoMasTRe</a>.</p></p class="citation"></blockquote><h3 id=3143--152271-slot-abstractors-toward-scalable-abstract-visual-reasoning-shanka-subhra-mondal-et-al-2024>(31/43 | 152/271) Slot Abstractors: Toward Scalable Abstract Visual Reasoning (Shanka Subhra Mondal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shanka Subhra Mondal, Jonathan D. Cohen, Taylor W. Webb. (2024)<br><strong>Slot Abstractors: Toward Scalable Abstract Visual Reasoning</strong><br><button class=copy-to-clipboard title="Slot Abstractors: Toward Scalable Abstract Visual Reasoning" index=152>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-152 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 20<br>Keywords: Transformer, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03458v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03458v1.pdf filename=2403.03458v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Abstract visual <b>reasoning</b> is a characteristically human ability, allowing the identification of relational patterns that are abstracted away from object features, and the systematic generalization of those patterns to unseen problems. Recent work has demonstrated strong systematic generalization in visual <b>reasoning</b> tasks involving multi-object inputs, through the integration of slot-based methods used for extracting object-centric representations coupled with strong inductive biases for relational abstraction. However, this approach was limited to problems containing a single rule, and was not scalable to visual <b>reasoning</b> problems containing a large number of objects. Other recent work proposed Abstractors, an extension of <b>Transformers</b> that incorporates strong relational inductive biases, thereby inheriting the <b>Transformer&rsquo;s</b> scalability and multi-head architecture, but it has yet to be demonstrated how this approach might be applied to multi-object visual inputs. Here we combine the strengths of the above approaches and propose Slot Abstractors, an approach to abstract visual <b>reasoning</b> that can be scaled to problems involving a large number of objects and multiple relations among them. The approach displays state-of-the-art performance across four abstract visual <b>reasoning</b> tasks.</p></p class="citation"></blockquote><h3 id=3243--153271-a-density-guided-temporal-attention-transformer-for-indiscernible-object-counting-in-underwater-video-cheng-yen-yang-et-al-2024>(32/43 | 153/271) A Density-Guided Temporal Attention Transformer for Indiscernible Object Counting in Underwater Video (Cheng-Yen Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Cheng-Yen Yang, Hsiang-Wei Huang, Zhongyu Jiang, Hao Wang, Farron Wallace, Jenq-Neng Hwang. (2024)<br><strong>A Density-Guided Temporal Attention Transformer for Indiscernible Object Counting in Underwater Video</strong><br><button class=copy-to-clipboard title="A Density-Guided Temporal Attention Transformer for Indiscernible Object Counting in Underwater Video" index=153>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-153 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 16<br>Keywords: Benchmarking, Benchmarking, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03461v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03461v1.pdf filename=2403.03461v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Dense object counting or crowd counting has come a long way thanks to the recent development in the vision community. However, indiscernible object counting, which aims to count the number of targets that are blended with respect to their surroundings, has been a challenge. Image-based object counting datasets have been the mainstream of the current publicly available datasets. Therefore, we propose a large-scale dataset called YoutubeFish-35, which contains a total of 35 sequences of high-definition videos with high frame-per-second and more than 150,000 annotated center points across a selected variety of scenes. For <b>benchmarking</b> purposes, we select three mainstream methods for dense object counting and carefully evaluate them on the newly collected dataset. We propose TransVidCount, a new strong baseline that combines density and regression branches along the temporal domain in a unified framework and can effectively tackle indiscernible object counting with state-of-the-art performance on YoutubeFish-35 dataset.</p></p class="citation"></blockquote><h3 id=3343--154271-causality-based-cross-modal-representation-learning-for-vision-and-language-navigation-liuyi-wang-et-al-2024>(33/43 | 154/271) Causality-based Cross-Modal Representation Learning for Vision-and-Language Navigation (Liuyi Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Liuyi Wang, Zongtao He, Ronghao Dang, Huiyi Chen, Chengju Liu, Qijun Chen. (2024)<br><strong>Causality-based Cross-Modal Representation Learning for Vision-and-Language Navigation</strong><br><button class=copy-to-clipboard title="Causality-based Cross-Modal Representation Learning for Vision-and-Language Navigation" index=154>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-154 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 15<br>Keywords: Representation Learning, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03405v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03405v1.pdf filename=2403.03405v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Vision-and-Language</b> Navigation (VLN) has gained significant research interest in recent years due to its potential applications in real-world scenarios. However, existing VLN methods struggle with the issue of spurious associations, resulting in poor generalization with a significant performance gap between seen and unseen environments. In this paper, we tackle this challenge by proposing a unified framework CausalVLN based on the causal learning paradigm to train a robust navigator capable of learning unbiased feature <b>representations.</b> <b>Specifically,</b> we establish reasonable assumptions about confounders for vision and language in VLN using the structured causal model (SCM). Building upon this, we propose an iterative backdoor-based <b>representation</b> <b>learning</b> (IBRL) method that allows for the adaptive and effective intervention on confounders. Furthermore, we introduce the visual and linguistic backdoor causal encoders to enable unbiased feature expression for multi-modalities during training and validation, enhancing the agent&rsquo;s capability to generalize across different environments. Experiments on three VLN datasets (R2R, RxR, and REVERIE) showcase the superiority of our proposed method over previous state-of-the-art approaches. Moreover, detailed visualization analysis demonstrates the effectiveness of CausalVLN in significantly narrowing down the performance gap between seen and unseen environments, underscoring its strong generalization capability.</p></p class="citation"></blockquote><h3 id=3443--155271-vasttrack-vast-category-visual-object-tracking-liang-peng-et-al-2024>(34/43 | 155/271) VastTrack: Vast Category Visual Object Tracking (Liang Peng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Liang Peng, Junyuan Gao, Xinran Liu, Weihong Li, Shaohua Dong, Zhipeng Zhang, Heng Fan, Libo Zhang. (2024)<br><strong>VastTrack: Vast Category Visual Object Tracking</strong><br><button class=copy-to-clipboard title="VastTrack: Vast Category Visual Object Tracking" index=155>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-155 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03493v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03493v1.pdf filename=2403.03493v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we introduce a novel <b>benchmark,</b> dubbed VastTrack, towards facilitating the development of more general visual tracking via encompassing abundant classes and videos. VastTrack possesses several attractive properties: (1) Vast Object Category. In particular, it covers target objects from 2,115 classes, largely surpassing object categories of existing popular <b>benchmarks</b> (e.g., GOT-10k with 563 classes and LaSOT with 70 categories). With such vast object classes, we expect to learn more general object tracking. (2) Larger scale. Compared with current <b>benchmarks,</b> VastTrack offers 50,610 sequences with 4.2 million frames, which makes it to date the largest <b>benchmark</b> regarding the number of videos, and thus could benefit training even more powerful visual trackers in the deep learning era. (3) Rich Annotation. Besides conventional bounding box annotations, VastTrack also provides linguistic descriptions for the videos. The rich annotations of VastTrack enables development of both the vision-only and the <b>vision-language</b> tracking. To ensure precise annotation, all videos are manually labeled with multiple rounds of careful inspection and refinement. To understand performance of existing trackers and to provide baselines for future comparison, we extensively assess 25 representative trackers. The results, not surprisingly, show significant drops compared to those on current datasets due to lack of abundant categories and videos from diverse scenarios for training, and more efforts are required to improve general tracking. Our VastTrack and all the evaluation results will be made publicly available <a href=https://github.com/HengLan/VastTrack>https://github.com/HengLan/VastTrack</a>.</p></p class="citation"></blockquote><h3 id=3543--156271-video-relationship-detection-using-mixture-of-experts-ala-shaabana-et-al-2024>(35/43 | 156/271) Video Relationship Detection Using Mixture of Experts (Ala Shaabana et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ala Shaabana, Zahra Gharaee, Paul Fieguth. (2024)<br><strong>Video Relationship Detection Using Mixture of Experts</strong><br><button class=copy-to-clipboard title="Video Relationship Detection Using Mixture of Experts" index=156>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-156 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 10<br>Keywords: Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03994v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03994v1.pdf filename=2403.03994v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Machine comprehension of visual information from images and videos by neural networks faces two primary challenges. Firstly, there exists a computational and inference gap in connecting vision and language, making it difficult to accurately determine which object a given agent acts on and represent it through language. Secondly, classifiers trained by a single, monolithic neural network often lack stability and generalization. To overcome these challenges, we introduce MoE-VRD, a novel approach to visual relationship detection utilizing a mixture of experts. MoE-VRD identifies language triplets in the form of &lt; subject, predicate, object> tuples to extract relationships from visual processing. Leveraging recent advancements in visual relationship detection, MoE-VRD addresses the requirement for action recognition in establishing relationships between subjects (acting) and objects (being acted upon). In contrast to single monolithic networks, MoE-VRD employs multiple small models as experts, whose outputs are aggregated. Each expert in MoE-VRD specializes in visual relationship learning and object tagging. By utilizing a sparsely-gated mixture of experts, MoE-VRD enables conditional computation and significantly enhances neural network capacity without increasing computational complexity. Our experimental results demonstrate that the conditional computation capabilities and scalability of the mixture-of-experts approach lead to superior performance in visual relationship detection compared to state-of-the-art methods.</p></p class="citation"></blockquote><h3 id=3643--157271-investigation-of-the-impact-of-synthetic-training-data-in-the-industrial-application-of-terminal-strip-object-detection-nico-baumgart-et-al-2024>(36/43 | 157/271) Investigation of the Impact of Synthetic Training Data in the Industrial Application of Terminal Strip Object Detection (Nico Baumgart et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nico Baumgart, Markus Lange-Hegermann, Mike Mücke. (2024)<br><strong>Investigation of the Impact of Synthetic Training Data in the Industrial Application of Terminal Strip Object Detection</strong><br><button class=copy-to-clipboard title="Investigation of the Impact of Synthetic Training Data in the Industrial Application of Terminal Strip Object Detection" index=157>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-157 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 10<br>Keywords: Object Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04809v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04809v1.pdf filename=2403.04809v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In industrial manufacturing, numerous tasks of visually inspecting or detecting specific <b>objects</b> <b>exist</b> that are currently performed manually or by classical image processing methods. Therefore, introducing recent deep learning models to industrial environments holds the potential to increase productivity and enable new applications. However, gathering and labeling sufficient data is often intractable, complicating the implementation of such projects. Hence, image synthesis methods are commonly used to generate synthetic training data from 3D models and annotate them automatically, although it results in a sim-to-real domain gap. In this paper, we investigate the sim-to-real generalization performance of standard <b>object</b> <b>detectors</b> on the complex industrial application of terminal strip <b>object</b> <b>detection.</b> Combining domain randomization and domain knowledge, we created an image synthesis pipeline for automatically generating the training data. Moreover, we manually annotated 300 real images of terminal strips for the evaluation. The results show the cruciality of the <b>objects</b> <b>of</b> interest to have the same scale in either domain. Nevertheless, under optimized scaling conditions, the sim-to-real performance difference in mean average precision amounts to 2.69 % for RetinaNet and 0.98 % for Faster R-CNN, qualifying this approach for industrial requirements.</p></p class="citation"></blockquote><h3 id=3743--158271-learning-3d-object-centric-representation-through-prediction-john-day-et-al-2024>(37/43 | 158/271) Learning 3D object-centric representation through prediction (John Day et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>John Day, Tushar Arora, Jirui Liu, Li Erran Li, Ming Bo Cai. (2024)<br><strong>Learning 3D object-centric representation through prediction</strong><br><button class=copy-to-clipboard title="Learning 3D object-centric representation through prediction" index=158>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-158 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: I-2-10; I-4-8; I-4-6; I-4-10; I-2-6, cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03730v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03730v1.pdf filename=2403.03730v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As part of human core knowledge, the representation of objects is the building block of mental representation that supports high-level concepts and symbolic <b>reasoning.</b> While humans develop the ability of perceiving objects situated in 3D environments without supervision, models that learn the same set of abilities with similar constraints faced by human infants are lacking. Towards this end, we developed a novel network architecture that simultaneously learns to 1) segment objects from discrete images, 2) infer their 3D locations, and 3) perceive depth, all while using only information directly available to the brain as training data, namely: sequences of images and self-motion. The core idea is treating objects as latent causes of visual input which the brain uses to make efficient predictions of future scenes. This results in object representations being learned as an essential byproduct of learning to predict.</p></p class="citation"></blockquote><h3 id=3843--159271-portraying-the-need-for-temporal-data-in-flood-detection-via-sentinel-1-xavier-bou-et-al-2024>(38/43 | 159/271) Portraying the Need for Temporal Data in Flood Detection via Sentinel-1 (Xavier Bou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xavier Bou, Thibaud Ehret, Rafael Grompone von Gioi, Jeremy Anger. (2024)<br><strong>Portraying the Need for Temporal Data in Flood Detection via Sentinel-1</strong><br><button class=copy-to-clipboard title="Portraying the Need for Temporal Data in Flood Detection via Sentinel-1" index=159>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-159 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV<br>Keyword Score: 10<br>Keywords: Anomaly Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03671v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03671v1.pdf filename=2403.03671v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Identifying flood affected areas in remote sensing data is a critical problem in earth observation to analyze flood impact and drive responses. While a number of methods have been proposed in the literature, there are two main limitations in available flood detection datasets: (1) a lack of region variability is commonly observed and/or (2) they require to distinguish permanent water bodies from flooded areas from a single image, which becomes an ill-posed setup. Consequently, we extend the globally diverse MMFlood dataset to multi-date by providing one year of Sentinel-1 observations around each flood event. To our surprise, we notice that the definition of flooded pixels in MMFlood is inconsistent when observing the entire image sequence. Hence, we re-frame the flood detection task as a temporal <b>anomaly</b> <b>detection</b> problem, where anomalous water bodies are segmented from a Sentinel-1 temporal sequence. From this definition, we provide a simple method inspired by the popular video change detector ViBe, results of which quantitatively align with the SAR image time series, providing a reasonable baseline for future works.</p></p class="citation"></blockquote><h3 id=3943--160271-multi-task-learning-for-real-time-autonomous-driving-leveraging-task-adaptive-attention-generator-wonhyeok-choi-et-al-2024>(39/43 | 160/271) Multi-task Learning for Real-time Autonomous Driving Leveraging Task-adaptive Attention Generator (Wonhyeok Choi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wonhyeok Choi, Mingyu Shin, Hyukzae Lee, Jaehoon Cho, Jaehyeon Park, Sunghoon Im. (2024)<br><strong>Multi-task Learning for Real-time Autonomous Driving Leveraging Task-adaptive Attention Generator</strong><br><button class=copy-to-clipboard title="Multi-task Learning for Real-time Autonomous Driving Leveraging Task-adaptive Attention Generator" index=160>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-160 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Object Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03468v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03468v1.pdf filename=2403.03468v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Real-time processing is crucial in autonomous driving systems due to the imperative of instantaneous decision-making and rapid response. In real-world scenarios, autonomous vehicles are continuously tasked with interpreting their surroundings, analyzing intricate sensor data, and making decisions within split seconds to ensure safety through numerous computer vision tasks. In this paper, we present a new real-time multi-task network adept at three vital autonomous driving tasks: monocular 3D <b>object</b> <b>detection,</b> semantic segmentation, and dense depth estimation. To counter the challenge of negative transfer, which is the prevalent issue in multi-task learning, we introduce a task-adaptive attention generator. This generator is designed to automatically discern interrelations across the three tasks and arrange the task-sharing pattern, all while leveraging the efficiency of the hard-parameter sharing approach. To the best of our knowledge, the proposed model is pioneering in its capability to concurrently handle multiple tasks, notably 3D <b>object</b> <b>detection,</b> while maintaining real-time processing speeds. Our rigorously optimized network, when tested on the Cityscapes-3D datasets, consistently outperforms various baseline models. Moreover, an in-depth ablation study substantiates the efficacy of the methodologies integrated into our framework.</p></p class="citation"></blockquote><h3 id=4043--161271-d4c-glove-train-solving-the-rpm-and-bongard-logo-problem-by-distributing-and-circumscribing-concepts-ruizhuo-song-et-al-2024>(40/43 | 161/271) D4C glove-train: solving the RPM and Bongard-logo problem by distributing and Circumscribing concepts (Ruizhuo Song et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruizhuo Song, Beiming Yuan. (2024)<br><strong>D4C glove-train: solving the RPM and Bongard-logo problem by distributing and Circumscribing concepts</strong><br><button class=copy-to-clipboard title="D4C glove-train: solving the RPM and Bongard-logo problem by distributing and Circumscribing concepts" index=161>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-161 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03452v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03452v2.pdf filename=2403.03452v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper achieves significant progress in the field of abstract <b>reasoning,</b> particularly in addressing Raven&rsquo;s Progressive Matrices (RPM) and Bongard-Logo problems. We propose the D2C approach, which redefines conceptual boundaries in these domains and bridges the gap between high-level concepts and their low-dimensional representations. Based on this, we further introduce the D3C method that handles Bongard-Logo problems and significantly improves <b>reasoning</b> accuracy by estimating the distribution of image representations and measuring their Sinkhorn distance. To enhance computational efficiency, we introduce the D3C-cos variant, which provides an efficient and accurate solution for RPM problems by constraining distribution distances. Additionally, we present Lico-Net, a network that combines D3C and D3C-cos to achieve state-of-the-art performance in both problem-solving and interpretability. Finally, we extend our approach to D4C, employing adversarial strategies to further refine conceptual boundaries and demonstrate notable improvements for both RPM and Bongard-Logo problems. Overall, our contributions offer a new perspective and practical solutions to the field of abstract <b>reasoning.</b></p></p class="citation"></blockquote><h3 id=4143--162271-scene-depth-estimation-from-traditional-oriental-landscape-paintings-sungho-kang-et-al-2024>(41/43 | 162/271) Scene Depth Estimation from Traditional Oriental Landscape Paintings (Sungho Kang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sungho Kang, YeongHyeon Park, Hyunkyu Park, Juneho Yi. (2024)<br><strong>Scene Depth Estimation from Traditional Oriental Landscape Paintings</strong><br><button class=copy-to-clipboard title="Scene Depth Estimation from Traditional Oriental Landscape Paintings" index=162>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-162 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03408v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03408v2.pdf filename=2403.03408v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Scene depth estimation from paintings can streamline the process of 3D sculpture creation so that visually impaired people appreciate the paintings with tactile sense. However, measuring depth of oriental landscape painting images is extremely challenging due to its unique method of depicting depth and poor preservation. To address the problem of scene depth estimation from oriental landscape painting images, we propose a novel framework that consists of two-step Image-to-Image translation method with CLIP-based image matching at the front end to predict the real scene image that best matches with the given oriental landscape painting image. Then, we employ a pre-trained SOTA depth estimation model for the generated real scene image. In the first step, CycleGAN converts an oriental landscape painting image into a pseudo-real scene image. We utilize CLIP to semantically match landscape photo images with an oriental landscape painting image for training CycleGAN in an <b>unsupervised</b> manner. Then, the pseudo-real scene image and oriental landscape painting image are fed into DiffuseIT to predict a final real scene image in the second step. Finally, we measure depth of the generated real scene image using a pre-trained depth estimation model such as MiDaS. Experimental results show that our approach performs well enough to predict real scene images corresponding to oriental landscape painting images. To the best of our knowledge, this is the first study to measure the depth of oriental landscape painting images. Our research potentially assists visually impaired people in experiencing paintings in diverse ways. We will release our code and resulting dataset.</p></p class="citation"></blockquote><h3 id=4243--163271-gsnerf-generalizable-semantic-neural-radiance-fields-with-enhanced-3d-scene-understanding-zi-ting-chou-et-al-2024>(42/43 | 163/271) GSNeRF: Generalizable Semantic Neural Radiance Fields with Enhanced 3D Scene Understanding (Zi-Ting Chou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zi-Ting Chou, Sheng-Yu Huang, I-Jieh Liu, Yu-Chiang Frank Wang. (2024)<br><strong>GSNeRF: Generalizable Semantic Neural Radiance Fields with Enhanced 3D Scene Understanding</strong><br><button class=copy-to-clipboard title="GSNeRF: Generalizable Semantic Neural Radiance Fields with Enhanced 3D Scene Understanding" index=163>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-163 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03608v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03608v1.pdf filename=2403.03608v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Utilizing multi-view inputs to synthesize novel-view images, Neural Radiance Fields (NeRF) have emerged as a popular research topic in 3D vision. In this work, we introduce a Generalizable Semantic Neural Radiance Field (GSNeRF), which uniquely takes image semantics into the synthesis process so that both novel view images and the associated semantic maps can be produced for unseen scenes. Our GSNeRF is composed of two stages: Semantic Geo-Reasoning and Depth-Guided Visual rendering. The former is able to observe multi-view image inputs to extract semantic and <b>geometry</b> features from a scene. Guided by the resulting image <b>geometry</b> information, the latter performs both image and semantic rendering with improved performances. Our experiments not only confirm that GSNeRF performs favorably against prior works on both novel-view image and semantic segmentation synthesis but the effectiveness of our sampling strategy for visual rendering is further verified.</p></p class="citation"></blockquote><h3 id=4343--164271-hdrflow-real-time-hdr-video-reconstruction-with-large-motions-gangwei-xu-et-al-2024>(43/43 | 164/271) HDRFlow: Real-Time HDR Video Reconstruction with Large Motions (Gangwei Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gangwei Xu, Yujin Wang, Jinwei Gu, Tianfan Xue, Xin Yang. (2024)<br><strong>HDRFlow: Real-Time HDR Video Reconstruction with Large Motions</strong><br><button class=copy-to-clipboard title="HDRFlow: Real-Time HDR Video Reconstruction with Large Motions" index=164>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-164 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03447v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03447v1.pdf filename=2403.03447v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Reconstructing High Dynamic Range (HDR) video from image sequences captured with alternating exposures is challenging, especially in the presence of large camera or object motion. Existing methods typically align low dynamic range sequences using optical flow or attention mechanism for deghosting. However, they often struggle to handle large complex motions and are computationally expensive. To address these challenges, we propose a robust and efficient flow estimator tailored for real-time HDR video reconstruction, named HDRFlow. HDRFlow has three novel designs: an HDR-domain alignment loss (HALoss), an efficient flow network with a multi-size large kernel (MLK), and a new HDR flow training scheme. The HALoss supervises our flow network to learn an HDR-oriented flow for accurate alignment in saturated and dark regions. The MLK can effectively model large motions at a negligible cost. In addition, we incorporate synthetic data, Sintel, into our training dataset, utilizing both its provided forward flow and backward flow generated by us to supervise our flow network, enhancing our performance in large motion regions. Extensive experiments demonstrate that our HDRFlow outperforms previous methods on standard <b>benchmarks.</b> To the best of our knowledge, HDRFlow is the first real-time HDR video reconstruction method for video sequences captured with alternating exposures, capable of processing 720p resolution inputs at 25ms.</p></p class="citation"></blockquote><h2 id=csir-7>cs.IR (7)</h2><h3 id=17--165271-intent-aware-recommendation-via-disentangled-graph-contrastive-learning-yuling-wang-et-al-2024>(1/7 | 165/271) Intent-aware Recommendation via Disentangled Graph Contrastive Learning (Yuling Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuling Wang, Xiao Wang, Xiangzhou Huang, Yanhua Yu, Haoyang Li, Mengdi Zhang, Zirui Guo, Wei Wu. (2024)<br><strong>Intent-aware Recommendation via Disentangled Graph Contrastive Learning</strong><br><button class=copy-to-clipboard title="Intent-aware Recommendation via Disentangled Graph Contrastive Learning" index=165>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-165 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 63<br>Keywords: Graph, Graph Contrastive Learning, Graph Neural Network, Graph Neural Network, Contrastive Learning, Recommendation, Recommender System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03714v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03714v1.pdf filename=2403.03714v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>neural</b> <b>network</b> <b>(GNN)</b> based <b>recommender</b> <b>systems</b> have become one of the mainstream trends due to the powerful learning ability from user behavior data. Understanding the user intents from behavior data is the key to <b>recommender</b> <b>systems,</b> which poses two basic requirements for <b>GNN-based</b> <b>recommender</b> <b>systems.</b> One is how to learn complex and diverse intents especially when the user behavior is usually inadequate in reality. The other is different behaviors have different intent distributions, so how to establish their relations for a more explainable <b>recommender</b> <b>system.</b> In this paper, we present the Intent-aware <b>Recommendation</b> via Disentangled <b>Graph</b> <b>Contrastive</b> <b>Learning</b> (IDCL), which simultaneously learns interpretable intents and behavior distributions over those intents. Specifically, we first model the user behavior data as a user-item-concept <b>graph,</b> <b>and</b> <b>design</b> a <b>GNN</b> based behavior disentangling module to learn the different intents. Then we propose the intent-wise <b>contrastive</b> <b>learning</b> to enhance the intent disentangling and meanwhile infer the behavior distributions. Finally, the coding rate reduction regularization is introduced to make the behaviors of different intents orthogonal. Extensive experiments demonstrate the effectiveness of IDCL in terms of substantial improvement and the interpretability.</p></p class="citation"></blockquote><h3 id=27--166271-towards-efficient-and-effective-unlearning-of-large-language-models-for-recommendation-hangyu-wang-et-al-2024>(2/7 | 166/271) Towards Efficient and Effective Unlearning of Large Language Models for Recommendation (Hangyu Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hangyu Wang, Jianghao Lin, Bo Chen, Yang Yang, Ruiming Tang, Weinan Zhang, Yong Yu. (2024)<br><strong>Towards Efficient and Effective Unlearning of Large Language Models for Recommendation</strong><br><button class=copy-to-clipboard title="Towards Efficient and Effective Unlearning of Large Language Models for Recommendation" index=166>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-166 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-AI, cs-IR, cs.IR<br>Keyword Score: 50<br>Keywords: Recommendation, Reasoning, Instruction Tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03536v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03536v1.pdf filename=2403.03536v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The significant advancements in <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> give rise to a promising research direction, i.e., leveraging <b>LLMs</b> as recommenders (LLMRec). The efficacy of LLMRec arises from the open-world knowledge and <b>reasoning</b> capabilities inherent in <b>LLMs.</b> LLMRec acquires the <b>recommendation</b> capabilities through <b>instruction</b> <b>tuning</b> based on user interaction data. However, in order to protect user privacy and optimize utility, it is also crucial for LLMRec to intentionally forget specific user data, which is generally referred to as <b>recommendation</b> unlearning. In the era of <b>LLMs,</b> <b>recommendation</b> unlearning poses new challenges for LLMRec in terms of \textit{inefficiency} and \textit{ineffectiveness}. Existing unlearning methods require updating billions of parameters in LLMRec, which is costly and time-consuming. Besides, they always impact the model utility during the unlearning process. To this end, we propose \textbf{E2URec}, the first \underline{E}fficient and \underline{E}ffective \underline{U}nlearning method for LLM\underline{Rec}. Our proposed E2URec enhances the unlearning efficiency by updating only a few additional LoRA parameters, and improves the unlearning effectiveness by employing a teacher-student framework, where we maintain multiple teacher networks to guide the unlearning process. Extensive experiments show that E2URec outperforms state-of-the-art baselines on two real-world datasets. Specifically, E2URec can efficiently forget specific data without affecting <b>recommendation</b> performance. The source code is at \url{https://github.com/justarter/E2URec}.</p></p class="citation"></blockquote><h3 id=37--167271-bridging-language-and-items-for-retrieval-and-recommendation-yupeng-hou-et-al-2024>(3/7 | 167/271) Bridging Language and Items for Retrieval and Recommendation (Yupeng Hou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yupeng Hou, Jiacheng Li, Zhankui He, An Yan, Xiusi Chen, Julian McAuley. (2024)<br><strong>Bridging Language and Items for Retrieval and Recommendation</strong><br><button class=copy-to-clipboard title="Bridging Language and Items for Retrieval and Recommendation" index=167>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-167 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 40<br>Keywords: Recommendation, ChatGPT, Sentence Embedding, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03952v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03952v1.pdf filename=2403.03952v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces BLaIR, a series of pretrained <b>sentence</b> <b>embedding</b> models specialized for <b>recommendation</b> scenarios. BLaIR is trained to learn correlations between item metadata and potential natural language context, which is useful for retrieving and recommending items. To pretrain BLaIR, we collect Amazon Reviews 2023, a new dataset comprising over 570 million reviews and 48 million items from 33 categories, significantly expanding beyond the scope of previous versions. We evaluate the generalization ability of BLaIR across multiple domains and tasks, including a new task named complex product search, referring to retrieving relevant items given long, complex natural language contexts. Leveraging <b>large</b> <b>language</b> <b>models</b> like <b>ChatGPT,</b> we correspondingly construct a semi-synthetic evaluation set, Amazon-C4. Empirical results on the new task, as well as conventional retrieval and <b>recommendation</b> tasks, demonstrate that BLaIR exhibit strong text and item representation capacity. Our datasets, code, and checkpoints are available at: <a href=https://github.com/hyp1231/AmazonReviews2023>https://github.com/hyp1231/AmazonReviews2023</a>.</p></p class="citation"></blockquote><h3 id=47--168271-generative-news-recommendation-shen-gao-et-al-2024>(4/7 | 168/271) Generative News Recommendation (Shen Gao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shen Gao, Jiabao Fang, Quan Tu, Zhitao Yao, Zhumin Chen, Pengjie Ren, Zhaochun Ren. (2024)<br><strong>Generative News Recommendation</strong><br><button class=copy-to-clipboard title="Generative News Recommendation" index=168>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-168 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 40<br>Keywords: Recommendation, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03424v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03424v1.pdf filename=2403.03424v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Most existing news <b>recommendation</b> methods tackle this task by conducting semantic matching between candidate news and user representation produced by historical clicked news. However, they overlook the high-level connections among different news articles and also ignore the profound relationship between these news articles and users. And the definition of these methods dictates that they can only deliver news articles as-is. On the contrary, integrating several relevant news articles into a coherent narrative would assist users in gaining a quicker and more comprehensive understanding of events. In this paper, we propose a novel generative news <b>recommendation</b> paradigm that includes two steps: (1) Leveraging the internal knowledge and <b>reasoning</b> capabilities of the <b>Large</b> <b>Language</b> <b>Model</b> <b>(LLM)</b> to perform high-level matching between candidate news and user representation; (2) Generating a coherent and logically structured narrative based on the associations between related news and user interests, thus engaging users in further reading of the news. Specifically, we propose GNR to implement the generative news <b>recommendation</b> paradigm. First, we compose the dual-level representation of news and users by leveraging <b>LLM</b> to generate theme-level representations and combine them with semantic-level representations. Next, in order to generate a coherent narrative, we explore the news relation and filter the related news according to the user preference. Finally, we propose a novel training method named UIFT to train the <b>LLM</b> to fuse multiple news articles in a coherent narrative. Extensive experiments show that GNR can improve <b>recommendation</b> accuracy and eventually generate more personalized and factually consistent narratives.</p></p class="citation"></blockquote><h3 id=57--169271-personalized-negative-reservoir-for-incremental-learning-in-recommender-systems-antonios-valkanas-et-al-2024>(5/7 | 169/271) Personalized Negative Reservoir for Incremental Learning in Recommender Systems (Antonios Valkanas et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Antonios Valkanas, Yuening Wang, Yingxue Zhang, Mark Coates. (2024)<br><strong>Personalized Negative Reservoir for Incremental Learning in Recommender Systems</strong><br><button class=copy-to-clipboard title="Personalized Negative Reservoir for Incremental Learning in Recommender Systems" index=169>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-169 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-AI, cs-IR, cs.IR<br>Keyword Score: 33<br>Keywords: Benchmarking, Fine-tuning, Recommendation, Recommender System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03993v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03993v1.pdf filename=2403.03993v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Recommender</b> <b>systems</b> have become an integral part of online platforms. Every day the volume of training data is expanding and the number of user interactions is constantly increasing. The exploration of larger and more expressive models has become a necessary pursuit to improve user experience. However, this progression carries with it an increased computational burden. In commercial settings, once a <b>recommendation</b> system model has been trained and deployed it typically needs to be updated frequently as new client data arrive. Cumulatively, the mounting volume of data is guaranteed to eventually make full batch retraining of the model from scratch computationally infeasible. Naively <b>fine-tuning</b> solely on the new data runs into the well-documented problem of catastrophic forgetting. Despite the fact that negative sampling is a crucial part of training with implicit feedback, no specialized technique exists that is tailored to the incremental learning framework. In this work, we take the first step to propose, a personalized negative reservoir strategy which is used to obtain negative samples for the standard triplet loss. This technique balances alleviation of forgetting with plasticity by encouraging the model to remember stable user preferences and selectively forget when user interests change. We derive the mathematical formulation of a negative sampler to populate and update the reservoir. We integrate our design in three SOTA and commonly used incremental <b>recommendation</b> models. We show that these concrete realizations of our negative reservoir framework achieve state-of-the-art results in standard <b>benchmarks,</b> on multiple standard top-k evaluation metrics.</p></p class="citation"></blockquote><h3 id=67--170271-backtracing-retrieving-the-cause-of-the-query-rose-e-wang-et-al-2024>(6/7 | 170/271) Backtracing: Retrieving the Cause of the Query (Rose E. Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rose E. Wang, Pawan Wirawarn, Omar Khattab, Noah Goodman, Dorottya Demszky. (2024)<br><strong>Backtracing: Retrieving the Cause of the Query</strong><br><button class=copy-to-clipboard title="Backtracing: Retrieving the Cause of the Query" index=170>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-170 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-CL, cs-IR, cs.IR<br>Keyword Score: 33<br>Keywords: Benchmarking, Zero-shot, ChatGPT, Information Retrieval<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03956v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03956v1.pdf filename=2403.03956v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Many online content portals allow users to ask questions to supplement their understanding (e.g., of lectures). While <b>information</b> <b>retrieval</b> (IR) systems may provide answers for such user queries, they do not directly assist content creators &ndash; such as lecturers who want to improve their content &ndash; identify segments that <em>caused</em> a user to ask those questions. We introduce the task of backtracing, in which systems retrieve the text segment that most likely caused a user query. We formalize three real-world domains for which backtracing is important in improving content delivery and communication: understanding the cause of (a) student confusion in the Lecture domain, (b) reader curiosity in the News Article domain, and (c) user emotion in the Conversation domain. We evaluate the <b>zero-shot</b> performance of popular <b>information</b> <b>retrieval</b> methods and language modeling methods, including bi-encoder, re-ranking and likelihood-based methods and <b>ChatGPT.</b> While traditional IR systems retrieve semantically relevant <b>information</b> <b>(e.g.,</b> details on &ldquo;projection matrices&rdquo; for a query &ldquo;does projecting multiple times still lead to the same point?&rdquo;), they often miss the causally relevant context (e.g., the lecturer states &ldquo;projecting twice gets me the same answer as one projection&rdquo;). Our results show that there is room for improvement on backtracing and it requires new retrieval approaches. We hope our <b>benchmark</b> serves to improve future retrieval systems for backtracing, spawning systems that refine content generation and identify linguistic triggers influencing user queries. Our code and data are open-sourced: <a href=https://github.com/rosewang2008/backtracing>https://github.com/rosewang2008/backtracing</a>.</p></p class="citation"></blockquote><h3 id=77--171271-mamba4rec-towards-efficient-sequential-recommendation-with-selective-state-space-models-chengkai-liu-et-al-2024>(7/7 | 171/271) Mamba4Rec: Towards Efficient Sequential Recommendation with Selective State Space Models (Chengkai Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chengkai Liu, Jianghao Lin, Jianling Wang, Hanzhou Liu, James Caverlee. (2024)<br><strong>Mamba4Rec: Towards Efficient Sequential Recommendation with Selective State Space Models</strong><br><button class=copy-to-clipboard title="Mamba4Rec: Towards Efficient Sequential Recommendation with Selective State Space Models" index=171>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-171 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 30<br>Keywords: Recommendation, Transformer, Stemming<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03900v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03900v1.pdf filename=2403.03900v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Sequential <b>recommendation</b> aims to estimate the dynamic user preferences and sequential dependencies among historical user behaviors. Although <b>Transformer-based</b> models have proven to be effective for sequential <b>recommendation,</b> they suffer from the inference inefficiency problem <b>stemming</b> from the quadratic computational complexity of attention operators, especially for long-range behavior sequences. Inspired by the recent success of state space models (SSMs), we propose Mamba4Rec, which is the first work to explore the potential of selective SSMs for efficient sequential <b>recommendation.</b> Built upon the basic Mamba block which is a selective SSM with an efficient hardware-aware parallel algorithm, we incorporate a series of sequential modeling techniques to further promote the model performance and meanwhile maintain the inference efficiency. Experiments on two public datasets demonstrate that Mamba4Rec is able to well address the effectiveness-efficiency dilemma, and defeat both RNN- and attention-based baselines in terms of both effectiveness and efficiency.</p></p class="citation"></blockquote><h2 id=csro-15>cs.RO (15)</h2><h3 id=115--172271-on-device-self-supervised-learning-of-visual-perception-tasks-aboard-hardware-limited-nano-quadrotors-elia-cereda-et-al-2024>(1/15 | 172/271) On-device Self-supervised Learning of Visual Perception Tasks aboard Hardware-limited Nano-quadrotors (Elia Cereda et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Elia Cereda, Manuele Rusci, Alessandro Giusti, Daniele Palossi. (2024)<br><strong>On-device Self-supervised Learning of Visual Perception Tasks aboard Hardware-limited Nano-quadrotors</strong><br><button class=copy-to-clipboard title="On-device Self-supervised Learning of Visual Perception Tasks aboard Hardware-limited Nano-quadrotors" index=172>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-172 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs.RO<br>Keyword Score: 60<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, Fine-tuning, Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04071v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04071v1.pdf filename=2403.04071v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Sub-\SI{50}{\gram} nano-drones are gaining momentum in both academia and industry. Their most compelling applications rely on onboard deep learning models for perception despite severe hardware constraints (\ie sub-\SI{100}{\milli\watt} processor). When deployed in unknown environments not represented in the training data, these models often underperform due to domain shift. To cope with this fundamental problem, we propose, for the first time, on-device learning aboard nano-drones, where the first part of the in-field mission is dedicated to <b>self-supervised</b> <b>fine-tuning</b> of a pre-trained <b>convolutional</b> <b>neural</b> <b>network</b> <b>(CNN).</b> Leveraging a real-world vision-based regression task, we thoroughly explore performance-cost trade-offs of the <b>fine-tuning</b> phase along three axes: \textit{i}) dataset size (more data increases the regression performance but requires more memory and longer computation); \textit{ii}) methodologies (\eg <b>fine-tuning</b> all model parameters vs. only a subset); and \textit{iii}) self-supervision strategy. Our approach demonstrates an improvement in mean absolute error up to 30% compared to the pre-trained baseline, requiring only \SI{22}{\second} <b>fine-tuning</b> on an ultra-low-power GWT GAP9 System-on-Chip. Addressing the domain shift problem via on-device learning aboard nano-drones not only marks a novel result for hardware-limited robots but lays the ground for more general advancements for the entire robotics community.</p></p class="citation"></blockquote><h3 id=215--173271-reconciling-reality-through-simulation-a-real-to-sim-to-real-approach-for-robust-manipulation-marcel-torne-et-al-2024>(2/15 | 173/271) Reconciling Reality through Simulation: A Real-to-Sim-to-Real Approach for Robust Manipulation (Marcel Torne et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marcel Torne, Anthony Simeonov, Zechu Li, April Chan, Tao Chen, Abhishek Gupta, Pulkit Agrawal. (2024)<br><strong>Reconciling Reality through Simulation: A Real-to-Sim-to-Real Approach for Robust Manipulation</strong><br><button class=copy-to-clipboard title="Reconciling Reality through Simulation: A Real-to-Sim-to-Real Approach for Robust Manipulation" index=173>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-173 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-LG, cs-RO, cs.RO<br>Keyword Score: 60<br>Keywords: Fine-tuning, Human Intervention, Knowledge Distillation, Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03949v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03949v1.pdf filename=2403.03949v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Imitation learning methods need significant <b>human</b> <b>supervision</b> to learn policies robust to changes in object poses, physical disturbances, and visual distractors. <b>Reinforcement</b> <b>learning,</b> on the other hand, can explore the environment autonomously to learn robust behaviors but may require impractical amounts of unsafe real-world data collection. To learn performant, robust policies without the burden of unsafe real-world data collection or extensive <b>human</b> <b>supervision,</b> we propose RialTo, a system for robustifying real-world imitation learning policies via <b>reinforcement</b> <b>learning</b> in &ldquo;digital twin&rdquo; <b>simulation</b> environments constructed on the fly from small amounts of real-world data. To enable this real-to-sim-to-real pipeline, RialTo proposes an easy-to-use interface for quickly scanning and constructing digital twins of real-world environments. We also introduce a novel &ldquo;inverse <b>distillation&rdquo;</b> procedure for bringing real-world demonstrations into simulated environments for efficient <b>fine-tuning,</b> with minimal <b>human</b> <b>intervention</b> and engineering required. We evaluate RialTo across a variety of robotic manipulation problems in the real world, such as robustly stacking dishes on a rack, placing books on a shelf, and six other tasks. RialTo increases (over 67%) in policy robustness without requiring extensive <b>human</b> <b>data</b> collection. Project website and videos at <a href=https://real-to-sim-to-real.github.io/RialTo/>https://real-to-sim-to-real.github.io/RialTo/</a></p></p class="citation"></blockquote><h3 id=315--174271-bidirectional-progressive-neural-networks-with-episodic-return-progress-for-emergent-task-sequencing-and-robotic-skill-transfer-suzan-ece-ada-et-al-2024>(3/15 | 174/271) Bidirectional Progressive Neural Networks with Episodic Return Progress for Emergent Task Sequencing and Robotic Skill Transfer (Suzan Ece Ada et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Suzan Ece Ada, Hanne Say, Emre Ugur, Erhan Oztop. (2024)<br><strong>Bidirectional Progressive Neural Networks with Episodic Return Progress for Emergent Task Sequencing and Robotic Skill Transfer</strong><br><button class=copy-to-clipboard title="Bidirectional Progressive Neural Networks with Episodic Return Progress for Emergent Task Sequencing and Robotic Skill Transfer" index=174>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-174 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-LG, cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04001v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04001v1.pdf filename=2403.04001v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Human brain and behavior provide a rich venue that can inspire novel control and learning methods for robotics. In an attempt to exemplify such a development by inspiring how humans acquire knowledge and transfer skills among tasks, we introduce a novel multi-task <b>reinforcement</b> <b>learning</b> framework named Episodic Return Progress with Bidirectional Progressive Neural Networks (ERP-BPNN). The proposed ERP-BPNN model (1) learns in a human-like interleaved manner by (2) autonomous task switching based on a novel intrinsic motivation signal and, in contrast to existing methods, (3) allows bidirectional skill transfer among tasks. ERP-BPNN is a general architecture applicable to several multi-task learning settings; in this paper, we present the details of its neural architecture and show its ability to enable effective learning and skill transfer among morphologically different robots in a reaching task. The developed Bidirectional Progressive Neural Network (BPNN) architecture enables bidirectional skill transfer without requiring incremental training and seamlessly integrates with online task arbitration. The task arbitration mechanism developed is based on soft Episodic Return progress (ERP), a novel intrinsic motivation (IM) signal. To evaluate our method, we use quantifiable robotics metrics such as &rsquo;expected distance to goal&rsquo; and &lsquo;path straightness&rsquo; in addition to the usual reward-based measure of episodic return common in <b>reinforcement</b> <b>learning.</b> With <b>simulation</b> experiments, we show that ERP-BPNN achieves faster cumulative convergence and improves performance in all metrics considered among morphologically different robots compared to the baselines.</p></p class="citation"></blockquote><h3 id=415--175271-3d-diffusion-policy-yanjie-ze-et-al-2024>(4/15 | 175/271) 3D Diffusion Policy (Yanjie Ze et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yanjie Ze, Gu Zhang, Kangning Zhang, Chenyuan Hu, Muhan Wang, Huazhe Xu. (2024)<br><strong>3D Diffusion Policy</strong><br><button class=copy-to-clipboard title="3D Diffusion Policy" index=175>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-175 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-CV, cs-LG, cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Human Intervention, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03954v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03954v1.pdf filename=2403.03954v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Imitation learning provides an efficient way to teach robots dexterous skills; however, learning complex skills robustly and generalizablely usually consumes large amounts of <b>human</b> <b>demonstrations.</b> To tackle this challenging problem, we present 3D Diffusion Policy (DP3), a novel visual imitation learning approach that incorporates the power of 3D visual representations into diffusion policies, a class of conditional action generative models. The core design of DP3 is the utilization of a compact 3D visual representation, extracted from sparse point clouds with an efficient point encoder. In our experiments involving 72 <b>simulation</b> tasks, DP3 successfully handles most tasks with just 10 demonstrations and surpasses baselines with a 55.3% relative improvement. In 4 real robot tasks, DP3 demonstrates precise control with a high success rate of 85%, given only 40 demonstrations of each task, and shows excellent generalization abilities in diverse aspects, including space, viewpoint, appearance, and instance. Interestingly, in real robot experiments, DP3 rarely violates safety requirements, in contrast to baseline methods which frequently do, necessitating <b>human</b> <b>intervention.</b> Our extensive evaluation highlights the critical importance of 3D representations in real-world robot learning. Videos, code, and data are available on <a href=https://3d-diffusion-policy.github.io>https://3d-diffusion-policy.github.io</a> .</p></p class="citation"></blockquote><h3 id=515--176271-hierarchical-diffusion-policy-for-kinematics-aware-multi-task-robotic-manipulation-xiao-ma-et-al-2024>(5/15 | 176/271) Hierarchical Diffusion Policy for Kinematics-Aware Multi-Task Robotic Manipulation (Xiao Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiao Ma, Sumit Patidar, Iain Haughton, Stephen James. (2024)<br><strong>Hierarchical Diffusion Policy for Kinematics-Aware Multi-Task Robotic Manipulation</strong><br><button class=copy-to-clipboard title="Hierarchical Diffusion Policy for Kinematics-Aware Multi-Task Robotic Manipulation" index=176>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-176 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-CV, cs-LG, cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Knowledge Distillation, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03890v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03890v1.pdf filename=2403.03890v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces Hierarchical Diffusion Policy (HDP), a hierarchical agent for multi-task robotic manipulation. HDP factorises a manipulation policy into a hierarchical structure: a high-level task-planning agent which predicts a distant next-best end-effector pose (NBP), and a low-level goal-conditioned diffusion policy which generates optimal motion trajectories. The factorised policy representation allows HDP to tackle both long-horizon task planning while generating fine-grained low-level actions. To generate context-aware motion trajectories while satisfying robot kinematics constraints, we present a novel kinematics-aware goal-conditioned control agent, Robot Kinematics Diffuser (RK-Diffuser). Specifically, RK-Diffuser learns to generate both the end-effector pose and joint position trajectories, and <b>distill</b> the accurate but kinematics-unaware end-effector pose diffuser to the kinematics-aware but less accurate joint position diffuser via differentiable kinematics. Empirically, we show that HDP achieves a significantly higher success rate than the state-of-the-art methods in both <b>simulation</b> and real-world.</p></p class="citation"></blockquote><h3 id=615--177271-dexterous-legged-locomotion-in-confined-3d-spaces-with-reinforcement-learning-zifan-xu-et-al-2024>(6/15 | 177/271) Dexterous Legged Locomotion in Confined 3D Spaces with Reinforcement Learning (Zifan Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zifan Xu, Amir Hossain Raj, Xuesu Xiao, Peter Stone. (2024)<br><strong>Dexterous Legged Locomotion in Confined 3D Spaces with Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Dexterous Legged Locomotion in Confined 3D Spaces with Reinforcement Learning" index=177>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-177 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-LG, cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03848v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03848v1.pdf filename=2403.03848v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advances of locomotion controllers utilizing deep <b>reinforcement</b> <b>learning</b> (RL) have yielded impressive results in terms of achieving rapid and robust locomotion across challenging terrain, such as rugged rocks, non-rigid ground, and slippery surfaces. However, while these controllers primarily address challenges underneath the robot, relatively little research has investigated legged mobility through confined 3D spaces, such as narrow tunnels or irregular voids, which impose all-around constraints. The cyclic gait patterns resulted from existing RL-based methods to learn parameterized locomotion skills characterized by motion parameters, such as velocity and body height, may not be adequate to navigate robots through challenging confined 3D spaces, requiring both agile 3D obstacle avoidance and robust legged locomotion. Instead, we propose to learn locomotion skills end-to-end from goal-oriented navigation in confined 3D spaces. To address the inefficiency of tracking distant navigation goals, we introduce a hierarchical locomotion controller that combines a classical planner tasked with planning waypoints to reach a faraway global goal location, and an RL-based policy trained to follow these waypoints by generating low-level motion commands. This approach allows the policy to explore its own locomotion skills within the entire solution space and facilitates smooth transitions between local goals, enabling long-term navigation towards distant goals. In <b>simulation,</b> our hierarchical approach succeeds at navigating through demanding confined 3D environments, outperforming both pure end-to-end learning approaches and parameterized locomotion skills. We further demonstrate the successful real-world deployment of our <b>simulation-trained</b> controller on a real robot.</p></p class="citation"></blockquote><h3 id=715--178271-efficient-search-and-learning-for-agile-locomotion-on-stepping-stones-adithya-kumar-chinnakkonda-ravi-et-al-2024>(7/15 | 178/271) Efficient Search and Learning for Agile Locomotion on Stepping Stones (Adithya Kumar Chinnakkonda Ravi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Adithya Kumar Chinnakkonda Ravi, Victor Dhédin, Armand Jordana, Huaijiang Zhu, Avadesh Meduri, Ludovic Righetti, Bernhard Schölkopf, Majid Khadiv. (2024)<br><strong>Efficient Search and Learning for Agile Locomotion on Stepping Stones</strong><br><button class=copy-to-clipboard title="Efficient Search and Learning for Agile Locomotion on Stepping Stones" index=178>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-178 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Diffusion Model, Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03639v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03639v1.pdf filename=2403.03639v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Legged robots have become capable of performing highly dynamic maneuvers in the past few years. However, agile locomotion in highly constrained environments such as stepping stones is still a challenge. In this paper, we propose a combination of model-based control, search, and learning to design efficient control policies for agile locomotion on stepping stones. In our framework, we use nonlinear model predictive control (NMPC) to generate whole-body motions for a given contact plan. To efficiently search for an optimal contact plan, we propose to use Monte Carlo tree search (MCTS). While the combination of MCTS and NMPC can quickly find a feasible plan for a given environment (a few seconds), it is not yet suitable to be used as a reactive policy. Hence, we generate a dataset for optimal goal-conditioned policy for a given scene and learn it through <b>supervised</b> <b>learning.</b> In particular, we leverage the power of <b>diffusion</b> <b>models</b> in handling multi-modality in the dataset. We test our proposed framework on a scenario where our quadruped robot Solo12 successfully jumps to different goals in a highly constrained environment.</p></p class="citation"></blockquote><h3 id=815--179271-multi-object-tracking-with-camera-lidar-fusion-for-autonomous-driving-riccardo-pieroni-et-al-2024>(8/15 | 179/271) Multi-Object Tracking with Camera-LiDAR Fusion for Autonomous Driving (Riccardo Pieroni et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Riccardo Pieroni, Simone Specchia, Matteo Corno, Sergio Matteo Savaresi. (2024)<br><strong>Multi-Object Tracking with Camera-LiDAR Fusion for Autonomous Driving</strong><br><button class=copy-to-clipboard title="Multi-Object Tracking with Camera-LiDAR Fusion for Autonomous Driving" index=179>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-179 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-CV, cs-RO, cs.RO<br>Keyword Score: 26<br>Keywords: Clustering, Multi-modal, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04112v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04112v1.pdf filename=2403.04112v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a novel <b>multi-modal</b> Multi-Object Tracking (MOT) algorithm for self-driving cars that combines camera and LiDAR data. Camera frames are processed with a state-of-the-art 3D object detector, whereas classical <b>clustering</b> techniques are used to process LiDAR observations. The proposed MOT algorithm comprises a three-step association process, an Extended Kalman filter for estimating the motion of each detected dynamic obstacle, and a track management phase. The EKF motion model requires the current measured relative position and orientation of the observed object and the longitudinal and angular velocities of the ego vehicle as inputs. Unlike most state-of-the-art <b>multi-modal</b> MOT approaches, the proposed algorithm does not rely on maps or knowledge of the ego global pose. Moreover, it uses a 3D detector exclusively for cameras and is agnostic to the type of LiDAR sensor used. The algorithm is validated both in <b>simulation</b> and with real-world data, with satisfactory results.</p></p class="citation"></blockquote><h3 id=915--180271-multimodal-anomaly-detection-based-on-deep-auto-encoder-for-object-slip-perception-of-mobile-manipulation-robots-youngjae-yoo-et-al-2024>(9/15 | 180/271) Multimodal Anomaly Detection based on Deep Auto-Encoder for Object Slip Perception of Mobile Manipulation Robots (Youngjae Yoo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Youngjae Yoo, Chung-Yeon Lee, Byoung-Tak Zhang. (2024)<br><strong>Multimodal Anomaly Detection based on Deep Auto-Encoder for Object Slip Perception of Mobile Manipulation Robots</strong><br><button class=copy-to-clipboard title="Multimodal Anomaly Detection based on Deep Auto-Encoder for Object Slip Perception of Mobile Manipulation Robots" index=180>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-180 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-LG, cs-RO, cs.RO<br>Keyword Score: 26<br>Keywords: Anomaly Detection, Autoencoder, Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03563v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03563v1.pdf filename=2403.03563v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Object slip perception is essential for mobile manipulation robots to perform manipulation tasks reliably in the dynamic real-world. Traditional approaches to robot arms&rsquo; slip perception use tactile or vision sensors. However, mobile robots still have to deal with noise in their sensor signals caused by the robot&rsquo;s movement in a changing environment. To solve this problem, we present an <b>anomaly</b> <b>detection</b> method that utilizes multisensory data based on a deep <b>autoencoder</b> model. The proposed framework integrates heterogeneous data streams collected from various robot sensors, including RGB and depth cameras, a microphone, and a force-torque sensor. The integrated data is used to train a deep <b>autoencoder</b> to construct latent representations of the multisensory data that indicate the normal status. Anomalies can then be identified by error scores measured by the difference between the trained encoder&rsquo;s latent values and the latent values of reconstructed input data. In order to evaluate the proposed framework, we conducted an experiment that mimics an object slip by a mobile service robot operating in a real-world environment with diverse household objects and different moving patterns. The experimental results verified that the proposed framework reliably detects anomalies in object slip situations despite various object types and robot behaviors, and visual and auditory noise in the environment.</p></p class="citation"></blockquote><h3 id=1015--181271-confidence-aware-decision-making-and-control-for-tool-selection-ajith-anil-meera-et-al-2024>(10/15 | 181/271) Confidence-Aware Decision-Making and Control for Tool Selection (Ajith Anil Meera et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ajith Anil Meera, Pablo Lanillos. (2024)<br><strong>Confidence-Aware Decision-Making and Control for Tool Selection</strong><br><button class=copy-to-clipboard title="Confidence-Aware Decision-Making and Control for Tool Selection" index=181>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-181 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs-SY, cs.RO, eess-SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03808v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03808v1.pdf filename=2403.03808v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Self-reflecting about our performance (e.g., how confident we are) before doing a task is essential for decision making, such as selecting the most suitable tool or choosing the best route to drive. While this form of awareness &ndash; thinking about our performance or metacognitive performance &ndash; is well-known in humans, robots still lack this cognitive ability. This reflective monitoring can enhance their embodied decision power, robustness and safety. Here, we take a step in this direction by introducing a mathematical framework that allows robots to use their control self-confidence to make better-informed decisions. We derive a mathematical closed-form expression for control confidence for dynamic systems (i.e., the posterior inverse covariance of the control action). This control confidence seamlessly integrates within an objective function for decision making, that balances the: i) performance for task completion, ii) control effort, and iii) self-confidence. To evaluate our theoretical account, we framed the decision-making within the tool selection problem, where the agent has to select the best robot arm for a particular control task. The statistical analysis of the numerical <b>simulations</b> with randomized 2DOF arms shows that using control confidence during tool selection improves both real task performance, and the reliability of the tool for performance under unmodelled perturbations (e.g., external forces). Furthermore, our results indicate that control confidence is an early indicator of performance and thus, it can be used as a heuristic for making decisions when computation power is restricted or decision-making is intractable. Overall, we show the advantages of using confidence-aware decision-making and control scheme for dynamic systems.</p></p class="citation"></blockquote><h3 id=1115--182271-robust-mitl-planning-under-uncertain-navigation-times-alexis-linard-et-al-2024>(11/15 | 182/271) Robust MITL planning under uncertain navigation times (Alexis Linard et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alexis Linard, Anna Gautier, Daniel Duberg, Jana Tumova. (2024)<br><strong>Robust MITL planning under uncertain navigation times</strong><br><button class=copy-to-clipboard title="Robust MITL planning under uncertain navigation times" index=182>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-182 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-FL, cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03727v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03727v1.pdf filename=2403.03727v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In environments like offices, the duration of a robot&rsquo;s navigation between two locations may vary over time. For instance, reaching a kitchen may take more time during lunchtime since the corridors are crowded with people heading the same way. In this work, we address the problem of routing in such environments with tasks expressed in Metric Interval Temporal Logic (MITL) - a rich robot task specification language that allows us to capture explicit time requirements. Our objective is to find a strategy that maximizes the temporal robustness of the robot&rsquo;s MITL task. As the first step towards a solution, we define a Mixed-integer linear programming approach to solving the task planning problem over a Varying Weighted Transition System, where navigation durations are deterministic but vary depending on the time of day. Then, we apply this planner to optimize for MITL temporal robustness in Markov Decision Processes, where the navigation durations between physical locations are uncertain, but the time-dependent distribution over possible delays is known. Finally, we develop a receding horizon planner for Markov Decision Processes that preserves guarantees over MITL temporal robustness. We show the scalability of our planning algorithms in <b>simulations</b> of robotic tasks.</p></p class="citation"></blockquote><h3 id=1215--183271-interactive-continual-learning-architecture-for-long-term-personalization-of-home-service-robots-ali-ayub-et-al-2024>(12/15 | 183/271) Interactive Continual Learning Architecture for Long-Term Personalization of Home Service Robots (Ali Ayub et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ali Ayub, Chrystopher Nehaniv, Kerstin Dautenhahn. (2024)<br><strong>Interactive Continual Learning Architecture for Long-Term Personalization of Home Service Robots</strong><br><button class=copy-to-clipboard title="Interactive Continual Learning Architecture for Long-Term Personalization of Home Service Robots" index=183>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-183 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-CV, cs-LG, cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Continual Learning, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03462v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03462v1.pdf filename=2403.03462v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>For robots to perform assistive tasks in unstructured home environments, they must learn and reason on the semantic knowledge of the environments. Despite a resurgence in the development of semantic <b>reasoning</b> architectures, these methods assume that all the training data is available a priori. However, each user&rsquo;s environment is unique and can continue to change over time, which makes these methods unsuitable for personalized home service robots. Although research in <b>continual</b> <b>learning</b> develops methods that can learn and adapt over time, most of these methods are tested in the narrow context of object classification on static image datasets. In this paper, we combine ideas from <b>continual</b> <b>learning,</b> semantic <b>reasoning,</b> and interactive machine learning literature and develop a novel interactive <b>continual</b> <b>learning</b> architecture for <b>continual</b> <b>learning</b> of semantic knowledge in a home environment through human-robot interaction. The architecture builds on core cognitive principles of learning and memory for efficient and real-time learning of new knowledge from humans. We integrate our architecture with a physical mobile manipulator robot and perform extensive system evaluations in a laboratory environment over two months. Our results demonstrate the effectiveness of our architecture to allow a physical robot to continually adapt to the changes in the environment from limited data provided by the users (experimenters), and use the learned knowledge to perform object fetching tasks.</p></p class="citation"></blockquote><h3 id=1315--184271-feel-the-bite-robot-assisted-inside-mouth-bite-transfer-using-robust-mouth-perception-and-physical-interaction-aware-control-rajat-kumar-jenamani-et-al-2024>(13/15 | 184/271) Feel the Bite: Robot-Assisted Inside-Mouth Bite Transfer using Robust Mouth Perception and Physical Interaction-Aware Control (Rajat Kumar Jenamani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rajat Kumar Jenamani, Daniel Stabile, Ziang Liu, Abrar Anwar, Katherine Dimitropoulou, Tapomayukh Bhattacharjee. (2024)<br><strong>Feel the Bite: Robot-Assisted Inside-Mouth Bite Transfer using Robust Mouth Perception and Physical Interaction-Aware Control</strong><br><button class=copy-to-clipboard title="Feel the Bite: Robot-Assisted Inside-Mouth Bite Transfer using Robust Mouth Perception and Physical Interaction-Aware Control" index=184>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-184 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 6<br>Keywords: Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04067v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04067v1.pdf filename=2403.04067v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Robot-assisted feeding can greatly enhance the lives of those with mobility limitations. Modern feeding systems can pick up and position food in front of a care recipient&rsquo;s mouth for a bite. However, many with severe mobility constraints cannot lean forward and need direct inside-mouth food placement. This demands precision, especially for those with restricted mouth openings, and appropriately reacting to various physical interactions - incidental contacts as the utensil moves inside, impulsive contacts due to sudden muscle spasms, deliberate tongue maneuvers by the person being fed to guide the utensil, and intentional bites. In this paper, we propose an inside-mouth bite transfer system that addresses these challenges with two key components: a multi-view mouth perception pipeline robust to tool occlusion, and a control mechanism that employs <b>multimodal</b> time-series classification to discern and react to different physical interactions. We demonstrate the efficacy of these individual components through two ablation studies. In a full system evaluation, our system successfully fed 13 care recipients with diverse mobility challenges. Participants consistently emphasized the comfort and safety of our inside-mouth bite transfer system, and gave it high technology acceptance ratings - underscoring its transformative potential in real-world scenarios. Supplementary materials and videos can be found at <a href=http://emprise.cs.cornell.edu/bitetransfer/>http://emprise.cs.cornell.edu/bitetransfer/</a> .</p></p class="citation"></blockquote><h3 id=1415--185271-deployable-polyhedrons-with-one-dof-radial-transformation-yuanqing-gu-et-al-2024>(14/15 | 185/271) Deployable polyhedrons with one-DOF radial transformation (Yuanqing Gu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuanqing Gu, Yan Chen. (2024)<br><strong>Deployable polyhedrons with one-DOF radial transformation</strong><br><button class=copy-to-clipboard title="Deployable polyhedrons with one-DOF radial transformation" index=185>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-185 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03577v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03577v1.pdf filename=2403.03577v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deployable polyhedrons can transform between Platonic and Archimedean polyhedrons to meet the demands of various engineering applications. However, the existing design solutions are often with multiple degrees of freedom and complicated mechanism links and joints, which greatly limited their potential in practice. Combining the fundamentals of solid <b>geometry</b> and mechanism kinematics, this paper proposes a family of kirigami Archimedean polyhedrons based on the N-fold-symmetric loops of spatial 7R linkage, which perform one-DOF radial transformation following tetrahedral, octahedral, or icosahedral symmetry. Moreover, in each symmetric polyhedral group, three different transforming paths can be achieved from one identical deployed configuration. We also demonstrated that such design strategy can be readily applied to polyhedral tessellation. This work provides a family of rich solutions for deployable polyhedrons to facilitate their applications in aerospace exploration, architecture, metamaterials and so on.</p></p class="citation"></blockquote><h3 id=1515--186271-seamless-virtual-reality-with-integrated-synchronizer-and-synthesizer-for-autonomous-driving-he-li-et-al-2024>(15/15 | 186/271) Seamless Virtual Reality with Integrated Synchronizer and Synthesizer for Autonomous Driving (He Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>He Li, Ruihua Han, Zirui Zhao, Wei Xu, Qi Hao, Shuai Wang, Chengzhong Xu. (2024)<br><strong>Seamless Virtual Reality with Integrated Synchronizer and Synthesizer for Autonomous Driving</strong><br><button class=copy-to-clipboard title="Seamless Virtual Reality with Integrated Synchronizer and Synthesizer for Autonomous Driving" index=186>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-186 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03541v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03541v1.pdf filename=2403.03541v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Virtual reality (VR) is a promising data engine for autonomous driving (AD). However, data fidelity in this paradigm is often degraded by VR inconsistency, for which the existing VR approaches become ineffective, as they ignore the inter-dependency between low-level VR synchronizer designs (i.e., data collector) and high-level VR synthesizer designs (i.e., data processor). This paper presents a seamless virtual reality SVR platform for AD, which mitigates such inconsistency, enabling VR agents to interact with each other in a shared symbiotic world. The crux to SVR is an integrated synchronizer and synthesizer IS2 design, which consists of a drift-aware lidar-inertial synchronizer for VR colocation and a motion-aware deep visual synthesis network for augmented reality image generation. We implement SVR on car-like robots in two sandbox platforms, achieving a cm-level VR colocalization accuracy and 3.2% VR image deviation, thereby avoiding missed collisions or model clippings. Experiments show that the proposed SVR reduces the intervention times, missed turns, and failure rates compared to other <b>benchmarks.</b> The SVR-trained neural network can handle unseen situations in real-world environments, by leveraging its knowledge learnt from the VR space.</p></p class="citation"></blockquote><h2 id=eessiv-7>eess.IV (7)</h2><h3 id=17--187271-multi-modal-deep-learning-chen-yuhua-2024>(1/7 | 187/271) Multi-modal Deep Learning (Chen Yuhua, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chen Yuhua. (2024)<br><strong>Multi-modal Deep Learning</strong><br><button class=copy-to-clipboard title="Multi-modal Deep Learning" index=187>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-187 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-AI, cs-CV, eess-IV, eess.IV<br>Keyword Score: 56<br>Keywords: Vision Transformer, Convolution, Multi-modal, Multi-modal, Transfer Learning, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03385v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03385v1.pdf filename=2403.03385v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This article investigates deep learning methodologies for single-modality clinical data analysis, as a crucial precursor to <b>multi-modal</b> medical research. Building on Guo JingYuan&rsquo;s work, the study refines clinical data processing through Compact <b>Convolutional</b> <b>Transformer</b> (CCT), Patch Up, and the innovative CamCenterLoss technique, establishing a foundation for future <b>multimodal</b> investigations. The proposed methodology demonstrates improved prediction accuracy and at tentiveness to critically ill patients compared to Guo JingYuan&rsquo;s ResNet and StageNet approaches. Novelty that using image-pretrained <b>vision</b> <b>transformer</b> backbone to perform <b>transfer</b> <b>learning</b> time-series clinical data.The study highlights the potential of CCT, Patch Up, and novel CamCenterLoss in processing single modality clinical data within deep learning frameworks, paving the way for future <b>multimodal</b> medical research and promoting precision and personalized healthcare</p></p class="citation"></blockquote><h3 id=27--188271-generative-active-learning-with-variational-autoencoder-for-radiology-data-generation-in-veterinary-medicine-in-gyu-lee-et-al-2024>(2/7 | 188/271) Generative Active Learning with Variational Autoencoder for Radiology Data Generation in Veterinary Medicine (In-Gyu Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>In-Gyu Lee, Jun-Young Oh, Hee-Jung Yu, Jae-Hwan Kim, Ki-Dong Eom, Ji-Hoon Jeong. (2024)<br><strong>Generative Active Learning with Variational Autoencoder for Radiology Data Generation in Veterinary Medicine</strong><br><button class=copy-to-clipboard title="Generative Active Learning with Variational Autoencoder for Radiology Data Generation in Veterinary Medicine" index=188>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-188 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 40<br>Keywords: Active Learning, Autoencoder, Data Augmentation, Variational Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03642v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03642v1.pdf filename=2403.03642v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, with increasing interest in pet healthcare, the demand for computer-aided diagnosis (CAD) systems in veterinary medicine has increased. The development of veterinary CAD has stagnated due to a lack of sufficient radiology <b>data.</b> <b>To</b> overcome the challenge, we propose a generative <b>active</b> <b>learning</b> framework based on a <b>variational</b> <b>autoencoder.</b> This approach aims to alleviate the scarcity of reliable <b>data</b> <b>for</b> CAD systems in veterinary medicine. This study utilizes datasets comprising cardiomegaly radiograph <b>data.</b> <b>After</b> removing annotations and standardizing images, we employed a framework for <b>data</b> <b>augmentation,</b> which consists of a <b>data</b> <b>generation</b> phase and a query phase for filtering the generated <b>data.</b> <b>The</b> experimental results revealed that as the <b>data</b> <b>generated</b> through this framework was added to the training <b>data</b> <b>of</b> the generative model, the frechet inception distance consistently decreased from 84.14 to 50.75 on the radiograph. Subsequently, when the generated <b>data</b> <b>were</b> incorporated into the training of the classification model, the false positive of the confusion matrix also improved from 0.16 to 0.66 on the radiograph. The proposed framework has the potential to address the challenges of <b>data</b> <b>scarcity</b> in medical CAD, contributing to its advancement.</p></p class="citation"></blockquote><h3 id=37--189271-enhancing-chest-x-ray-datasets-with-privacy-preserving-large-language-models-and-multi-type-annotations-a-data-driven-approach-for-improved-classification-ricardo-bigolin-lanfredi-et-al-2024>(3/7 | 189/271) Enhancing chest X-ray datasets with privacy-preserving large language models and multi-type annotations: a data-driven approach for improved classification (Ricardo Bigolin Lanfredi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ricardo Bigolin Lanfredi, Pritam Mukherjee, Ronald Summers. (2024)<br><strong>Enhancing chest X-ray datasets with privacy-preserving large language models and multi-type annotations: a data-driven approach for improved classification</strong><br><button class=copy-to-clipboard title="Enhancing chest X-ray datasets with privacy-preserving large language models and multi-type annotations: a data-driven approach for improved classification" index=189>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-189 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 30<br>Keywords: Zero-shot, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04024v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04024v1.pdf filename=2403.04024v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In chest X-ray (CXR) image analysis, rule-based systems are usually employed to extract labels from reports, but concerns exist about label quality. These datasets typically offer only presence labels, sometimes with binary uncertainty indicators, which limits their usefulness. In this work, we present MAPLEZ (Medical report Annotations with Privacy-preserving <b>Large</b> <b>language</b> <b>model</b> using Expeditious Zero shot answers), a novel approach leveraging a locally executable <b>Large</b> <b>Language</b> <b>Model</b> <b>(LLM)</b> to extract and enhance findings labels on CXR reports. MAPLEZ extracts not only binary labels indicating the presence or absence of a finding but also the location, severity, and radiologists&rsquo; uncertainty about the finding. Over eight abnormalities from five test sets, we show that our method can extract these annotations with an increase of 5 percentage points (pp) in F1 score for categorical presence annotations and more than 30 pp increase in F1 score for the location annotations over competing labelers. Additionally, using these improved annotations in classification supervision, we demonstrate substantial advancements in model quality, with an increase of 1.7 pp in AUROC over models trained with annotations from the state-of-the-art approach. We share code and annotations.</p></p class="citation"></blockquote><h3 id=47--190271-medmamba-vision-mamba-for-medical-image-classification-yubiao-yue-et-al-2024>(4/7 | 190/271) MedMamba: Vision Mamba for Medical Image Classification (Yubiao Yue et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yubiao Yue, Zhenzhang Li. (2024)<br><strong>MedMamba: Vision Mamba for Medical Image Classification</strong><br><button class=copy-to-clipboard title="MedMamba: Vision Mamba for Medical Image Classification" index=190>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-190 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03849v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03849v1.pdf filename=2403.03849v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Medical image classification is a very fundamental and crucial task in the field of computer vision. These years, <b>CNN-based</b> and <b>Transformer-based</b> models are widely used in classifying various medical images. Unfortunately, The limitation of <b>CNNs</b> in long-range modeling capabilities prevent them from effectively extracting fine-grained features in medical images , while <b>Transformers</b> are hampered by their quadratic computational complexity. Recent research has shown that the state space model (SSM) represented by Mamba can efficiently model long-range interactions while maintaining linear computational complexity. Inspired by this, we propose Vision Mamba for medical image classification (MedMamba). More specifically, we introduce a novel Conv-SSM module, which combines the local feature extraction ability of <b>convolutional</b> layers with the ability of SSM to capture long-range dependency. To demonstrate the potential of MedMamba, we conduct extensive experiments using three publicly available medical datasets with different imaging techniques (i.e., Kvasir (endoscopic images), FETAL_PLANES_DB (ultrasound images) and Covid19-Pneumonia-Normal Chest X-Ray (X-ray images)) and two private datasets built by ourselves. Experimental results show that the proposed MedMamba performs well in detecting lesions in various medical images. To the best of our knowledge, this is the first Vision Mamba tailored for medical image classification. The purpose of this work is to establish a new baseline for medical image classification tasks and provide valuable insights for the future development of more efficient and effective SSM-based artificial intelligence algorithms and application systems in the medical. Source code has been available at <a href=https://github.com/YubiaoYue/MedMamba>https://github.com/YubiaoYue/MedMamba</a>.</p></p class="citation"></blockquote><h3 id=57--191271-fast-nonlocal-and-neural-a-lightweight-high-quality-solution-to-image-denoising-yu-guo-et-al-2024>(5/7 | 191/271) Fast, nonlocal and neural: a lightweight high quality solution to image denoising (Yu Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yu Guo, Axel Davy, Gabriele Facciolo, Jean-Michel Morel, Qiyu Jin. (2024)<br><strong>Fast, nonlocal and neural: a lightweight high quality solution to image denoising</strong><br><button class=copy-to-clipboard title="Fast, nonlocal and neural: a lightweight high quality solution to image denoising" index=191>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-191 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03488v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03488v1.pdf filename=2403.03488v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the widespread application of <b>convolutional</b> <b>neural</b> <b>networks</b> <b>(CNNs),</b> the traditional model based denoising algorithms are now outperformed. However, <b>CNNs</b> face two problems. First, they are computationally demanding, which makes their deployment especially difficult for mobile terminals. Second, experimental evidence shows that <b>CNNs</b> often over-smooth regular textures present in images, in contrast to traditional non-local models. In this letter, we propose a solution to both issues by combining a nonlocal algorithm with a lightweight residual <b>CNN.</b> This solution gives full latitude to the advantages of both models. We apply this framework to two GPU implementations of classic nonlocal algorithms (NLM and BM3D) and observe a substantial gain in both cases, performing better than the state-of-the-art with low computational requirements. Our solution is between 10 and 20 times faster than <b>CNNs</b> with equivalent performance and attains higher PSNR. In addition the final method shows a notable gain on images containing complex textures like the ones of the MIT Moire dataset.</p></p class="citation"></blockquote><h3 id=67--192271-joint-multi-task-learning-improves-weakly-supervised-biomarker-prediction-in-computational-pathology-omar-s-m-el-nahhas-et-al-2024>(6/7 | 192/271) Joint multi-task learning improves weakly-supervised biomarker prediction in computational pathology (Omar S. M. El Nahhas et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Omar S. M. El Nahhas, Georg Wölflein, Marta Ligero, Tim Lenz, Marko van Treeck, Firas Khader, Daniel Truhn, Jakob Nikolas Kather. (2024)<br><strong>Joint multi-task learning improves weakly-supervised biomarker prediction in computational pathology</strong><br><button class=copy-to-clipboard title="Joint multi-task learning improves weakly-supervised biomarker prediction in computational pathology" index=192>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-192 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 26<br>Keywords: Benchmarking, Clustering, Weakly-supervised Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03891v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03891v1.pdf filename=2403.03891v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep Learning (DL) can predict biomarkers directly from digitized cancer histology in a <b>weakly-supervised</b> setting. Recently, the prediction of continuous biomarkers through regression-based DL has seen an increasing interest. Nonetheless, clinical decision making often requires a categorical outcome. Consequently, we developed a <b>weakly-supervised</b> joint multi-task <b>Transformer</b> architecture which has been trained and evaluated on four public patient cohorts for the prediction of two key predictive biomarkers, microsatellite instability (MSI) and homologous recombination deficiency (HRD), trained with auxiliary regression tasks related to the tumor microenvironment. Moreover, we perform a comprehensive <b>benchmark</b> of 16 approaches of task balancing for <b>weakly-supervised</b> joint multi-task learning in computational pathology. Using our novel approach, we improve over the state-of-the-art area under the receiver operating characteristic by +7.7% and +4.1%, as well as yielding better <b>clustering</b> of latent embeddings by +8% and +5% for the prediction of MSI and HRD in external cohorts, respectively.</p></p class="citation"></blockquote><h3 id=77--193271-low-dose-ct-image-reconstruction-by-fine-tuning-a-unet-pretrained-for-gaussian-denoising-for-the-downstream-task-of-image-enhancement-tim-selig-et-al-2024>(7/7 | 193/271) Low-Dose CT Image Reconstruction by Fine-Tuning a UNet Pretrained for Gaussian Denoising for the Downstream Task of Image Enhancement (Tim Selig et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tim Selig, Thomas März, Martin Storath, Andreas Weinmann. (2024)<br><strong>Low-Dose CT Image Reconstruction by Fine-Tuning a UNet Pretrained for Gaussian Denoising for the Downstream Task of Image Enhancement</strong><br><button class=copy-to-clipboard title="Low-Dose CT Image Reconstruction by Fine-Tuning a UNet Pretrained for Gaussian Denoising for the Downstream Task of Image Enhancement" index=193>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-193 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 23<br>Keywords: Benchmarking, Fine-tuning, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03551v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03551v1.pdf filename=2403.03551v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Computed Tomography (CT) is a widely used medical imaging modality, and as it is based on ionizing radiation, it is desirable to minimize the radiation dose. However, a reduced radiation dose comes with reduced image quality, and reconstruction from low-dose CT (LDCT) data is still a challenging task which is subject to research. According to the LoDoPaB-CT <b>benchmark,</b> a <b>benchmark</b> for LDCT reconstruction, many state-of-the-art methods use pipelines involving UNet-type architectures. Specifically the top ranking method, ItNet, employs a three-stage process involving filtered backprojection (FBP), a UNet trained on CT data, and an iterative refinement step. In this paper, we propose a less complex two-stage method. The first stage also employs FBP, while the novelty lies in the training strategy for the second stage, characterized as the CT image enhancement stage. The crucial point of our approach is that the neural network is pretrained on a distinctly different pretraining task with non-CT data, namely Gaussian noise removal on a variety of natural grayscale images (photographs). We then <b>fine-tune</b> this network for the downstream task of CT image enhancement using pairs of LDCT images and corresponding normal-dose CT images (NDCT). Despite being notably simpler than the state-of-the-art, as the pretraining did not depend on domain-specific CT data and no further iterative refinement step was necessary, the proposed two-stage method achieves competitive results. The proposed method achieves a shared top ranking in the LoDoPaB-CT challenge and a first position with respect to the SSIM metric.</p></p class="citation"></blockquote><h2 id=eesssy-8>eess.SY (8)</h2><h3 id=18--194271-cnn-based-end-to-end-adaptive-controller-with-stability-guarantees-myeongseok-ryu-et-al-2024>(1/8 | 194/271) CNN-based End-to-End Adaptive Controller with Stability Guarantees (Myeongseok Ryu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Myeongseok Ryu, Kyunghwan Choi. (2024)<br><strong>CNN-based End-to-End Adaptive Controller with Stability Guarantees</strong><br><button class=copy-to-clipboard title="CNN-based End-to-End Adaptive Controller with Stability Guarantees" index=194>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-194 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 50<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03499v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03499v1.pdf filename=2403.03499v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This letter proposes a <b>convolutional</b> <b>neural</b> <b>network</b> <b>(CNN)-based</b> adaptive controller wtih three notable features: 1) it determines control input directly from historical sensor data (in an end-to-end process); 2) it learns the desired control policy during real-time implementation without using a pretrained network (in an online adaptive manner); and 3) the asymptotic tracking error convergence is proven during the learning process (to deliver a stability guarantee). An adaptive law for learning the desired control policy is derived using the gradient descent optimization method, and its stability is analyzed based on the Lyapunov approach. A <b>simulation</b> study using a control-affine nonlinear system demonstrated that the proposed controller exhibits these features, and its performance can be tuned by manipulating the design parameters. In addition, it is shown that the proposed controller has a superior tracking performance to that of a deep neural network (DNN)-based adaptive controller.</p></p class="citation"></blockquote><h3 id=28--195271-electrical-load-forecasting-model-using-hybrid-lstm-neural-networks-with-online-correction-nan-lu-et-al-2024>(2/8 | 195/271) Electrical Load Forecasting Model Using Hybrid LSTM Neural Networks with Online Correction (Nan Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nan Lu, Quan Ouyang, Yang Li, Changfu Zou. (2024)<br><strong>Electrical Load Forecasting Model Using Hybrid LSTM Neural Networks with Online Correction</strong><br><button class=copy-to-clipboard title="Electrical Load Forecasting Model Using Hybrid LSTM Neural Networks with Online Correction" index=195>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-195 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 40<br>Keywords: Fine-tuning, LSTM, LSTM, LSTM<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03898v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03898v1.pdf filename=2403.03898v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Accurate electrical load forecasting is of great importance for the efficient operation and control of modern power systems. In this work, a hybrid <b>long</b> <b>short-term</b> <b>memory</b> <b>(LSTM)-based</b> model with online correction is developed for day-ahead electrical load forecasting. Firstly, four types of features are extracted from the original electrical load dataset, including the historical time series, time index features, historical statistical features, and similarity features. Then, a hybrid <b>LSTM-based</b> electrical load forecasting model is designed, where an <b>LSTM</b> neural network block and a fully-connected neural network block are integrated that can model both temporal features (historical time series) and non-temporal features (the rest features). A gradient regularization-based offline training algorithm and an output layer parameter <b>fine-tuning-based</b> online model correction method are developed to enhance the model&rsquo;s capabilities to defend against disturbance and adapt to the latest load data distribution, thus improving the forecasting accuracy. At last, extensive experiments are carried out to validate the effectiveness of the proposed electrical load forecasting strategy with superior accuracy compared with commonly used forecasting models.</p></p class="citation"></blockquote><h3 id=38--196271-linear-and-nonlinear-system-identification-under-ell_1--and-group-lasso-regularization-via-l-bfgs-b-alberto-bemporad-2024>(3/8 | 196/271) Linear and nonlinear system identification under $\ell_1$- and group-Lasso regularization via L-BFGS-B (Alberto Bemporad, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alberto Bemporad. (2024)<br><strong>Linear and nonlinear system identification under $\ell_1$- and group-Lasso regularization via L-BFGS-B</strong><br><button class=copy-to-clipboard title="Linear and nonlinear system identification under $\ell_1$- and group-Lasso regularization via L-BFGS-B" index=196>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-196 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-LG, cs-SY, eess-SY, eess.SY, math-OC<br>Keyword Score: 23<br>Keywords: Benchmarking, Discrete Time, Discrete Time, Recurrent Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03827v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03827v1.pdf filename=2403.03827v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we propose an approach for identifying linear and nonlinear <b>discrete-time</b> <b>state-space</b> models, possibly under $\ell_1$- and group-Lasso regularization, based on the L-BFGS-B algorithm. For the identification of linear models, we show that, compared to classical linear subspace methods, the approach often provides better results, is much more general in terms of the loss and regularization terms used, and is also more stable from a numerical point of view. The proposed method not only enriches the existing set of linear system identification tools but can be also applied to identifying a very broad class of parametric nonlinear state-space models, including <b>recurrent</b> <b>neural</b> <b>networks.</b> We illustrate the approach on synthetic and experimental datasets and apply it to solve the challenging industrial robot <b>benchmark</b> for nonlinear multi-input/multi-output system identification proposed by Weigand et al. (2022). A Python implementation of the proposed identification method is available in the package \texttt{jax-sysid}, available at \url{https://github.com/bemporad/jax-sysid}.</p></p class="citation"></blockquote><h3 id=48--197271-a-unified-model-for-active-battery-equalization-systems-quan-ouyang-et-al-2024>(4/8 | 197/271) A Unified Model for Active Battery Equalization Systems (Quan Ouyang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Quan Ouyang, Nourallah Ghaeminezhad, Yang Li, Torsten Wik, Changfu Zou. (2024)<br><strong>A Unified Model for Active Battery Equalization Systems</strong><br><button class=copy-to-clipboard title="A Unified Model for Active Battery Equalization Systems" index=197>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-197 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03910v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03910v1.pdf filename=2403.03910v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Lithium-ion battery packs demand effective active equalization systems to enhance their usable capacity and lifetime. Despite numerous topologies and control schemes proposed in the literature, conducting quantitative analyses, comprehensive comparisons, and systematic optimization of their performance remains challenging due to the absence of a unified mathematical model at the pack level. To address this gap, we introduce a novel, hypergraph-based approach to establish the first unified model for various active battery equalization systems. This model reveals the intrinsic relationship between battery cells and equalizers by representing them as the vertices and hyperedges of hypergraphs, respectively. With the developed model, we identify the necessary condition for all equalization systems to achieve balance through controllability analysis, offering valuable insights for selecting the number of equalizers. Moreover, we prove that the battery equalization time is inversely correlated with the second smallest eigenvalue of the hypergraph&rsquo;s Laplacian matrix of each equalization system. This significantly simplifies the selection and optimized design of equalization systems, obviating the need for extensive experiments or <b>simulations</b> to derive the equalization time. Illustrative results demonstrate the efficiency of the proposed model and validate our findings.</p></p class="citation"></blockquote><h3 id=58--198271-robust-safety-critical-control-for-systems-with-sporadic-measurements-and-dwell-time-constraints-joseph-breeden-et-al-2024>(5/8 | 198/271) Robust Safety-Critical Control for Systems with Sporadic Measurements and Dwell Time Constraints (Joseph Breeden et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Joseph Breeden, Luca Zaccarian, Dimitra Panagou. (2024)<br><strong>Robust Safety-Critical Control for Systems with Sporadic Measurements and Dwell Time Constraints</strong><br><button class=copy-to-clipboard title="Robust Safety-Critical Control for Systems with Sporadic Measurements and Dwell Time Constraints" index=198>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-198 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY, math-OC<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03663v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03663v1.pdf filename=2403.03663v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents extensions of control barrier function (CBF) theory to systems with disturbances wherein a controller only receives measurements infrequently and operates open-loop between measurements, while still satisfying state constraints. The paper considers both impulsive and continuous actuators, and models the actuators, measurements, disturbances, and timing constraints as a hybrid dynamical system. We then design an open-loop observer that bounds the worst-case uncertainty between measurements. We develop definitions of CBFs for both actuation cases, and corresponding conditions on the control input to guarantee satisfaction of the state constraints. We apply these conditions to <b>simulations</b> of a satellite rendezvous in an elliptical orbit and autonomous orbit stationkeeping.</p></p class="citation"></blockquote><h3 id=68--199271-a-hybrid-dynamical-system-approach-to-the-impulsive-control-of-spacecraft-rendezvous-extended-version-alexandre-seuret-et-al-2024>(6/8 | 199/271) A hybrid dynamical system approach to the impulsive control of spacecraft rendezvous (extended version) (Alexandre Seuret et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alexandre Seuret, Rafael Vazquez, Luca Zaccarian. (2024)<br><strong>A hybrid dynamical system approach to the impulsive control of spacecraft rendezvous (extended version)</strong><br><button class=copy-to-clipboard title="A hybrid dynamical system approach to the impulsive control of spacecraft rendezvous (extended version)" index=199>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-199 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY, math-OC<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03633v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03633v1.pdf filename=2403.03633v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces a hybrid dynamical system methodology for managing impulsive control in spacecraft rendezvous and proximity operations under the Hill-Clohessy-Wiltshire model. We address the control design problem by isolating the out-of-plane from the in-plane dynamics and present a feedback control law for each of them. This law is based on a Lyapunov function tailored to each of the dynamics, capable of addressing thruster saturation and also a minimum impulse bit. These Lyapunov functions were found by reformulating the system&rsquo;s dynamics into coordinates that more intuitively represent their physical behavior. The effectiveness of our control laws is then shown through numerical <b>simulation.</b> This is an extended version of an ECC24 article of the same name, which includes the proofs omitted for lack of space.</p></p class="citation"></blockquote><h3 id=78--200271-data-based-in-cylinder-pressure-model-with-cyclic-variations-for-combustion-control-a-rcci-engine-application-maarten-vlaswinkel-et-al-2024>(7/8 | 200/271) Data-Based In-Cylinder Pressure Model with Cyclic Variations for Combustion Control: A RCCI Engine Application (Maarten Vlaswinkel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Maarten Vlaswinkel, Frank Willems. (2024)<br><strong>Data-Based In-Cylinder Pressure Model with Cyclic Variations for Combustion Control: A RCCI Engine Application</strong><br><button class=copy-to-clipboard title="Data-Based In-Cylinder Pressure Model with Cyclic Variations for Combustion Control: A RCCI Engine Application" index=200>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-200 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY, stat-ML<br>Keyword Score: 10<br>Keywords: Gaussian Process<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03602v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03602v2.pdf filename=2403.03602v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Cylinder pressure-based control is a key enabler for advanced pre-mixed combustion concepts. Besides guaranteeing robust and safe operation, it allows for cylinder pressure and heat release shaping. This requires fast control-oriented combustion models. Over the years, mean-value models have been proposed that can predict combustion measures (e.g., Gross Indicated Mean Effective Pressure, or the crank angle where 50% of the total heat is released) or models that predict the full in-cylinder pressure. However, these models are not able to capture cyclic variations. This is important in the control design for combustion concepts, like Reactivity Controlled Compression Ignition, that can suffer from large cyclic variations. In this study, the in-cylinder pressure and cyclic variation are modelled using a data-based approach. The model combines Principle Component Decomposition and <b>Gaussian</b> <b>Process</b> Regression. A detailed study is performed on the effects of the different hyperparameters and kernel choices. The approach is applicable to any combustion concept, but most valuable for advance combustion concepts with large cyclic variation. The potential of the proposed approach is demonstrated for an Reactivity Controlled Compression Ignition engine running on Diesel and E85. The prediction quality of the evaluated combustion measures has an overall accuracy of 13.5% and 65.5% in mean behaviour and standard deviation, respectively. The peak-pressure rise-rate is traditionally hard to predict, in the proposed model it has an accuracy of 22.7% and 96.4% in mean behaviour and standard deviation, respectively. This Principle Component Decomposition-based approach is an important step towards in-cylinder pressure shaping. The use of <b>Gaussian</b> <b>Process</b> Regression provides important information on cyclic variation and provides next-cycle controls information on safety and performance criteria.</p></p class="citation"></blockquote><h3 id=88--201271-on-discrete-time-polynomial-dynamical-systems-on-hypergraphs-shaoxuan-cui-et-al-2024>(8/8 | 201/271) On discrete-time polynomial dynamical systems on hypergraphs (Shaoxuan Cui et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shaoxuan Cui, Guofeng Zhang, Hildeberto Jardon-Kojakhmetov, Ming Cao. (2024)<br><strong>On discrete-time polynomial dynamical systems on hypergraphs</strong><br><button class=copy-to-clipboard title="On discrete-time polynomial dynamical systems on hypergraphs" index=201>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-201 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 10<br>Keywords: Discrete Time, Discrete Time<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03416v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03416v1.pdf filename=2403.03416v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper studies the stability of <b>discrete-time</b> <b>polynomial</b> dynamical systems on hypergraphs by utilizing the Perron-Frobenius theorem for nonnegative tensors with respect to the tensors&rsquo; Z-eigenvalues and Z-eigenvectors. First of all, for a multilinear polynomial system on a uniform hypergraph, we study the stability of the origin of the corresponding systems. Afterward, we extend our results to non-homogeneous polynomial systems on non-uniform hypergraphs. We confirm that the local stability of any <b>discrete-time</b> <b>polynomial</b> system is in general dominated by pairwise terms. In particular, given the origin is locally stable, we construct a conservative (but explicit) region of attraction from the system parameters. Finally, we validate our results via some numerical examples.</p></p class="citation"></blockquote><h2 id=cshc-5>cs.HC (5)</h2><h3 id=15--202271-promptcharm-text-to-image-generation-through-multi-modal-prompting-and-refinement-zhijie-wang-et-al-2024>(1/5 | 202/271) PromptCharm: Text-to-Image Generation through Multi-modal Prompting and Refinement (Zhijie Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhijie Wang, Yuheng Huang, Da Song, Lei Ma, Tianyi Zhang. (2024)<br><strong>PromptCharm: Text-to-Image Generation through Multi-modal Prompting and Refinement</strong><br><button class=copy-to-clipboard title="PromptCharm: Text-to-Image Generation through Multi-modal Prompting and Refinement" index=202>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-202 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-AI, cs-HC, cs.HC<br>Keyword Score: 43<br>Keywords: Diffusion Model, Generative AI, Multi-modal, Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04014v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04014v1.pdf filename=2403.04014v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The recent advancements in <b>Generative</b> <b>AI</b> have significantly advanced the field of <b>text-to-image</b> generation. The state-of-the-art <b>text-to-image</b> model, Stable <b>Diffusion,</b> <b>is</b> now capable of synthesizing high-quality images with a strong sense of aesthetics. Crafting text <b>prompts</b> that align with the model&rsquo;s interpretation and the user&rsquo;s intent thus becomes crucial. However, <b>prompting</b> remains challenging for novice users due to the complexity of the stable <b>diffusion</b> <b>model</b> and the non-trivial efforts required for iteratively editing and refining the text <b>prompts.</b> To address these challenges, we propose PromptCharm, a mixed-initiative system that facilitates <b>text-to-image</b> creation through <b>multi-modal</b> <b>prompt</b> engineering and refinement. To assist novice users in <b>prompting,</b> PromptCharm first automatically refines and optimizes the user&rsquo;s initial <b>prompt.</b> Furthermore, PromptCharm supports the user in exploring and selecting different image styles within a large database. To assist users in effectively refining their <b>prompts</b> and images, PromptCharm renders model explanations by visualizing the model&rsquo;s attention values. If the user notices any unsatisfactory areas in the generated images, they can further refine the images through model attention adjustment or image inpainting within the rich feedback loop of PromptCharm. To evaluate the effectiveness and usability of PromptCharm, we conducted a controlled user study with 12 participants and an exploratory user study with another 12 participants. These two studies show that participants using PromptCharm were able to create images with higher quality and better aligned with the user&rsquo;s expectations compared with using two variants of PromptCharm that lacked interaction or visualization support.</p></p class="citation"></blockquote><h3 id=25--203271-human-io-towards-a-unified-approach-to-detecting-situational-impairments-xingyu-bruce-liu-et-al-2024>(2/5 | 203/271) Human I/O: Towards a Unified Approach to Detecting Situational Impairments (Xingyu Bruce Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xingyu Bruce Liu, Jiahao Nick Li, David Kim, Xiang &lsquo;Anthony&rsquo; Chen, Ruofei Du. (2024)<br><strong>Human I/O: Towards a Unified Approach to Detecting Situational Impairments</strong><br><button class=copy-to-clipboard title="Human I/O: Towards a Unified Approach to Detecting Situational Impairments" index=203>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-203 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 36<br>Keywords: Multi-modal, Multi-modal, Reasoning, In-context Learning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04008v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04008v1.pdf filename=2403.04008v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Situationally Induced Impairments and Disabilities (SIIDs) can significantly hinder user experience in contexts such as poor lighting, noise, and multi-tasking. While prior research has introduced algorithms and systems to address these impairments, they predominantly cater to specific tasks or environments and fail to accommodate the diverse and dynamic nature of SIIDs. We introduce Human I/O, a unified approach to detecting a wide range of SIIDs by gauging the availability of human input/output channels. Leveraging egocentric vision, <b>multimodal</b> sensing and <b>reasoning</b> with <b>large</b> <b>language</b> <b>models,</b> Human I/O achieves a 0.22 mean absolute error and a 82% accuracy in availability prediction across 60 in-the-wild egocentric video recordings in 32 different scenarios. Furthermore, while the core focus of our work is on the detection of SIIDs rather than the creation of adaptive user interfaces, we showcase the efficacy of our prototype via a user study with 10 participants. Findings suggest that Human I/O significantly reduces effort and improves user experience in the presence of SIIDs, paving the way for more adaptive and accessible interactive systems in the future.</p></p class="citation"></blockquote><h3 id=35--204271-salientime-user-driven-selection-of-salient-time-steps-for-large-scale-geospatial-data-visualization-juntong-chen-et-al-2024>(3/5 | 204/271) SalienTime: User-driven Selection of Salient Time Steps for Large-Scale Geospatial Data Visualization (Juntong Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Juntong Chen, Haiwen Huang, Huayuan Ye, Zhong Peng, Chenhui Li, Changbo Wang. (2024)<br><strong>SalienTime: User-driven Selection of Salient Time Steps for Large-Scale Geospatial Data Visualization</strong><br><button class=copy-to-clipboard title="SalienTime: User-driven Selection of Salient Time Steps for Large-Scale Geospatial Data Visualization" index=204>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-204 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs-LG, cs.HC<br>Keyword Score: 30<br>Keywords: Autoencoder, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03449v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03449v1.pdf filename=2403.03449v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The voluminous nature of geospatial temporal data from physical monitors and <b>simulation</b> models poses challenges to efficient data access, often resulting in cumbersome temporal selection experiences in web-based data portals. Thus, selecting a subset of time steps for prioritized visualization and pre-loading is highly desirable. Addressing this issue, this paper establishes a multifaceted definition of salient time steps via extensive need-finding studies with domain experts to understand their workflows. Building on this, we propose a novel approach that leverages <b>autoencoders</b> and dynamic programming to facilitate user-driven temporal selections. Structural features, statistical variations, and distance penalties are incorporated to make more flexible selections. User-specified priorities, spatial regions, and aggregations are used to combine different perspectives. We design and implement a web-based interface to enable efficient and context-aware selection of time steps and evaluate its efficacy and usability through case studies, quantitative evaluations, and expert interviews.</p></p class="citation"></blockquote><h3 id=45--205271-assisting-international-migrants-with-everyday-information-seeking-from-the-providers-lens-yongle-zhang-et-al-2024>(4/5 | 205/271) Assisting International Migrants with Everyday Information Seeking: From the Providers&rsquo; Lens (Yongle Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yongle Zhang, Ge Gao. (2024)<br><strong>Assisting International Migrants with Everyday Information Seeking: From the Providers&rsquo; Lens</strong><br><button class=copy-to-clipboard title="Assisting International Migrants with Everyday Information Seeking: From the Providers' Lens" index=205>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-205 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 3<br>Keywords: Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04096v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04096v1.pdf filename=2403.04096v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>International migrants face difficulties obtaining information for a quality life and well-being in the host country. Prior research indicates that international migrants often seek information from their co-national cohort or contacts from the same country. The downside of this practice, however, is that people can end up <b>clustering</b> in a small-world environment, hindering the information seekers&rsquo; social adaptation in the long run. In the current research, we investigated the ongoing practices and future opportunities to connect international migrants with others beyond their co-national contacts. Our work zooms in on the providers&rsquo; perspectives, which complements previous studies that pay exclusive attention to the information seekers. Specifically, we conducted in-depth interviews with 21 participants assisting the needs of informational migrants in the United States. Some of these people are fellow migrants from a different home country than the information seeker, whereas the rest are domestic residents. Our data revealed how these participants dealt with language barriers, overcame knowledge disparities, and calibrated their effort commitment as information providers. Based on these findings, we discuss directions for future information and communication technologies (ICT) design that can facilitate international migrants&rsquo; daily information seeking by accounting for the provider&rsquo;s needs and concerns.</p></p class="citation"></blockquote><h3 id=55--206271-holens-a-visual-analytics-design-for-higher-order-movement-modeling-and-visualization-zezheng-feng-et-al-2024>(5/5 | 206/271) HoLens: A Visual Analytics Design for Higher-order Movement Modeling and Visualization (Zezheng Feng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zezheng Feng, Fang Zhu, Hongjun Wang, Jianing Hao, ShuangHua Yang, Wei Zeng, Huamin Qu. (2024)<br><strong>HoLens: A Visual Analytics Design for Higher-order Movement Modeling and Visualization</strong><br><button class=copy-to-clipboard title="HoLens: A Visual Analytics Design for Higher-order Movement Modeling and Visualization" index=206>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-206 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03822v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03822v1.pdf filename=2403.03822v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Higher-order patterns reveal sequential multistep state transitions, which are usually superior to origin-destination analysis, which depicts only first-order geospatial movement patterns. Conventional methods for higher-order movement modeling first construct a directed acyclic <b>graph</b> (DAG) of movements, then extract higher-order patterns from the DAG. However, DAG-based methods heavily rely on the identification of movement keypoints that are challenging for sparse movements and fail to consider the temporal variants that are critical for movements in urban environments. To overcome the limitations, we propose HoLens, a novel approach for modeling and visualizing higher-order movement patterns in the context of an urban environment. HoLens mainly makes twofold contributions: first, we design an auto-adaptive movement aggregation algorithm that self-organizes movements hierarchically by considering spatial proximity, contextual information, and temporal variability; second, we develop an interactive visual analytics interface consisting of well-established visualization techniques, including the H-Flow for visualizing the higher-order patterns on the map and the higher-order state sequence chart for representing the higher-order state transitions. Two real-world case studies manifest that the method can adaptively aggregate the data and exhibit the process of how to explore the higher-order patterns by HoLens. We also demonstrate our approach&rsquo;s feasibility, usability, and effectiveness through an expert interview with three domain experts.</p></p class="citation"></blockquote><h2 id=cscr-7>cs.CR (7)</h2><h3 id=17--207271-neural-exec-learning-and-learning-from-execution-triggers-for-prompt-injection-attacks-dario-pasquini-et-al-2024>(1/7 | 207/271) Neural Exec: Learning (and Learning from) Execution Triggers for Prompt Injection Attacks (Dario Pasquini et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dario Pasquini, Martin Strohmeier, Carmela Troncoso. (2024)<br><strong>Neural Exec: Learning (and Learning from) Execution Triggers for Prompt Injection Attacks</strong><br><button class=copy-to-clipboard title="Neural Exec: Learning (and Learning from) Execution Triggers for Prompt Injection Attacks" index=207>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-207 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-LG, cs.CR<br>Keyword Score: 40<br>Keywords: Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03792v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03792v1.pdf filename=2403.03792v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce a new family of <b>prompt</b> injection attacks, termed Neural Exec. Unlike known attacks that rely on handcrafted strings (e.g., &ldquo;Ignore previous instructions and&mldr;&rdquo;), we show that it is possible to conceptualize the creation of execution triggers as a differentiable search problem and use learning-based methods to autonomously generate them. Our results demonstrate that a motivated adversary can forge triggers that are not only drastically more effective than current handcrafted ones but also exhibit inherent flexibility in shape, properties, and functionality. In this direction, we show that an attacker can design and generate Neural Execs capable of persisting through multi-stage preprocessing pipelines, such as in the case of <b>Retrieval-Augmented</b> <b>Generation</b> <b>(RAG)-based</b> applications. More critically, our findings show that attackers can produce triggers that deviate markedly in form and shape from any known attack, sidestepping existing blacklist-based detection and sanitation approaches.</p></p class="citation"></blockquote><h3 id=27--208271-watermax-breaking-the-llm-watermark-detectability-robustness-quality-trade-off-eva-giboulot-et-al-2024>(2/7 | 208/271) WaterMax: breaking the LLM watermark detectability-robustness-quality trade-off (Eva Giboulot et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Eva Giboulot, Furon Teddy. (2024)<br><strong>WaterMax: breaking the LLM watermark detectability-robustness-quality trade-off</strong><br><button class=copy-to-clipboard title="WaterMax: breaking the LLM watermark detectability-robustness-quality trade-off" index=208>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-208 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CL, cs-CR, cs-LG, cs.CR<br>Keyword Score: 23<br>Keywords: Benchmarking, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04808v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04808v1.pdf filename=2403.04808v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Watermarking is a technical means to dissuade malfeasant usage of <b>Large</b> <b>Language</b> <b>Models.</b> This paper proposes a novel watermarking scheme, so-called WaterMax, that enjoys high detectability while sustaining the quality of the generated text of the original <b>LLM.</b> Its new design leaves the <b>LLM</b> untouched (no modification of the weights, logits, temperature, or sampling technique). WaterMax balances robustness and complexity contrary to the watermarking techniques of the literature inherently provoking a trade-off between quality and robustness. Its performance is both theoretically proven and experimentally validated. It outperforms all the SotA techniques under the most complete <b>benchmark</b> suite.</p></p class="citation"></blockquote><h3 id=37--209271-ztran-prototyping-zero-trust-security-xapps-for-open-radio-access-network-deployments-aly-s-abdalla-et-al-2024>(3/7 | 209/271) ZTRAN: Prototyping Zero Trust Security xApps for Open Radio Access Network Deployments (Aly S. Abdalla et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aly S. Abdalla, Joshua Moore, Nisha Adhikari, Vuk Marojevic. (2024)<br><strong>ZTRAN: Prototyping Zero Trust Security xApps for Open Radio Access Network Deployments</strong><br><button class=copy-to-clipboard title="ZTRAN: Prototyping Zero Trust Security xApps for Open Radio Access Network Deployments" index=209>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-209 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-ET, cs-SY, cs.CR, eess-SY<br>Keyword Score: 10<br>Keywords: Zero Trust<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04113v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04113v1.pdf filename=2403.04113v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The open radio access network (O-RAN) offers new degrees of freedom for building and operating advanced cellular networks. Emphasizing on RAN disaggregation, open interfaces, multi-vendor support, and RAN intelligent controllers (RICs), O-RAN facilitates adaptation to new applications and technology trends. Yet, this architecture introduces new security challenges. This paper proposes leveraging <b>zero</b> <b>trust</b> principles for O-RAN security. We introduce <b>zero</b> <b>trust</b> RAN (ZTRAN), which embeds service authentication, intrusion detection, and secure slicing subsystems that are encapsulated as xApps. We implement ZTRAN on the open artificial intelligence cellular (OAIC) research platform and demonstrate its feasibility and effectiveness in terms of legitimate user throughput and latency figures. Our experimental analysis illustrates how ZTRAN&rsquo;s intrusion detection and secure slicing microservices operate effectively and in concert as part of O-RAN Alliance&rsquo;s containerized near-real time RIC. Research directions include exploring machine learning and additional threat intelligence feeds for improving the performance and extending the scope of ZTRAN.</p></p class="citation"></blockquote><h3 id=47--210271-do-you-trust-your-model-emerging-malware-threats-in-the-deep-learning-ecosystem-dorjan-hitaj-et-al-2024>(4/7 | 210/271) Do You Trust Your Model? Emerging Malware Threats in the Deep Learning Ecosystem (Dorjan Hitaj et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dorjan Hitaj, Giulio Pagnotta, Fabio De Gaspari, Sediola Ruko, Briland Hitaj, Luigi V. Mancini, Fernando Perez-Cruz. (2024)<br><strong>Do You Trust Your Model? Emerging Malware Threats in the Deep Learning Ecosystem</strong><br><button class=copy-to-clipboard title="Do You Trust Your Model? Emerging Malware Threats in the Deep Learning Ecosystem" index=210>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-210 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs.CR<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03593v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03593v1.pdf filename=2403.03593v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Training high-quality deep learning models is a challenging task due to computational and technical requirements. A growing number of individuals, institutions, and companies increasingly rely on pre-trained, third-party models made available in public repositories. These models are often used directly or integrated in product pipelines with no particular precautions, since they are effectively just data in tensor form and considered safe. In this paper, we raise awareness of a new machine learning supply chain threat targeting neural networks. We introduce MaleficNet 2.0, a novel technique to embed self-extracting, self-executing malware in neural networks. MaleficNet 2.0 uses spread-spectrum channel coding combined with error correction techniques to inject malicious payloads in the parameters of deep neural networks. MaleficNet 2.0 injection technique is stealthy, does not degrade the performance of the model, and is robust against removal techniques. We design our approach to work both in traditional and distributed learning settings such as <b>Federated</b> <b>Learning,</b> and demonstrate that it is effective even when a reduced number of bits is used for the model parameters. Finally, we implement a proof-of-concept self-extracting neural network malware using MaleficNet 2.0, demonstrating the practicality of the attack against a widely adopted machine learning framework. Our aim with this work is to raise awareness against these new, dangerous attacks both in the research community and industry, and we hope to encourage further research in mitigation techniques against such threats.</p></p class="citation"></blockquote><h3 id=57--211271-deepeclipse-how-to-break-white-box-dnn-watermarking-schemes-alessandro-pegoraro-et-al-2024>(5/7 | 211/271) DeepEclipse: How to Break White-Box DNN-Watermarking Schemes (Alessandro Pegoraro et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alessandro Pegoraro, Carlotta Segna, Kavita Kumari, Ahmad-Reza Sadeghi. (2024)<br><strong>DeepEclipse: How to Break White-Box DNN-Watermarking Schemes</strong><br><button class=copy-to-clipboard title="DeepEclipse: How to Break White-Box DNN-Watermarking Schemes" index=211>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-211 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-LG, cs.CR<br>Keyword Score: 10<br>Keywords: Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03590v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03590v1.pdf filename=2403.03590v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep Learning (DL) models have become crucial in digital transformation, thus raising concerns about their intellectual property rights. Different watermarking techniques have been developed to protect Deep Neural Networks (DNNs) from IP infringement, creating a competitive field for DNN watermarking and removal methods. The predominant watermarking schemes use white-box techniques, which involve modifying weights by adding a unique signature to specific DNN layers. On the other hand, existing attacks on white-box watermarking usually require knowledge of the specific deployed watermarking scheme or access to the underlying data for further training and <b>fine-tuning.</b> We propose DeepEclipse, a novel and unified framework designed to remove white-box watermarks. We present obfuscation techniques that significantly differ from the existing white-box watermarking removal schemes. DeepEclipse can evade watermark detection without prior knowledge of the underlying watermarking scheme, additional data, or training and <b>fine-tuning.</b> Our evaluation reveals that DeepEclipse excels in breaking multiple white-box watermarking schemes, reducing watermark detection to random guessing while maintaining a similar model accuracy as the original one. Our framework showcases a promising solution to address the ongoing DNN watermark protection and removal challenges.</p></p class="citation"></blockquote><h3 id=67--212271-phenoauth-a-novel-puf-phenotype-based-authentication-protocol-for-iot-devices-hongming-fei-et-al-2024>(6/7 | 212/271) PhenoAuth: A Novel PUF-Phenotype-based Authentication Protocol for IoT Devices (Hongming Fei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hongming Fei, Owen Millwood, Gope Prosanta, Jack Miskelly, Biplab Sikdar. (2024)<br><strong>PhenoAuth: A Novel PUF-Phenotype-based Authentication Protocol for IoT Devices</strong><br><button class=copy-to-clipboard title="PhenoAuth: A Novel PUF-Phenotype-based Authentication Protocol for IoT Devices" index=212>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-212 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: 68M25, I-2-8, cs-CR, cs.CR<br>Keyword Score: 10<br>Keywords: Noise-tolerant<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03486v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03486v1.pdf filename=2403.03486v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Physical Unclonable Functions (PUFs) have been shown to be a highly promising solution for enabling high security systems tailored for low-power devices. Commonly, PUFs are utilised to generate cryptographic keys on-the-fly, replacing the need to store keys in vulnerable, non-volatile memories. Due to the physical nature of PUFs, environmental variations cause noise, manifesting themselves as errors which are apparent in the initial PUF measurements. This necessitates expensive active error correction techniques which can run counter to the goal of lightweight security. ML-based techniques for authenticating noisy PUF measurements were explored as an alternative to error correction techniques, bringing about the concept of a PUF Phenotype, where PUF identity is considered as a structure agnostic representation of the PUF, with relevant noise encoding. This work proposes a full <b>noise-tolerant</b> authentication protocol based on the PUF Phenotype concept and methodology for an Internet-of-Things (IoT) network, demonstrating mutual authentication and forward secrecy in a setting suitable for device-to-device communication. Upon conducting security and performance analyses, it is evident that our proposed scheme demonstrates resilience against various attacks compared to the currently existing PUF protocols.</p></p class="citation"></blockquote><h3 id=77--213271-wildest-dreams-reproducible-research-in-privacy-preserving-neural-network-training-tanveer-khan-et-al-2024>(7/7 | 213/271) Wildest Dreams: Reproducible Research in Privacy-preserving Neural Network Training (Tanveer Khan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tanveer Khan, Mindaugas Budzys, Khoa Nguyen, Antonis Michalas. (2024)<br><strong>Wildest Dreams: Reproducible Research in Privacy-preserving Neural Network Training</strong><br><button class=copy-to-clipboard title="Wildest Dreams: Reproducible Research in Privacy-preserving Neural Network Training" index=213>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-213 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs.CR<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03592v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03592v1.pdf filename=2403.03592v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Machine Learning (ML), addresses a multitude of complex issues in multiple disciplines, including social sciences, finance, and medical research. ML models require substantial computing power and are only as powerful as the data utilized. Due to high computational cost of ML methods, data scientists frequently use Machine Learning-as-a-Service (MLaaS) to outsource computation to external servers. However, when working with private information, like financial data or health records, outsourcing the computation might result in privacy issues. Recent advances in Privacy-Preserving Techniques (PPTs) have enabled ML training and inference over protected data through the use of Privacy-Preserving Machine Learning (PPML). However, these techniques are still at a preliminary stage and their application in real-world situations is demanding. In order to comprehend discrepancy between theoretical research suggestions and actual applications, this work examines the past and present of PPML, focusing on Homomorphic Encryption (HE) and Secure Multi-party Computation (SMPC) applied to ML. This work primarily focuses on the ML model&rsquo;s training phase, where maintaining user data privacy is of utmost importance. We provide a solid theoretical background that eases the understanding of current approaches and their limitations. In addition, we present a SoK of the most recent PPML frameworks for model training and provide a comprehensive comparison in terms of the unique properties and performances on standard <b>benchmarks.</b> Also, we reproduce the results for some of the papers and examine at what level existing works in the field provide support for open science. We believe our work serves as a valuable contribution by raising awareness about the current gap between theoretical advancements and real-world applications in PPML, specifically regarding open-source availability, reproducibility, and usability.</p></p class="citation"></blockquote><h2 id=csce-3>cs.CE (3)</h2><h3 id=13--214271-unsupervised-incremental-learning-with-dual-concept-drift-detection-for-identifying-anomalous-sequences-jin-li-et-al-2024>(1/3 | 214/271) Unsupervised Incremental Learning with Dual Concept Drift Detection for Identifying Anomalous Sequences (Jin Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jin Li, Kleanthis Malialis, Christos G. Panayiotou, Marios M. Polycarpou. (2024)<br><strong>Unsupervised Incremental Learning with Dual Concept Drift Detection for Identifying Anomalous Sequences</strong><br><button class=copy-to-clipboard title="Unsupervised Incremental Learning with Dual Concept Drift Detection for Identifying Anomalous Sequences" index=214>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-214 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CE<br>Categories: cs-CE, cs.CE<br>Keyword Score: 40<br>Keywords: Anomaly Detection, Autoencoder, Unsupervised Learning, Variational Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03576v3 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03576v3.pdf filename=2403.03576v3.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the contemporary digital landscape, the continuous generation of extensive streaming data across diverse domains has become pervasive. Yet, a significant portion of this data remains unlabeled, posing a challenge in identifying infrequent events such as anomalies. This challenge is further amplified in non-stationary environments, where the performance of models can degrade over time due to concept drift. To address these challenges, this paper introduces a new method referred to as VAE4AS <b>(Variational</b> <b>Autoencoder</b> for Anomalous Sequences). VAE4AS integrates incremental learning with dual drift detection mechanisms, employing both a statistical test and a distance-based test. The <b>anomaly</b> <b>detection</b> is facilitated by a <b>Variational</b> <b>Autoencoder.</b> To gauge the effectiveness of VAE4AS, a comprehensive experimental study is conducted using real-world and synthetic datasets characterized by anomalous rates below 10% and recurrent drift. The results show that the proposed method surpasses both robust baselines and state-of-the-art techniques, providing compelling evidence for their efficacy in effectively addressing some of the challenges associated with anomalous sequence detection in non-stationary streaming data.</p></p class="citation"></blockquote><h3 id=23--215271-blockchain-and-carbon-markets-standards-overview-pedro-baiz-2024>(2/3 | 215/271) Blockchain and Carbon Markets: Standards Overview (Pedro Baiz, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pedro Baiz. (2024)<br><strong>Blockchain and Carbon Markets: Standards Overview</strong><br><button class=copy-to-clipboard title="Blockchain and Carbon Markets: Standards Overview" index=215>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-215 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CE<br>Categories: cs-CE, cs.CE<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03865v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03865v1.pdf filename=2403.03865v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The increasing significance of sustainability considerations within both public spheres (such as policies and regulations) and private sectors (including voluntary commitments by major multinational corporations) underscores the imperative to harness cutting-edge technological advancements. This is essential to ensure that the momentum of this trend translates into tangible outcomes, thwarting phenomena like greenwashing and upholding high standards of integrity, all while expediting progress through automation. This paper focuses specifically on carbon markets, which, after enduring years of confusion and controversy, may finally be on the brink of converging toward internationally recognized minimum standards. Beginning with an introduction to fundamental concepts pertaining to carbon markets and Distributed Ledger Technologies (DLTs), the paper proceeds to dissect the challenges and opportunities within this burgeoning field. Its primary contribution lies in offering a comprehensive overview of recent developments across various initiatives (such as ICVCM, IETA/WorldBank/CAD Trust, IEEE/ISO) and providing a layered analysis of the entire ecosystem. This framework aids in understanding and prioritising future endeavours. Ultimately, the paper furnishes a set of <b>recommendations</b> aimed at bolstering scalability and fostering widespread adoption of best practices within international markets.</p></p class="citation"></blockquote><h3 id=33--216271-multi-time-step-coupling-of-peridynamics-and-classical-continuum-mechanics-for-dynamic-brittle-fracture-zhong-jiandong-et-al-2024>(3/3 | 216/271) Multi-time-step coupling of peridynamics and classical continuum mechanics for dynamic brittle fracture (Zhong Jiandong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhong Jiandong, Han Fei, Du Zongliang, Guo Xu. (2024)<br><strong>Multi-time-step coupling of peridynamics and classical continuum mechanics for dynamic brittle fracture</strong><br><button class=copy-to-clipboard title="Multi-time-step coupling of peridynamics and classical continuum mechanics for dynamic brittle fracture" index=216>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-216 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CE<br>Categories: cs-CE, cs.CE<br>Keyword Score: 10<br>Keywords: Neural Machine Translation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03605v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03605v1.pdf filename=2403.03605v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Peridynamics (PD), as a nonlocal theory, is well-suited for solving problems with discontinuities, such as cracks. However, the nonlocal effect of peridynamics makes it computationally expensive for dynamic fracture problems in large-scale engineering applications. As an alternative, this study proposes a multi-time-step <b>(MTS)</b> coupling model of PD and classical continuum mechanics (CCM) based on the Arlequin framework. Peridynamics is applied to the fracture domain of the structure, while continuum mechanics is applied to the rest of the structure. The <b>MTS</b> method enables the peridynamic model to be solved at a small time step and the continuum mechanical model is solved at a larger time step. Consequently, higher computational efficiency is achieved for the fracture domain of the structure while ensuring computational accuracy, and this coupling method can be easily applied to large-scale engineering fracture problems.</p></p class="citation"></blockquote><h2 id=eesssp-2>eess.SP (2)</h2><h3 id=12--217271-diffusion-based-generative-prior-for-low-complexity-mimo-channel-estimation-benedikt-fesl-et-al-2024>(1/2 | 217/271) Diffusion-based Generative Prior for Low-Complexity MIMO Channel Estimation (Benedikt Fesl et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Benedikt Fesl, Michael Baur, Florian Strasser, Michael Joham, Wolfgang Utschick. (2024)<br><strong>Diffusion-based Generative Prior for Low-Complexity MIMO Channel Estimation</strong><br><button class=copy-to-clipboard title="Diffusion-based Generative Prior for Low-Complexity MIMO Channel Estimation" index=217>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-217 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SP<br>Categories: cs-LG, eess-SP, eess.SP<br>Keyword Score: 40<br>Keywords: Diffusion Model, Convolution, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03545v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03545v1.pdf filename=2403.03545v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work proposes a novel channel estimator based on <b>diffusion</b> <b>models</b> (DMs), one of the currently top-rated generative models. Contrary to related works utilizing generative priors, a lightweight <b>convolutional</b> <b>neural</b> <b>network</b> <b>(CNN)</b> with positional embedding of the signal-to-noise ratio (SNR) information is designed by learning the channel distribution in the sparse angular domain. Combined with an estimation strategy that avoids stochastic resampling and truncates reverse <b>diffusion</b> <b>steps</b> that account for lower SNR than the given pilot observation, the resulting DM estimator has both low complexity and memory overhead. Numerical results exhibit better performance than state-of-the-art channel estimators utilizing generative priors.</p></p class="citation"></blockquote><h3 id=22--218271-joint-sparsity-pattern-learning-based-channel-estimation-for-massive-mimo-otfs-systems-kuo-meng-et-al-2024>(2/2 | 218/271) Joint Sparsity Pattern Learning Based Channel Estimation for Massive MIMO-OTFS Systems (Kuo Meng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kuo Meng, Shaoshi Yang, Xiao-Yang Wang, Yan Bu, Yurong Tang, Jianhua Zhang, Lajos Hanzo. (2024)<br><strong>Joint Sparsity Pattern Learning Based Channel Estimation for Massive MIMO-OTFS Systems</strong><br><button class=copy-to-clipboard title="Joint Sparsity Pattern Learning Based Channel Estimation for Massive MIMO-OTFS Systems" index=218>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-218 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SP<br>Categories: cs-LG, eess-SP, eess.SP<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03771v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03771v1.pdf filename=2403.03771v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose a channel estimation scheme based on joint sparsity pattern learning (JSPL) for massive multi-input multi-output (MIMO) orthogonal time-frequency-space (OTFS) modulation aided systems. By exploiting the potential joint sparsity of the delay-Doppler-angle (DDA) domain channel, the channel estimation problem is transformed into a sparse recovery problem. To solve it, we first apply the spike and slab prior model to iteratively estimate the support set of the channel matrix, and a higher-accuracy parameter update rule relying on the identified support set is introduced into the iteration. Then the specific values of the channel elements corresponding to the support set are estimated by the orthogonal matching pursuit (OMP) method. Both our <b>simulation</b> results and analysis demonstrate that the proposed JSPL channel estimation scheme achieves an improved performance over the representative state-of-the-art baseline schemes, despite its reduced pilot overhead.</p></p class="citation"></blockquote><h2 id=q-fincp-1>q-fin.CP (1)</h2><h3 id=11--219271-enhancing-price-prediction-in-cryptocurrency-using-transformer-neural-network-and-technical-indicators-mohammad-ali-labbaf-khaniki-et-al-2024>(1/1 | 219/271) Enhancing Price Prediction in Cryptocurrency Using Transformer Neural Network and Technical Indicators (Mohammad Ali Labbaf Khaniki et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohammad Ali Labbaf Khaniki, Mohammad Manthouri. (2024)<br><strong>Enhancing Price Prediction in Cryptocurrency Using Transformer Neural Network and Technical Indicators</strong><br><button class=copy-to-clipboard title="Enhancing Price Prediction in Cryptocurrency Using Transformer Neural Network and Technical Indicators" index=219>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-219 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-fin.CP<br>Categories: cs-AI, cs-LG, q-fin-CP, q-fin.CP<br>Keyword Score: 33<br>Keywords: Benchmarking, LSTM, LSTM, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03606v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03606v1.pdf filename=2403.03606v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study presents an innovative approach for predicting cryptocurrency time series, specifically focusing on Bitcoin, Ethereum, and Litecoin. The methodology integrates the use of technical indicators, a Performer neural network, and BiLSTM (Bidirectional <b>Long</b> <b>Short-Term</b> <b>Memory)</b> <b>to</b> capture temporal dynamics and extract significant features from raw cryptocurrency data. The application of technical indicators, such facilitates the extraction of intricate patterns, momentum, volatility, and trends. The Performer neural network, employing Fast Attention Via positive Orthogonal Random features (FAVOR+), has demonstrated superior computational efficiency and scalability compared to the traditional Multi-head attention mechanism in <b>Transformer</b> models. Additionally, the integration of BiLSTM in the feedforward network enhances the model&rsquo;s capacity to capture temporal dynamics in the data, processing it in both forward and backward directions. This is particularly advantageous for time series data where past and future data points can influence the current state. The proposed method has been applied to the hourly and daily timeframes of the major cryptocurrencies and its performance has been <b>benchmarked</b> against other methods documented in the literature. The results underscore the potential of the proposed method to outperform existing models, marking a significant progression in the field of cryptocurrency price prediction.</p></p class="citation"></blockquote><h2 id=csgt-7>cs.GT (7)</h2><h3 id=17--220271-fair-artificial-currency-incentives-in-repeated-weighted-congestion-games-equity-vs-equality-leonardo-pedroso-et-al-2024>(1/7 | 220/271) Fair Artificial Currency Incentives in Repeated Weighted Congestion Games: Equity vs. Equality (Leonardo Pedroso et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Leonardo Pedroso, Andrea Agazzi, W. P. M. H. Heemels, Mauro Salazar. (2024)<br><strong>Fair Artificial Currency Incentives in Repeated Weighted Congestion Games: Equity vs. Equality</strong><br><button class=copy-to-clipboard title="Fair Artificial Currency Incentives in Repeated Weighted Congestion Games: Equity vs. Equality" index=220>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-220 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-GT, cs-SY, cs.GT, eess-SY<br>Keyword Score: 30<br>Keywords: Fairness, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03999v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03999v1.pdf filename=2403.03999v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>When users access shared resources in a selfish manner, the resulting societal cost and perceived users&rsquo; cost is often higher than what would result from a centrally coordinated optimal allocation. While several contributions in mechanism design manage to steer the aggregate users choices to the desired optimum by using monetary tolls, such approaches bear the inherent drawback of discriminating against users with a lower income. More recently, incentive schemes based on artificial currencies have been studied with the goal of achieving a system-optimal resource allocation that is also fair. In this resource-sharing context, this paper focuses on repeated weighted congestion game with two resources, where users contribute to the congestion to different extents that are captured by individual weights. First, we address the broad concept of <b>fairness</b> by providing a rigorous mathematical characterization of the distinct societal metrics of equity and equality, i.e., the concepts of providing equal outcomes and equal opportunities, respectively. Second, we devise weight-dependent and time-invariant optimal pricing policies to maximize equity and equality, and prove convergence of the aggregate user choices to the system-optimum. In our framework it is always possible to achieve system-optimal allocations with perfect equity, while the maximum equality that can be reached may not be perfect, which is also shown via numerical <b>simulations.</b></p></p class="citation"></blockquote><h3 id=27--221271-application-of-nash-equilibrium-for-developing-an-optimal-forest-harvesting-strategy-in-toruń-forest-district-jan-kotlarz-2024>(2/7 | 221/271) Application of Nash equilibrium for developing an optimal forest harvesting strategy in Toruń Forest District (Jan Kotlarz, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jan Kotlarz. (2024)<br><strong>Application of Nash equilibrium for developing an optimal forest harvesting strategy in Toruń Forest District</strong><br><button class=copy-to-clipboard title="Application of Nash equilibrium for developing an optimal forest harvesting strategy in Toruń Forest District" index=221>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-221 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-GT, cs.GT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03555v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03555v1.pdf filename=2403.03555v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study investigates the application of Nash equilibrium strategies in optimizing forest harvesting decisions, focusing on multiple management objectives in forestry. Through <b>simulation-based</b> analysis, the research explores the evolution of various indicators during the game: 1) the mass of CO2 sequestration, 2) forest stands biodiversity, 3) the harvested wood volume, 4) native species fraction, and 5) protective functions. The results underscore the importance of considering diverse objectives and balancing competing interests in forestry decision processes. The forest stands designated for harvesting in the Toru'n Forest District were defined as the initial strategy, and indicators for all objectives were calculated accordingly. A Nash equilibrium was identified through a game involving five players representing individual objectives with partially conflicting aims. The final strategy was obtained by modifying specific forest stands designated for harvesting, thereby maintaining the planned wood volume extraction while simultaneously reducing biodiversity loss by nearly 40%, preserving protective functions across over 600 hectares of forested areas, enhancing decadal carbon sequestration in the forest district by 100,000 tons, and additionally improving species suitability by nearly 10%. The findings suggest the potential for further research and refinement of Nash equilibrium-based optimization approaches to enhance the effectiveness and sustainability of forest management practices.</p></p class="citation"></blockquote><h3 id=37--222271-adaptive-coordination-promotes-collective-cooperation-in-repeated-social-dilemmas-feipeng-zhang-et-al-2024>(3/7 | 222/271) Adaptive coordination promotes collective cooperation in repeated social dilemmas (Feipeng Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Feipeng Zhang, Te Wu, Long Wang. (2024)<br><strong>Adaptive coordination promotes collective cooperation in repeated social dilemmas</strong><br><button class=copy-to-clipboard title="Adaptive coordination promotes collective cooperation in repeated social dilemmas" index=222>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-222 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-GT, cs.GT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03497v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03497v1.pdf filename=2403.03497v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Direct reciprocity based on the repeated prisoner&rsquo;s dilemma has been intensively studied. Most theoretical investigations have concentrated on memory-$1$ strategies, a class of elementary strategies just reacting to the previous-round outcomes. Though the properties of &ldquo;All-or-None&rdquo; strategies ($AoN_K$) have been discovered, <b>simulations</b> just confirmed the good performance of $AoN_K$ of very short memory lengths. It remains unclear how $AoN_K$ strategies would fare when players have access to longer rounds of history information. We construct a theoretical model to investigate the performance of the class of $AoN_K$ strategies of varying memory length $K$. We rigorously derive the payoffs and show that $AoN_K$ strategies of intermediate memory length $K$ are most prevalent, while strategies of larger memory lengths are less competent. Larger memory lengths make it hard for $AoN_K$ strategies to coordinate, and thus inhibiting their mutual reciprocity. We then propose the adaptive coordination strategy combining tolerance and $AoN_K$&rsquo; coordination rule. This strategy behaves like $AoN_K$ strategy when coordination is not sufficient, and tolerates opponents&rsquo; occasional deviations by still cooperating when coordination is sufficient. We found that the adaptive coordination strategy wins over other classic memory-$1$ strategies in various typical competition environments, and stabilizes the population at high levels of cooperation, suggesting the effectiveness of high level adaptability in resolving social dilemmas. Our work may offer a theoretical framework for exploring complex strategies using history information, which are different from traditional memory-$n$ strategies.</p></p class="citation"></blockquote><h3 id=47--223271-to-spend-or-to-gain-online-learning-in-repeated-karma-auctions-damien-berriaud-et-al-2024>(4/7 | 223/271) To Spend or to Gain: Online Learning in Repeated Karma Auctions (Damien Berriaud et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Damien Berriaud, Ezzat Elokda, Devansh Jalota, Emilio Frazzoli, Marco Pavone, Florian Dörfler. (2024)<br><strong>To Spend or to Gain: Online Learning in Repeated Karma Auctions</strong><br><button class=copy-to-clipboard title="To Spend or to Gain: Online Learning in Repeated Karma Auctions" index=223>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-223 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-GT, cs.GT, econ-TH<br>Keyword Score: 10<br>Keywords: In-context Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04057v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04057v1.pdf filename=2403.04057v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent years have seen a surge of artificial currency-based mechanisms in contexts where monetary instruments are deemed unfair or inappropriate, e.g., in allocating food donations to food banks, course seats to students, and, more recently, even for traffic congestion management. Yet the applicability of these mechanisms remains limited in repeated auction settings, as it is challenging for users to learn how to bid an artificial currency that has no value outside the auctions. Indeed, users must jointly learn the value of the currency in addition to how to spend it optimally. In this work, we study the problem of learning to bid in two prominent classes of artificial currency auctions: those in which currency, which users spend to obtain public resources, is only issued at the beginning of a finite period; and those where, in addition to the initial currency endowment, currency payments are redistributed to users at each time step. In the latter class, the currency has been referred to as karma, since users do not only spend karma to obtain public resources but also gain karma for yielding them. In both classes, we propose a simple learning strategy, called adaptive karma pacing, and show that this strategy a) is asymptotically optimal for a single user bidding against competing bids drawn from a stationary distribution; b) leads to convergent learning dynamics when all users adopt it; and c) constitutes an approximate Nash equilibrium as the number of users grows. Our results require a novel analysis in comparison to adaptive pacing strategies in monetary auctions, since we depart from the classical assumption that the currency has known value outside the auctions, and moreover consider that the currency is both spent and gained in the class of auctions with redistribution.</p></p class="citation"></blockquote><h3 id=57--224271-empirical-game-theoretic-analysis-a-survey-michael-p-wellman-et-al-2024>(5/7 | 224/271) Empirical Game-Theoretic Analysis: A Survey (Michael P. Wellman et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michael P. Wellman, Karl Tuyls, Amy Greenwald. (2024)<br><strong>Empirical Game-Theoretic Analysis: A Survey</strong><br><button class=copy-to-clipboard title="Empirical Game-Theoretic Analysis: A Survey" index=224>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-224 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-GT, cs.GT<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04018v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04018v1.pdf filename=2403.04018v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the empirical approach to game-theoretic analysis (EGTA), the model of the game comes not from declarative representation, but is derived by interrogation of a procedural description of the game environment. The motivation for developing this approach was to enable game-theoretic <b>reasoning</b> about strategic situations too complex for analytic specification and solution. Since its introduction over twenty years ago, EGTA has been applied to a wide range of multiagent domains, from auctions and markets to recreational games to cyber-security. We survey the extensive methodology developed for EGTA over the years, organized by the elemental subproblems comprising the EGTA process. We describe key EGTA concepts and techniques, and the questions at the frontier of EGTA research. Recent advances in machine learning have accelerated progress in EGTA, and promise to significantly expand our capacities for <b>reasoning</b> about complex game situations.</p></p class="citation"></blockquote><h3 id=67--225271-population-aware-online-mirror-descent-for-mean-field-games-by-deep-reinforcement-learning-zida-wu-et-al-2024>(6/7 | 225/271) Population-aware Online Mirror Descent for Mean-Field Games by Deep Reinforcement Learning (Zida Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zida Wu, Mathieu Lauriere, Samuel Jia Cong Chua, Matthieu Geist, Olivier Pietquin, Ankur Mehta. (2024)<br><strong>Population-aware Online Mirror Descent for Mean-Field Games by Deep Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Population-aware Online Mirror Descent for Mean-Field Games by Deep Reinforcement Learning" index=225>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-225 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-GT, cs-LG, cs-MA, cs-SY, cs.GT, eess-SY<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03552v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03552v1.pdf filename=2403.03552v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Mean Field Games (MFGs) have the ability to handle large-scale multi-agent systems, but learning Nash equilibria in MFGs remains a challenging task. In this paper, we propose a deep <b>reinforcement</b> <b>learning</b> (DRL) algorithm that achieves population-dependent Nash equilibrium without the need for averaging or sampling from history, inspired by Munchausen RL and Online Mirror Descent. Through the design of an additional inner-loop replay buffer, the agents can effectively learn to achieve Nash equilibrium from any distribution, mitigating catastrophic forgetting. The resulting policy can be applied to various initial distributions. Numerical experiments on four canonical examples demonstrate our algorithm has better convergence properties than SOTA algorithms, in particular a DRL version of Fictitious Play for population-dependent policies.</p></p class="citation"></blockquote><h3 id=77--226271-to-trust-or-not-to-trust-assignment-mechanisms-with-predictions-in-the-private-graph-model-riccardo-colini-baldeschi-et-al-2024>(7/7 | 226/271) To Trust or Not to Trust: Assignment Mechanisms with Predictions in the Private Graph Model (Riccardo Colini-Baldeschi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Riccardo Colini-Baldeschi, Sophie Klumper, Guido Schäfer, Artem Tsikiridis. (2024)<br><strong>To Trust or Not to Trust: Assignment Mechanisms with Predictions in the Private Graph Model</strong><br><button class=copy-to-clipboard title="To Trust or Not to Trust: Assignment Mechanisms with Predictions in the Private Graph Model" index=226>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-226 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-GT, cs.GT<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03725v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03725v1.pdf filename=2403.03725v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The realm of algorithms with predictions has led to the development of several new algorithms that leverage (potentially erroneous) predictions to enhance their performance guarantees. The challenge is to devise algorithms that achieve optimal approximation guarantees as the prediction quality varies from perfect (consistency) to imperfect (robustness). This framework is particularly appealing in mechanism design contexts, where predictions might convey private information about the agents. In this paper, we design strategyproof mechanisms that leverage predictions to achieve improved approximation guarantees for several variants of the Generalized Assignment Problem (GAP) in the private <b>graph</b> model. In this model, first introduced by Dughmi & Ghosh (2010), the set of resources that an agent is compatible with is private information. For the Bipartite Matching Problem (BMP), we give a deterministic group-strategyproof (GSP) mechanism that is $(1 +1/\gamma)$-consistent and $(1 + \gamma)$-robust, where $\gamma \ge 1$ is some confidence parameter. We also prove that this is best possible. Remarkably, our mechanism draws inspiration from the renowned Gale-Shapley algorithm, incorporating predictions as a crucial element. Additionally, we give a randomized mechanism that is universally GSP and improves on the guarantees in expectation. The other GAP variants that we consider all make use of a unified greedy mechanism that adds edges to the assignment according to a specific order. Our universally GSP mechanism randomizes over the greedy mechanism, our mechanism for BMP and the predicted assignment, leading to $(1+3/\gamma)$-consistency and $(3+\gamma)$-robustness in expectation. All our mechanisms also provide more fine-grained approximation guarantees that interpolate between the consistency and the robustness, depending on some natural error measure of the prediction.</p></p class="citation"></blockquote><h2 id=physicssoc-ph-1>physics.soc-ph (1)</h2><h3 id=11--227271-introducing-first-principles-calculations-new-approach-to-group-dynamics-and-bridging-social-phenomena-in-tenp-chain-based-social-dynamics-simulations-yasuko-kawahata-2024>(1/1 | 227/271) Introducing First-Principles Calculations: New Approach to Group Dynamics and Bridging Social Phenomena in TeNP-Chain Based Social Dynamics Simulations (Yasuko Kawahata, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yasuko Kawahata. (2024)<br><strong>Introducing First-Principles Calculations: New Approach to Group Dynamics and Bridging Social Phenomena in TeNP-Chain Based Social Dynamics Simulations</strong><br><button class=copy-to-clipboard title="Introducing First-Principles Calculations: New Approach to Group Dynamics and Bridging Social Phenomena in TeNP-Chain Based Social Dynamics Simulations" index=227>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-227 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.soc-ph<br>Categories: cs-AI, physics-ed-ph, physics-soc-ph, physics.soc-ph<br>Keyword Score: 30<br>Keywords: Simulation, Simulator, Fake News Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05593v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05593v1.pdf filename=2403.05593v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This note considers an innovative interdisciplinary methodology that bridges the gap between the fundamental principles of quantum mechanics applied to the study of materials such as tellurium nanoparticles (TeNPs) and graphene and the complex dynamics of social systems. The basis for this approach lies in the metaphorical parallels drawn between the structural features of TeNPs and graphene and the behavioral patterns of social groups in the face of misinformation. TeNPs exhibit unique properties such as the strengthening of covalent bonds within telluric chains and the disruption of secondary structure leading to the separation of these chains. This is analogous to increased cohesion within social groups and disruption of information flow between different subgroups, respectively. . Similarly, the outstanding properties of graphene, such as high electrical conductivity, strength, and flexibility, provide additional aspects for understanding the resilience and adaptability of social structures in response to external stimuli such as <b>fake</b> <b>news.</b> This research note proposes a novel metaphorical framework for analyzing the spread of <b>fake</b> <b>news</b> within social groups, analogous to the structural features of telluric nanoparticles (TeNPs). We investigate how the strengthening of covalent bonds within TeNPs reflects the strengthening of social cohesion in groups that share common beliefs and values.</p></p class="citation"></blockquote><h2 id=csdc-2>cs.DC (2)</h2><h3 id=12--228271-portable-heterogeneous-ensemble-workflows-at-scale-using-libensemble-stephen-hudson-et-al-2024>(1/2 | 228/271) Portable, heterogeneous ensemble workflows at scale using libEnsemble (Stephen Hudson et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Stephen Hudson, Jeffrey Larson, John-Luke Navarro, Stefan M. Wild. (2024)<br><strong>Portable, heterogeneous ensemble workflows at scale using libEnsemble</strong><br><button class=copy-to-clipboard title="Portable, heterogeneous ensemble workflows at scale using libEnsemble" index=228>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-228 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs.DC<br>Keyword Score: 30<br>Keywords: Gaussian Process, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03709v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03709v2.pdf filename=2403.03709v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>libEnsemble is a Python-based toolkit for running dynamic ensembles, developed as part of the DOE Exascale Computing Project. The toolkit utilizes a unique generator-simulator-allocator paradigm, where generators produce input for simulators, simulators evaluate those inputs, and allocators decide whether and when a simulator or generator should be called. The generator steers the ensemble based on <b>simulation</b> results. libEnsemble communicates between a manager and workers. Flexibility is provided through multiple manager-worker communication substrates each of which has different benefits. These include Python&rsquo;s multiprocessing, mpi4py, and TCP. Multisite ensembles are supported using Balsam or Globus Compute. We overview the unique characteristics of libEnsemble as well as current and potential interoperability with other packages in the workflow ecosystem. We highlight libEnsemble&rsquo;s dynamic resource features: libEnsemble can detect system resources (nodes, cores, and GPUs) and assign these in a portable way. These features allow users to specify resources required for each <b>simulation</b> automatically on a range of systems, including Frontier, Aurora, and Perlmutter. Such ensembles can include multiple <b>simulation</b> types, some using GPUs and others using only CPUs, sharing nodes for maximum efficiency. We demonstrate libEnsemble&rsquo;s capabilities, scalability, and scientific impact via a <b>Gaussian</b> <b>process</b> surrogate training problem for the longitudinal density profile at the exit of a plasma accelerator stage using Wake-T and WarpX <b>simulations.</b> We also describe the benefits of libEnsemble&rsquo;s generator-simulator coupling, which easily exposes to the user the ability to cancel, and portably kill, running <b>simulations.</b> Such control can be directed from the generator or allocator based on models that are updated with intermediate <b>simulation</b> output.</p></p class="citation"></blockquote><h3 id=22--229271-model-parallelism-on-distributed-infrastructure-a-literature-review-from-theory-to-llm-case-studies-felix-brakel-et-al-2024>(2/2 | 229/271) Model Parallelism on Distributed Infrastructure: A Literature Review from Theory to LLM Case-Studies (Felix Brakel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Felix Brakel, Uraz Odyurt, Ana-Lucia Varbanescu. (2024)<br><strong>Model Parallelism on Distributed Infrastructure: A Literature Review from Theory to LLM Case-Studies</strong><br><button class=copy-to-clipboard title="Model Parallelism on Distributed Infrastructure: A Literature Review from Theory to LLM Case-Studies" index=229>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-229 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs-LG, cs.DC<br>Keyword Score: 23<br>Keywords: Graph, Transformer, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03699v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03699v1.pdf filename=2403.03699v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neural networks have become a cornerstone of machine learning. As the trend for these to get more and more complex continues, so does the underlying hardware and software infrastructure for training and deployment. In this survey we answer three research questions: &ldquo;What types of model parallelism exist?&rdquo;, &ldquo;What are the challenges of model parallelism?&rdquo;, and &ldquo;What is a modern use-case of model parallelism?&rdquo; We answer the first question by looking at how neural networks can be parallelised and expressing these as operator <b>graphs</b> while exploring the available dimensions. The dimensions along which neural networks can be parallelised are intra-operator and inter-operator. We answer the second question by collecting and listing both implementation challenges for the types of parallelism, as well as the problem of optimally partitioning the operator <b>graph.</b> We answer the last question by collecting and listing how parallelism is applied in modern multi-billion parameter <b>transformer</b> networks, to the extend that this is possible with the limited information shared about these networks.</p></p class="citation"></blockquote><h2 id=cspl-1>cs.PL (1)</h2><h3 id=11--230271-generative-explanations-for-program-synthesizers-amirmohammad-nazari-et-al-2024>(1/1 | 230/271) Generative Explanations for Program Synthesizers (Amirmohammad Nazari et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amirmohammad Nazari, Souti Chattopadhyay, Swabha Swayamdipta, Mukund Raghothaman. (2024)<br><strong>Generative Explanations for Program Synthesizers</strong><br><button class=copy-to-clipboard title="Generative Explanations for Program Synthesizers" index=230>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-230 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.PL<br>Categories: cs-PL, cs.PL<br>Keyword Score: 30<br>Keywords: Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03429v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03429v1.pdf filename=2403.03429v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite great advances in program synthesis techniques, they remain algorithmic black boxes. Although they guarantee that when synthesis is successful, the implementation satisfies the specification, they provide no additional information regarding how the implementation works or the manner in which the specification is realized. One possibility to answer these questions is to use <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to construct human-readable explanations. Unfortunately, experiments reveal that <b>LLMs</b> frequently produce nonsensical or misleading explanations when applied to the unidiomatic code produced by program synthesizers. In this paper, we develop an approach to reliably augment the implementation with explanatory names. We recover fine-grained input-output data from the synthesis algorithm to enhance the <b>prompt</b> supplied to the <b>LLM,</b> and use a combination of a program verifier and a second language model to validate the proposed explanations before presenting them to the user. Together, these techniques massively improve the accuracy of the proposed names, from 24% to 79% respectively. Through a pair of small user studies, we find that users significantly prefer the explanations produced by our technique (76% of responses indicating the appropriateness of the presenting names) to the baseline (with only 2% of responses approving of the suggestions), and that the proposed names measurably help users in understanding the synthesized implementation.</p></p class="citation"></blockquote><h2 id=cscy-1>cs.CY (1)</h2><h3 id=11--231271-human-vs-machine-language-models-and-wargames-max-lamparth-et-al-2024>(1/1 | 231/271) Human vs. Machine: Language Models and Wargames (Max Lamparth et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Max Lamparth, Anthony Corso, Jacob Ganz, Oriana Skylar Mastro, Jacquelyn Schneider, Harold Trinkunas. (2024)<br><strong>Human vs. Machine: Language Models and Wargames</strong><br><button class=copy-to-clipboard title="Human vs. Machine: Language Models and Wargames" index=231>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-231 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-AI, cs-CL, cs-CY, cs.CY<br>Keyword Score: 30<br>Keywords: Recommendation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03407v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03407v1.pdf filename=2403.03407v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Wargames have a long history in the development of military strategy and the response of nations to threats or attacks. The advent of artificial intelligence (AI) promises better decision-making and increased military effectiveness. However, there is still debate about how AI systems, especially <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> behave as compared to humans. To this end, we use a wargame experiment with 107 national security expert human players designed to look at crisis escalation in a fictional US-China scenario and compare human players to <b>LLM-simulated</b> responses. We find considerable agreement in the <b>LLM</b> and human responses but also significant quantitative and qualitative differences between simulated and human players in the wargame, motivating caution to policymakers before handing over autonomy or following AI-based strategy <b>recommendations.</b></p></p class="citation"></blockquote><h2 id=eessas-1>eess.AS (1)</h2><h3 id=11--232271-comparison-performance-of-spectrogram-and-scalogram-as-input-of-acoustic-recognition-task-dang-thoai-phan-et-al-2024>(1/1 | 232/271) Comparison Performance of Spectrogram and Scalogram as Input of Acoustic Recognition Task (Dang Thoai Phan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dang Thoai Phan, Andre Jakob, Marcus Purat. (2024)<br><strong>Comparison Performance of Spectrogram and Scalogram as Input of Acoustic Recognition Task</strong><br><button class=copy-to-clipboard title="Comparison Performance of Spectrogram and Scalogram as Input of Acoustic Recognition Task" index=232>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-232 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-SD, eess-AS, eess.AS<br>Keyword Score: 26<br>Keywords: Benchmarking, Benchmarking, Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03611v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03611v1.pdf filename=2403.03611v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Acoustic recognition is a common task for deep learning in recent researches, with the employment of spectral feature extraction such as Short-time Fourier transform and Wavelet transform. However, not many researches have found that discuss the advantages and drawbacks, as well as performance comparison amongst spectral feature extractors. In this consideration, this paper aims to comparing the attributes of these two transform types, called spectrogram and scalogram. A <b>Convolutional</b> <b>Neural</b> <b>Networks</b> for acoustic faults recognition is implemented, then the performance of these two types of spectral extractor is recorded for comparison. A latest research on the same audio database is considered for <b>benchmarking</b> to see how good the designed spectrogram and scalogram is. The advantages and limitations of them are also analyzed. By doing so, the results of this paper provide indications for application scenarios of spectrogram and scalogram, as well as potential further research directions in acoustic recognition.</p></p class="citation"></blockquote><h2 id=cssd-7>cs.SD (7)</h2><h3 id=17--233271-metamat-01-a-semi-analytic-solution-for-benchmarking-wave-propagation-simulations-of-homogeneous-absorbers-in-1d3d-and-2d-stefan-schoder-et-al-2024>(1/7 | 233/271) METAMAT 01: A semi-analytic Solution for Benchmarking Wave Propagation Simulations of homogeneous Absorbers in 1D/3D and 2D (Stefan Schoder et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Stefan Schoder, Paul Maurerlehner. (2024)<br><strong>METAMAT 01: A semi-analytic Solution for Benchmarking Wave Propagation Simulations of homogeneous Absorbers in 1D/3D and 2D</strong><br><button class=copy-to-clipboard title="METAMAT 01: A semi-analytic Solution for Benchmarking Wave Propagation Simulations of homogeneous Absorbers in 1D/3D and 2D" index=233>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-233 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-SD, cs.SD, eess-AS, physics-class-ph<br>Keyword Score: 26<br>Keywords: Benchmarking, Benchmarking, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03510v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03510v1.pdf filename=2403.03510v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The development of acoustic <b>simulation</b> workflows in the time-domain description is essential for predicting the sound of aeroacoustic or other transient acoustic effects. A common practice for noise mitigation is using absorbers. The modeling of these acoustic absorbers is typically provided in the frequency domain. Several, methods established bridging this gap, investigating methods to model absorber in the time domain. Therefore, this short article, describes the analytic solution in time-domain for <b>benchmarking</b> absorber <b>simulations</b> with infinite 1D, 2D, and 3D domains. Connected to the analytic solution, a Matlab script is provided to easily obtain the reference solution. The reference codes are provided as <b>benchmark</b> solution in the EAA TCCA <b>Benchmarking</b> database as METAMAT 01.</p></p class="citation"></blockquote><h3 id=27--234271-radia----radio-advertisement-detection-with-intelligent-analytics-jorge-álvarez-et-al-2024>(2/7 | 234/271) RADIA &ndash; Radio Advertisement Detection with Intelligent Analytics (Jorge Álvarez et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jorge Álvarez, Juan Carlos Armenteros, Camilo Torrón, Miguel Ortega-Martín, Alfonso Ardoiz, Óscar García, Ignacio Arranz, Íñigo Galdeano, Ignacio Garrido, Adrián Alonso, Fernando Bayón, Oleg Vorontsov. (2024)<br><strong>RADIA &ndash; Radio Advertisement Detection with Intelligent Analytics</strong><br><button class=copy-to-clipboard title="RADIA -- Radio Advertisement Detection with Intelligent Analytics" index=234>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-234 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-AI, cs-CL, cs-SD, cs.SD, eess-AS<br>Keyword Score: 20<br>Keywords: Automatic Speech Recognition, Text Classification<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03538v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03538v1.pdf filename=2403.03538v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Radio advertising remains an integral part of modern marketing strategies, with its appeal and potential for targeted reach undeniably effective. However, the dynamic nature of radio airtime and the rising trend of multiple radio spots necessitates an efficient system for monitoring advertisement broadcasts. This study investigates a novel automated radio advertisement detection technique incorporating advanced <b>speech</b> <b>recognition</b> and <b>text</b> <b>classification</b> algorithms. RadIA&rsquo;s approach surpasses traditional methods by eliminating the need for prior knowledge of the broadcast content. This contribution allows for detecting impromptu and newly introduced advertisements, providing a comprehensive solution for advertisement detection in radio broadcasting. Experimental results show that the resulting model, trained on carefully segmented and tagged <b>text</b> <b>data,</b> achieves an F1-macro score of 87.76 against a theoretical maximum of 89.33. This paper provides insights into the choice of hyperparameters and their impact on the model&rsquo;s performance. This study demonstrates its potential to ensure compliance with advertising broadcast contracts and offer competitive surveillance. This groundbreaking research could fundamentally change how radio advertising is monitored and open new doors for marketing optimization.</p></p class="citation"></blockquote><h3 id=37--235271-non-verbal-information-in-spontaneous-speech----towards-a-new-framework-of-analysis-tirza-biron-et-al-2024>(3/7 | 235/271) Non-verbal information in spontaneous speech &ndash; towards a new framework of analysis (Tirza Biron et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tirza Biron, Moshe Barboy, Eran Ben-Artzy, Alona Golubchik, Yanir Marmor, Smadar Szekely, Yaron Winter, David Harel. (2024)<br><strong>Non-verbal information in spontaneous speech &ndash; towards a new framework of analysis</strong><br><button class=copy-to-clipboard title="Non-verbal information in spontaneous speech -- towards a new framework of analysis" index=235>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-235 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-CL, cs-LG, cs-SD, cs.SD, eess-AS<br>Keyword Score: 20<br>Keywords: Fine-tuning, Automatic Speech Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03522v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03522v1.pdf filename=2403.03522v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Non-verbal signals in <b>speech</b> <b>are</b> encoded by prosody and carry information that ranges from conversation action to attitude and emotion. Despite its importance, the principles that govern prosodic structure are not yet adequately understood. This paper offers an analytical schema and a technological proof-of-concept for the categorization of prosodic signals and their association with meaning. The schema interprets surface-representations of multi-layered prosodic events. As a first step towards implementation, we present a classification process that disentangles prosodic phenomena of three orders. It relies on <b>fine-tuning</b> a pre-trained <b>speech</b> <b>recognition</b> model, enabling the simultaneous multi-class/multi-label detection. It generalizes over a large variety of spontaneous data, performing on a par with, or superior to, human annotation. In addition to a standardized formalization of prosody, disentangling prosodic patterns can direct a theory of communication and <b>speech</b> <b>organization.</b> A welcome by-product is an interpretation of prosody that will enhance speech- and language-related technologies.</p></p class="citation"></blockquote><h3 id=47--236271-interactive-melody-generation-system-for-enhancing-the-creativity-of-musicians-so-hirawata-et-al-2024>(4/7 | 236/271) Interactive Melody Generation System for Enhancing the Creativity of Musicians (So Hirawata et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>So Hirawata, Noriko Otani. (2024)<br><strong>Interactive Melody Generation System for Enhancing the Creativity of Musicians</strong><br><button class=copy-to-clipboard title="Interactive Melody Generation System for Enhancing the Creativity of Musicians" index=236>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-236 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-AI, cs-HC, cs-SD, cs.SD, eess-AS<br>Keyword Score: 20<br>Keywords: Recurrent Neural Network, Recurrent Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03395v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03395v1.pdf filename=2403.03395v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study proposes a system designed to enumerate the process of collaborative composition among humans, using automatic music composition technology. By integrating multiple <b>Recurrent</b> <b>Neural</b> <b>Network</b> <b>(RNN)</b> models, the system provides an experience akin to collaborating with several composers, thereby fostering diverse creativity. Through dynamic adaptation to the user&rsquo;s creative intentions, based on feedback, the system enhances its capability to generate melodies that align with user preferences and creative needs. The system&rsquo;s effectiveness was evaluated through experiments with composers of varying backgrounds, revealing its potential to facilitate musical creativity and suggesting avenues for further refinement. The study underscores the importance of interaction between the composer and AI, aiming to make music composition more accessible and personalized. This system represents a step towards integrating AI into the creative process, offering a new tool for composition support and collaborative artistic exploration.</p></p class="citation"></blockquote><h3 id=57--237271-can-audio-reveal-music-performance-difficulty-insights-from-the-piano-syllabus-dataset-pedro-ramoneda-et-al-2024>(5/7 | 237/271) Can Audio Reveal Music Performance Difficulty? Insights from the Piano Syllabus Dataset (Pedro Ramoneda et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pedro Ramoneda, Minhee Lee, Dasaem Jeong, J. J. Valero-Mas, Xavier Serra. (2024)<br><strong>Can Audio Reveal Music Performance Difficulty? Insights from the Piano Syllabus Dataset</strong><br><button class=copy-to-clipboard title="Can Audio Reveal Music Performance Difficulty? Insights from the Piano Syllabus Dataset" index=237>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-237 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-SD, cs.SD, eess-AS<br>Keyword Score: 16<br>Keywords: Multi-modal, Multi-modal, Information Retrieval<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03947v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03947v1.pdf filename=2403.03947v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Automatically estimating the performance difficulty of a music piece represents a key process in music education to create tailored curricula according to the individual needs of the students. Given its relevance, the Music <b>Information</b> <b>Retrieval</b> (MIR) field depicts some proof-of-concept works addressing this task that mainly focuses on high-level music abstractions such as machine-readable scores or music sheet images. In this regard, the potential of directly analyzing audio recordings has been generally neglected, which prevents students from exploring diverse music pieces that may not have a formal symbolic-level transcription. This work pioneers in the automatic estimation of performance difficulty of music pieces on audio recordings with two precise contributions: (i) the first audio-based difficulty estimation dataset &ndash; namely, Piano Syllabus (PSyllabus) dataset &ndash; featuring 7,901 piano pieces across 11 difficulty levels from 1,233 composers; and (ii) a recognition framework capable of managing different input representations &ndash; both unimodal and <b>multimodal</b> manners &ndash; directly derived from audio to perform the difficulty estimation task. The comprehensive experimentation comprising different pre-training schemes, input modalities, and multi-task scenarios prove the validity of the proposal and establishes PSyllabus as a reference dataset for audio-based difficulty estimation in the MIR field. The dataset as well as the developed code and trained models are publicly shared to promote further research in the field.</p></p class="citation"></blockquote><h3 id=67--238271-multi-level-attention-aggregation-for-language-agnostic-speaker-replication-yejin-jeon-et-al-2024>(6/7 | 238/271) Multi-Level Attention Aggregation for Language-Agnostic Speaker Replication (Yejin Jeon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yejin Jeon, Gary Geunbae Lee. (2024)<br><strong>Multi-Level Attention Aggregation for Language-Agnostic Speaker Replication</strong><br><button class=copy-to-clipboard title="Multi-Level Attention Aggregation for Language-Agnostic Speaker Replication" index=238>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-238 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-SD, cs.SD, eess-AS<br>Keyword Score: 10<br>Keywords: Out-of-domain<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04111v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04111v1.pdf filename=2403.04111v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper explores the task of language-agnostic speaker replication, a novel endeavor that seeks to replicate a speaker&rsquo;s voice irrespective of the language they are speaking. Towards this end, we introduce a multi-level attention aggregation approach that systematically probes and amplifies various speaker-specific attributes in a hierarchical manner. Through rigorous evaluations across a wide range of scenarios including seen and unseen speakers conversing in seen and unseen lingua, we establish that our proposed model is able to achieve substantial speaker similarity, and is able to generalize to <b>out-of-domain</b> (OOD) cases.</p></p class="citation"></blockquote><h3 id=77--239271-crossnet-leveraging-global-cross-band-narrow-band-and-positional-encoding-for-single--and-multi-channel-speaker-separation-vahid-ahmadi-kalkhorani-et-al-2024>(7/7 | 239/271) CrossNet: Leveraging Global, Cross-Band, Narrow-Band, and Positional Encoding for Single- and Multi-Channel Speaker Separation (Vahid Ahmadi Kalkhorani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vahid Ahmadi Kalkhorani, DeLiang Wang. (2024)<br><strong>CrossNet: Leveraging Global, Cross-Band, Narrow-Band, and Positional Encoding for Single- and Multi-Channel Speaker Separation</strong><br><button class=copy-to-clipboard title="CrossNet: Leveraging Global, Cross-Band, Narrow-Band, and Positional Encoding for Single- and Multi-Channel Speaker Separation" index=239>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-239 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-SD, cs.SD, eess-AS<br>Keyword Score: 10<br>Keywords: Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03411v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03411v1.pdf filename=2403.03411v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce CrossNet, a complex spectral mapping approach to speaker separation and enhancement in reverberant and noisy conditions. The proposed architecture comprises an encoder layer, a global multi-head <b>self-attention</b> module, a cross-band module, a narrow-band module, and an output layer. CrossNet captures global, cross-band, and narrow-band correlations in the time-frequency domain. To address performance degradation in long utterances, we introduce a random chunk positional encoding. Experimental results on multiple datasets demonstrate the effectiveness and robustness of CrossNet, achieving state-of-the-art performance in tasks including reverberant and noisy-reverberant speaker separation. Furthermore, CrossNet exhibits faster and more stable training in comparison to recent baselines. Additionally, CrossNet&rsquo;s high performance extends to multi-microphone conditions, demonstrating its versatility in various acoustic scenarios.</p></p class="citation"></blockquote><h2 id=mathna-7>math.NA (7)</h2><h3 id=17--240271-robust-radial-basis-function-interpolation-based-on-geodesic-distance-for-the-numerical-coupling-of-multiphysics-problems-michele-bucelli-et-al-2024>(1/7 | 240/271) Robust radial basis function interpolation based on geodesic distance for the numerical coupling of multiphysics problems (Michele Bucelli et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michele Bucelli, Francesco Regazzoni, Luca Dede&rsquo;, Alfio Quarteroni. (2024)<br><strong>Robust radial basis function interpolation based on geodesic distance for the numerical coupling of multiphysics problems</strong><br><button class=copy-to-clipboard title="Robust radial basis function interpolation based on geodesic distance for the numerical coupling of multiphysics problems" index=240>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-240 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: 65D05, 65D12, 65M60, cs-NA, math-NA, math.NA<br>Keyword Score: 25<br>Keywords: Geometry, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03665v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03665v1.pdf filename=2403.03665v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multiphysics <b>simulations</b> frequently require transferring solution fields between subproblems with non-matching spatial discretizations, typically using interpolation techniques. Standard methods are usually based on measuring the closeness between points by means of the Euclidean distance, which does not account for curvature, cuts, cavities or other non-trivial geometrical or topological features of the domain. This may lead to spurious oscillations in the interpolant in proximity to these features. To overcome this issue, we propose a modification to rescaled localized radial basis function (RL-RBF) interpolation to account for the <b>geometry</b> of the interpolation domain, by yielding conformity and fidelity to geometrical and topological features. The proposed method, referred to as RL-RBF-G, relies on measuring the geodesic distance between data points. RL-RBF-G removes spurious oscillations appearing in the RL-RBF interpolant, resulting in increased accuracy in domains with complex geometries. We demonstrate the effectiveness of RL-RBF-G interpolation through a convergence study in an idealized setting. Furthermore, we discuss the algorithmic aspects and the implementation of RL-RBF-G interpolation in a distributed-memory parallel framework, and present the results of a strong scalability test yielding nearly ideal results. Finally, we show the effectiveness of RL-RBF-G interpolation in multiphysics <b>simulations</b> by considering an application to a whole-heart cardiac electromecanics model.</p></p class="citation"></blockquote><h3 id=27--241271-a-component-splitting-implicit-time-integration-for-multicomponent-reacting-flows-simulations-jingchao-zhang-et-al-2024>(2/7 | 241/271) A component-splitting implicit time integration for multicomponent reacting flows simulations (Jingchao Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jingchao Zhang, Jinsheng Cai, Shucheng Pan. (2024)<br><strong>A component-splitting implicit time integration for multicomponent reacting flows simulations</strong><br><button class=copy-to-clipboard title="A component-splitting implicit time integration for multicomponent reacting flows simulations" index=241>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-241 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA, physics-comp-ph<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03440v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03440v1.pdf filename=2403.03440v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A component-splitting method is proposed to improve convergence characteristics for implicit time integration of compressible multicomponent reactive flows. The characteristic decomposition of flux jacobian of multicomponent Navier-Stokes equations yields a large sparse eigensystem, presenting challenges of slow convergence and high computational costs for implicit methods. To addresses this issue, the component-splitting method segregates the implicit operator into two parts: one for the flow equations (density/momentum/energy) and the other for the component equations. Each part&rsquo;s implicit operator employs flux-vector splitting based on their respective spectral radii to achieve accelerated convergence. This approach improves the computational efficiency of implicit iteration, mitigating the quadratic increase in time cost with the number of species. Two consistence corrections are developed to reduce the introduced component-splitting error and ensure the numerical consistency of mass fraction. Importantly, the impact of component-splitting method on accuracy is minimal as the residual approaches convergence. The accuracy, efficiency, and robustness of component-splitting method are thoroughly investigated and compared with the coupled implicit scheme through several numerical cases involving thermo-chemical nonequilibrium hypersonic flows. The results demonstrate that the component-splitting method decreases the required number of iteration steps for convergence of residual and wall heat flux, decreases the computation time per iteration step, and diminishes the residual to lower magnitude. The acceleration efficiency is enhanced with increases in CFL number and number of species.</p></p class="citation"></blockquote><h3 id=37--242271-electromagnetic-inverse-wave-scattering-in-anisotropic-media-via-reduced-order-modeling-liliana-borcea-et-al-2024>(3/7 | 242/271) Electromagnetic inverse wave scattering in anisotropic media via reduced order modeling (Liliana Borcea et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Liliana Borcea, Yiyang Liu, Jörn Zimmerling. (2024)<br><strong>Electromagnetic inverse wave scattering in anisotropic media via reduced order modeling</strong><br><button class=copy-to-clipboard title="Electromagnetic inverse wave scattering in anisotropic media via reduced order modeling" index=242>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-242 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: 65M32, 41A20, cs-NA, math-AP, math-NA, math.NA, physics-comp-ph<br>Keyword Score: 10<br>Keywords: Discrete Time, Discrete Time<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03844v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03844v1.pdf filename=2403.03844v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The inverse wave scattering problem seeks to estimate a heterogeneous, inaccessible medium, modeled by unknown variable coefficients in wave equations, from transient recordings of waves generated by probing signals. It is a widely studied inverse problem with important applications, that is typically formulated as a nonlinear least squares data fit optimization. For typical measurement setups and band-limited probing signals, the least squares objective function has spurious local minima far and near the true solution, so Newton-type optimization methods fail. We introduce a different approach, for electromagnetic inverse wave scattering in lossless, anisotropic media. Our reduced order model (ROM) is an algebraic, <b>discrete</b> <b>time</b> dynamical system derived from Maxwell&rsquo;s equations with four important properties: (1) It is data driven, without knowledge of the medium. (2) The data to ROM mapping is nonlinear and yet the ROM can be obtained in a non-iterative fashion. (3) It has a special algebraic structure that captures the causal Wave propagation. (4) The ROM interpolates the data on a uniform time grid. We show how to obtain from the ROM an estimate of the wave field at inaccessible points inside the unknown medium. The use of this wave is twofold: First, it defines a computationally inexpensive imaging function designed to estimate the support of reflective structures in the medium, modeled by jump discontinuities of the matrix valued dielectric permittivity. Second, it gives an objective function for quantitative estimation of the dielectric permittivity, that has better behavior than the least squares data fitting objective function. The methodology introduced in this paper applies to Maxwell&rsquo;s equations in three dimensions. To avoid high computational costs, we limit the study to a cylindrical domain filled with an orthotropic medium, so the problem becomes two dimensional.</p></p class="citation"></blockquote><h3 id=47--243271-tgpt-pinn-nonlinear-model-reduction-with-transformed-gpt-pinns-yanlai-chen-et-al-2024>(4/7 | 243/271) TGPT-PINN: Nonlinear model reduction with transformed GPT-PINNs (Yanlai Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yanlai Chen, Yajie Ji, Akil Narayan, Zhenli Xu. (2024)<br><strong>TGPT-PINN: Nonlinear model reduction with transformed GPT-PINNs</strong><br><button class=copy-to-clipboard title="TGPT-PINN: Nonlinear model reduction with transformed GPT-PINNs" index=243>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-243 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-LG, cs-NA, math-NA, math.NA<br>Keyword Score: 10<br>Keywords: GPT<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03459v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03459v1.pdf filename=2403.03459v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce the Transformed Generative Pre-Trained Physics-Informed Neural Networks (TGPT-PINN) for accomplishing nonlinear model order reduction (MOR) of transport-dominated partial differential equations in an MOR-integrating PINNs framework. Building on the recent development of the <b>GPT-PINN</b> that is a network-of-networks design achieving snapshot-based model reduction, we design and test a novel paradigm for nonlinear model reduction that can effectively tackle problems with parameter-dependent discontinuities. Through incorporation of a shock-capturing loss function component as well as a parameter-dependent transform layer, the TGPT-PINN overcomes the limitations of linear model reduction in the transport-dominated regime. We demonstrate this new capability for nonlinear model reduction in the PINNs framework by several nontrivial parametric partial differential equations.</p></p class="citation"></blockquote><h3 id=57--244271-helmholtz-preconditioning-for-the-compressible-euler-equations-using-mixed-finite-elements-with-lorenz-staggering-david-lee-et-al-2024>(5/7 | 244/271) Helmholtz preconditioning for the compressible Euler equations using mixed finite elements with Lorenz staggering (David Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>David Lee, Alberto Martin, Kieran Ricardo. (2024)<br><strong>Helmholtz preconditioning for the compressible Euler equations using mixed finite elements with Lorenz staggering</strong><br><button class=copy-to-clipboard title="Helmholtz preconditioning for the compressible Euler equations using mixed finite elements with Lorenz staggering" index=244>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-244 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04095v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04095v1.pdf filename=2403.04095v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Implicit solvers for atmospheric models are often accelerated via the solution of a preconditioned system. For block preconditioners this typically involves the factorisation of the (approximate) Jacobian for the coupled system into a Helmholtz equation for some function of the pressure. Here we present a preconditioner for the compressible Euler equations with a flux form representation of the potential temperature on the Lorenz grid using mixed finite elements. This formulation allows for spatial discretisations that conserve both energy and potential temperature variance. By introducing the dry thermodynamic entropy as an auxiliary variable for the solution of the algebraic system, the resulting preconditioner is shown to have a similar block structure to an existing preconditioner for the material form transport of potential temperature on the Charney-Phillips grid, and to be more efficient and stable than either this or a previous Helmholtz preconditioner for the flux form transport of density weighted potential temperature on the Lorenz grid for a one dimensional thermal bubble configuration. The new preconditioner is further verified against standard two dimensional test cases in a vertical slice <b>geometry.</b></p></p class="citation"></blockquote><h3 id=67--245271-black-box-k-to-1-pca-reductions-theory-and-applications-arun-jambulapati-et-al-2024>(6/7 | 245/271) Black-Box $k$-to-$1$-PCA Reductions: Theory and Applications (Arun Jambulapati et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Arun Jambulapati, Syamantak Kumar, Jerry Li, Shourya Pandey, Ankit Pensia, Kevin Tian. (2024)<br><strong>Black-Box $k$-to-$1$-PCA Reductions: Theory and Applications</strong><br><button class=copy-to-clipboard title="Black-Box $k$-to-$1$-PCA Reductions: Theory and Applications" index=245>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-245 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-DS, cs-LG, cs-NA, math-NA, math.NA, stat-ML<br>Keyword Score: 5<br>Keywords: Black Box<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03905v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03905v2.pdf filename=2403.03905v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The $k$-principal component analysis ($k$-PCA) problem is a fundamental algorithmic primitive that is widely-used in data analysis and dimensionality reduction applications. In statistical settings, the goal of $k$-PCA is to identify a top eigenspace of the covariance matrix of a distribution, which we only have implicit access to via samples. Motivated by these implicit settings, we analyze <b>black-box</b> <b>deflation</b> methods as a framework for designing $k$-PCA algorithms, where we model access to the unknown target matrix via a <b>black-box</b> <b>$1$-PCA</b> oracle which returns an approximate top eigenvector, under two popular notions of approximation. Despite being arguably the most natural reduction-based approach to $k$-PCA algorithm design, such <b>black-box</b> <b>methods,</b> which recursively call a $1$-PCA oracle $k$ times, were previously poorly-understood. Our main contribution is significantly sharper bounds on the approximation parameter degradation of deflation methods for $k$-PCA. For a quadratic form notion of approximation we term ePCA (energy PCA), we show deflation methods suffer no parameter loss. For an alternative well-studied approximation notion we term cPCA (correlation PCA), we tightly characterize the parameter regimes where deflation methods are feasible. Moreover, we show that in all feasible regimes, $k$-cPCA deflation algorithms suffer no asymptotic parameter loss for any constant $k$. We apply our framework to obtain state-of-the-art $k$-PCA algorithms robust to dataset contamination, improving prior work both in sample complexity and approximation quality.</p></p class="citation"></blockquote><h3 id=77--246271-application-of-deep-learning-reduced-order-modeling-for-single-phase-flow-in-faulted-porous-media-enrico-ballini-et-al-2024>(7/7 | 246/271) Application of Deep Learning Reduced-Order Modeling for Single-Phase Flow in Faulted Porous Media (Enrico Ballini et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Enrico Ballini, Luca Formaggia, Alessio Fumagalli, Anna Scotti, Paolo Zunino. (2024)<br><strong>Application of Deep Learning Reduced-Order Modeling for Single-Phase Flow in Faulted Porous Media</strong><br><button class=copy-to-clipboard title="Application of Deep Learning Reduced-Order Modeling for Single-Phase Flow in Faulted Porous Media" index=246>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-246 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03678v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03678v1.pdf filename=2403.03678v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We apply reduced-order modeling (ROM) techniques to single-phase flow in faulted porous media, accounting for changing rock properties and fault <b>geometry</b> variations using a radial basis function mesh deformation method. This approach benefits from a mixed-dimensional framework that effectively manages the resulting non-conforming mesh. To streamline complex and repetitive calculations such as sensitivity analysis and solution of inverse problems, we utilize the Deep Learning Reduced Order Model (DL-ROM). This non-intrusive neural network-based technique is evaluated against the traditional Proper Orthogonal Decomposition (POD) method across various scenarios, demonstrating DL-ROM&rsquo;s capacity to expedite complex analyses with promising accuracy and efficiency.</p></p class="citation"></blockquote><h2 id=physicschem-ph-1>physics.chem-ph (1)</h2><h3 id=11--247271-predicting-the-temperature-dependence-of-surfactant-cmcs-using-graph-neural-networks-christoforos-brozos-et-al-2024>(1/1 | 247/271) Predicting the Temperature Dependence of Surfactant CMCs Using Graph Neural Networks (Christoforos Brozos et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Christoforos Brozos, Jan G. Rittig, Sandip Bhattacharya, Elie Akanny, Christina Kohlmann, Alexander Mitsos. (2024)<br><strong>Predicting the Temperature Dependence of Surfactant CMCs Using Graph Neural Networks</strong><br><button class=copy-to-clipboard title="Predicting the Temperature Dependence of Surfactant CMCs Using Graph Neural Networks" index=247>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-247 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.chem-ph<br>Categories: cs-LG, physics-chem-ph, physics.chem-ph<br>Keyword Score: 23<br>Keywords: Graph, Graph Neural Network, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03767v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03767v1.pdf filename=2403.03767v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The critical micelle concentration (CMC) of surfactant molecules is an essential property for surfactant applications in industry. Recently, classical QSPR and <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs),</b> a deep learning technique, have been successfully applied to predict the CMC of surfactants at room temperature. However, these models have not yet considered the temperature dependency of the CMC, which is highly relevant for practical applications. We herein develop a <b>GNN</b> model for temperature-dependent CMC prediction of surfactants. We collect about 1400 data points from public sources for all surfactant classes, i.e., ionic, nonionic, and zwitterionic, at multiple temperatures. We test the predictive quality of the model for following scenarios: i) when CMC data for surfactants are present in the training of the model in at least one different temperature, and ii) CMC data for surfactants are not present in the training, i.e., generalizing to unseen surfactants. In both test scenarios, our model exhibits a high predictive performance of R$^2 \geq $ 0.94 on test data. We also find that the model performance varies by surfactant class. Finally, we evaluate the model for sugar-based surfactants with complex molecular structures, as these represent a more sustainable alternative to synthetic surfactants and are therefore of great interest for future applications in the personal and home care industries.</p></p class="citation"></blockquote><h2 id=statml-3>stat.ML (3)</h2><h3 id=13--248271-conformal-prediction-for-multi-dimensional-time-series-by-ellipsoidal-sets-chen-xu-et-al-2024>(1/3 | 248/271) Conformal prediction for multi-dimensional time series by ellipsoidal sets (Chen Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chen Xu, Hanyang Jiang, Yao Xie. (2024)<br><strong>Conformal prediction for multi-dimensional time series by ellipsoidal sets</strong><br><button class=copy-to-clipboard title="Conformal prediction for multi-dimensional time series by ellipsoidal sets" index=248>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-248 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 20<br>Keywords: Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03850v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03850v1.pdf filename=2403.03850v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Conformal prediction (CP) has been a popular method for uncertainty quantification because it is distribution-free, model-agnostic, and theoretically sound. For forecasting problems in <b>supervised</b> <b>learning,</b> most CP methods focus on building prediction intervals for univariate responses. In this work, we develop a sequential CP method called $\texttt{MultiDimSPCI}$ that builds prediction regions for a multivariate response, especially in the context of multivariate time series, which are not exchangeable. Theoretically, we estimate finite-sample high-probability bounds on the conditional coverage gap. Empirically, we demonstrate that $\texttt{MultiDimSPCI}$ maintains valid coverage on a wide range of multivariate time series while producing smaller prediction regions than CP and non-CP baselines.</p></p class="citation"></blockquote><h3 id=23--249271-targeted-variance-reduction-robust-bayesian-optimization-of-black-box-simulators-with-noise-parameters-john-joshua-miller-et-al-2024>(2/3 | 249/271) Targeted Variance Reduction: Robust Bayesian Optimization of Black-Box Simulators with Noise Parameters (John Joshua Miller et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>John Joshua Miller, Simon Mak. (2024)<br><strong>Targeted Variance Reduction: Robust Bayesian Optimization of Black-Box Simulators with Noise Parameters</strong><br><button class=copy-to-clipboard title="Targeted Variance Reduction: Robust Bayesian Optimization of Black-Box Simulators with Noise Parameters" index=249>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-249 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 15<br>Keywords: Black Box, Gaussian Process<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03816v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03816v1.pdf filename=2403.03816v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The optimization of a <b>black-box</b> <b>simulator</b> over control parameters $\mathbf{x}$ arises in a myriad of scientific applications. In such applications, the simulator often takes the form $f(\mathbf{x},\boldsymbol{\theta})$, where $\boldsymbol{\theta}$ are parameters that are uncertain in practice. Robust optimization aims to optimize the objective $\mathbb{E}[f(\mathbf{x},\boldsymbol{\Theta})]$, where $\boldsymbol{\Theta} \sim \mathcal{P}$ is a random variable that models uncertainty on $\boldsymbol{\theta}$. For this, existing <b>black-box</b> <b>methods</b> typically employ a two-stage approach for selecting the next point $(\mathbf{x},\boldsymbol{\theta})$, where $\mathbf{x}$ and $\boldsymbol{\theta}$ are optimized separately via different acquisition functions. As such, these approaches do not employ a joint acquisition over $(\mathbf{x},\boldsymbol{\theta})$, and thus may fail to fully exploit control-to-noise interactions for effective robust optimization. To address this, we propose a new Bayesian optimization method called Targeted Variance Reduction (TVR). The TVR leverages a novel joint acquisition function over $(\mathbf{x},\boldsymbol{\theta})$, which targets variance reduction on the objective within the desired region of improvement. Under a <b>Gaussian</b> <b>process</b> surrogate on $f$, the TVR acquisition can be evaluated in closed form, and reveals an insightful exploration-exploitation-precision trade-off for robust <b>black-box</b> <b>optimization.</b> The TVR can further accommodate a broad class of non-Gaussian distributions on $\mathcal{P}$ via a careful integration of normalizing flows. We demonstrate the improved performance of TVR over the state-of-the-art in a suite of numerical experiments and an application to the robust design of automobile brake discs under operational uncertainty.</p></p class="citation"></blockquote><h3 id=33--250271-incentivized-learning-in-principal-agent-bandit-games-antoine-scheid-et-al-2024>(3/3 | 250/271) Incentivized Learning in Principal-Agent Bandit Games (Antoine Scheid et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Antoine Scheid, Daniil Tiapkin, Etienne Boursier, Aymeric Capitaine, El Mahdi El Mhamdi, Eric Moulines, Michael I. Jordan, Alain Durmus. (2024)<br><strong>Incentivized Learning in Principal-Agent Bandit Games</strong><br><button class=copy-to-clipboard title="Incentivized Learning in Principal-Agent Bandit Games" index=250>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-250 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-GT, cs-LG, stat-ML, stat.ML<br>Keyword Score: 10<br>Keywords: Bandit Algorithm<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03811v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03811v1.pdf filename=2403.03811v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work considers a repeated principal-agent <b>bandit</b> game, where the principal can only interact with her environment through the agent. The principal and the agent have misaligned objectives and the choice of action is only left to the agent. However, the principal can influence the agent&rsquo;s decisions by offering incentives which add up to his rewards. The principal aims to iteratively learn an incentive policy to maximize her own total utility. This framework extends usual <b>bandit</b> problems and is motivated by several practical applications, such as healthcare or ecological taxation, where traditionally used mechanism design theories often overlook the learning aspect of the problem. We present nearly optimal (with respect to a horizon $T$) learning algorithms for the principal&rsquo;s regret in both multi-armed and linear contextual settings. Finally, we support our theoretical guarantees through numerical experiments.</p></p class="citation"></blockquote><h2 id=quant-ph-1>quant-ph (1)</h2><h3 id=11--251271-parameterized-quantum-comb-and-simpler-circuits-for-reversing-unknown-qubit-unitary-operations-yin-mo-et-al-2024>(1/1 | 251/271) Parameterized quantum comb and simpler circuits for reversing unknown qubit-unitary operations (Yin Mo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yin Mo, Lei Zhang, Yu-Ao Chen, Yingjian Liu, Tengxiang Lin, Xin Wang. (2024)<br><strong>Parameterized quantum comb and simpler circuits for reversing unknown qubit-unitary operations</strong><br><button class=copy-to-clipboard title="Parameterized quantum comb and simpler circuits for reversing unknown qubit-unitary operations" index=251>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-251 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-IT, cs-LG, math-IT, quant-ph, quant-ph<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03761v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03761v1.pdf filename=2403.03761v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Quantum comb is an essential tool for characterizing complex quantum protocols in quantum information processing. In this work, we introduce PQComb, a framework leveraging parameterized quantum circuits to explore the capabilities of quantum combs for general quantum process transformation tasks and beyond. By optimizing PQComb for time-reversal <b>simulations</b> of unknown unitary evolutions, we develop a simpler protocol for unknown qubit unitary inversion that reduces the ancilla qubit overhead from 6 to 3 compared to the existing method in [Yoshida, Soeda, Murao, PRL 131, 120602, 2023]. This demonstrates the utility of quantum comb structures and showcases PQComb&rsquo;s potential for solving complex quantum tasks. Our results pave the way for broader PQComb applications in quantum computing and quantum information, emphasizing its versatility for tackling diverse problems in quantum machine learning.</p></p class="citation"></blockquote><h2 id=cssi-2>cs.SI (2)</h2><h3 id=12--252271-causal-disentanglement-for-regulating-social-influence-bias-in-social-recommendation-li-wang-et-al-2024>(1/2 | 252/271) Causal Disentanglement for Regulating Social Influence Bias in Social Recommendation (Li Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Li Wang, Min Xu, Quangui Zhang, Yunxiao Shi, Qiang Wu. (2024)<br><strong>Causal Disentanglement for Regulating Social Influence Bias in Social Recommendation</strong><br><button class=copy-to-clipboard title="Causal Disentanglement for Regulating Social Influence Bias in Social Recommendation" index=252>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-252 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-AI, cs-SI, cs.SI<br>Keyword Score: 20<br>Keywords: Mutual Information, Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03578v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03578v1.pdf filename=2403.03578v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Social <b>recommendation</b> systems face the problem of social influence bias, which can lead to an overemphasis on recommending items that friends have interacted with. Addressing this problem is crucial, and existing methods often rely on techniques such as weight adjustment or leveraging unbiased data to eliminate this bias. However, we argue that not all biases are detrimental, i.e., some items recommended by friends may align with the user&rsquo;s interests. Blindly eliminating such biases could undermine these positive effects, potentially diminishing <b>recommendation</b> accuracy. In this paper, we propose a Causal Disentanglement-based framework for Regulating Social influence Bias in social <b>recommendation,</b> named CDRSB, to improve <b>recommendation</b> performance. From the perspective of causal inference, we find that the user social network could be regarded as a confounder between the user and item embeddings (treatment) and ratings (outcome). Due to the presence of this social network confounder, two paths exist from user and item embeddings to ratings: a non-causal social influence path and a causal interest path. Building upon this insight, we propose a disentangled encoder that focuses on disentangling user and item embeddings into interest and social influence embeddings. <b>Mutual</b> <b>information-based</b> objectives are designed to enhance the distinctiveness of these disentangled embeddings, eliminating redundant information. Additionally, a regulatory decoder that employs a weight calculation module to dynamically learn the weights of social influence embeddings for effectively regulating social influence bias has been designed. Experimental results on four large-scale real-world datasets Ciao, Epinions, Dianping, and Douban book demonstrate the effectiveness of CDRSB compared to state-of-the-art baselines.</p></p class="citation"></blockquote><h3 id=22--253271-quantifying-media-influence-on-covid-19-mask-wearing-beliefs-nicholas-rabb-et-al-2024>(2/2 | 253/271) Quantifying Media Influence on Covid-19 Mask-Wearing Beliefs (Nicholas Rabb et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nicholas Rabb, Nitya Nadgir, Jan P. de Ruiter, Lenore Cowen. (2024)<br><strong>Quantifying Media Influence on Covid-19 Mask-Wearing Beliefs</strong><br><button class=copy-to-clipboard title="Quantifying Media Influence on Covid-19 Mask-Wearing Beliefs" index=253>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-253 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-CY, cs-SI, cs.SI, physics-soc-ph<br>Keyword Score: 10<br>Keywords: Keyword Extraction<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03684v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03684v1.pdf filename=2403.03684v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>How political beliefs change in accordance with media exposure is a complicated matter. Some studies have been able to demonstrate that groups with different media diets in the aggregate (e.g., U.S. media consumers ingesting partisan news) arrive at different beliefs about policy issues, but proving this from data at a granular level &ndash; at the level of attitudes expressed in news stories &ndash; remains difficult. In contrast to existing opinion formation models that describe granular detail but are not data-driven, or data-driven studies that rely on simple <b>keyword</b> <b>detection</b> and miss linguistic nuances, being able to identify complicated attitudes in news text and use this data to drive models would enable more nuanced empirical study of opinion formation from media messaging. This study contributes a dataset as well as an analysis that allows the mapping of attitudes from individual news stories to aggregate changes of opinion over time for an important public health topic where opinion differed in the U.S. by partisan media diet: Covid mask-wearing beliefs. By gathering a dataset of U.S. news media stories, from April 6 to June 8, 2020, annotated according to Howard 2020&rsquo;s Face Mask Perception Scale for their statements regarding Covid-19 mask-wearing, we demonstrate fine-grained correlations between media messaging and empirical opinion polling data from a Gallup survey conducted during the same period. We also demonstrate that the data can be used for quantitative analysis of pro- and anti-mask sentiment throughout the period, identifying major events that drove opinion changes. This dataset is made publicly available and can be used by other researchers seeking to evaluate how mask-wearing attitudes were driven by news media content. Additionally, we hope that its general method can be used to enable other media researchers to conduct more detailed analyses of media effects on opinion.</p></p class="citation"></blockquote><h2 id=csar-1>cs.AR (1)</h2><h3 id=11--254271-camasim-a-comprehensive-simulation-framework-for-content-addressable-memory-based-accelerators-mengyuan-li-et-al-2024>(1/1 | 254/271) CAMASim: A Comprehensive Simulation Framework for Content-Addressable Memory based Accelerators (Mengyuan Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mengyuan Li, Shiyi Liu, Mohammad Mehdi Sharifi, X. Sharon Hu. (2024)<br><strong>CAMASim: A Comprehensive Simulation Framework for Content-Addressable Memory based Accelerators</strong><br><button class=copy-to-clipboard title="CAMASim: A Comprehensive Simulation Framework for Content-Addressable Memory based Accelerators" index=254>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-254 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AR<br>Categories: cs-AR, cs.AR<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03442v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03442v2.pdf filename=2403.03442v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Content addressable memory (CAM) stands out as an efficient hardware solution for memory-intensive search operations by supporting parallel computation in memory. However, developing a CAM-based accelerator architecture that achieves acceptable accuracy, while minimizing hardware cost and catering to both exact and approximate search, still presents a significant challenge especially when considering a broader spectrum of applications. This complexity stems from CAM&rsquo;s rapid evolution across multiple levels&ndash;algorithms, architectures, circuits, and underlying devices. This paper introduces CAMASim, a first comprehensive CAM accelerator <b>simulation</b> framework, emphasizing modularity, flexibility, and generality. CAMASim establishes the detailed design space for CAM-based accelerators, incorporates automated functional <b>simulation</b> for accuracy, and enables hardware performance prediction, by leveraging a circuit-level CAM modeling tool. This work streamlines the design space exploration for CAM-based accelerator, aiding researchers in developing effective CAM-based accelerators for various search-intensive applications.</p></p class="citation"></blockquote><h2 id=astro-phep-1>astro-ph.EP (1)</h2><h3 id=11--255271-single-transit-detection-in-kepler-with-machine-learning-and-onboard-spacecraft-diagnostics-matthew-t-hansen-et-al-2024>(1/1 | 255/271) Single Transit Detection In Kepler With Machine Learning And Onboard Spacecraft Diagnostics (Matthew T. Hansen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Matthew T. Hansen, Jason A. Dittmann. (2024)<br><strong>Single Transit Detection In Kepler With Machine Learning And Onboard Spacecraft Diagnostics</strong><br><button class=copy-to-clipboard title="Single Transit Detection In Kepler With Machine Learning And Onboard Spacecraft Diagnostics" index=255>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-255 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: astro-ph.EP<br>Categories: astro-ph-EP, astro-ph-IM, astro-ph.EP, cs-LG<br>Keyword Score: 20<br>Keywords: Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03427v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03427v1.pdf filename=2403.03427v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Exoplanet discovery at long orbital periods requires reliably detecting individual transits without additional information about the system. Techniques like phase-folding of light curves and periodogram analysis of radial velocity data are more sensitive to planets with shorter orbital periods, leaving a dearth of planet discoveries at long periods. We present a novel technique using an ensemble of <b>Convolutional</b> <b>Neural</b> <b>Networks</b> incorporating the onboard spacecraft diagnostics of \emph{Kepler} to classify transits within a light curve. We create a pipeline to recover the location of individual transits, and the period of the orbiting planet, which maintains $>80%$ transit recovery sensitivity out to an 800-day orbital period. Our neural network pipeline has the potential to discover additional planets in the \emph{Kepler} dataset, and crucially, within the $\eta$-Earth regime. We report our first candidate from this pipeline, KOI 1271.02. KOI 1271.01 is known to exhibit strong Transit Timing Variations (TTVs), and so we jointly model the TTVs and transits of both transiting planets to constrain the orbital configuration and planetary parameters and conclude with a series of potential parameters for KOI 1271.02, as there is not enough data currently to uniquely constrain the system. We conclude that KOI 1271.02 has a radius of 5.32 $\pm$ 0.20 $R_{\oplus}$ and a mass of $28.94^{0.23}<em>{-0.47}$ $M</em>{\oplus}$. Future constraints on the nature of KOI 1271.02 require measuring additional TTVs of KOI 1271.01 or observing a second transit of KOI 1271.02.</p></p class="citation"></blockquote><h2 id=csds-5>cs.DS (5)</h2><h3 id=15--256271-largest-common-subgraph-of-two-forests-dieter-rautenbach-et-al-2024>(1/5 | 256/271) Largest common subgraph of two forests (Dieter Rautenbach et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dieter Rautenbach, Florian Werner. (2024)<br><strong>Largest common subgraph of two forests</strong><br><button class=copy-to-clipboard title="Largest common subgraph of two forests" index=256>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-256 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DM, cs-DS, cs.DS, math-CO<br>Keyword Score: 13<br>Keywords: Graph, Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03696v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03696v1.pdf filename=2403.03696v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A common subgraph of two <b>graphs</b> $G_1$ and $G_2$ is a <b>graph</b> that is isomorphic to subgraphs of $G_1$ and $G_2$. In the largest common subgraph problem the task is to determine a common subgraph for two given <b>graphs</b> $G_1$ and $G_2$ that is of maximum possible size ${\rm lcs}(G_1,G_2)$. This natural problem generalizes the well-studied <b>graph</b> isomorphism problem, has many applications, and remains NP-hard even restricted to unions of paths. We present a simple $4$-approximation algorithm for forests, and, for every fixed $\epsilon\in (0,1)$, we show that, for two given forests $F_1$ and $F_2$ of order at most $n$, one can determine in polynomial time a common subgraph $F$ of $F_1$ and $F_2$ with at least ${\rm lcs}(F_1,F_2)-\epsilon n$ edges. Restricted to instances with ${\rm lcs}(F_1,F_2)\geq cn$ for some fixed positive $c$, this yields a polynomial time approximation scheme. Our approach relies on the approximation of the given forests by structurally simpler forests that are composed of copies of only $O(\log (n))$ different starlike rooted trees and iterative <b>quantizations</b> of the options for the solutions.</p></p class="citation"></blockquote><h3 id=25--257271-parameterized-algorithms-for-balanced-cluster-edge-modification-problems-jayakrishnan-madathil-et-al-2024>(2/5 | 257/271) Parameterized Algorithms for Balanced Cluster Edge Modification Problems (Jayakrishnan Madathil et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jayakrishnan Madathil, Kitty Meeks. (2024)<br><strong>Parameterized Algorithms for Balanced Cluster Edge Modification Problems</strong><br><button class=copy-to-clipboard title="Parameterized Algorithms for Balanced Cluster Edge Modification Problems" index=257>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-257 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DM, cs-DS, cs.DS<br>Keyword Score: 6<br>Keywords: Graph, Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03830v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03830v1.pdf filename=2403.03830v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce Cluster Edge Modification problems with constraints on the size of the clusters and study their complexity. A <b>graph</b> $G$ is a cluster <b>graph</b> if every connected component of $G$ is a clique. In a typical Cluster Edge Modification problem such as the widely studied Cluster Editing, we are given a <b>graph</b> $G$ and a non-negative integer $k$ as input, and we have to decide if we can turn $G$ into a cluster <b>graph</b> by way of at most $k$ edge modifications &ndash; that is, by adding or deleting edges. In this paper, we study the parameterized complexity of such problems, but with an additional constraint: The size difference between any two connected components of the resulting cluster <b>graph</b> should not exceed a given threshold. Depending on which modifications are permissible &ndash; only adding edges, only deleting edges, both adding and deleting edges &ndash; we have three different computational problems. We show that all three problems, when parameterized by $k$, admit single-exponential time FPT algorithms and polynomial kernels. Our problems may be thought of as the size-constrained or balanced counterparts of the typical Cluster Edge Modification problems, similar to the well-studied size-constrained or balanced counterparts of other <b>clustering</b> problems such as $k$-Means <b>Clustering.</b></p></p class="citation"></blockquote><h3 id=35--258271-on-htlc-based-protocols-for-multi-party-cross-chain-swaps-emily-clark-et-al-2024>(3/5 | 258/271) On HTLC-Based Protocols for Multi-Party Cross-Chain Swaps (Emily Clark et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Emily Clark, Chloe Georgiou, Katelyn Poon, Marek Chrobak. (2024)<br><strong>On HTLC-Based Protocols for Multi-Party Cross-Chain Swaps</strong><br><button class=copy-to-clipboard title="On HTLC-Based Protocols for Multi-Party Cross-Chain Swaps" index=258>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-258 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: F-2, cs-DS, cs.DS<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03906v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03906v1.pdf filename=2403.03906v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In his 2018 paper, Herlihy introduced an atomic protocol for multi-party asset swaps across different blockchains. His model represents an asset swap by a directed <b>graph</b> whose nodes are the participating parties and edges represent asset transfers, and rational behavior of the participants is captured by a preference relation between a protocol&rsquo;s outcomes. Asset transfers between parties are achieved using smart contracts. These smart contracts are quite involved and they require storage and processing of a large number of paths in the swap digraph, limiting practical significance of his protocol. His paper also describes a different protocol that uses only standard hash time-lock contracts (HTLC&rsquo;s), but this simpler protocol applies only to some special types of digraphs. He left open the question whether there is a simple and efficient protocol for cross-chain asset swaps in arbitrary digraphs. Motivated by this open problem, we conducted a comprehensive study of \emph{HTLC-based protocols}, in which all asset transfers are implemented with HTLCs. Our main contribution is a full characterization of swap digraphs that have such protocols.</p></p class="citation"></blockquote><h3 id=45--259271-graph-visualization-for-blockchain-data-marcell-dietl-et-al-2024>(4/5 | 259/271) Graph Visualization for Blockchain Data (Marcell Dietl et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marcell Dietl, Andre Gemünd, Daniel Oeltz, Felix M. Thiele, Christian Werner. (2024)<br><strong>Graph Visualization for Blockchain Data</strong><br><button class=copy-to-clipboard title="Graph Visualization for Blockchain Data" index=259>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-259 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DM, cs-DS, cs.DS<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03504v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03504v1.pdf filename=2403.03504v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this report, we introduce a novel approach to visualize extremely large <b>graphs</b> efficiently. Our method combines two force-directed algorithms, Kamada-Kawai and ForceAtlas2, to handle different <b>graph</b> components based on their node count. Additionally, we suggest utilizing the Fast Multipole method to enhance the speed of ForceAtlas2. Although initially designed for analyzing bitcoin transaction <b>graphs,</b> for which we present results here, this algorithm can also be applied to other crypto currency transaction <b>graphs</b> or <b>graphs</b> from diverse domains.</p></p class="citation"></blockquote><h3 id=55--260271-double-exponential-lower-bound-for-telephone-broadcast-prafullkumar-tale-2024>(5/5 | 260/271) Double Exponential Lower Bound for Telephone Broadcast (Prafullkumar Tale, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Prafullkumar Tale. (2024)<br><strong>Double Exponential Lower Bound for Telephone Broadcast</strong><br><button class=copy-to-clipboard title="Double Exponential Lower Bound for Telephone Broadcast" index=260>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-260 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs.DS<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03501v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03501v1.pdf filename=2403.03501v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Consider the Telephone Broadcast problem in which an input is a connected <b>graph</b> $G$ on $n$ vertices, a source vertex $s \in V(G)$, and a positive integer $t$. The objective is to decide whether there is a broadcast protocol from $s$ that ensures that all the vertices of $G$ get the message in at most $t$ rounds. We consider the broadcast protocol where, in a round, any node aware of the message can forward it to at most one of its neighbors. As the number of nodes aware of the message can at most double at each round, for a non-trivial instance we have $n \le 2^t$. Hence, the brute force algorithm that checks all the permutations of the vertices runs in time $2^{2^{\calO(t)}} \cdot n^{\calO(1)}$. As our first result, we prove this simple algorithm is the best possible in the following sense. Telephone Broadcast does not admit an algorithm running in time $2^{2^{o(t)}} \cdot n^{\calO(1)}$, unless the \ETH\ fails. To the best of our knowledge, this is only the fourth example of \NP-Complete problem that admits a double exponential lower bound when parameterized by the solution size. It also resolves the question by Fomin, Fraigniaud, and Golovach [WG 2023]. In the same article, the authors asked whether the problem is \FPT\ when parameterized by the feedback vertex set number of the <b>graph.</b> We answer this question in the negative. Telephone Broadcast, when restricted to <b>graphs</b> of the feedback vertex number one, and hence treewidth of two, is \NP-\complete. We find this a relatively rare example of problems that admit a polynomial-time algorithm on trees but is \NP-\complete\ on <b>graphs</b> of treewidth two.</p></p class="citation"></blockquote><h2 id=csit-1>cs.IT (1)</h2><h3 id=11--261271-risnet-a-domain-knowledge-driven-neural-network-architecture-for-ris-optimization-with-mutual-coupling-and-partial-csi-bile-peng-et-al-2024>(1/1 | 261/271) RISnet: A Domain-Knowledge Driven Neural Network Architecture for RIS Optimization with Mutual Coupling and Partial CSI (Bile Peng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bile Peng, Karl-Ludwig Besser, Shanpu Shen, Finn Siegismund-Poschmann, Ramprasad Raghunath, Daniel Mittleman, Vahid Jamali, Eduard A. Jorswieck. (2024)<br><strong>RISnet: A Domain-Knowledge Driven Neural Network Architecture for RIS Optimization with Mutual Coupling and Partial CSI</strong><br><button class=copy-to-clipboard title="RISnet: A Domain-Knowledge Driven Neural Network Architecture for RIS Optimization with Mutual Coupling and Partial CSI" index=261>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-261 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 10<br>Keywords: Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04028v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04028v1.pdf filename=2403.04028v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multiple access techniques are cornerstones of wireless communications. Their performance depends on the channel properties, which can be improved by reconfigurable intelligent surfaces (RISs). In this work, we jointly optimize MA precoding at the base station (BS) and RIS configuration. We tackle difficulties of mutual coupling between RIS elements, scalability to more than 1000 RIS elements, and channel estimation. We first derive an RIS-assisted channel model considering mutual coupling, then propose an <b>unsupervised</b> machine learning (ML) approach to optimize the RIS. In particular, we design a dedicated neural network (NN) architecture RISnet with good scalability and desired symmetry. Moreover, we combine ML-enabled RIS configuration and analytical precoding at BS since there exist analytical precoding schemes. Furthermore, we propose another variant of RISnet, which requires the channel state information (CSI) of a small portion of RIS elements (in this work, 16 out of 1296 elements) if the channel comprises a few specular propagation paths. More generally, this work is an early contribution to combine ML technique and domain knowledge in communication for NN architecture design. Compared to generic ML, the problem-specific ML can achieve higher performance, lower complexity and symmetry.</p></p class="citation"></blockquote><h2 id=q-bioto-1>q-bio.TO (1)</h2><h3 id=11--262271-hitchhikers-guide-to-cancer-associated-lymphoid-aggregates-in-histology-images-manual-and-deep-learning-based-quantification-approaches-karina-silina-et-al-2024>(1/1 | 262/271) Hitchhiker&rsquo;s guide to cancer-associated lymphoid aggregates in histology images: manual and deep learning-based quantification approaches (Karina Silina et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Karina Silina, Francesco Ciompi. (2024)<br><strong>Hitchhiker&rsquo;s guide to cancer-associated lymphoid aggregates in histology images: manual and deep learning-based quantification approaches</strong><br><button class=copy-to-clipboard title="Hitchhiker's guide to cancer-associated lymphoid aggregates in histology images: manual and deep learning-based quantification approaches" index=262>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-262 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.TO<br>Categories: cs-CV, q-bio-QM, q-bio-TO, q-bio.TO<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04142v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04142v1.pdf filename=2403.04142v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Quantification of lymphoid aggregates including tertiary lymphoid structures with germinal centers in histology images of cancer is a promising approach for developing prognostic and predictive tissue biomarkers. In this article, we provide <b>recommendations</b> for identifying lymphoid aggregates in tissue sections from routine pathology workflows such as hematoxylin and eosin staining. To overcome the intrinsic variability associated with manual image analysis (such as subjective decision making, attention span), we recently developed a deep learning-based algorithm called HookNet-TLS to detect lymphoid aggregates and germinal centers in various tissues. Here, we additionally provide a guideline for using manually annotated images for training and implementing HookNet-TLS for automated and objective quantification of lymphoid aggregates in various cancer types.</p></p class="citation"></blockquote><h2 id=cslo-1>cs.LO (1)</h2><h3 id=11--263271-saturating-sorting-without-sorts-pamina-georgiou-et-al-2024>(1/1 | 263/271) Saturating Sorting without Sorts (Pamina Georgiou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pamina Georgiou, Márton Hajdu, Laura Kovács. (2024)<br><strong>Saturating Sorting without Sorts</strong><br><button class=copy-to-clipboard title="Saturating Sorting without Sorts" index=263>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-263 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LO<br>Categories: cs-LO, cs-SC, cs.LO<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03712v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03712v1.pdf filename=2403.03712v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a first-order theorem proving framework for establishing the correctness of functional programs implementing sorting algorithms with recursive data structures. We formalize the semantics of recursive programs in many-sorted first-order logic and integrate sortedness/permutation properties within our first-order formalization. Rather than focusing on sorting lists of elements of specific first-order theories, such as integer arithmetic, our list formalization relies on a sort parameter abstracting (arithmetic) theories and hence concrete sorts. We formalize the permutation property of lists in first-order logic so that we automatically prove verification conditions of such algorithms purely by superpositon-based first-order <b>reasoning.</b> Doing so, we adjust recent efforts for automating inducion in saturation. We advocate a compositional approach for automating proofs by induction required to verify functional programs implementing and preserving sorting and permutation properties over parameterized list structures. Our work turns saturation-based first-order theorem proving into an automated verification engine by (i) guiding automated inductive <b>reasoning</b> with manual proof splits and (ii) fully automating inductive <b>reasoning</b> in saturation. We showcase the applicability of our framework over recursive sorting algorithms, including Mergesort and Quicksort.</p></p class="citation"></blockquote><h2 id=mathoc-1>math.OC (1)</h2><h3 id=11--264271-data-driven-superstabilizing-control-under-quadratically-bounded-errors-in-variables-noise-jared-miller-et-al-2024>(1/1 | 264/271) Data-Driven Superstabilizing Control under Quadratically-Bounded Errors-in-Variables Noise (Jared Miller et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jared Miller, Tianyu Dai, Mario Sznaier. (2024)<br><strong>Data-Driven Superstabilizing Control under Quadratically-Bounded Errors-in-Variables Noise</strong><br><button class=copy-to-clipboard title="Data-Driven Superstabilizing Control under Quadratically-Bounded Errors-in-Variables Noise" index=264>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-264 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-SY, eess-SY, math-OC, math.OC<br>Keyword Score: 10<br>Keywords: Discrete Time, Discrete Time<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03624v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03624v1.pdf filename=2403.03624v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Error-in-Variables model of system identification/control involves nontrivial input and measurement corruption of observed data, resulting in generically nonconvex optimization problems. This paper performs full-state-feedback stabilizing control of all <b>discrete-time</b> <b>linear</b> systems that are consistent with observed data for which the input and measurement noise obey quadratic bounds. Instances of such quadratic bounds include elementwise norm bounds (at each time sample), energy bounds (across the entire signal), and chance constraints arising from (sub)gaussian noise. Superstabilizing controllers are generated through the solution of a sum-of-squares hierarchy of semidefinite programs. A theorem of alternatives is employed to eliminate the input and measurement noise process, thus improving tractability. Effectiveness of the scheme is generated on an example system in the chance-constrained set-membership setting where the input and state-measurement noise are i.i.d. normally distributed.</p></p class="citation"></blockquote><h2 id=csni-1>cs.NI (1)</h2><h3 id=11--265271-spectrum-occupancy-detection-supported-by-federated-learning-łukasz-kułacz-2024>(1/1 | 265/271) Spectrum Occupancy Detection Supported by Federated Learning (Łukasz Kułacz, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Łukasz Kułacz. (2024)<br><strong>Spectrum Occupancy Detection Supported by Federated Learning</strong><br><button class=copy-to-clipboard title="Spectrum Occupancy Detection Supported by Federated Learning" index=265>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-265 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI, eess-SP<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03617v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03617v1.pdf filename=2403.03617v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Dynamic spectrum access is essential for radiocommunication and its limited spectrum resources. The key element of dynamic spectrum access systems is effective spectrum occupancy detection. In many cases, machine learning algorithms improve detection effectiveness. Because of the recent trend of using <b>federated</b> <b>learning,</b> a <b>federated</b> <b>learning</b> algorithm is presented in the context of distributed spectrum occupancy detection. The results of the work presented in the paper are based on actual signal samples collected in the laboratory. The proposed algorithm is effective, especially in the context of a set of sensors with faulty sensors.</p></p class="citation"></blockquote><h2 id=mathat-1>math.AT (1)</h2><h3 id=11--266271-computing-representatives-of-persistent-homology-generators-with-a-double-twist-tuyen-pham-et-al-2024>(1/1 | 266/271) Computing Representatives of Persistent Homology Generators with a Double Twist (Tuyen Pham et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tuyen Pham, Hubert Wagner. (2024)<br><strong>Computing Representatives of Persistent Homology Generators with a Double Twist</strong><br><button class=copy-to-clipboard title="Computing Representatives of Persistent Homology Generators with a Double Twist" index=266>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-266 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.AT<br>Categories: cs-CG, math-AT, math.AT<br>Keyword Score: 5<br>Keywords: Black Box<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04100v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04100v1.pdf filename=2403.04100v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the growing availability of efficient tools, persistent homology is becoming a useful methodology in a variety of applications. Significant work has been devoted to implementing tools for persistent homology diagrams; however, computing representative cycles corresponding to each point in the diagram can still be inefficient. To circumvent this problem, we extend the twist algorithm of Chen and Kerber. Our extension is based on a new technique we call saving, which supplements their existing killing technique. The resulting two-pass strategy can be realized using an existing matrix reduction implementation as a <b>black-box</b> <b>and</b> improves the efficiency of computing representatives of persistent homology generators. We prove the correctness of the new approach and experimentally show its performance.</p></p class="citation"></blockquote><h2 id=csgl-1>cs.GL (1)</h2><h3 id=11--267271-eternal-sunshine-of-the-mechanical-mind-the-irreconcilability-of-machine-learning-and-the-right-to-be-forgotten-meem-arafat-manab-2024>(1/1 | 267/271) Eternal Sunshine of the Mechanical Mind: The Irreconcilability of Machine Learning and the Right to be Forgotten (Meem Arafat Manab, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Meem Arafat Manab. (2024)<br><strong>Eternal Sunshine of the Mechanical Mind: The Irreconcilability of Machine Learning and the Right to be Forgotten</strong><br><button class=copy-to-clipboard title="Eternal Sunshine of the Mechanical Mind: The Irreconcilability of Machine Learning and the Right to be Forgotten" index=267>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-267 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GL<br>Categories: 68P27, K-4-1; K-5-2; I-2-0, cs-AI, cs-GL, cs.GL<br>Keyword Score: 5<br>Keywords: Black Box<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05592v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05592v1.pdf filename=2403.05592v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As we keep rapidly advancing toward an era where artificial intelligence is a constant and normative experience for most of us, we must also be aware of what this vision and this progress entail. By first approximating neural connections and activities in computer circuits and then creating more and more sophisticated versions of this crude approximation, we are now facing an age to come where modern deep learning-based artificial intelligence systems can rightly be called thinking machines, and they are sometimes even lauded for their emergent behavior and <b>black-box</b> <b>approaches.</b> But as we create more powerful electronic brains, with billions of neural connections and parameters, can we guarantee that these mammoths built of artificial neurons will be able to forget the data that we store in them? If they are at some level like a brain, can the right to be forgotten still be protected while dealing with these AIs? The essential gap between machine learning and the RTBF is explored in this article, with a premonition of far-reaching conclusions if the gap is not bridged or reconciled any time soon. The core argument is that deep learning models, due to their structure and size, cannot be expected to forget or delete a data as it would be expected from a tabular database, and they should be treated more like a mechanical brain, albeit still in development.</p></p class="citation"></blockquote><h2 id=csdb-1>cs.DB (1)</h2><h3 id=11--268271-spanning-tree-based-query-plan-enumeration-yesdaulet-izenov-et-al-2024>(1/1 | 268/271) Spanning Tree-based Query Plan Enumeration (Yesdaulet Izenov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yesdaulet Izenov, Asoke Datta, Brian Tsan, Abylay Amanbayev, Florin Rusu. (2024)<br><strong>Spanning Tree-based Query Plan Enumeration</strong><br><button class=copy-to-clipboard title="Spanning Tree-based Query Plan Enumeration" index=268>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-268 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DB<br>Categories: cs-DB, cs.DB<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.04026v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.04026v1.pdf filename=2403.04026v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we define the problem of finding an optimal query plan as finding spanning trees with low costs. This approach empowers the utilization of a series of spanning tree algorithms, thereby enabling systematic exploration of the plan search space over a join <b>graph.</b> Capitalizing on the polynomial time complexity of spanning tree algorithms, we present the Ensemble Spanning Tree Enumeration (ESTE) strategy. ESTE employs two conventional spanning tree algorithms, Prim&rsquo;s and Kruskal&rsquo;s, together to enhance the robustness of the query optimizer. In ESTE, multiple query plans are enumerated exploring different areas of the search space. This positions ESTE as an intermediate strategy between exhaustive and heuristic enumeration strategies. We show that ESTE is more robust in identifying efficient query plans for large queries. In the case of data modifications and workload demand increase, we believe that our approach can be a cheaper alternative to maintain optimizer robustness by integrating additional spanning tree algorithms rather than completely changing the optimizer to another plan enumeration algorithm. The experimental evaluation shows that ESTE achieves better consistency in plan quality and optimization time than existing solutions while identifying similarly optimal plans.</p></p class="citation"></blockquote><h2 id=physicsoptics-1>physics.optics (1)</h2><h3 id=11--269271-photonic-electronic-spiking-neuron-with-multi-modal-and-multi-wavelength-excitatory-and-inhibitory-operation-for-high-speed-neuromorphic-sensing-and-computing-weikang-zhang-et-al-2024>(1/1 | 269/271) Photonic-electronic spiking neuron with multi-modal and multi-wavelength excitatory and inhibitory operation for high-speed neuromorphic sensing and computing (Weikang Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Weikang Zhang, Matěj Hejda, Qusay Raghib Ali Al-Taai, Dafydd Owen-Newns, Bruno Romeira, José M. L. Figueiredo, Joshua Robertson, Edward Wasige, Antonio Hurtado. (2024)<br><strong>Photonic-electronic spiking neuron with multi-modal and multi-wavelength excitatory and inhibitory operation for high-speed neuromorphic sensing and computing</strong><br><button class=copy-to-clipboard title="Photonic-electronic spiking neuron with multi-modal and multi-wavelength excitatory and inhibitory operation for high-speed neuromorphic sensing and computing" index=269>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-269 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.optics<br>Categories: cs-ET, physics-optics, physics.optics<br>Keyword Score: 3<br>Keywords: Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03775v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03775v1.pdf filename=2403.03775v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We report a <b>multi-modal</b> spiking neuron that allows optical and electronic input and control, and wavelength-multiplexing operation, for use in novel high-speed neuromorphic sensing and computing functionalities. The photonic-electronic neuron is built with a micro-scale, nanostructure resonant tunnelling diode (RTD) with photodetection (PD) capability. Leveraging the advantageous intrinsic properties of this RTD-PD system, namely highly nonlinear characteristics, photo-sensitivity, light-induced I-V curve shift, and the ability to deliver excitable responses under electrical and optical inputs, we successfully achieve flexible neuromorphic spike activation and inhibition regimes through photonic-electrical control. We also demonstrate the ability of this RTD-PD spiking sensing-processing neuron to operate under the simultaneous arrival of multiple wavelength-multiplexed optical signals, due to its large photodetection spectral window (covering the 1310 and 1550 nm telecom wavelength bands). Our results highlight the potential of RTD photonic-electronic neurons to reproduce multiple key excitatory and inhibitory spiking regimes, at high speed (ns-rate spiking responses, with faster sub-ns regimes theoretically predicted) and low energy (requiring only ~10 mV and ~150 microW, electrical and optical input amplitudes, respectively), similar in nature to those commonly found in the biological neurons of the visual system and the brain. This work offers a highly promising approach for the realisation of high-speed, energy-efficient photonic-electronic spiking neurons and spiking neural networks, enabling <b>multi-modal</b> and multi-wavelength operation for sensing and information processing tasks. This work therefore paves the way for innovative high-speed, photonic-electronic, and spike-based neuromorphic sensing and computing systems and artificial intelligence hardware.</p></p class="citation"></blockquote><h2 id=mathco-2>math.CO (2)</h2><h3 id=12--270271-on-the-structure-of-hamiltonian-graphs-with-small-independence-number-nikola-jedličková-et-al-2024>(1/2 | 270/271) On the Structure of Hamiltonian Graphs with Small Independence Number (Nikola Jedličková et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nikola Jedličková, Jan Kratochvíl. (2024)<br><strong>On the Structure of Hamiltonian Graphs with Small Independence Number</strong><br><button class=copy-to-clipboard title="On the Structure of Hamiltonian Graphs with Small Independence Number" index=270>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-270 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.CO<br>Categories: cs-CC, math-CO, math.CO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03668v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03668v1.pdf filename=2403.03668v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A Hamiltonian path (cycle) in a <b>graph</b> is a path (cycle, respectively) which passes through all of its vertices. The problems of deciding the existence of a Hamiltonian cycle (path) in an input <b>graph</b> are well known to be NP-complete, and restricted classes of <b>graphs</b> which allow for their polynomial-time solutions are intensively investigated. Until very recently the complexity was open even for <b>graphs</b> of independence number at most 3. So far unpublished result of Jedli\v{c}kov'{a} and Kratochv'{\i}l [arXiv:2309.09228] shows that for every integer $k$, Hamiltonian path and cycle are polynomial-time solvable in <b>graphs</b> of independence number bounded by $k$. As a companion structural result, we determine explicit obstacles for the existence of a Hamiltonian path for small values of $k$, namely for <b>graphs</b> of independence number 2, 3, and 4. Identifying these obstacles in an input <b>graph</b> yields alternative polynomial-time algorithms for Hamiltonian path and cycle with no large hidden multiplicative constants.</p></p class="citation"></blockquote><h3 id=22--271271-secure-total-domination-number-in-maximal-outerplanar-graphs-yasufumi-aita-et-al-2024>(2/2 | 271/271) Secure Total Domination Number in Maximal Outerplanar Graphs (Yasufumi Aita et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yasufumi Aita, Toru Araki. (2024)<br><strong>Secure Total Domination Number in Maximal Outerplanar Graphs</strong><br><button class=copy-to-clipboard title="Secure Total Domination Number in Maximal Outerplanar Graphs" index=271>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-271 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.CO<br>Categories: 05C69, 05C10, G-2-2, cs-DM, math-CO, math.CO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.03404v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.03404v1.pdf filename=2403.03404v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A subset $S$ of vertices in a <b>graph</b> $G$ is a secure total dominating set of $G$ if $S$ is a total dominating set of $G$ and, for each vertex $u \not\in S$, there is a vertex $v \in S$ such that $uv$ is an edge and $(S \setminus {v}) \cup {u}$ is also a total dominating set of $G$. We show that if $G$ is a maximal outerplanar <b>graph</b> of order $n$, then $G$ has a total secure dominating set of size at most $\lfloor 2n/3 \rfloor$. Moreover, if an outerplanar <b>graph</b> $G$ of order $n$, then each secure total dominating set has at least $\lceil (n+2)/3 \rceil$ vertices. We show that these bounds are best possible.</p></p class="citation"></blockquote></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/ title="arXiv @ 2024.03.07" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Prev</div><div class=next-prev-text>arXiv @ 2024.03.07</div></a></div><div class="col-md-6 next-article"><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/ title="arXiv @ 2024.03.09" class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>arXiv @ 2024.03.09</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#primary-categories>Primary Categories</a></li><li><a href=#keywords>Keywords</a></li><li><a href=#cscl-39>cs.CL (39)</a><ul><li><a href=#139--1271-benchmarking-hallucination-in-large-language-models-based-on-unanswerable-math-word-problem-yuhong-sun-et-al-2024>(1/39 | 1/271) Benchmarking Hallucination in Large Language Models based on Unanswerable Math Word Problem (Yuhong Sun et al., 2024)</a></li><li><a href=#239--2271-general2specialized-llms-translation-for-e-commerce-kaidi-chen-et-al-2024>(2/39 | 2/271) General2Specialized LLMs Translation for E-commerce (Kaidi Chen et al., 2024)</a></li><li><a href=#339--3271-can-large-language-models-do-analytical-reasoning-yebowen-hu-et-al-2024>(3/39 | 3/271) Can Large Language Models do Analytical Reasoning? (Yebowen Hu et al., 2024)</a></li><li><a href=#439--4271-rapidly-developing-high-quality-instruction-data-and-evaluation-benchmark-for-large-language-models-with-minimal-human-effort-a-case-study-on-japanese-yikun-sun-et-al-2024>(4/39 | 4/271) Rapidly Developing High-quality Instruction Data and Evaluation Benchmark for Large Language Models with Minimal Human Effort: A Case Study on Japanese (Yikun Sun et al., 2024)</a></li><li><a href=#539--5271-designing-informative-metrics-for-few-shot-example-selection-rishabh-adiga-et-al-2024>(5/39 | 5/271) Designing Informative Metrics for Few-Shot Example Selection (Rishabh Adiga et al., 2024)</a></li><li><a href=#639--6271-german-also-hallucinates-inconsistency-detection-in-news-summaries-with-the-absinth-dataset-laura-mascarell-et-al-2024>(6/39 | 6/271) German also Hallucinates! Inconsistency Detection in News Summaries with the Absinth Dataset (Laura Mascarell et al., 2024)</a></li><li><a href=#739--7271-japanese-english-sentence-translation-exercises-dataset-for-automatic-grading-naoki-miura-et-al-2024>(7/39 | 7/271) Japanese-English Sentence Translation Exercises Dataset for Automatic Grading (Naoki Miura et al., 2024)</a></li><li><a href=#839--8271-evaluating-the-elementary-multilingual-capabilities-of-large-language-models-with-multiq-carolin-holtermann-et-al-2024>(8/39 | 8/271) Evaluating the Elementary Multilingual Capabilities of Large Language Models with MultiQ (Carolin Holtermann et al., 2024)</a></li><li><a href=#939--9271-x-shot-a-unified-system-to-handle-frequent-few-shot-and-zero-shot-learning-simultaneously-in-classification-hanzi-xu-et-al-2024>(9/39 | 9/271) X-Shot: A Unified System to Handle Frequent, Few-shot and Zero-shot Learning Simultaneously in Classification (Hanzi Xu et al., 2024)</a></li><li><a href=#1039--10271-multimodal-large-language-models-to-support-real-world-fact-checking-jiahui-geng-et-al-2024>(10/39 | 10/271) Multimodal Large Language Models to Support Real-World Fact-Checking (Jiahui Geng et al., 2024)</a></li><li><a href=#1139--11271-did-translation-models-get-more-robust-without-anyone-even-noticing-ben-peters-et-al-2024>(11/39 | 11/271) Did Translation Models Get More Robust Without Anyone Even Noticing? (Ben Peters et al., 2024)</a></li><li><a href=#1239--12271-faaf-facts-as-a-function-for-the-evaluation-of-rag-systems-vasileios-katranidis-et-al-2024>(12/39 | 12/271) FaaF: Facts as a Function for the evaluation of RAG systems (Vasileios Katranidis et al., 2024)</a></li><li><a href=#1339--13271-learning-to-decode-collaboratively-with-multiple-language-models-shannon-zejiang-shen-et-al-2024>(13/39 | 13/271) Learning to Decode Collaboratively with Multiple Language Models (Shannon Zejiang Shen et al., 2024)</a></li><li><a href=#1439--14271-shortgpt-layers-in-large-language-models-are-more-redundant-than-you-expect-xin-men-et-al-2024>(14/39 | 14/271) ShortGPT: Layers in Large Language Models are More Redundant Than You Expect (Xin Men et al., 2024)</a></li><li><a href=#1539--15271-pptc-r-benchmark-towards-evaluating-the-robustness-of-large-language-models-for-powerpoint-task-completion-zekai-zhang-et-al-2024>(15/39 | 15/271) PPTC-R benchmark: Towards Evaluating the Robustness of Large Language Models for PowerPoint Task Completion (Zekai Zhang et al., 2024)</a></li><li><a href=#1639--16271-design-of-an-open-source-architecture-for-neural-machine-translation-séamus-lankford-et-al-2024>(16/39 | 16/271) Design of an Open-Source Architecture for Neural Machine Translation (Séamus Lankford et al., 2024)</a></li><li><a href=#1739--17271-unsupervised-multilingual-dense-retrieval-via-generative-pseudo-labeling-chao-wei-huang-et-al-2024>(17/39 | 17/271) Unsupervised Multilingual Dense Retrieval via Generative Pseudo Labeling (Chao-Wei Huang et al., 2024)</a></li><li><a href=#1839--18271-semi-supervised-dialogue-abstractive-summarization-via-high-quality-pseudolabel-selection-jianfeng-he-et-al-2024>(18/39 | 18/271) Semi-Supervised Dialogue Abstractive Summarization via High-Quality Pseudolabel Selection (Jianfeng He et al., 2024)</a></li><li><a href=#1939--19271-saullm-7b-a-pioneering-large-language-model-for-law-pierre-colombo-et-al-2024>(19/39 | 19/271) SaulLM-7B: A pioneering Large Language Model for Law (Pierre Colombo et al., 2024)</a></li><li><a href=#2039--20271-gptopic-dynamic-and-interactive-topic-representations-arik-reuter-et-al-2024>(20/39 | 20/271) GPTopic: Dynamic and Interactive Topic Representations (Arik Reuter et al., 2024)</a></li><li><a href=#2139--21271-gahealth-an-english-irish-bilingual-corpus-of-health-data-séamus-lankford-et-al-2024>(21/39 | 21/271) gaHealth: An English-Irish Bilingual Corpus of Health Data (Séamus Lankford et al., 2024)</a></li><li><a href=#2239--22271-bivert-bidirectional-vocabulary-evaluation-using-relations-for-machine-translation-carinne-cherf-et-al-2024>(22/39 | 22/271) BiVert: Bidirectional Vocabulary Evaluation using Relations for Machine Translation (Carinne Cherf et al., 2024)</a></li><li><a href=#2339--23271-mixture-of-loras-an-efficient-multitask-tuning-for-large-language-models-wenfeng-feng-et-al-2024>(23/39 | 23/271) Mixture-of-LoRAs: An Efficient Multitask Tuning for Large Language Models (Wenfeng Feng et al., 2024)</a></li><li><a href=#2439--24271-a-modular-approach-for-multimodal-summarization-of-tv-shows-louis-mahon-et-al-2024>(24/39 | 24/271) A Modular Approach for Multimodal Summarization of TV Shows (Louis Mahon et al., 2024)</a></li><li><a href=#2539--25271-a-knowledge-plug-and-play-test-bed-for-open-domain-dialogue-generation-xiangci-li-et-al-2024>(25/39 | 25/271) A Knowledge Plug-and-Play Test Bed for Open-domain Dialogue Generation (Xiangci Li et al., 2024)</a></li><li><a href=#2639--26271-kiwi-a-dataset-of-knowledge-intensive-writing-instructions-for-answering-research-questions-fangyuan-xu-et-al-2024>(26/39 | 26/271) KIWI: A Dataset of Knowledge-Intensive Writing Instructions for Answering Research Questions (Fangyuan Xu et al., 2024)</a></li><li><a href=#2739--27271-enhancing-asd-detection-accuracy-a-combined-approach-of-machine-learning-and-deep-learning-models-with-natural-language-processing-sergio-rubio-martín-et-al-2024>(27/39 | 27/271) Enhancing ASD detection accuracy: a combined approach of machine learning and deep learning models with natural language processing (Sergio Rubio-Martín et al., 2024)</a></li><li><a href=#2839--28271-apollo-an-lightweight-multilingual-medical-llm-towards-democratizing-medical-ai-to-6b-people-xidong-wang-et-al-2024>(28/39 | 28/271) Apollo: An Lightweight Multilingual Medical LLM towards Democratizing Medical AI to 6B People (Xidong Wang et al., 2024)</a></li><li><a href=#2939--29271-clongeval-a-chinese-benchmark-for-evaluating-long-context-large-language-models-zexuan-qiu-et-al-2024>(29/39 | 29/271) CLongEval: A Chinese Benchmark for Evaluating Long-Context Large Language Models (Zexuan Qiu et al., 2024)</a></li><li><a href=#3039--30271-the-heuristic-core-understanding-subnetwork-generalization-in-pretrained-language-models-adithya-bhaskar-et-al-2024>(30/39 | 30/271) The Heuristic Core: Understanding Subnetwork Generalization in Pretrained Language Models (Adithya Bhaskar et al., 2024)</a></li><li><a href=#3139--31271-on-the-origins-of-linear-representations-in-large-language-models-yibo-jiang-et-al-2024>(31/39 | 31/271) On the Origins of Linear Representations in Large Language Models (Yibo Jiang et al., 2024)</a></li><li><a href=#3239--32271-emojinize-enriching-any-text-with-emoji-translations-lars-henning-klein-et-al-2024>(32/39 | 32/271) Emojinize: Enriching Any Text with Emoji Translations (Lars Henning Klein et al., 2024)</a></li><li><a href=#3339--33271-towards-detecting-ai-generated-text-within-human-ai-collaborative-hybrid-texts-zijie-zeng-et-al-2024>(33/39 | 33/271) Towards Detecting AI-Generated Text within Human-AI Collaborative Hybrid Texts (Zijie Zeng et al., 2024)</a></li><li><a href=#3439--34271-negating-negatives-alignment-without-human-positive-samples-via-distributional-dispreference-optimization-shitong-duan-et-al-2024>(34/39 | 34/271) Negating Negatives: Alignment without Human Positive Samples via Distributional Dispreference Optimization (Shitong Duan et al., 2024)</a></li><li><a href=#3539--35271-transformers-and-language-models-in-form-understanding-a-comprehensive-review-of-scanned-document-analysis-abdelrahman-abdallah-et-al-2024>(35/39 | 35/271) Transformers and Language Models in Form Understanding: A Comprehensive Review of Scanned Document Analysis (Abdelrahman Abdallah et al., 2024)</a></li><li><a href=#3639--36271-magic-markup-maintaining-document-external-markup-with-an-llm-edward-misback-et-al-2024>(36/39 | 36/271) Magic Markup: Maintaining Document-External Markup with an LLM (Edward Misback et al., 2024)</a></li><li><a href=#3739--37271-from-one-to-many-expanding-the-scope-of-toxicity-mitigation-in-language-models-luiza-pozzobon-et-al-2024>(37/39 | 37/271) From One to Many: Expanding the Scope of Toxicity Mitigation in Language Models (Luiza Pozzobon et al., 2024)</a></li><li><a href=#3839--38271-vlsp-2023----lter-a-summary-of-the-challenge-on-legal-textual-entailment-recognition-vu-tran-et-al-2024>(38/39 | 38/271) VLSP 2023 &ndash; LTER: A Summary of the Challenge on Legal Textual Entailment Recognition (Vu Tran et al., 2024)</a></li><li><a href=#3939--39271-a-measure-for-transparent-comparison-of-linguistic-diversity-in-multilingual-nlp-data-sets-tanja-samardzic-et-al-2024>(39/39 | 39/271) A Measure for Transparent Comparison of Linguistic Diversity in Multilingual NLP Data Sets (Tanja Samardzic et al., 2024)</a></li></ul></li><li><a href=#cslg-54>cs.LG (54)</a><ul><li><a href=#154--40271-self-attention-empowered-graph-convolutional-network-for-structure-learning-and-node-embedding-mengying-jiang-et-al-2024>(1/54 | 40/271) Self-Attention Empowered Graph Convolutional Network for Structure Learning and Node Embedding (Mengying Jiang et al., 2024)</a></li><li><a href=#254--41271-a-teacher-free-graph-knowledge-distillation-framework-with-dual-self-distillation-lirong-wu-et-al-2024>(2/54 | 41/271) A Teacher-Free Graph Knowledge Distillation Framework with Dual Self-Distillation (Lirong Wu et al., 2024)</a></li><li><a href=#354--42271-galore-memory-efficient-llm-training-by-gradient-low-rank-projection-jiawei-zhao-et-al-2024>(3/54 | 42/271) GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection (Jiawei Zhao et al., 2024)</a></li><li><a href=#454--43271-on-the-effectiveness-of-distillation-in-mitigating-backdoors-in-pre-trained-encoder-tingxu-han-et-al-2024>(4/54 | 43/271) On the Effectiveness of Distillation in Mitigating Backdoors in Pre-trained Encoder (Tingxu Han et al., 2024)</a></li><li><a href=#554--44271-three-revisits-to-node-level-graph-anomaly-detection-outliers-message-passing-and-hyperbolic-neural-networks-jing-gu-et-al-2024>(5/54 | 44/271) Three Revisits to Node-Level Graph Anomaly Detection: Outliers, Message Passing and Hyperbolic Neural Networks (Jing Gu et al., 2024)</a></li><li><a href=#654--45271-probabilistic-topic-modelling-with-transformer-representations-arik-reuter-et-al-2024>(6/54 | 45/271) Probabilistic Topic Modelling with Transformer Representations (Arik Reuter et al., 2024)</a></li><li><a href=#754--46271-simplified-pcnet-with-robustness-bingheng-li-et-al-2024>(7/54 | 46/271) Simplified PCNet with Robustness (Bingheng Li et al., 2024)</a></li><li><a href=#854--47271-unsupervised-contrastive-learning-for-robust-rf-device-fingerprinting-under-time-domain-shift-jun-chen-et-al-2024>(8/54 | 47/271) Unsupervised Contrastive Learning for Robust RF Device Fingerprinting Under Time-Domain Shift (Jun Chen et al., 2024)</a></li><li><a href=#954--48271-knockoff-guided-feature-selection-via-a-single-pre-trained-reinforced-agent-xinyuan-wang-et-al-2024>(9/54 | 48/271) Knockoff-Guided Feature Selection via A Single Pre-trained Reinforced Agent (Xinyuan Wang et al., 2024)</a></li><li><a href=#1054--49271-stop-regressing-training-value-functions-via-classification-for-scalable-deep-rl-jesse-farebrother-et-al-2024>(10/54 | 49/271) Stop Regressing: Training Value Functions via Classification for Scalable Deep RL (Jesse Farebrother et al., 2024)</a></li><li><a href=#1154--50271-on-transfer-in-classification-how-well-do-subsets-of-classes-generalize-raphael-baena-et-al-2024>(11/54 | 50/271) On Transfer in Classification: How Well do Subsets of Classes Generalize? (Raphael Baena et al., 2024)</a></li><li><a href=#1254--51271-provable-filter-for-real-world-graph-clustering-xuanting-xie-et-al-2024>(12/54 | 51/271) Provable Filter for Real-world Graph Clustering (Xuanting Xie et al., 2024)</a></li><li><a href=#1354--52271-sampling-based-safe-reinforcement-learning-for-nonlinear-dynamical-systems-wesley-a-suttle-et-al-2024>(13/54 | 52/271) Sampling-based Safe Reinforcement Learning for Nonlinear Dynamical Systems (Wesley A. Suttle et al., 2024)</a></li><li><a href=#1454--53271-graph-neural-network-outputs-are-almost-surely-asymptotically-constant-sam-adam-day-et-al-2024>(14/54 | 53/271) Graph neural network outputs are almost surely asymptotically constant (Sam Adam-Day et al., 2024)</a></li><li><a href=#1554--54271-kg-treat-pre-training-for-treatment-effect-estimation-by-synergizing-patient-data-with-knowledge-graphs-ruoqi-liu-et-al-2024>(15/54 | 54/271) KG-TREAT: Pre-training for Treatment Effect Estimation by Synergizing Patient Data with Knowledge Graphs (Ruoqi Liu et al., 2024)</a></li><li><a href=#1654--55271-prediction-of-cryptocurrency-prices-using-lstm-svm-and-polynomial-regression-novan-fauzi-al-giffary-et-al-2024>(16/54 | 55/271) Prediction Of Cryptocurrency Prices Using LSTM, SVM And Polynomial Regression (Novan Fauzi Al Giffary et al., 2024)</a></li><li><a href=#1754--56271-inference-via-interpolation-contrastive-representations-provably-enable-planning-and-inference-benjamin-eysenbach-et-al-2024>(17/54 | 56/271) Inference via Interpolation: Contrastive Representations Provably Enable Planning and Inference (Benjamin Eysenbach et al., 2024)</a></li><li><a href=#1854--57271-bridging-diversity-and-uncertainty-in-active-learning-with-self-supervised-pre-training-paul-doucet-et-al-2024>(18/54 | 57/271) Bridging Diversity and Uncertainty in Active learning with Self-Supervised Pre-Training (Paul Doucet et al., 2024)</a></li><li><a href=#1954--58271-routeexplainer-an-explanation-framework-for-vehicle-routing-problem-daisuke-kikuta-et-al-2024>(19/54 | 58/271) RouteExplainer: An Explanation Framework for Vehicle Routing Problem (Daisuke Kikuta et al., 2024)</a></li><li><a href=#2054--59271-inverse-free-fast-natural-gradient-descent-method-for-deep-learning-xinwei-ou-et-al-2024>(20/54 | 59/271) Inverse-Free Fast Natural Gradient Descent Method for Deep Learning (Xinwei Ou et al., 2024)</a></li><li><a href=#2154--60271-boosting-meta-training-with-base-class-information-for-few-shot-learning-weihao-jiang-et-al-2024>(21/54 | 60/271) Boosting Meta-Training with Base Class Information for Few-Shot Learning (Weihao Jiang et al., 2024)</a></li><li><a href=#2254--61271-temporal-cross-attention-for-dynamic-embedding-and-tokenization-of-multimodal-electronic-health-records-yingbo-ma-et-al-2024>(22/54 | 61/271) Temporal Cross-Attention for Dynamic Embedding and Tokenization of Multimodal Electronic Health Records (Yingbo Ma et al., 2024)</a></li><li><a href=#2354--62271-learning-invariant-representations-of-graph-neural-networks-via-cluster-generalization-donglin-xia-et-al-2024>(23/54 | 62/271) Learning Invariant Representations of Graph Neural Networks via Cluster Generalization (Donglin Xia et al., 2024)</a></li><li><a href=#2454--63271-dpot-auto-regressive-denoising-operator-transformer-for-large-scale-pde-pre-training-zhongkai-hao-et-al-2024>(24/54 | 63/271) DPOT: Auto-Regressive Denoising Operator Transformer for Large-Scale PDE Pre-Training (Zhongkai Hao et al., 2024)</a></li><li><a href=#2554--64271-many-objective-multi-solution-transport-ziyue-li-et-al-2024>(25/54 | 64/271) Many-Objective Multi-Solution Transport (Ziyue Li et al., 2024)</a></li><li><a href=#2654--65271-improving-adversarial-training-using-vulnerability-aware-perturbation-budget-olukorede-fakorede-et-al-2024>(26/54 | 65/271) Improving Adversarial Training using Vulnerability-Aware Perturbation Budget (Olukorede Fakorede et al., 2024)</a></li><li><a href=#2754--66271-guide-guidance-based-incremental-learning-with-diffusion-models-bartosz-cywiński-et-al-2024>(27/54 | 66/271) GUIDE: Guidance-based Incremental Learning with Diffusion Models (Bartosz Cywiński et al., 2024)</a></li><li><a href=#2854--67271-public-data-assisted-private-stochastic-optimization-power-and-limitations-enayat-ullah-et-al-2024>(28/54 | 67/271) Public-data Assisted Private Stochastic Optimization: Power and Limitations (Enayat Ullah et al., 2024)</a></li><li><a href=#2954--68271-feature-selection-as-deep-sequential-generative-learning-wangyang-ying-et-al-2024>(29/54 | 68/271) Feature Selection as Deep Sequential Generative Learning (Wangyang Ying et al., 2024)</a></li><li><a href=#3054--69271-verified-training-for-counterfactual-explanation-robustness-under-data-shift-anna-p-meyer-et-al-2024>(30/54 | 69/271) Verified Training for Counterfactual Explanation Robustness under Data Shift (Anna P. Meyer et al., 2024)</a></li><li><a href=#3154--70271-diffusion-on-language-model-embeddings-for-protein-sequence-generation-viacheslav-meshchaninov-et-al-2024>(31/54 | 70/271) Diffusion on language model embeddings for protein sequence generation (Viacheslav Meshchaninov et al., 2024)</a></li><li><a href=#3254--71271-learning-adversarial-mdps-with-stochastic-hard-constraints-francesco-emanuele-stradi-et-al-2024>(32/54 | 71/271) Learning Adversarial MDPs with Stochastic Hard Constraints (Francesco Emanuele Stradi et al., 2024)</a></li><li><a href=#3354--72271-a-survey-on-applications-of-reinforcement-learning-in-spatial-resource-allocation-di-zhang-et-al-2024>(33/54 | 72/271) A Survey on Applications of Reinforcement Learning in Spatial Resource Allocation (Di Zhang et al., 2024)</a></li><li><a href=#3454--73271-probing-the-robustness-of-time-series-forecasting-models-with-counterfacts-håkon-hanisch-kjærnli-et-al-2024>(34/54 | 73/271) Probing the Robustness of Time-series Forecasting Models with CounterfacTS (Håkon Hanisch Kjærnli et al., 2024)</a></li><li><a href=#3554--74271-advancing-out-of-distribution-detection-through-data-purification-and-dynamic-activation-function-design-yingrui-ji-et-al-2024>(35/54 | 74/271) Advancing Out-of-Distribution Detection through Data Purification and Dynamic Activation Function Design (Yingrui Ji et al., 2024)</a></li><li><a href=#3654--75271-kernel-correlation-dissimilarity-for-multiple-kernel-k-means-clustering-rina-su-et-al-2024>(36/54 | 75/271) Kernel Correlation-Dissimilarity for Multiple Kernel k-Means Clustering (Rina Su et al., 2024)</a></li><li><a href=#3754--76271-sample-size-planning-for-conditional-counterfactual-mean-estimation-with-a-k-armed-randomized-experiment-gabriel-ruiz-2024>(37/54 | 76/271) Sample size planning for conditional counterfactual mean estimation with a K-armed randomized experiment (Gabriel Ruiz, 2024)</a></li><li><a href=#3854--77271-enot-expectile-regularization-for-fast-and-accurate-training-of-neural-optimal-transport-nazar-buzun-et-al-2024>(38/54 | 77/271) ENOT: Expectile Regularization for Fast and Accurate Training of Neural Optimal Transport (Nazar Buzun et al., 2024)</a></li><li><a href=#3954--78271-cdc-a-simple-framework-for-complex-data-clustering-zhao-kang-et-al-2024>(39/54 | 78/271) CDC: A Simple Framework for Complex Data Clustering (Zhao Kang et al., 2024)</a></li><li><a href=#4054--79271-robust-graph-structure-learning-under-heterophily-xuanting-xie-et-al-2024>(40/54 | 79/271) Robust Graph Structure Learning under Heterophily (Xuanting Xie et al., 2024)</a></li><li><a href=#4154--80271-automated-multi-task-learning-for-joint-disease-prediction-on-electronic-health-records-suhan-cui-et-al-2024>(41/54 | 80/271) Automated Multi-Task Learning for Joint Disease Prediction on Electronic Health Records (Suhan Cui et al., 2024)</a></li><li><a href=#4254--81271-directional-smoothness-and-gradient-methods-convergence-and-adaptivity-aaron-mishkin-et-al-2024>(42/54 | 81/271) Directional Smoothness and Gradient Methods: Convergence and Adaptivity (Aaron Mishkin et al., 2024)</a></li><li><a href=#4354--82271-belief-enriched-pessimistic-q-learning-against-adversarial-state-perturbations-xiaolin-sun-et-al-2024>(43/54 | 82/271) Belief-Enriched Pessimistic Q-Learning against Adversarial State Perturbations (Xiaolin Sun et al., 2024)</a></li><li><a href=#4454--83271-ocd-fl-a-novel-communication-efficient-peer-selection-based-decentralized-federated-learning-nizar-masmoudi-et-al-2024>(44/54 | 83/271) OCD-FL: A Novel Communication-Efficient Peer Selection-based Decentralized Federated Learning (Nizar Masmoudi et al., 2024)</a></li><li><a href=#4554--84271-spearexact-gradient-inversion-of-batches-in-federated-learning-dimitar-i-dimitrov-et-al-2024>(45/54 | 84/271) SPEAR:Exact Gradient Inversion of Batches in Federated Learning (Dimitar I. Dimitrov et al., 2024)</a></li><li><a href=#4654--85271-extreme-precipitation-nowcasting-using-transformer-based-generative-models-cristian-meo-et-al-2024>(46/54 | 85/271) Extreme Precipitation Nowcasting using Transformer-based Generative Models (Cristian Meo et al., 2024)</a></li><li><a href=#4754--86271-decoupled-vertical-federated-learning-for-practical-training-on-vertically-partitioned-data-avi-amalanshu-et-al-2024>(47/54 | 86/271) Decoupled Vertical Federated Learning for Practical Training on Vertically Partitioned Data (Avi Amalanshu et al., 2024)</a></li><li><a href=#4854--87271-accelerating-convergence-of-score-based-diffusion-models-provably-gen-li-et-al-2024>(48/54 | 87/271) Accelerating Convergence of Score-Based Diffusion Models, Provably (Gen Li et al., 2024)</a></li><li><a href=#4954--88271-effect-of-ambient-intrinsic-dimension-gap-on-adversarial-vulnerability-rajdeep-haldar-et-al-2024>(49/54 | 88/271) Effect of Ambient-Intrinsic Dimension Gap on Adversarial Vulnerability (Rajdeep Haldar et al., 2024)</a></li><li><a href=#5054--89271-supclust-active-learning-at-the-boundaries-yuta-ono-et-al-2024>(50/54 | 89/271) SUPClust: Active Learning at the Boundaries (Yuta Ono et al., 2024)</a></li><li><a href=#5154--90271-ab-bnn-addbit-operation-only-hardware-friendly-binary-neural-network-ruichen-ma-et-al-2024>(51/54 | 90/271) A&amp;B BNN: Add&amp;Bit-Operation-Only Hardware-Friendly Binary Neural Network (Ruichen Ma et al., 2024)</a></li><li><a href=#5254--91271-restricted-bayesian-neural-network-sourav-ganguly-2024>(52/54 | 91/271) Restricted Bayesian Neural Network (Sourav Ganguly, 2024)</a></li><li><a href=#5354--92271-acceleratedlingam-learning-causal-dags-at-the-speed-of-gpus-victor-akinwande-et-al-2024>(53/54 | 92/271) AcceleratedLiNGAM: Learning Causal DAGs at the speed of GPUs (Victor Akinwande et al., 2024)</a></li><li><a href=#5454--93271-uncertainty-quantification-for-deeponets-with-ensemble-kalman-inversion-andrew-pensoneault-et-al-2024>(54/54 | 93/271) Uncertainty quantification for deeponets with ensemble kalman inversion (Andrew Pensoneault et al., 2024)</a></li></ul></li><li><a href=#csai-18>cs.AI (18)</a><ul><li><a href=#118--94271-emotional-manipulation-through-prompt-engineering-amplifies-disinformation-generation-in-ai-large-language-models-rasita-vinay-et-al-2024>(1/18 | 94/271) Emotional Manipulation Through Prompt Engineering Amplifies Disinformation Generation in AI Large Language Models (Rasita Vinay et al., 2024)</a></li><li><a href=#218--95271-guiding-enumerative-program-synthesis-with-large-language-models-yixuan-li-et-al-2024>(2/18 | 95/271) Guiding Enumerative Program Synthesis with Large Language Models (Yixuan Li et al., 2024)</a></li><li><a href=#318--96271-k-link-knowledge-link-graph-from-llms-for-enhanced-representation-learning-in-multivariate-time-series-data-yucheng-wang-et-al-2024>(3/18 | 96/271) K-Link: Knowledge-Link Graph from LLMs for Enhanced Representation Learning in Multivariate Time-Series Data (Yucheng Wang et al., 2024)</a></li><li><a href=#418--97271-a-privacy-preserving-framework-with-multi-modal-data-for-cross-domain-recommendation-li-wang-et-al-2024>(4/18 | 97/271) A Privacy-Preserving Framework with Multi-Modal Data for Cross-Domain Recommendation (Li Wang et al., 2024)</a></li><li><a href=#518--98271-ircoder-intermediate-representations-make-language-models-robust-multilingual-code-generators-indraneil-paul-et-al-2024>(5/18 | 98/271) IRCoder: Intermediate Representations Make Language Models Robust Multilingual Code Generators (Indraneil Paul et al., 2024)</a></li><li><a href=#618--99271-assessing-the-aesthetic-evaluation-capabilities-of-gpt-4-with-vision-insights-from-group-and-individual-assessments-yoshia-abe-et-al-2024>(6/18 | 99/271) Assessing the Aesthetic Evaluation Capabilities of GPT-4 with Vision: Insights from Group and Individual Assessments (Yoshia Abe et al., 2024)</a></li><li><a href=#718--100271-sheetagent-a-generalist-agent-for-spreadsheet-reasoning-and-manipulation-via-large-language-models-yibin-chen-et-al-2024>(7/18 | 100/271) SheetAgent: A Generalist Agent for Spreadsheet Reasoning and Manipulation via Large Language Models (Yibin Chen et al., 2024)</a></li><li><a href=#818--101271-towards-safe-and-aligned-large-language-models-for-medicine-tessa-han-et-al-2024>(8/18 | 101/271) Towards Safe and Aligned Large Language Models for Medicine (Tessa Han et al., 2024)</a></li><li><a href=#918--102271-an-enkf-lstm-assimilation-algorithm-for-crop-growth-model-siqi-zhou-et-al-2024>(9/18 | 102/271) An EnKF-LSTM Assimilation Algorithm for Crop Growth Model (Siqi Zhou et al., 2024)</a></li><li><a href=#1018--103271-prompt-mining-for-language-based-human-mobility-forecasting-hao-xue-et-al-2024>(10/18 | 103/271) Prompt Mining for Language-based Human Mobility Forecasting (Hao Xue et al., 2024)</a></li><li><a href=#1118--104271-bait-benchmarking-embedding-architectures-for-interactive-theorem-proving-sean-lamont-et-al-2024>(11/18 | 104/271) BAIT: Benchmarking (Embedding) Architectures for Interactive Theorem-Proving (Sean Lamont et al., 2024)</a></li><li><a href=#1218--105271-the-geometric-structure-of-topic-models-johannes-hirth-et-al-2024>(12/18 | 105/271) The Geometric Structure of Topic Models (Johannes Hirth et al., 2024)</a></li><li><a href=#1318--106271-ib-net-initial-branch-network-for-variable-decision-in-boolean-satisfiability-tsz-ho-chan-et-al-2024>(13/18 | 106/271) IB-Net: Initial Branch Network for Variable Decision in Boolean Satisfiability (Tsz Ho Chan et al., 2024)</a></li><li><a href=#1418--107271-understanding-biology-in-the-age-of-artificial-intelligence-elsa-lawrence-et-al-2024>(14/18 | 107/271) Understanding Biology in the Age of Artificial Intelligence (Elsa Lawrence et al., 2024)</a></li><li><a href=#1518--108271-artificial-intelligence-exploring-the-patent-field-lekang-jiang-et-al-2024>(15/18 | 108/271) Artificial Intelligence Exploring the Patent Field (Lekang Jiang et al., 2024)</a></li><li><a href=#1618--109271-learning-guided-automated-reasoning-a-brief-survey-lasse-blaauwbroek-et-al-2024>(16/18 | 109/271) Learning Guided Automated Reasoning: A Brief Survey (Lasse Blaauwbroek et al., 2024)</a></li><li><a href=#1718--110271-rethinking-urban-flood-risk-assessment-by-adapting-health-domain-perspective-zhewei-liu-et-al-2024>(17/18 | 110/271) Rethinking Urban Flood Risk Assessment By Adapting Health Domain Perspective (Zhewei Liu et al., 2024)</a></li><li><a href=#1818--111271-adaptive-discovering-and-merging-for-incremental-novel-class-discovery-guangyao-chen-et-al-2024>(18/18 | 111/271) Adaptive Discovering and Merging for Incremental Novel Class Discovery (Guangyao Chen et al., 2024)</a></li></ul></li><li><a href=#csma-1>cs.MA (1)</a><ul><li><a href=#11--112271-an-ai-enabled-agent-based-model-and-its-application-in-measles-outbreak-simulation-for-new-zealand-sijin-zhang-et-al-2024>(1/1 | 112/271) An AI-enabled Agent-Based Model and Its Application in Measles Outbreak Simulation for New Zealand (Sijin Zhang et al., 2024)</a></li></ul></li><li><a href=#csse-6>cs.SE (6)</a><ul><li><a href=#16--113271-automatic-bi-modal-question-title-generation-for-stack-overflow-with-prompt-learning-shaoyu-yang-et-al-2024>(1/6 | 113/271) Automatic Bi-modal Question Title Generation for Stack Overflow with Prompt Learning (Shaoyu Yang et al., 2024)</a></li><li><a href=#26--114271-quantifying-contamination-in-evaluating-code-generation-capabilities-of-language-models-martin-riddell-et-al-2024>(2/6 | 114/271) Quantifying Contamination in Evaluating Code Generation Capabilities of Language Models (Martin Riddell et al., 2024)</a></li><li><a href=#36--115271-whodunit-classifying-code-as-human-authored-or-gpt-4-generated----a-case-study-on-codechef-problems-oseremen-joy-idialu-et-al-2024>(3/6 | 115/271) Whodunit: Classifying Code as Human Authored or GPT-4 Generated &ndash; A case study on CodeChef problems (Oseremen Joy Idialu et al., 2024)</a></li><li><a href=#46--116271-fuzzing-busybox-leveraging-llm-and-crash-reuse-for-embedded-bug-unearthing-asmita-et-al-2024>(4/6 | 116/271) Fuzzing BusyBox: Leveraging LLM and Crash Reuse for Embedded Bug Unearthing (Asmita et al., 2024)</a></li><li><a href=#56--117271-does-documentation-matter-an-empirical-study-of-practitioners-perspective-on-open-source-software-adoption-aaron-imani-et-al-2024>(5/6 | 117/271) Does Documentation Matter? An Empirical Study of Practitioners&rsquo; Perspective on Open-Source Software Adoption (Aaron Imani et al., 2024)</a></li><li><a href=#66--118271-an-ide-plugin-for-gamified-continuous-integration-philipp-straubinger-et-al-2024>(6/6 | 118/271) An IDE Plugin for Gamified Continuous Integration (Philipp Straubinger et al., 2024)</a></li></ul></li><li><a href=#csne-3>cs.NE (3)</a><ul><li><a href=#13--119271-explaining-genetic-programming-trees-using-large-language-models-paula-maddigan-et-al-2024>(1/3 | 119/271) Explaining Genetic Programming Trees using Large Language Models (Paula Maddigan et al., 2024)</a></li><li><a href=#23--120271-neural-architecture-search-using-particle-swarm-and-ant-colony-optimization-séamus-lankford-et-al-2024>(2/3 | 120/271) Neural Architecture Search using Particle Swarm and Ant Colony Optimization (Séamus Lankford et al., 2024)</a></li><li><a href=#33--121271-sparse-spiking-neural-network-exploiting-heterogeneity-in-timescales-for-pruning-recurrent-snn-biswadeep-chakraborty-et-al-2024>(3/3 | 121/271) Sparse Spiking Neural Network: Exploiting Heterogeneity in Timescales for Pruning Recurrent SNN (Biswadeep Chakraborty et al., 2024)</a></li></ul></li><li><a href=#cscv-43>cs.CV (43)</a><ul><li><a href=#143--122271-are-language-models-puzzle-prodigies-algorithmic-puzzles-unveil-serious-challenges-in-multimodal-reasoning-deepanway-ghosal-et-al-2024>(1/43 | 122/271) Are Language Models Puzzle Prodigies? Algorithmic Puzzles Unveil Serious Challenges in Multimodal Reasoning (Deepanway Ghosal et al., 2024)</a></li><li><a href=#243--123271-multimodal-transformer-for-comics-text-cloze-emanuele-vivoli-et-al-2024>(2/43 | 123/271) Multimodal Transformer for Comics Text-Cloze (Emanuele Vivoli et al., 2024)</a></li><li><a href=#343--124271-molnextr-a-generalized-deep-learning-model-for-molecular-image-recognition-yufan-chen-et-al-2024>(3/43 | 124/271) MolNexTR: A Generalized Deep Learning Model for Molecular Image Recognition (Yufan Chen et al., 2024)</a></li><li><a href=#443--125271-self-supervised-photographic-image-layout-representation-learning-zhaoran-zhao-et-al-2024>(4/43 | 125/271) Self-supervised Photographic Image Layout Representation Learning (Zhaoran Zhao et al., 2024)</a></li><li><a href=#543--126271-meacap-memory-augmented-zero-shot-image-captioning-zequn-zeng-et-al-2024>(5/43 | 126/271) MeaCap: Memory-Augmented Zero-shot Image Captioning (Zequn Zeng et al., 2024)</a></li><li><a href=#643--127271-performance-evaluation-of-semi-supervised-learning-frameworks-for-multi-class-weed-detection-jiajia-li-et-al-2024>(6/43 | 127/271) Performance Evaluation of Semi-supervised Learning Frameworks for Multi-Class Weed Detection (Jiajia Li et al., 2024)</a></li><li><a href=#743--128271-contrastive-learning-of-person-independent-representations-for-facial-action-unit-detection-yong-li-et-al-2024>(7/43 | 128/271) Contrastive Learning of Person-independent Representations for Facial Action Unit Detection (Yong Li et al., 2024)</a></li><li><a href=#843--129271-ecap-extensive-cut-and-paste-augmentation-for-unsupervised-domain-adaptive-semantic-segmentation-erik-brorsson-et-al-2024>(8/43 | 129/271) ECAP: Extensive Cut-and-Paste Augmentation for Unsupervised Domain Adaptive Semantic Segmentation (Erik Brorsson et al., 2024)</a></li><li><a href=#943--130271-cmda-cross-modal-and-domain-adversarial-adaptation-for-lidar-based-3d-object-detection-gyusam-chang-et-al-2024>(9/43 | 130/271) CMDA: Cross-Modal and Domain Adversarial Adaptation for LiDAR-Based 3D Object Detection (Gyusam Chang et al., 2024)</a></li><li><a href=#1043--131271-task-attribute-distance-for-few-shot-learning-theoretical-analysis-and-applications-minyang-hu-et-al-2024>(10/43 | 131/271) Task Attribute Distance for Few-Shot Learning: Theoretical Analysis and Applications (Minyang Hu et al., 2024)</a></li><li><a href=#1143--132271-dlp-gan-learning-to-draw-modern-chinese-landscape-photos-with-generative-adversarial-network-xiangquan-gui-et-al-2024>(11/43 | 132/271) DLP-GAN: learning to draw modern Chinese landscape photos with generative adversarial network (Xiangquan Gui et al., 2024)</a></li><li><a href=#1243--133271-self-and-mixed-supervision-to-improve-training-labels-for-multi-class-medical-image-segmentation-jianfei-liu-et-al-2024>(12/43 | 133/271) Self and Mixed Supervision to Improve Training Labels for Multi-Class Medical Image Segmentation (Jianfei Liu et al., 2024)</a></li><li><a href=#1343--134271-latent-dataset-distillation-with-diffusion-models-brian-b-moser-et-al-2024>(13/43 | 134/271) Latent Dataset Distillation with Diffusion Models (Brian B. Moser et al., 2024)</a></li><li><a href=#1443--135271-popeye-a-unified-visual-language-model-for-multi-source-ship-detection-from-remote-sensing-imagery-wei-zhang-et-al-2024>(14/43 | 135/271) Popeye: A Unified Visual-Language Model for Multi-Source Ship Detection from Remote Sensing Imagery (Wei Zhang et al., 2024)</a></li><li><a href=#1543--136271-multi-grained-cross-modal-alignment-for-learning-open-vocabulary-semantic-segmentation-from-text-supervision-yajie-liu-et-al-2024>(15/43 | 136/271) Multi-Grained Cross-modal Alignment for Learning Open-vocabulary Semantic Segmentation from Text Supervision (Yajie Liu et al., 2024)</a></li><li><a href=#1643--137271-causal-prototype-inspired-contrast-adaptation-for-unsupervised-domain-adaptive-semantic-segmentation-of-high-resolution-remote-sensing-imagery-jingru-zhu-et-al-2024>(16/43 | 137/271) Causal Prototype-inspired Contrast Adaptation for Unsupervised Domain Adaptive Semantic Segmentation of High-resolution Remote Sensing Imagery (Jingru Zhu et al., 2024)</a></li><li><a href=#1743--138271-adversarial-infrared-geometry-using-geometry-to-perform-adversarial-attack-against-infrared-pedestrian-detectors-kalibinuer-tiliwalidi-2024>(17/43 | 138/271) Adversarial Infrared Geometry: Using Geometry to Perform Adversarial Attack against Infrared Pedestrian Detectors (Kalibinuer Tiliwalidi, 2024)</a></li><li><a href=#1843--139271-noisecollage-a-layout-aware-text-to-image-diffusion-model-based-on-noise-cropping-and-merging-takahiro-shirakawa-et-al-2024>(18/43 | 139/271) NoiseCollage: A Layout-Aware Text-to-Image Diffusion Model Based on Noise Cropping and Merging (Takahiro Shirakawa et al., 2024)</a></li><li><a href=#1943--140271-flame-diffuser-grounded-wildfire-image-synthesis-using-mask-guided-diffusion-hao-wang-et-al-2024>(19/43 | 140/271) FLAME Diffuser: Grounded Wildfire Image Synthesis using Mask Guided Diffusion (Hao Wang et al., 2024)</a></li><li><a href=#2043--141271-towards-understanding-cross-and-self-attention-in-stable-diffusion-for-text-guided-image-editing-bingyan-liu-et-al-2024>(20/43 | 141/271) Towards Understanding Cross and Self-Attention in Stable Diffusion for Text-Guided Image Editing (Bingyan Liu et al., 2024)</a></li><li><a href=#2143--142271-lodisc-learning-global-local-discriminative-features-for-self-supervised-fine-grained-visual-recognition-jialu-shi-et-al-2024>(21/43 | 142/271) LoDisc: Learning Global-Local Discriminative Features for Self-Supervised Fine-Grained Visual Recognition (Jialu Shi et al., 2024)</a></li><li><a href=#2243--143271-lead-learning-decomposition-for-source-free-universal-domain-adaptation-sanqing-qu-et-al-2024>(22/43 | 143/271) LEAD: Learning Decomposition for Source-free Universal Domain Adaptation (Sanqing Qu et al., 2024)</a></li><li><a href=#2343--144271-dart-implicit-doppler-tomography-for-radar-novel-view-synthesis-tianshu-huang-et-al-2024>(23/43 | 144/271) DART: Implicit Doppler Tomography for Radar Novel View Synthesis (Tianshu Huang et al., 2024)</a></li><li><a href=#2443--145271-redefining-cystoscopy-with-ai-bladder-cancer-diagnosis-using-an-efficient-hybrid-cnn-transformer-model-meryem-amaouche-et-al-2024>(24/43 | 145/271) Redefining cystoscopy with ai: bladder cancer diagnosis using an efficient hybrid cnn-transformer model (Meryem Amaouche et al., 2024)</a></li><li><a href=#2543--146271-temporal-enhanced-floating-car-observers-jeremias-gerner-et-al-2024>(25/43 | 146/271) Temporal Enhanced Floating Car Observers (Jeremias Gerner et al., 2024)</a></li><li><a href=#2643--147271-unifying-generation-and-compression-ultra-low-bitrate-image-coding-via-multi-stage-transformer-naifu-xue-et-al-2024>(26/43 | 147/271) Unifying Generation and Compression: Ultra-low bitrate Image Coding Via Multi-stage Transformer (Naifu Xue et al., 2024)</a></li><li><a href=#2743--148271-harnessing-meta-learning-for-improving-full-frame-video-stabilization-muhammad-kashif-ali-et-al-2024>(27/43 | 148/271) Harnessing Meta-Learning for Improving Full-Frame Video Stabilization (Muhammad Kashif Ali et al., 2024)</a></li><li><a href=#2843--149271-extend-your-own-correspondences-unsupervised-distant-point-cloud-registration-by-progressive-distance-extension-quan-liu-et-al-2024>(28/43 | 149/271) Extend Your Own Correspondences: Unsupervised Distant Point Cloud Registration by Progressive Distance Extension (Quan Liu et al., 2024)</a></li><li><a href=#2943--150271-dcl-net-dual-contrastive-learning-network-for-semi-supervised-multi-organ-segmentation-lu-wen-et-al-2024>(29/43 | 150/271) Dcl-Net: Dual Contrastive Learning Network for Semi-Supervised Multi-Organ Segmentation (Lu Wen et al., 2024)</a></li><li><a href=#3043--151271-continual-segmentation-with-disentangled-objectness-learning-and-class-recognition-yizheng-gong-et-al-2024>(30/43 | 151/271) Continual Segmentation with Disentangled Objectness Learning and Class Recognition (Yizheng Gong et al., 2024)</a></li><li><a href=#3143--152271-slot-abstractors-toward-scalable-abstract-visual-reasoning-shanka-subhra-mondal-et-al-2024>(31/43 | 152/271) Slot Abstractors: Toward Scalable Abstract Visual Reasoning (Shanka Subhra Mondal et al., 2024)</a></li><li><a href=#3243--153271-a-density-guided-temporal-attention-transformer-for-indiscernible-object-counting-in-underwater-video-cheng-yen-yang-et-al-2024>(32/43 | 153/271) A Density-Guided Temporal Attention Transformer for Indiscernible Object Counting in Underwater Video (Cheng-Yen Yang et al., 2024)</a></li><li><a href=#3343--154271-causality-based-cross-modal-representation-learning-for-vision-and-language-navigation-liuyi-wang-et-al-2024>(33/43 | 154/271) Causality-based Cross-Modal Representation Learning for Vision-and-Language Navigation (Liuyi Wang et al., 2024)</a></li><li><a href=#3443--155271-vasttrack-vast-category-visual-object-tracking-liang-peng-et-al-2024>(34/43 | 155/271) VastTrack: Vast Category Visual Object Tracking (Liang Peng et al., 2024)</a></li><li><a href=#3543--156271-video-relationship-detection-using-mixture-of-experts-ala-shaabana-et-al-2024>(35/43 | 156/271) Video Relationship Detection Using Mixture of Experts (Ala Shaabana et al., 2024)</a></li><li><a href=#3643--157271-investigation-of-the-impact-of-synthetic-training-data-in-the-industrial-application-of-terminal-strip-object-detection-nico-baumgart-et-al-2024>(36/43 | 157/271) Investigation of the Impact of Synthetic Training Data in the Industrial Application of Terminal Strip Object Detection (Nico Baumgart et al., 2024)</a></li><li><a href=#3743--158271-learning-3d-object-centric-representation-through-prediction-john-day-et-al-2024>(37/43 | 158/271) Learning 3D object-centric representation through prediction (John Day et al., 2024)</a></li><li><a href=#3843--159271-portraying-the-need-for-temporal-data-in-flood-detection-via-sentinel-1-xavier-bou-et-al-2024>(38/43 | 159/271) Portraying the Need for Temporal Data in Flood Detection via Sentinel-1 (Xavier Bou et al., 2024)</a></li><li><a href=#3943--160271-multi-task-learning-for-real-time-autonomous-driving-leveraging-task-adaptive-attention-generator-wonhyeok-choi-et-al-2024>(39/43 | 160/271) Multi-task Learning for Real-time Autonomous Driving Leveraging Task-adaptive Attention Generator (Wonhyeok Choi et al., 2024)</a></li><li><a href=#4043--161271-d4c-glove-train-solving-the-rpm-and-bongard-logo-problem-by-distributing-and-circumscribing-concepts-ruizhuo-song-et-al-2024>(40/43 | 161/271) D4C glove-train: solving the RPM and Bongard-logo problem by distributing and Circumscribing concepts (Ruizhuo Song et al., 2024)</a></li><li><a href=#4143--162271-scene-depth-estimation-from-traditional-oriental-landscape-paintings-sungho-kang-et-al-2024>(41/43 | 162/271) Scene Depth Estimation from Traditional Oriental Landscape Paintings (Sungho Kang et al., 2024)</a></li><li><a href=#4243--163271-gsnerf-generalizable-semantic-neural-radiance-fields-with-enhanced-3d-scene-understanding-zi-ting-chou-et-al-2024>(42/43 | 163/271) GSNeRF: Generalizable Semantic Neural Radiance Fields with Enhanced 3D Scene Understanding (Zi-Ting Chou et al., 2024)</a></li><li><a href=#4343--164271-hdrflow-real-time-hdr-video-reconstruction-with-large-motions-gangwei-xu-et-al-2024>(43/43 | 164/271) HDRFlow: Real-Time HDR Video Reconstruction with Large Motions (Gangwei Xu et al., 2024)</a></li></ul></li><li><a href=#csir-7>cs.IR (7)</a><ul><li><a href=#17--165271-intent-aware-recommendation-via-disentangled-graph-contrastive-learning-yuling-wang-et-al-2024>(1/7 | 165/271) Intent-aware Recommendation via Disentangled Graph Contrastive Learning (Yuling Wang et al., 2024)</a></li><li><a href=#27--166271-towards-efficient-and-effective-unlearning-of-large-language-models-for-recommendation-hangyu-wang-et-al-2024>(2/7 | 166/271) Towards Efficient and Effective Unlearning of Large Language Models for Recommendation (Hangyu Wang et al., 2024)</a></li><li><a href=#37--167271-bridging-language-and-items-for-retrieval-and-recommendation-yupeng-hou-et-al-2024>(3/7 | 167/271) Bridging Language and Items for Retrieval and Recommendation (Yupeng Hou et al., 2024)</a></li><li><a href=#47--168271-generative-news-recommendation-shen-gao-et-al-2024>(4/7 | 168/271) Generative News Recommendation (Shen Gao et al., 2024)</a></li><li><a href=#57--169271-personalized-negative-reservoir-for-incremental-learning-in-recommender-systems-antonios-valkanas-et-al-2024>(5/7 | 169/271) Personalized Negative Reservoir for Incremental Learning in Recommender Systems (Antonios Valkanas et al., 2024)</a></li><li><a href=#67--170271-backtracing-retrieving-the-cause-of-the-query-rose-e-wang-et-al-2024>(6/7 | 170/271) Backtracing: Retrieving the Cause of the Query (Rose E. Wang et al., 2024)</a></li><li><a href=#77--171271-mamba4rec-towards-efficient-sequential-recommendation-with-selective-state-space-models-chengkai-liu-et-al-2024>(7/7 | 171/271) Mamba4Rec: Towards Efficient Sequential Recommendation with Selective State Space Models (Chengkai Liu et al., 2024)</a></li></ul></li><li><a href=#csro-15>cs.RO (15)</a><ul><li><a href=#115--172271-on-device-self-supervised-learning-of-visual-perception-tasks-aboard-hardware-limited-nano-quadrotors-elia-cereda-et-al-2024>(1/15 | 172/271) On-device Self-supervised Learning of Visual Perception Tasks aboard Hardware-limited Nano-quadrotors (Elia Cereda et al., 2024)</a></li><li><a href=#215--173271-reconciling-reality-through-simulation-a-real-to-sim-to-real-approach-for-robust-manipulation-marcel-torne-et-al-2024>(2/15 | 173/271) Reconciling Reality through Simulation: A Real-to-Sim-to-Real Approach for Robust Manipulation (Marcel Torne et al., 2024)</a></li><li><a href=#315--174271-bidirectional-progressive-neural-networks-with-episodic-return-progress-for-emergent-task-sequencing-and-robotic-skill-transfer-suzan-ece-ada-et-al-2024>(3/15 | 174/271) Bidirectional Progressive Neural Networks with Episodic Return Progress for Emergent Task Sequencing and Robotic Skill Transfer (Suzan Ece Ada et al., 2024)</a></li><li><a href=#415--175271-3d-diffusion-policy-yanjie-ze-et-al-2024>(4/15 | 175/271) 3D Diffusion Policy (Yanjie Ze et al., 2024)</a></li><li><a href=#515--176271-hierarchical-diffusion-policy-for-kinematics-aware-multi-task-robotic-manipulation-xiao-ma-et-al-2024>(5/15 | 176/271) Hierarchical Diffusion Policy for Kinematics-Aware Multi-Task Robotic Manipulation (Xiao Ma et al., 2024)</a></li><li><a href=#615--177271-dexterous-legged-locomotion-in-confined-3d-spaces-with-reinforcement-learning-zifan-xu-et-al-2024>(6/15 | 177/271) Dexterous Legged Locomotion in Confined 3D Spaces with Reinforcement Learning (Zifan Xu et al., 2024)</a></li><li><a href=#715--178271-efficient-search-and-learning-for-agile-locomotion-on-stepping-stones-adithya-kumar-chinnakkonda-ravi-et-al-2024>(7/15 | 178/271) Efficient Search and Learning for Agile Locomotion on Stepping Stones (Adithya Kumar Chinnakkonda Ravi et al., 2024)</a></li><li><a href=#815--179271-multi-object-tracking-with-camera-lidar-fusion-for-autonomous-driving-riccardo-pieroni-et-al-2024>(8/15 | 179/271) Multi-Object Tracking with Camera-LiDAR Fusion for Autonomous Driving (Riccardo Pieroni et al., 2024)</a></li><li><a href=#915--180271-multimodal-anomaly-detection-based-on-deep-auto-encoder-for-object-slip-perception-of-mobile-manipulation-robots-youngjae-yoo-et-al-2024>(9/15 | 180/271) Multimodal Anomaly Detection based on Deep Auto-Encoder for Object Slip Perception of Mobile Manipulation Robots (Youngjae Yoo et al., 2024)</a></li><li><a href=#1015--181271-confidence-aware-decision-making-and-control-for-tool-selection-ajith-anil-meera-et-al-2024>(10/15 | 181/271) Confidence-Aware Decision-Making and Control for Tool Selection (Ajith Anil Meera et al., 2024)</a></li><li><a href=#1115--182271-robust-mitl-planning-under-uncertain-navigation-times-alexis-linard-et-al-2024>(11/15 | 182/271) Robust MITL planning under uncertain navigation times (Alexis Linard et al., 2024)</a></li><li><a href=#1215--183271-interactive-continual-learning-architecture-for-long-term-personalization-of-home-service-robots-ali-ayub-et-al-2024>(12/15 | 183/271) Interactive Continual Learning Architecture for Long-Term Personalization of Home Service Robots (Ali Ayub et al., 2024)</a></li><li><a href=#1315--184271-feel-the-bite-robot-assisted-inside-mouth-bite-transfer-using-robust-mouth-perception-and-physical-interaction-aware-control-rajat-kumar-jenamani-et-al-2024>(13/15 | 184/271) Feel the Bite: Robot-Assisted Inside-Mouth Bite Transfer using Robust Mouth Perception and Physical Interaction-Aware Control (Rajat Kumar Jenamani et al., 2024)</a></li><li><a href=#1415--185271-deployable-polyhedrons-with-one-dof-radial-transformation-yuanqing-gu-et-al-2024>(14/15 | 185/271) Deployable polyhedrons with one-DOF radial transformation (Yuanqing Gu et al., 2024)</a></li><li><a href=#1515--186271-seamless-virtual-reality-with-integrated-synchronizer-and-synthesizer-for-autonomous-driving-he-li-et-al-2024>(15/15 | 186/271) Seamless Virtual Reality with Integrated Synchronizer and Synthesizer for Autonomous Driving (He Li et al., 2024)</a></li></ul></li><li><a href=#eessiv-7>eess.IV (7)</a><ul><li><a href=#17--187271-multi-modal-deep-learning-chen-yuhua-2024>(1/7 | 187/271) Multi-modal Deep Learning (Chen Yuhua, 2024)</a></li><li><a href=#27--188271-generative-active-learning-with-variational-autoencoder-for-radiology-data-generation-in-veterinary-medicine-in-gyu-lee-et-al-2024>(2/7 | 188/271) Generative Active Learning with Variational Autoencoder for Radiology Data Generation in Veterinary Medicine (In-Gyu Lee et al., 2024)</a></li><li><a href=#37--189271-enhancing-chest-x-ray-datasets-with-privacy-preserving-large-language-models-and-multi-type-annotations-a-data-driven-approach-for-improved-classification-ricardo-bigolin-lanfredi-et-al-2024>(3/7 | 189/271) Enhancing chest X-ray datasets with privacy-preserving large language models and multi-type annotations: a data-driven approach for improved classification (Ricardo Bigolin Lanfredi et al., 2024)</a></li><li><a href=#47--190271-medmamba-vision-mamba-for-medical-image-classification-yubiao-yue-et-al-2024>(4/7 | 190/271) MedMamba: Vision Mamba for Medical Image Classification (Yubiao Yue et al., 2024)</a></li><li><a href=#57--191271-fast-nonlocal-and-neural-a-lightweight-high-quality-solution-to-image-denoising-yu-guo-et-al-2024>(5/7 | 191/271) Fast, nonlocal and neural: a lightweight high quality solution to image denoising (Yu Guo et al., 2024)</a></li><li><a href=#67--192271-joint-multi-task-learning-improves-weakly-supervised-biomarker-prediction-in-computational-pathology-omar-s-m-el-nahhas-et-al-2024>(6/7 | 192/271) Joint multi-task learning improves weakly-supervised biomarker prediction in computational pathology (Omar S. M. El Nahhas et al., 2024)</a></li><li><a href=#77--193271-low-dose-ct-image-reconstruction-by-fine-tuning-a-unet-pretrained-for-gaussian-denoising-for-the-downstream-task-of-image-enhancement-tim-selig-et-al-2024>(7/7 | 193/271) Low-Dose CT Image Reconstruction by Fine-Tuning a UNet Pretrained for Gaussian Denoising for the Downstream Task of Image Enhancement (Tim Selig et al., 2024)</a></li></ul></li><li><a href=#eesssy-8>eess.SY (8)</a><ul><li><a href=#18--194271-cnn-based-end-to-end-adaptive-controller-with-stability-guarantees-myeongseok-ryu-et-al-2024>(1/8 | 194/271) CNN-based End-to-End Adaptive Controller with Stability Guarantees (Myeongseok Ryu et al., 2024)</a></li><li><a href=#28--195271-electrical-load-forecasting-model-using-hybrid-lstm-neural-networks-with-online-correction-nan-lu-et-al-2024>(2/8 | 195/271) Electrical Load Forecasting Model Using Hybrid LSTM Neural Networks with Online Correction (Nan Lu et al., 2024)</a></li><li><a href=#38--196271-linear-and-nonlinear-system-identification-under-ell_1--and-group-lasso-regularization-via-l-bfgs-b-alberto-bemporad-2024>(3/8 | 196/271) Linear and nonlinear system identification under $\ell_1$- and group-Lasso regularization via L-BFGS-B (Alberto Bemporad, 2024)</a></li><li><a href=#48--197271-a-unified-model-for-active-battery-equalization-systems-quan-ouyang-et-al-2024>(4/8 | 197/271) A Unified Model for Active Battery Equalization Systems (Quan Ouyang et al., 2024)</a></li><li><a href=#58--198271-robust-safety-critical-control-for-systems-with-sporadic-measurements-and-dwell-time-constraints-joseph-breeden-et-al-2024>(5/8 | 198/271) Robust Safety-Critical Control for Systems with Sporadic Measurements and Dwell Time Constraints (Joseph Breeden et al., 2024)</a></li><li><a href=#68--199271-a-hybrid-dynamical-system-approach-to-the-impulsive-control-of-spacecraft-rendezvous-extended-version-alexandre-seuret-et-al-2024>(6/8 | 199/271) A hybrid dynamical system approach to the impulsive control of spacecraft rendezvous (extended version) (Alexandre Seuret et al., 2024)</a></li><li><a href=#78--200271-data-based-in-cylinder-pressure-model-with-cyclic-variations-for-combustion-control-a-rcci-engine-application-maarten-vlaswinkel-et-al-2024>(7/8 | 200/271) Data-Based In-Cylinder Pressure Model with Cyclic Variations for Combustion Control: A RCCI Engine Application (Maarten Vlaswinkel et al., 2024)</a></li><li><a href=#88--201271-on-discrete-time-polynomial-dynamical-systems-on-hypergraphs-shaoxuan-cui-et-al-2024>(8/8 | 201/271) On discrete-time polynomial dynamical systems on hypergraphs (Shaoxuan Cui et al., 2024)</a></li></ul></li><li><a href=#cshc-5>cs.HC (5)</a><ul><li><a href=#15--202271-promptcharm-text-to-image-generation-through-multi-modal-prompting-and-refinement-zhijie-wang-et-al-2024>(1/5 | 202/271) PromptCharm: Text-to-Image Generation through Multi-modal Prompting and Refinement (Zhijie Wang et al., 2024)</a></li><li><a href=#25--203271-human-io-towards-a-unified-approach-to-detecting-situational-impairments-xingyu-bruce-liu-et-al-2024>(2/5 | 203/271) Human I/O: Towards a Unified Approach to Detecting Situational Impairments (Xingyu Bruce Liu et al., 2024)</a></li><li><a href=#35--204271-salientime-user-driven-selection-of-salient-time-steps-for-large-scale-geospatial-data-visualization-juntong-chen-et-al-2024>(3/5 | 204/271) SalienTime: User-driven Selection of Salient Time Steps for Large-Scale Geospatial Data Visualization (Juntong Chen et al., 2024)</a></li><li><a href=#45--205271-assisting-international-migrants-with-everyday-information-seeking-from-the-providers-lens-yongle-zhang-et-al-2024>(4/5 | 205/271) Assisting International Migrants with Everyday Information Seeking: From the Providers&rsquo; Lens (Yongle Zhang et al., 2024)</a></li><li><a href=#55--206271-holens-a-visual-analytics-design-for-higher-order-movement-modeling-and-visualization-zezheng-feng-et-al-2024>(5/5 | 206/271) HoLens: A Visual Analytics Design for Higher-order Movement Modeling and Visualization (Zezheng Feng et al., 2024)</a></li></ul></li><li><a href=#cscr-7>cs.CR (7)</a><ul><li><a href=#17--207271-neural-exec-learning-and-learning-from-execution-triggers-for-prompt-injection-attacks-dario-pasquini-et-al-2024>(1/7 | 207/271) Neural Exec: Learning (and Learning from) Execution Triggers for Prompt Injection Attacks (Dario Pasquini et al., 2024)</a></li><li><a href=#27--208271-watermax-breaking-the-llm-watermark-detectability-robustness-quality-trade-off-eva-giboulot-et-al-2024>(2/7 | 208/271) WaterMax: breaking the LLM watermark detectability-robustness-quality trade-off (Eva Giboulot et al., 2024)</a></li><li><a href=#37--209271-ztran-prototyping-zero-trust-security-xapps-for-open-radio-access-network-deployments-aly-s-abdalla-et-al-2024>(3/7 | 209/271) ZTRAN: Prototyping Zero Trust Security xApps for Open Radio Access Network Deployments (Aly S. Abdalla et al., 2024)</a></li><li><a href=#47--210271-do-you-trust-your-model-emerging-malware-threats-in-the-deep-learning-ecosystem-dorjan-hitaj-et-al-2024>(4/7 | 210/271) Do You Trust Your Model? Emerging Malware Threats in the Deep Learning Ecosystem (Dorjan Hitaj et al., 2024)</a></li><li><a href=#57--211271-deepeclipse-how-to-break-white-box-dnn-watermarking-schemes-alessandro-pegoraro-et-al-2024>(5/7 | 211/271) DeepEclipse: How to Break White-Box DNN-Watermarking Schemes (Alessandro Pegoraro et al., 2024)</a></li><li><a href=#67--212271-phenoauth-a-novel-puf-phenotype-based-authentication-protocol-for-iot-devices-hongming-fei-et-al-2024>(6/7 | 212/271) PhenoAuth: A Novel PUF-Phenotype-based Authentication Protocol for IoT Devices (Hongming Fei et al., 2024)</a></li><li><a href=#77--213271-wildest-dreams-reproducible-research-in-privacy-preserving-neural-network-training-tanveer-khan-et-al-2024>(7/7 | 213/271) Wildest Dreams: Reproducible Research in Privacy-preserving Neural Network Training (Tanveer Khan et al., 2024)</a></li></ul></li><li><a href=#csce-3>cs.CE (3)</a><ul><li><a href=#13--214271-unsupervised-incremental-learning-with-dual-concept-drift-detection-for-identifying-anomalous-sequences-jin-li-et-al-2024>(1/3 | 214/271) Unsupervised Incremental Learning with Dual Concept Drift Detection for Identifying Anomalous Sequences (Jin Li et al., 2024)</a></li><li><a href=#23--215271-blockchain-and-carbon-markets-standards-overview-pedro-baiz-2024>(2/3 | 215/271) Blockchain and Carbon Markets: Standards Overview (Pedro Baiz, 2024)</a></li><li><a href=#33--216271-multi-time-step-coupling-of-peridynamics-and-classical-continuum-mechanics-for-dynamic-brittle-fracture-zhong-jiandong-et-al-2024>(3/3 | 216/271) Multi-time-step coupling of peridynamics and classical continuum mechanics for dynamic brittle fracture (Zhong Jiandong et al., 2024)</a></li></ul></li><li><a href=#eesssp-2>eess.SP (2)</a><ul><li><a href=#12--217271-diffusion-based-generative-prior-for-low-complexity-mimo-channel-estimation-benedikt-fesl-et-al-2024>(1/2 | 217/271) Diffusion-based Generative Prior for Low-Complexity MIMO Channel Estimation (Benedikt Fesl et al., 2024)</a></li><li><a href=#22--218271-joint-sparsity-pattern-learning-based-channel-estimation-for-massive-mimo-otfs-systems-kuo-meng-et-al-2024>(2/2 | 218/271) Joint Sparsity Pattern Learning Based Channel Estimation for Massive MIMO-OTFS Systems (Kuo Meng et al., 2024)</a></li></ul></li><li><a href=#q-fincp-1>q-fin.CP (1)</a><ul><li><a href=#11--219271-enhancing-price-prediction-in-cryptocurrency-using-transformer-neural-network-and-technical-indicators-mohammad-ali-labbaf-khaniki-et-al-2024>(1/1 | 219/271) Enhancing Price Prediction in Cryptocurrency Using Transformer Neural Network and Technical Indicators (Mohammad Ali Labbaf Khaniki et al., 2024)</a></li></ul></li><li><a href=#csgt-7>cs.GT (7)</a><ul><li><a href=#17--220271-fair-artificial-currency-incentives-in-repeated-weighted-congestion-games-equity-vs-equality-leonardo-pedroso-et-al-2024>(1/7 | 220/271) Fair Artificial Currency Incentives in Repeated Weighted Congestion Games: Equity vs. Equality (Leonardo Pedroso et al., 2024)</a></li><li><a href=#27--221271-application-of-nash-equilibrium-for-developing-an-optimal-forest-harvesting-strategy-in-toruń-forest-district-jan-kotlarz-2024>(2/7 | 221/271) Application of Nash equilibrium for developing an optimal forest harvesting strategy in Toruń Forest District (Jan Kotlarz, 2024)</a></li><li><a href=#37--222271-adaptive-coordination-promotes-collective-cooperation-in-repeated-social-dilemmas-feipeng-zhang-et-al-2024>(3/7 | 222/271) Adaptive coordination promotes collective cooperation in repeated social dilemmas (Feipeng Zhang et al., 2024)</a></li><li><a href=#47--223271-to-spend-or-to-gain-online-learning-in-repeated-karma-auctions-damien-berriaud-et-al-2024>(4/7 | 223/271) To Spend or to Gain: Online Learning in Repeated Karma Auctions (Damien Berriaud et al., 2024)</a></li><li><a href=#57--224271-empirical-game-theoretic-analysis-a-survey-michael-p-wellman-et-al-2024>(5/7 | 224/271) Empirical Game-Theoretic Analysis: A Survey (Michael P. Wellman et al., 2024)</a></li><li><a href=#67--225271-population-aware-online-mirror-descent-for-mean-field-games-by-deep-reinforcement-learning-zida-wu-et-al-2024>(6/7 | 225/271) Population-aware Online Mirror Descent for Mean-Field Games by Deep Reinforcement Learning (Zida Wu et al., 2024)</a></li><li><a href=#77--226271-to-trust-or-not-to-trust-assignment-mechanisms-with-predictions-in-the-private-graph-model-riccardo-colini-baldeschi-et-al-2024>(7/7 | 226/271) To Trust or Not to Trust: Assignment Mechanisms with Predictions in the Private Graph Model (Riccardo Colini-Baldeschi et al., 2024)</a></li></ul></li><li><a href=#physicssoc-ph-1>physics.soc-ph (1)</a><ul><li><a href=#11--227271-introducing-first-principles-calculations-new-approach-to-group-dynamics-and-bridging-social-phenomena-in-tenp-chain-based-social-dynamics-simulations-yasuko-kawahata-2024>(1/1 | 227/271) Introducing First-Principles Calculations: New Approach to Group Dynamics and Bridging Social Phenomena in TeNP-Chain Based Social Dynamics Simulations (Yasuko Kawahata, 2024)</a></li></ul></li><li><a href=#csdc-2>cs.DC (2)</a><ul><li><a href=#12--228271-portable-heterogeneous-ensemble-workflows-at-scale-using-libensemble-stephen-hudson-et-al-2024>(1/2 | 228/271) Portable, heterogeneous ensemble workflows at scale using libEnsemble (Stephen Hudson et al., 2024)</a></li><li><a href=#22--229271-model-parallelism-on-distributed-infrastructure-a-literature-review-from-theory-to-llm-case-studies-felix-brakel-et-al-2024>(2/2 | 229/271) Model Parallelism on Distributed Infrastructure: A Literature Review from Theory to LLM Case-Studies (Felix Brakel et al., 2024)</a></li></ul></li><li><a href=#cspl-1>cs.PL (1)</a><ul><li><a href=#11--230271-generative-explanations-for-program-synthesizers-amirmohammad-nazari-et-al-2024>(1/1 | 230/271) Generative Explanations for Program Synthesizers (Amirmohammad Nazari et al., 2024)</a></li></ul></li><li><a href=#cscy-1>cs.CY (1)</a><ul><li><a href=#11--231271-human-vs-machine-language-models-and-wargames-max-lamparth-et-al-2024>(1/1 | 231/271) Human vs. Machine: Language Models and Wargames (Max Lamparth et al., 2024)</a></li></ul></li><li><a href=#eessas-1>eess.AS (1)</a><ul><li><a href=#11--232271-comparison-performance-of-spectrogram-and-scalogram-as-input-of-acoustic-recognition-task-dang-thoai-phan-et-al-2024>(1/1 | 232/271) Comparison Performance of Spectrogram and Scalogram as Input of Acoustic Recognition Task (Dang Thoai Phan et al., 2024)</a></li></ul></li><li><a href=#cssd-7>cs.SD (7)</a><ul><li><a href=#17--233271-metamat-01-a-semi-analytic-solution-for-benchmarking-wave-propagation-simulations-of-homogeneous-absorbers-in-1d3d-and-2d-stefan-schoder-et-al-2024>(1/7 | 233/271) METAMAT 01: A semi-analytic Solution for Benchmarking Wave Propagation Simulations of homogeneous Absorbers in 1D/3D and 2D (Stefan Schoder et al., 2024)</a></li><li><a href=#27--234271-radia----radio-advertisement-detection-with-intelligent-analytics-jorge-álvarez-et-al-2024>(2/7 | 234/271) RADIA &ndash; Radio Advertisement Detection with Intelligent Analytics (Jorge Álvarez et al., 2024)</a></li><li><a href=#37--235271-non-verbal-information-in-spontaneous-speech----towards-a-new-framework-of-analysis-tirza-biron-et-al-2024>(3/7 | 235/271) Non-verbal information in spontaneous speech &ndash; towards a new framework of analysis (Tirza Biron et al., 2024)</a></li><li><a href=#47--236271-interactive-melody-generation-system-for-enhancing-the-creativity-of-musicians-so-hirawata-et-al-2024>(4/7 | 236/271) Interactive Melody Generation System for Enhancing the Creativity of Musicians (So Hirawata et al., 2024)</a></li><li><a href=#57--237271-can-audio-reveal-music-performance-difficulty-insights-from-the-piano-syllabus-dataset-pedro-ramoneda-et-al-2024>(5/7 | 237/271) Can Audio Reveal Music Performance Difficulty? Insights from the Piano Syllabus Dataset (Pedro Ramoneda et al., 2024)</a></li><li><a href=#67--238271-multi-level-attention-aggregation-for-language-agnostic-speaker-replication-yejin-jeon-et-al-2024>(6/7 | 238/271) Multi-Level Attention Aggregation for Language-Agnostic Speaker Replication (Yejin Jeon et al., 2024)</a></li><li><a href=#77--239271-crossnet-leveraging-global-cross-band-narrow-band-and-positional-encoding-for-single--and-multi-channel-speaker-separation-vahid-ahmadi-kalkhorani-et-al-2024>(7/7 | 239/271) CrossNet: Leveraging Global, Cross-Band, Narrow-Band, and Positional Encoding for Single- and Multi-Channel Speaker Separation (Vahid Ahmadi Kalkhorani et al., 2024)</a></li></ul></li><li><a href=#mathna-7>math.NA (7)</a><ul><li><a href=#17--240271-robust-radial-basis-function-interpolation-based-on-geodesic-distance-for-the-numerical-coupling-of-multiphysics-problems-michele-bucelli-et-al-2024>(1/7 | 240/271) Robust radial basis function interpolation based on geodesic distance for the numerical coupling of multiphysics problems (Michele Bucelli et al., 2024)</a></li><li><a href=#27--241271-a-component-splitting-implicit-time-integration-for-multicomponent-reacting-flows-simulations-jingchao-zhang-et-al-2024>(2/7 | 241/271) A component-splitting implicit time integration for multicomponent reacting flows simulations (Jingchao Zhang et al., 2024)</a></li><li><a href=#37--242271-electromagnetic-inverse-wave-scattering-in-anisotropic-media-via-reduced-order-modeling-liliana-borcea-et-al-2024>(3/7 | 242/271) Electromagnetic inverse wave scattering in anisotropic media via reduced order modeling (Liliana Borcea et al., 2024)</a></li><li><a href=#47--243271-tgpt-pinn-nonlinear-model-reduction-with-transformed-gpt-pinns-yanlai-chen-et-al-2024>(4/7 | 243/271) TGPT-PINN: Nonlinear model reduction with transformed GPT-PINNs (Yanlai Chen et al., 2024)</a></li><li><a href=#57--244271-helmholtz-preconditioning-for-the-compressible-euler-equations-using-mixed-finite-elements-with-lorenz-staggering-david-lee-et-al-2024>(5/7 | 244/271) Helmholtz preconditioning for the compressible Euler equations using mixed finite elements with Lorenz staggering (David Lee et al., 2024)</a></li><li><a href=#67--245271-black-box-k-to-1-pca-reductions-theory-and-applications-arun-jambulapati-et-al-2024>(6/7 | 245/271) Black-Box $k$-to-$1$-PCA Reductions: Theory and Applications (Arun Jambulapati et al., 2024)</a></li><li><a href=#77--246271-application-of-deep-learning-reduced-order-modeling-for-single-phase-flow-in-faulted-porous-media-enrico-ballini-et-al-2024>(7/7 | 246/271) Application of Deep Learning Reduced-Order Modeling for Single-Phase Flow in Faulted Porous Media (Enrico Ballini et al., 2024)</a></li></ul></li><li><a href=#physicschem-ph-1>physics.chem-ph (1)</a><ul><li><a href=#11--247271-predicting-the-temperature-dependence-of-surfactant-cmcs-using-graph-neural-networks-christoforos-brozos-et-al-2024>(1/1 | 247/271) Predicting the Temperature Dependence of Surfactant CMCs Using Graph Neural Networks (Christoforos Brozos et al., 2024)</a></li></ul></li><li><a href=#statml-3>stat.ML (3)</a><ul><li><a href=#13--248271-conformal-prediction-for-multi-dimensional-time-series-by-ellipsoidal-sets-chen-xu-et-al-2024>(1/3 | 248/271) Conformal prediction for multi-dimensional time series by ellipsoidal sets (Chen Xu et al., 2024)</a></li><li><a href=#23--249271-targeted-variance-reduction-robust-bayesian-optimization-of-black-box-simulators-with-noise-parameters-john-joshua-miller-et-al-2024>(2/3 | 249/271) Targeted Variance Reduction: Robust Bayesian Optimization of Black-Box Simulators with Noise Parameters (John Joshua Miller et al., 2024)</a></li><li><a href=#33--250271-incentivized-learning-in-principal-agent-bandit-games-antoine-scheid-et-al-2024>(3/3 | 250/271) Incentivized Learning in Principal-Agent Bandit Games (Antoine Scheid et al., 2024)</a></li></ul></li><li><a href=#quant-ph-1>quant-ph (1)</a><ul><li><a href=#11--251271-parameterized-quantum-comb-and-simpler-circuits-for-reversing-unknown-qubit-unitary-operations-yin-mo-et-al-2024>(1/1 | 251/271) Parameterized quantum comb and simpler circuits for reversing unknown qubit-unitary operations (Yin Mo et al., 2024)</a></li></ul></li><li><a href=#cssi-2>cs.SI (2)</a><ul><li><a href=#12--252271-causal-disentanglement-for-regulating-social-influence-bias-in-social-recommendation-li-wang-et-al-2024>(1/2 | 252/271) Causal Disentanglement for Regulating Social Influence Bias in Social Recommendation (Li Wang et al., 2024)</a></li><li><a href=#22--253271-quantifying-media-influence-on-covid-19-mask-wearing-beliefs-nicholas-rabb-et-al-2024>(2/2 | 253/271) Quantifying Media Influence on Covid-19 Mask-Wearing Beliefs (Nicholas Rabb et al., 2024)</a></li></ul></li><li><a href=#csar-1>cs.AR (1)</a><ul><li><a href=#11--254271-camasim-a-comprehensive-simulation-framework-for-content-addressable-memory-based-accelerators-mengyuan-li-et-al-2024>(1/1 | 254/271) CAMASim: A Comprehensive Simulation Framework for Content-Addressable Memory based Accelerators (Mengyuan Li et al., 2024)</a></li></ul></li><li><a href=#astro-phep-1>astro-ph.EP (1)</a><ul><li><a href=#11--255271-single-transit-detection-in-kepler-with-machine-learning-and-onboard-spacecraft-diagnostics-matthew-t-hansen-et-al-2024>(1/1 | 255/271) Single Transit Detection In Kepler With Machine Learning And Onboard Spacecraft Diagnostics (Matthew T. Hansen et al., 2024)</a></li></ul></li><li><a href=#csds-5>cs.DS (5)</a><ul><li><a href=#15--256271-largest-common-subgraph-of-two-forests-dieter-rautenbach-et-al-2024>(1/5 | 256/271) Largest common subgraph of two forests (Dieter Rautenbach et al., 2024)</a></li><li><a href=#25--257271-parameterized-algorithms-for-balanced-cluster-edge-modification-problems-jayakrishnan-madathil-et-al-2024>(2/5 | 257/271) Parameterized Algorithms for Balanced Cluster Edge Modification Problems (Jayakrishnan Madathil et al., 2024)</a></li><li><a href=#35--258271-on-htlc-based-protocols-for-multi-party-cross-chain-swaps-emily-clark-et-al-2024>(3/5 | 258/271) On HTLC-Based Protocols for Multi-Party Cross-Chain Swaps (Emily Clark et al., 2024)</a></li><li><a href=#45--259271-graph-visualization-for-blockchain-data-marcell-dietl-et-al-2024>(4/5 | 259/271) Graph Visualization for Blockchain Data (Marcell Dietl et al., 2024)</a></li><li><a href=#55--260271-double-exponential-lower-bound-for-telephone-broadcast-prafullkumar-tale-2024>(5/5 | 260/271) Double Exponential Lower Bound for Telephone Broadcast (Prafullkumar Tale, 2024)</a></li></ul></li><li><a href=#csit-1>cs.IT (1)</a><ul><li><a href=#11--261271-risnet-a-domain-knowledge-driven-neural-network-architecture-for-ris-optimization-with-mutual-coupling-and-partial-csi-bile-peng-et-al-2024>(1/1 | 261/271) RISnet: A Domain-Knowledge Driven Neural Network Architecture for RIS Optimization with Mutual Coupling and Partial CSI (Bile Peng et al., 2024)</a></li></ul></li><li><a href=#q-bioto-1>q-bio.TO (1)</a><ul><li><a href=#11--262271-hitchhikers-guide-to-cancer-associated-lymphoid-aggregates-in-histology-images-manual-and-deep-learning-based-quantification-approaches-karina-silina-et-al-2024>(1/1 | 262/271) Hitchhiker&rsquo;s guide to cancer-associated lymphoid aggregates in histology images: manual and deep learning-based quantification approaches (Karina Silina et al., 2024)</a></li></ul></li><li><a href=#cslo-1>cs.LO (1)</a><ul><li><a href=#11--263271-saturating-sorting-without-sorts-pamina-georgiou-et-al-2024>(1/1 | 263/271) Saturating Sorting without Sorts (Pamina Georgiou et al., 2024)</a></li></ul></li><li><a href=#mathoc-1>math.OC (1)</a><ul><li><a href=#11--264271-data-driven-superstabilizing-control-under-quadratically-bounded-errors-in-variables-noise-jared-miller-et-al-2024>(1/1 | 264/271) Data-Driven Superstabilizing Control under Quadratically-Bounded Errors-in-Variables Noise (Jared Miller et al., 2024)</a></li></ul></li><li><a href=#csni-1>cs.NI (1)</a><ul><li><a href=#11--265271-spectrum-occupancy-detection-supported-by-federated-learning-łukasz-kułacz-2024>(1/1 | 265/271) Spectrum Occupancy Detection Supported by Federated Learning (Łukasz Kułacz, 2024)</a></li></ul></li><li><a href=#mathat-1>math.AT (1)</a><ul><li><a href=#11--266271-computing-representatives-of-persistent-homology-generators-with-a-double-twist-tuyen-pham-et-al-2024>(1/1 | 266/271) Computing Representatives of Persistent Homology Generators with a Double Twist (Tuyen Pham et al., 2024)</a></li></ul></li><li><a href=#csgl-1>cs.GL (1)</a><ul><li><a href=#11--267271-eternal-sunshine-of-the-mechanical-mind-the-irreconcilability-of-machine-learning-and-the-right-to-be-forgotten-meem-arafat-manab-2024>(1/1 | 267/271) Eternal Sunshine of the Mechanical Mind: The Irreconcilability of Machine Learning and the Right to be Forgotten (Meem Arafat Manab, 2024)</a></li></ul></li><li><a href=#csdb-1>cs.DB (1)</a><ul><li><a href=#11--268271-spanning-tree-based-query-plan-enumeration-yesdaulet-izenov-et-al-2024>(1/1 | 268/271) Spanning Tree-based Query Plan Enumeration (Yesdaulet Izenov et al., 2024)</a></li></ul></li><li><a href=#physicsoptics-1>physics.optics (1)</a><ul><li><a href=#11--269271-photonic-electronic-spiking-neuron-with-multi-modal-and-multi-wavelength-excitatory-and-inhibitory-operation-for-high-speed-neuromorphic-sensing-and-computing-weikang-zhang-et-al-2024>(1/1 | 269/271) Photonic-electronic spiking neuron with multi-modal and multi-wavelength excitatory and inhibitory operation for high-speed neuromorphic sensing and computing (Weikang Zhang et al., 2024)</a></li></ul></li><li><a href=#mathco-2>math.CO (2)</a><ul><li><a href=#12--270271-on-the-structure-of-hamiltonian-graphs-with-small-independence-number-nikola-jedličková-et-al-2024>(1/2 | 270/271) On the Structure of Hamiltonian Graphs with Small Independence Number (Nikola Jedličková et al., 2024)</a></li><li><a href=#22--271271-secure-total-domination-number-in-maximal-outerplanar-graphs-yasufumi-aita-et-al-2024>(2/2 | 271/271) Secure Total Domination Number in Maximal Outerplanar Graphs (Yasufumi Aita et al., 2024)</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:contact.to.akitenkrad@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>contact.to.akitenkrad@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/akitenkrad-blog/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2020 Akitenkrad.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/akitenkrad-blog/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/jquery-3.4.1.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/popper.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/bootstrap.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/navbar.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/plyr.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=https://akitenkrad.github.io/akitenkrad-blog/js/single.js></script><script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.css><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.js></script><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!0})})</script></body></html>