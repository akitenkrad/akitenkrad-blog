<!doctype html><html><head><title>arXiv @ 2024.03.16</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/bootstrap.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/main.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/navbar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/plyr.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/flag-icon.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/custom/style.css><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM crossorigin=anonymous><script async src=https://cdn.jsdelivr.net/npm/es-module-shims@1/dist/es-module-shims.min.js crossorigin=anonymous></script><script type=importmap>
{
  "imports": {
    "@popperjs/core": "https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.8/dist/esm/popper.min.js",
    "bootstrap": "https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.esm.min.js"
  }
}
</script><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js integrity=sha384-geWF76RCwLtnZ8qwWowPQNguL3RmwHVBC9FhGdlKrxdiJJigb/j/68SIy3Te4Bkz crossorigin=anonymous></script><script type=module src=https://akitenkrad.github.io/akitenkrad-blog/js/custom/custom.js></script><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/fontawesome/css/all.min.css><link rel=icon type=image/png href=https://akitenkrad.github.io/akitenkrad-blog/images/favicons/favicon-96x96_huf1ee13f0caf27d1547f91fb46207d708_13005_42x0_resize_box_3.png><meta property="og:title" content="arXiv @ 2024.03.16"><meta property="og:description" content="Primary Categories cond-mat.mtrl-sci (1) cs.AI (7) cs.AR (2) cs.CE (2) cs.CL (42) cs.CR (12) cs.CV (101) cs.CY (3) cs.DB (1) cs.DC (2) cs.DM (2) cs.GR (2) cs.GT (1) cs.HC (10) cs.IR (5) cs.IT (6) cs.LG (45) cs.LO (1) cs.NI (2) cs.RO (16) cs.SD (7) cs.SE (8) cs.SI (3) econ.TH (1) eess.AS (1) eess.IV (13) eess.SY (11) math.CO (1) math.LO (2) math.NA (2) math.OC (1) nlin.AO (1) physics.optics (1) physics.space-ph (1) q-bio."><meta property="og:type" content="article"><meta property="og:url" content="https://akitenkrad.github.io/akitenkrad-blog/posts/arxiv/202403/20240316000000/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-03-16T00:00:00+00:00"><meta property="article:modified_time" content="2024-03-16T00:00:00+00:00"><meta name=description content="arXiv @ 2024.03.16"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/single.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/sidebar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/style.css><script type=text/javascript src=//ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js></script><script language=JavaScript>$(document).ready(function(){$("a[href^='http']:not([href*='"+location.hostname+"'])").attr("target","_blank")})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-1MYYZQG0WE"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-1MYYZQG0WE",{anonymize_ip:!1})}</script></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow" style=position:fixed><div class=container><button class="navbar-toggler navbar-light navbar-for-sidebar" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span>
</button>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav mr-auto"><li class=nav-item><a class=navbar-brand href=/akitenkrad-blog><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png alt=Logo>
Akitenkrad's Blog</a></li></ul><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=/akitenkrad-blog#home>Home</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#about>About</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#recent-posts>Recent Posts</a></li><li class=nav-item><a class=nav-link id=blog-link href=https://akitenkrad.github.io/akitenkrad-blog/posts>Posts</a></li><li class=nav-item><a class=nav-link id=tag-link href=https://akitenkrad.github.io/akitenkrad-blog/tags>Tags</a></li></ul></div></div><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/akitenkrad-blog/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/>Papers</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202205/>2022.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202205/20220518224923/ title="A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks">A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220508162318/ title="A Deep Cascade Model for Multi-Document Reading Comprehension">A Deep Cascade Model for Multi-Document Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220506021208/ title="A Primer in BERTology: What We Know About How BERT Works">A Primer in BERTology: What We Know About How BERT Works</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220529131339/ title="Attention Is All You Need">Attention Is All You Need</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220503010000/ title="DeBERTa: Decoding-Enhanced BERT with Disentangled Attention">DeBERTa: Decoding-Enhanced BERT with Disentangled Attention</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220505222900/ title="Dense Passage Retrieval for Open-Domain Question Answering">Dense Passage Retrieval for Open-Domain Question Answering</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220511010217/ title="Multi-Style Generative Reading Comprehension">Multi-Style Generative Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220530102936/ title="Neural Machine Translation of Rare Words with Subword Units">Neural Machine Translation of Rare Words with Subword Units</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220523223206/ title="RoBERTa: A Robustly Optimized BERT Pretraining Approach">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220520124748/ title="Semi-Supervised Classification with Graph Convolutional Networks">Semi-Supervised Classification with Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220509110738/ title="Survey on graph embeddings and their applications to machine learning problems on graphs">Survey on graph embeddings and their applications to machine learning problems on graphs</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220514151839/ title="UnitedQA: A Hybrid Approach for Open Domain Question Answering">UnitedQA: A Hybrid Approach for Open Domain Question Answering</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202206/>2022.06</a><ul><li><a href=/akitenkrad-blog/posts/papers/202206/20220612105422/ title="Attributed Network Embedding for Learning in a Dynamic Environment">Attributed Network Embedding for Learning in a Dynamic Environment</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220608085622/ title="CodeBERT: A Pre-Trained Model for Programming and Natural Languages">CodeBERT: A Pre-Trained Model for Programming and Natural Languages</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220618223844/ title="High-order Proximity Preserved Embedding for Dynamic Networks">High-order Proximity Preserved Embedding for Dynamic Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220602171700/ title="S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension">S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202207/>2022.07</a><ul><li><a href=/akitenkrad-blog/posts/papers/202207/20220727145036/ title="Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions">Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions</a></li><li><a href=/akitenkrad-blog/posts/papers/202207/20220726163444/ title="Dynamic Network Embedding Survey">Dynamic Network Embedding Survey</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/papers/202208/20220802103319/ title="Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN">Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/papers/202209/20220909180827/ title="Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting">Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202305/>2023.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202305/20230513094804/ title="Improving Language Understanding by Generative Pre-Training">Improving Language Understanding by Generative Pre-Training</a></li><li><a href=/akitenkrad-blog/posts/papers/202305/20230514200241/ title="Language Models are Unsupervised Multitask Learners">Language Models are Unsupervised Multitask Learners</a></li></ul></li><li><a href=/akitenkrad-blog/posts/papers/202306/ title=2023.06>2023.06</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/>arXiv</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202307/>2023.07</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230701000000/ title="arXiv @ 2023.07.01">arXiv @ 2023.07.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230702000000/ title="arXiv @ 2023.07.02">arXiv @ 2023.07.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230703000000/ title="arXiv @ 2023.07.03">arXiv @ 2023.07.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230704000000/ title="arXiv @ 2023.07.04">arXiv @ 2023.07.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230705000000/ title="arXiv @ 2023.07.05">arXiv @ 2023.07.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230706000000/ title="arXiv @ 2023.07.06">arXiv @ 2023.07.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230707000000/ title="arXiv @ 2023.07.07">arXiv @ 2023.07.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230708000000/ title="arXiv @ 2023.07.08">arXiv @ 2023.07.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230709000000/ title="arXiv @ 2023.07.09">arXiv @ 2023.07.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230710000000/ title="arXiv @ 2023.07.10">arXiv @ 2023.07.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230711000000/ title="arXiv @ 2023.07.11">arXiv @ 2023.07.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230712000000/ title="arXiv @ 2023.07.12">arXiv @ 2023.07.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230713000000/ title="arXiv @ 2023.07.13">arXiv @ 2023.07.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230714000000/ title="arXiv @ 2023.07.14">arXiv @ 2023.07.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230715000000/ title="arXiv @ 2023.07.15">arXiv @ 2023.07.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230716000000/ title="arXiv @ 2023.07.16">arXiv @ 2023.07.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230717000000/ title="arXiv @ 2023.07.17">arXiv @ 2023.07.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230718000000/ title="arXiv @ 2023.07.18">arXiv @ 2023.07.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230719000000/ title="arXiv @ 2023.07.19">arXiv @ 2023.07.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230720000000/ title="arXiv @ 2023.07.20">arXiv @ 2023.07.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230721000000/ title="arXiv @ 2023.07.21">arXiv @ 2023.07.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230722000000/ title="arXiv @ 2023.07.22">arXiv @ 2023.07.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230723000000/ title="arXiv @ 2023.07.23">arXiv @ 2023.07.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230724000000/ title="arXiv @ 2023.07.24">arXiv @ 2023.07.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230725000000/ title="arXiv @ 2023.07.25">arXiv @ 2023.07.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230726000000/ title="arXiv @ 2023.07.26">arXiv @ 2023.07.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230727000000/ title="arXiv @ 2023.07.27">arXiv @ 2023.07.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230728000000/ title="arXiv @ 2023.07.28">arXiv @ 2023.07.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230729000000/ title="arXiv @ 2023.07.29">arXiv @ 2023.07.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230730000000/ title="arXiv @ 2023.07.30">arXiv @ 2023.07.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230731000000/ title="arXiv @ 2023.07.31">arXiv @ 2023.07.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202308/>2023.08</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230801000000/ title="arXiv @ 2023.08.01">arXiv @ 2023.08.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230802000000/ title="arXiv @ 2023.08.02">arXiv @ 2023.08.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230803000000/ title="arXiv @ 2023.08.03">arXiv @ 2023.08.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230804000000/ title="arXiv @ 2023.08.04">arXiv @ 2023.08.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230805000000/ title="arXiv @ 2023.08.05">arXiv @ 2023.08.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230806000000/ title="arXiv @ 2023.08.06">arXiv @ 2023.08.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230807000000/ title="arXiv @ 2023.08.07">arXiv @ 2023.08.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230808000000/ title="arXiv @ 2023.08.08">arXiv @ 2023.08.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230809000000/ title="arXiv @ 2023.08.09">arXiv @ 2023.08.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230810000000/ title="arXiv @ 2023.08.10">arXiv @ 2023.08.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230811000000/ title="arXiv @ 2023.08.11">arXiv @ 2023.08.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230812000000/ title="arXiv @ 2023.08.12">arXiv @ 2023.08.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230813000000/ title="arXiv @ 2023.08.13">arXiv @ 2023.08.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230814000000/ title="arXiv @ 2023.08.14">arXiv @ 2023.08.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230815000000/ title="arXiv @ 2023.08.15">arXiv @ 2023.08.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230816000000/ title="arXiv @ 2023.08.16">arXiv @ 2023.08.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230817000000/ title="arXiv @ 2023.08.17">arXiv @ 2023.08.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230818000000/ title="arXiv @ 2023.08.18">arXiv @ 2023.08.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230819000000/ title="arXiv @ 2023.08.19">arXiv @ 2023.08.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230820000000/ title="arXiv @ 2023.08.20">arXiv @ 2023.08.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230821000000/ title="arXiv @ 2023.08.21">arXiv @ 2023.08.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230822000000/ title="arXiv @ 2023.08.22">arXiv @ 2023.08.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230823000000/ title="arXiv @ 2023.08.23">arXiv @ 2023.08.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230824000000/ title="arXiv @ 2023.08.24">arXiv @ 2023.08.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230825000000/ title="arXiv @ 2023.08.25">arXiv @ 2023.08.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230826000000/ title="arXiv @ 2023.08.26">arXiv @ 2023.08.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230827000000/ title="arXiv @ 2023.08.27">arXiv @ 2023.08.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230828000000/ title="arXiv @ 2023.08.28">arXiv @ 2023.08.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230829000000/ title="arXiv @ 2023.08.29">arXiv @ 2023.08.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230830000000/ title="arXiv @ 2023.08.30">arXiv @ 2023.08.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230831000000/ title="arXiv @ 2023.08.31">arXiv @ 2023.08.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202309/>2023.09</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230901000000/ title="arXiv @ 2023.09.01">arXiv @ 2023.09.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230902000000/ title="arXiv @ 2023.09.02">arXiv @ 2023.09.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230903000000/ title="arXiv @ 2023.09.03">arXiv @ 2023.09.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230904000000/ title="arXiv @ 2023.09.04">arXiv @ 2023.09.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230905000000/ title="arXiv @ 2023.09.05">arXiv @ 2023.09.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230906000000/ title="arXiv @ 2023.09.06">arXiv @ 2023.09.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230907000000/ title="arXiv @ 2023.09.07">arXiv @ 2023.09.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230908000000/ title="arXiv @ 2023.09.08">arXiv @ 2023.09.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230909000000/ title="arXiv @ 2023.09.09">arXiv @ 2023.09.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230910000000/ title="arXiv @ 2023.09.10">arXiv @ 2023.09.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230911000000/ title="arXiv @ 2023.09.11">arXiv @ 2023.09.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230912000000/ title="arXiv @ 2023.09.12">arXiv @ 2023.09.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230913000000/ title="arXiv @ 2023.09.13">arXiv @ 2023.09.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230914000000/ title="arXiv @ 2023.09.14">arXiv @ 2023.09.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230915000000/ title="arXiv @ 2023.09.15">arXiv @ 2023.09.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230916000000/ title="arXiv @ 2023.09.16">arXiv @ 2023.09.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230917000000/ title="arXiv @ 2023.09.17">arXiv @ 2023.09.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230918000000/ title="arXiv @ 2023.09.18">arXiv @ 2023.09.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230919000000/ title="arXiv @ 2023.09.19">arXiv @ 2023.09.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230920000000/ title="arXiv @ 2023.09.20">arXiv @ 2023.09.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230921000000/ title="arXiv @ 2023.09.21">arXiv @ 2023.09.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230922000000/ title="arXiv @ 2023.09.22">arXiv @ 2023.09.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230923000000/ title="arXiv @ 2023.09.23">arXiv @ 2023.09.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230924000000/ title="arXiv @ 2023.09.24">arXiv @ 2023.09.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230925000000/ title="arXiv @ 2023.09.25">arXiv @ 2023.09.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230926000000/ title="arXiv @ 2023.09.26">arXiv @ 2023.09.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230927000000/ title="arXiv @ 2023.09.27">arXiv @ 2023.09.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230928000000/ title="arXiv @ 2023.09.28">arXiv @ 2023.09.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230929000000/ title="arXiv @ 2023.09.29">arXiv @ 2023.09.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230930000000/ title="arXiv @ 2023.09.30">arXiv @ 2023.09.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202310/>2023.1</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231001000000/ title="arXiv @ 2023.10.01">arXiv @ 2023.10.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231002000000/ title="arXiv @ 2023.10.02">arXiv @ 2023.10.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231003000000/ title="arXiv @ 2023.10.03">arXiv @ 2023.10.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231004000000/ title="arXiv @ 2023.10.04">arXiv @ 2023.10.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231005000000/ title="arXiv @ 2023.10.05">arXiv @ 2023.10.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231006000000/ title="arXiv @ 2023.10.06">arXiv @ 2023.10.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231007000000/ title="arXiv @ 2023.10.07">arXiv @ 2023.10.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231008000000/ title="arXiv @ 2023.10.08">arXiv @ 2023.10.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231009000000/ title="arXiv @ 2023.10.09">arXiv @ 2023.10.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231010000000/ title="arXiv @ 2023.10.10">arXiv @ 2023.10.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231011000000/ title="arXiv @ 2023.10.11">arXiv @ 2023.10.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231012000000/ title="arXiv @ 2023.10.12">arXiv @ 2023.10.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231013000000/ title="arXiv @ 2023.10.13">arXiv @ 2023.10.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231014000000/ title="arXiv @ 2023.10.14">arXiv @ 2023.10.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231015000000/ title="arXiv @ 2023.10.15">arXiv @ 2023.10.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231016000000/ title="arXiv @ 2023.10.16">arXiv @ 2023.10.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231017000000/ title="arXiv @ 2023.10.17">arXiv @ 2023.10.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231018000000/ title="arXiv @ 2023.10.18">arXiv @ 2023.10.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231019000000/ title="arXiv @ 2023.10.19">arXiv @ 2023.10.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231020000000/ title="arXiv @ 2023.10.20">arXiv @ 2023.10.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231021000000/ title="arXiv @ 2023.10.21">arXiv @ 2023.10.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231022000000/ title="arXiv @ 2023.10.22">arXiv @ 2023.10.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231023000000/ title="arXiv @ 2023.10.23">arXiv @ 2023.10.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231024000000/ title="arXiv @ 2023.10.24">arXiv @ 2023.10.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231025000000/ title="arXiv @ 2023.10.25">arXiv @ 2023.10.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231026000000/ title="arXiv @ 2023.10.26">arXiv @ 2023.10.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231027000000/ title="arXiv @ 2023.10.27">arXiv @ 2023.10.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231028000000/ title="arXiv @ 2023.10.28">arXiv @ 2023.10.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231029000000/ title="arXiv @ 2023.10.29">arXiv @ 2023.10.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231030000000/ title="arXiv @ 2023.10.30">arXiv @ 2023.10.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231031000000/ title="arXiv @ 2023.10.31">arXiv @ 2023.10.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202311/>2023.11</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231101000000/ title="arXiv @ 2023.11.01">arXiv @ 2023.11.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231102000000/ title="arXiv @ 2023.11.02">arXiv @ 2023.11.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231103000000/ title="arXiv @ 2023.11.03">arXiv @ 2023.11.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231104000000/ title="arXiv @ 2023.11.04">arXiv @ 2023.11.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231105000000/ title="arXiv @ 2023.11.05">arXiv @ 2023.11.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231106000000/ title="arXiv @ 2023.11.06">arXiv @ 2023.11.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231107000000/ title="arXiv @ 2023.11.07">arXiv @ 2023.11.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231108000000/ title="arXiv @ 2023.11.08">arXiv @ 2023.11.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231109000000/ title="arXiv @ 2023.11.09">arXiv @ 2023.11.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231110000000/ title="arXiv @ 2023.11.10">arXiv @ 2023.11.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231111000000/ title="arXiv @ 2023.11.11">arXiv @ 2023.11.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231112000000/ title="arXiv @ 2023.11.12">arXiv @ 2023.11.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231113000000/ title="arXiv @ 2023.11.13">arXiv @ 2023.11.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231114000000/ title="arXiv @ 2023.11.14">arXiv @ 2023.11.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231115000000/ title="arXiv @ 2023.11.15">arXiv @ 2023.11.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231116000000/ title="arXiv @ 2023.11.16">arXiv @ 2023.11.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231117000000/ title="arXiv @ 2023.11.17">arXiv @ 2023.11.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231118000000/ title="arXiv @ 2023.11.18">arXiv @ 2023.11.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231119000000/ title="arXiv @ 2023.11.19">arXiv @ 2023.11.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231120000000/ title="arXiv @ 2023.11.20">arXiv @ 2023.11.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231121000000/ title="arXiv @ 2023.11.21">arXiv @ 2023.11.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231122000000/ title="arXiv @ 2023.11.22">arXiv @ 2023.11.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231123000000/ title="arXiv @ 2023.11.23">arXiv @ 2023.11.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231124000000/ title="arXiv @ 2023.11.24">arXiv @ 2023.11.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231125000000/ title="arXiv @ 2023.11.25">arXiv @ 2023.11.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231126000000/ title="arXiv @ 2023.11.26">arXiv @ 2023.11.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231127000000/ title="arXiv @ 2023.11.27">arXiv @ 2023.11.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231128000000/ title="arXiv @ 2023.11.28">arXiv @ 2023.11.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231129000000/ title="arXiv @ 2023.11.29">arXiv @ 2023.11.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231130000000/ title="arXiv @ 2023.11.30">arXiv @ 2023.11.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202312/>2023.12</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231201000000/ title="arXiv @ 2023.12.01">arXiv @ 2023.12.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231202000000/ title="arXiv @ 2023.12.02">arXiv @ 2023.12.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231203000000/ title="arXiv @ 2023.12.03">arXiv @ 2023.12.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231204000000/ title="arXiv @ 2023.12.04">arXiv @ 2023.12.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231205000000/ title="arXiv @ 2023.12.05">arXiv @ 2023.12.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231206000000/ title="arXiv @ 2023.12.06">arXiv @ 2023.12.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231207000000/ title="arXiv @ 2023.12.07">arXiv @ 2023.12.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231208000000/ title="arXiv @ 2023.12.08">arXiv @ 2023.12.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231209000000/ title="arXiv @ 2023.12.09">arXiv @ 2023.12.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231210000000/ title="arXiv @ 2023.12.10">arXiv @ 2023.12.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231211000000/ title="arXiv @ 2023.12.11">arXiv @ 2023.12.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231212000000/ title="arXiv @ 2023.12.12">arXiv @ 2023.12.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231213000000/ title="arXiv @ 2023.12.13">arXiv @ 2023.12.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231214000000/ title="arXiv @ 2023.12.14">arXiv @ 2023.12.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231215000000/ title="arXiv @ 2023.12.15">arXiv @ 2023.12.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231216000000/ title="arXiv @ 2023.12.16">arXiv @ 2023.12.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231217000000/ title="arXiv @ 2023.12.17">arXiv @ 2023.12.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231218000000/ title="arXiv @ 2023.12.18">arXiv @ 2023.12.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231219000000/ title="arXiv @ 2023.12.19">arXiv @ 2023.12.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231220000000/ title="arXiv @ 2023.12.20">arXiv @ 2023.12.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231221000000/ title="arXiv @ 2023.12.21">arXiv @ 2023.12.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231222000000/ title="arXiv @ 2023.12.22">arXiv @ 2023.12.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231223000000/ title="arXiv @ 2023.12.23">arXiv @ 2023.12.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231224000000/ title="arXiv @ 2023.12.24">arXiv @ 2023.12.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231225000000/ title="arXiv @ 2023.12.25">arXiv @ 2023.12.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231226000000/ title="arXiv @ 2023.12.26">arXiv @ 2023.12.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231227000000/ title="arXiv @ 2023.12.27">arXiv @ 2023.12.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231228000000/ title="arXiv @ 2023.12.28">arXiv @ 2023.12.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231229000000/ title="arXiv @ 2023.12.29">arXiv @ 2023.12.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231230000000/ title="arXiv @ 2023.12.30">arXiv @ 2023.12.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231231000000/ title="arXiv @ 2023.12.31">arXiv @ 2023.12.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202401/>2024.01</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240101000000/ title="arXiv @ 2024.01.01">arXiv @ 2024.01.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240102000000/ title="arXiv @ 2024.01.02">arXiv @ 2024.01.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240103000000/ title="arXiv @ 2024.01.03">arXiv @ 2024.01.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240104000000/ title="arXiv @ 2024.01.04">arXiv @ 2024.01.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240105000000/ title="arXiv @ 2024.01.05">arXiv @ 2024.01.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240106000000/ title="arXiv @ 2024.01.06">arXiv @ 2024.01.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240107000000/ title="arXiv @ 2024.01.07">arXiv @ 2024.01.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240108000000/ title="arXiv @ 2024.01.08">arXiv @ 2024.01.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240109000000/ title="arXiv @ 2024.01.09">arXiv @ 2024.01.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240110000000/ title="arXiv @ 2024.01.10">arXiv @ 2024.01.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240111000000/ title="arXiv @ 2024.01.11">arXiv @ 2024.01.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240112000000/ title="arXiv @ 2024.01.12">arXiv @ 2024.01.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240113000000/ title="arXiv @ 2024.01.13">arXiv @ 2024.01.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240114000000/ title="arXiv @ 2024.01.14">arXiv @ 2024.01.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240115000000/ title="arXiv @ 2024.01.15">arXiv @ 2024.01.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240116000000/ title="arXiv @ 2024.01.16">arXiv @ 2024.01.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240117000000/ title="arXiv @ 2024.01.17">arXiv @ 2024.01.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240118000000/ title="arXiv @ 2024.01.18">arXiv @ 2024.01.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240119000000/ title="arXiv @ 2024.01.19">arXiv @ 2024.01.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240120000000/ title="arXiv @ 2024.01.20">arXiv @ 2024.01.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240121000000/ title="arXiv @ 2024.01.21">arXiv @ 2024.01.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240122000000/ title="arXiv @ 2024.01.22">arXiv @ 2024.01.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240123000000/ title="arXiv @ 2024.01.23">arXiv @ 2024.01.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240124000000/ title="arXiv @ 2024.01.24">arXiv @ 2024.01.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240125000000/ title="arXiv @ 2024.01.25">arXiv @ 2024.01.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240126000000/ title="arXiv @ 2024.01.26">arXiv @ 2024.01.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240127000000/ title="arXiv @ 2024.01.27">arXiv @ 2024.01.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240128000000/ title="arXiv @ 2024.01.28">arXiv @ 2024.01.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240129000000/ title="arXiv @ 2024.01.29">arXiv @ 2024.01.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240130000000/ title="arXiv @ 2024.01.30">arXiv @ 2024.01.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240131000000/ title="arXiv @ 2024.01.31">arXiv @ 2024.01.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202402/>2024.02</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240201000000/ title="arXiv @ 2024.02.01">arXiv @ 2024.02.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240202000000/ title="arXiv @ 2024.02.02">arXiv @ 2024.02.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240203000000/ title="arXiv @ 2024.02.03">arXiv @ 2024.02.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240204000000/ title="arXiv @ 2024.02.04">arXiv @ 2024.02.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240205000000/ title="arXiv @ 2024.02.05">arXiv @ 2024.02.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240206000000/ title="arXiv @ 2024.02.06">arXiv @ 2024.02.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/ title="arXiv @ 2024.02.07">arXiv @ 2024.02.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/ title="arXiv @ 2024.02.08">arXiv @ 2024.02.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/ title="arXiv @ 2024.02.09">arXiv @ 2024.02.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/ title="arXiv @ 2024.02.10">arXiv @ 2024.02.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240211000000/ title="arXiv @ 2024.02.11">arXiv @ 2024.02.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/ title="arXiv @ 2024.02.12">arXiv @ 2024.02.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/ title="arXiv @ 2024.02.13">arXiv @ 2024.02.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/ title="arXiv @ 2024.02.14">arXiv @ 2024.02.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/ title="arXiv @ 2024.02.15">arXiv @ 2024.02.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240216000000/ title="arXiv @ 2024.02.16">arXiv @ 2024.02.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/ title="arXiv @ 2024.02.17">arXiv @ 2024.02.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240218000000/ title="arXiv @ 2024.02.18">arXiv @ 2024.02.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/ title="arXiv @ 2024.02.19">arXiv @ 2024.02.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/ title="arXiv @ 2024.02.20">arXiv @ 2024.02.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/ title="arXiv @ 2024.02.21">arXiv @ 2024.02.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/ title="arXiv @ 2024.02.22">arXiv @ 2024.02.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/ title="arXiv @ 2024.02.23">arXiv @ 2024.02.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/ title="arXiv @ 2024.02.24">arXiv @ 2024.02.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/ title="arXiv @ 2024.02.25">arXiv @ 2024.02.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/ title="arXiv @ 2024.02.26">arXiv @ 2024.02.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/ title="arXiv @ 2024.02.27">arXiv @ 2024.02.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/ title="arXiv @ 2024.02.28">arXiv @ 2024.02.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/ title="arXiv @ 2024.02.29">arXiv @ 2024.02.29</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/202403/>2024.03</a><ul class=active><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/ title="arXiv @ 2024.03.01">arXiv @ 2024.03.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/ title="arXiv @ 2024.03.02">arXiv @ 2024.03.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/ title="arXiv @ 2024.03.03">arXiv @ 2024.03.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240304000000/ title="arXiv @ 2024.03.04">arXiv @ 2024.03.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/ title="arXiv @ 2024.03.05">arXiv @ 2024.03.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/ title="arXiv @ 2024.03.06">arXiv @ 2024.03.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/ title="arXiv @ 2024.03.07">arXiv @ 2024.03.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/ title="arXiv @ 2024.03.08">arXiv @ 2024.03.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/ title="arXiv @ 2024.03.09">arXiv @ 2024.03.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/ title="arXiv @ 2024.03.10">arXiv @ 2024.03.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/ title="arXiv @ 2024.03.11">arXiv @ 2024.03.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/ title="arXiv @ 2024.03.12">arXiv @ 2024.03.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/ title="arXiv @ 2024.03.13">arXiv @ 2024.03.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/ title="arXiv @ 2024.03.14">arXiv @ 2024.03.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/ title="arXiv @ 2024.03.15">arXiv @ 2024.03.15</a></li><li><a class=active href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/ title="arXiv @ 2024.03.16">arXiv @ 2024.03.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/ title="arXiv @ 2024.03.17">arXiv @ 2024.03.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/ title="arXiv @ 2024.03.18">arXiv @ 2024.03.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240319000000/ title="arXiv @ 2024.03.19">arXiv @ 2024.03.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/ title="arXiv @ 2024.03.20">arXiv @ 2024.03.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/ title="arXiv @ 2024.03.21">arXiv @ 2024.03.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/ title="arXiv @ 2024.03.22">arXiv @ 2024.03.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240323000000/ title="arXiv @ 2024.03.23">arXiv @ 2024.03.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/ title="arXiv @ 2024.03.24">arXiv @ 2024.03.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240325000000/ title="arXiv @ 2024.03.25">arXiv @ 2024.03.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240326000000/ title="arXiv @ 2024.03.26">arXiv @ 2024.03.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240327000000/ title="arXiv @ 2024.03.27">arXiv @ 2024.03.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240328000000/ title="arXiv @ 2024.03.28">arXiv @ 2024.03.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240329000000/ title="arXiv @ 2024.03.29">arXiv @ 2024.03.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240330000000/ title="arXiv @ 2024.03.30">arXiv @ 2024.03.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240331000000/ title="arXiv @ 2024.03.31">arXiv @ 2024.03.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202404/>2024.04</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240401000000/ title="arXiv @ 2024.04.01">arXiv @ 2024.04.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240402000000/ title="arXiv @ 2024.04.02">arXiv @ 2024.04.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240403000000/ title="arXiv @ 2024.04.03">arXiv @ 2024.04.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240404000000/ title="arXiv @ 2024.04.04">arXiv @ 2024.04.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240405000000/ title="arXiv @ 2024.04.05">arXiv @ 2024.04.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240406000000/ title="arXiv @ 2024.04.06">arXiv @ 2024.04.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240407000000/ title="arXiv @ 2024.04.07">arXiv @ 2024.04.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240408000000/ title="arXiv @ 2024.04.08">arXiv @ 2024.04.08</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/algorithms/>Algorithms</a><ul><li><a href=/akitenkrad-blog/posts/algorithms/bandit-algorithm-basic/ title="Bandit Algorithm Basic">Bandit Algorithm Basic</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/conference/>Conference</a><ul><li><a href=/akitenkrad-blog/posts/conference/acl/ title="Annual Meeting of the Association for Computational Linguistics">Annual Meeting of the Association for Computational Linguistics</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/>Figures</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/figures/202208/20220822092748/ title=1ヶ月当たりの実労働時間の推移>1ヶ月当たりの実労働時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220813115200/ title=コンビニエンスストアの店舗数の推移>コンビニエンスストアの店舗数の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220821113322/ title=地域別1世帯当たり1ヶ月間の支出額の推移>地域別1世帯当たり1ヶ月間の支出額の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220828111836/ title="夫婦別子供の有無別生活時間 (2016)">夫婦別子供の有無別生活時間 (2016)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220827103615/ title=家族類型の変遷>家族類型の変遷</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220815131016/ title="年齢別大学院入学者数 (2021)">年齢別大学院入学者数 (2021)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220818122628/ title=情報サービス業における企業特殊的人的資本（名目）の推移>情報サービス業における企業特殊的人的資本（名目）の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220824095127/ title=有業者の社会生活における活動の変化>有業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220814175022/ title=東京23区の乗用車保有台数>東京23区の乗用車保有台数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220817121117/ title="業種別企業特殊的人的資本 (2018)">業種別企業特殊的人的資本 (2018)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220825094111/ title=無業者の社会生活における活動の変化>無業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220823104100/ title=睡眠時間の推移>睡眠時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220826102652/ title=総人口の推移>総人口の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220820225528/ title=都道府県別国公立別学校数・学生数>都道府県別国公立別学校数・学生数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220816120845/ title="都道府県別消費者物価指数/教育（全国平均=100）">都道府県別消費者物価指数/教育（全国平均=100）</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220812173814/ title=金沢市の夏の月平均気温の遷移>金沢市の夏の月平均気温の遷移</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/figures/202209/20220907122906/ title="ACL-2022 WordCloud">ACL-2022 WordCloud</a></li></ul></li></ul></li><li><a href=/akitenkrad-blog/posts/latex/ title="Latex Mathematics Syntax Guide">Latex Mathematics Syntax Guide</a></li><li><a href=/akitenkrad-blog/posts/markdown/ title="Markdown Sample">Markdown Sample</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/akitenkrad-blog/posts/arxiv/202403/20240316000000/hero.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_120x120_fit_box_3.png alt="Author Image"><h5 class=author-name></h5><p>Saturday, Mar 16, 2024</p></div><div class=title><h1>arXiv @ 2024.03.16</h1></div><div class=taxonomy-terms><ul><li class=rounded><a href=/akitenkrad-blog/tags/arxiv class="btn, btn-sm">arXiv</a></li><li class=rounded><a href=/akitenkrad-blog/tags/published2024 class="btn, btn-sm">2024</a></li></ul></div><div class=post-content id=post-content><figure style=border:none;width:100%;display:flex;justify-content:center><iframe src=pie.html width=900 height=620 style=border:none></iframe></figure><h2 id=primary-categories>Primary Categories</h2><ul><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/#cond-matmtrl-sci-1>cond-mat.mtrl-sci (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/#csai-7>cs.AI (7)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/#csar-2>cs.AR (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/#csce-2>cs.CE (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/#cscl-42>cs.CL (42)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/#cscr-12>cs.CR (12)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/#cscv-101>cs.CV (101)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/#cscy-3>cs.CY (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/#csdb-1>cs.DB (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/#csdc-2>cs.DC (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/#csdm-2>cs.DM (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/#csgr-2>cs.GR (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/#csgt-1>cs.GT (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/#cshc-10>cs.HC (10)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/#csir-5>cs.IR (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/#csit-6>cs.IT (6)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/#cslg-45>cs.LG (45)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/#cslo-1>cs.LO (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/#csni-2>cs.NI (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/#csro-16>cs.RO (16)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/#cssd-7>cs.SD (7)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/#csse-8>cs.SE (8)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/#cssi-3>cs.SI (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/#econth-1>econ.TH (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/#eessas-1>eess.AS (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/#eessiv-13>eess.IV (13)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/#eesssy-11>eess.SY (11)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/#mathco-1>math.CO (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/#mathlo-2>math.LO (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/#mathna-2>math.NA (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/#mathoc-1>math.OC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/#nlinao-1>nlin.AO (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/#physicsoptics-1>physics.optics (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/#physicsspace-ph-1>physics.space-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/#q-biobm-1>q-bio.BM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/#quant-ph-1>quant-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/#statml-2>stat.ML (2)</a></li></ul><h2 id=keywords>Keywords</h2><table border=1 class=dataframe><thead><tr style=text-align:right><th>keyword</th><th>cs.CL</th><th>cs.CR</th><th>cs.CV</th><th>cs.HC</th><th>cs.LG</th><th>cs.RO</th><th>eess.IV</th><th>eess.SY</th></tr></thead><tbody><tr><td>Active Learning</td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Adversarial Attack</td><td></td><td>2</td><td>1</td><td></td><td>3</td><td></td><td></td><td></td></tr><tr><td>Adversarial Learning</td><td></td><td></td><td>1</td><td></td><td>4</td><td></td><td></td><td></td></tr><tr><td>Anomaly Detection</td><td></td><td>3</td><td>1</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Autoencoder</td><td></td><td></td><td>1</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Automatic Speech Recognition</td><td></td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>BERT</td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Benchmarking</td><td>8</td><td></td><td>25</td><td></td><td>11</td><td>4</td><td>1</td><td>1</td></tr><tr><td>Black Box</td><td></td><td>2</td><td>1</td><td></td><td>2</td><td>1</td><td></td><td></td></tr><tr><td>Chain-of-thought Prompt</td><td></td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>ChatGPT</td><td>1</td><td>1</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Chatbot</td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td><td>1</td></tr><tr><td>Clustering</td><td></td><td></td><td></td><td></td><td>2</td><td></td><td></td><td>1</td></tr><tr><td>Cohere</td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Continual Learning</td><td></td><td></td><td>1</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Continuous Time</td><td></td><td></td><td>2</td><td></td><td></td><td></td><td></td><td>2</td></tr><tr><td>Contrastive Learning</td><td>1</td><td></td><td>6</td><td></td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>ControlNet</td><td></td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Convolution</td><td></td><td>1</td><td>7</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Convolutional Neural Network</td><td></td><td>1</td><td>13</td><td></td><td>3</td><td></td><td>2</td><td></td></tr><tr><td>Counter-factual</td><td></td><td></td><td>2</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Curriculum Learning</td><td></td><td></td><td></td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Data Augmentation</td><td>1</td><td></td><td>4</td><td></td><td>1</td><td></td><td>1</td><td></td></tr><tr><td>Differential Privacy</td><td></td><td>1</td><td>1</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Diffusion Model</td><td></td><td></td><td>16</td><td></td><td>2</td><td>1</td><td>2</td><td></td></tr><tr><td>Discrete Time</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>2</td></tr><tr><td>Distribution Shift</td><td></td><td></td><td>6</td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><td>Domain Adaptation</td><td>2</td><td></td><td>3</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Emotion Recognition</td><td></td><td></td><td></td><td>1</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Event Detection</td><td></td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Face Recognition</td><td></td><td></td><td>3</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Fact Verification</td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Fairness</td><td>2</td><td></td><td>1</td><td></td><td></td><td></td><td></td><td>2</td></tr><tr><td>Fake News Detection</td><td>5</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Federated Learning</td><td></td><td>1</td><td>1</td><td></td><td>4</td><td></td><td>1</td><td>2</td></tr><tr><td>Few-shot</td><td>3</td><td></td><td>1</td><td></td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>Few-shot Learning</td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Fine-tuning</td><td>18</td><td>5</td><td>15</td><td>1</td><td>6</td><td>1</td><td>1</td><td>1</td></tr><tr><td>Foundation Model</td><td></td><td>1</td><td>12</td><td></td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>GPT</td><td>9</td><td>1</td><td>3</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>GPT-2</td><td></td><td>1</td><td>1</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>GPT-3</td><td>4</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>GPT-3.5</td><td>4</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>GPT-4</td><td>5</td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Gaussian Process</td><td></td><td></td><td></td><td></td><td>1</td><td>1</td><td>1</td><td></td></tr><tr><td>Gemini</td><td></td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Generative AI</td><td></td><td></td><td>1</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Generative Adversarial Network</td><td></td><td></td><td>3</td><td></td><td>1</td><td></td><td>1</td><td></td></tr><tr><td>Geometry</td><td></td><td></td><td>8</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Graph</td><td>1</td><td>2</td><td>7</td><td>1</td><td>5</td><td></td><td></td><td></td></tr><tr><td>Graph Attention Networks</td><td></td><td></td><td></td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Graph Contrastive Learning</td><td></td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Graph Convolutional Network</td><td></td><td>2</td><td>5</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Graph Embedding</td><td></td><td></td><td></td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Graph Neural Network</td><td></td><td></td><td></td><td></td><td>6</td><td></td><td></td><td></td></tr><tr><td>Grounding</td><td></td><td></td><td>1</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Human Intervention</td><td></td><td></td><td>1</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Image2text</td><td></td><td></td><td>2</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>In-context Learning</td><td>9</td><td></td><td>4</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Information Retrieval</td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Instruction Following</td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Instruction Tuning</td><td>3</td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Intent Detection</td><td></td><td></td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Key Point Analysis</td><td></td><td></td><td></td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Knowledge Distillation</td><td>3</td><td></td><td>13</td><td></td><td>3</td><td>2</td><td>1</td><td></td></tr><tr><td>Knowledge Graph</td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Knowledge Transfer</td><td></td><td></td><td>2</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>LLaMA</td><td>3</td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Large Language Model</td><td>46</td><td>6</td><td>16</td><td>8</td><td>3</td><td>4</td><td></td><td>2</td></tr><tr><td>Logistic Regression</td><td></td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>MNIST</td><td></td><td></td><td></td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Markov Decision Process</td><td></td><td></td><td></td><td></td><td>1</td><td></td><td></td><td>1</td></tr><tr><td>Message-Passing</td><td></td><td></td><td></td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Meta Learning</td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Mistral</td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Model Compression</td><td></td><td></td><td>1</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Model Distillation</td><td></td><td></td><td></td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Model Pruning</td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Multi-modal</td><td>2</td><td>2</td><td>43</td><td></td><td>4</td><td>1</td><td></td><td></td></tr><tr><td>Named Entity Recognition</td><td>1</td><td></td><td></td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Natural Language Explanation</td><td></td><td></td><td></td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Natural Language Understanding</td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Neural Machine Translation</td><td>8</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Node Classification</td><td></td><td></td><td></td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Node Embedding</td><td></td><td></td><td></td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Object Detection</td><td></td><td></td><td>11</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Offline Reinforcement Learning</td><td></td><td></td><td></td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Optical Character Recognition</td><td></td><td></td><td>2</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Out-of-distribution</td><td></td><td></td><td>2</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Out-of-domain</td><td></td><td></td><td></td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>PaLM</td><td></td><td></td><td></td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Perplexity</td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Pre-trained Language Model</td><td></td><td>1</td><td>1</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Probabilistic Model</td><td></td><td></td><td></td><td></td><td>1</td><td></td><td></td><td>1</td></tr><tr><td>Prompt</td><td>7</td><td>2</td><td>20</td><td>4</td><td>3</td><td></td><td>1</td><td></td></tr><tr><td>Prompt Learning</td><td></td><td></td><td>2</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Pruning</td><td>2</td><td></td><td></td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Quantization</td><td>1</td><td></td><td>2</td><td></td><td>3</td><td></td><td></td><td></td></tr><tr><td>Question Answering</td><td>5</td><td>1</td><td>3</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Reasoning</td><td>10</td><td></td><td>5</td><td></td><td>2</td><td>1</td><td></td><td></td></tr><tr><td>Recommendation</td><td>1</td><td></td><td></td><td>2</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Recurrent Neural Network</td><td></td><td></td><td>2</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Reinforcement Learning</td><td></td><td>1</td><td>1</td><td></td><td>4</td><td>2</td><td></td><td></td></tr><tr><td>Relation Extraction</td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Representation Learning</td><td>1</td><td></td><td>2</td><td></td><td>4</td><td></td><td></td><td></td></tr><tr><td>Retrieval-Augmented Generation</td><td>6</td><td></td><td></td><td></td><td></td><td></td><td></td><td>3</td></tr><tr><td>Sample Size</td><td>1</td><td></td><td></td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Self-Attention</td><td></td><td></td><td>3</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Self-supervised Learning</td><td></td><td></td><td>5</td><td></td><td>3</td><td>2</td><td>1</td><td></td></tr><tr><td>Sentence Embedding</td><td>2</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Sentiment Analysis</td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Simulation</td><td></td><td></td><td>3</td><td></td><td>3</td><td>6</td><td>1</td><td>6</td></tr><tr><td>Simulator</td><td></td><td></td><td>3</td><td></td><td>3</td><td>6</td><td>1</td><td>6</td></tr><tr><td>Speech-to-Speech Translation</td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Stemming</td><td></td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Stochastic Gradient Descent</td><td></td><td></td><td></td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Style Transfer</td><td></td><td></td><td></td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Summarization</td><td>2</td><td></td><td></td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Supervised Learning</td><td></td><td></td><td>12</td><td></td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>T5</td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Text Generation</td><td>1</td><td></td><td></td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Text Summarization</td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Text2SQL</td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Text2image</td><td></td><td></td><td>3</td><td>1</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Transfer Learning</td><td></td><td></td><td>1</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Transformer</td><td>6</td><td></td><td>11</td><td></td><td>3</td><td></td><td>1</td><td></td></tr><tr><td>Unsupervised Learning</td><td>1</td><td></td><td>7</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Vision Transformer</td><td></td><td></td><td>4</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Vision-and-Language</td><td>1</td><td></td><td>18</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Visual Question Answering</td><td></td><td></td><td>4</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Weakly Supervised Learning</td><td></td><td></td><td>1</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Weakly-supervised Learning</td><td></td><td></td><td>3</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Zero-shot</td><td>4</td><td></td><td>9</td><td></td><td></td><td>1</td><td>1</td><td></td></tr><tr><td>Zero-shot Learning</td><td></td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td></tr></tbody></table><script>$(function(){$("table").addClass("keyword-table table-bordered border-success"),$("table thead").addClass("sticky-top"),$("table tbody td").css("text-align","")})</script><h2 id=cscv-101>cs.CV (101)</h2><h3 id=1101--1320-mm1-methods-analysis--insights-from-multimodal-llm-pre-training-brandon-mckinzie-et-al-2024>(1/101 | 1/320) MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training (Brandon McKinzie et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Floris Weers, Anton Belyi, Haotian Zhang, Karanjeet Singh, Doug Kang, Ankur Jain, Hongyu Hè, Max Schwarzer, Tom Gunter, Xiang Kong, Aonan Zhang, Jianyu Wang, Chong Wang, Nan Du, Tao Lei, Sam Wiseman, Mark Lee, Zirui Wang, Ruoming Pang, Peter Grasch, Alexander Toshev, Yinfei Yang. (2024)<br><strong>MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training</strong><br><button class=copy-to-clipboard title="MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training" index=1>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-1 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CL, cs-CV, cs-LG, cs.CV<br>Keyword Score: 129<br>Keywords: Benchmarking, Few-shot, Fine-tuning, Multi-modal, Multi-modal, Supervised Learning, Image2text, Reasoning, Chain-of-thought Prompt, In-context Learning, In-context Learning, Large Language Model, Large Language Model, Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09611v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09611v2.pdf filename=2403.09611v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we discuss building performant <b>Multimodal</b> <b>Large</b> <b>Language</b> <b>Models</b> (MLLMs). In particular, we study the importance of various architecture components and data choices. Through careful and comprehensive ablations of the image encoder, the vision language connector, and various pre-training data choices, we identified several crucial design lessons. For example, we demonstrate that for <b>large-scale</b> <b>multimodal</b> <b>pre-training</b> using a careful mix of image-caption, interleaved <b>image-text,</b> and text-only data is crucial for achieving state-of-the-art (SOTA) <b>few-shot</b> results across multiple <b>benchmarks,</b> compared to other published pre-training results. Further, we show that the image encoder together with image resolution and the image token count has substantial impact, while the <b>vision-language</b> connector design is of comparatively negligible importance. By scaling up the presented recipe, we build MM1, a family of <b>multimodal</b> models up to 30B parameters, including both dense models and mixture-of-experts (MoE) variants, that are SOTA in pre-training metrics and achieve competitive performance after <b>supervised</b> <b>fine-tuning</b> on a range of established <b>multimodal</b> <b>benchmarks.</b> Thanks to <b>large-scale</b> <b>pre-training,</b> <b>MM1</b> enjoys appealing properties such as enhanced <b>in-context</b> <b>learning,</b> and multi-image <b>reasoning,</b> enabling <b>few-shot</b> <b>chain-of-thought</b> <b>prompting.</b></p></p class="citation"></blockquote><h3 id=2101--2320-git-towards-generalist-vision-transformer-through-universal-language-interface-haiyang-wang-et-al-2024>(2/101 | 2/320) GiT: Towards Generalist Vision Transformer through Universal Language Interface (Haiyang Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haiyang Wang, Hao Tang, Li Jiang, Shaoshuai Shi, Muhammad Ferjad Naeem, Hongsheng Li, Bernt Schiele, Liwei Wang. (2024)<br><strong>GiT: Towards Generalist Vision Transformer through Universal Language Interface</strong><br><button class=copy-to-clipboard title="GiT: Towards Generalist Vision Transformer through Universal Language Interface" index=2>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-2 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 103<br>Keywords: Vision Transformer, Benchmarking, Fine-tuning, Foundation Model, Zero-shot, GPT, Transformer, Large Language Model, Large Language Model, Vision Transformer, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09394v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09394v1.pdf filename=2403.09394v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper proposes a simple, yet effective framework, called GiT, simultaneously applicable for various <b>vision</b> <b>tasks</b> only with a vanilla ViT. Motivated by the universality of the Multi-layer <b>Transformer</b> architecture (e.g, <b>GPT)</b> widely used in <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> we seek to broaden its scope to serve as a powerful <b>vision</b> <b>foundation</b> <b>model</b> (VFM). However, unlike language modeling, visual tasks typically require specific modules, such as bounding box heads for detection and pixel decoders for segmentation, greatly hindering the application of powerful multi-layer <b>transformers</b> in the <b>vision</b> <b>domain.</b> To solve this, we design a universal language interface that empowers the successful auto-regressive decoding to adeptly unify various visual tasks, from image-level understanding (e.g., captioning), over sparse perception (e.g., detection), to dense prediction (e.g., segmentation). Based on the above designs, the entire model is composed solely of a ViT, without any specific additions, offering a remarkable architectural simplification. GiT is a multi-task visual model, jointly trained across five representative <b>benchmarks</b> without task-specific <b>fine-tuning.</b> Interestingly, our GiT builds a new <b>benchmark</b> in generalist performance, and fosters mutual enhancement across tasks, leading to significant improvements compared to isolated training. This reflects a similar impact observed in <b>LLMs.</b> Further enriching training with 27 datasets, GiT achieves strong <b>zero-shot</b> results over various tasks. Due to its simple design, this paradigm holds promise for narrowing the architectural gap between <b>vision</b> <b>and</b> language. Code and models will be available at \url{https://github.com/Haiyang-W/GiT}.</p></p class="citation"></blockquote><h3 id=3101--3320-select-and-distill-selective-dual-teacher-knowledge-transfer-for-continual-learning-on-vision-language-models-yu-chu-yu-et-al-2024>(3/101 | 3/320) Select and Distill: Selective Dual-Teacher Knowledge Transfer for Continual Learning on Vision-Language Models (Yu-Chu Yu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yu-Chu Yu, Chi-Pin Huang, Jr-Jen Chen, Kai-Po Chang, Yung-Hsuan Lai, Fu-En Yang, Yu-Chiang Frank Wang. (2024)<br><strong>Select and Distill: Selective Dual-Teacher Knowledge Transfer for Continual Learning on Vision-Language Models</strong><br><button class=copy-to-clipboard title="Select and Distill: Selective Dual-Teacher Knowledge Transfer for Continual Learning on Vision-Language Models" index=3>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-3 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 83<br>Keywords: Benchmarking, Continual Learning, Fine-tuning, Knowledge Distillation, Knowledge Distillation, Knowledge Distillation, Knowledge Transfer, Zero-shot, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09296v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09296v1.pdf filename=2403.09296v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Large-scale <b>vision-language</b> models (VLMs) have shown a strong <b>zero-shot</b> generalization capability on unseen-domain data. However, when adapting pre-trained VLMs to a sequence of downstream tasks, they are prone to forgetting previously learned <b>knowledge</b> <b>and</b> degrade their <b>zero-shot</b> classification capability. To tackle this problem, we propose a unique Selective Dual-Teacher <b>Knowledge</b> <b>Transfer</b> framework that leverages the most recent <b>fine-tuned</b> and the original pre-trained VLMs as dual teachers to preserve the previously learned <b>knowledge</b> <b>and</b> <b>zero-shot</b> capabilities, respectively. With only access to an unlabeled reference dataset, our proposed framework performs a selective <b>knowledge</b> <b>distillation</b> mechanism by measuring the feature discrepancy from the dual teacher VLMs. Consequently, our selective dual-teacher <b>knowledge</b> <b>distillation</b> would mitigate catastrophic forgetting of previously learned <b>knowledge</b> <b>while</b> preserving the <b>zero-shot</b> capabilities from pre-trained VLMs. Through extensive experiments on <b>benchmark</b> datasets, we show that our proposed framework is favorable against state-of-the-art <b>continual</b> <b>learning</b> approaches for preventing catastrophic forgetting and <b>zero-shot</b> degradation.</p></p class="citation"></blockquote><h3 id=4101--4320-adversarial-training-with-ocr-modality-perturbation-for-scene-text-visual-question-answering-zhixuan-shen-et-al-2024>(4/101 | 4/320) Adversarial Training with OCR Modality Perturbation for Scene-Text Visual Question Answering (Zhixuan Shen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhixuan Shen, Haonan Luo, Sijia Li, Tianrui Li. (2024)<br><strong>Adversarial Training with OCR Modality Perturbation for Scene-Text Visual Question Answering</strong><br><button class=copy-to-clipboard title="Adversarial Training with OCR Modality Perturbation for Scene-Text Visual Question Answering" index=4>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-4 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 76<br>Keywords: Optical Character Recognition, Optical Character Recognition, Adversarial Learning, Fine-tuning, Multi-modal, Multi-modal, Question Answering, Visual Question Answering, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09288v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09288v1.pdf filename=2403.09288v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Scene-Text <b>Visual</b> <b>Question</b> <b>Answering</b> (ST-VQA) aims to understand scene text in images and answer <b>questions</b> <b>related</b> to the text content. Most existing methods heavily rely on the accuracy of <b>Optical</b> <b>Character</b> <b>Recognition</b> <b>(OCR)</b> systems, and aggressive <b>fine-tuning</b> based on limited spatial location information and erroneous <b>OCR</b> text information often leads to inevitable overfitting. In this paper, we propose a <b>multimodal</b> <b>adversarial</b> <b>training</b> architecture with spatial awareness capabilities. Specifically, we introduce an <b>Adversarial</b> <b>OCR</b> Enhancement (AOE) module, which leverages <b>adversarial</b> <b>training</b> in the embedding space of <b>OCR</b> modality to enhance fault-tolerant representation of <b>OCR</b> texts, thereby reducing noise caused by <b>OCR</b> errors. Simultaneously, We add a Spatial-Aware <b>Self-Attention</b> (SASA) mechanism to help the model better capture the spatial relationships among <b>OCR</b> tokens. Various experiments demonstrate that our method achieves significant performance improvements on both the ST-VQA and TextVQA datasets and provides a novel paradigm for <b>multimodal</b> <b>adversarial</b> <b>training.</b></p></p class="citation"></blockquote><h3 id=5101--5320-visiongpt-vision-language-understanding-agent-using-generalized-multimodal-framework-chris-kelly-et-al-2024>(5/101 | 5/320) VisionGPT: Vision-Language Understanding Agent Using Generalized Multimodal Framework (Chris Kelly et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chris Kelly, Luhui Hu, Bang Yang, Yu Tian, Deshun Yang, Cindy Yang, Zaoshan Huang, Zihao Li, Jiayin Hu, Yuexian Zou. (2024)<br><strong>VisionGPT: Vision-Language Understanding Agent Using Generalized Multimodal Framework</strong><br><button class=copy-to-clipboard title="VisionGPT: Vision-Language Understanding Agent Using Generalized Multimodal Framework" index=5>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-5 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 76<br>Keywords: Foundation Model, Multi-modal, Multi-modal, LLaMA, Question Answering, Visual Question Answering, Large Language Model, Large Language Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09027v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09027v1.pdf filename=2403.09027v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the emergence of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> and vision <b>foundation</b> <b>models,</b> how to combine the intelligence and capacity of these open-sourced or API-available models to achieve open-world <b>visual</b> <b>perception</b> <b>remains</b> an open <b>question.</b> <b>In</b> this paper, we introduce VisionGPT to consolidate and automate the integration of state-of-the-art <b>foundation</b> <b>models,</b> thereby facilitating <b>vision-language</b> understanding and the development of vision-oriented AI. VisionGPT builds upon a generalized <b>multimodal</b> framework that distinguishes itself through three key features: (1) utilizing <b>LLMs</b> (e.g., <b>LLaMA-2)</b> as the pivot to break down users&rsquo; requests into detailed action proposals to call suitable <b>foundation</b> <b>models;</b> (2) integrating multi-source outputs from <b>foundation</b> <b>models</b> automatically and generating comprehensive responses for users; (3) adaptable to a wide range of applications such as text-conditioned image understanding/generation/editing and <b>visual</b> <b>question</b> <b>answering.</b> This paper outlines the architecture and capabilities of VisionGPT, demonstrating its potential to revolutionize the field of computer vision through enhanced efficiency, versatility, and generalization, and performance. Our code and models will be made publicly available. Keywords: VisionGPT, Open-world <b>visual</b> <b>perception,</b> <b>Vision-language</b> understanding, <b>Large</b> <b>language</b> <b>model,</b> and <b>Foundation</b> <b>model</b></p></p class="citation"></blockquote><h3 id=6101--6320-eyes-closed-safety-on-protecting-multimodal-llms-via-image-to-text-transformation-yunhao-gou-et-al-2024>(6/101 | 6/320) Eyes Closed, Safety On: Protecting Multimodal LLMs via Image-to-Text Transformation (Yunhao Gou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yunhao Gou, Kai Chen, Zhili Liu, Lanqing Hong, Hang Xu, Zhenguo Li, Dit-Yan Yeung, James T. Kwok, Yu Zhang. (2024)<br><strong>Eyes Closed, Safety On: Protecting Multimodal LLMs via Image-to-Text Transformation</strong><br><button class=copy-to-clipboard title="Eyes Closed, Safety On: Protecting Multimodal LLMs via Image-to-Text Transformation" index=6>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-6 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 69<br>Keywords: Benchmarking, Human Intervention, Multi-modal, Multi-modal, Supervised Learning, Image2text, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09572v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09572v1.pdf filename=2403.09572v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multimodal</b> <b>large</b> <b>language</b> <b>models</b> (MLLMs) have shown impressive <b>reasoning</b> abilities, which, however, are also more vulnerable to jailbreak attacks than their <b>LLM</b> predecessors. Although still capable of detecting unsafe responses, we observe that safety mechanisms of the pre-aligned <b>LLMs</b> in MLLMs can be easily bypassed due to the introduction of image features. To construct robust MLLMs, we propose ECSO(Eyes Closed, Safety On), a novel training-free protecting approach that exploits the inherent safety awareness of MLLMs, and generates safer responses via adaptively transforming unsafe images into texts to activate intrinsic safety mechanism of pre-aligned <b>LLMs</b> in MLLMs. Experiments on five state-of-the-art (SoTA) MLLMs demonstrate that our ECSO enhances model safety significantly (e.g., a 37.6% improvement on the MM-SafetyBench (SD+OCR), and 71.3% on VLSafe for the LLaVA-1.5-7B), while consistently maintaining utility results on common MLLM <b>benchmarks.</b> Furthermore, we show that ECSO can be used as a data engine to generate <b>supervised-finetuning</b> (SFT) data for MLLM alignment without extra <b>human</b> <b>intervention.</b></p></p class="citation"></blockquote><h3 id=7101--7320-3d-vla-a-3d-vision-language-action-generative-world-model-haoyu-zhen-et-al-2024>(7/101 | 7/320) 3D-VLA: A 3D Vision-Language-Action Generative World Model (Haoyu Zhen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haoyu Zhen, Xiaowen Qiu, Peihao Chen, Jincheng Yang, Xin Yan, Yilun Du, Yining Hong, Chuang Gan. (2024)<br><strong>3D-VLA: A 3D Vision-Language-Action Generative World Model</strong><br><button class=copy-to-clipboard title="3D-VLA: A 3D Vision-Language-Action Generative World Model" index=7>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-7 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CL, cs-CV, cs-RO, cs.CV<br>Keyword Score: 66<br>Keywords: Diffusion Model, Foundation Model, Multi-modal, Multi-modal, Reasoning, Large Language Model, Large Language Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09631v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09631v1.pdf filename=2403.09631v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent <b>vision-language-action</b> (VLA) models rely on 2D inputs, lacking integration with the broader realm of the 3D physical world. Furthermore, they perform action prediction by learning a direct mapping from perception to action, neglecting the vast dynamics of the world and the relations between actions and dynamics. In contrast, human beings are endowed with world models that depict imagination about future scenarios to plan actions accordingly. To this end, we propose 3D-VLA by introducing a new family of embodied <b>foundation</b> <b>models</b> that seamlessly link 3D perception, <b>reasoning,</b> and action through a generative world model. Specifically, 3D-VLA is built on top of a 3D-based <b>large</b> <b>language</b> <b>model</b> <b>(LLM),</b> and a set of interaction tokens is introduced to engage with the embodied environment. Furthermore, to inject generation abilities into the model, we train a series of embodied <b>diffusion</b> <b>models</b> and align them into the <b>LLM</b> for predicting the goal images and point clouds. To train our 3D-VLA, we curate a <b>large-scale</b> <b>3D</b> <b>embodied</b> instruction dataset by extracting vast 3D-related information from existing robotics datasets. Our experiments on held-in datasets demonstrate that 3D-VLA significantly improves the <b>reasoning,</b> <b>multimodal</b> generation, and planning capabilities in embodied environments, showcasing its potential in real-world applications.</p></p class="citation"></blockquote><h3 id=8101--8320-visiongpt-3d-a-generalized-multimodal-agent-for-enhanced-3d-vision-understanding-chris-kelly-et-al-2024>(8/101 | 8/320) VisionGPT-3D: A Generalized Multimodal Agent for Enhanced 3D Vision Understanding (Chris Kelly et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chris Kelly, Luhui Hu, Jiayin Hu, Yu Tian, Deshun Yang, Bang Yang, Cindy Yang, Zihao Li, Zaoshan Huang, Yuexian Zou. (2024)<br><strong>VisionGPT-3D: A Generalized Multimodal Agent for Enhanced 3D Vision Understanding</strong><br><button class=copy-to-clipboard title="VisionGPT-3D: A Generalized Multimodal Agent for Enhanced 3D Vision Understanding" index=8>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-8 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CL, cs-CV, cs-GR, cs.CV<br>Keyword Score: 66<br>Keywords: Foundation Model, Multi-modal, Multi-modal, GPT, GPT-4, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09530v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09530v1.pdf filename=2403.09530v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The evolution of text to visual components facilitates people&rsquo;s daily lives, such as generating image, videos from text and identifying the desired elements within the images. Computer vision models involving the <b>multimodal</b> abilities in the previous days are focused on image detection, classification based on well-defined objects. <b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> introduces the transformation from nature language to visual objects, which present the visual layout for text contexts. OpenAI <b>GPT-4</b> has emerged as the pinnacle in <b>LLMs,</b> while the computer vision (CV) domain boasts a plethora of state-of-the-art (SOTA) models and algorithms to convert 2D images to their 3D representations. However, the mismatching between the algorithms with the problem could lead to undesired results. In response to this challenge, we propose an unified VisionGPT-3D framework to consolidate the state-of-the-art vision models, thereby facilitating the development of vision-oriented AI. VisionGPT-3D provides a versatile <b>multimodal</b> framework building upon the strengths of <b>multimodal</b> <b>foundation</b> <b>models.</b> It seamlessly integrates various SOTA vision models and brings the automation in the selection of SOTA vision models, identifies the suitable 3D mesh creation algorithms corresponding to 2D depth maps analysis, generates optimal results based on diverse <b>multimodal</b> inputs such as text <b>prompts.</b> Keywords: VisionGPT-3D, 3D vision understanding, <b>Multimodal</b> agent</p></p class="citation"></blockquote><h3 id=9101--9320-skateformer-skeletal-temporal-transformer-for-human-action-recognition-jeonghyeok-do-et-al-2024>(9/101 | 9/320) SkateFormer: Skeletal-Temporal Transformer for Human Action Recognition (Jeonghyeok Do et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jeonghyeok Do, Munchurl Kim. (2024)<br><strong>SkateFormer: Skeletal-Temporal Transformer for Human Action Recognition</strong><br><button class=copy-to-clipboard title="SkateFormer: Skeletal-Temporal Transformer for Human Action Recognition" index=9>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-9 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 66<br>Keywords: Graph Convolutional Network, Graph Convolutional Network, Graph, Benchmarking, Convolution, Convolutional Neural Network, Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09508v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09508v1.pdf filename=2403.09508v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Skeleton-based action recognition, which classifies human actions based on the coordinates of joints and their connectivity within skeleton data, is widely utilized in various scenarios. While <b>Graph</b> <b>Convolutional</b> <b>Networks</b> <b>(GCNs)</b> have been proposed for skeleton data represented as <b>graphs,</b> <b>they</b> <b>suffer</b> from limited receptive fields constrained by joint connectivity. To address this limitation, recent advancements have introduced <b>transformer-based</b> methods. However, capturing correlations between all joints in all frames requires substantial memory resources. To alleviate this, we propose a novel approach called Skeletal-Temporal <b>Transformer</b> (SkateFormer) that partitions joints and frames based on different types of skeletal-temporal relation (Skate-Type) and performs skeletal-temporal <b>self-attention</b> (Skate-MSA) within each partition. We categorize the key skeletal-temporal relations for action recognition into a total of four distinct types. These types combine (i) two skeletal relation types based on physically neighboring and distant joints, and (ii) two temporal relation types based on neighboring and distant frames. Through this partition-specific attention strategy, our SkateFormer can selectively focus on key joints and frames crucial for action recognition in an action-adaptive manner with efficient computation. Extensive experiments on various <b>benchmark</b> datasets validate that our SkateFormer outperforms recent state-of-the-art methods.</p></p class="citation"></blockquote><h3 id=10101--10320-df4lcz-a-sam-empowered-data-fusion-framework-for-scene-level-local-climate-zone-classification-qianqian-wu-et-al-2024>(10/101 | 10/320) DF4LCZ: A SAM-Empowered Data Fusion Framework for Scene-Level Local Climate Zone Classification (Qianqian Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qianqian Wu, Xianping Ma, Jialu Sui, Man-On Pun. (2024)<br><strong>DF4LCZ: A SAM-Empowered Data Fusion Framework for Scene-Level Local Climate Zone Classification</strong><br><button class=copy-to-clipboard title="DF4LCZ: A SAM-Empowered Data Fusion Framework for Scene-Level Local Climate Zone Classification" index=10>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-10 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 63<br>Keywords: Graph Convolutional Network, Graph Convolutional Network, Graph, Convolution, Convolutional Neural Network, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09367v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09367v1.pdf filename=2403.09367v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in remote sensing (RS) technologies have shown their potential in accurately classifying local climate zones (LCZs). However, traditional scene-level methods using <b>convolutional</b> <b>neural</b> <b>networks</b> <b>(CNNs)</b> often struggle to integrate prior knowledge of ground objects effectively. Moreover, commonly utilized data sources like Sentinel-2 encounter difficulties in capturing detailed ground object information. To tackle these challenges, we propose a data fusion method that integrates ground object priors extracted from high-resolution Google imagery with Sentinel-2 multispectral imagery. The proposed method introduces a novel Dual-stream Fusion framework for LCZ classification (DF4LCZ), integrating instance-based location features from Google imagery with the scene-level spatial-spectral features extracted from Sentinel-2 imagery. The framework incorporates a <b>Graph</b> <b>Convolutional</b> <b>Network</b> <b>(GCN)</b> module empowered by the Segment Anything Model (SAM) to enhance feature extraction from Google imagery. Simultaneously, the framework employs a 3D-CNN architecture to learn the spectral-spatial features of Sentinel-2 imagery. Experiments are conducted on a multi-source remote sensing image dataset specifically designed for LCZ classification, validating the effectiveness of the proposed DF4LCZ. The related code and dataset are available at <a href=https://github.com/ctrlovefly/DF4LCZ>https://github.com/ctrlovefly/DF4LCZ</a>.</p></p class="citation"></blockquote><h3 id=11101--11320-scp-diff-photo-realistic-semantic-image-synthesis-with-spatial-categorical-joint-prior-huan-ang-gao-et-al-2024>(11/101 | 11/320) SCP-Diff: Photo-Realistic Semantic Image Synthesis with Spatial-Categorical Joint Prior (Huan-ang Gao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Huan-ang Gao, Mingju Gao, Jiaju Li, Wenyi Li, Rong Zhi, Hao Tang, Hao Zhao. (2024)<br><strong>SCP-Diff: Photo-Realistic Semantic Image Synthesis with Spatial-Categorical Joint Prior</strong><br><button class=copy-to-clipboard title="SCP-Diff: Photo-Realistic Semantic Image Synthesis with Spatial-Categorical Joint Prior" index=11>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-11 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 60<br>Keywords: ControlNet, Diffusion Model, Generative Adversarial Network, Simulation, Simulator, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09638v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09638v1.pdf filename=2403.09638v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Semantic image synthesis (SIS) shows good promises for sensor <b>simulation.</b> However, current best practices in this field, based on <b>GANs,</b> have not yet reached the desired level of quality. As latent <b>diffusion</b> <b>models</b> make significant strides in image generation, we are <b>prompted</b> to evaluate <b>ControlNet,</b> a notable method for its dense control capabilities. Our investigation uncovered two primary issues with its results: the presence of weird sub-structures within large semantic areas and the misalignment of content with the semantic mask. Through empirical study, we pinpointed the cause of these problems as a mismatch between the noised training data distribution and the standard normal prior applied at the inference stage. To address this challenge, we developed specific noise priors for SIS, encompassing spatial, categorical, and a novel spatial-categorical joint prior for inference. This approach, which we have named SCP-Diff, has yielded exceptional results, achieving an FID of 10.53 on Cityscapes and 12.66 on ADE20K.The code and models can be accessed via the project page.</p></p class="citation"></blockquote><h3 id=12101--12320-cloud-gap-filling-with-deep-learning-for-improved-grassland-monitoring-iason-tsardanidis-et-al-2024>(12/101 | 12/320) Cloud gap-filling with deep learning for improved grassland monitoring (Iason Tsardanidis et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Iason Tsardanidis, Alkiviadis Koukos, Vasileios Sitokonstantinou, Thanassis Drivas, Charalampos Kontoes. (2024)<br><strong>Cloud gap-filling with deep learning for improved grassland monitoring</strong><br><button class=copy-to-clipboard title="Cloud gap-filling with deep learning for improved grassland monitoring" index=12>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-12 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV<br>Keyword Score: 60<br>Keywords: Continuous Time, Continuous Time, Convolution, Convolutional Neural Network, Convolutional Neural Network, Recurrent Neural Network, Event Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09554v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09554v1.pdf filename=2403.09554v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Uninterrupted optical image time series are crucial for the timely monitoring of agricultural land changes. However, the continuity of such time series is often disrupted by clouds. In response to this challenge, we propose a deep learning method that integrates cloud-free optical (Sentinel-2) observations and weather-independent (Sentinel-1) Synthetic Aperture Radar (SAR) data, using a combined <b>Convolutional</b> <b>Neural</b> <b>Network</b> <b>(CNN)-Recurrent</b> Neural Network <b>(RNN)</b> architecture to generate <b>continuous</b> <b>Normalized</b> Difference Vegetation Index (NDVI) time series. We emphasize the significance of observation continuity by assessing the impact of the generated time series on the detection of grassland mowing <b>events.</b> <b>We</b> focus on Lithuania, a country characterized by extensive cloud coverage, and compare our approach with alternative interpolation techniques (i.e., linear, Akima, quadratic). Our method surpasses these techniques, with an average MAE of 0.024 and R^2 of 0.92. It not only improves the accuracy of <b>event</b> <b>detection</b> tasks by employing a <b>continuous</b> <b>time</b> series, but also effectively filters out sudden shifts and noise originating from cloudy observations that cloud masks often fail to detect.</p></p class="citation"></blockquote><h3 id=13101--13320-anomaly-detection-by-adapting-a-pre-trained-vision-language-model-yuxuan-cai-et-al-2024>(13/101 | 13/320) Anomaly Detection by Adapting a pre-trained Vision Language Model (Yuxuan Cai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuxuan Cai, Xinwei He, Dingkang Liang, Ao Tong, Xiang Bai. (2024)<br><strong>Anomaly Detection by Adapting a pre-trained Vision Language Model</strong><br><button class=copy-to-clipboard title="Anomaly Detection by Adapting a pre-trained Vision Language Model" index=13>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-13 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 60<br>Keywords: Anomaly Detection, Self-supervised Learning, Self-supervised Learning, Prompt, Vision-and-Language, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09493v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09493v1.pdf filename=2403.09493v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, large vision and language models have shown their success when adapting them to many downstream tasks. In this paper, we present a unified framework named CLIP-ADA for <b>Anomaly</b> <b>Detection</b> by Adapting a pre-trained CLIP model. To this end, we make two important improvements: 1) To acquire unified <b>anomaly</b> <b>detection</b> across industrial images of multiple categories, we introduce the learnable <b>prompt</b> and propose to associate it with abnormal patterns through <b>self-supervised</b> <b>learning.</b> 2) To fully exploit the representation power of CLIP, we introduce an <b>anomaly</b> <b>region</b> refinement strategy to refine the localization quality. During testing, the anomalies are localized by directly calculating the similarity between the representation of the learnable <b>prompt</b> and the image. Comprehensive experiments demonstrate the superiority of our framework, e.g., we achieve the state-of-the-art 97.5/55.6 and 89.3/33.1 on MVTec-AD and VisA for <b>anomaly</b> <b>detection</b> and localization. In addition, the proposed method also achieves encouraging performance with marginal training data, which is more challenging.</p></p class="citation"></blockquote><h3 id=14101--14320-localmamba-visual-state-space-model-with-windowed-selective-scan-tao-huang-et-al-2024>(14/101 | 14/320) LocalMamba: Visual State Space Model with Windowed Selective Scan (Tao Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tao Huang, Xiaohuan Pei, Shan You, Fei Wang, Chen Qian, Chang Xu. (2024)<br><strong>LocalMamba: Visual State Space Model with Windowed Selective Scan</strong><br><button class=copy-to-clipboard title="LocalMamba: Visual State Space Model with Windowed Selective Scan" index=14>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-14 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 60<br>Keywords: Vision Transformer, Convolution, Convolutional Neural Network, Convolutional Neural Network, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09338v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09338v1.pdf filename=2403.09338v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in state space models, notably Mamba, have demonstrated significant progress in modeling long sequences for tasks like language understanding. Yet, their application in <b>vision</b> <b>tasks</b> has not markedly surpassed the performance of traditional <b>Convolutional</b> <b>Neural</b> <b>Networks</b> <b>(CNNs)</b> and <b>Vision</b> <b>Transformers</b> (ViTs). This paper posits that the key to enhancing <b>Vision</b> <b>Mamba</b> (ViM) lies in optimizing scan directions for sequence modeling. Traditional ViM approaches, which flatten spatial tokens, overlook the preservation of local 2D dependencies, thereby elongating the distance between adjacent tokens. We introduce a novel local scanning strategy that divides images into distinct windows, effectively capturing local dependencies while maintaining a global perspective. Additionally, acknowledging the varying preferences for scan patterns across different network layers, we propose a dynamic method to independently search for the optimal scan choices for each layer, substantially improving performance. Extensive experiments across both plain and hierarchical models underscore our approach&rsquo;s superiority in effectively capturing image representations. For example, our model significantly outperforms Vim-Ti by 3.1% on ImageNet with the same 1.5G FLOPs. Code is available at: <a href=https://github.com/hunto/LocalMamba>https://github.com/hunto/LocalMamba</a>.</p></p class="citation"></blockquote><h3 id=15101--15320-unicode-learning-a-unified-codebook-for-multimodal-large-language-models-sipeng-zheng-et-al-2024>(15/101 | 15/320) UniCode: Learning a Unified Codebook for Multimodal Large Language Models (Sipeng Zheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sipeng Zheng, Bohan Zhou, Yicheng Feng, Ye Wang, Zongqing Lu. (2024)<br><strong>UniCode: Learning a Unified Codebook for Multimodal Large Language Models</strong><br><button class=copy-to-clipboard title="UniCode: Learning a Unified Codebook for Multimodal Large Language Models" index=15>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-15 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CL, cs-CV, cs.CV<br>Keyword Score: 59<br>Keywords: Benchmarking, Multi-modal, Multi-modal, Quantization, Visual Question Answering, In-context Learning, Instruction Tuning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09072v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09072v1.pdf filename=2403.09072v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we propose \textbf{UniCode}, a novel approach within the domain of <b>multimodal</b> <b>large</b> <b>language</b> <b>models</b> (MLLMs) that learns a unified codebook to efficiently tokenize visual, text, and potentially other types of signals. This innovation addresses a critical limitation in existing MLLMs: their reliance on a text-only codebook, which restricts MLLM&rsquo;s ability to generate images and texts in a <b>multimodal</b> context. Towards this end, we propose a language-driven iterative training paradigm, coupled with an <b>in-context</b> pre-training task we term ``image decompression&rsquo;&rsquo;, enabling our model to interpret compressed visual data and generate high-quality images.The unified codebook empowers our model to extend visual <b>instruction</b> <b>tuning</b> to non-linguistic generation tasks. Moreover, UniCode is adaptable to diverse stacked <b>quantization</b> approaches in order to compress visual signals into a more compact token representation. Despite using significantly fewer parameters and less data during training, Unicode demonstrates promising capabilities in visual reconstruction and generation. It also achieves performances comparable to leading MLLMs across a spectrum of <b>VQA</b> <b>benchmarks.</b></p></p class="citation"></blockquote><h3 id=16101--16320-griffon-v2-advancing-multimodal-perception-with-high-resolution-scaling-and-visual-language-co-referring-yufei-zhan-et-al-2024>(16/101 | 16/320) Griffon v2: Advancing Multimodal Perception with High-Resolution Scaling and Visual-Language Co-Referring (Yufei Zhan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yufei Zhan, Yousong Zhu, Hongyin Zhao, Fan Yang, Ming Tang, Jinqiao Wang. (2024)<br><strong>Griffon v2: Advancing Multimodal Perception with High-Resolution Scaling and Visual-Language Co-Referring</strong><br><button class=copy-to-clipboard title="Griffon v2: Advancing Multimodal Perception with High-Resolution Scaling and Visual-Language Co-Referring" index=16>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-16 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 56<br>Keywords: Object Detection, Multi-modal, Multi-modal, Grounding, Large Language Model, Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09333v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09333v1.pdf filename=2403.09333v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Vision</b> <b>Language</b> Models have achieved fine-grained <b>object</b> <b>perception,</b> but the limitation of image resolution remains a significant obstacle to surpass the performance of task-specific experts in complex and dense scenarios. Such limitation further restricts the model&rsquo;s potential to achieve nuanced visual and language referring in domains such as GUI Agents, Counting and \etc. To address this issue, we introduce a unified high-resolution generalist model, Griffon v2, enabling flexible <b>object</b> <b>referring</b> with visual and textual <b>prompts.</b> To efficiently scaling up image resolution, we design a simple and lightweight down-sampling projector to overcome the input tokens constraint in <b>Large</b> <b>Language</b> <b>Models.</b> This design inherently preserves the complete contexts and fine details, and significantly improves <b>multimodal</b> perception ability especially for small <b>objects.</b> <b>Building</b> upon this, we further equip the model with visual-language co-referring capabilities through a plug-and-play visual tokenizer. It enables user-friendly interaction with flexible target images, free-form texts and even coordinates. Experiments demonstrate that Griffon v2 can localize any <b>objects</b> <b>of</b> interest with visual and textual referring, achieve state-of-the-art performance on REC, phrase <b>grounding,</b> and REG tasks, and outperform expert models in <b>object</b> <b>detection</b> and <b>object</b> <b>counting.</b> Data, codes and models will be released at <a href=https://github.com/jefferyZhan/Griffon>https://github.com/jefferyZhan/Griffon</a>.</p></p class="citation"></blockquote><h3 id=17101--17320-are-vision-language-models-texture-or-shape-biased-and-can-we-steer-them-paul-gavrikov-et-al-2024>(17/101 | 17/320) Are Vision Language Models Texture or Shape Biased and Can We Steer Them? (Paul Gavrikov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Paul Gavrikov, Jovita Lukasik, Steffen Jung, Robert Geirhos, Bianca Lamm, Muhammad Jehanzeb Mirza, Margret Keuper, Janis Keuper. (2024)<br><strong>Are Vision Language Models Texture or Shape Biased and Can We Steer Them?</strong><br><button class=copy-to-clipboard title="Are Vision Language Models Texture or Shape Biased and Can We Steer Them?" index=17>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-17 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV, q-bio-NC<br>Keyword Score: 56<br>Keywords: Multi-modal, Multi-modal, Zero-shot, Question Answering, Visual Question Answering, Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09193v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09193v1.pdf filename=2403.09193v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Vision language models (VLMs) have drastically changed the computer vision model landscape in only a few years, opening an exciting array of new applications from <b>zero-shot</b> image classification, over to image captioning, and <b>visual</b> <b>question</b> <b>answering.</b> Unlike pure vision models, they offer an intuitive way to access <b>visual</b> <b>content</b> <b>through</b> language <b>prompting.</b> The wide applicability of such models encourages us to ask whether they also align with human vision - specifically, how far they adopt human-induced <b>visual</b> <b>biases</b> <b>through</b> <b>multimodal</b> fusion, or whether they simply inherit biases from pure vision models. One important <b>visual</b> <b>bias</b> <b>is</b> the texture vs. shape bias, or the dominance of local over global information. In this paper, we study this bias in a wide range of popular VLMs. Interestingly, we find that VLMs are often more shape-biased than their vision encoders, indicating that <b>visual</b> <b>biases</b> <b>are</b> modulated to some extent through text in <b>multimodal</b> models. If text does indeed influence <b>visual</b> <b>biases,</b> <b>this</b> suggests that we may be able to steer <b>visual</b> <b>biases</b> <b>not</b> just through <b>visual</b> <b>input</b> <b>but</b> also through language: a hypothesis that we confirm through extensive experiments. For instance, we are able to steer shape bias from as low as 49% to as high as 72% through <b>prompting</b> alone. For now, the strong human bias towards shape (96%) remains out of reach for all tested VLMs.</p></p class="citation"></blockquote><h3 id=18101--18320-metadata-driven-federated-learning-of-connectional-brain-templates-in-non-iid-multi-domain-scenarios-geng-chen-et-al-2024>(18/101 | 18/320) Metadata-Driven Federated Learning of Connectional Brain Templates in Non-IID Multi-Domain Scenarios (Geng Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Geng Chen, Qingyue Wang, Islem Rekik. (2024)<br><strong>Metadata-Driven Federated Learning of Connectional Brain Templates in Non-IID Multi-Domain Scenarios</strong><br><button class=copy-to-clipboard title="Metadata-Driven Federated Learning of Connectional Brain Templates in Non-IID Multi-Domain Scenarios" index=18>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-18 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 53<br>Keywords: Graph, Federated Learning, Supervised Learning, Unsupervised Learning, Unsupervised Learning, Stemming<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09139v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09139v1.pdf filename=2403.09139v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A connectional brain template (CBT) is a holistic representation of a population of multi-view brain connectivity <b>graphs,</b> encoding shared patterns and normalizing typical variations across individuals. The federation of CBT learning allows for an inclusive estimation of the representative center of multi-domain brain connectivity datasets in a fully data-preserving manner. However, existing methods overlook the non-independent and identically distributed (non-IDD) issue <b>stemming</b> from multidomain brain connectivity heterogeneity, in which data domains are drawn from different hospitals and imaging modalities. To overcome this limitation, we unprecedentedly propose a metadata-driven <b>federated</b> <b>learning</b> framework, called MetaFedCBT, for cross-domain CBT learning. Given the data drawn from a specific domain (i.e., hospital), our model aims to learn metadata in a fully <b>supervised</b> manner by introducing a local client-based regressor network. The generated meta-data is forced to meet the statistical attributes (e.g., mean) of other domains, while preserving their privacy. Our <b>supervised</b> meta-data generation approach boosts the <b>unsupervised</b> <b>learning</b> of a more centered, representative, and holistic CBT of a particular brain state across diverse domains. As the <b>federated</b> <b>learning</b> progresses over multiple rounds, the learned metadata and associated generated connectivities are continuously updated to better approximate the target domain information. MetaFedCBT overcomes the non-IID issue of existing methods by generating informative brain connectivities for privacy-preserving holistic CBT learning with guidance using metadata. Extensive experiments on multi-view morphological brain networks of normal and patient subjects demonstrate that our MetaFedCBT is a superior <b>federated</b> <b>CBT</b> learning model and significantly advances the state-of-the-art performance.</p></p class="citation"></blockquote><h3 id=19101--19320-spikereveal-unlocking-temporal-sequences-from-real-blurry-inputs-with-spike-streams-kang-chen-et-al-2024>(19/101 | 19/320) SpikeReveal: Unlocking Temporal Sequences from Real Blurry Inputs with Spike Streams (Kang Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kang Chen, Shiyan Chen, Jiyuan Zhang, Baoyue Zhang, Yajing Zheng, Tiejun Huang, Zhaofei Yu. (2024)<br><strong>SpikeReveal: Unlocking Temporal Sequences from Real Blurry Inputs with Spike Streams</strong><br><button class=copy-to-clipboard title="SpikeReveal: Unlocking Temporal Sequences from Real Blurry Inputs with Spike Streams" index=19>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-19 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Knowledge Distillation, Knowledge Distillation, Self-supervised Learning, Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09486v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09486v1.pdf filename=2403.09486v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Reconstructing a sequence of sharp images from the blurry input is crucial for enhancing our insights into the captured scene and poses a significant challenge due to the limited temporal features embedded in the image. Spike cameras, sampling at rates up to 40,000 Hz, have proven effective in capturing motion features and beneficial for solving this ill-posed problem. Nonetheless, existing methods fall into the <b>supervised</b> <b>learning</b> paradigm, which suffers from notable performance degradation when applied to real-world scenarios that diverge from the synthetic training data domain. Moreover, the quality of reconstructed images is capped by the generated images based on motion analysis interpolation, which inherently differs from the actual scene, affecting the generalization ability of these methods in real high-speed scenarios. To address these challenges, we propose the first <b>self-supervised</b> framework for the task of spike-guided motion deblurring. Our approach begins with the formulation of a spike-guided deblurring model that explores the theoretical relationships among spike streams, blurry images, and their corresponding sharp sequences. We subsequently develop a <b>self-supervised</b> cascaded framework to alleviate the issues of spike noise and spatial-resolution mismatching encountered in the deblurring model. With <b>knowledge</b> <b>distillation</b> and re-blurring loss, we further design a lightweight deblur network to generate high-quality sequences with brightness and texture consistency with the original input. Quantitative and qualitative experiments conducted on our real-world and synthetic datasets with spikes validate the superior generalization of the proposed framework. Our code, data and trained models will be available at \url{https://github.com/chenkang455/S-SDM}.</p></p class="citation"></blockquote><h3 id=20101--20320-open-vocabulary-object-detection-with-meta-prompt-representation-and-instance-contrastive-optimization-zhao-wang-et-al-2024>(20/101 | 20/320) Open-Vocabulary Object Detection with Meta Prompt Representation and Instance Contrastive Optimization (Zhao Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhao Wang, Aoxue Li, Fengwei Zhou, Zhenguo Li, Qi Dou. (2024)<br><strong>Open-Vocabulary Object Detection with Meta Prompt Representation and Instance Contrastive Optimization</strong><br><button class=copy-to-clipboard title="Open-Vocabulary Object Detection with Meta Prompt Representation and Instance Contrastive Optimization" index=20>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-20 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Object Detection, Contrastive Learning, Knowledge Distillation, Knowledge Distillation, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09433v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09433v1.pdf filename=2403.09433v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Classical <b>object</b> <b>detectors</b> are incapable of detecting novel class <b>objects</b> <b>that</b> are not encountered before. Regarding this issue, Open-Vocabulary <b>Object</b> <b>Detection</b> (OVOD) is proposed, which aims to detect the <b>objects</b> <b>in</b> the candidate class list. However, current OVOD models are suffering from overfitting on the base classes, heavily relying on the large-scale extra data, and complex training process. To overcome these issues, we propose a novel framework with Meta <b>prompt</b> and Instance <b>Contrastive</b> <b>learning</b> (MIC) schemes. Firstly, we simulate a novel-class-emerging scenario to help the <b>prompt</b> learner that learns class and background <b>prompts</b> generalize to novel classes. Secondly, we design an instance-level <b>contrastive</b> <b>strategy</b> to promote intra-class compactness and inter-class separation, which benefits generalization of the detector to novel class <b>objects.</b> <b>Without</b> using <b>knowledge</b> <b>distillation,</b> ensemble model or extra training data during detector training, our proposed MIC outperforms previous SOTA methods trained with these complex techniques on LVIS. Most importantly, MIC shows great generalization ability on novel classes, e.g., with $+4.3%$ and $+1.9% \ \mathrm{AP}$ improvement compared with previous SOTA on COCO and Objects365, respectively.</p></p class="citation"></blockquote><h3 id=21101--21320-xcoop-explainable-prompt-learning-for-computer-aided-diagnosis-via-concept-guided-context-optimization-yequan-bie-et-al-2024>(21/101 | 21/320) XCoOp: Explainable Prompt Learning for Computer-Aided Diagnosis via Concept-guided Context Optimization (Yequan Bie et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yequan Bie, Luyang Luo, Zhixuan Chen, Hao Chen. (2024)<br><strong>XCoOp: Explainable Prompt Learning for Computer-Aided Diagnosis via Concept-guided Context Optimization</strong><br><button class=copy-to-clipboard title="XCoOp: Explainable Prompt Learning for Computer-Aided Diagnosis via Concept-guided Context Optimization" index=21>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-21 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Foundation Model, Large Language Model, Prompt, Prompt Learning, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09410v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09410v1.pdf filename=2403.09410v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Utilizing potent representations of the <b>large</b> <b>vision-language</b> <b>models</b> (VLMs) to accomplish various downstream tasks has attracted increasing attention. Within this research field, soft <b>prompt</b> <b>learning</b> has become a representative approach for efficiently adapting VLMs such as CLIP, to tasks like image classification. However, most existing <b>prompt</b> <b>learning</b> methods learn text tokens that are unexplainable, which cannot satisfy the stringent interpretability requirements of Explainable Artificial Intelligence (XAI) in high-stakes scenarios like healthcare. To address this issue, we propose a novel explainable <b>prompt</b> <b>learning</b> framework that leverages medical knowledge by aligning the semantics of images, learnable <b>prompts,</b> <b>and</b> clinical concept-driven <b>prompts</b> <b>at</b> multiple granularities. Moreover, our framework addresses the lack of valuable concept annotations by eliciting knowledge from <b>large</b> <b>language</b> <b>models</b> and offers both visual and textual explanations for the <b>prompts.</b> <b>Extensive</b> experiments and explainability analyses conducted on various datasets, with and without concept labels, demonstrate that our method simultaneously achieves superior diagnostic performance, flexibility, and interpretability, shedding light on the effectiveness of <b>foundation</b> <b>models</b> in facilitating XAI. The code will be made publically available.</p></p class="citation"></blockquote><h3 id=22101--22320-video-editing-via-factorized-diffusion-distillation-uriel-singer-et-al-2024>(22/101 | 22/320) Video Editing via Factorized Diffusion Distillation (Uriel Singer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Uriel Singer, Amit Zohar, Yuval Kirstain, Shelly Sheynin, Adam Polyak, Devi Parikh, Yaniv Taigman. (2024)<br><strong>Video Editing via Factorized Diffusion Distillation</strong><br><button class=copy-to-clipboard title="Video Editing via Factorized Diffusion Distillation" index=22>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-22 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Knowledge Distillation, Knowledge Distillation, Supervised Learning, Unsupervised Learning, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09334v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09334v1.pdf filename=2403.09334v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce Emu Video Edit (EVE), a model that establishes a new state-of-the art in video editing without relying on any <b>supervised</b> video editing data. To develop EVE we separately train an image editing adapter and a video generation adapter, and attach both to the same <b>text-to-image</b> model. Then, to align the adapters towards video editing we introduce a new <b>unsupervised</b> <b>distillation</b> procedure, Factorized Diffusion <b>Distillation.</b> This procedure <b>distills</b> knowledge from one or more teachers simultaneously, without any <b>supervised</b> data. We utilize this procedure to teach EVE to edit videos by jointly <b>distilling</b> knowledge to (i) precisely edit each individual frame from the image editing adapter, and (ii) ensure temporal consistency among the edited frames using the video generation adapter. Finally, to demonstrate the potential of our approach in unlocking other capabilities, we align additional combinations of adapters</p></p class="citation"></blockquote><h3 id=23101--23320-explore-in-context-segmentation-via-latent-diffusion-models-chaoyang-wang-et-al-2024>(23/101 | 23/320) Explore In-Context Segmentation via Latent Diffusion Models (Chaoyang Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chaoyang Wang, Xiangtai Li, Henghui Ding, Lu Qi, Jiangning Zhang, Yunhai Tong, Chen Change Loy, Shuicheng Yan. (2024)<br><strong>Explore In-Context Segmentation via Latent Diffusion Models</strong><br><button class=copy-to-clipboard title="Explore In-Context Segmentation via Latent Diffusion Models" index=23>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-23 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 43<br>Keywords: Diffusion Model, Benchmarking, Foundation Model, In-context Learning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09616v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09616v1.pdf filename=2403.09616v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>In-context</b> segmentation has drawn more attention with the introduction of vision <b>foundation</b> <b>models.</b> Most existing approaches adopt metric learning or masked image modeling to build the correlation between visual <b>prompts</b> and input image queries. In this work, we explore this problem from a new perspective, using one representative generation model, the latent <b>diffusion</b> <b>model</b> (LDM). We observe a task gap between generation and segmentation in <b>diffusion</b> <b>models,</b> but LDM is still an effective minimalist for <b>in-context</b> segmentation. In particular, we propose two meta-architectures and correspondingly design several output alignment and optimization strategies. We have conducted comprehensive ablation studies and empirically found that the segmentation quality counts on output alignment and <b>in-context</b> instructions. Moreover, we build a new and fair <b>in-context</b> segmentation <b>benchmark</b> that includes both image and video datasets. Experiments validate the efficiency of our approach, demonstrating comparable or even stronger results than previous specialist models or visual <b>foundation</b> <b>models.</b> Our study shows that LDMs can also achieve good enough results for challenging <b>in-context</b> segmentation tasks.</p></p class="citation"></blockquote><h3 id=24101--24320-semi--and-weakly-supervised-learning-for-mammogram-mass-segmentation-with-limited-annotations-xinyu-xiong-et-al-2024>(24/101 | 24/320) Semi- and Weakly-Supervised Learning for Mammogram Mass Segmentation with Limited Annotations (Xinyu Xiong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinyu Xiong, Churan Wang, Wenxue Li, Guanbin Li. (2024)<br><strong>Semi- and Weakly-Supervised Learning for Mammogram Mass Segmentation with Limited Annotations</strong><br><button class=copy-to-clipboard title="Semi- and Weakly-Supervised Learning for Mammogram Mass Segmentation with Limited Annotations" index=24>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-24 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Weakly-supervised Learning, Weakly-supervised Learning, Prompt, Weakly Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09315v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09315v1.pdf filename=2403.09315v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Accurate identification of breast masses is crucial in diagnosing breast cancer; however, it can be challenging due to their small size and being camouflaged in surrounding normal glands. Worse still, it is also expensive in clinical practice to obtain adequate pixel-wise annotations for training deep neural networks. To overcome these two difficulties with one stone, we propose a semi- and <b>weakly-supervised</b> <b>learning</b> <b>framework</b> for mass segmentation that utilizes limited strongly-labeled samples and sufficient <b>weakly-labeled</b> <b>samples</b> <b>to</b> achieve satisfactory performance. The framework consists of an auxiliary branch to exclude lesion-irrelevant background areas, a segmentation branch for final prediction, and a spatial <b>prompting</b> module to integrate the complementary information of the two branches. We further disentangle encoded obscure features into lesion-related and others to boost performance. Experiments on CBIS-DDSM and INbreast datasets demonstrate the effectiveness of our method.</p></p class="citation"></blockquote><h3 id=25101--25320-knowledge-distillation-in-yolox-vit-for-side-scan-sonar-object-detection-martin-aubard-et-al-2024>(25/101 | 25/320) Knowledge Distillation in YOLOX-ViT for Side-Scan Sonar Object Detection (Martin Aubard et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Martin Aubard, László Antal, Ana Madureira, Erika Ábrahám. (2024)<br><strong>Knowledge Distillation in YOLOX-ViT for Side-Scan Sonar Object Detection</strong><br><button class=copy-to-clipboard title="Knowledge Distillation in YOLOX-ViT for Side-Scan Sonar Object Detection" index=25>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-25 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Object Detection, Knowledge Distillation, Knowledge Distillation, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09313v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09313v1.pdf filename=2403.09313v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper we present YOLOX-ViT, a novel <b>object</b> <b>detection</b> model, and investigate the efficacy of <b>knowledge</b> <b>distillation</b> for model size reduction without sacrificing performance. Focused on underwater robotics, our research addresses key questions about the viability of smaller models and the impact of the visual <b>transformer</b> layer in YOLOX. Furthermore, we introduce a new side-scan sonar image dataset, and use it to evaluate our <b>object</b> <b>detector&rsquo;s</b> performance. Results show that <b>knowledge</b> <b>distillation</b> effectively reduces false positives in wall detection. Additionally, the introduced visual <b>transformer</b> layer significantly improves <b>object</b> <b>detection</b> accuracy in the underwater environment. The source code of the <b>knowledge</b> <b>distillation</b> in the YOLOX-ViT is at <a href=https://github.com/remaro-network/KD-YOLOX-ViT>https://github.com/remaro-network/KD-YOLOX-ViT</a>.</p></p class="citation"></blockquote><h3 id=26101--26320-annotation-free-semantic-segmentation-with-vision-foundation-models-soroush-seifi-et-al-2024>(26/101 | 26/320) Annotation Free Semantic Segmentation with Vision Foundation Models (Soroush Seifi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Soroush Seifi, Daniel Olmeda Reino, Fabien Despinoy, Rahaf Aljundi. (2024)<br><strong>Annotation Free Semantic Segmentation with Vision Foundation Models</strong><br><button class=copy-to-clipboard title="Annotation Free Semantic Segmentation with Vision Foundation Models" index=26>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-26 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Foundation Model, Self-supervised Learning, Zero-shot, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09307v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09307v1.pdf filename=2403.09307v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Semantic Segmentation is one of the most challenging vision tasks, usually requiring large amounts of training data with expensive pixel-level annotations. With the success of <b>foundation</b> <b>models</b> and especially <b>vision-language</b> models, recent works attempt to achieve <b>zero-shot</b> semantic segmentation while requiring either large scale training or additional image/pixel-level annotations. In this work, we build a lightweight module on top of a <b>self-supervised</b> pretrained vision encoder to align patch features with a pre-trained text encoder. Importantly, we generate free annotations for any semantic segmentation dataset using existing <b>foundation</b> <b>models</b> and train our alignment module cost free. We use CLIP to detect objects and SAM to generate high quality object masks. Our approach can bring language-based semantics to any pre-trained vision encoder with minimal training. Our module is lightweight, uses <b>foundation</b> <b>models</b> as a sole source of supervision and shows impressive generalization capability from little training data with no annotation.</p></p class="citation"></blockquote><h3 id=27101--27320-customizing-segmentation-foundation-model-via-prompt-learning-for-instance-segmentation-hyung-il-kim-et-al-2024>(27/101 | 27/320) Customizing Segmentation Foundation Model via Prompt Learning for Instance Segmentation (Hyung-Il Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hyung-Il Kim, Kimin Yun, Jun-Seok Yun, Yuseok Bae. (2024)<br><strong>Customizing Segmentation Foundation Model via Prompt Learning for Instance Segmentation</strong><br><button class=copy-to-clipboard title="Customizing Segmentation Foundation Model via Prompt Learning for Instance Segmentation" index=27>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-27 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Foundation Model, Pre-trained Language Model, Prompt, Prompt Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09199v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09199v1.pdf filename=2403.09199v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, <b>foundation</b> <b>models</b> trained on massive datasets to adapt to a wide range of domains have attracted considerable attention and are actively being explored within the computer vision community. Among these, the Segment Anything Model (SAM) stands out for its remarkable progress in generalizability and flexibility for image segmentation tasks, achieved through <b>prompt-based</b> <b>object</b> mask generation. However, despite its strength, SAM faces two key limitations when applied to customized instance segmentation that segments specific objects or those in unique environments not typically present in the training data: 1) the ambiguity inherent in input <b>prompts</b> <b>and</b> 2) the necessity for extensive additional training to achieve optimal segmentation. To address these challenges, we propose a novel method, customized instance segmentation via <b>prompt</b> <b>learning</b> tailored to SAM. Our method involves a <b>prompt</b> <b>learning</b> module <b>(PLM),</b> which adjusts input <b>prompts</b> <b>into</b> the embedding space to better align with user intentions, thereby enabling more efficient training. Furthermore, we introduce a point matching module (PMM) to enhance the feature representation for finer segmentation by ensuring detailed alignment with ground truth boundaries. Experimental results on various customized instance segmentation scenarios demonstrate the effectiveness of the proposed method.</p></p class="citation"></blockquote><h3 id=28101--28320-sam-lightening-a-lightweight-segment-anything-model-with-dilated-flash-attention-to-achieve-30-times-acceleration-yanfei-song-et-al-2024>(28/101 | 28/320) SAM-Lightening: A Lightweight Segment Anything Model with Dilated Flash Attention to Achieve 30 times Acceleration (Yanfei Song et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yanfei Song, Bangzheng Pu, Peng Wang, Hongxu Jiang, Dong Dong, Yongxiang Cao, Yiqing Shen. (2024)<br><strong>SAM-Lightening: A Lightweight Segment Anything Model with Dilated Flash Attention to Achieve 30 times Acceleration</strong><br><button class=copy-to-clipboard title="SAM-Lightening: A Lightweight Segment Anything Model with Dilated Flash Attention to Achieve 30 times Acceleration" index=28>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-28 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Knowledge Distillation, Knowledge Distillation, Knowledge Transfer, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09195v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09195v2.pdf filename=2403.09195v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Segment Anything Model (SAM) has garnered significant attention in segmentation tasks due to their <b>zero-shot</b> generalization ability. However, a broader application of SAMs to real-world practice has been restricted by their low inference speed and high computational memory demands, which mainly stem from the attention mechanism. Existing work concentrated on optimizing the encoder, yet has not adequately addressed the inefficiency of the attention mechanism itself, even when <b>distilled</b> to a smaller model, which thus leaves space for further improvement. In response, we introduce SAM-Lightening, a variant of SAM, that features a re-engineered attention mechanism, termed Dilated Flash Attention. It not only facilitates higher parallelism, enhancing processing efficiency but also retains compatibility with the existing FlashAttention. Correspondingly, we propose a progressive <b>distillation</b> to enable an efficient <b>knowledge</b> <b>transfer</b> from the vanilla SAM without costly training from scratch. Experiments on COCO and LVIS reveal that SAM-Lightening significantly outperforms the state-of-the-art methods in both run-time efficiency and segmentation accuracy. Specifically, it can achieve an inference speed of 7 milliseconds (ms) per image, for images of size 1024*1024 pixels, which is 30.1 times faster than the vanilla SAM and 2.1 times than the state-of-the-art. Moreover, it takes only 244MB memory, which is 3.5% of the vanilla SAM. The code and weights are available at <a href=https://anonymous.4open.science/r/SAM-LIGHTENING-BC25/>https://anonymous.4open.science/r/SAM-LIGHTENING-BC25/</a>.</p></p class="citation"></blockquote><h3 id=29101--29320-pyra-parallel-yielding-re-activation-for-training-inference-efficient-task-adaptation-yizhe-xiong-et-al-2024>(29/101 | 29/320) PYRA: Parallel Yielding Re-Activation for Training-Inference Efficient Task Adaptation (Yizhe Xiong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yizhe Xiong, Hui Chen, Tianxiang Hao, Zijia Lin, Jungong Han, Yuesong Zhang, Guoxin Wang, Yongjun Bao, Guiguang Ding. (2024)<br><strong>PYRA: Parallel Yielding Re-Activation for Training-Inference Efficient Task Adaptation</strong><br><button class=copy-to-clipboard title="PYRA: Parallel Yielding Re-Activation for Training-Inference Efficient Task Adaptation" index=29>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-29 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Fine-tuning, Foundation Model, Model Compression, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09192v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09192v1.pdf filename=2403.09192v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, the scale of <b>transformers</b> has grown rapidly, which introduces considerable challenges in terms of training overhead and inference efficiency in the scope of task adaptation. Existing works, namely Parameter-Efficient <b>Fine-Tuning</b> (PEFT) and <b>model</b> <b>compression,</b> have separately investigated the challenges. However, PEFT cannot guarantee the inference efficiency of the original backbone, especially for large-scale <b>models.</b> <b>Model</b> <b>compression</b> requires significant training costs for structure searching and re-training. Consequently, a simple combination of them cannot guarantee accomplishing both training efficiency and inference efficiency with minimal costs. In this paper, we propose a novel Parallel Yielding Re-Activation (PYRA) method for such a challenge of training-inference efficient task adaptation. PYRA first utilizes parallel yielding adaptive weights to comprehensively perceive the data distribution in downstream tasks. A re-activation strategy for token modulation is then applied for tokens to be merged, leading to calibrated token features. Extensive experiments demonstrate that PYRA outperforms all competing methods under both low compression rate and high compression rate, demonstrating its effectiveness and superiority in maintaining both training efficiency and inference efficiency for large-scale <b>foundation</b> <b>models.</b> <b>Our</b> code will be released to the public.</p></p class="citation"></blockquote><h3 id=30101--30320-cardiocaps-attention-based-capsule-network-for-class-imbalanced-echocardiogram-classification-hyunkyung-han-et-al-2024>(30/101 | 30/320) CardioCaps: Attention-based Capsule Network for Class-Imbalanced Echocardiogram Classification (Hyunkyung Han et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hyunkyung Han, Jihyeon Seong, Jaesik Choi. (2024)<br><strong>CardioCaps: Attention-based Capsule Network for Class-Imbalanced Echocardiogram Classification</strong><br><button class=copy-to-clipboard title="CardioCaps: Attention-based Capsule Network for Class-Imbalanced Echocardiogram Classification" index=30>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-30 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, Logistic Regression<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09108v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09108v2.pdf filename=2403.09108v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Capsule Neural Networks (CapsNets) is a novel architecture that utilizes vector-wise representations formed by multiple neurons. Specifically, the Dynamic Routing CapsNets (DR-CapsNets) employ an affine matrix and dynamic routing mechanism to train capsules and acquire translation-equivariance properties, enhancing its robustness compared to traditional <b>Convolutional</b> <b>Neural</b> <b>Networks</b> <b>(CNNs).</b> Echocardiograms, which capture moving images of the heart, present unique challenges for traditional image classification methods. In this paper, we explore the potential of DR-CapsNets and propose CardioCaps, a novel attention-based DR-CapsNet architecture for class-imbalanced echocardiogram classification. CardioCaps comprises two key components: a weighted margin loss incorporating a regression auxiliary loss and an attention mechanism. First, the weighted margin loss prioritizes positive cases, supplemented by an auxiliary loss function based on the Ejection Fraction (EF) regression task, a crucial measure of cardiac function. This approach enhances the model&rsquo;s resilience in the face of class imbalance. Second, recognizing the quadratic complexity of dynamic routing leading to training inefficiencies, we adopt the attention mechanism as a more computationally efficient alternative. Our results demonstrate that CardioCaps surpasses traditional machine learning baseline methods, including <b>Logistic</b> <b>Regression,</b> Random Forest, and XGBoost with sampling methods and a class weight matrix. Furthermore, CardioCaps outperforms other deep learning baseline methods such as <b>CNNs,</b> ResNets, U-Nets, and ViTs, as well as advanced CapsNets methods such as EM-CapsNets and Efficient-CapsNets. Notably, our model demonstrates robustness to class imbalance, achieving high precision even in datasets with a substantial proportion of negative cases.</p></p class="citation"></blockquote><h3 id=31101--31320-adaptive-hybrid-masking-strategy-for-privacy-preserving-face-recognition-against-model-inversion-attack-yuanqing-huang-et-al-2024>(31/101 | 31/320) Adaptive Hybrid Masking Strategy for Privacy-Preserving Face Recognition Against Model Inversion Attack (Yuanqing Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuanqing Huang, Yinggui Wang, Jianshu Li, Le Yang, Kai Song, Lei Wang. (2024)<br><strong>Adaptive Hybrid Masking Strategy for Privacy-Preserving Face Recognition Against Model Inversion Attack</strong><br><button class=copy-to-clipboard title="Adaptive Hybrid Masking Strategy for Privacy-Preserving Face Recognition Against Model Inversion Attack" index=31>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-31 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CR, cs-CV, cs-LG, cs.CV<br>Keyword Score: 40<br>Keywords: Face Recognition, Data Augmentation, Reinforcement Learning, Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10558v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10558v1.pdf filename=2403.10558v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The utilization of personal sensitive <b>data</b> <b>in</b> training <b>face</b> <b>recognition</b> (FR) models poses significant privacy concerns, as adversaries can employ model inversion attacks (MIA) to infer the original training <b>data.</b> <b>Existing</b> defense methods, such as <b>data</b> <b>augmentation</b> and <b>differential</b> <b>privacy,</b> have been employed to mitigate this issue. However, these methods often fail to strike an optimal balance between privacy and accuracy. To address this limitation, this paper introduces an adaptive hybrid masking algorithm against MIA. Specifically, <b>face</b> <b>images</b> are masked in the frequency domain using an adaptive MixUp strategy. Unlike the traditional MixUp algorithm, which is predominantly used for <b>data</b> <b>augmentation,</b> our modified approach incorporates frequency domain mixing. Previous studies have shown that increasing the number of images mixed in MixUp can enhance privacy preservation but at the expense of reduced <b>face</b> <b>recognition</b> accuracy. To overcome this trade-off, we develop an enhanced adaptive MixUp strategy based on <b>reinforcement</b> <b>learning,</b> which enables us to mix a larger number of images while maintaining satisfactory recognition accuracy. To optimize privacy protection, we propose maximizing the reward function (i.e., the loss function of the FR system) during the training of the strategy network. While the loss function of the FR network is minimized in the phase of training the FR network. The strategy network and the <b>face</b> <b>recognition</b> network can be viewed as antagonistic entities in the training process, ultimately reaching a more balanced trade-off. Experimental results demonstrate that our proposed hybrid masking scheme outperforms existing defense algorithms in terms of privacy preservation and recognition accuracy against MIA.</p></p class="citation"></blockquote><h3 id=32101--32320-opengraph-open-vocabulary-hierarchical-3d-graph-representation-in-large-scale-outdoor-environments-yinan-deng-et-al-2024>(32/101 | 32/320) OpenGraph: Open-Vocabulary Hierarchical 3D Graph Representation in Large-Scale Outdoor Environments (Yinan Deng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yinan Deng, Jiahui Wang, Jingyu Zhao, Xinyu Tian, Guangyan Chen, Yi Yang, Yufeng Yue. (2024)<br><strong>OpenGraph: Open-Vocabulary Hierarchical 3D Graph Representation in Large-Scale Outdoor Environments</strong><br><button class=copy-to-clipboard title="OpenGraph: Open-Vocabulary Hierarchical 3D Graph Representation in Large-Scale Outdoor Environments" index=32>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-32 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-RO, cs.CV<br>Keyword Score: 39<br>Keywords: Graph, Fine-tuning, Foundation Model, Multi-modal, Multi-modal, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09412v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09412v1.pdf filename=2403.09412v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Environment maps endowed with sophisticated semantics are pivotal for facilitating seamless interaction between robots and humans, enabling them to effectively carry out various tasks. Open-vocabulary maps, powered by Visual-Language models (VLMs), possess inherent advantages, including <b>multimodal</b> retrieval and open-set classes. However, existing open-vocabulary maps are constrained to closed indoor scenarios and VLM features, thereby diminishing their usability and inference capabilities. Moreover, the absence of topological relationships further complicates the accurate querying of specific instances. In this work, we propose OpenGraph, a representation of open-vocabulary hierarchical <b>graph</b> structure designed for large-scale outdoor environments. OpenGraph initially extracts instances and their captions from visual images using 2D <b>foundation</b> <b>models,</b> encoding the captions with features to enhance textual <b>reasoning.</b> Subsequently, 3D incremental panoramic mapping with feature embedding is achieved by projecting images onto LiDAR point clouds. Finally, the environment is segmented based on lane <b>graph</b> connectivity to construct a hierarchical <b>graph.</b> Validation results from real public dataset SemanticKITTI demonstrate that, even without <b>fine-tuning</b> the models, OpenGraph exhibits the ability to generalize to novel semantic classes and achieve the highest segmentation and query accuracy. The source code of OpenGraph is publicly available at <a href=https://github.com/BIT-DYN/OpenGraph>https://github.com/BIT-DYN/OpenGraph</a>.</p></p class="citation"></blockquote><h3 id=33101--33320-avibench-towards-evaluating-the-robustness-of-large-vision-language-model-on-adversarial-visual-instructions-hao-zhang-et-al-2024>(33/101 | 33/320) AVIBench: Towards Evaluating the Robustness of Large Vision-Language Model on Adversarial Visual-Instructions (Hao Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hao Zhang, Wenqi Shao, Hong Liu, Yongqiang Ma, Ping Luo, Yu Qiao, Kaipeng Zhang. (2024)<br><strong>AVIBench: Towards Evaluating the Robustness of Large Vision-Language Model on Adversarial Visual-Instructions</strong><br><button class=copy-to-clipboard title="AVIBench: Towards Evaluating the Robustness of Large Vision-Language Model on Adversarial Visual-Instructions" index=33>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-33 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 39<br>Keywords: Benchmarking, Fairness, Multi-modal, Multi-modal, GPT, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09346v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09346v1.pdf filename=2403.09346v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Large <b>Vision-Language</b> Models (LVLMs) have shown significant progress in well responding to visual-instructions from users. However, these instructions, encompassing images and text, are susceptible to both intentional and inadvertent attacks. Despite the critical importance of LVLMs&rsquo; robustness against such threats, current research in this area remains limited. To bridge this gap, we introduce AVIBench, a framework designed to analyze the robustness of LVLMs when facing various adversarial visual-instructions (AVIs), including four types of image-based AVIs, ten types of text-based AVIs, and nine types of content bias AVIs (such as gender, violence, cultural, and racial biases, among others). We generate 260K AVIs encompassing five categories of <b>multimodal</b> capabilities (nine tasks) and content bias. We then conduct a comprehensive evaluation involving 14 open-source LVLMs to assess their performance. AVIBench also serves as a convenient tool for practitioners to evaluate the robustness of LVLMs against AVIs. Our findings and extensive experimental results shed light on the vulnerabilities of LVLMs, and highlight that inherent biases exist even in advanced closed-source LVLMs like GeminiProVision and <b>GPT-4V.</b> This underscores the importance of enhancing the robustness, security, and <b>fairness</b> of LVLMs. The source code and <b>benchmark</b> will be made publicly available.</p></p class="citation"></blockquote><h3 id=34101--34320-attention-based-class-conditioned-alignment-for-multi-source-domain-adaptive-object-detection-atif-belal-et-al-2024>(34/101 | 34/320) Attention-based Class-Conditioned Alignment for Multi-Source Domain Adaptive Object Detection (Atif Belal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Atif Belal, Akhil Meethal, Francisco Perdigon Romero, Marco Pedersoli, Eric Granger. (2024)<br><strong>Attention-based Class-Conditioned Alignment for Multi-Source Domain Adaptive Object Detection</strong><br><button class=copy-to-clipboard title="Attention-based Class-Conditioned Alignment for Multi-Source Domain Adaptive Object Detection" index=34>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-34 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 36<br>Keywords: Object Detection, Benchmarking, Benchmarking, Distribution Shift, Distribution Shift, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09918v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09918v1.pdf filename=2403.09918v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Domain</b> <b>adaptation</b> methods for <b>object</b> <b>detection</b> (OD) strive to mitigate the impact of <b>distribution</b> <b>shifts</b> by promoting feature alignment across source and target <b>domains.</b> <b>Multi-source</b> <b>domain</b> <b>adaptation</b> (MSDA) allows leveraging multiple annotated source datasets, and unlabeled target data to improve the accuracy and robustness of the detection model. Most state-of-the-art MSDA methods for OD perform feature alignment in a class-agnostic manner. This is challenging since the <b>objects</b> <b>have</b> unique modal information due to variations in <b>object</b> <b>appearance</b> across <b>domains.</b> <b>A</b> recent prototype-based approach proposed a class-wise alignment, yet it suffers from error accumulation due to noisy pseudo-labels which can negatively affect adaptation with imbalanced data. To overcome these limitations, we propose an attention-based class-conditioned alignment scheme for MSDA that aligns instances of each <b>object</b> <b>category</b> across <b>domains.</b> <b>In</b> particular, an attention module coupled with an adversarial <b>domain</b> <b>classifier</b> allows learning <b>domain-invariant</b> <b>and</b> class-specific instance representations. Experimental results on multiple <b>benchmarking</b> MSDA datasets indicate that our method outperforms the state-of-the-art methods and is robust to class imbalance. Our code is available at <a href=https://github.com/imatif17/ACIA>https://github.com/imatif17/ACIA</a>.</p></p class="citation"></blockquote><h3 id=35101--35320-gazemotion-gaze-guided-human-motion-forecasting-zhiming-hu-et-al-2024>(35/101 | 35/320) GazeMotion: Gaze-guided Human Motion Forecasting (Zhiming Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhiming Hu, Syn Schmitt, Daniel Haeufle, Andreas Bulling. (2024)<br><strong>GazeMotion: Gaze-guided Human Motion Forecasting</strong><br><button class=copy-to-clipboard title="GazeMotion: Gaze-guided Human Motion Forecasting" index=35>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-35 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 36<br>Keywords: Graph Convolutional Network, Graph, Benchmarking, Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09885v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09885v1.pdf filename=2403.09885v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present GazeMotion, a novel method for human motion forecasting that combines information on past human poses with human eye gaze. Inspired by evidence from behavioural sciences showing that human eye and body movements are closely coordinated, GazeMotion first predicts future eye gaze from past gaze, then fuses predicted future gaze and past poses into a gaze-pose <b>graph,</b> <b>and</b> <b>finally</b> uses a residual <b>graph</b> <b>convolutional</b> <b>network</b> to forecast body motion. We extensively evaluate our method on the MoGaze, ADT, and GIMO <b>benchmark</b> datasets and show that it outperforms state-of-the-art methods by up to 7.4% improvement in mean per joint position error. Using head direction as a proxy to gaze, our method still achieves an average improvement of 5.5%. We finally report an online user study showing that our method also outperforms prior methods in terms of perceived realism. These results show the significant information content available in eye gaze for human motion forecasting as well as the effectiveness of our method in exploiting this information.</p></p class="citation"></blockquote><h3 id=36101--36320-images-are-achilles-heel-of-alignment-exploiting-visual-vulnerabilities-for-jailbreaking-multimodal-large-language-models-yifan-li-et-al-2024>(36/101 | 36/320) Images are Achilles&rsquo; Heel of Alignment: Exploiting Visual Vulnerabilities for Jailbreaking Multimodal Large Language Models (Yifan Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yifan Li, Hangyu Guo, Kun Zhou, Wayne Xin Zhao, Ji-Rong Wen. (2024)<br><strong>Images are Achilles&rsquo; Heel of Alignment: Exploiting Visual Vulnerabilities for Jailbreaking Multimodal Large Language Models</strong><br><button class=copy-to-clipboard title="Images are Achilles' Heel of Alignment: Exploiting Visual Vulnerabilities for Jailbreaking Multimodal Large Language Models" index=36>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-36 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CL, cs-CV, cs.CV<br>Keyword Score: 36<br>Keywords: Multi-modal, Multi-modal, Gemini, Automatic Speech Recognition, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09792v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09792v1.pdf filename=2403.09792v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we study the harmlessness alignment problem of <b>multimodal</b> <b>large</b> <b>language</b> <b>models~(MLLMs).</b> We conduct a systematic empirical analysis of the harmlessness performance of representative MLLMs and reveal that the image input poses the alignment vulnerability of MLLMs. Inspired by this, we propose a novel jailbreak method named HADES, which hides and amplifies the harmfulness of the malicious intent within the text input, using meticulously crafted images. Experimental results show that HADES can effectively jailbreak existing MLLMs, which achieves an average Attack Success Rate~(ASR) of 90.26% for LLaVA-1.5 and 71.60% for <b>Gemini</b> Pro Vision. Our code and data will be publicly released.</p></p class="citation"></blockquote><h3 id=37101--37320-unsupervised-modality-transferable-video-highlight-detection-with-representation-activation-sequence-learning-tingtian-li-et-al-2024>(37/101 | 37/320) Unsupervised Modality-Transferable Video Highlight Detection with Representation Activation Sequence Learning (Tingtian Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tingtian Li, Zixun Sun, Xinyu Xiao. (2024)<br><strong>Unsupervised Modality-Transferable Video Highlight Detection with Representation Activation Sequence Learning</strong><br><button class=copy-to-clipboard title="Unsupervised Modality-Transferable Video Highlight Detection with Representation Activation Sequence Learning" index=37>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-37 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 36<br>Keywords: Contrastive Learning, Multi-modal, Multi-modal, Supervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09401v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09401v2.pdf filename=2403.09401v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Identifying highlight moments of raw video materials is crucial for improving the efficiency of editing videos that are pervasive on internet platforms. However, the extensive work of manually labeling footage has created obstacles to applying <b>supervised</b> methods to videos of unseen categories. The absence of an audio modality that contains valuable cues for highlight detection in many videos also makes it difficult to use <b>multimodal</b> strategies. In this paper, we propose a novel model with cross-modal perception for <b>unsupervised</b> highlight detection. The proposed model learns representations with visual-audio level semantics from image-audio pair data via a self-reconstruction task. To achieve <b>unsupervised</b> highlight detection, we investigate the latent representations of the network and propose the representation activation sequence learning (RASL) module with k-point <b>contrastive</b> <b>learning</b> to learn significant representation activations. To connect the visual modality with the audio modality, we use the symmetric <b>contrastive</b> <b>learning</b> (SCL) module to learn the paired visual and audio representations. Furthermore, an auxiliary task of masked feature vector sequence (FVS) reconstruction is simultaneously conducted during pretraining for representation enhancement. During inference, the cross-modal pretrained model can generate representations with paired visual-audio semantics given only the visual modality. The RASL module is used to output the highlight scores. The experimental results show that the proposed framework achieves superior performance compared to other state-of-the-art approaches.</p></p class="citation"></blockquote><h3 id=38101--38320-introducing-routing-functions-to-vision-language-parameter-efficient-fine-tuning-with-low-rank-bottlenecks-tingyu-qu-et-al-2024>(38/101 | 38/320) Introducing Routing Functions to Vision-Language Parameter-Efficient Fine-Tuning with Low-Rank Bottlenecks (Tingyu Qu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tingyu Qu, Tinne Tuytelaars, Marie-Francine Moens. (2024)<br><strong>Introducing Routing Functions to Vision-Language Parameter-Efficient Fine-Tuning with Low-Rank Bottlenecks</strong><br><button class=copy-to-clipboard title="Introducing Routing Functions to Vision-Language Parameter-Efficient Fine-Tuning with Low-Rank Bottlenecks" index=38>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-38 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 36<br>Keywords: Fine-tuning, Multi-modal, Multi-modal, GPT-2, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09377v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09377v1.pdf filename=2403.09377v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Mainstream parameter-efficient <b>fine-tuning</b> (PEFT) methods, such as LoRA or Adapter, project a model&rsquo;s hidden states to a lower dimension, allowing pre-trained models to adapt to new data through this low-rank bottleneck. However, PEFT tasks involving multiple modalities, like <b>vision-language</b> (VL) tasks, require not only adaptation to new data but also learning the relationship between different modalities. Targeting at VL PEFT tasks, we propose a family of operations, called routing functions, to enhance VL alignment in the low-rank bottlenecks. The routing functions adopt linear operations and do not introduce new trainable parameters. In-depth analyses are conducted to study their behavior. In various VL PEFT settings, the routing functions significantly improve performance of the original PEFT methods, achieving over 20% improvement on VQAv2 ($\text{RoBERTa}_{\text{large}}$+ViT-L/16) and 30% on COCO Captioning <b>(GPT2-medium+ViT-L/16).</b> Also when <b>fine-tuning</b> a pre-trained <b>multimodal</b> model such as CLIP-BART, we observe smaller but consistent improvements across a range of VL PEFT tasks.</p></p class="citation"></blockquote><h3 id=39101--39320-groupcontrast-semantic-aware-self-supervised-representation-learning-for-3d-understanding-chengyao-wang-et-al-2024>(39/101 | 39/320) GroupContrast: Semantic-aware Self-supervised Representation Learning for 3D Understanding (Chengyao Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chengyao Wang, Li Jiang, Xiaoyang Wu, Zhuotao Tian, Bohao Peng, Hengshuang Zhao, Jiaya Jia. (2024)<br><strong>GroupContrast: Semantic-aware Self-supervised Representation Learning for 3D Understanding</strong><br><button class=copy-to-clipboard title="GroupContrast: Semantic-aware Self-supervised Representation Learning for 3D Understanding" index=39>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-39 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 35<br>Keywords: Contrastive Learning, Representation Learning, Self-supervised Learning, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09639v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09639v1.pdf filename=2403.09639v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Self-supervised</b> 3D <b>representation</b> <b>learning</b> aims to learn effective <b>representations</b> <b>from</b> large-scale unlabeled point clouds. Most existing approaches adopt point discrimination as the pretext task, which assigns matched points in two distinct views as positive pairs and unmatched points as negative pairs. However, this approach often results in semantically identical points having dissimilar <b>representations,</b> <b>leading</b> to a high number of false negatives and introducing a &ldquo;semantic conflict&rdquo; problem. To address this issue, we propose GroupContrast, a novel approach that combines segment grouping and semantic-aware <b>contrastive</b> <b>learning.</b> Segment grouping partitions points into semantically meaningful regions, which enhances semantic coherence and provides semantic guidance for the subsequent <b>contrastive</b> <b>representation</b> <b>learning.</b> Semantic-aware <b>contrastive</b> <b>learning</b> augments the semantic information extracted from segment grouping and helps to alleviate the issue of &ldquo;semantic conflict&rdquo;. We conducted extensive experiments on multiple 3D scene understanding tasks. The results demonstrate that GroupContrast learns semantically meaningful <b>representations</b> <b>and</b> achieves promising <b>transfer</b> <b>learning</b> performance.</p></p class="citation"></blockquote><h3 id=40101--40320-onetracker-unifying-visual-object-tracking-with-foundation-models-and-efficient-tuning-lingyi-hong-et-al-2024>(40/101 | 40/320) OneTracker: Unifying Visual Object Tracking with Foundation Models and Efficient Tuning (Lingyi Hong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lingyi Hong, Shilin Yan, Renrui Zhang, Wanyun Li, Xinyu Zhou, Pinxue Guo, Kaixun Jiang, Yiting Chen, Jinglun Li, Zhaoyu Chen, Wenqiang Zhang. (2024)<br><strong>OneTracker: Unifying Visual Object Tracking with Foundation Models and Efficient Tuning</strong><br><button class=copy-to-clipboard title="OneTracker: Unifying Visual Object Tracking with Foundation Models and Efficient Tuning" index=40>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-40 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Benchmarking, Fine-tuning, Foundation Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09634v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09634v1.pdf filename=2403.09634v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Visual object tracking aims to localize the target object of each frame based on its initial appearance in the first frame. Depending on the input modility, tracking tasks can be divided into RGB tracking and RGB+X (e.g. RGB+N, and RGB+D) tracking. Despite the different input modalities, the core aspect of tracking is the temporal matching. Based on this common ground, we present a general framework to unify various tracking tasks, termed as OneTracker. OneTracker first performs a large-scale pre-training on a RGB tracker called <b>Foundation</b> <b>Tracker.</b> This pretraining phase equips the <b>Foundation</b> <b>Tracker</b> with a stable ability to estimate the location of the target object. Then we regard other modality information as <b>prompt</b> and build <b>Prompt</b> Tracker upon <b>Foundation</b> <b>Tracker.</b> Through freezing the <b>Foundation</b> <b>Tracker</b> and only adjusting some additional trainable parameters, <b>Prompt</b> Tracker inhibits the strong localization ability from <b>Foundation</b> <b>Tracker</b> and achieves parameter-efficient <b>finetuning</b> on downstream RGB+X tracking tasks. To evaluate the effectiveness of our general framework OneTracker, which is consisted of <b>Foundation</b> <b>Tracker</b> and <b>Prompt</b> Tracker, we conduct extensive experiments on 6 popular tracking tasks across 11 <b>benchmarks</b> and our OneTracker outperforms other models and achieves state-of-the-art performance.</p></p class="citation"></blockquote><h3 id=41101--41320-promark-proactive-diffusion-watermarking-for-causal-attribution-vishal-asnani-et-al-2024>(41/101 | 41/320) ProMark: Proactive Diffusion Watermarking for Causal Attribution (Vishal Asnani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vishal Asnani, John Collomosse, Tu Bui, Xiaoming Liu, Shruti Agarwal. (2024)<br><strong>ProMark: Proactive Diffusion Watermarking for Causal Attribution</strong><br><button class=copy-to-clipboard title="ProMark: Proactive Diffusion Watermarking for Causal Attribution" index=41>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-41 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Diffusion Model, Generative AI, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09914v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09914v1.pdf filename=2403.09914v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Generative</b> <b>AI</b> (GenAI) is transforming creative workflows through the capability to synthesize and manipulate images via high-level <b>prompts.</b> Yet creatives are not well supported to receive recognition or reward for the use of their content in GenAI training. To this end, we propose ProMark, a causal attribution technique to attribute a synthetically generated image to its training data concepts like objects, motifs, templates, artists, or styles. The concept information is proactively embedded into the input training images using imperceptible watermarks, and the <b>diffusion</b> <b>models</b> (unconditional or conditional) are trained to retain the corresponding watermarks in generated images. We show that we can embed as many as $2^{16}$ unique watermarks into the training data, and each training image can contain more than one watermark. ProMark can maintain image quality whilst outperforming correlation-based attribution. Finally, several qualitative examples are presented, providing the confidence that the presence of the watermark conveys a causative relationship between training data and synthetic images.</p></p class="citation"></blockquote><h3 id=42101--42320-generalized-predictive-model-for-autonomous-driving-jiazhi-yang-et-al-2024>(42/101 | 42/320) Generalized Predictive Model for Autonomous Driving (Jiazhi Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiazhi Yang, Shenyuan Gao, Yihang Qiu, Li Chen, Tianyu Li, Bo Dai, Kashyap Chitta, Penghao Wu, Jia Zeng, Ping Luo, Jun Zhang, Andreas Geiger, Yu Qiao, Hongyang Li. (2024)<br><strong>Generalized Predictive Model for Autonomous Driving</strong><br><button class=copy-to-clipboard title="Generalized Predictive Model for Autonomous Driving" index=42>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-42 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Diffusion Model, Zero-shot, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09630v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09630v1.pdf filename=2403.09630v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we introduce the first large-scale video prediction model in the autonomous driving discipline. To eliminate the restriction of high-cost data collection and empower the generalization ability of our model, we acquire massive data from the web and pair it with diverse and high-quality text descriptions. The resultant dataset accumulates over 2000 hours of driving videos, spanning areas all over the world with diverse weather conditions and traffic scenarios. Inheriting the merits from recent latent <b>diffusion</b> <b>models,</b> our model, dubbed GenAD, handles the challenging dynamics in driving scenes with novel temporal <b>reasoning</b> blocks. We showcase that it can generalize to various unseen driving datasets in a <b>zero-shot</b> manner, surpassing general or driving-specific video prediction counterparts. Furthermore, GenAD can be adapted into an action-conditioned prediction model or a motion planner, holding great potential for real-world driving applications.</p></p class="citation"></blockquote><h3 id=43101--43320-video-mamba-suite-state-space-model-as-a-versatile-alternative-for-video-understanding-guo-chen-et-al-2024>(43/101 | 43/320) Video Mamba Suite: State Space Model as a Versatile Alternative for Video Understanding (Guo Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guo Chen, Yifei Huang, Jilan Xu, Baoqi Pei, Zhe Chen, Zhiqi Li, Jiahao Wang, Kunchang Li, Tong Lu, Limin Wang. (2024)<br><strong>Video Mamba Suite: State Space Model as a Versatile Alternative for Video Understanding</strong><br><button class=copy-to-clipboard title="Video Mamba Suite: State Space Model as a Versatile Alternative for Video Understanding" index=43>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-43 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Convolutional Neural Network, Recurrent Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09626v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09626v1.pdf filename=2403.09626v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Understanding videos is one of the fundamental directions in computer vision research, with extensive efforts dedicated to exploring various architectures such as <b>RNN,</b> 3D <b>CNN,</b> and <b>Transformers.</b> The newly proposed architecture of state space model, e.g., Mamba, shows promising traits to extend its success in long sequence modeling to video modeling. To assess whether Mamba can be a viable alternative to <b>Transformers</b> in the video understanding domain, in this work, we conduct a comprehensive set of studies, probing different roles Mamba can play in modeling videos, while investigating diverse tasks where Mamba could exhibit superiority. We categorize Mamba into four roles for modeling videos, deriving a Video Mamba Suite composed of 14 models/modules, and evaluating them on 12 video understanding tasks. Our extensive experiments reveal the strong potential of Mamba on both video-only and video-language tasks while showing promising efficiency-performance trade-offs. We hope this work could provide valuable data points and insights for future research on video understanding. Code is public: <a href=https://github.com/OpenGVLab/video-mamba-suite>https://github.com/OpenGVLab/video-mamba-suite</a>.</p></p class="citation"></blockquote><h3 id=44101--44320-counterfactual-contrastive-learning-robust-representations-via-causal-image-synthesis-melanie-roschewitz-et-al-2024>(44/101 | 44/320) Counterfactual contrastive learning: robust representations via causal image synthesis (Melanie Roschewitz et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Melanie Roschewitz, Fabio De Sousa Ribeiro, Tian Xia, Galvin Khara, Ben Glocker. (2024)<br><strong>Counterfactual contrastive learning: robust representations via causal image synthesis</strong><br><button class=copy-to-clipboard title="Counterfactual contrastive learning: robust representations via causal image synthesis" index=44>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-44 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Contrastive Learning, Counter-factual, Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09605v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09605v1.pdf filename=2403.09605v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Contrastive</b> <b>pretraining</b> is well-known to improve downstream task performance and model generalisation, especially in limited label settings. However, it is sensitive to the choice of augmentation pipeline. Positive pairs should preserve semantic information while destroying domain-specific information. Standard augmentation pipelines emulate domain-specific changes with pre-defined photometric transformations, but what if we could simulate realistic domain changes instead? In this work, we show how to utilise recent progress in <b>counterfactual</b> image generation to this effect. We propose CF-SimCLR, a <b>counterfactual</b> <b>contrastive</b> <b>learning</b> approach which leverages approximate <b>counterfactual</b> inference for positive pair creation. Comprehensive evaluation across five datasets, on chest radiography and mammography, demonstrates that CF-SimCLR substantially improves robustness to acquisition shift with higher downstream performance on both in- and <b>out-of-distribution</b> data, particularly for domains which are under-represented during training.</p></p class="citation"></blockquote><h3 id=45101--45320-faceptor-a-generalist-model-for-face-perception-lixiong-qin-et-al-2024>(45/101 | 45/320) Faceptor: A Generalist Model for Face Perception (Lixiong Qin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lixiong Qin, Mei Wang, Xuannan Liu, Yuhang Zhang, Wei Deng, Xiaoshuai Song, Weiran Xu, Weihong Deng. (2024)<br><strong>Faceptor: A Generalist Model for Face Perception</strong><br><button class=copy-to-clipboard title="Faceptor: A Generalist Model for Face Perception" index=45>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-45 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Face Recognition, Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09500v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09500v1.pdf filename=2403.09500v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the comprehensive research conducted on various <b>face</b> <b>analysis</b> tasks, there is a growing interest among researchers to develop a unified approach to <b>face</b> <b>perception.</b> Existing methods mainly discuss unified representation and training, which lack task extensibility and application efficiency. To tackle this issue, we focus on the unified model structure, exploring a <b>face</b> <b>generalist</b> model. As an intuitive design, Naive Faceptor enables tasks with the same output shape and granularity to share the structural design of the standardized output head, achieving improved task extensibility. Furthermore, Faceptor is proposed to adopt a well-designed single-encoder dual-decoder architecture, allowing task-specific queries to represent new-coming semantics. This design enhances the unification of model structure while improving application efficiency in terms of storage overhead. Additionally, we introduce Layer-Attention into Faceptor, enabling the model to adaptively select features from optimal layers to perform the desired tasks. Through joint training on 13 <b>face</b> <b>perception</b> datasets, Faceptor achieves exceptional performance in facial landmark localization, <b>face</b> <b>parsing,</b> age estimation, expression recognition, binary attribute classification, and <b>face</b> <b>recognition,</b> achieving or surpassing specialized methods in most tasks. Our training framework can also be applied to auxiliary <b>supervised</b> <b>learning,</b> significantly improving performance in data-sparse tasks such as age estimation and expression recognition. The code and models will be made publicly available at <a href=https://github.com/lxq1000/Faceptor>https://github.com/lxq1000/Faceptor</a>.</p></p class="citation"></blockquote><h3 id=46101--46320-condisr-contrastive-disentanglement-and-style-regularization-for-single-domain-generalization-aleksandr-matsun-et-al-2024>(46/101 | 46/320) ConDiSR: Contrastive Disentanglement and Style Regularization for Single Domain Generalization (Aleksandr Matsun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aleksandr Matsun, Numan Saeed, Fadillah Adamsyah Maani, Mohammad Yaqub. (2024)<br><strong>ConDiSR: Contrastive Disentanglement and Style Regularization for Single Domain Generalization</strong><br><button class=copy-to-clipboard title="ConDiSR: Contrastive Disentanglement and Style Regularization for Single Domain Generalization" index=46>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-46 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Distribution Shift, Distribution Shift, Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09400v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09400v1.pdf filename=2403.09400v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Medical data often exhibits <b>distribution</b> <b>shifts,</b> which cause test-time performance degradation for deep learning models trained using standard <b>supervised</b> <b>learning</b> pipelines. This challenge is addressed in the field of Domain Generalization (DG) with the sub-field of Single Domain Generalization (SDG) being specifically interesting due to the privacy- or logistics-related issues often associated with medical data. Existing disentanglement-based SDG methods heavily rely on structural information embedded in segmentation masks, however classification labels do not provide such dense information. This work introduces a novel SDG method aimed at medical image classification that leverages channel-wise contrastive disentanglement. It is further enhanced with reconstruction-based style regularization to ensure extraction of distinct style and structure feature representations. We evaluate our method on the complex task of multicenter histopathology image classification, comparing it against state-of-the-art (SOTA) SDG baselines. Results demonstrate that our method surpasses the SOTA by a margin of 1% in average accuracy while also showing more stable performance. This study highlights the importance and challenges of exploring SDG frameworks in the context of the classification task. The code is publicly available at <a href=https://github.com/BioMedIA-MBZUAI/ConDiSR>https://github.com/BioMedIA-MBZUAI/ConDiSR</a></p></p class="citation"></blockquote><h3 id=47101--47320-streammultidiffusion-real-time-interactive-generation-with-region-based-semantic-control-jaerin-lee-et-al-2024>(47/101 | 47/320) StreamMultiDiffusion: Real-Time Interactive Generation with Region-Based Semantic Control (Jaerin Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jaerin Lee, Daniel Sungho Jung, Kanggeon Lee, Kyoung Mu Lee. (2024)<br><strong>StreamMultiDiffusion: Real-Time Interactive Generation with Region-Based Semantic Control</strong><br><button class=copy-to-clipboard title="StreamMultiDiffusion: Real-Time Interactive Generation with Region-Based Semantic Control" index=47>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-47 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Diffusion Model, Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09055v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09055v1.pdf filename=2403.09055v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The enormous success of <b>diffusion</b> <b>models</b> in <b>text-to-image</b> synthesis has made them promising candidates for the next generation of end-user applications for image generation and editing. Previous works have focused on improving the usability of <b>diffusion</b> <b>models</b> by reducing the inference time or increasing user interactivity by allowing new, fine-grained controls such as region-based text <b>prompts.</b> However, we empirically find that integrating both branches of works is nontrivial, limiting the potential of <b>diffusion</b> <b>models.</b> To solve this incompatibility, we present StreamMultiDiffusion, the first real-time region-based <b>text-to-image</b> generation framework. By stabilizing fast inference techniques and restructuring the model into a newly proposed multi-prompt stream batch architecture, we achieve $\times 10$ faster panorama generation than existing solutions, and the generation speed of 1.57 FPS in region-based <b>text-to-image</b> synthesis on a single RTX 2080 Ti GPU. Our solution opens up a new paradigm for interactive image generation named semantic palette, where high-quality images are generated in real-time from given multiple hand-drawn regions, encoding prescribed semantic meanings (e.g., eagle, girl). Our code and demo application are available at <a href=https://github.com/ironjr/StreamMultiDiffusion>https://github.com/ironjr/StreamMultiDiffusion</a>.</p></p class="citation"></blockquote><h3 id=48101--48320-selector-heterogeneous-graph-network-with-convolutional-masked-autoencoder-for-multimodal-robust-prediction-of-cancer-survival-liangrui-pan-et-al-2024>(48/101 | 48/320) SELECTOR: Heterogeneous graph network with convolutional masked autoencoder for multimodal robust prediction of cancer survival (Liangrui Pan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Liangrui Pan, Yijun Peng, Yan Li, Xiang Wang, Wenjuan Liu, Liwen Xu, Qingchun Liang, Shaoliang Peng. (2024)<br><strong>SELECTOR: Heterogeneous graph network with convolutional masked autoencoder for multimodal robust prediction of cancer survival</strong><br><button class=copy-to-clipboard title="SELECTOR: Heterogeneous graph network with convolutional masked autoencoder for multimodal robust prediction of cancer survival" index=48>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-48 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 29<br>Keywords: Graph, Autoencoder, Convolution, Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09290v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09290v1.pdf filename=2403.09290v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Accurately predicting the survival rate of cancer patients is crucial for aiding clinicians in planning appropriate treatment, reducing cancer-related medical expenses, and significantly enhancing patients&rsquo; quality of life. <b>Multimodal</b> prediction of cancer patient survival offers a more comprehensive and precise approach. However, existing methods still grapple with challenges related to missing <b>multimodal</b> data and information interaction within modalities. This paper introduces SELECTOR, a heterogeneous <b>graph-aware</b> network based on <b>convolutional</b> mask encoders for robust <b>multimodal</b> prediction of cancer patient survival. SELECTOR comprises feature edge reconstruction, <b>convolutional</b> mask encoder, feature cross-fusion, and <b>multimodal</b> survival prediction modules. Initially, we construct a <b>multimodal</b> heterogeneous <b>graph</b> and employ the meta-path method for feature edge reconstruction, ensuring comprehensive incorporation of feature information from <b>graph</b> edges and effective embedding of nodes. To mitigate the impact of missing features within the modality on prediction accuracy, we devised a <b>convolutional</b> masked <b>autoencoder</b> (CMAE) to process the heterogeneous <b>graph</b> post-feature reconstruction. Subsequently, the feature cross-fusion module facilitates communication between modalities, ensuring that output features encompass all features of the modality and relevant information from other modalities. Extensive experiments and analysis on six cancer datasets from TCGA demonstrate that our method significantly outperforms state-of-the-art methods in both modality-missing and intra-modality information-confirmed cases. Our codes are made available at <a href=https://github.com/panliangrui/Selector>https://github.com/panliangrui/Selector</a>.</p></p class="citation"></blockquote><h3 id=49101--49320-anatomical-structure-guided-medical-vision-language-pre-training-qingqiu-li-et-al-2024>(49/101 | 49/320) Anatomical Structure-Guided Medical Vision-Language Pre-training (Qingqiu Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qingqiu Li, Xiaohan Yan, Jilan Xu, Runtian Yuan, Yuejie Zhang, Rui Feng, Quanli Shen, Xiaobo Zhang, Shujun Wang. (2024)<br><strong>Anatomical Structure-Guided Medical Vision-Language Pre-training</strong><br><button class=copy-to-clipboard title="Anatomical Structure-Guided Medical Vision-Language Pre-training" index=49>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-49 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CL, cs-CV, cs.CV<br>Keyword Score: 28<br>Keywords: Benchmarking, Contrastive Learning, Representation Learning, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09294v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09294v1.pdf filename=2403.09294v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Learning medical visual <b>representations</b> <b>through</b> <b>vision-language</b> pre-training has reached remarkable progress. Despite the promising performance, it still faces challenges, i.e., local alignment lacks interpretability and clinical relevance, and the insufficient internal and external <b>representation</b> <b>learning</b> of image-report pairs. To address these issues, we propose an Anatomical Structure-Guided (ASG) framework. Specifically, we parse raw reports into triplets &lt;anatomical region, finding, existence>, and fully utilize each element as supervision to enhance <b>representation</b> <b>learning.</b> For anatomical region, we design an automatic anatomical region-sentence alignment paradigm in collaboration with radiologists, considering them as the minimum semantic units to explore fine-grained local alignment. For finding and existence, we regard them as image tags, applying an image-tag recognition decoder to associate image features with their respective tags within each sample and constructing soft labels for <b>contrastive</b> <b>learning</b> to improve the semantic association of different image-report pairs. We evaluate the proposed ASG framework on two downstream tasks, including five public <b>benchmarks.</b> Experimental results demonstrate that our method outperforms the state-of-the-art methods.</p></p class="citation"></blockquote><h3 id=50101--50320-shan-object-level-privacy-detection-via-inference-on-scene-heterogeneous-graph-zhuohang-jiang-et-al-2024>(50/101 | 50/320) SHAN: Object-Level Privacy Detection via Inference on Scene Heterogeneous Graph (Zhuohang Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhuohang Jiang, Bingkui Tong, Xia Du, Ahmed Alhammadi, Jizhe Zhou. (2024)<br><strong>SHAN: Object-Level Privacy Detection via Inference on Scene Heterogeneous Graph</strong><br><button class=copy-to-clipboard title="SHAN: Object-Level Privacy Detection via Inference on Scene Heterogeneous Graph" index=50>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-50 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 26<br>Keywords: Object Detection, Graph, Benchmarking, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09172v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09172v1.pdf filename=2403.09172v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the rise of social platforms, protecting privacy has become an important issue. Privacy <b>object</b> <b>detection</b> aims to accurately locate private <b>objects</b> <b>in</b> images. It is the foundation of safeguarding individuals&rsquo; privacy rights and ensuring responsible data handling practices in the digital age. Since privacy of <b>object</b> <b>is</b> not shift-invariant, the essence of the privacy <b>object</b> <b>detection</b> task is inferring <b>object</b> <b>privacy</b> based on scene information. However, privacy <b>object</b> <b>detection</b> has long been studied as a subproblem of common <b>object</b> <b>detection</b> tasks. Therefore, existing methods suffer from serious deficiencies in accuracy, generalization, and interpretability. Moreover, creating large-scale privacy datasets is difficult due to legal constraints and existing privacy datasets lack label granularity. The granularity of existing privacy detection methods remains limited to the image level. To address the above two issues, we introduce two <b>benchmark</b> datasets for <b>object-level</b> <b>privacy</b> detection and propose SHAN, Scene Heterogeneous <b>graph</b> Attention Network, a model constructs a scene heterogeneous <b>graph</b> from an image and utilizes <b>self-attention</b> mechanisms for scene inference to obtain <b>object</b> <b>privacy.</b> Through experiments, we demonstrated that SHAN performs excellently in privacy <b>object</b> <b>detection</b> tasks, with all metrics surpassing those of the baseline model.</p></p class="citation"></blockquote><h3 id=51101--51320-marvis-motion--geometry-aware-real-and-virtual-image-segmentation-jiayi-wu-et-al-2024>(51/101 | 51/320) MARVIS: Motion & Geometry Aware Real and Virtual Image Segmentation (Jiayi Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiayi Wu, Xiaomin Lin, Shahriar Negahdaripour, Cornelia Fermüller, Yiannis Aloimonos. (2024)<br><strong>MARVIS: Motion & Geometry Aware Real and Virtual Image Segmentation</strong><br><button class=copy-to-clipboard title="MARVIS: Motion & Geometry Aware Real and Virtual Image Segmentation" index=51>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-51 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 25<br>Keywords: Geometry, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09850v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09850v1.pdf filename=2403.09850v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Tasks such as autonomous navigation, 3D reconstruction, and object recognition near the water surfaces are crucial in marine robotics applications. However, challenges arise due to dynamic disturbances, e.g., light reflections and refraction from the random air-water interface, irregular liquid flow, and similar factors, which can lead to potential failures in perception and navigation systems. Traditional computer vision algorithms struggle to differentiate between real and virtual image regions, significantly complicating tasks. A virtual image region is an apparent representation formed by the redirection of light rays, typically through reflection or refraction, creating the illusion of an object&rsquo;s presence without its actual physical location. This work proposes a novel approach for segmentation on real and virtual image regions, exploiting synthetic images combined with domain-invariant information, a Motion Entropy Kernel, and Epipolar Geometric Consistency. Our segmentation network does not need to be re-trained if the domain changes. We show this by deploying the same segmentation network in two different domains: <b>simulation</b> and the real world. By creating realistic synthetic images that mimic the complexities of the water surface, we provide fine-grained training data for our network (MARVIS) to discern between real and virtual images effectively. By motion & <b>geometry-aware</b> design choices and through comprehensive experimental analysis, we achieve state-of-the-art real-virtual image segmentation performance in unseen real world domain, achieving an IoU over 78% and a F1-Score over 86% while ensuring a small computational footprint. MARVIS offers over 43 FPS (8 FPS) inference rates on a single GPU (CPU core). Our code and dataset are available here <a href=https://github.com/jiayi-wu-umd/MARVIS>https://github.com/jiayi-wu-umd/MARVIS</a>.</p></p class="citation"></blockquote><h3 id=52101--52320-reconstruction-and-simulation-of-elastic-objects-with-spring-mass-3d-gaussians-licheng-zhong-et-al-2024>(52/101 | 52/320) Reconstruction and Simulation of Elastic Objects with Spring-Mass 3D Gaussians (Licheng Zhong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Licheng Zhong, Hong-Xing Yu, Jiajun Wu, Yunzhu Li. (2024)<br><strong>Reconstruction and Simulation of Elastic Objects with Spring-Mass 3D Gaussians</strong><br><button class=copy-to-clipboard title="Reconstruction and Simulation of Elastic Objects with Spring-Mass 3D Gaussians" index=52>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-52 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 25<br>Keywords: Geometry, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09434v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09434v1.pdf filename=2403.09434v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Reconstructing and simulating elastic objects from visual observations is crucial for applications in computer vision and robotics. Existing methods, such as 3D Gaussians, provide modeling for 3D appearance and <b>geometry</b> but lack the ability to simulate physical properties or optimize parameters for heterogeneous objects. We propose Spring-Gaus, a novel framework that integrates 3D Gaussians with physics-based <b>simulation</b> for reconstructing and simulating elastic objects from multi-view videos. Our method utilizes a 3D Spring-Mass model, enabling the optimization of physical parameters at the individual point level while decoupling the learning of physics and appearance. This approach achieves great sample efficiency, enhances generalization, and reduces sensitivity to the distribution of <b>simulation</b> particles. We evaluate Spring-Gaus on both synthetic and real-world datasets, demonstrating accurate reconstruction and <b>simulation</b> of elastic objects. This includes future prediction and <b>simulation</b> under varying initial states and environmental parameters. Project page: <a href=https://zlicheng.com/spring_gaus>https://zlicheng.com/spring_gaus</a>.</p></p class="citation"></blockquote><h3 id=53101--53320-sentinel-guided-zero-shot-learning-a-collaborative-paradigm-without-real-data-exposure-fan-wan-et-al-2024>(53/101 | 53/320) Sentinel-Guided Zero-Shot Learning: A Collaborative Paradigm without Real Data Exposure (Fan Wan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fan Wan, Xingyu Miao, Haoran Duan, Jingjing Deng, Rui Gao, Yang Long. (2024)<br><strong>Sentinel-Guided Zero-Shot Learning: A Collaborative Paradigm without Real Data Exposure</strong><br><button class=copy-to-clipboard title="Sentinel-Guided Zero-Shot Learning: A Collaborative Paradigm without Real Data Exposure" index=53>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-53 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 25<br>Keywords: Black Box, Zero-shot, Zero-shot Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09363v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09363v1.pdf filename=2403.09363v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With increasing concerns over data privacy and model copyrights, especially in the context of collaborations between AI service providers and data owners, an innovative SG-ZSL paradigm is proposed in this work. SG-ZSL is designed to foster efficient collaboration without the need to exchange models or sensitive data. It consists of a teacher model, a student model and a generator that links both model entities. The teacher model serves as a sentinel on behalf of the data owner, replacing real data, to guide the student model at the AI service provider&rsquo;s end during training. Considering the disparity of knowledge space between the teacher and student, we introduce two variants of the teacher model: the omniscient and the quasi-omniscient teachers. Under these teachers&rsquo; guidance, the student model seeks to match the teacher model&rsquo;s performance and explores domains that the teacher has not covered. To trade off between privacy and performance, we further introduce two distinct security-level training protocols: white-box and <b>black-box,</b> <b>enhancing</b> the paradigm&rsquo;s adaptability. Despite the inherent challenges of real data absence in the SG-ZSL paradigm, it consistently outperforms in ZSL and GZSL tasks, notably in the white-box protocol. Our comprehensive evaluation further attests to its robustness and efficiency across various setups, including stringent <b>black-box</b> <b>training</b> protocol.</p></p class="citation"></blockquote><h3 id=54101--54320-glyph-byt5-a-customized-text-encoder-for-accurate-visual-text-rendering-zeyu-liu-et-al-2024>(54/101 | 54/320) Glyph-ByT5: A Customized Text Encoder for Accurate Visual Text Rendering (Zeyu Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zeyu Liu, Weicong Liang, Zhanhao Liang, Chong Luo, Ji Li, Gao Huang, Yuhui Yuan. (2024)<br><strong>Glyph-ByT5: A Customized Text Encoder for Accurate Visual Text Rendering</strong><br><button class=copy-to-clipboard title="Glyph-ByT5: A Customized Text Encoder for Accurate Visual Text Rendering" index=54>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-54 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Benchmarking, Fine-tuning, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09622v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09622v1.pdf filename=2403.09622v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Visual text rendering poses a fundamental challenge for contemporary <b>text-to-image</b> generation models, with the core problem lying in text encoder deficiencies. To achieve accurate text rendering, we identify two crucial requirements for text encoders: character awareness and alignment with glyphs. Our solution involves crafting a series of customized text encoder, Glyph-ByT5, by <b>fine-tuning</b> the character-aware ByT5 encoder using a meticulously curated paired glyph-text dataset. We present an effective method for integrating Glyph-ByT5 with SDXL, resulting in the creation of the Glyph-SDXL model for design image generation. This significantly enhances text rendering accuracy, improving it from less than $20%$ to nearly $90%$ on our design image <b>benchmark.</b> Noteworthy is Glyph-SDXL&rsquo;s newfound ability for text paragraph rendering, achieving high spelling accuracy for tens to hundreds of characters with automated multi-line layouts. Finally, through <b>fine-tuning</b> Glyph-SDXL with a small set of high-quality, photorealistic images featuring visual text, we showcase a substantial improvement in scene text rendering capabilities in open-domain real images. These compelling outcomes aim to encourage further exploration in designing customized text encoders for diverse and challenging tasks.</p></p class="citation"></blockquote><h3 id=55101--55320-renovating-names-in-open-vocabulary-segmentation-benchmarks-haiwen-huang-et-al-2024>(55/101 | 55/320) Renovating Names in Open-Vocabulary Segmentation Benchmarks (Haiwen Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haiwen Huang, Songyou Peng, Dan Zhang, Andreas Geiger. (2024)<br><strong>Renovating Names in Open-Vocabulary Segmentation Benchmarks</strong><br><button class=copy-to-clipboard title="Renovating Names in Open-Vocabulary Segmentation Benchmarks" index=55>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-55 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Benchmarking, Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09593v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09593v1.pdf filename=2403.09593v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Names are essential to both human cognition and <b>vision-language</b> models. Open-vocabulary models utilize class names as text <b>prompts</b> to generalize to categories unseen during training. However, name qualities are often overlooked and lack sufficient precision in existing datasets. In this paper, we address this underexplored problem by presenting a framework for &ldquo;renovating&rdquo; names in open-vocabulary segmentation <b>benchmarks</b> (RENOVATE). Through human study, we demonstrate that the names generated by our model are more precise descriptions of the visual segments and hence enhance the quality of existing datasets by means of simple renaming. We further demonstrate that using our renovated names enables training of stronger open-vocabulary segmentation models. Using open-vocabulary segmentation for name quality evaluation, we show that our renovated names lead to up to 16% relative improvement from the original names on various <b>benchmarks</b> across various state-of-the-art models. We provide our code and relabelings for several popular segmentation datasets (ADE20K, Cityscapes, PASCAL Context) to the research community.</p></p class="citation"></blockquote><h3 id=56101--56320-weaksurg-weakly-supervised-surgical-instrument-segmentation-using-temporal-equivariance-and-semantic-continuity-qiyuan-wang-et-al-2024>(56/101 | 56/320) WeakSurg: Weakly supervised surgical instrument segmentation using temporal equivariance and semantic continuity (Qiyuan Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qiyuan Wang, Yanzhe Liu, Shang Zhao, Rong Liu, S. Kevin Zhou. (2024)<br><strong>WeakSurg: Weakly supervised surgical instrument segmentation using temporal equivariance and semantic continuity</strong><br><button class=copy-to-clipboard title="WeakSurg: Weakly supervised surgical instrument segmentation using temporal equivariance and semantic continuity" index=56>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-56 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Benchmarking, Supervised Learning, Weakly-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09551v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09551v1.pdf filename=2403.09551v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Weakly <b>supervised</b> surgical instrument segmentation with only instrument presence labels has been rarely explored in surgical domain. To mitigate the highly under-constrained challenges, we extend a two-stage weakly <b>supervised</b> segmentation paradigm with temporal attributes from two perspectives. From a temporal equivariance perspective, we propose a prototype-based temporal equivariance regulation loss to enhance pixel-wise consistency between adjacent features. From a semantic continuity perspective, we propose a class-aware temporal semantic continuity loss to constrain the semantic consistency between a global view of target frame and local non-discriminative regions of adjacent reference frame. To the best of our knowledge, WeakSurg is the first instrument-presence-only weakly <b>supervised</b> segmentation architecture to take temporal information into account for surgical scenarios. Extensive experiments are validated on Cholec80, an open <b>benchmark</b> for phase and instrument recognition. We annotate instance-wise instrument labels with fixed time-steps which are double checked by a clinician with 3-years experience. Our results show that WeakSurg compares favorably with state-of-the-art methods not only on semantic segmentation metrics but also on instance segmentation metrics.</p></p class="citation"></blockquote><h3 id=57101--57320-eta-inversion-designing-an-optimal-eta-function-for-diffusion-based-real-image-editing-wonjun-kang-et-al-2024>(57/101 | 57/320) Eta Inversion: Designing an Optimal Eta Function for Diffusion-based Real Image Editing (Wonjun Kang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wonjun Kang, Kevin Galim, Hyung Il Koo. (2024)<br><strong>Eta Inversion: Designing an Optimal Eta Function for Diffusion-based Real Image Editing</strong><br><button class=copy-to-clipboard title="Eta Inversion: Designing an Optimal Eta Function for Diffusion-based Real Image Editing" index=57>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-57 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Diffusion Model, Benchmarking, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09468v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09468v1.pdf filename=2403.09468v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Diffusion</b> <b>models</b> have achieved remarkable success in the domain of text-guided image generation and, more recently, in text-guided image editing. A commonly adopted strategy for editing real images involves inverting the <b>diffusion</b> <b>process</b> to obtain a noisy representation of the original image, which is then denoised to achieve the desired edits. However, current methods for <b>diffusion</b> <b>inversion</b> often struggle to produce edits that are both faithful to the specified text <b>prompt</b> and closely resemble the source image. To overcome these limitations, we introduce a novel and adaptable <b>diffusion</b> <b>inversion</b> technique for real image editing, which is grounded in a theoretical analysis of the role of $\eta$ in the DDIM sampling equation for enhanced editability. By designing a universal <b>diffusion</b> <b>inversion</b> method with a time- and region-dependent $\eta$ function, we enable flexible control over the editing extent. Through a comprehensive series of quantitative and qualitative assessments, involving a comparison with a broad array of recent methods, we demonstrate the superiority of our approach. Our method not only sets a new <b>benchmark</b> in the field but also significantly outperforms existing strategies. Our code is available at <a href=https://github.com/furiosa-ai/eta-inversion>https://github.com/furiosa-ai/eta-inversion</a></p></p class="citation"></blockquote><h3 id=58101--58320-distribution-and-depth-aware-transformers-for-3d-human-mesh-recovery-jerrin-bright-et-al-2024>(58/101 | 58/320) Distribution and Depth-Aware Transformers for 3D Human Mesh Recovery (Jerrin Bright et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jerrin Bright, Bavesh Balaji, Harish Prakash, Yuhao Chen, David A Clausi, John Zelek. (2024)<br><strong>Distribution and Depth-Aware Transformers for 3D Human Mesh Recovery</strong><br><button class=copy-to-clipboard title="Distribution and Depth-Aware Transformers for 3D Human Mesh Recovery" index=58>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-58 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Multi-modal, Out-of-distribution, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09063v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09063v1.pdf filename=2403.09063v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Precise Human Mesh Recovery (HMR) with in-the-wild data is a formidable challenge and is often hindered by depth ambiguities and reduced precision. Existing works resort to either pose priors or <b>multi-modal</b> data such as multi-view or point cloud information, though their methods often overlook the valuable scene-depth information inherently present in a single image. Moreover, achieving robust HMR for <b>out-of-distribution</b> (OOD) data is exceedingly challenging due to inherent variations in pose, shape and depth. Consequently, understanding the underlying distribution becomes a vital subproblem in modeling human forms. Motivated by the need for unambiguous and robust human modeling, we introduce Distribution and depth-aware human mesh recovery (D2A-HMR), an end-to-end <b>transformer</b> architecture meticulously designed to minimize the disparity between distributions and incorporate scene-depth leveraging prior depth information. Our approach demonstrates superior performance in handling OOD data in certain scenarios while consistently achieving competitive results against state-of-the-art HMR methods on controlled datasets.</p></p class="citation"></blockquote><h3 id=59101--59320-the-first-to-know-how-token-distributions-reveal-hidden-knowledge-in-large-vision-language-models-qinyu-zhao-et-al-2024>(59/101 | 59/320) The First to Know: How Token Distributions Reveal Hidden Knowledge in Large Vision-Language Models? (Qinyu Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qinyu Zhao, Ming Xu, Kartik Gupta, Akshay Asthana, Liang Zheng, Stephen Gould. (2024)<br><strong>The First to Know: How Token Distributions Reveal Hidden Knowledge in Large Vision-Language Models?</strong><br><button class=copy-to-clipboard title="The First to Know: How Token Distributions Reveal Hidden Knowledge in Large Vision-Language Models?" index=59>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-59 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CL, cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Fine-tuning, Multi-modal, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09037v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09037v1.pdf filename=2403.09037v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Large <b>vision-language</b> models (LVLMs), designed to interpret and respond to human instructions, occasionally generate hallucinated or harmful content due to inappropriate instructions. This study uses linear probing to shed light on the hidden knowledge at the output layer of LVLMs. We demonstrate that the logit distributions of the first tokens contain sufficient information to determine whether to respond to the instructions, including recognizing unanswerable visual questions, defending against <b>multi-modal</b> jailbreaking attack, and identifying deceptive questions. Such hidden knowledge is gradually lost in logits of subsequent tokens during response generation. Then, we illustrate a simple decoding strategy at the generation of the first token, effectively improving the generated content. In experiments, we find a few interesting insights: First, the CLIP model already contains a strong signal for solving these tasks, indicating potential bias in the existing datasets. Second, we observe performance improvement by utilizing the first logit distributions on three additional tasks, including indicting uncertainty in math solving, mitigating hallucination, and image classification. Last, with the same training data, simply <b>finetuning</b> LVLMs improve models&rsquo; performance but is still inferior to linear probing on these tasks.</p></p class="citation"></blockquote><h3 id=60101--60320-an-image-is-worth-1000-lies-adversarial-transferability-across-prompts-on-vision-language-models-haochen-luo-et-al-2024>(60/101 | 60/320) An Image Is Worth 1000 Lies: Adversarial Transferability across Prompts on Vision-Language Models (Haochen Luo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haochen Luo, Jindong Gu, Fengyuan Liu, Philip Torr. (2024)<br><strong>An Image Is Worth 1000 Lies: Adversarial Transferability across Prompts on Vision-Language Models</strong><br><button class=copy-to-clipboard title="An Image Is Worth 1000 Lies: Adversarial Transferability across Prompts on Vision-Language Models" index=60>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-60 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09766v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09766v1.pdf filename=2403.09766v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Different from traditional task-specific vision models, recent large VLMs can readily adapt to different vision tasks by simply using different textual instructions, i.e., <b>prompts.</b> However, a well-known concern about traditional task-specific vision models is that they can be misled by imperceptible adversarial perturbations. Furthermore, the concern is exacerbated by the phenomenon that the same adversarial perturbations can fool different task-specific models. Given that VLMs rely on <b>prompts</b> to adapt to different tasks, an intriguing question emerges: Can a single adversarial image mislead all predictions of VLMs when a thousand different <b>prompts</b> are given? This question essentially introduces a novel perspective on adversarial transferability: cross-prompt adversarial transferability. In this work, we propose the Cross-Prompt Attack (CroPA). This proposed method updates the visual adversarial perturbation with learnable <b>prompts,</b> which are designed to counteract the misleading effects of the adversarial image. By doing this, CroPA significantly improves the transferability of adversarial examples across <b>prompts.</b> Extensive experiments are conducted to verify the strong cross-prompt adversarial transferability of CroPA with prevalent VLMs including Flamingo, BLIP-2, and InstructBLIP in various different tasks. Our source code is available at \url{https://github.com/Haochen-Luo/CroPA}.</p></p class="citation"></blockquote><h3 id=61101--61320-make-your-3d-fast-and-consistent-subject-driven-3d-content-generation-fangfu-liu-et-al-2024>(61/101 | 61/320) Make-Your-3D: Fast and Consistent Subject-Driven 3D Content Generation (Fangfu Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fangfu Liu, Hanyang Wang, Weiliang Chen, Haowen Sun, Yueqi Duan. (2024)<br><strong>Make-Your-3D: Fast and Consistent Subject-Driven 3D Content Generation</strong><br><button class=copy-to-clipboard title="Make-Your-3D: Fast and Consistent Subject-Driven 3D Content Generation" index=61>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-61 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09625v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09625v1.pdf filename=2403.09625v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent years have witnessed the strong power of 3D generation models, which offer a new level of creative flexibility by allowing users to guide the 3D content generation process through a single image or natural language. However, it remains challenging for existing 3D generation methods to create subject-driven 3D content across diverse <b>prompts.</b> In this paper, we introduce a novel 3D customization method, dubbed Make-Your-3D that can personalize high-fidelity and consistent 3D content from only a single image of a subject with text description within 5 minutes. Our key insight is to harmonize the distributions of a multi-view <b>diffusion</b> <b>model</b> and an identity-specific 2D generative model, aligning them with the distribution of the desired 3D subject. Specifically, we design a co-evolution framework to reduce the variance of distributions, where each model undergoes a process of learning from the other through identity-aware optimization and subject-prior optimization, respectively. Extensive experiments demonstrate that our method can produce high-quality, consistent, and subject-specific 3D content with text-driven modifications that are unseen in subject image.</p></p class="citation"></blockquote><h3 id=62101--62320-possam-panoptic-open-vocabulary-segment-anything-vibashan-vs-et-al-2024>(62/101 | 62/320) PosSAM: Panoptic Open-vocabulary Segment Anything (Vibashan VS et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vibashan VS, Shubhankar Borse, Hyojin Park, Debasmit Das, Vishal Patel, Munawar Hayat, Fatih Porikli. (2024)<br><strong>PosSAM: Panoptic Open-vocabulary Segment Anything</strong><br><button class=copy-to-clipboard title="PosSAM: Panoptic Open-vocabulary Segment Anything" index=62>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-62 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09620v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09620v1.pdf filename=2403.09620v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we introduce an open-vocabulary panoptic segmentation model that effectively unifies the strengths of the Segment Anything Model (SAM) with the <b>vision-language</b> CLIP model in an end-to-end framework. While SAM excels in generating spatially-aware masks, it&rsquo;s decoder falls short in recognizing object class information and tends to oversegment without additional guidance. Existing approaches address this limitation by using multi-stage techniques and employing separate models to generate class-aware <b>prompts,</b> such as bounding boxes or segmentation masks. Our proposed method, PosSAM is an end-to-end model which leverages SAM&rsquo;s spatially rich features to produce instance-aware masks and harnesses CLIP&rsquo;s semantically discriminative features for effective instance classification. Specifically, we address the limitations of SAM and propose a novel Local Discriminative Pooling (LDP) module leveraging class-agnostic SAM and class-aware CLIP features for unbiased open-vocabulary classification. Furthermore, we introduce a Mask-Aware Selective Ensembling (MASE) algorithm that adaptively enhances the quality of generated masks and boosts the performance of open-vocabulary classification during inference for each image. We conducted extensive experiments to demonstrate our methods strong generalization properties across multiple datasets, achieving state-of-the-art performance with substantial improvements over SOTA open-vocabulary panoptic segmentation methods. In both COCO to ADE20K and ADE20K to COCO settings, PosSAM outperforms the previous state-of-the-art methods by a large margin, 2.4 PQ and 4.6 PQ, respectively. Project Website: <a href=https://vibashan.github.io/possam-web/>https://vibashan.github.io/possam-web/</a>.</p></p class="citation"></blockquote><h3 id=63101--63320-dont-judge-by-the-look-a-motion-coherent-augmentation-for-video-recognition-yitian-zhang-et-al-2024>(63/101 | 63/320) Don&rsquo;t Judge by the Look: A Motion Coherent Augmentation for Video Recognition (Yitian Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yitian Zhang, Yue Bai, Huan Wang, Yizhou Wang, Yun Fu. (2024)<br><strong>Don&rsquo;t Judge by the Look: A Motion Coherent Augmentation for Video Recognition</strong><br><button class=copy-to-clipboard title="Don't Judge by the Look: A Motion Coherent Augmentation for Video Recognition" index=63>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-63 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 20<br>Keywords: Data Augmentation, Distribution Shift, Distribution Shift<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09506v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09506v1.pdf filename=2403.09506v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Current training pipelines in object recognition neglect Hue Jittering when doing <b>data</b> <b>augmentation</b> as it not only brings appearance changes that are detrimental to classification, but also the implementation is inefficient in practice. In this study, we investigate the effect of hue variance in the context of video recognition and find this variance to be beneficial since static appearances are less important in videos that contain motion information. Based on this observation, we propose a <b>data</b> <b>augmentation</b> method for video recognition, named Motion Coherent Augmentation (MCA), that introduces appearance variation in videos and implicitly encourages the model to prioritize motion patterns, rather than static appearances. Concretely, we propose an operation SwapMix to efficiently modify the appearance of video samples, and introduce Variation Alignment (VA) to resolve the <b>distribution</b> <b>shift</b> caused by SwapMix, enforcing the model to learn appearance invariant representations. Comprehensive empirical evaluation across various architectures and different datasets solidly validates the effectiveness and generalization ability of MCA, and the application of VA in other augmentation methods. Code is available at <a href=https://github.com/BeSpontaneous/MCA-pytorch>https://github.com/BeSpontaneous/MCA-pytorch</a>.</p></p class="citation"></blockquote><h3 id=64101--64320-mitigating-attribute-amplification-in-counterfactual-image-generation-tian-xia-et-al-2024>(64/101 | 64/320) Mitigating attribute amplification in counterfactual image generation (Tian Xia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tian Xia, Mélanie Roschewitz, Fabio De Sousa Ribeiro, Charles Jones, Ben Glocker. (2024)<br><strong>Mitigating attribute amplification in counterfactual image generation</strong><br><button class=copy-to-clipboard title="Mitigating attribute amplification in counterfactual image generation" index=64>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-64 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Counter-factual, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09422v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09422v1.pdf filename=2403.09422v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Causal generative modelling is gaining interest in medical imaging due to its ability to answer interventional and <b>counterfactual</b> queries. Most work focuses on generating <b>counterfactual</b> images that look plausible, using auxiliary classifiers to enforce effectiveness of simulated interventions. We investigate pitfalls in this approach, discovering the issue of attribute amplification, where unrelated attributes are spuriously affected during interventions, leading to biases across protected characteristics and disease status. We show that attribute amplification is caused by the use of hard labels in the <b>counterfactual</b> training process and propose soft <b>counterfactual</b> <b>fine-tuning</b> to mitigate this issue. Our method substantially reduces the amplification effect while maintaining effectiveness of generated images, demonstrated on a large chest X-ray dataset. Our work makes an important advancement towards more faithful and unbiased causal modelling in medical imaging.</p></p class="citation"></blockquote><h3 id=65101--65320-d3t-distinctive-dual-domain-teacher-zigzagging-across-rgb-thermal-gap-for-domain-adaptive-object-detection-dinh-phat-do-et-al-2024>(65/101 | 65/320) D3T: Distinctive Dual-Domain Teacher Zigzagging Across RGB-Thermal Gap for Domain-Adaptive Object Detection (Dinh Phat Do et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dinh Phat Do, Taehoon Kim, Jaemin Na, Jiwon Kim, Keonho Lee, Kyunghwan Cho, Wonjun Hwang. (2024)<br><strong>D3T: Distinctive Dual-Domain Teacher Zigzagging Across RGB-Thermal Gap for Domain-Adaptive Object Detection</strong><br><button class=copy-to-clipboard title="D3T: Distinctive Dual-Domain Teacher Zigzagging Across RGB-Thermal Gap for Domain-Adaptive Object Detection" index=65>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-65 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Object Detection, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09359v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09359v1.pdf filename=2403.09359v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Domain</b> <b>adaptation</b> for <b>object</b> <b>detection</b> typically entails transferring knowledge from one visible <b>domain</b> <b>to</b> another visible <b>domain.</b> <b>However,</b> there are limited studies on adapting from the visible to the thermal <b>domain,</b> <b>because</b> the <b>domain</b> <b>gap</b> between the visible and thermal <b>domains</b> <b>is</b> much larger than expected, and traditional <b>domain</b> <b>adaptation</b> can not successfully facilitate learning in this situation. To overcome this challenge, we propose a Distinctive Dual-Domain Teacher (D3T) framework that employs distinct training paradigms for each <b>domain.</b> <b>Specifically,</b> we segregate the source and target training sets for building dual-teachers and successively deploy exponential moving average to the student model to individual teachers of each <b>domain.</b> <b>The</b> framework further incorporates a zigzag learning method between dual teachers, facilitating a gradual transition from the visible to thermal <b>domains</b> <b>during</b> training. We validate the superiority of our method through newly designed experimental protocols with well-known thermal datasets, i.e., FLIR and KAIST. Source code is available at <a href=https://github.com/EdwardDo69/D3T>https://github.com/EdwardDo69/D3T</a> .</p></p class="citation"></blockquote><h3 id=66101--66320-perspective-equivariant-imaging-an-unsupervised-framework-for-multispectral-pansharpening-andrew-wang-et-al-2024>(66/101 | 66/320) Perspective-Equivariant Imaging: an Unsupervised Framework for Multispectral Pansharpening (Andrew Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andrew Wang, Mike Davies. (2024)<br><strong>Perspective-Equivariant Imaging: an Unsupervised Framework for Multispectral Pansharpening</strong><br><button class=copy-to-clipboard title="Perspective-Equivariant Imaging: an Unsupervised Framework for Multispectral Pansharpening" index=66>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-66 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV<br>Keyword Score: 20<br>Keywords: Unsupervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09327v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09327v1.pdf filename=2403.09327v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Ill-posed image reconstruction problems appear in many scenarios such as remote sensing, where obtaining high quality images is crucial for environmental monitoring, disaster management and urban planning. Deep learning has seen great success in overcoming the limitations of traditional methods. However, these inverse problems rarely come with ground truth data, highlighting the importance of <b>unsupervised</b> <b>learning</b> from partial and noisy measurements alone. We propose perspective-equivariant imaging (EI), a framework that leverages perspective variability in optical camera-based imaging systems, such as satellites or handheld cameras, to recover information lost in ill-posed optical camera imaging problems. This extends previous EI work to include a much richer non-linear class of group transforms and is shown to be an excellent prior for satellite and urban image data, where perspective-EI achieves state-of-the-art results in multispectral pansharpening, outperforming other <b>unsupervised</b> <b>methods</b> in the literature. Code at <a href=https://andrewwango.github.io/perspective-equivariant-imaging>https://andrewwango.github.io/perspective-equivariant-imaging</a></p></p class="citation"></blockquote><h3 id=67101--67320-clip-ebc-clip-can-count-accurately-through-enhanced-blockwise-classification-yiming-ma-et-al-2024>(67/101 | 67/320) CLIP-EBC: CLIP Can Count Accurately through Enhanced Blockwise Classification (Yiming Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yiming Ma, Victor Sanchez, Tanaya Guha. (2024)<br><strong>CLIP-EBC: CLIP Can Count Accurately through Enhanced Blockwise Classification</strong><br><button class=copy-to-clipboard title="CLIP-EBC: CLIP Can Count Accurately through Enhanced Blockwise Classification" index=67>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-67 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Object Detection, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09281v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09281v1.pdf filename=2403.09281v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The CLIP (Contrastive Language-Image Pretraining) model has exhibited outstanding performance in recognition problems, such as <b>zero-shot</b> image classification and <b>object</b> <b>detection.</b> However, its ability to count remains understudied due to the inherent challenges of transforming counting&ndash;a regression task&ndash;into a recognition task. In this paper, we investigate CLIP&rsquo;s potential in counting, focusing specifically on estimating crowd sizes. Existing classification-based crowd-counting methods have encountered issues, including inappropriate discretization strategies, which impede the application of CLIP and result in suboptimal performance. To address these challenges, we propose the Enhanced Blockwise Classification (EBC) framework. In contrast to previous methods, EBC relies on integer-valued bins that facilitate the learning of robust decision boundaries. Within our model-agnostic EBC framework, we introduce CLIP-EBC, the first fully CLIP-based crowd-counting model capable of generating density maps. Comprehensive evaluations across diverse crowd-counting datasets demonstrate the state-of-the-art performance of our methods. Particularly, EBC can improve existing models by up to 76.9%. Moreover, our CLIP-EBC model surpasses current crowd-counting methods, achieving mean absolute errors of 55.0 and 6.3 on ShanghaiTech part A and part B datasets, respectively. The code will be made publicly available.</p></p class="citation"></blockquote><h3 id=68101--68320-wsi-sam-multi-resolution-segment-anything-model-sam-for-histopathology-whole-slide-images-hong-liu-et-al-2024>(68/101 | 68/320) WSI-SAM: Multi-resolution Segment Anything Model (SAM) for histopathology whole-slide images (Hong Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hong Liu, Haosen Yang, Paul J. van Diest, Josien P. W. Pluim, Mitko Veta. (2024)<br><strong>WSI-SAM: Multi-resolution Segment Anything Model (SAM) for histopathology whole-slide images</strong><br><button class=copy-to-clipboard title="WSI-SAM: Multi-resolution Segment Anything Model (SAM) for histopathology whole-slide images" index=68>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-68 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Zero-shot, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09257v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09257v2.pdf filename=2403.09257v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Segment Anything Model (SAM) marks a significant advancement in segmentation models, offering robust <b>zero-shot</b> abilities and dynamic <b>prompting.</b> However, existing medical SAMs are not suitable for the multi-scale nature of whole-slide images (WSIs), restricting their effectiveness. To resolve this drawback, we present WSI-SAM, enhancing SAM with precise object segmentation capabilities for histopathology images using multi-resolution patches, while preserving its efficient, <b>prompt-driven</b> design, and <b>zero-shot</b> abilities. To fully exploit pretrained knowledge while minimizing training overhead, we keep SAM frozen, introducing only minimal extra parameters and computational overhead. In particular, we introduce High-Resolution (HR) token, Low-Resolution (LR) token and dual mask decoder. This decoder integrates the original SAM mask decoder with a lightweight fusion module that integrates features at multiple scales. Instead of predicting a mask independently, we integrate HR and LR token at intermediate layer to jointly learn features of the same object across multiple resolutions. Experiments show that our WSI-SAM outperforms state-of-the-art SAM and its variants. In particular, our model outperforms SAM by 4.1 and 2.5 percent points on a ductal carcinoma in situ (DCIS) segmentation tasks and breast cancer metastasis segmentation task (CAMELYON16 dataset). The code will be available at <a href=https://github.com/HongLiuuuuu/WSI-SAM>https://github.com/HongLiuuuuu/WSI-SAM</a>.</p></p class="citation"></blockquote><h3 id=69101--69320-generalized-relevance-learning-grassmann-quantization-m-mohammadi-et-al-2024>(69/101 | 69/320) Generalized Relevance Learning Grassmann Quantization (M. Mohammadi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>M. Mohammadi, M. Babai, M. H. F. Wilkinson. (2024)<br><strong>Generalized Relevance Learning Grassmann Quantization</strong><br><button class=copy-to-clipboard title="Generalized Relevance Learning Grassmann Quantization" index=69>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-69 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 20<br>Keywords: Face Recognition, Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09183v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09183v1.pdf filename=2403.09183v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Due to advancements in digital cameras, it is easy to gather multiple images (or videos) from an object under different conditions. Therefore, image-set classification has attracted more attention, and different solutions were proposed to model them. A popular way to model image sets is subspaces, which form a manifold called the Grassmann manifold. In this contribution, we extend the application of Generalized Relevance Learning Vector <b>Quantization</b> to deal with Grassmann manifold. The proposed model returns a set of prototype subspaces and a relevance vector. While prototypes model typical behaviours within classes, the relevance factors specify the most discriminative principal vectors (or images) for the classification task. They both provide insights into the model&rsquo;s decisions by highlighting influential images and pixels for predictions. Moreover, due to learning prototypes, the model complexity of the new method during inference is independent of dataset size, unlike previous works. We applied it to several recognition tasks including handwritten digit recognition, <b>face</b> <b>recognition,</b> activity recognition, and object recognition. Experiments demonstrate that it outperforms previous works with lower complexity and can successfully model the variation, such as handwritten style or lighting conditions. Moreover, the presence of relevances makes the model robust to the selection of subspaces&rsquo; dimensionality.</p></p class="citation"></blockquote><h3 id=70101--70320-switch-diffusion-transformer-synergizing-denoising-tasks-with-sparse-mixture-of-experts-byeongjun-park-et-al-2024>(70/101 | 70/320) Switch Diffusion Transformer: Synergizing Denoising Tasks with Sparse Mixture-of-Experts (Byeongjun Park et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Byeongjun Park, Hyojun Go, Jin-Young Kim, Sangmin Woo, Seokil Ham, Changick Kim. (2024)<br><strong>Switch Diffusion Transformer: Synergizing Denoising Tasks with Sparse Mixture-of-Experts</strong><br><button class=copy-to-clipboard title="Switch Diffusion Transformer: Synergizing Denoising Tasks with Sparse Mixture-of-Experts" index=70>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-70 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09176v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09176v1.pdf filename=2403.09176v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Diffusion</b> <b>models</b> have achieved remarkable success across a range of generative tasks. Recent efforts to enhance <b>diffusion</b> <b>model</b> architectures have reimagined them as a form of multi-task learning, where each task corresponds to a denoising task at a specific noise level. While these efforts have focused on parameter isolation and task routing, they fall short of capturing detailed inter-task relationships and risk losing semantic information, respectively. In response, we introduce Switch <b>Diffusion</b> <b>Transformer</b> (Switch-DiT), which establishes inter-task relationships between conflicting tasks without compromising semantic information. To achieve this, we employ a sparse mixture-of-experts within each <b>transformer</b> block to utilize semantic information and facilitate handling conflicts in tasks through parameter isolation. Additionally, we propose a <b>diffusion</b> <b>prior</b> loss, encouraging similar tasks to share their denoising paths while isolating conflicting ones. Through these, each <b>transformer</b> block contains a shared expert across all tasks, where the common and task-specific denoising paths enable the <b>diffusion</b> <b>model</b> to construct its beneficial way of synergizing denoising tasks. Extensive experiments validate the effectiveness of our approach in improving both image quality and convergence rate, and further analysis demonstrates that Switch-DiT constructs tailored denoising paths across various generation scenarios.</p></p class="citation"></blockquote><h3 id=71101--71320-dyadic-interaction-modeling-for-social-behavior-generation-minh-tran-et-al-2024>(71/101 | 71/320) Dyadic Interaction Modeling for Social Behavior Generation (Minh Tran et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Minh Tran, Di Chang, Maksim Siniukov, Mohammad Soleymani. (2024)<br><strong>Dyadic Interaction Modeling for Social Behavior Generation</strong><br><button class=copy-to-clipboard title="Dyadic Interaction Modeling for Social Behavior Generation" index=71>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-71 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Contrastive Learning, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09069v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09069v1.pdf filename=2403.09069v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Human-human communication is like a delicate dance where listeners and speakers concurrently interact to maintain conversational dynamics. Hence, an effective model for generating listener nonverbal behaviors requires understanding the dyadic context and interaction. In this paper, we present an effective framework for creating 3D facial motions in dyadic interactions. Existing work consider a listener as a reactive agent with reflexive behaviors to the speaker&rsquo;s voice and facial motions. The heart of our framework is Dyadic Interaction Modeling (DIM), a pre-training approach that jointly models speakers&rsquo; and listeners&rsquo; motions through masking and <b>contrastive</b> <b>learning</b> to learn representations that capture the dyadic context. To enable the generation of non-deterministic behaviors, we encode both listener and speaker motions into discrete latent representations, through VQ-VAE. The pre-trained model is further <b>fine-tuned</b> for motion generation. Extensive experiments demonstrate the superiority of our framework in generating listener motions, establishing a new state-of-the-art according to the quantitative measures capturing the diversity and realism of generated motions. Qualitative results demonstrate the superior capabilities of the proposed approach in generating diverse and realistic expressions, eye blinks and head gestures.</p></p class="citation"></blockquote><h3 id=72101--72320-leveraging-foundation-model-automatic-data-augmentation-strategies-and-skeletal-points-for-hands-action-recognition-in-industrial-assembly-lines-liang-wu-et-al-2024>(72/101 | 72/320) Leveraging Foundation Model Automatic Data Augmentation Strategies and Skeletal Points for Hands Action Recognition in Industrial Assembly Lines (Liang Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Liang Wu, X. -G. Ma. (2024)<br><strong>Leveraging Foundation Model Automatic Data Augmentation Strategies and Skeletal Points for Hands Action Recognition in Industrial Assembly Lines</strong><br><button class=copy-to-clipboard title="Leveraging Foundation Model Automatic Data Augmentation Strategies and Skeletal Points for Hands Action Recognition in Industrial Assembly Lines" index=72>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-72 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Data Augmentation, Foundation Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09056v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09056v1.pdf filename=2403.09056v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>On modern industrial assembly lines, many intelligent algorithms have been developed to replace or supervise workers. However, we found that there were bottlenecks in both training datasets and real-time performance when deploying algorithms on actual assembly line. Therefore, we developed a promising strategy for expanding industrial datasets, which utilized large models with strong generalization abilities to achieve efficient, high-quality, and large-scale dataset expansion, solving the problem of insufficient and low-quality industrial datasets. We also applied this strategy to video action recognition. We proposed a method of converting hand action recognition problems into hand skeletal trajectory classification problems, which solved the real-time performance problem of industrial algorithms. In the &ldquo;hand movements during wire insertion&rdquo; scenarios on the actual assembly line, the accuracy of hand action recognition reached 98.8%. We conducted detailed experimental analysis to demonstrate the effectiveness and superiority of the method, and deployed the entire process on Midea&rsquo;s actual assembly line.</p></p class="citation"></blockquote><h3 id=73101--73320-towards-comprehensive-multimodal-perception-introducing-the-touch-language-vision-dataset-ning-cheng-et-al-2024>(73/101 | 73/320) Towards Comprehensive Multimodal Perception: Introducing the Touch-Language-Vision Dataset (Ning Cheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ning Cheng, You Li, Jing Gao, Bin Fang, Jinan Xu, Wenjuan Han. (2024)<br><strong>Towards Comprehensive Multimodal Perception: Introducing the Touch-Language-Vision Dataset</strong><br><button class=copy-to-clipboard title="Towards Comprehensive Multimodal Perception: Introducing the Touch-Language-Vision Dataset" index=73>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-73 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV<br>Keyword Score: 16<br>Keywords: Fine-tuning, Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09813v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09813v1.pdf filename=2403.09813v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Tactility provides crucial support and enhancement for the perception and interaction capabilities of both humans and robots. Nevertheless, the <b>multimodal</b> research related to touch primarily focuses on visual and tactile modalities, with limited exploration in the domain of language. Beyond vocabulary, sentence-level descriptions contain richer semantics. Based on this, we construct a touch-language-vision dataset named TLV (Touch-Language-Vision) by human-machine cascade collaboration, featuring sentence-level descriptions for multimode alignment. The new dataset is used to <b>fine-tune</b> our proposed lightweight training framework, TLV-Link (Linking Touch, Language, and Vision through Alignment), achieving effective semantic alignment with minimal parameter adjustments (1%). Project Page: <a href=https://xiaoen0.github.io/touch.page/>https://xiaoen0.github.io/touch.page/</a>.</p></p class="citation"></blockquote><h3 id=74101--74320-on-the-utility-of-3d-hand-poses-for-action-recognition-md-salman-shamil-et-al-2024>(74/101 | 74/320) On the Utility of 3D Hand Poses for Action Recognition (Md Salman Shamil et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Md Salman Shamil, Dibyadip Chatterjee, Fadime Sener, Shugao Ma, Angela Yao. (2024)<br><strong>On the Utility of 3D Hand Poses for Action Recognition</strong><br><button class=copy-to-clipboard title="On the Utility of 3D Hand Poses for Action Recognition" index=74>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-74 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 16<br>Keywords: Multi-modal, Multi-modal, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09805v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09805v1.pdf filename=2403.09805v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>3D hand poses are an under-explored modality for action recognition. Poses are compact yet informative and can greatly benefit applications with limited compute budgets. However, poses alone offer an incomplete understanding of actions, as they cannot fully capture objects and environments with which humans interact. To efficiently model hand-object interactions, we propose HandFormer, a novel <b>multimodal</b> <b>transformer.</b> HandFormer combines 3D hand poses at a high temporal resolution for fine-grained motion modeling with sparsely sampled RGB frames for encoding scene semantics. Observing the unique characteristics of hand poses, we temporally factorize hand modeling and represent each joint by its short-term trajectories. This factorized pose representation combined with sparse RGB samples is remarkably efficient and achieves high accuracy. Unimodal HandFormer with only hand poses outperforms existing skeleton-based methods at 5x fewer FLOPs. With RGB, we achieve new state-of-the-art performance on Assembly101 and H2O with significant improvements in egocentric action recognition.</p></p class="citation"></blockquote><h3 id=75101--75320-mambatalk-efficient-holistic-gesture-synthesis-with-selective-state-space-models-zunnan-xu-et-al-2024>(75/101 | 75/320) MambaTalk: Efficient Holistic Gesture Synthesis with Selective State Space Models (Zunnan Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zunnan Xu, Yukang Lin, Haonan Han, Sicheng Yang, Ronghui Li, Yachao Zhang, Xiu Li. (2024)<br><strong>MambaTalk: Efficient Holistic Gesture Synthesis with Selective State Space Models</strong><br><button class=copy-to-clipboard title="MambaTalk: Efficient Holistic Gesture Synthesis with Selective State Space Models" index=75>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-75 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-HC, cs.CV<br>Keyword Score: 16<br>Keywords: Diffusion Model, Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09471v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09471v1.pdf filename=2403.09471v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Gesture synthesis is a vital realm of human-computer interaction, with wide-ranging applications across various fields like film, robotics, and virtual reality. Recent advancements have utilized the <b>diffusion</b> <b>model</b> and attention mechanisms to improve gesture synthesis. However, due to the high computational complexity of these techniques, generating long and diverse sequences with low latency remains a challenge. We explore the potential of state space models (SSMs) to address the challenge, implementing a two-stage modeling strategy with discrete motion priors to enhance the quality of gestures. Leveraging the foundational Mamba block, we introduce MambaTalk, enhancing gesture diversity and rhythm through <b>multimodal</b> integration. Extensive experiments demonstrate that our method matches or exceeds the performance of state-of-the-art models.</p></p class="citation"></blockquote><h3 id=76101--76320-efficientmfd-towards-more-efficient-multimodal-synchronous-fusion-detection-jiaqing-zhang-et-al-2024>(76/101 | 76/320) EfficientMFD: Towards More Efficient Multimodal Synchronous Fusion Detection (Jiaqing Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiaqing Zhang, Mingxiang Cao, Xue Yang, Weiying Xie, Jie Lei, Daixun Li, Geng Yang, Wenbo Huang, Yunsong Li. (2024)<br><strong>EfficientMFD: Towards More Efficient Multimodal Synchronous Fusion Detection</strong><br><button class=copy-to-clipboard title="EfficientMFD: Towards More Efficient Multimodal Synchronous Fusion Detection" index=76>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-76 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 16<br>Keywords: Object Detection, Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09323v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09323v1.pdf filename=2403.09323v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multimodal</b> image fusion and <b>object</b> <b>detection</b> play a vital role in autonomous driving. Current joint learning methods have made significant progress in the <b>multimodal</b> fusion detection task combining the texture detail and objective semantic information. However, the tedious training steps have limited its applications to wider real-world industrial deployment. To address this limitation, we propose a novel end-to-end <b>multimodal</b> fusion detection algorithm, named EfficientMFD, to simplify models that exhibit decent performance with only one training step. Synchronous joint optimization is utilized in an end-to-end manner between two components, thus not being affected by the local optimal solution of the individual task. Besides, a comprehensive optimization is established in the gradient matrix between the shared parameters for both tasks. It can converge to an optimal point with fusion detection weights. We extensively test it on several public datasets, demonstrating superior performance on not only visually appealing fusion but also favorable detection performance (e.g., 6.6% mAP50:95) over other state-of-the-art approaches.</p></p class="citation"></blockquote><h3 id=77101--77320-poifusion-multi-modal-3d-object-detection-via-fusion-at-points-of-interest-jiajun-deng-et-al-2024>(77/101 | 77/320) PoIFusion: Multi-Modal 3D Object Detection via Fusion at Points of Interest (Jiajun Deng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiajun Deng, Sha Zhang, Feras Dayoub, Wanli Ouyang, Yanyong Zhang, Ian Reid. (2024)<br><strong>PoIFusion: Multi-Modal 3D Object Detection via Fusion at Points of Interest</strong><br><button class=copy-to-clipboard title="PoIFusion: Multi-Modal 3D Object Detection via Fusion at Points of Interest" index=77>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-77 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 16<br>Keywords: Object Detection, Benchmarking, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09212v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09212v1.pdf filename=2403.09212v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we present PoIFusion, a simple yet effective <b>multi-modal</b> 3D <b>object</b> <b>detection</b> framework to fuse the information of RGB images and LiDAR point clouds at the point of interest (abbreviated as PoI). Technically, our PoIFusion follows the paradigm of query-based <b>object</b> <b>detection,</b> formulating <b>object</b> <b>queries</b> as dynamic 3D boxes. The PoIs are adaptively generated from each query box on the fly, serving as the keypoints to represent a 3D <b>object</b> <b>and</b> play the role of basic units in <b>multi-modal</b> fusion. Specifically, we project PoIs into the view of each modality to sample the corresponding feature and integrate the <b>multi-modal</b> features at each PoI through a dynamic fusion block. Furthermore, the features of PoIs derived from the same query box are aggregated together to update the query feature. Our approach prevents information loss caused by view transformation and eliminates the computation-intensive global attention, making the <b>multi-modal</b> 3D <b>object</b> <b>detector</b> more applicable. We conducted extensive experiments on the nuScenes dataset to evaluate our approach. Remarkably, our PoIFusion achieves 74.9% NDS and 73.4% mAP, setting a state-of-the-art record on the <b>multi-modal</b> 3D <b>object</b> <b>detection</b> <b>benchmark.</b> Codes will be made available via \url{https://djiajunustc.github.io/projects/poifusion}.</p></p class="citation"></blockquote><h3 id=78101--78320-holo-relighting-controllable-volumetric-portrait-relighting-from-a-single-image-yiqun-mei-et-al-2024>(78/101 | 78/320) Holo-Relighting: Controllable Volumetric Portrait Relighting from a Single Image (Yiqun Mei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yiqun Mei, Yu Zeng, He Zhang, Zhixin Shu, Xuaner Zhang, Sai Bi, Jianming Zhang, HyunJoon Jung, Vishal M. Patel. (2024)<br><strong>Holo-Relighting: Controllable Volumetric Portrait Relighting from a Single Image</strong><br><button class=copy-to-clipboard title="Holo-Relighting: Controllable Volumetric Portrait Relighting from a Single Image" index=78>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-78 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 15<br>Keywords: Generative Adversarial Network, Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09632v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09632v1.pdf filename=2403.09632v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>At the core of portrait photography is the search for ideal lighting and viewpoint. The process often requires advanced knowledge in photography and an elaborate studio setup. In this work, we propose Holo-Relighting, a volumetric relighting method that is capable of synthesizing novel viewpoints, and novel lighting from a single image. Holo-Relighting leverages the pretrained 3D <b>GAN</b> (EG3D) to reconstruct <b>geometry</b> and appearance from an input portrait as a set of 3D-aware features. We design a relighting module conditioned on a given lighting to process these features, and predict a relit 3D representation in the form of a tri-plane, which can render to an arbitrary viewpoint through volume rendering. Besides viewpoint and lighting control, Holo-Relighting also takes the head pose as a condition to enable head-pose-dependent lighting effects. With these novel designs, Holo-Relighting can generate complex non-Lambertian lighting effects (e.g., specular highlights and cast shadows) without using any explicit physical lighting priors. We train Holo-Relighting with data captured with a light stage, and propose two data-rendering techniques to improve the data quality for training the volumetric relighting system. Through quantitative and qualitative experiments, we demonstrate Holo-Relighting can achieve state-of-the-arts relighting quality with better photorealism, 3D consistency and controllability.</p></p class="citation"></blockquote><h3 id=79101--79320-3d-scenedreamer-text-driven-3d-consistent-scene-generation-frank-zhang-et-al-2024>(79/101 | 79/320) 3D-SceneDreamer: Text-Driven 3D-Consistent Scene Generation (Frank Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Frank Zhang, Yibo Zhang, Quan Zheng, Rui Ma, Wei Hua, Hujun Bao, Weiwei Xu, Changqing Zou. (2024)<br><strong>3D-SceneDreamer: Text-Driven 3D-Consistent Scene Generation</strong><br><button class=copy-to-clipboard title="3D-SceneDreamer: Text-Driven 3D-Consistent Scene Generation" index=79>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-79 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 15<br>Keywords: Diffusion Model, Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09439v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09439v1.pdf filename=2403.09439v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Text-driven 3D scene generation techniques have made rapid progress in recent years. Their success is mainly attributed to using existing generative models to iteratively perform image warping and inpainting to generate 3D scenes. However, these methods heavily rely on the outputs of existing models, leading to error accumulation in <b>geometry</b> and appearance that prevent the models from being used in various scenarios (e.g., outdoor and unreal scenarios). To address this limitation, we generatively refine the newly generated local views by querying and aggregating global 3D information, and then progressively generate the 3D scene. Specifically, we employ a tri-plane features-based NeRF as a unified representation of the 3D scene to constrain global 3D consistency, and propose a generative refinement network to synthesize new contents with higher quality by exploiting the natural image prior from 2D <b>diffusion</b> <b>model</b> as well as the global 3D information of the current scene. Our extensive experiments demonstrate that, in comparison to previous methods, our approach supports wide variety of scene generation and arbitrary camera trajectories with improved visual quality and 3D consistency.</p></p class="citation"></blockquote><h3 id=80101--80320-sd-net-symmetric-aware-keypoint-prediction-and-domain-adaptation-for-6d-pose-estimation-in-bin-picking-scenarios-ding-tao-huang-et-al-2024>(80/101 | 80/320) SD-Net: Symmetric-Aware Keypoint Prediction and Domain Adaptation for 6D Pose Estimation In Bin-picking Scenarios (Ding-Tao Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ding-Tao Huang, En-Te Lin, Lipeng Chen, Li-Fu Liu, Long Zeng. (2024)<br><strong>SD-Net: Symmetric-Aware Keypoint Prediction and Domain Adaptation for 6D Pose Estimation In Bin-picking Scenarios</strong><br><button class=copy-to-clipboard title="SD-Net: Symmetric-Aware Keypoint Prediction and Domain Adaptation for 6D Pose Estimation In Bin-picking Scenarios" index=80>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-80 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 15<br>Keywords: Geometry, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09317v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09317v1.pdf filename=2403.09317v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite the success in 6D pose estimation in bin-picking scenarios, existing methods still struggle to produce accurate prediction results for symmetry objects and real world scenarios. The primary bottlenecks include 1) the ambiguity keypoints caused by object symmetries; 2) the <b>domain</b> <b>gap</b> between real and synthetic data. To circumvent these problem, we propose a new 6D pose estimation network with symmetric-aware keypoint prediction and self-training <b>domain</b> <b>adaptation</b> (SD-Net). SD-Net builds on pointwise keypoint regression and deep hough voting to perform reliable detection keypoint under clutter and occlusion. Specifically, at the keypoint prediction stage, we designe a robust 3D keypoints selection strategy considering the symmetry class of objects and equivalent keypoints, which facilitate locating 3D keypoints even in highly occluded scenes. Additionally, we build an effective filtering algorithm on predicted keypoint to dynamically eliminate multiple ambiguity and outlier keypoint candidates. At the <b>domain</b> <b>adaptation</b> stage, we propose the self-training framework using a student-teacher training scheme. To carefully distinguish reliable predictions, we harnesses a tailored heuristics for 3D <b>geometry</b> pseudo labelling based on semi-chamfer distance. On public Sil&rsquo;eane dataset, SD-Net achieves state-of-the-art results, obtaining an average precision of 96%. Testing learning and generalization abilities on public Parametric datasets, SD-Net is 8% higher than the state-of-the-art method. The code is available at <a href=https://github.com/dingthuang/SD-Net>https://github.com/dingthuang/SD-Net</a>.</p></p class="citation"></blockquote><h3 id=81101--81320-sculpt3d-multi-view-consistent-text-to-3d-generation-with-sparse-3d-prior-cheng-chen-et-al-2024>(81/101 | 81/320) Sculpt3D: Multi-View Consistent Text-to-3D Generation with Sparse 3D Prior (Cheng Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Cheng Chen, Xiaofeng Yang, Fan Yang, Chengzeng Feng, Zhoujie Fu, Chuan-Sheng Foo, Guosheng Lin, Fayao Liu. (2024)<br><strong>Sculpt3D: Multi-View Consistent Text-to-3D Generation with Sparse 3D Prior</strong><br><button class=copy-to-clipboard title="Sculpt3D: Multi-View Consistent Text-to-3D Generation with Sparse 3D Prior" index=81>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-81 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 15<br>Keywords: Diffusion Model, Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09140v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09140v1.pdf filename=2403.09140v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent works on text-to-3d generation show that using only 2D <b>diffusion</b> <b>supervision</b> for 3D generation tends to produce results with inconsistent appearances (e.g., faces on the back view) and inaccurate shapes (e.g., animals with extra legs). Existing methods mainly address this issue by retraining <b>diffusion</b> <b>models</b> with images rendered from 3D data to ensure multi-view consistency while struggling to balance 2D generation quality with 3D consistency. In this paper, we present a new framework Sculpt3D that equips the current pipeline with explicit injection of 3D priors from retrieved reference objects without re-training the 2D <b>diffusion</b> <b>model.</b> Specifically, we demonstrate that high-quality and diverse 3D <b>geometry</b> can be guaranteed by keypoints supervision through a sparse ray sampling approach. Moreover, to ensure accurate appearances of different views, we further modulate the output of the 2D <b>diffusion</b> <b>model</b> to the correct patterns of the template views without altering the generated object&rsquo;s style. These two decoupled designs effectively harness 3D information from reference objects to generate 3D objects while preserving the generation quality of the 2D <b>diffusion</b> <b>model.</b> Extensive experiments show our method can largely improve the multi-view consistency while retaining fidelity and diversity. Our project page is available at: <a href=https://stellarcheng.github.io/Sculpt3D/>https://stellarcheng.github.io/Sculpt3D/</a>.</p></p class="citation"></blockquote><h3 id=82101--82320-thermohands-a-benchmark-for-3d-hand-pose-estimation-from-egocentric-thermal-image-fangqiang-ding-et-al-2024>(82/101 | 82/320) ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric Thermal Image (Fangqiang Ding et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fangqiang Ding, Yunzhou Zhu, Xiangyu Wen, Chris Xiaoxuan Lu. (2024)<br><strong>ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric Thermal Image</strong><br><button class=copy-to-clipboard title="ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric Thermal Image" index=82>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-82 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-HC, cs-LG, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09871v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09871v1.pdf filename=2403.09871v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we present ThermoHands, a new <b>benchmark</b> for thermal image-based egocentric 3D hand pose estimation, aimed at overcoming challenges like varying lighting and obstructions (e.g., handwear). The <b>benchmark</b> includes a diverse dataset from 28 subjects performing hand-object and hand-virtual interactions, accurately annotated with 3D hand poses through an automated process. We introduce a bespoken baseline method, TheFormer, utilizing dual <b>transformer</b> modules for effective egocentric 3D hand pose estimation in thermal imagery. Our experimental results highlight TheFormer&rsquo;s leading performance and affirm thermal imaging&rsquo;s effectiveness in enabling robust 3D hand pose estimation in adverse conditions.</p></p class="citation"></blockquote><h3 id=83101--83320-score-guided-diffusion-for-3d-human-recovery-anastasis-stathopoulos-et-al-2024>(83/101 | 83/320) Score-Guided Diffusion for 3D Human Recovery (Anastasis Stathopoulos et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anastasis Stathopoulos, Ligong Han, Dimitris Metaxas. (2024)<br><strong>Score-Guided Diffusion for 3D Human Recovery</strong><br><button class=copy-to-clipboard title="Score-Guided Diffusion for 3D Human Recovery" index=83>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-83 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Diffusion Model, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09623v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09623v1.pdf filename=2403.09623v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present Score-Guided Human Mesh Recovery (ScoreHMR), an approach for solving inverse problems for 3D human pose and shape reconstruction. These inverse problems involve fitting a human body model to image observations, traditionally solved through optimization techniques. ScoreHMR mimics model fitting approaches, but alignment with the image observation is achieved through score guidance in the latent space of a <b>diffusion</b> <b>model.</b> The <b>diffusion</b> <b>model</b> is trained to capture the conditional distribution of the human model parameters given an input image. By guiding its denoising process with a task-specific score, ScoreHMR effectively solves inverse problems for various applications without the need for retraining the task-agnostic <b>diffusion</b> <b>model.</b> We evaluate our approach on three settings/applications. These are: (i) single-frame model fitting; (ii) reconstruction from multiple uncalibrated views; (iii) reconstructing humans in video sequences. ScoreHMR consistently outperforms all optimization baselines on popular <b>benchmarks</b> across all settings. We make our code and models available at the <a href=https://statho.github.io/ScoreHMR>https://statho.github.io/ScoreHMR</a>.</p></p class="citation"></blockquote><h3 id=84101--84320-efficient-transferability-assessment-for-selection-of-pre-trained-detectors-zhao-wang-et-al-2024>(84/101 | 84/320) Efficient Transferability Assessment for Selection of Pre-trained Detectors (Zhao Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhao Wang, Aoxue Li, Zhenguo Li, Qi Dou. (2024)<br><strong>Efficient Transferability Assessment for Selection of Pre-trained Detectors</strong><br><button class=copy-to-clipboard title="Efficient Transferability Assessment for Selection of Pre-trained Detectors" index=84>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-84 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09432v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09432v1.pdf filename=2403.09432v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Large-scale pre-training followed by downstream <b>fine-tuning</b> is an effective solution for transferring deep-learning-based models. Since <b>finetuning</b> all possible pre-trained models is computational costly, we aim to predict the transferability performance of these pre-trained models in a computational efficient manner. Different from previous work that seek out suitable models for downstream classification and segmentation tasks, this paper studies the efficient transferability assessment of pre-trained object detectors. To this end, we build up a detector transferability <b>benchmark</b> which contains a large and diverse zoo of pre-trained detectors with various architectures, source datasets and training schemes. Given this zoo, we adopt 7 target datasets from 5 diverse domains as the downstream target tasks for evaluation. Further, we propose to assess classification and regression sub-tasks simultaneously in a unified framework. Additionally, we design a complementary metric for evaluating tasks with varying objects. Experimental results demonstrate that our method outperforms other state-of-the-art approaches in assessing transferability under different target domains while efficiently reducing wall-clock time 32$\times$ and requires a mere 5.2% memory footprint compared to brute-force <b>fine-tuning</b> of all pre-trained detectors.</p></p class="citation"></blockquote><h3 id=85101--85320-gradient-aware-logit-adjustment-loss-for-long-tailed-classifier-fan-zhang-et-al-2024>(85/101 | 85/320) Gradient-Aware Logit Adjustment Loss for Long-tailed Classifier (Fan Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fan Zhang, Wei Qin, Weijieying Ren, Lei Wang, Zetong Chen, Richang Hong. (2024)<br><strong>Gradient-Aware Logit Adjustment Loss for Long-tailed Classifier</strong><br><button class=copy-to-clipboard title="Gradient-Aware Logit Adjustment Loss for Long-tailed Classifier" index=85>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-85 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Graph Contrastive Learning, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09036v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09036v1.pdf filename=2403.09036v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the real-world setting, data often follows a long-tailed distribution, where head classes contain significantly more training samples than tail classes. Consequently, models trained on such data tend to be biased toward head classes. The medium of this bias is imbalanced gradients, which include not only the ratio of scale between positive and negative gradients but also imbalanced gradients from different negative classes. Therefore, we propose the Gradient-Aware Logit Adjustment (GALA) loss, which adjusts the logits based on accumulated gradients to balance the optimization process. Additionally, We find that most of the solutions to long-tailed problems are still biased towards head classes in the end, and we propose a simple and post hoc prediction re-balancing strategy to further mitigate the basis toward head class. Extensive experiments are conducted on multiple popular long-tailed recognition <b>benchmark</b> datasets to evaluate the effectiveness of these two designs. Our approach achieves top-1 accuracy of 48.5%, 41.4%, and 73.3% on CIFAR100-LT, Places-LT, and iNaturalist, outperforming the state-of-the-art method <b>GCL</b> by a significant margin of 3.62%, 0.76% and 1.2%, respectively. Code is available at <a href=https://github.com/lt-project-repository/lt-project>https://github.com/lt-project-repository/lt-project</a>.</p></p class="citation"></blockquote><h3 id=86101--86320-explorations-in-texture-learning-blaine-hoak-et-al-2024>(86/101 | 86/320) Explorations in Texture Learning (Blaine Hoak et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Blaine Hoak, Patrick McDaniel. (2024)<br><strong>Explorations in Texture Learning</strong><br><button class=copy-to-clipboard title="Explorations in Texture Learning" index=86>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-86 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 10<br>Keywords: Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09543v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09543v1.pdf filename=2403.09543v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we investigate \textit{texture learning}: the identification of textures learned by object classification models, and the extent to which they rely on these textures. We build texture-object associations that uncover new insights about the relationships between texture and object classes in <b>CNNs</b> and find three classes of results: associations that are strong and expected, strong and not expected, and expected but not present. Our analysis demonstrates that investigations in texture learning enable new methods for interpretability and have the potential to uncover unexpected biases.</p></p class="citation"></blockquote><h3 id=87101--87320-what-sketch-explainability-really-means-for-downstream-tasks-hmrishav-bandyopadhyay-et-al-2024>(87/101 | 87/320) What Sketch Explainability Really Means for Downstream Tasks (Hmrishav Bandyopadhyay et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hmrishav Bandyopadhyay, Pinaki Nath Chowdhury, Ayan Kumar Bhunia, Aneeshan Sain, Tao Xiang, Yi-Zhe Song. (2024)<br><strong>What Sketch Explainability Really Means for Downstream Tasks</strong><br><button class=copy-to-clipboard title="What Sketch Explainability Really Means for Downstream Tasks" index=87>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-87 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09480v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09480v1.pdf filename=2403.09480v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we explore the unique modality of sketch for explainability, emphasising the profound impact of human strokes compared to conventional pixel-oriented studies. Beyond explanations of network behavior, we discern the genuine implications of explainability across diverse downstream sketch-related tasks. We propose a lightweight and portable explainability solution &ndash; a seamless plugin that integrates effortlessly with any pre-trained model, eliminating the need for re-training. Demonstrating its adaptability, we present four applications: highly studied retrieval and generation, and completely novel assisted drawing and sketch <b>adversarial</b> <b>attacks.</b> The centrepiece to our solution is a stroke-level attribution map that takes different forms when linked with downstream tasks. By addressing the inherent non-differentiability of rasterisation, we enable explanations at both coarse stroke level (SLA) and partial stroke level (P-SLA), each with its advantages for specific downstream tasks.</p></p class="citation"></blockquote><h3 id=88101--88320-improving-real-time-omnidirectional-3d-multi-person-human-pose-estimation-with-people-matching-and-unsupervised-2d-3d-lifting-pawel-knap-et-al-2024>(88/101 | 88/320) Improving Real-Time Omnidirectional 3D Multi-Person Human Pose Estimation with People Matching and Unsupervised 2D-3D Lifting (Pawel Knap et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pawel Knap, Peter Hardy, Alberto Tamajo, Hwasup Lim, Hansung Kim. (2024)<br><strong>Improving Real-Time Omnidirectional 3D Multi-Person Human Pose Estimation with People Matching and Unsupervised 2D-3D Lifting</strong><br><button class=copy-to-clipboard title="Improving Real-Time Omnidirectional 3D Multi-Person Human Pose Estimation with People Matching and Unsupervised 2D-3D Lifting" index=88>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-88 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09437v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09437v1.pdf filename=2403.09437v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Current human pose estimation systems focus on retrieving an accurate 3D global estimate of a single person. Therefore, this paper presents one of the first 3D multi-person human pose estimation systems that is able to work in real-time and is also able to handle basic forms of occlusion. First, we adjust an off-the-shelf 2D detector and an <b>unsupervised</b> 2D-3D lifting model for use with a 360$^\circ$ panoramic camera and mmWave radar sensors. We then introduce several contributions, including camera and radar calibrations, and the improved matching of people within the image and radar space. The system addresses both the depth and scale ambiguity problems by employing a lightweight 2D-3D pose lifting algorithm that is able to work in real-time while exhibiting accurate performance in both indoor and outdoor environments which offers both an affordable and scalable solution. Notably, our system&rsquo;s time complexity remains nearly constant irrespective of the number of detected individuals, achieving a frame rate of approximately 7-8 fps on a laptop with a commercial-grade GPU.</p></p class="citation"></blockquote><h3 id=89101--89320-eventrpg-event-data-augmentation-with-relevance-propagation-guidance-mingyuan-sun-et-al-2024>(89/101 | 89/320) EventRPG: Event Data Augmentation with Relevance Propagation Guidance (Mingyuan Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mingyuan Sun, Donghao Zhang, Zongyuan Ge, Jiaxu Wang, Jia Li, Zheng Fang, Renjing Xu. (2024)<br><strong>EventRPG: Event Data Augmentation with Relevance Propagation Guidance</strong><br><button class=copy-to-clipboard title="EventRPG: Event Data Augmentation with Relevance Propagation Guidance" index=89>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-89 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Data Augmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09274v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09274v1.pdf filename=2403.09274v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Event camera, a novel bio-inspired vision sensor, has drawn a lot of attention for its low latency, low power consumption, and high dynamic range. Currently, overfitting remains a critical problem in event-based classification tasks for Spiking Neural Network (SNN) due to its relatively weak spatial representation capability. <b>Data</b> <b>augmentation</b> is a simple but efficient method to alleviate overfitting and improve the generalization ability of neural networks, and saliency-based augmentation methods are proven to be effective in the image processing field. However, there is no approach available for extracting saliency maps from SNNs. Therefore, for the first time, we present Spiking Layer-Time-wise Relevance Propagation rule (SLTRP) and Spiking Layer-wise Relevance Propagation rule (SLRP) in order for SNN to generate stable and accurate CAMs and saliency maps. Based on this, we propose EventRPG, which leverages relevance propagation on the spiking neural network for more efficient augmentation. Our proposed method has been evaluated on several SNN structures, achieving state-of-the-art performance in object recognition tasks including N-Caltech101, CIFAR10-DVS, with accuracies of 85.62% and 85.55%, as well as action recognition task SL-Animals with an accuracy of 91.59%. Our code is available at <a href=https://github.com/myuansun/EventRPG>https://github.com/myuansun/EventRPG</a>.</p></p class="citation"></blockquote><h3 id=90101--90320-d-yolo-a-robust-framework-for-object-detection-in-adverse-weather-conditions-zihan-chu-2024>(90/101 | 90/320) D-YOLO a robust framework for object detection in adverse weather conditions (Zihan Chu, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zihan Chu. (2024)<br><strong>D-YOLO a robust framework for object detection in adverse weather conditions</strong><br><button class=copy-to-clipboard title="D-YOLO a robust framework for object detection in adverse weather conditions" index=90>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-90 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV<br>Keyword Score: 10<br>Keywords: Object Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09233v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09233v2.pdf filename=2403.09233v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Adverse weather conditions including haze, snow and rain lead to decline in image qualities, which often causes a decline in performance for deep-learning based detection networks. Most existing approaches attempts to rectify hazy images before performing <b>object</b> <b>detection,</b> which increases the complexity of the network and may result in the loss in latent information. To better integrate image restoration and <b>object</b> <b>detection</b> tasks, we designed a double-route network with an attention feature fusion module, taking both hazy and dehazed features into consideration. We also proposed a subnetwork to provide haze-free features to the detection network. Specifically, our D-YOLO improves the performance of the detection network by minimizing the distance between the clear feature extraction subnetwork and detection network. Experiments on RTTS and FoggyCityscapes datasets show that D-YOLO demonstrates better performance compared to the state-of-the-art methods. It is a robust detection framework for bridging the gap between low-level dehazing and high-level detection.</p></p class="citation"></blockquote><h3 id=91101--91320-improving-distant-3d-object-detection-using-2d-box-supervision-zetong-yang-et-al-2024>(91/101 | 91/320) Improving Distant 3D Object Detection Using 2D Box Supervision (Zetong Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zetong Yang, Zhiding Yu, Chris Choy, Renhao Wang, Anima Anandkumar, Jose M. Alvarez. (2024)<br><strong>Improving Distant 3D Object Detection Using 2D Box Supervision</strong><br><button class=copy-to-clipboard title="Improving Distant 3D Object Detection Using 2D Box Supervision" index=91>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-91 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Object Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09230v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09230v1.pdf filename=2403.09230v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Improving the detection of distant 3d <b>objects</b> <b>is</b> an important yet challenging task. For camera-based 3D perception, the annotation of 3d bounding relies heavily on LiDAR for accurate depth information. As such, the distance of annotation is often limited due to the sparsity of LiDAR points on distant <b>objects,</b> <b>which</b> hampers the capability of existing detectors for long-range scenarios. We address this challenge by considering only 2D box supervision for distant <b>objects</b> <b>since</b> they are easy to annotate. We propose LR3D, a framework that learns to recover the missing depth of distant <b>objects.</b> <b>LR3D</b> adopts an implicit projection head to learn the generation of mapping between 2D boxes and depth using the 3D supervision on close <b>objects.</b> <b>This</b> mapping allows the depth estimation of distant <b>objects</b> <b>conditioned</b> on their 2D boxes, making long-range 3D detection with 2D supervision feasible. Experiments show that without distant 3D annotations, LR3D allows camera-based methods to detect distant <b>objects</b> <b>(over</b> 200m) with comparable accuracy to full 3D supervision. Our framework is general, and could widely benefit 3D detection methods to a large extent.</p></p class="citation"></blockquote><h3 id=92101--92320-noise-dimension-of-gan-an-image-compression-perspective-ziran-zhu-et-al-2024>(92/101 | 92/320) Noise Dimension of GAN: An Image Compression Perspective (Ziran Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ziran Zhu, Tongda Xu, Ling Li, Yan Wang. (2024)<br><strong>Noise Dimension of GAN: An Image Compression Perspective</strong><br><button class=copy-to-clipboard title="Noise Dimension of GAN: An Image Compression Perspective" index=92>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-92 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09196v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09196v1.pdf filename=2403.09196v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Generative adversial network <b>(GAN)</b> is a type of generative model that maps a high-dimensional noise to samples in target distribution. However, the dimension of noise required in <b>GAN</b> is not well understood. Previous approaches view <b>GAN</b> as a mapping from a continuous distribution to another continous distribution. In this paper, we propose to view <b>GAN</b> as a discrete sampler instead. From this perspective, we build a connection between the minimum noise required and the bits to losslessly compress the images. Furthermore, to understand the behaviour of <b>GAN</b> when noise dimension is limited, we propose divergence-entropy trade-off. This trade-off depicts the best divergence we can achieve when noise is limited. And as rate distortion trade-off, it can be numerically solved when source distribution is known. Finally, we verifies our theory with experiments on image generation.</p></p class="citation"></blockquote><h3 id=93101--93320-intention-driven-ego-to-exo-video-generation-hongchen-luo-et-al-2024>(93/101 | 93/320) Intention-driven Ego-to-Exo Video Generation (Hongchen Luo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hongchen Luo, Kai Zhu, Wei Zhai, Yang Cao. (2024)<br><strong>Intention-driven Ego-to-Exo Video Generation</strong><br><button class=copy-to-clipboard title="Intention-driven Ego-to-Exo Video Generation" index=93>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-93 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09194v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09194v2.pdf filename=2403.09194v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Ego-to-exo video generation refers to generating the corresponding exocentric video according to the egocentric video, providing valuable applications in AR/VR and embodied AI. Benefiting from advancements in <b>diffusion</b> <b>model</b> techniques, notable progress has been achieved in video generation. However, existing methods build upon the spatiotemporal consistency assumptions between adjacent frames, which cannot be satisfied in the ego-to-exo scenarios due to drastic changes in views. To this end, this paper proposes an Intention-Driven Ego-to-exo video generation framework (IDE) that leverages action intention consisting of human movement and action description as view-independent representation to guide video generation, preserving the consistency of content and motion. Specifically, the egocentric head trajectory is first estimated through multi-view stereo matching. Then, cross-view feature perception module is introduced to establish correspondences between exo- and ego- views, guiding the trajectory transformation module to infer human full-body movement from the head trajectory. Meanwhile, we present an action description unit that maps the action semantics into the feature space consistent with the exocentric image. Finally, the inferred human movement and high-level action descriptions jointly guide the generation of exocentric motion and interaction content (i.e., corresponding optical flow and occlusion maps) in the backward process of the <b>diffusion</b> <b>model,</b> ultimately warping them into the corresponding exocentric video. We conduct extensive experiments on the relevant dataset with diverse exo-ego video pairs, and our IDE outperforms state-of-the-art models in both subjective and objective assessments, demonstrating its efficacy in ego-to-exo video generation.</p></p class="citation"></blockquote><h3 id=94101--94320-intention-aware-denoising-diffusion-model-for-trajectory-prediction-chen-liu-et-al-2024>(94/101 | 94/320) Intention-aware Denoising Diffusion Model for Trajectory Prediction (Chen Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chen Liu, Shibo He, Haoyu Liu, Jiming Chen. (2024)<br><strong>Intention-aware Denoising Diffusion Model for Trajectory Prediction</strong><br><button class=copy-to-clipboard title="Intention-aware Denoising Diffusion Model for Trajectory Prediction" index=94>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-94 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09190v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09190v1.pdf filename=2403.09190v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Trajectory prediction is an essential component in autonomous driving, particularly for collision avoidance systems. Considering the inherent uncertainty of the task, numerous studies have utilized generative models to produce multiple plausible future trajectories for each agent. However, most of them suffer from restricted representation ability or unstable training issues. To overcome these limitations, we propose utilizing the <b>diffusion</b> <b>model</b> to generate the distribution of future trajectories. Two cruxes are to be settled to realize such an idea. First, the diversity of intention is intertwined with the uncertain surroundings, making the true distribution hard to parameterize. Second, the <b>diffusion</b> <b>process</b> is time-consuming during the inference phase, rendering it unrealistic to implement in a real-time driving system. We propose an Intention-aware denoising <b>Diffusion</b> <b>Model</b> (IDM), which tackles the above two problems. We decouple the original uncertainty into intention uncertainty and action uncertainty and model them with two dependent <b>diffusion</b> <b>processes.</b> To decrease the inference time, we reduce the variable dimensions in the intention-aware <b>diffusion</b> <b>process</b> and restrict the initial distribution of the action-aware <b>diffusion</b> <b>process,</b> which leads to fewer <b>diffusion</b> <b>steps.</b> To validate our approach, we conduct experiments on the Stanford Drone Dataset (SDD) and ETH/UCY dataset. Our methods achieve state-of-the-art results, with an FDE of 13.83 pixels on the SDD dataset and 0.36 meters on the ETH/UCY dataset. Compared with the original <b>diffusion</b> <b>model,</b> IDM reduces inference time by two-thirds. Interestingly, our experiments further reveal that introducing intention information is beneficial in modeling the <b>diffusion</b> <b>process</b> of fewer steps.</p></p class="citation"></blockquote><h3 id=95101--95320-rethinking-referring-object-removal-xiangtian-xue-et-al-2024>(95/101 | 95/320) Rethinking Referring Object Removal (Xiangtian Xue et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiangtian Xue, Jiasong Wu, Youyong Kong, Lotfi Senhadji, Huazhong Shu. (2024)<br><strong>Rethinking Referring Object Removal</strong><br><button class=copy-to-clipboard title="Rethinking Referring Object Removal" index=95>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-95 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09128v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09128v1.pdf filename=2403.09128v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Referring object removal refers to removing the specific object in an image referred by natural language expressions and filling the missing region with reasonable semantics. To address this task, we construct the ComCOCO, a synthetic dataset consisting of 136,495 referring expressions for 34,615 objects in 23,951 image pairs. Each pair contains an image with referring expressions and the ground truth after elimination. We further propose an end-to-end syntax-aware hybrid mapping network with an encoding-decoding structure. Linguistic features are hierarchically extracted at the syntactic level and fused in the downsampling process of visual features with multi-head attention. The feature-aligned pyramid network is leveraged to generate segmentation masks and replace internal pixels with region affinity learned from external semantics in high-level feature maps. Extensive experiments demonstrate that our model outperforms <b>diffusion</b> <b>models</b> and two-stage methods which process the segmentation and inpainting task separately by a significant margin.</p></p class="citation"></blockquote><h3 id=96101--96320-desigen-a-pipeline-for-controllable-design-template-generation-haohan-weng-et-al-2024>(96/101 | 96/320) Desigen: A Pipeline for Controllable Design Template Generation (Haohan Weng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haohan Weng, Danqing Huang, Yu Qiao, Zheng Hu, Chin-Yew Lin, Tong Zhang, C. L. Philip Chen. (2024)<br><strong>Desigen: A Pipeline for Controllable Design Template Generation</strong><br><button class=copy-to-clipboard title="Desigen: A Pipeline for Controllable Design Template Generation" index=96>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-96 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09093v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09093v1.pdf filename=2403.09093v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Templates serve as a good starting point to implement a design (e.g., banner, slide) but it takes great effort from designers to manually create. In this paper, we present Desigen, an automatic template creation pipeline which generates background images as well as harmonious layout elements over the background. Different from natural images, a background image should preserve enough non-salient space for the overlaying layout elements. To equip existing advanced diffusion-based models with stronger spatial control, we propose two simple but effective techniques to constrain the saliency distribution and reduce the attention weight in desired regions during the background generation process. Then conditioned on the background, we synthesize the layout with a <b>Transformer-based</b> autoregressive generator. To achieve a more harmonious composition, we propose an iterative inference strategy to adjust the synthesized background and layout in multiple rounds. We constructed a design dataset with more than 40k advertisement banners to verify our approach. Extensive experiments demonstrate that the proposed pipeline generates high-quality templates comparable to human designers. More than a single-page design, we further show an application of presentation generation that outputs a set of theme-consistent slides. The data and code are available at <a href=https://whaohan.github.io/desigen>https://whaohan.github.io/desigen</a>.</p></p class="citation"></blockquote><h3 id=97101--97320-cloaf-collision-aware-human-flow-andrey-davydov-et-al-2024>(97/101 | 97/320) CLOAF: CoLlisiOn-Aware Human Flow (Andrey Davydov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andrey Davydov, Martin Engilberge, Mathieu Salzmann, Pascal Fua. (2024)<br><strong>CLOAF: CoLlisiOn-Aware Human Flow</strong><br><button class=copy-to-clipboard title="CLOAF: CoLlisiOn-Aware Human Flow" index=97>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-97 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09050v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09050v1.pdf filename=2403.09050v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Even the best current algorithms for estimating body 3D shape and pose yield results that include body self-intersections. In this paper, we present CLOAF, which exploits the diffeomorphic nature of Ordinary Differential Equations to eliminate such self-intersections while still imposing body shape constraints. We show that, unlike earlier approaches to addressing this issue, ours completely eliminates the self-intersections without compromising the accuracy of the reconstructions. Being differentiable, CLOAF can be used to <b>fine-tune</b> pose and shape estimation baselines to improve their overall performance and eliminate self-intersections in their predictions. Furthermore, we demonstrate how our CLOAF strategy can be applied to practically any motion field induced by the user. CLOAF also makes it possible to edit motion to interact with the environment without worrying about potential collision or loss of body-shape prior.</p></p class="citation"></blockquote><h3 id=98101--98320-the-nerfect-match-exploring-nerf-features-for-visual-localization-qunjie-zhou-et-al-2024>(98/101 | 98/320) The NeRFect Match: Exploring NeRF Features for Visual Localization (Qunjie Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qunjie Zhou, Maxim Maximov, Or Litany, Laura Leal-Taixé. (2024)<br><strong>The NeRFect Match: Exploring NeRF Features for Visual Localization</strong><br><button class=copy-to-clipboard title="The NeRFect Match: Exploring NeRF Features for Visual Localization" index=98>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-98 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 8<br>Keywords: Benchmarking, Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09577v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09577v1.pdf filename=2403.09577v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we propose the use of Neural Radiance Fields (NeRF) as a scene representation for visual localization. Recently, NeRF has been employed to enhance pose regression and scene coordinate regression models by augmenting the training database, providing auxiliary supervision through rendered images, or serving as an iterative refinement module. We extend its recognized advantages &ndash; its ability to provide a compact scene representation with realistic appearances and accurate <b>geometry</b> &ndash; by exploring the potential of NeRF&rsquo;s internal features in establishing precise 2D-3D matches for localization. To this end, we conduct a comprehensive examination of NeRF&rsquo;s implicit knowledge, acquired through view synthesis, for matching under various conditions. This includes exploring different matching network architectures, extracting encoder features at multiple layers, and varying training configurations. Significantly, we introduce NeRFMatch, an advanced 2D-3D matching function that capitalizes on the internal knowledge of NeRF learned via view synthesis. Our evaluation of NeRFMatch on standard localization <b>benchmarks,</b> within a structure-based pipeline, sets a new state-of-the-art for localization performance on Cambridge Landmarks.</p></p class="citation"></blockquote><h3 id=99101--99320-mm-multimodal-multitask-model-integrating-audiovisual-cues-in-cognitive-load-assessment-long-nguyen-phuoc-et-al-2024>(99/101 | 99/320) M&amp;M: Multimodal-Multitask Model Integrating Audiovisual Cues in Cognitive Load Assessment (Long Nguyen-Phuoc et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Long Nguyen-Phuoc, Renald Gaboriau, Dimitri Delacroix, Laurent Navarro. (2024)<br><strong>M&amp;M: Multimodal-Multitask Model Integrating Audiovisual Cues in Cognitive Load Assessment</strong><br><button class=copy-to-clipboard title="M&M: Multimodal-Multitask Model Integrating Audiovisual Cues in Cognitive Load Assessment" index=99>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-99 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-MM, cs-SD, cs.CV, eess-AS<br>Keyword Score: 6<br>Keywords: Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09451v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09451v1.pdf filename=2403.09451v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces the M&amp;M model, a novel <b>multimodal-multitask</b> learning framework, applied to the AVCAffe dataset for cognitive load assessment (CLA). M&amp;M uniquely integrates audiovisual cues through a dual-pathway architecture, featuring specialized streams for audio and video inputs. A key innovation lies in its cross-modality multihead attention mechanism, fusing the different modalities for synchronized multitasking. Another notable feature is the model&rsquo;s three specialized branches, each tailored to a specific cognitive load label, enabling nuanced, task-specific analysis. While it shows modest performance compared to the AVCAffe&rsquo;s single-task baseline, M&amp;M demonstrates a promising framework for integrated <b>multimodal</b> processing. This work paves the way for future enhancements in <b>multimodal-multitask</b> learning systems, emphasizing the fusion of diverse data types for complex task handling.</p></p class="citation"></blockquote><h3 id=100101--100320-hyper-3dg-text-to-3d-gaussian-generation-via-hypergraph-donglin-di-et-al-2024>(100/101 | 100/320) Hyper-3DG: Text-to-3D Gaussian Generation via Hypergraph (Donglin Di et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Donglin Di, Jiahui Yang, Chaofan Luo, Zhou Xue, Wei Chen, Xun Yang, Yue Gao. (2024)<br><strong>Hyper-3DG: Text-to-3D Gaussian Generation via Hypergraph</strong><br><button class=copy-to-clipboard title="Hyper-3DG: Text-to-3D Gaussian Generation via Hypergraph" index=100>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-100 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09236v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09236v1.pdf filename=2403.09236v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Text-to-3D generation represents an exciting field that has seen rapid advancements, facilitating the transformation of textual descriptions into detailed 3D models. However, current progress often neglects the intricate high-order correlation of <b>geometry</b> and texture within 3D objects, leading to challenges such as over-smoothness, over-saturation and the Janus problem. In this work, we propose a method named <code>3D Gaussian Generation via Hypergraph (Hyper-3DG)'', designed to capture the sophisticated high-order correlations present within 3D objects. Our framework is anchored by a well-established mainflow and an essential module, named </code>Geometry and Texture Hypergraph Refiner (HGRefiner)&rsquo;&rsquo;. This module not only refines the representation of 3D Gaussians but also accelerates the update process of these 3D Gaussians by conducting the Patch-3DGS Hypergraph Learning on both explicit attributes and latent visual features. Our framework allows for the production of finely generated 3D objects within a cohesive optimization, effectively circumventing degradation. Extensive experimentation has shown that our proposed method significantly enhances the quality of 3D generation while incurring no additional computational overhead for the underlying framework. (Project code: <a href=https://github.com/yjhboy/Hyper3DG>https://github.com/yjhboy/Hyper3DG</a>)</p></p class="citation"></blockquote><h3 id=101101--101320-rfacenet-an-end-to-end-network-for-enhanced-physiological-signal-extraction-through-identity-specific-facial-contours-dali-zhu-et-al-2024>(101/101 | 101/320) rFaceNet: An End-to-End Network for Enhanced Physiological Signal Extraction through Identity-Specific Facial Contours (Dali Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dali Zhu, Wenli Zhang, Hualin Zeng, Xiaohao Liu, Long Yang, Jiaqi Zheng. (2024)<br><strong>rFaceNet: An End-to-End Network for Enhanced Physiological Signal Extraction through Identity-Specific Facial Contours</strong><br><button class=copy-to-clipboard title="rFaceNet: An End-to-End Network for Enhanced Physiological Signal Extraction through Identity-Specific Facial Contours" index=101>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-101 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09034v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09034v2.pdf filename=2403.09034v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Remote photoplethysmography (rPPG) technique extracts blood volume pulse (BVP) signals from subtle pixel changes in video frames. This study introduces rFaceNet, an advanced rPPG method that enhances the extraction of facial BVP signals with a focus on facial contours. rFaceNet integrates identity-specific facial contour information and eliminates redundant data. It efficiently extracts facial contours from temporally normalized frame inputs through a Temporal Compressor Unit (TCU) and steers the model focus to relevant facial regions by using the Cross-Task Feature Combiner (CTFC). Through elaborate training, the quality and interpretability of facial physiological signals extracted by rFaceNet are greatly improved compared to previous methods. Moreover, our novel approach demonstrates superior performance than SOTA methods in various heart rate estimation <b>benchmarks.</b></p></p class="citation"></blockquote><h2 id=cslg-45>cs.LG (45)</h2><h3 id=145--102320-robust-subgraph-learning-by-monitoring-early-training-representations-sepideh-neshatfar-et-al-2024>(1/45 | 102/320) Robust Subgraph Learning by Monitoring Early Training Representations (Sepideh Neshatfar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sepideh Neshatfar, Salimeh Yasaei Sekeh. (2024)<br><strong>Robust Subgraph Learning by Monitoring Early Training Representations</strong><br><button class=copy-to-clipboard title="Robust Subgraph Learning by Monitoring Early Training Representations" index=102>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-102 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-LG, cs.LG<br>Keyword Score: 93<br>Keywords: Graph Convolutional Network, Graph Convolutional Network, Node Classification, Graph, Graph Neural Network, Graph Neural Network, Convolution, Convolutional Neural Network, Summarization, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09901v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09901v1.pdf filename=2403.09901v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>neural</b> <b>networks</b> <b>(GNNs)</b> have attracted significant attention for their outstanding performance in <b>graph</b> <b>learning</b> <b>and</b> <b>node</b> <b>classification</b> tasks. However, their vulnerability to <b>adversarial</b> <b>attacks,</b> particularly through susceptible <b>nodes,</b> <b>poses</b> a challenge in decision-making. The need for robust <b>graph</b> <b>summarization</b> <b>is</b> evident in <b>adversarial</b> <b>challenges</b> resulting from the propagation of attacks throughout the entire <b>graph.</b> <b>In</b> <b>this</b> paper, we address both performance and <b>adversarial</b> <b>robustness</b> in <b>graph</b> <b>input</b> <b>by</b> introducing the novel technique SHERD (Subgraph Learning Hale through Early Training Representation Distances). SHERD leverages information from layers of a partially trained <b>graph</b> <b>convolutional</b> <b>network</b> <b>(GCN)</b> to detect susceptible <b>nodes</b> <b>during</b> <b>adversarial</b> <b>attacks</b> using standard distance metrics. The method identifies &ldquo;vulnerable (bad)&rdquo; <b>nodes</b> <b>and</b> removes such <b>nodes</b> <b>to</b> form a robust subgraph while maintaining <b>node</b> <b>classification</b> performance. Through our experiments, we demonstrate the increased performance of SHERD in enhancing robustness by comparing the network&rsquo;s performance on original and subgraph inputs against various baselines alongside existing <b>adversarial</b> <b>attacks.</b> Our experiments across multiple datasets, including citation datasets such as Cora, Citeseer, and Pubmed, as well as microanatomical tissue structures of cell <b>graphs</b> <b>in</b> <b>the</b> placenta, highlight that SHERD not only achieves substantial improvement in robust performance but also outperforms several baselines in terms of <b>node</b> <b>classification</b> accuracy and computational complexity.</p></p class="citation"></blockquote><h3 id=245--103320-spatial-temporal-memories-enhanced-graph-autoencoder-for-anomaly-detection-in-dynamic-graphs-jie-liu-et-al-2024>(2/45 | 103/320) Spatial-temporal Memories Enhanced Graph Autoencoder for Anomaly Detection in Dynamic Graphs (Jie Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jie Liu, Xuequn Shang, Xiaolin Han, Wentao Zhang, Hongzhi Yin. (2024)<br><strong>Spatial-temporal Memories Enhanced Graph Autoencoder for Anomaly Detection in Dynamic Graphs</strong><br><button class=copy-to-clipboard title="Spatial-temporal Memories Enhanced Graph Autoencoder for Anomaly Detection in Dynamic Graphs" index=103>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-103 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 93<br>Keywords: Graph Attention Networks, Graph, Graph Embedding, Graph Neural Network, Graph Neural Network, Anomaly Detection, Autoencoder, Convolution, Unsupervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09039v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09039v1.pdf filename=2403.09039v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Anomaly</b> <b>detection</b> in dynamic <b>graphs</b> <b>presents</b> <b>a</b> significant challenge due to the temporal evolution of <b>graph</b> <b>structures</b> <b>and</b> attributes. The conventional approaches that tackle this problem typically employ an <b>unsupervised</b> <b>learning</b> framework, capturing normality patterns with exclusive normal data during training and identifying deviations as anomalies during testing. However, these methods face critical drawbacks: they either only depend on proxy tasks for general representation without directly pinpointing normal patterns, or they neglect to differentiate between spatial and temporal normality patterns, leading to diminished efficacy in <b>anomaly</b> <b>detection.</b> To address these challenges, we introduce a novel Spatial-Temporal memories-enhanced <b>graph</b> <b>autoencoder</b> <b>(STRIPE).</b> Initially, STRIPE employs <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> and <b>gated</b> temporal <b>convolution</b> layers to extract spatial features and temporal features, respectively. Then STRIPE incorporates separate spatial and temporal memory networks, which capture and store prototypes of normal patterns, thereby preserving the uniqueness of spatial and temporal normality. After that, through a mutual attention mechanism, these stored patterns are then retrieved and integrated with encoded <b>graph</b> <b>embeddings.</b> <b>Finally,</b> the integrated features are fed into the decoder to reconstruct the <b>graph</b> <b>streams</b> <b>which</b> serve as the proxy task for <b>anomaly</b> <b>detection.</b> This comprehensive approach not only minimizes reconstruction errors but also refines the model by emphasizing the compactness and distinctiveness of the embeddings in relation to the nearest memory prototypes. Through extensive testing, STRIPE has demonstrated a superior capability to discern anomalies by effectively leveraging the distinct spatial and temporal dynamics of dynamic <b>graphs,</b> <b>significantly</b> <b>outperforming</b> existing methodologies, with an average improvement of 15.39% on AUC values.</p></p class="citation"></blockquote><h3 id=345--104320-keyformer-kv-cache-reduction-through-key-tokens-selection-for-efficient-generative-inference-muhammad-adnan-et-al-2024>(3/45 | 104/320) Keyformer: KV Cache Reduction through Key Tokens Selection for Efficient Generative Inference (Muhammad Adnan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Muhammad Adnan, Akhil Arunkumar, Gaurav Jain, Prashant J. Nair, Ilya Soloveychik, Purushotham Kamath. (2024)<br><strong>Keyformer: KV Cache Reduction through Key Tokens Selection for Efficient Generative Inference</strong><br><button class=copy-to-clipboard title="Keyformer: KV Cache Reduction through Key Tokens Selection for Efficient Generative Inference" index=104>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-104 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: 68U35, I-2-7; C-0, cs-AI, cs-AR, cs-CL, cs-LG, cs.LG<br>Keyword Score: 70<br>Keywords: GPT, Transformer, Text Generation, Large Language Model, Large Language Model, Prompt, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09054v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09054v1.pdf filename=2403.09054v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Transformers</b> have emerged as the underpinning architecture for <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> In generative language models, the inference process involves two primary phases: <b>prompt</b> processing and token generation. Token generation, which constitutes the majority of the computational workload, primarily entails vector-matrix multiplications and interactions with the Key-Value (KV) Cache. This phase is constrained by memory bandwidth due to the overhead of transferring weights and KV cache values from the memory system to the computing units. This memory bottleneck becomes particularly pronounced in applications that require long-context and extensive <b>text</b> <b>generation,</b> both of which are increasingly crucial for <b>LLMs.</b> This paper introduces &ldquo;Keyformer&rdquo;, an innovative inference-time approach, to mitigate the challenges associated with KV cache size and memory bandwidth utilization. Keyformer leverages the observation that approximately 90% of the attention weight in generative inference focuses on a specific subset of tokens, referred to as &ldquo;key&rdquo; tokens. Keyformer retains only the key tokens in the KV cache by identifying these crucial tokens using a novel score function. This approach effectively reduces both the KV cache size and memory bandwidth usage without compromising model accuracy. We evaluate Keyformer&rsquo;s performance across three foundational models: <b>GPT-J,</b> Cerebras-GPT, and MPT, which employ various positional embedding algorithms. Our assessment encompasses a variety of tasks, with a particular emphasis on <b>summarization</b> and conversation tasks involving extended contexts. Keyformer&rsquo;s reduction of KV cache reduces inference latency by 2.1x and improves token generation throughput by 2.4x, while preserving the model&rsquo;s accuracy.</p></p class="citation"></blockquote><h3 id=445--105320-adversarial-fine-tuning-of-compressed-neural-networks-for-joint-improvement-of-robustness-and-efficiency-hallgrimur-thorsteinsson-et-al-2024>(4/45 | 105/320) Adversarial Fine-tuning of Compressed Neural Networks for Joint Improvement of Robustness and Efficiency (Hallgrimur Thorsteinsson et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hallgrimur Thorsteinsson, Valdemar J Henriksen, Tong Chen, Raghavendra Selvan. (2024)<br><strong>Adversarial Fine-tuning of Compressed Neural Networks for Joint Improvement of Robustness and Efficiency</strong><br><button class=copy-to-clipboard title="Adversarial Fine-tuning of Compressed Neural Networks for Joint Improvement of Robustness and Efficiency" index=105>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-105 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 63<br>Keywords: Adversarial Learning, Benchmarking, Fine-tuning, Model Compression, Pruning, Quantization, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09441v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09441v1.pdf filename=2403.09441v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As deep learning (DL) <b>models</b> <b>are</b> increasingly being integrated into our everyday lives, ensuring their safety by making them robust against <b>adversarial</b> <b>attacks</b> has become increasingly critical. DL <b>models</b> <b>have</b> been found to be susceptible to <b>adversarial</b> <b>attacks</b> which can be achieved by introducing small, targeted perturbations to disrupt the input data. <b>Adversarial</b> <b>training</b> has been presented as a mitigation strategy which can result in more robust <b>models.</b> <b>This</b> <b>adversarial</b> <b>robustness</b> comes with additional computational costs required to design <b>adversarial</b> <b>attacks</b> during training. The two objectives &ndash; <b>adversarial</b> <b>robustness</b> and computational efficiency &ndash; then appear to be in conflict of each other. In this work, we explore the effects of two different <b>model</b> <b>compression</b> methods &ndash; structured weight <b>pruning</b> and <b>quantization</b> &ndash; on <b>adversarial</b> <b>robustness.</b> We specifically explore the effects of <b>fine-tuning</b> on compressed <b>models,</b> <b>and</b> present the trade-off between standard <b>fine-tuning</b> and <b>adversarial</b> <b>fine-tuning.</b> Our results show that compression does not inherently lead to loss in <b>model</b> <b>robustness</b> and <b>adversarial</b> <b>fine-tuning</b> of a compressed <b>model</b> <b>can</b> yield large improvement to the robustness performance of <b>models.</b> <b>We</b> present experiments on two <b>benchmark</b> datasets showing that <b>adversarial</b> <b>fine-tuning</b> of compressed <b>models</b> <b>can</b> achieve robustness performance comparable to adversarially trained <b>models,</b> <b>while</b> also improving computational efficiency.</p></p class="citation"></blockquote><h3 id=545--106320-adedgedrop-adversarial-edge-dropping-for-robust-graph-neural-networks-zhaoliang-chen-et-al-2024>(5/45 | 106/320) ADEdgeDrop: Adversarial Edge Dropping for Robust Graph Neural Networks (Zhaoliang Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhaoliang Chen, Zhihao Wu, Ylli Sadikaj, Claudia Plant, Hong-Ning Dai, Shiping Wang, Wenzhong Guo. (2024)<br><strong>ADEdgeDrop: Adversarial Edge Dropping for Robust Graph Neural Networks</strong><br><button class=copy-to-clipboard title="ADEdgeDrop: Adversarial Edge Dropping for Robust Graph Neural Networks" index=106>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-106 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 56<br>Keywords: Message-Passing, Graph, Graph Neural Network, Graph Neural Network, Adversarial Learning, Benchmarking, Stochastic Gradient Descent<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09171v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09171v1.pdf filename=2403.09171v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Although <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> have exhibited the powerful ability to gather <b>graph-structured</b> <b>information</b> <b>from</b> neighborhood nodes via various <b>message-passing</b> mechanisms, the performance of <b>GNNs</b> is limited by poor generalization and fragile robustness caused by noisy and redundant <b>graph</b> <b>data.</b> <b>As</b> a prominent solution, <b>Graph</b> <b>Augmentation</b> <b>Learning</b> (GAL) has recently received increasing attention. Among prior GAL approaches, edge-dropping methods that randomly remove edges from a <b>graph</b> <b>during</b> <b>training</b> are effective techniques to improve the robustness of <b>GNNs.</b> However, randomly dropping edges often results in bypassing critical edges, consequently weakening the effectiveness of message passing. In this paper, we propose a novel <b>adversarial</b> <b>edge-dropping</b> method (ADEdgeDrop) that leverages an <b>adversarial</b> <b>edge</b> predictor guiding the removal of edges, which can be flexibly incorporated into diverse <b>GNN</b> backbones. Employing an <b>adversarial</b> <b>training</b> framework, the edge predictor utilizes the line <b>graph</b> <b>transformed</b> <b>from</b> the original <b>graph</b> <b>to</b> <b>estimate</b> the edges to be dropped, which improves the interpretability of the edge-dropping method. The proposed ADEdgeDrop is optimized alternately by <b>stochastic</b> <b>gradient</b> <b>descent</b> and projected gradient descent. Comprehensive experiments on six <b>graph</b> <b>benchmark</b> <b>datasets</b> demonstrate that the proposed ADEdgeDrop outperforms state-of-the-art baselines across various <b>GNN</b> backbones, demonstrating improved generalization and robustness.</p></p class="citation"></blockquote><h3 id=645--107320-learning-from-straggler-clients-in-federated-learning-andrew-hard-et-al-2024>(6/45 | 107/320) Learning from straggler clients in federated learning (Andrew Hard et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andrew Hard, Antonious M. Girgis, Ehsan Amid, Sean Augenstein, Lara McConnaughey, Rajiv Mathews, Rohan Anil. (2024)<br><strong>Learning from straggler clients in federated learning</strong><br><button class=copy-to-clipboard title="Learning from straggler clients in federated learning" index=107>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-107 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 43<br>Keywords: Benchmarking, Federated Learning, Knowledge Distillation, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09086v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09086v1.pdf filename=2403.09086v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>How well do existing <b>federated</b> <b>learning</b> algorithms learn from client devices that return model updates with a significant time delay? Is it even possible to learn effectively from clients that report back minutes, hours, or days after being scheduled? We answer these questions by developing Monte Carlo <b>simulations</b> of client latency that are guided by real-world applications. We study synchronous optimization algorithms like FedAvg and FedAdam as well as the asynchronous FedBuff algorithm, and observe that all these existing approaches struggle to learn from severely delayed clients. To improve upon this situation, we experiment with modifications, including <b>distillation</b> regularization and exponential moving averages of model weights. Finally, we introduce two new algorithms, FARe-DUST and FeAST-on-MSG, based on <b>distillation</b> and averaging, respectively. Experiments with the EMNIST, CIFAR-100, and StackOverflow <b>benchmark</b> <b>federated</b> <b>learning</b> tasks demonstrate that our new algorithms outperform existing ones in terms of accuracy for straggler clients, while also providing better trade-offs between training time and total accuracy.</p></p class="citation"></blockquote><h3 id=745--108320-fedcomloc-communication-efficient-distributed-training-of-sparse-and-quantized-models-kai-yi-et-al-2024>(7/45 | 108/320) FedComLoc: Communication-Efficient Distributed Training of Sparse and Quantized Models (Kai Yi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kai Yi, Georg Meinhardt, Laurent Condat, Peter Richtárik. (2024)<br><strong>FedComLoc: Communication-Efficient Distributed Training of Sparse and Quantized Models</strong><br><button class=copy-to-clipboard title="FedComLoc: Communication-Efficient Distributed Training of Sparse and Quantized Models" index=108>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-108 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-DC, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Federated Learning, Quantization, Quantization, Stochastic Gradient Descent<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09904v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09904v1.pdf filename=2403.09904v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Federated</b> <b>Learning</b> (FL) has garnered increasing attention due to its unique characteristic of allowing heterogeneous clients to process their private data locally and interact with a central server, while being respectful of privacy. A critical bottleneck in FL is the communication cost. A pivotal strategy to mitigate this burden is \emph{Local Training}, which involves running multiple local <b>stochastic</b> <b>gradient</b> <b>descent</b> iterations between communication phases. Our work is inspired by the innovative \emph{Scaffnew} algorithm, which has considerably advanced the reduction of communication complexity in FL. We introduce FedComLoc <b>(Federated</b> <b>Compressed</b> and Local Training), integrating practical and effective compression into \emph{Scaffnew} to further enhance communication efficiency. Extensive experiments, using the popular TopK compressor and <b>quantization,</b> demonstrate its prowess in substantially reducing communication overheads in heterogeneous settings.</p></p class="citation"></blockquote><h3 id=845--109320-cooling-guide-diffusion-model-for-battery-cell-arrangement-nicholas-sung-et-al-2024>(8/45 | 109/320) Cooling-Guide Diffusion Model for Battery Cell Arrangement (Nicholas Sung et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nicholas Sung, Liu Zheng, Pingfeng Wang, Faez Ahmed. (2024)<br><strong>Cooling-Guide Diffusion Model for Battery Cell Arrangement</strong><br><button class=copy-to-clipboard title="Cooling-Guide Diffusion Model for Battery Cell Arrangement" index=109>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-109 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Diffusion Model, Generative AI, Generative Adversarial Network, Probabilistic Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10566v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10566v1.pdf filename=2403.10566v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Our study introduces a <b>Generative</b> <b>AI</b> method that employs a cooling-guided <b>diffusion</b> <b>model</b> to optimize the layout of battery cells, a crucial step for enhancing the cooling performance and efficiency of battery thermal management systems. Traditional design processes, which rely heavily on iterative optimization and extensive guesswork, are notoriously slow and inefficient, often leading to suboptimal solutions. In contrast, our innovative method uses a parametric denoising <b>diffusion</b> <b>probabilistic</b> <b>model</b> (DDPM) with classifier and cooling guidance to generate optimized cell layouts with enhanced cooling paths, significantly lowering the maximum temperature of the cells. By incorporating position-based classifier guidance, we ensure the feasibility of generated layouts. Meanwhile, cooling guidance directly optimizes cooling-efficiency, making our approach uniquely effective. When compared to two advanced models, the Tabular Denoising <b>Diffusion</b> <b>Probabilistic</b> <b>Model</b> (TabDDPM) and the Conditional Tabular <b>GAN</b> (CTGAN), our cooling-guided <b>diffusion</b> <b>model</b> notably outperforms both. It is five times more effective than TabDDPM and sixty-six times better than CTGAN across key metrics such as feasibility, diversity, and cooling efficiency. This research marks a significant leap forward in the field, aiming to optimize battery cell layouts for superior cooling efficiency, thus setting the stage for the development of more effective and dependable battery thermal management systems.</p></p class="citation"></blockquote><h3 id=945--110320-borrowing-treasures-from-neighbors-in-context-learning-for-multimodal-learning-with-missing-modalities-and-data-scarcity-zhuo-zhi-et-al-2024>(9/45 | 110/320) Borrowing Treasures from Neighbors: In-Context Learning for Multimodal Learning with Missing Modalities and Data Scarcity (Zhuo Zhi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhuo Zhi, Ziquan Liu, Moe Elbadawi, Adam Daneshmend, Mine Orlu, Abdul Basit, Andreas Demosthenous, Miguel Rodrigues. (2024)<br><strong>Borrowing Treasures from Neighbors: In-Context Learning for Multimodal Learning with Missing Modalities and Data Scarcity</strong><br><button class=copy-to-clipboard title="Borrowing Treasures from Neighbors: In-Context Learning for Multimodal Learning with Missing Modalities and Data Scarcity" index=110>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-110 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 39<br>Keywords: Multi-modal, Multi-modal, Sample Size, Transformer, In-context Learning, In-context Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09428v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09428v1.pdf filename=2403.09428v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multimodal</b> machine learning with missing modalities is an increasingly relevant challenge arising in various applications such as healthcare. This paper extends the current research into missing modalities to the low-data regime, i.e., a downstream task has both missing modalities and limited <b>sample</b> <b>size</b> issues. This problem setting is particularly challenging and also practical as it is often expensive to get full-modality data and sufficient annotated training <b>samples.</b> <b>We</b> propose to use retrieval-augmented <b>in-context</b> <b>learning</b> to address these two crucial issues by unleashing the potential of a <b>transformer&rsquo;s</b> <b>in-context</b> <b>learning</b> ability. Diverging from existing methods, which primarily belong to the parametric paradigm and often require sufficient training <b>samples,</b> <b>our</b> work exploits the value of the available full-modality data, offering a novel perspective on resolving the challenge. The proposed data-dependent framework exhibits a higher degree of <b>sample</b> <b>efficiency</b> and is empirically demonstrated to enhance the classification model&rsquo;s performance on both full- and missing-modality data in the low-data regime across various <b>multimodal</b> learning tasks. When only 1% of the training data are available, our proposed method demonstrates an average improvement of 6.1% over a recent strong baseline across various datasets and missing states. Notably, our method also reduces the performance gap between full-modality and missing-modality data compared with the baseline.</p></p class="citation"></blockquote><h3 id=1045--111320-equiav-leveraging-equivariance-for-audio-visual-contrastive-learning-jongsuk-kim-et-al-2024>(10/45 | 111/320) EquiAV: Leveraging Equivariance for Audio-Visual Contrastive Learning (Jongsuk Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jongsuk Kim, Hyeongkeun Lee, Kyeongha Rho, Junmo Kim, Joon Son Chung. (2024)<br><strong>EquiAV: Leveraging Equivariance for Audio-Visual Contrastive Learning</strong><br><button class=copy-to-clipboard title="EquiAV: Leveraging Equivariance for Audio-Visual Contrastive Learning" index=111>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-111 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-MM, cs.LG<br>Keyword Score: 38<br>Keywords: Benchmarking, Contrastive Learning, Data Augmentation, Representation Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09502v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09502v1.pdf filename=2403.09502v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in <b>self-supervised</b> audio-visual <b>representation</b> <b>learning</b> have demonstrated its potential to capture rich and comprehensive <b>representations.</b> <b>However,</b> despite the advantages of <b>data</b> <b>augmentation</b> verified in many learning methods, audio-visual learning has struggled to fully harness these benefits, as augmentations can easily disrupt the correspondence between input pairs. To address this limitation, we introduce EquiAV, a novel framework that leverages equivariance for audio-visual <b>contrastive</b> <b>learning.</b> Our approach begins with extending equivariance to audio-visual learning, facilitated by a shared attention-based transformation predictor. It enables the aggregation of features from diverse augmentations into a representative embedding, providing robust supervision. Notably, this is achieved with minimal computational overhead. Extensive ablation studies and qualitative results verify the effectiveness of our method. EquiAV outperforms previous works across various audio-visual <b>benchmarks.</b></p></p class="citation"></blockquote><h3 id=1145--112320-mope-parameter-efficient-and-scalable-multimodal-fusion-via-mixture-of-prompt-experts-ruixiang-jiang-et-al-2024>(11/45 | 112/320) MoPE: Parameter-Efficient and Scalable Multimodal Fusion via Mixture of Prompt Experts (Ruixiang Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruixiang Jiang, Lingbo Liu, Changwen Chen. (2024)<br><strong>MoPE: Parameter-Efficient and Scalable Multimodal Fusion via Mixture of Prompt Experts</strong><br><button class=copy-to-clipboard title="MoPE: Parameter-Efficient and Scalable Multimodal Fusion via Mixture of Prompt Experts" index=112>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-112 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-CV, cs-LG, cs.LG<br>Keyword Score: 36<br>Keywords: Fine-tuning, Foundation Model, Multi-modal, Multi-modal, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10568v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10568v1.pdf filename=2403.10568v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Prompt-tuning</b> has demonstrated parameter-efficiency in fusing unimodal <b>foundation</b> <b>models</b> for <b>multimodal</b> tasks. However, its limited adaptivity and expressiveness lead to suboptimal performance when compared with other tuning methods. In this paper, we address this issue by disentangling the vanilla <b>prompts</b> to adaptively capture dataset-level and instance-level features. Building upon this disentanglement, we introduce the mixture of <b>prompt</b> experts (MoPE) technique to enhance expressiveness. MoPE leverages <b>multimodal</b> pairing priors to route the most effective <b>prompt</b> on a per-instance basis. Compared to vanilla <b>prompting,</b> our MoPE-based conditional <b>prompting</b> exhibits greater expressiveness for <b>multimodal</b> fusion, scaling better with the training data and the overall number of trainable parameters. We also study a regularization term for expert routing, leading to emergent expert specialization, where different experts focus on different concepts, enabling interpretable soft <b>prompting.</b> Extensive experiments across three <b>multimodal</b> datasets demonstrate that our method achieves state-of-the-art results, matching or even surpassing the performance of <b>fine-tuning,</b> while requiring only 0.8% of the trainable parameters. Code will be released: <a href=https://github.com/songrise/MoPE>https://github.com/songrise/MoPE</a>.</p></p class="citation"></blockquote><h3 id=1245--113320-self-supervised-learning-for-time-series-contrastive-or-generative-ziyu-liu-et-al-2024>(12/45 | 113/320) Self-Supervised Learning for Time Series: Contrastive or Generative? (Ziyu Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ziyu Liu, Azadeh Alavi, Minyi Li, Xiang Zhang. (2024)<br><strong>Self-Supervised Learning for Time Series: Contrastive or Generative?</strong><br><button class=copy-to-clipboard title="Self-Supervised Learning for Time Series: Contrastive or Generative?" index=113>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-113 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-ET, cs-LG, cs.LG<br>Keyword Score: 35<br>Keywords: Recommendation, Representation Learning, Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09809v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09809v1.pdf filename=2403.09809v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Self-supervised</b> <b>learning</b> (SSL) has recently emerged as a powerful approach to learning <b>representations</b> <b>from</b> large-scale unlabeled data, showing promising results in time series analysis. The <b>self-supervised</b> <b>representation</b> <b>learning</b> can be categorized into two mainstream: contrastive and generative. In this paper, we will present a comprehensive comparative study between contrastive and generative methods in time series. We first introduce the basic frameworks for contrastive and generative SSL, respectively, and discuss how to obtain the supervision signal that guides the model optimization. We then implement classical algorithms (SimCLR vs. MAE) for each type and conduct a comparative analysis in fair settings. Our results provide insights into the strengths and weaknesses of each approach and offer practical <b>recommendations</b> for choosing suitable SSL methods. We also discuss the implications of our findings for the broader field of <b>representation</b> <b>learning</b> and propose future research directions. All the code and data are released at \url{https://github.com/DL4mHealth/SSL_Comparison}.</p></p class="citation"></blockquote><h3 id=1345--114320-few-shot-class-incremental-learning-with-attention-aware-self-adaptive-prompt-chenxi-liu-et-al-2024>(13/45 | 114/320) Few-Shot Class Incremental Learning with Attention-Aware Self-Adaptive Prompt (Chenxi Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chenxi Liu, Zhenyi Wang, Tianyi Xiong, Ruibo Chen, Yihan Wu, Junfeng Guo, Heng Huang. (2024)<br><strong>Few-Shot Class Incremental Learning with Attention-Aware Self-Adaptive Prompt</strong><br><button class=copy-to-clipboard title="Few-Shot Class Incremental Learning with Attention-Aware Self-Adaptive Prompt" index=114>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-114 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs.LG<br>Keyword Score: 33<br>Keywords: Benchmarking, Few-shot, Fine-tuning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09857v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09857v1.pdf filename=2403.09857v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Few-Shot</b> Class-Incremental Learning (FSCIL) models aim to incrementally learn new classes with scarce samples while preserving knowledge of old ones. Existing FSCIL methods usually <b>fine-tune</b> the entire backbone, leading to overfitting and hindering the potential to learn new classes. On the other hand, recent <b>prompt-based</b> CIL approaches alleviate forgetting by training <b>prompts</b> with sufficient data in each task. In this work, we propose a novel framework named Attention-aware Self-adaptive <b>Prompt</b> (ASP). ASP encourages task-invariant <b>prompts</b> to capture shared knowledge by reducing specific information from the attention aspect. Additionally, self-adaptive task-specific <b>prompts</b> in ASP provide specific information and transfer knowledge from old classes to new classes with an Information Bottleneck learning objective. In summary, ASP prevents overfitting on base task and does not require enormous data in <b>few-shot</b> incremental tasks. Extensive experiments on three <b>benchmark</b> datasets validate that ASP consistently outperforms state-of-the-art FSCIL and <b>prompt-based</b> CIL methods in terms of both learning new classes and mitigating forgetting.</p></p class="citation"></blockquote><h3 id=1445--115320-towards-a-theory-of-model-distillation-enric-boix-adsera-2024>(14/45 | 115/320) Towards a theory of model distillation (Enric Boix-Adsera, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Enric Boix-Adsera. (2024)<br><strong>Towards a theory of model distillation</strong><br><button class=copy-to-clipboard title="Towards a theory of model distillation" index=115>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-115 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-NE, cs.LG<br>Keyword Score: 30<br>Keywords: Knowledge Distillation, Knowledge Distillation, Model Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09053v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09053v1.pdf filename=2403.09053v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Distillation</b> is the task of replacing a complicated machine learning <b>model</b> <b>with</b> a simpler <b>model</b> <b>that</b> approximates the original [BCNM06,HVD15]. Despite many practical applications, basic questions about the extent to which <b>models</b> <b>can</b> be <b>distilled,</b> and the runtime and amount of data needed to <b>distill,</b> remain largely open. To study these questions, we initiate a general theory of <b>distillation,</b> defining PAC-distillation in an analogous way to PAC-learning [Val84]. As applications of this theory: (1) we propose new algorithms to extract the knowledge stored in the trained weights of neural networks &ndash; we show how to efficiently <b>distill</b> neural networks into succinct, explicit decision tree representations when possible by using the ``linear representation hypothesis&rsquo;&rsquo;; and (2) we prove that <b>distillation</b> can be much cheaper than learning from scratch, and make progress on characterizing its complexity.</p></p class="citation"></blockquote><h3 id=1545--116320-soften-to-defend-towards-adversarial-robustness-via-self-guided-label-refinement-daiwei-yu-et-al-2024>(15/45 | 116/320) Soften to Defend: Towards Adversarial Robustness via Self-Guided Label Refinement (Daiwei Yu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daiwei Yu, Zhuorong Li, Lina Wei, Canghong Jin, Yun Zhang, Sixian Chan. (2024)<br><strong>Soften to Defend: Towards Adversarial Robustness via Self-Guided Label Refinement</strong><br><button class=copy-to-clipboard title="Soften to Defend: Towards Adversarial Robustness via Self-Guided Label Refinement" index=116>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-116 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-CV, cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Adversarial Learning, Benchmarking, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09101v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09101v1.pdf filename=2403.09101v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Adversarial</b> <b>training</b> (AT) is currently one of the most effective ways to obtain the robustness of deep neural networks against <b>adversarial</b> <b>attacks.</b> However, most AT methods suffer from robust overfitting, i.e., a significant generalization gap in <b>adversarial</b> <b>robustness</b> between the training and testing curves. In this paper, we first identify a connection between robust overfitting and the excessive memorization of noisy labels in AT from a view of gradient norm. As such label noise is mainly caused by a distribution mismatch and improper label assignments, we are motivated to propose a label refinement approach for AT. Specifically, our Self-Guided Label Refinement first self-refines a more accurate and informative label distribution from over-confident hard labels, and then it calibrates the training by dynamically incorporating knowledge from self-distilled models into the current model and thus requiring no external teachers. Empirical results demonstrate that our method can simultaneously boost the standard accuracy and robust performance across multiple <b>benchmark</b> datasets, attack types, and architectures. In addition, we also provide a set of analyses from the perspectives of information theory to dive into our method and suggest the importance of soft labels for robust generalization.</p></p class="citation"></blockquote><h3 id=1645--117320-a-conceptual-framework-for-white-box-neural-networks-maciej-satkiewicz-2024>(16/45 | 117/320) A Conceptual Framework For White Box Neural Networks (Maciej Satkiewicz, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Maciej Satkiewicz. (2024)<br><strong>A Conceptual Framework For White Box Neural Networks</strong><br><button class=copy-to-clipboard title="A Conceptual Framework For White Box Neural Networks" index=117>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-117 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-NE, cs.LG<br>Keyword Score: 20<br>Keywords: MNIST, Adversarial Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09863v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09863v1.pdf filename=2403.09863v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces semantic features as a general conceptual framework for fully explainable neural network layers. A well-motivated proof of concept model for relevant subproblem of <b>MNIST</b> consists of 4 such layers with the total of 4.8K learnable parameters. The model is easily interpretable, achieves human-level <b>adversarial</b> <b>test</b> accuracy with no form of <b>adversarial</b> <b>training,</b> requires little hyperparameter tuning and can be quickly trained on a single CPU. The general nature of the technique bears promise for a paradigm shift towards radically democratised and truly generalizable white box neural networks. The code is available at <a href=https://github.com/314-Foundation/white-box-nn>https://github.com/314-Foundation/white-box-nn</a></p></p class="citation"></blockquote><h3 id=1745--118320-minimax-optimal-and-computationally-efficient-algorithms-for-distributionally-robust-offline-reinforcement-learning-zhishuai-liu-et-al-2024>(17/45 | 118/320) Minimax Optimal and Computationally Efficient Algorithms for Distributionally Robust Offline Reinforcement Learning (Zhishuai Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhishuai Liu, Pan Xu. (2024)<br><strong>Minimax Optimal and Computationally Efficient Algorithms for Distributionally Robust Offline Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Minimax Optimal and Computationally Efficient Algorithms for Distributionally Robust Offline Reinforcement Learning" index=118>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-118 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, stat-ML<br>Keyword Score: 20<br>Keywords: Offline Reinforcement Learning, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09621v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09621v1.pdf filename=2403.09621v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Distributionally robust <b>offline</b> <b>reinforcement</b> <b>learning</b> (RL), which seeks robust policy training against environment perturbation by modeling dynamics uncertainty, calls for function approximations when facing large state-action spaces. However, the consideration of dynamics uncertainty introduces essential nonlinearity and computational burden, posing unique challenges for analyzing and practically employing function approximation. Focusing on a basic setting where the nominal model and perturbed models are linearly parameterized, we propose minimax optimal and computationally efficient algorithms realizing function approximation and initiate the study on instance-dependent suboptimality analysis in the context of robust <b>offline</b> <b>RL.</b> <b>Our</b> results uncover that function approximation in robust <b>offline</b> <b>RL</b> <b>is</b> essentially distinct from and probably harder than that in standard <b>offline</b> <b>RL.</b> <b>Our</b> algorithms and theoretical results crucially depend on a variety of new techniques, involving a novel function approximation mechanism incorporating variance information, a new procedure of suboptimality and estimation uncertainty decomposition, a quantification of the robust value function shrinkage, and a meticulously designed family of hard instances, which might be of independent interest.</p></p class="citation"></blockquote><h3 id=1845--119320-reawakening-knowledge-anticipatory-recovery-from-catastrophic-interference-via-structured-training-yanlai-yang-et-al-2024>(18/45 | 119/320) Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training (Yanlai Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yanlai Yang, Matt Jones, Michael C. Mozer, Mengye Ren. (2024)<br><strong>Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training</strong><br><button class=copy-to-clipboard title="Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training" index=119>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-119 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Fine-tuning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09613v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09613v1.pdf filename=2403.09613v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We explore the training dynamics of neural networks in a structured non-IID setting where documents are presented cyclically in a fixed, repeated sequence. Typically, networks suffer from catastrophic interference when training on a sequence of documents; however, we discover a curious and remarkable property of <b>LLMs</b> <b>fine-tuned</b> sequentially in this setting: they exhibit anticipatory behavior, recovering from the forgetting on documents before encountering them again. The behavior emerges and becomes more robust as the architecture scales up its number of parameters. Through comprehensive experiments and visualizations, we uncover new insights into training over-parameterized networks in structured environments.</p></p class="citation"></blockquote><h3 id=1945--120320-self-consistency-training-for-hamiltonian-prediction-he-zhang-et-al-2024>(19/45 | 120/320) Self-Consistency Training for Hamiltonian Prediction (He Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>He Zhang, Chang Liu, Zun Wang, Xinran Wei, Siyuan Liu, Nanning Zheng, Bin Shao, Tie-Yan Liu. (2024)<br><strong>Self-Consistency Training for Hamiltonian Prediction</strong><br><button class=copy-to-clipboard title="Self-Consistency Training for Hamiltonian Prediction" index=120>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-120 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, physics-chem-ph, q-bio-BM<br>Keyword Score: 20<br>Keywords: Out-of-distribution, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09560v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09560v1.pdf filename=2403.09560v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Hamiltonian prediction is a versatile formulation to leverage machine learning for solving molecular science problems. Yet, its applicability is limited by insufficient labeled data for training. In this work, we highlight that Hamiltonian prediction possesses a self-consistency principle, based on which we propose an exact training method that does not require labeled data. This merit addresses the data scarcity difficulty, and distinguishes the task from other property prediction formulations with unique benefits: (1) self-consistency training enables the model to be trained on a large amount of unlabeled data, hence substantially enhances generalization; (2) self-consistency training is more efficient than labeling data with DFT for <b>supervised</b> training, since it is an amortization of DFT calculation over a set of molecular structures. We empirically demonstrate the better generalization in data-scarce and <b>out-of-distribution</b> scenarios, and the better efficiency from the amortization. These benefits push forward the applicability of Hamiltonian prediction to an ever larger scale.</p></p class="citation"></blockquote><h3 id=2045--121320-on-using-machine-learning-algorithms-for-motorcycle-collision-detection-philipp-rodegast-et-al-2024>(20/45 | 121/320) On using Machine Learning Algorithms for Motorcycle Collision Detection (Philipp Rodegast et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Philipp Rodegast, Steffen Maier, Jonas Kneifl, Jörg Fehr. (2024)<br><strong>On using Machine Learning Algorithms for Motorcycle Collision Detection</strong><br><button class=copy-to-clipboard title="On using Machine Learning Algorithms for Motorcycle Collision Detection" index=121>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-121 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-DS<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09491v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09491v1.pdf filename=2403.09491v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Globally, motorcycles attract vast and varied users. However, since the rate of severe injury and fatality in motorcycle accidents far exceeds passenger car accidents, efforts have been directed toward increasing passive safety systems. Impact <b>simulations</b> show that the risk of severe injury or death in the event of a motorcycle-to-car impact can be greatly reduced if the motorcycle is equipped with passive safety measures such as airbags and seat belts. For the passive safety systems to be activated, a collision must be detected within milliseconds for a wide variety of impact configurations, but under no circumstances may it be falsely triggered. For the challenge of reliably detecting impending collisions, this paper presents an investigation towards the applicability of machine learning algorithms. First, a series of <b>simulations</b> of accidents and driving operation is introduced to collect data to train machine learning classification models. Their performance is henceforth assessed and compared via multiple representative and application-oriented criteria.</p></p class="citation"></blockquote><h3 id=2145--122320-laying-the-foundation-first-investigating-the-generalization-from-atomic-skills-to-complex-reasoning-tasks-yuncheng-huang-et-al-2024>(21/45 | 122/320) Laying the Foundation First? Investigating the Generalization from Atomic Skills to Complex Reasoning Tasks (Yuncheng Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuncheng Huang, Qianyu He, Yipei Xu, Jiaqing Liang, Yanghua Xiao. (2024)<br><strong>Laying the Foundation First? Investigating the Generalization from Atomic Skills to Complex Reasoning Tasks</strong><br><button class=copy-to-clipboard title="Laying the Foundation First? Investigating the Generalization from Atomic Skills to Complex Reasoning Tasks" index=122>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-122 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Curriculum Learning, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09479v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09479v1.pdf filename=2403.09479v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Current language models have demonstrated their capability to develop basic <b>reasoning,</b> but struggle in more complicated <b>reasoning</b> tasks that require a combination of atomic skills, such as math word problem requiring skills like arithmetic and unit conversion. Previous methods either do not improve the inherent atomic skills of models or not attempt to generalize the atomic skills to complex <b>reasoning</b> tasks. In this paper, we first propose a probing framework to investigate whether the atomic skill can spontaneously generalize to complex <b>reasoning</b> tasks. Then, we introduce a hierarchical <b>curriculum</b> <b>learning</b> training strategy to achieve better skill generalization. In our experiments, we find that atomic skills can not spontaneously generalize to compositional tasks. By leveraging hierarchical <b>curriculum</b> <b>learning,</b> we successfully induce generalization, significantly improve the performance of open-source LMs on complex <b>reasoning</b> tasks. Promisingly, the skill generalization exhibit effective in cross-dataset and cross-domain scenarios. Complex <b>reasoning</b> can also help enhance atomic skills. Our findings offer valuable guidance for designing better training strategies for complex <b>reasoning</b> tasks.</p></p class="citation"></blockquote><h3 id=2245--123320-easy-to-hard-generalization-scalable-alignment-beyond-human-supervision-zhiqing-sun-et-al-2024>(22/45 | 123/320) Easy-to-Hard Generalization: Scalable Alignment Beyond Human Supervision (Zhiqing Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhiqing Sun, Longhui Yu, Yikang Shen, Weiyang Liu, Yiming Yang, Sean Welleck, Chuang Gan. (2024)<br><strong>Easy-to-Hard Generalization: Scalable Alignment Beyond Human Supervision</strong><br><button class=copy-to-clipboard title="Easy-to-Hard Generalization: Scalable Alignment Beyond Human Supervision" index=123>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-123 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Reinforcement Learning, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09472v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09472v1.pdf filename=2403.09472v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Current AI alignment methodologies rely on human-provided demonstrations or judgments, and the learned capabilities of AI systems would be upper-bounded by human capabilities as a result. This raises a challenging research question: How can we keep improving the systems when their capabilities have surpassed the levels of humans? This paper answers this question in the context of tackling hard <b>reasoning</b> tasks (e.g., level 4-5 MATH problems) via learning from human annotations on easier tasks (e.g., level 1-3 MATH problems), which we term as \textit{easy-to-hard generalization}. Our key insight is that an evaluator (reward model) trained on supervisions for easier tasks can be effectively used for scoring candidate solutions of harder tasks and hence facilitating easy-to-hard generalization over different levels of tasks. Based on this insight, we propose a novel approach to scalable alignment, which firstly trains the process-supervised reward models on easy problems (e.g., level 1-3), and then uses them to evaluate the performance of policy models on hard problems. We show that such \textit{easy-to-hard generalization from evaluators} can enable \textit{easy-to-hard generalizations in generators} either through re-ranking or <b>reinforcement</b> <b>learning</b> (RL). Notably, our process-supervised 7b RL model achieves an accuracy of 34.0% on MATH500, despite only using human supervision on easy problems. Our approach suggests a promising path toward AI systems that advance beyond the frontier of human supervision.</p></p class="citation"></blockquote><h3 id=2345--124320-shake-to-leak-fine-tuning-diffusion-models-can-amplify-the-generative-privacy-risk-zhangheng-li-et-al-2024>(23/45 | 124/320) Shake to Leak: Fine-tuning Diffusion Models Can Amplify the Generative Privacy Risk (Zhangheng Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhangheng Li, Junyuan Hong, Bo Li, Zhangyang Wang. (2024)<br><strong>Shake to Leak: Fine-tuning Diffusion Models Can Amplify the Generative Privacy Risk</strong><br><button class=copy-to-clipboard title="Shake to Leak: Fine-tuning Diffusion Models Can Amplify the Generative Privacy Risk" index=124>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-124 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Diffusion Model, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09450v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09450v1.pdf filename=2403.09450v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While <b>diffusion</b> <b>models</b> have recently demonstrated remarkable progress in generating realistic images, privacy risks also arise: published models or APIs could generate training images and thus leak privacy-sensitive training information. In this paper, we reveal a new risk, Shake-to-Leak (S2L), that <b>fine-tuning</b> the pre-trained models with manipulated data can amplify the existing privacy risks. We demonstrate that S2L could occur in various standard <b>fine-tuning</b> strategies for <b>diffusion</b> <b>models,</b> including concept-injection methods (DreamBooth and Textual Inversion) and parameter-efficient methods (LoRA and Hypernetwork), as well as their combinations. In the worst case, S2L can amplify the state-of-the-art membership inference attack (MIA) on <b>diffusion</b> <b>models</b> by $5.4%$ (absolute difference) AUC and can increase extracted private samples from almost $0$ samples to $16.3$ samples on average per target domain. This discovery underscores that the privacy risk with <b>diffusion</b> <b>models</b> is even more severe than previously recognized. Codes are available at <a href=https://github.com/VITA-Group/Shake-to-Leak>https://github.com/VITA-Group/Shake-to-Leak</a>.</p></p class="citation"></blockquote><h3 id=2445--125320-rethinking-autoencoders-for-medical-anomaly-detection-from-a-theoretical-perspective-yu-cai-et-al-2024>(24/45 | 125/320) Rethinking Autoencoders for Medical Anomaly Detection from A Theoretical Perspective (Yu Cai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yu Cai, Hao Chen, Kwang-Ting Cheng. (2024)<br><strong>Rethinking Autoencoders for Medical Anomaly Detection from A Theoretical Perspective</strong><br><button class=copy-to-clipboard title="Rethinking Autoencoders for Medical Anomaly Detection from A Theoretical Perspective" index=125>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-125 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Anomaly Detection, Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09303v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09303v2.pdf filename=2403.09303v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Medical <b>anomaly</b> <b>detection</b> aims to identify abnormal findings using only normal training data, playing a crucial role in health screening and recognizing rare diseases. Reconstruction-based methods, particularly those utilizing <b>autoencoders</b> (AEs), are dominant in this field. They work under the assumption that AEs trained on only normal data cannot reconstruct unseen abnormal regions well, thereby enabling the <b>anomaly</b> <b>detection</b> based on reconstruction errors. However, this assumption does not always hold due to the mismatch between the reconstruction training objective and the <b>anomaly</b> <b>detection</b> task objective, rendering these methods theoretically unsound. This study focuses on providing a theoretical foundation for AE-based reconstruction methods in <b>anomaly</b> <b>detection.</b> By leveraging information theory, we elucidate the principles of these methods and reveal that the key to improving AE in <b>anomaly</b> <b>detection</b> lies in minimizing the information entropy of latent vectors. Experiments on four datasets with two image modalities validate the effectiveness of our theory. To the best of our knowledge, this is the first effort to theoretically clarify the principles and design philosophy of AE for <b>anomaly</b> <b>detection.</b> Code will be available upon acceptance.</p></p class="citation"></blockquote><h3 id=2545--126320-generative-models-and-connected-and-automated-vehicles-a-survey-in-exploring-the-intersection-of-transportation-and-ai-dong-shu-et-al-2024>(25/45 | 126/320) Generative Models and Connected and Automated Vehicles: A Survey in Exploring the Intersection of Transportation and AI (Dong Shu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dong Shu, Zhouyao Zhu. (2024)<br><strong>Generative Models and Connected and Automated Vehicles: A Survey in Exploring the Intersection of Transportation and AI</strong><br><button class=copy-to-clipboard title="Generative Models and Connected and Automated Vehicles: A Survey in Exploring the Intersection of Transportation and AI" index=126>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-126 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-RO, cs.LG<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10559v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10559v1.pdf filename=2403.10559v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This report investigates the history and impact of Generative Models and Connected and Automated Vehicles (CAVs), two groundbreaking forces pushing progress in technology and transportation. By focusing on the application of generative models within the context of CAVs, the study aims to unravel how this integration could enhance predictive modeling, <b>simulation</b> accuracy, and decision-making processes in autonomous vehicles. This thesis discusses the benefits and challenges of integrating generative models and CAV technology in transportation. It aims to highlight the progress made, the remaining obstacles, and the potential for advancements in safety and innovation.</p></p class="citation"></blockquote><h3 id=2645--127320-sindy-rl-interpretable-and-efficient-model-based-reinforcement-learning-nicholas-zolman-et-al-2024>(26/45 | 127/320) SINDy-RL: Interpretable and Efficient Model-Based Reinforcement Learning (Nicholas Zolman et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nicholas Zolman, Urban Fasel, J. Nathan Kutz, Steven L. Brunton. (2024)<br><strong>SINDy-RL: Interpretable and Efficient Model-Based Reinforcement Learning</strong><br><button class=copy-to-clipboard title="SINDy-RL: Interpretable and Efficient Model-Based Reinforcement Learning" index=127>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-127 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-SY, cs.LG, eess-SY, math-DS, math-OC<br>Keyword Score: 18<br>Keywords: Benchmarking, Black Box, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09110v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09110v1.pdf filename=2403.09110v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep <b>reinforcement</b> <b>learning</b> (DRL) has shown significant promise for uncovering sophisticated control policies that interact in environments with complicated dynamics, such as stabilizing the magnetohydrodynamics of a tokamak fusion reactor or minimizing the drag force exerted on an object in a fluid flow. However, these algorithms require an abundance of training examples and may become prohibitively expensive for many applications. In addition, the reliance on deep neural networks often results in an uninterpretable, <b>black-box</b> <b>policy</b> that may be too computationally expensive to use with certain embedded systems. Recent advances in sparse dictionary learning, such as the sparse identification of nonlinear dynamics (SINDy), have shown promise for creating efficient and interpretable data-driven models in the low-data regime. In this work we introduce SINDy-RL, a unifying framework for combining SINDy and DRL to create efficient, interpretable, and trustworthy representations of the dynamics model, reward function, and control policy. We demonstrate the effectiveness of our approaches on <b>benchmark</b> control environments and challenging fluids problems. SINDy-RL achieves comparable performance to state-of-the-art DRL algorithms using significantly fewer interactions in the environment and results in an interpretable control policy orders of magnitude smaller than a deep neural network policy.</p></p class="citation"></blockquote><h3 id=2745--128320-mamba-an-effective-world-model-approach-for-meta-reinforcement-learning-zohar-rimon-et-al-2024>(27/45 | 128/320) MAMBA: an Effective World Model Approach for Meta-Reinforcement Learning (Zohar Rimon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zohar Rimon, Tom Jurgenson, Orr Krupnik, Gilad Adler, Aviv Tamar. (2024)<br><strong>MAMBA: an Effective World Model Approach for Meta-Reinforcement Learning</strong><br><button class=copy-to-clipboard title="MAMBA: an Effective World Model Approach for Meta-Reinforcement Learning" index=128>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-128 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 13<br>Keywords: Benchmarking, Markov Decision Process<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09859v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09859v1.pdf filename=2403.09859v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Meta-reinforcement learning (meta-RL) is a promising framework for tackling challenging domains requiring efficient exploration. Existing meta-RL algorithms are characterized by low sample efficiency, and mostly focus on low-dimensional task distributions. In parallel, model-based RL methods have been successful in solving partially observable <b>MDPs,</b> of which meta-RL is a special case. In this work, we leverage this success and propose a new model-based approach to meta-RL, based on elements from existing state-of-the-art model-based and meta-RL methods. We demonstrate the effectiveness of our approach on common meta-RL <b>benchmark</b> domains, attaining greater return with better sample efficiency (up to $15\times$) while requiring very little hyperparameter tuning. In addition, we validate our approach on a slate of more challenging, higher-dimensional domains, taking a step towards real-world generalizing agents.</p></p class="citation"></blockquote><h3 id=2845--129320-hyperparameters-in-continual-learning-a-reality-check-sungmin-cha-et-al-2024>(28/45 | 129/320) Hyperparameters in Continual Learning: a Reality Check (Sungmin Cha et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sungmin Cha, Kyunghyun Cho. (2024)<br><strong>Hyperparameters in Continual Learning: a Reality Check</strong><br><button class=copy-to-clipboard title="Hyperparameters in Continual Learning: a Reality Check" index=129>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-129 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 13<br>Keywords: Benchmarking, Continual Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09066v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09066v1.pdf filename=2403.09066v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Various algorithms for <b>continual</b> <b>learning</b> (CL) have been designed with the goal of effectively alleviating the trade-off between stability and plasticity during the CL process. To achieve this goal, tuning appropriate hyperparameters for each algorithm is essential. As an evaluation protocol, it has been common practice to train a CL algorithm using diverse hyperparameter values on a CL scenario constructed with a <b>benchmark</b> dataset. Subsequently, the best performance attained with the optimal hyperparameter value serves as the criterion for evaluating the CL algorithm. In this paper, we contend that this evaluation protocol is not only impractical but also incapable of effectively assessing the CL capability of a CL algorithm. Returning to the fundamental principles of model evaluation in machine learning, we propose an evaluation protocol that involves Hyperparameter Tuning and Evaluation phases. Those phases consist of different datasets but share the same CL scenario. In the Hyperparameter Tuning phase, each algorithm is iteratively trained with different hyperparameter values to find the optimal hyperparameter values. Subsequently, in the Evaluation phase, the optimal hyperparameter values is directly applied for training each algorithm, and their performance in the Evaluation phase serves as the criterion for evaluating them. Through experiments on CIFAR-100 and ImageNet-100 based on the proposed protocol in class-incremental learning, we not only observed that the existing evaluation method fail to properly assess the CL capability of each algorithm but also observe that some recently proposed state-of-the-art algorithms, which reported superior performance, actually exhibit inferior performance compared to the previous algorithm.</p></p class="citation"></blockquote><h3 id=2945--130320-taming-cross-domain-representation-variance-in-federated-prototype-learning-with-heterogeneous-data-domains-lei-wang-et-al-2024>(29/45 | 130/320) Taming Cross-Domain Representation Variance in Federated Prototype Learning with Heterogeneous Data Domains (Lei Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lei Wang, Jieming Bian, Letian Zhang, Chen Chen, Jie Xu. (2024)<br><strong>Taming Cross-Domain Representation Variance in Federated Prototype Learning with Heterogeneous Data Domains</strong><br><button class=copy-to-clipboard title="Taming Cross-Domain Representation Variance in Federated Prototype Learning with Heterogeneous Data Domains" index=130>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-130 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 13<br>Keywords: Clustering, Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09048v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09048v1.pdf filename=2403.09048v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Federated</b> <b>learning</b> (FL) allows collaborative machine learning training without sharing private data. While most FL methods assume identical data domains across clients, real-world scenarios often involve heterogeneous data domains. <b>Federated</b> <b>Prototype</b> Learning (FedPL) addresses this issue, using mean feature vectors as prototypes to enhance model generalization. However, existing FedPL methods create the same number of prototypes for each client, leading to cross-domain performance gaps and disparities for clients with varied data distributions. To mitigate cross-domain feature representation variance, we introduce FedPLVM, which establishes variance-aware dual-level prototypes <b>clustering</b> and employs a novel $\alpha$-sparsity prototype loss. The dual-level prototypes <b>clustering</b> strategy creates local clustered prototypes based on private data features, then performs global prototypes <b>clustering</b> to reduce communication complexity and preserve local data privacy. The $\alpha$-sparsity prototype loss aligns samples from underrepresented domains, enhancing intra-class similarity and reducing inter-class similarity. Evaluations on Digit-5, Office-10, and DomainNet datasets demonstrate our method&rsquo;s superiority over existing approaches.</p></p class="citation"></blockquote><h3 id=3045--131320-achieving-pareto-optimality-using-efficient-parameter-reduction-for-dnns-in-resource-constrained-edge-environment-atah-nuh-mih-et-al-2024>(30/45 | 131/320) Achieving Pareto Optimality using Efficient Parameter Reduction for DNNs in Resource-Constrained Edge Environment (Atah Nuh Mih et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Atah Nuh Mih, Alireza Rahimi, Asfia Kawnine, Francis Palma, Monica Wachowicz, Rickey Dubay, Hung Cao. (2024)<br><strong>Achieving Pareto Optimality using Efficient Parameter Reduction for DNNs in Resource-Constrained Edge Environment</strong><br><button class=copy-to-clipboard title="Achieving Pareto Optimality using Efficient Parameter Reduction for DNNs in Resource-Constrained Edge Environment" index=131>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-131 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10569v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10569v1.pdf filename=2403.10569v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper proposes an optimization of an existing Deep Neural Network (DNN) that improves its hardware utilization and facilitates on-device training for resource-constrained edge environments. We implement efficient parameter reduction strategies on Xception that shrink the model size without sacrificing accuracy, thus decreasing memory utilization during training. We evaluate our model in two experiments: Caltech-101 image classification and PCB defect detection and compare its performance against the original Xception and lightweight models, EfficientNetV2B1 and MobileNetV2. The results of the Caltech-101 image classification show that our model has a better test accuracy (76.21%) than Xception (75.89%), uses less memory on average (847.9MB) than Xception (874.6MB), and has faster training and inference times. The lightweight models overfit with EfficientNetV2B1 having a 30.52% test accuracy and MobileNetV2 having a 58.11% test accuracy. Both lightweight models have better memory usage than our model and Xception. On the PCB defect detection, our model has the best test accuracy (90.30%), compared to Xception (88.10%), EfficientNetV2B1 (55.25%), and MobileNetV2 (50.50%). MobileNetV2 has the least average memory usage (849.4MB), followed by our model (865.8MB), then EfficientNetV2B1 (874.8MB), and Xception has the highest (893.6MB). We further experiment with pre-trained weights and observe that memory usage decreases thereby showing the benefits of <b>transfer</b> <b>learning.</b> A Pareto analysis of the models&rsquo; performance shows that our optimized model architecture satisfies accuracy and low memory utilization objectives.</p></p class="citation"></blockquote><h3 id=3145--132320-generalizing-denoising-to-non-equilibrium-structures-improves-equivariant-force-fields-yi-lun-liao-et-al-2024>(31/45 | 132/320) Generalizing Denoising to Non-Equilibrium Structures Improves Equivariant Force Fields (Yi-Lun Liao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yi-Lun Liao, Tess Smidt, Abhishek Das. (2024)<br><strong>Generalizing Denoising to Non-Equilibrium Structures Improves Equivariant Force Fields</strong><br><button class=copy-to-clipboard title="Generalizing Denoising to Non-Equilibrium Structures Improves Equivariant Force Fields" index=132>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-132 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, physics-chem-ph, physics-comp-ph<br>Keyword Score: 10<br>Keywords: Node Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09549v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09549v1.pdf filename=2403.09549v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Understanding the interactions of atoms such as forces in 3D atomistic systems is fundamental to many applications like molecular dynamics and catalyst design. However, simulating these interactions requires compute-intensive ab initio calculations and thus results in limited data for training neural networks. In this paper, we propose to use denoising non-equilibrium structures (DeNS) as an auxiliary task to better leverage training data and improve performance. For training with DeNS, we first corrupt a 3D structure by adding noise to its 3D coordinates and then predict the noise. Different from previous works on denoising, which are limited to equilibrium structures, the proposed method generalizes denoising to a much larger set of non-equilibrium structures. The main difference is that a non-equilibrium structure does not correspond to local energy minima and has non-zero forces, and therefore it can have many possible atomic positions compared to an equilibrium structure. This makes denoising non-equilibrium structures an ill-posed problem since the target of denoising is not uniquely defined. Our key insight is to additionally encode the forces of the original non-equilibrium structure to specify which non-equilibrium structure we are denoising. Concretely, given a corrupted non-equilibrium structure and the forces of the original one, we predict the non-equilibrium structure satisfying the input forces instead of any arbitrary structures. Since DeNS requires encoding forces, DeNS favors equivariant networks, which can easily incorporate forces and other higher-order tensors in <b>node</b> <b>embeddings.</b> We study the effectiveness of training equivariant networks with DeNS on OC20, OC22 and MD17 datasets and demonstrate that DeNS can achieve new state-of-the-art results on OC20 and OC22 and significantly improve training efficiency on MD17.</p></p class="citation"></blockquote><h3 id=3245--133320-a-reinforcement-learning-approach-to-dairy-farm-battery-management-using-q-learning-nawazish-ali-et-al-2024>(32/45 | 133/320) A Reinforcement Learning Approach to Dairy Farm Battery Management using Q Learning (Nawazish Ali et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nawazish Ali, Abdul Wahid, Rachael Shaw, Karl Mason. (2024)<br><strong>A Reinforcement Learning Approach to Dairy Farm Battery Management using Q Learning</strong><br><button class=copy-to-clipboard title="A Reinforcement Learning Approach to Dairy Farm Battery Management using Q Learning" index=133>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-133 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09499v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09499v2.pdf filename=2403.09499v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Dairy farming consumes a significant amount of energy, making it an energy-intensive sector within agriculture. Integrating renewable energy generation into dairy farming could help address this challenge. Effective battery management is important for integrating renewable energy generation. Managing battery charging and discharging poses significant challenges because of fluctuations in electrical consumption, the intermittent nature of renewable energy generation, and fluctuations in energy prices. Artificial Intelligence (AI) has the potential to significantly improve the use of renewable energy in dairy farming, however, there is limited research conducted in this particular domain. This research considers Ireland as a case study as it works towards attaining its 2030 energy strategy centered on the utilization of renewable sources. This study proposes a Q-learning-based algorithm for scheduling battery charging and discharging in a dairy farm setting. This research also explores the effect of the proposed algorithm by adding wind generation data and considering additional case studies. The proposed algorithm reduces the cost of imported electricity from the grid by 13.41%, peak demand by 2%, and 24.49% when utilizing wind generation. These results underline how <b>reinforcement</b> <b>learning</b> is highly effective in managing batteries in the dairy farming sector.</p></p class="citation"></blockquote><h3 id=3345--134320-da-pfl-dynamic-affinity-aggregation-for-personalized-federated-learning-xu-yang-et-al-2024>(33/45 | 134/320) DA-PFL: Dynamic Affinity Aggregation for Personalized Federated Learning (Xu Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xu Yang, Jiyuan Feng, Songyue Guo, Ye Wang, Ye Ding, Binxing Fang, Qing Liao. (2024)<br><strong>DA-PFL: Dynamic Affinity Aggregation for Personalized Federated Learning</strong><br><button class=copy-to-clipboard title="DA-PFL: Dynamic Affinity Aggregation for Personalized Federated Learning" index=134>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-134 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-DC, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09284v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09284v1.pdf filename=2403.09284v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Personalized <b>federated</b> <b>learning</b> becomes a hot research topic that can learn a personalized learning model for each client. Existing personalized <b>federated</b> <b>learning</b> models prefer to aggregate similar clients with similar data distribution to improve the performance of learning models. However, similaritybased personalized <b>federated</b> <b>learning</b> methods may exacerbate the class imbalanced problem. In this paper, we propose a novel Dynamic Affinity-based Personalized <b>Federated</b> <b>Learning</b> model (DA-PFL) to alleviate the class imbalanced problem during <b>federated</b> <b>learning.</b> Specifically, we build an affinity metric from a complementary perspective to guide which clients should be aggregated. Then we design a dynamic aggregation strategy to dynamically aggregate clients based on the affinity metric in each round to reduce the class imbalanced risk. Extensive experiments show that the proposed DA-PFL model can significantly improve the accuracy of each client in three real-world datasets with state-of-the-art comparison methods.</p></p class="citation"></blockquote><h3 id=3445--135320-uncertainty-quantification-for-cross-subject-motor-imagery-classification-prithviraj-manivannan-et-al-2024>(34/45 | 135/320) Uncertainty Quantification for cross-subject Motor Imagery classification (Prithviraj Manivannan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Prithviraj Manivannan, Ivo Pascal de Jong, Matias Valdenegro-Toro, Andreea Ioana Sburlea. (2024)<br><strong>Uncertainty Quantification for cross-subject Motor Imagery classification</strong><br><button class=copy-to-clipboard title="Uncertainty Quantification for cross-subject Motor Imagery classification" index=135>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-135 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09228v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09228v1.pdf filename=2403.09228v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Uncertainty Quantification aims to determine when the prediction from a Machine Learning model is likely to be wrong. Computer Vision research has explored methods for determining epistemic uncertainty (also known as model uncertainty), which should correspond with generalisation error. These methods theoretically allow to predict misclassifications due to inter-subject variability. We applied a variety of Uncertainty Quantification methods to predict misclassifications for a Motor Imagery Brain Computer Interface. Deep Ensembles performed best, both in terms of classification performance and cross-subject Uncertainty Quantification performance. However, we found that standard <b>CNNs</b> with Softmax output performed better than some of the more advanced methods.</p></p class="citation"></blockquote><h3 id=3545--136320-mcformer-multivariate-time-series-forecasting-with-mixed-channels-transformer-wenyong-han-et-al-2024>(35/45 | 136/320) MCformer: Multivariate Time Series Forecasting with Mixed-Channels Transformer (Wenyong Han et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenyong Han, Tao Zhu Member, Liming Chen, Huansheng Ning, Yang Luo, Yaping Wan. (2024)<br><strong>MCformer: Multivariate Time Series Forecasting with Mixed-Channels Transformer</strong><br><button class=copy-to-clipboard title="MCformer: Multivariate Time Series Forecasting with Mixed-Channels Transformer" index=136>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-136 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, eess-SP<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09223v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09223v1.pdf filename=2403.09223v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The massive generation of time-series data by largescale Internet of Things (IoT) devices necessitates the exploration of more effective models for multivariate time-series forecasting. In previous models, there was a predominant use of the Channel Dependence (CD) strategy (where each channel represents a univariate sequence). Current state-of-the-art (SOTA) models primarily rely on the Channel Independence (CI) strategy. The CI strategy treats all channels as a single channel, expanding the dataset to improve generalization performance and avoiding inter-channel correlation that disrupts long-term features. However, the CI strategy faces the challenge of interchannel correlation forgetting. To address this issue, we propose an innovative Mixed Channels strategy, combining the data expansion advantages of the CI strategy with the ability to counteract inter-channel correlation forgetting. Based on this strategy, we introduce MCformer, a multivariate time-series forecasting model with mixed channel features. The model blends a specific number of channels, leveraging an attention mechanism to effectively capture inter-channel correlation information when modeling long-term features. Experimental results demonstrate that the Mixed Channels strategy outperforms pure CI strategy in multivariate time-series forecasting tasks.</p></p class="citation"></blockquote><h3 id=3645--137320-on-the-laplace-approximation-as-model-selection-criterion-for-gaussian-processes-andreas-besginow-et-al-2024>(36/45 | 137/320) On the Laplace Approximation as Model Selection Criterion for Gaussian Processes (Andreas Besginow et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andreas Besginow, Jan David Hüwel, Thomas Pawellek, Christian Beecks, Markus Lange-Hegermann. (2024)<br><strong>On the Laplace Approximation as Model Selection Criterion for Gaussian Processes</strong><br><button class=copy-to-clipboard title="On the Laplace Approximation as Model Selection Criterion for Gaussian Processes" index=137>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-137 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Gaussian Process<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09215v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09215v1.pdf filename=2403.09215v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Model selection aims to find the best model in terms of accuracy, interpretability or simplicity, preferably all at once. In this work, we focus on evaluating model performance of <b>Gaussian</b> <b>process</b> models, i.e. finding a metric that provides the best trade-off between all those criteria. While previous work considers metrics like the likelihood, AIC or dynamic nested sampling, they either lack performance or have significant runtime issues, which severely limits applicability. We address these challenges by introducing multiple metrics based on the Laplace approximation, where we overcome a severe inconsistency occuring during naive application of the Laplace approximation. Experiments show that our metrics are comparable in quality to the gold standard dynamic nested sampling without compromising for computational speed. Our model selection criteria allow significantly faster and high quality model selection of <b>Gaussian</b> <b>process</b> models.</p></p class="citation"></blockquote><h3 id=3745--138320-design-of-an-basis-projected-layer-for-sparse-datasets-in-deep-learning-training-using-gc-ms-spectra-as-a-case-study-yu-tang-chang-et-al-2024>(37/45 | 138/320) Design of an basis-projected layer for sparse datasets in deep learning training using gc-ms spectra as a case study (Yu Tang Chang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yu Tang Chang, Shih Fang Chen. (2024)<br><strong>Design of an basis-projected layer for sparse datasets in deep learning training using gc-ms spectra as a case study</strong><br><button class=copy-to-clipboard title="Design of an basis-projected layer for sparse datasets in deep learning training using gc-ms spectra as a case study" index=138>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-138 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: 68-06, I-2-4; J-2, cs-LG, cs.LG, eess-SP<br>Keyword Score: 10<br>Keywords: Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09188v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09188v1.pdf filename=2403.09188v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep learning (DL) models encompass millions or even billions of parameters and learn complex patterns from big data. However, not all data are initially stored in a suitable formation to effectively train a DL model, e.g., gas chromatography-mass spectrometry (GC-MS) spectra and DNA sequence. These datasets commonly contain many zero values, and the sparse data formation causes difficulties in optimizing DL models. A DL module called the basis-projected layer (BPL) was proposed to mitigate the issue by transforming the sparse data into a dense representation. The transformed data is expected to facilitate the gradient calculation and <b>finetuned</b> process in a DL training process. The dataset, example of a sparse dataset, contained 362 specialty coffee odorant spectra detected from GC-MS. The BPL layer was placed at the beginning of the DL model. The tunable parameters in the layer were learnable projected axes that were the bases of a new representation space. The layer rotated these bases when its parameters were updated. When the number of the bases was the same as the original dimension, the increasing percentage of the F1 scores was 8.56%. Furthermore, when the number was set as 768 (the original dimension was 490), the increasing percentage of the F1 score was 11.49%. The layer not only maintained the model performance and even constructed a better representation space in analyzing sparse datasets.</p></p class="citation"></blockquote><h3 id=3845--139320-towards-diverse-perspective-learning-with-selection-over-multiple-temporal-poolings-jihyeon-seong-et-al-2024>(38/45 | 139/320) Towards Diverse Perspective Learning with Selection over Multiple Temporal Poolings (Jihyeon Seong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jihyeon Seong, Jungmin Kim, Jaesik Choi. (2024)<br><strong>Towards Diverse Perspective Learning with Selection over Multiple Temporal Poolings</strong><br><button class=copy-to-clipboard title="Towards Diverse Perspective Learning with Selection over Multiple Temporal Poolings" index=139>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-139 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09749v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09749v1.pdf filename=2403.09749v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In Time Series Classification (TSC), temporal pooling methods that consider sequential information have been proposed. However, we found that each temporal pooling has a distinct mechanism, and can perform better or worse depending on time series data. We term this fixed pooling mechanism a single perspective of temporal poolings. In this paper, we propose a novel temporal pooling method with diverse perspective learning: Selection over Multiple Temporal Poolings (SoM-TP). SoM-TP dynamically selects the optimal temporal pooling among multiple methods for each data by attention. The dynamic pooling selection is motivated by the ensemble concept of Multiple Choice Learning (MCL), which selects the best among multiple outputs. The pooling selection by SoM-TP&rsquo;s attention enables a non-iterative pooling ensemble within a single classifier. Additionally, we define a perspective loss and Diverse Perspective Learning Network (DPLN). The loss works as a regularizer to reflect all the pooling perspectives from DPLN. Our perspective analysis using Layer-wise Relevance Propagation (LRP) reveals the limitation of a single perspective and ultimately demonstrates diverse perspective learning of SoM-TP. We also show that SoM-TP outperforms <b>CNN</b> models based on other temporal poolings and state-of-the-art models in TSC with extensive UCR/UEA repositories.</p></p class="citation"></blockquote><h3 id=3945--140320-ditmos-delving-into-diverse-tiny-model-selection-on-microcontrollers-xiao-ma-et-al-2024>(39/45 | 140/320) DiTMoS: Delving into Diverse Tiny-Model Selection on Microcontrollers (Xiao Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiao Ma, Shengfeng He, Hezhe Qiao, Dong Ma. (2024)<br><strong>DiTMoS: Delving into Diverse Tiny-Model Selection on Microcontrollers</strong><br><button class=copy-to-clipboard title="DiTMoS: Delving into Diverse Tiny-Model Selection on Microcontrollers" index=140>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-140 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Emotion Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09035v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09035v1.pdf filename=2403.09035v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Enabling efficient and accurate deep neural network (DNN) inference on microcontrollers is non-trivial due to the constrained on-chip resources. Current methodologies primarily focus on compressing larger models yet at the expense of model accuracy. In this paper, we rethink the problem from the inverse perspective by constructing small/weak models directly and improving their accuracy. Thus, we introduce DiTMoS, a novel DNN training and inference framework with a selector-classifiers architecture, where the selector routes each input sample to the appropriate classifier for classification. DiTMoS is grounded on a key insight: a composition of weak models can exhibit high diversity and the union of them can significantly boost the accuracy upper bound. To approach the upper bound, DiTMoS introduces three strategies including diverse training data splitting to increase the classifiers&rsquo; diversity, adversarial selector-classifiers training to ensure synergistic interactions thereby maximizing their complementarity, and heterogeneous feature aggregation to improve the capacity of classifiers. We further propose a network slicing technique to alleviate the extra memory overhead incurred by feature aggregation. We deploy DiTMoS on the Neucleo STM32F767ZI board and evaluate it based on three time-series datasets for human activity recognition, keywords spotting, and <b>emotion</b> <b>recognition,</b> respectively. The experiment results manifest that: (a) DiTMoS achieves up to 13.4% accuracy improvement compared to the best baseline; (b) network slicing almost completely eliminates the memory overhead incurred by feature aggregation with a marginal increase of latency.</p></p class="citation"></blockquote><h3 id=4045--141320-towards-the-reusability-and-compositionality-of-causal-representations-davide-talon-et-al-2024>(40/45 | 141/320) Towards the Reusability and Compositionality of Causal Representations (Davide Talon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Davide Talon, Phillip Lippe, Stuart James, Alessio Del Bue, Sara Magliacane. (2024)<br><strong>Towards the Reusability and Compositionality of Causal Representations</strong><br><button class=copy-to-clipboard title="Towards the Reusability and Compositionality of Causal Representations" index=141>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-141 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 8<br>Keywords: Benchmarking, Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09830v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09830v1.pdf filename=2403.09830v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Causal <b>Representation</b> <b>Learning</b> (CRL) aims at identifying high-level causal factors and their relationships from high-dimensional observations, e.g., images. While most CRL works focus on learning causal <b>representations</b> <b>in</b> a single environment, in this work we instead propose a first step towards learning causal <b>representations</b> <b>from</b> temporal sequences of images that can be adapted in a new environment, or composed across multiple related environments. In particular, we introduce DECAF, a framework that detects which causal factors can be reused and which need to be adapted from previously learned causal <b>representations.</b> <b>Our</b> approach is based on the availability of intervention targets, that indicate which variables are perturbed at each time step. Experiments on three <b>benchmark</b> datasets show that integrating our framework with four state-of-the-art CRL approaches leads to accurate <b>representations</b> <b>in</b> a new environment with only a few samples.</p></p class="citation"></blockquote><h3 id=4145--142320-s2mvtc-a-simple-yet-efficient-scalable-multi-view-tensor-clustering-zhen-long-et-al-2024>(41/45 | 142/320) S^2MVTC: a Simple yet Efficient Scalable Multi-View Tensor Clustering (Zhen Long et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhen Long, Qiyuan Wang, Yazhou Ren, Yipeng Liu, Ce Zhu. (2024)<br><strong>S^2MVTC: a Simple yet Efficient Scalable Multi-View Tensor Clustering</strong><br><button class=copy-to-clipboard title="S^2MVTC: a Simple yet Efficient Scalable Multi-View Tensor Clustering" index=142>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-142 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 6<br>Keywords: Graph, Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09107v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09107v1.pdf filename=2403.09107v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Anchor-based large-scale multi-view <b>clustering</b> has attracted considerable attention for its effectiveness in handling massive datasets. However, current methods mainly seek the consensus embedding feature for <b>clustering</b> by exploring global correlations between anchor <b>graphs</b> or projection matrices.In this paper, we propose a simple yet efficient scalable multi-view tensor <b>clustering</b> (S^2MVTC) approach, where our focus is on learning correlations of embedding features within and across views. Specifically, we first construct the embedding feature tensor by stacking the embedding features of different views into a tensor and rotating it. Additionally, we build a novel tensor low-frequency approximation (TLFA) operator, which incorporates <b>graph</b> similarity into embedding feature learning, efficiently achieving smooth representation of embedding features within different views. Furthermore, consensus constraints are applied to embedding features to ensure inter-view semantic consistency. Experimental results on six large-scale multi-view datasets demonstrate that S^2MVTC significantly outperforms state-of-the-art algorithms in terms of <b>clustering</b> performance and CPU execution time, especially when handling massive data. The code of S^2MVTC is publicly available at <a href=https://github.com/longzhen520/S2MVTC>https://github.com/longzhen520/S2MVTC</a>.</p></p class="citation"></blockquote><h3 id=4245--143320-multi-fidelity-bayesian-optimization-with-across-task-transferable-max-value-entropy-search-yunchuan-zhang-et-al-2024>(42/45 | 143/320) Multi-Fidelity Bayesian Optimization With Across-Task Transferable Max-Value Entropy Search (Yunchuan Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yunchuan Zhang, Sangwoo Park, Osvaldo Simeone. (2024)<br><strong>Multi-Fidelity Bayesian Optimization With Across-Task Transferable Max-Value Entropy Search</strong><br><button class=copy-to-clipboard title="Multi-Fidelity Bayesian Optimization With Across-Task Transferable Max-Value Entropy Search" index=143>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-143 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-IT, cs-LG, cs.LG, eess-SP, math-IT<br>Keyword Score: 5<br>Keywords: Black Box<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09570v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09570v1.pdf filename=2403.09570v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In many applications, ranging from logistics to engineering, a designer is faced with a sequence of optimization tasks for which the objectives are in the form of <b>black-box</b> <b>functions</b> that are costly to evaluate. For example, the designer may need to tune the hyperparameters of neural network models for different learning tasks over time. Rather than evaluating the objective function for each candidate solution, the designer may have access to approximations of the objective functions, for which higher-fidelity evaluations entail a larger cost. Existing multi-fidelity <b>black-box</b> <b>optimization</b> strategies select candidate solutions and fidelity levels with the goal of maximizing the information accrued about the optimal value or solution for the current task. Assuming that successive optimization tasks are related, this paper introduces a novel information-theoretic acquisition function that balances the need to acquire information about the current task with the goal of collecting information transferable to future tasks. The proposed method includes shared inter-task latent variables, which are transferred across tasks by implementing particle-based variational Bayesian updates. Experimental results across synthetic and real-world examples reveal that the proposed provident acquisition strategy that caters to future tasks can significantly improve the optimization efficiency as soon as a sufficient number of tasks is processed.</p></p class="citation"></blockquote><h3 id=4345--144320-a-collection-of-the-accepted-papers-for-the-human-centric-representation-learning-workshop-at-aaai-2024-dimitris-spathis-et-al-2024>(43/45 | 144/320) A collection of the accepted papers for the Human-Centric Representation Learning workshop at AAAI 2024 (Dimitris Spathis et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dimitris Spathis, Aaqib Saeed, Ali Etemad, Sana Tonekaboni, Stefanos Laskaridis, Shohreh Deldari, Chi Ian Tang, Patrick Schwab, Shyam Tailor. (2024)<br><strong>A collection of the accepted papers for the Human-Centric Representation Learning workshop at AAAI 2024</strong><br><button class=copy-to-clipboard title="A collection of the accepted papers for the Human-Centric Representation Learning workshop at AAAI 2024" index=144>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-144 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 5<br>Keywords: Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10561v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10561v1.pdf filename=2403.10561v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This non-archival index is not complete, as some accepted papers chose to opt-out of inclusion. The list of all accepted papers is available on the workshop website.</p></p class="citation"></blockquote><h3 id=4445--145320-timemachine-a-time-series-is-worth-4-mambas-for-long-term-forecasting-md-atik-ahamed-et-al-2024>(44/45 | 145/320) TimeMachine: A Time Series is Worth 4 Mambas for Long-term Forecasting (Md Atik Ahamed et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Md Atik Ahamed, Qiang Cheng. (2024)<br><strong>TimeMachine: A Time Series is Worth 4 Mambas for Long-term Forecasting</strong><br><button class=copy-to-clipboard title="TimeMachine: A Time Series is Worth 4 Mambas for Long-term Forecasting" index=145>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-145 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09898v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09898v1.pdf filename=2403.09898v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Long-term time-series forecasting remains challenging due to the difficulty in capturing long-term dependencies, achieving linear scalability, and maintaining computational efficiency. We introduce TimeMachine, an innovative model that leverages Mamba, a state-space model, to capture long-term dependencies in multivariate time series data while maintaining linear scalability and small memory footprints. TimeMachine exploits the unique properties of time series data to produce salient contextual cues at multi-scales and leverage an innovative integrated quadruple-Mamba architecture to unify the handling of channel-mixing and channel-independence situations, thus enabling effective selection of contents for prediction against global and local contexts at different scales. Experimentally, TimeMachine achieves superior performance in prediction accuracy, scalability, and memory efficiency, as extensively validated using <b>benchmark</b> datasets. Code availability: <a href=https://github.com/Atik-Ahamed/TimeMachine>https://github.com/Atik-Ahamed/TimeMachine</a></p></p class="citation"></blockquote><h3 id=4545--146320-recursive-causal-discovery-ehsan-mokhtarian-et-al-2024>(45/45 | 146/320) Recursive Causal Discovery (Ehsan Mokhtarian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ehsan Mokhtarian, Sepehr Elahi, Sina Akbari, Negar Kiyavash. (2024)<br><strong>Recursive Causal Discovery</strong><br><button class=copy-to-clipboard title="Recursive Causal Discovery" index=146>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-146 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09300v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09300v1.pdf filename=2403.09300v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Causal discovery, i.e., learning the causal <b>graph</b> from data, is often the first step toward the identification and estimation of causal effects, a key requirement in numerous scientific domains. Causal discovery is hampered by two main challenges: limited data results in errors in statistical testing and the computational complexity of the learning task is daunting. This paper builds upon and extends four of our prior publications (Mokhtarian et al., 2021; Akbari et al., 2021; Mokhtarian et al., 2022, 2023a). These works introduced the concept of removable variables, which are the only variables that can be removed recursively for the purpose of causal discovery. Presence and identification of removable variables allow recursive approaches for causal discovery, a promising solution that helps to address the aforementioned challenges by reducing the problem size successively. This reduction not only minimizes conditioning sets in each conditional independence (CI) test, leading to fewer errors but also significantly decreases the number of required CI tests. The worst-case performances of these methods nearly match the lower bound. In this paper, we present a unified framework for the proposed algorithms, refined with additional details and enhancements for a coherent presentation. A comprehensive literature review is also included, comparing the computational complexity of our methods with existing approaches, showcasing their state-of-the-art efficiency. Another contribution of this paper is the release of RCD, a Python package that efficiently implements these algorithms. This package is designed for practitioners and researchers interested in applying these methods in practical scenarios. The package is available at github.com/ban-epfl/rcd, with comprehensive documentation provided at rcdpackage.com.</p></p class="citation"></blockquote><h2 id=cscl-42>cs.CL (42)</h2><h3 id=142--147320-mt-patcher-selective-and-extendable-knowledge-distillation-from-large-language-models-for-machine-translation-jiahuan-li-et-al-2024>(1/42 | 147/320) MT-PATCHER: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation (Jiahuan Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiahuan Li, Shanbo Cheng, Shujian Huang, Jiajun Chen. (2024)<br><strong>MT-PATCHER: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation</strong><br><button class=copy-to-clipboard title="MT-PATCHER: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation" index=147>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-147 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 83<br>Keywords: Benchmarking, Fine-tuning, Knowledge Distillation, Knowledge Distillation, Knowledge Distillation, Neural Machine Translation, Neural Machine Translation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09522v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09522v1.pdf filename=2403.09522v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLM)</b> have demonstrated their strong ability in the field of <b>machine</b> <b>translation</b> <b>(MT),</b> yet they suffer from high computational cost and latency. Therefore, transferring translation <b>knowledge</b> <b>from</b> giant <b>LLMs</b> to medium-sized <b>machine</b> <b>translation</b> models is a promising research direction. However, traditional <b>knowledge</b> <b>distillation</b> methods do not take the capability of student and teacher models into consideration, therefore repeatedly teaching student models on the <b>knowledge</b> <b>they</b> have learned, and failing to extend to novel contexts and <b>knowledge.</b> <b>In</b> this paper, we propose a framework called <b>MT-Patcher,</b> which transfers <b>knowledge</b> <b>from</b> <b>LLMs</b> to existing <b>MT</b> models in a selective, comprehensive and proactive manner. Considering the current translation ability of student <b>MT</b> models, we only identify and correct their translation errors, instead of <b>distilling</b> the whole translation from the teacher. Leveraging the strong language abilities of <b>LLMs,</b> we instruct <b>LLM</b> teachers to synthesize diverse contexts and anticipate more potential errors for the student. Experiment results on translating both specific language phenomena and general <b>MT</b> <b>benchmarks</b> demonstrate that <b>finetuning</b> the student <b>MT</b> model on about 10% examples can achieve comparable results to the traditional <b>knowledge</b> <b>distillation</b> method, and synthesized potential errors and diverse contexts further improve translation performances on unseen contexts and words.</p></p class="citation"></blockquote><h3 id=242--148320-taxollama-wordnet-based-model-for-solving-multiple-lexical-sematic-tasks-viktor-moskvoretskii-et-al-2024>(2/42 | 148/320) TaxoLLaMA: WordNet-based Model for Solving Multiple Lexical Sematic Tasks (Viktor Moskvoretskii et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Viktor Moskvoretskii, Ekaterina Neminova, Alina Lobanova, Alexander Panchenko, Irina Nikishina. (2024)<br><strong>TaxoLLaMA: WordNet-based Model for Solving Multiple Lexical Sematic Tasks</strong><br><button class=copy-to-clipboard title="TaxoLLaMA: WordNet-based Model for Solving Multiple Lexical Sematic Tasks" index=148>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-148 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 80<br>Keywords: Few-shot, Few-shot Learning, Fine-tuning, Quantization, Zero-shot, LLaMA, Domain Adaptation, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09207v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09207v1.pdf filename=2403.09207v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we explore the capabilities of <b>LLMs</b> in capturing lexical-semantic knowledge from WordNet on the example of the <b>LLaMA-2-7b</b> model and test it on multiple lexical semantic tasks. As the outcome of our experiments, we present TaxoLLaMA, the everything-in-one model, lightweight due to 4-bit <b>quantization</b> and LoRA. It achieves 11 SotA results, 4 top-2 results out of 16 tasks for the Taxonomy Enrichment, Hypernym Discovery, Taxonomy Construction, and Lexical Entailment tasks. Moreover, it demonstrates very strong <b>zero-shot</b> performance on Lexical Entailment and Taxonomy Construction with no <b>fine-tuning.</b> We also explore its hidden multilingual and <b>domain</b> <b>adaptation</b> capabilities with a little tuning or <b>few-shot</b> <b>learning.</b> All datasets, code, and model are available online at <a href=https://github.com/VityaVitalich/TaxoLLaMA>https://github.com/VityaVitalich/TaxoLLaMA</a></p></p class="citation"></blockquote><h3 id=342--149320-evaluating-llms-for-gender-disparities-in-notable-persons-lauren-rhue-et-al-2024>(3/42 | 149/320) Evaluating LLMs for Gender Disparities in Notable Persons (Lauren Rhue et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lauren Rhue, Sofie Goethals, Arun Sundararajan. (2024)<br><strong>Evaluating LLMs for Gender Disparities in Notable Persons</strong><br><button class=copy-to-clipboard title="Evaluating LLMs for Gender Disparities in Notable Persons" index=149>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-149 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-IR, cs.CL<br>Keyword Score: 80<br>Keywords: Fairness, GPT, GPT-3, GPT-3.5, GPT-4, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09148v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09148v1.pdf filename=2403.09148v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study examines the use of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> for retrieving factual information, addressing concerns over their propensity to produce factually incorrect &ldquo;hallucinated&rdquo; responses or to altogether decline to even answer <b>prompt</b> at all. Specifically, it investigates the presence of gender-based biases in <b>LLMs&rsquo;</b> responses to factual inquiries. This paper takes a multi-pronged approach to evaluating <b>GPT</b> models by evaluating <b>fairness</b> across multiple dimensions of recall, hallucinations and declinations. Our findings reveal discernible gender disparities in the responses generated by <b>GPT-3.5.</b> While advancements in <b>GPT-4</b> have led to improvements in performance, they have not fully eradicated these gender disparities, notably in instances where responses are declined. The study further explores the origins of these disparities by examining the influence of gender associations in <b>prompts</b> and the homogeneity in the responses.</p></p class="citation"></blockquote><h3 id=442--150320-chartinstruct-instruction-tuning-for-chart-comprehension-and-reasoning-ahmed-masry-et-al-2024>(4/42 | 150/320) ChartInstruct: Instruction Tuning for Chart Comprehension and Reasoning (Ahmed Masry et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ahmed Masry, Mehrad Shahmohammadi, Md Rizwan Parvez, Enamul Hoque, Shafiq Joty. (2024)<br><strong>ChartInstruct: Instruction Tuning for Chart Comprehension and Reasoning</strong><br><button class=copy-to-clipboard title="ChartInstruct: Instruction Tuning for Chart Comprehension and Reasoning" index=150>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-150 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 80<br>Keywords: Fine-tuning, Instruction Following, Question Answering, Reasoning, Instruction Tuning, Large Language Model, Summarization, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09028v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09028v1.pdf filename=2403.09028v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Charts provide visual representations of data and are widely used for analyzing information, addressing queries, and conveying insights to others. Various chart-related downstream tasks have emerged recently, such as <b>question-answering</b> <b>and</b> <b>summarization.</b> A common strategy to solve these tasks is to <b>fine-tune</b> various models originally trained on vision tasks language. However, such task-specific models are not capable of solving a wide range of chart-related tasks, constraining their real-world applicability. To overcome these challenges, we introduce ChartInstruct: a novel chart-specific <b>vision-language</b> <b>Instruction-following</b> <b>dataset</b> comprising 191K <b>instructions</b> <b>generated</b> with 71K charts. We then present two distinct systems for <b>instruction</b> <b>tuning</b> on such datasets: (1) an end-to-end model that connects a vision encoder for chart understanding with a <b>LLM;</b> and (2) a pipeline model that employs a two-step approach to extract chart data tables and input them into the <b>LLM.</b> In experiments on four downstream tasks, we first show the effectiveness of our model&ndash;achieving a new set of state-of-the-art results. Further evaluation shows that our <b>instruction-tuning</b> <b>approach</b> supports a wide array of real-world chart comprehension and <b>reasoning</b> scenarios, thereby expanding the scope and applicability of our models to new kinds of tasks.</p></p class="citation"></blockquote><h3 id=542--151320-komodo-a-linguistic-expedition-into-indonesias-regional-languages-louis-owen-et-al-2024>(5/42 | 151/320) Komodo: A Linguistic Expedition into Indonesia&rsquo;s Regional Languages (Louis Owen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Louis Owen, Vishesh Tripathi, Abhay Kumar, Biddwan Ahmed. (2024)<br><strong>Komodo: A Linguistic Expedition into Indonesia&rsquo;s Regional Languages</strong><br><button class=copy-to-clipboard title="Komodo: A Linguistic Expedition into Indonesia's Regional Languages" index=151>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-151 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 73<br>Keywords: Benchmarking, Cohere, GPT, GPT-3, GPT-3.5, LLaMA, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09362v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09362v2.pdf filename=2403.09362v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The recent breakthroughs in <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have mostly focused on languages with easily available and sufficient resources, such as English. However, there remains a significant gap for languages that lack sufficient linguistic resources in the public domain. Our work introduces Komodo-7B, 7-billion-parameter <b>Large</b> <b>Language</b> <b>Models</b> designed to address this gap by seamlessly operating across Indonesian, English, and 11 regional languages in Indonesia. Komodo-7B is a family of <b>LLMs</b> that consist of Komodo-7B-Base and Komodo-7B-Instruct. Komodo-7B-Instruct stands out by achieving state-of-the-art performance in various tasks and languages, outperforming the <b>benchmarks</b> set by OpenAI&rsquo;s <b>GPT-3.5,</b> <b>Cohere&rsquo;s</b> Aya-101, <b>Llama-2-Chat-13B,</b> Mixtral-8x7B-Instruct-v0.1, Gemma-7B-it , and many more. This model not only demonstrates superior performance in both language-specific and overall assessments but also highlights its capability to excel in linguistic diversity. Our commitment to advancing language models extends beyond well-resourced languages, aiming to bridge the gap for those with limited linguistic assets. Additionally, Komodo-7B-Instruct&rsquo;s better cross-language understanding contributes to addressing educational disparities in Indonesia, offering direct translations from English to 11 regional languages, a significant improvement compared to existing language translation services. Komodo-7B represents a crucial step towards inclusivity and effectiveness in language models, providing to the linguistic needs of diverse communities.</p></p class="citation"></blockquote><h3 id=642--152320-retrieval-augmented-text-to-sql-generation-for-epidemiological-question-answering-using-electronic-health-records-angelo-ziletti-et-al-2024>(6/42 | 152/320) Retrieval augmented text-to-SQL generation for epidemiological question answering using electronic health records (Angelo Ziletti et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Angelo Ziletti, Leonardo D&rsquo;Ambrosi. (2024)<br><strong>Retrieval augmented text-to-SQL generation for epidemiological question answering using electronic health records</strong><br><button class=copy-to-clipboard title="Retrieval augmented text-to-SQL generation for epidemiological question answering using electronic health records" index=152>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-152 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Unsupervised Learning, Question Answering, Text2SQL, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09226v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09226v1.pdf filename=2403.09226v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Electronic health records (EHR) and claims data are rich sources of real-world data that reflect patient health status and healthcare utilization. Querying these databases to answer epidemiological <b>questions</b> <b>is</b> challenging due to the intricacy of medical terminology and the need for complex SQL queries. Here, we introduce an end-to-end methodology that combines <b>text-to-SQL</b> generation with <b>retrieval</b> <b>augmented</b> <b>generation</b> <b>(RAG)</b> to answer epidemiological <b>questions</b> <b>using</b> EHR and claims data. We show that our approach, which integrates a medical coding step into the <b>text-to-SQL</b> process, significantly improves the performance over simple <b>prompting.</b> Our findings indicate that although current language models are not yet sufficiently accurate for <b>unsupervised</b> use, <b>RAG</b> offers a promising direction for improving their capabilities, as shown in a realistic industry setting.</p></p class="citation"></blockquote><h3 id=742--153320-exploring-the-comprehension-of-chatgpt-in-traditional-chinese-medicine-knowledge-li-yizhen-et-al-2024>(7/42 | 153/320) Exploring the Comprehension of ChatGPT in Traditional Chinese Medicine Knowledge (Li Yizhen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Li Yizhen, Huang Shaohan, Qi Jiaxing, Quan Lei, Han Dongran, Luan Zhongzhi. (2024)<br><strong>Exploring the Comprehension of ChatGPT in Traditional Chinese Medicine Knowledge</strong><br><button class=copy-to-clipboard title="Exploring the Comprehension of ChatGPT in Traditional Chinese Medicine Knowledge" index=153>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-153 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL, stat-AP<br>Keyword Score: 70<br>Keywords: Few-shot, Zero-shot, ChatGPT, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09164v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09164v1.pdf filename=2403.09164v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>No previous work has studied the performance of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> in the context of Traditional Chinese Medicine (TCM), an essential and distinct branch of medical knowledge with a rich history. To bridge this gap, we present a TCM question dataset named TCM-QA, which comprises three question types: single choice, multiple choice, and true or false, to examine the <b>LLM&rsquo;s</b> capacity for knowledge recall and comprehensive <b>reasoning</b> within the TCM domain. In our study, we evaluate two settings of the <b>LLM,</b> <b>zero-shot</b> and <b>few-shot</b> settings, while concurrently discussing the differences between English and Chinese <b>prompts.</b> Our results indicate that <b>ChatGPT</b> performs best in true or false questions, achieving the highest precision of 0.688 while scoring the lowest precision is 0.241 in multiple-choice questions. Furthermore, we observed that Chinese <b>prompts</b> outperformed English <b>prompts</b> in our evaluations. Additionally, we assess the quality of explanations generated by <b>ChatGPT</b> and their potential contribution to TCM knowledge comprehension. This paper offers valuable insights into the applicability of <b>LLMs</b> in specialized domains and paves the way for future research in leveraging these powerful models to advance TCM.</p></p class="citation"></blockquote><h3 id=842--154320-proswitch-knowledge-guided-language-model-fine-tuning-to-generate-professional-and-non-professional-styled-text-chang-zong-et-al-2024>(8/42 | 154/320) ProSwitch: Knowledge-Guided Language Model Fine-Tuning to Generate Professional and Non-Professional Styled Text (Chang Zong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chang Zong, Yuyan Chen, Weiming Lu, Jian Shao, Yueting Zhuang. (2024)<br><strong>ProSwitch: Knowledge-Guided Language Model Fine-Tuning to Generate Professional and Non-Professional Styled Text</strong><br><button class=copy-to-clipboard title="ProSwitch: Knowledge-Guided Language Model Fine-Tuning to Generate Professional and Non-Professional Styled Text" index=154>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-154 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: 68T50, I-2-7, cs-AI, cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: Fine-tuning, Text Generation, Text Summarization, Instruction Tuning, Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09131v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09131v1.pdf filename=2403.09131v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have demonstrated efficacy in various linguistic applications, including <b>text</b> <b>summarization</b> and controlled <b>text</b> <b>generation.</b> However, studies into their capacity of switching between styles via <b>fine-tuning</b> remain underexplored. This study concentrates on textual professionalism and introduces a novel methodology, named ProSwitch, which equips a language model with the ability to produce both professional and non-professional responses through knowledge-guided <b>instruction</b> <b>tuning.</b> ProSwitch unfolds across three phases: data preparation for gathering domain knowledge and training corpus; <b>instruction</b> <b>tuning</b> for optimizing language models with multiple levels of <b>instruction</b> <b>formats;</b> and comprehensive evaluation for assessing the professionalism discrimination and reference-based quality of generated <b>text.</b> <b>Comparative</b> analysis of ProSwitch against both general and specialized language models reveals that our approach outperforms baselines in switching between professional and non-professional <b>text</b> <b>generation.</b></p></p class="citation"></blockquote><h3 id=942--155320-fisher-mask-nodes-for-language-model-merging-thennal-d-k-et-al-2024>(9/42 | 155/320) Fisher Mask Nodes for Language Model Merging (Thennal D K et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Thennal D K, Ganesh Nathan, Suchithra M S. (2024)<br><strong>Fisher Mask Nodes for Language Model Merging</strong><br><button class=copy-to-clipboard title="Fisher Mask Nodes for Language Model Merging" index=155>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-155 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 60<br>Keywords: Fine-tuning, Fine-tuning, Model Pruning, Pruning, BERT, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09891v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09891v2.pdf filename=2403.09891v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Fine-tuning</b> pre-trained <b>models</b> <b>provides</b> significant advantages in downstream performance. The ubiquitous nature of pre-trained <b>models</b> <b>such</b> as <b>BERT</b> and its derivatives in natural language processing has also led to a proliferation of task-specific <b>fine-tuned</b> <b>models.</b> <b>As</b> these <b>models</b> <b>typically</b> only perform one task well, additional training or ensembling is required in multi-task scenarios. The growing field of <b>model</b> <b>merging</b> provides a solution, dealing with the challenge of combining multiple task-specific <b>models</b> <b>into</b> a single multi-task <b>model.</b> <b>In</b> this study, we introduce a novel <b>model</b> <b>merging</b> method for <b>Transformers,</b> combining insights from previous work in Fisher-weighted averaging and the use of Fisher information in <b>model</b> <b>pruning.</b> Utilizing the Fisher information of mask nodes within the <b>Transformer</b> architecture, we devise a computationally efficient weighted-averaging scheme. Our method exhibits a regular and significant performance increase across various <b>models</b> <b>in</b> the <b>BERT</b> family, outperforming full-scale Fisher-weighted averaging in a fraction of the computational cost, with baseline performance improvements of up to +6.5 and a speedup of 57.4x in the biggest <b>model.</b> <b>Our</b> results prove the potential of our method in current multi-task learning environments and suggest its scalability and adaptability to new <b>model</b> <b>architectures</b> and learning scenarios.</p></p class="citation"></blockquote><h3 id=1042--156320-quiet-star-language-models-can-teach-themselves-to-think-before-speaking-eric-zelikman-et-al-2024>(10/42 | 156/320) Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking (Eric Zelikman et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Eric Zelikman, Georges Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, Noah D. Goodman. (2024)<br><strong>Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking</strong><br><button class=copy-to-clipboard title="Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking" index=156>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-156 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 60<br>Keywords: Few-shot, Fine-tuning, Zero-shot, Question Answering, Reasoning, Perplexity<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09629v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09629v2.pdf filename=2403.09629v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>When writing and talking, people sometimes pause to think. Although <b>reasoning-focused</b> works have often framed <b>reasoning</b> as a method of answering <b>questions</b> <b>or</b> completing agentic tasks, <b>reasoning</b> is implicit in almost all written text. For example, this applies to the steps not stated between the lines of a proof or to the theory of mind underlying a conversation. In the Self-Taught Reasoner (STaR, Zelikman et al. 2022), useful thinking is learned by inferring rationales from <b>few-shot</b> examples in <b>question-answering</b> <b>and</b> learning from those that lead to a correct answer. This is a highly constrained setting &ndash; ideally, a language model could instead learn to infer unstated rationales in arbitrary text. We present Quiet-STaR, a generalization of STaR in which LMs learn to generate rationales at each token to explain future text, improving their predictions. We address key challenges, including 1) the computational cost of generating continuations, 2) the fact that the LM does not initially know how to generate or use internal thoughts, and 3) the need to predict beyond individual next tokens. To resolve these, we propose a tokenwise parallel sampling algorithm, using learnable tokens indicating a thought&rsquo;s start and end, and an extended teacher-forcing technique. Encouragingly, generated rationales disproportionately help model difficult-to-predict tokens and improve the LM&rsquo;s ability to directly answer difficult <b>questions.</b> <b>In</b> particular, after continued pretraining of an LM on a corpus of internet text with Quiet-STaR, we find <b>zero-shot</b> improvements on GSM8K (5.9%$\rightarrow$10.9%) and CommonsenseQA (36.3%$\rightarrow$47.2%) and observe a <b>perplexity</b> improvement of difficult tokens in natural text. Crucially, these improvements require no <b>fine-tuning</b> on these tasks. Quiet-STaR marks a step towards LMs that can learn to reason in a more general and scalable way.</p></p class="citation"></blockquote><h3 id=1142--157320-rectifying-demonstration-shortcut-in-in-context-learning-joonwon-jang-et-al-2024>(11/42 | 157/320) Rectifying Demonstration Shortcut in In-Context Learning (Joonwon Jang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Joonwon Jang, Sanghwan Jang, Wonbin Kweon, Minjin Jeon, Hwanjo Yu. (2024)<br><strong>Rectifying Demonstration Shortcut in In-Context Learning</strong><br><button class=copy-to-clipboard title="Rectifying Demonstration Shortcut in In-Context Learning" index=157>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-157 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: GPT, In-context Learning, In-context Learning, In-context Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09488v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09488v1.pdf filename=2403.09488v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> are able to solve various tasks with only a few demonstrations utilizing their <b>in-context</b> <b>learning</b> <b>(ICL)</b> abilities. However, <b>LLMs</b> often rely on their pre-trained semantic priors of demonstrations rather than on the input-label relationships to proceed with <b>ICL</b> prediction. In this work, we term this phenomenon as the `Demonstration Shortcut&rsquo;. While previous works have primarily focused on improving <b>ICL</b> prediction results for predefined tasks, we aim to rectify the Demonstration Shortcut, thereby enabling the <b>LLM</b> to effectively learn new input-label relationships from demonstrations. To achieve this, we introduce <b>In-Context</b> <b>Calibration,</b> a demonstration-aware calibration method. We evaluate the effectiveness of the proposed method in two settings: (1) the Original <b>ICL</b> Task using the standard label space and (2) the Task Learning setting, where the label space is replaced with semantically unrelated tokens. In both settings, <b>In-Context</b> <b>Calibration</b> demonstrates substantial improvements, with results generalized across three <b>LLM</b> families (OPT, <b>GPT,</b> and Llama2) under various configurations.</p></p class="citation"></blockquote><h3 id=1242--158320-to-label-or-not-to-label-hybrid-active-learning-for-neural-machine-translation-abdul-hameed-azeemi-et-al-2024>(12/42 | 158/320) To Label or Not to Label: Hybrid Active Learning for Neural Machine Translation (Abdul Hameed Azeemi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Abdul Hameed Azeemi, Ihsan Ayyub Qazi, Agha Ali Raza. (2024)<br><strong>To Label or Not to Label: Hybrid Active Learning for Neural Machine Translation</strong><br><button class=copy-to-clipboard title="To Label or Not to Label: Hybrid Active Learning for Neural Machine Translation" index=158>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-158 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 60<br>Keywords: Active Learning, Neural Machine Translation, Neural Machine Translation, Neural Machine Translation, Sentence Embedding, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09259v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09259v1.pdf filename=2403.09259v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Active</b> <b>learning</b> (AL) techniques reduce labeling costs for training <b>neural</b> <b>machine</b> <b>translation</b> <b>(NMT)</b> models by selecting smaller representative subsets from unlabeled data for annotation. Diversity sampling techniques select heterogeneous instances, while uncertainty sampling methods select instances with the highest model uncertainty. Both approaches have limitations - diversity methods may extract varied but trivial examples, while uncertainty sampling can yield repetitive, uninformative instances. To bridge this gap, we propose HUDS, a hybrid AL strategy for <b>domain</b> <b>adaptation</b> in <b>NMT</b> that combines uncertainty and diversity for <b>sentence</b> <b>selection.</b> HUDS computes uncertainty scores for unlabeled <b>sentences</b> <b>and</b> subsequently stratifies them. It then clusters <b>sentence</b> <b>embeddings</b> within each stratum using k-MEANS and computes diversity scores by distance to the centroid. A weighted hybrid score that combines uncertainty and diversity is then used to select the top instances for annotation in each AL iteration. Experiments on multi-domain German-English datasets demonstrate the better performance of HUDS over other strong AL baselines. We analyze the <b>sentence</b> <b>selection</b> with HUDS and show that it prioritizes diverse instances having high model uncertainty for annotation in early AL iterations.</p></p class="citation"></blockquote><h3 id=1342--159320-unveiling-the-generalization-power-of-fine-tuned-large-language-models-haoran-yang-et-al-2024>(13/42 | 159/320) Unveiling the Generalization Power of Fine-Tuned Large Language Models (Haoran Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haoran Yang, Yumeng Zhang, Jiaqi Xu, Hongyuan Lu, Pheng Ann Heng, Wai Lam. (2024)<br><strong>Unveiling the Generalization Power of Fine-Tuned Large Language Models</strong><br><button class=copy-to-clipboard title="Unveiling the Generalization Power of Fine-Tuned Large Language Models" index=159>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-159 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Fine-tuning, Fine-tuning, In-context Learning, In-context Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09162v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09162v1.pdf filename=2403.09162v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have demonstrated exceptional multitasking abilities, <b>fine-tuning</b> these models on downstream, domain-specific datasets is often necessary to yield superior performance on test sets compared to their counterparts without <b>fine-tuning.</b> However, the comprehensive effects of <b>fine-tuning</b> on the <b>LLMs&rsquo;</b> generalization ability are not fully understood. This paper delves into the differences between original, unmodified <b>LLMs</b> and their <b>fine-tuned</b> variants. Our primary investigation centers on whether <b>fine-tuning</b> affects the generalization ability intrinsic to <b>LLMs.</b> To elaborate on this, we conduct extensive experiments across five distinct language tasks on various datasets. Our main findings reveal that models <b>fine-tuned</b> on generation and classification tasks exhibit dissimilar behaviors in generalizing to different domains and tasks. Intriguingly, we observe that integrating the <b>in-context</b> <b>learning</b> strategy during <b>fine-tuning</b> on generation tasks can enhance the model&rsquo;s generalization ability. Through this systematic investigation, we aim to contribute valuable insights into the evolving landscape of <b>fine-tuning</b> practices for <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=1442--160320-information-extraction-an-application-to-the-domain-of-hyper-local-financial-data-on-developing-countries-abuzar-royesh-et-al-2024>(14/42 | 160/320) Information Extraction: An application to the domain of hyper-local financial data on developing countries (Abuzar Royesh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Abuzar Royesh, Olamide Oladeji. (2024)<br><strong>Information Extraction: An application to the domain of hyper-local financial data on developing countries</strong><br><button class=copy-to-clipboard title="Information Extraction: An application to the domain of hyper-local financial data on developing countries" index=160>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-160 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Fine-tuning, T5, Transformer, Information Retrieval, Named Entity Recognition, Relation Extraction<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09077v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09077v1.pdf filename=2403.09077v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite the need for financial data on company activities in developing countries for development research and economic analysis, such data does not exist. In this project, we develop and evaluate two Natural Language Processing (NLP) based techniques to address this issue. First, we curate a custom dataset specific to the domain of financial text data on developing countries and explore multiple approaches for <b>information</b> <b>extraction.</b> We then explore a text-to-text approach with the <b>transformer-based</b> <b>T5</b> model with the goal of undertaking simultaneous <b>NER</b> and <b>relation</b> <b>extraction.</b> We find that this model is able to learn the custom text structure output data corresponding to the entities and their <b>relations,</b> <b>resulting</b> in an accuracy of 92.44%, a precision of 68.25% and a recall of 54.20% from our best <b>T5</b> model on the combined task. Secondly, we explore an approach with sequential <b>NER</b> and <b>relation</b> <b>extration.</b> For the <b>NER,</b> we run pre-trained and <b>fine-tuned</b> models using SpaCy, and we develop a custom <b>relation</b> <b>extraction</b> model using SpaCy&rsquo;s Dependency Parser output and some heuristics to determine entity relationships \cite{spacy}. We obtain an accuracy of 84.72%, a precision of 6.06% and a recall of 5.57% on this sequential task.</p></p class="citation"></blockquote><h3 id=1542--161320-large-language-models-are-parallel-multilingual-learners-yongyu-mu-et-al-2024>(15/42 | 161/320) Large Language Models are Parallel Multilingual Learners (Yongyu Mu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yongyu Mu, Peinan Feng, Zhiquan Cao, Yuzhang Wu, Bei Li, Chenglong Wang, Tong Xiao, Kai Song, Tongran Liu, Chunliang Zhang, Jingbo Zhu. (2024)<br><strong>Large Language Models are Parallel Multilingual Learners</strong><br><button class=copy-to-clipboard title="Large Language Models are Parallel Multilingual Learners" index=161>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-161 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Pruning, In-context Learning, In-context Learning, In-context Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09073v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09073v1.pdf filename=2403.09073v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this study, we reveal an <b>in-context</b> <b>learning</b> <b>(ICL)</b> capability of multilingual <b>large</b> <b>language</b> <b>models</b> <b>(LLMs):</b> by translating the input to several languages, we provide Parallel Input in Multiple Languages (PiM) to <b>LLMs,</b> which significantly enhances their comprehension abilities. To test this capability, we design extensive experiments encompassing 8 typical datasets, 7 languages and 8 state-of-the-art multilingual <b>LLMs.</b> Experimental results show that (1) incorporating more languages help PiM surpass the conventional <b>ICL</b> further; (2) even combining with the translations that are inferior to baseline performance can also help. Moreover, by examining the activated neurons in <b>LLMs,</b> we discover a counterintuitive but interesting phenomenon. Contrary to the common thought that PiM would activate more neurons than monolingual input to leverage knowledge learned from diverse languages, PiM actually inhibits neurons and promotes more precise neuron activation especially when more languages are added. This phenomenon aligns with the neuroscience insight about synaptic <b>pruning,</b> which removes less used neural connections, strengthens remainders, and then enhances brain intelligence.</p></p class="citation"></blockquote><h3 id=1642--162320-lamp-a-language-model-on-the-map-pasquale-balsebre-et-al-2024>(16/42 | 162/320) LAMP: A Language Model on the Map (Pasquale Balsebre et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pasquale Balsebre, Weiming Huang, Gao Cong. (2024)<br><strong>LAMP: A Language Model on the Map</strong><br><button class=copy-to-clipboard title="LAMP: A Language Model on the Map" index=162>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-162 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Fine-tuning, Recommendation, GPT, GPT-4, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09059v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09059v1.pdf filename=2403.09059v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> are poised to play an increasingly important role in our lives, providing assistance across a wide array of tasks. In the geospatial domain, <b>LLMs</b> have demonstrated the ability to answer generic questions, such as identifying a country&rsquo;s capital; nonetheless, their utility is hindered when it comes to answering fine-grained questions about specific places, such as grocery stores or restaurants, which constitute essential aspects of people&rsquo;s everyday lives. This is mainly because the places in our cities haven&rsquo;t been systematically fed into <b>LLMs,</b> so as to understand and memorize them. This study introduces a novel framework for <b>fine-tuning</b> a pre-trained model on city-specific data, to enable it to provide accurate <b>recommendations,</b> while minimizing hallucinations. We share our model, LAMP, and the data used to train it. We conduct experiments to analyze its ability to correctly retrieving spatial objects, and compare it to well-known open- and closed- source language models, such as <b>GPT-4.</b> Finally, we explore its emerging capabilities through a case study on day planning.</p></p class="citation"></blockquote><h3 id=1742--163320-less-is-more-data-value-estimation-for-visual-instruction-tuning-zikang-liu-et-al-2024>(17/42 | 163/320) Less is More: Data Value Estimation for Visual Instruction Tuning (Zikang Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zikang Liu, Kun Zhou, Wayne Xin Zhao, Dawei Gao, Yaliang Li, Ji-Rong Wen. (2024)<br><strong>Less is More: Data Value Estimation for Visual Instruction Tuning</strong><br><button class=copy-to-clipboard title="Less is More: Data Value Estimation for Visual Instruction Tuning" index=163>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-163 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CV, cs.CL<br>Keyword Score: 59<br>Keywords: Benchmarking, Fine-tuning, Multi-modal, Multi-modal, Reasoning, Instruction Tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09559v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09559v1.pdf filename=2403.09559v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Visual <b>instruction</b> <b>tuning</b> is the key to building <b>multimodal</b> <b>large</b> <b>language</b> <b>models</b> (MLLMs), which greatly improves the <b>reasoning</b> capabilities of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> in vision scenario. However, existing MLLMs mostly rely on a mixture of multiple highly diverse visual <b>instruction</b> <b>datasets</b> for training (even more than a million <b>instructions),</b> <b>which</b> may introduce data redundancy. To investigate this issue, we conduct a series of empirical studies, which reveal a significant redundancy within the visual <b>instruction</b> <b>datasets,</b> and show that greatly reducing the amount of several <b>instruction</b> <b>dataset</b> even do not affect the performance. Based on the findings, we propose a new data selection approach TIVE, to eliminate redundancy within visual <b>instruction</b> <b>data.</b> TIVE first estimates the task-level and instance-level value of the visual <b>instructions</b> <b>based</b> on computed gradients. Then, according to the estimated values, TIVE determines the task proportion within the visual <b>instructions,</b> <b>and</b> selects representative instances to compose a smaller visual <b>instruction</b> <b>subset</b> for training. Experiments on LLaVA-1.5 show that our approach using only about 7.5% data can achieve comparable performance as the full-data <b>fine-tuned</b> model across seven <b>benchmarks,</b> even surpassing it on four of the <b>benchmarks.</b> Our code and data will be publicly released.</p></p class="citation"></blockquote><h3 id=1842--164320-scaling-behavior-of-machine-translation-with-large-language-models-under-prompt-injection-attacks-zhifan-sun-et-al-2024>(18/42 | 164/320) Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks (Zhifan Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhifan Sun, Antonio Valerio Miceli-Barone. (2024)<br><strong>Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks</strong><br><button class=copy-to-clipboard title="Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks" index=164>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-164 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 53<br>Keywords: Benchmarking, Neural Machine Translation, In-context Learning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09832v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09832v1.pdf filename=2403.09832v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> are increasingly becoming the preferred foundation platforms for many Natural Language Processing tasks such as <b>Machine</b> <b>Translation,</b> owing to their quality often comparable to or better than task-specific models, and the simplicity of specifying the task through natural language instructions or <b>in-context</b> examples. Their generality, however, opens them up to subversion by end users who may embed into their requests instructions that cause the model to behave in unauthorized and possibly unsafe ways. In this work we study these <b>Prompt</b> Injection Attacks (PIAs) on multiple families of <b>LLMs</b> on a <b>Machine</b> <b>Translation</b> task, focusing on the effects of model size on the attack success rates. We introduce a new <b>benchmark</b> data set and we discover that on multiple language pairs and injected <b>prompts</b> written in English, larger models under certain conditions may become more susceptible to successful attacks, an instance of the Inverse Scaling phenomenon (McKenzie et al., 2023). To our knowledge, this is the first work to study non-trivial <b>LLM</b> scaling behaviour in a multi-lingual setting.</p></p class="citation"></blockquote><h3 id=1942--165320-aratrust-an-evaluation-of-trustworthiness-for-llms-in-arabic-emad-a-alghamdi-et-al-2024>(19/42 | 165/320) AraTrust: An Evaluation of Trustworthiness for LLMs in Arabic (Emad A. Alghamdi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Emad A. Alghamdi, Reem I. Masoud, Deema Alnuhait, Afnan Y. Alomairi, Ahmed Ashraf, Mohamed Zaytoon. (2024)<br><strong>AraTrust: An Evaluation of Trustworthiness for LLMs in Arabic</strong><br><button class=copy-to-clipboard title="AraTrust: An Evaluation of Trustworthiness for LLMs in Arabic" index=165>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-165 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 53<br>Keywords: Benchmarking, GPT, GPT-4, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09017v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09017v2.pdf filename=2403.09017v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The swift progress and widespread acceptance of artificial intelligence (AI) systems highlight a pressing requirement to comprehend both the capabilities and potential risks associated with AI. Given the linguistic complexity, cultural richness, and underrepresented status of Arabic in AI research, there is a pressing need to focus on <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> performance and safety for Arabic related tasks. Despite some progress in their development, there is a lack of comprehensive trustworthiness evaluation <b>benchmarks</b> which presents a major challenge in accurately assessing and improving the safety of <b>LLMs</b> when <b>prompted</b> in Arabic. In this paper, we introduce AraTrust, the first comprehensive trustworthiness <b>benchmark</b> for <b>LLMs</b> in Arabic. AraTrust comprises 516 human-written multiple-choice questions addressing diverse dimensions related to truthfulness, ethics, safety, physical health, mental health, unfairness, illegal activities, privacy, and offensive language. We evaluated a set of <b>LLMs</b> against our <b>benchmark</b> to assess their trustworthiness. <b>GPT-4</b> was the most trustworthy <b>LLM,</b> while open-source models, particularly AceGPT 7B and Jais 13B, struggled to achieve a score of 60% in our <b>benchmark.</b></p></p class="citation"></blockquote><h3 id=2042--166320-sabiá-2-a-new-generation-of-portuguese-large-language-models-thales-sales-almeida-et-al-2024>(20/42 | 166/320) Sabiá-2: A New Generation of Portuguese Large Language Models (Thales Sales Almeida et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Thales Sales Almeida, Hugo Abonizio, Rodrigo Nogueira, Ramon Pires. (2024)<br><strong>Sabiá-2: A New Generation of Portuguese Large Language Models</strong><br><button class=copy-to-clipboard title="Sabiá-2: A New Generation of Portuguese Large Language Models" index=166>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-166 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: GPT, GPT-3, GPT-3.5, GPT-4, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09887v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09887v1.pdf filename=2403.09887v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce Sabi'a-2, a family of <b>large</b> <b>language</b> <b>models</b> trained on Portuguese texts. The models are evaluated on a diverse range of exams, including entry-level tests for Brazilian universities, professional certification exams, and graduate-level exams for various disciplines such as accounting, economics, engineering, law and medicine. Our results reveal that our best model so far, Sabi'a-2 Medium, matches or surpasses <b>GPT-4&rsquo;s</b> performance in 23 out of 64 exams and outperforms <b>GPT-3.5</b> in 58 out of 64 exams. Notably, specialization has a significant impact on a model&rsquo;s performance without the need to increase its size, allowing us to offer Sabi'a-2 Medium at a price per token that is 10 times cheaper than <b>GPT-4.</b> Finally, we identified that math and coding are key abilities that need improvement.</p></p class="citation"></blockquote><h3 id=2142--167320-logits-of-api-protected-llms-leak-proprietary-information-matthew-finlayson-et-al-2024>(21/42 | 167/320) Logits of API-Protected LLMs Leak Proprietary Information (Matthew Finlayson et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Matthew Finlayson, Xiang Ren, Swabha Swayamdipta. (2024)<br><strong>Logits of API-Protected LLMs Leak Proprietary Information</strong><br><button class=copy-to-clipboard title="Logits of API-Protected LLMs Leak Proprietary Information" index=167>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-167 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: 68T50, I-2-7, cs-AI, cs-CL, cs-CR, cs-LG, cs.CL<br>Keyword Score: 50<br>Keywords: GPT, GPT-3, GPT-3.5, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09539v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09539v2.pdf filename=2403.09539v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The commercialization of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> has led to the common practice of high-level API-only access to proprietary models. In this work, we show that even with a conservative assumption about the model architecture, it is possible to learn a surprisingly <b>large</b> <b>amount</b> <b>of</b> non-public information about an API-protected <b>LLM</b> from a relatively small number of API queries (e.g., costing under $1,000 for OpenAI&rsquo;s <b>gpt-3.5-turbo).</b> Our findings are centered on one key observation: most modern <b>LLMs</b> suffer from a softmax bottleneck, which restricts the model outputs to a linear subspace of the full output space. We show that this lends itself to a model image or a model signature which unlocks several capabilities with affordable cost: efficiently discovering the <b>LLM&rsquo;s</b> hidden size, obtaining full-vocabulary outputs, detecting and disambiguating different model updates, identifying the source <b>LLM</b> given a single full <b>LLM</b> output, and even estimating the output layer parameters. Our empirical investigations show the effectiveness of our methods, which allow us to estimate the embedding size of OpenAI&rsquo;s <b>gpt-3.5-turbo</b> to be about 4,096. Lastly, we discuss ways that <b>LLM</b> providers can guard against these attacks, as well as how these capabilities can be viewed as a feature (rather than a bug) by allowing for greater transparency and accountability.</p></p class="citation"></blockquote><h3 id=2242--168320-basque-and-spanish-counter-narrative-generation-data-creation-and-evaluation-jaione-bengoetxea-et-al-2024>(22/42 | 168/320) Basque and Spanish Counter Narrative Generation: Data Creation and Evaluation (Jaione Bengoetxea et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jaione Bengoetxea, Yi-Ling Chung, Marco Guerini, Rodrigo Agerri. (2024)<br><strong>Basque and Spanish Counter Narrative Generation: Data Creation and Evaluation</strong><br><button class=copy-to-clipboard title="Basque and Spanish Counter Narrative Generation: Data Creation and Evaluation" index=168>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-168 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Data Augmentation, Fine-tuning, Zero-shot, Neural Machine Translation, Neural Machine Translation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09159v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09159v1.pdf filename=2403.09159v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Counter Narratives (CNs) are non-negative textual responses to Hate Speech (HS) aiming at defusing online hatred and mitigating its spreading across media. Despite the recent increase in HS content posted online, research on automatic CN generation has been relatively scarce and predominantly focused on English. In this paper, we present CONAN-EUS, a new Basque and Spanish dataset for CN generation developed by means of <b>Machine</b> <b>Translation</b> <b>(MT)</b> and professional post-edition. Being a parallel corpus, also with respect to the original English CONAN, it allows to perform novel research on multilingual and crosslingual automatic generation of CNs. Our experiments on CN generation with mT5, a multilingual encoder-decoder model, show that generation greatly benefits from training on post-edited <b>data,</b> <b>as</b> opposed to relying on silver <b>MT</b> <b>data</b> <b>only.</b> These results are confirmed by their correlation with a qualitative manual evaluation, demonstrating that manually revised training <b>data</b> <b>remains</b> crucial for the quality of the generated CNs. Furthermore, multilingual <b>data</b> <b>augmentation</b> improves results over monolingual settings for structurally similar languages such as English and Spanish, while being detrimental for Basque, a language isolate. Similar findings occur in <b>zero-shot</b> crosslingual evaluations, where model transfer <b>(fine-tuning</b> in English and generating in a different target language) outperforms <b>fine-tuning</b> mT5 on <b>machine</b> <b>translated</b> <b>data</b> <b>for</b> Spanish but not for Basque. This provides an interesting insight into the asymmetry in the multilinguality of generative models, a challenging topic which is still open to research.</p></p class="citation"></blockquote><h3 id=2342--169320-ai-on-ai-exploring-the-utility-of-gpt-as-an-expert-annotator-of-ai-publications-autumn-toney-wails-et-al-2024>(23/42 | 169/320) AI on AI: Exploring the Utility of GPT as an Expert Annotator of AI Publications (Autumn Toney-Wails et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Autumn Toney-Wails, Christian Schoeberl, James Dunham. (2024)<br><strong>AI on AI: Exploring the Utility of GPT as an Expert Annotator of AI Publications</strong><br><button class=copy-to-clipboard title="AI on AI: Exploring the Utility of GPT as an Expert Annotator of AI Publications" index=169>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-169 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Fine-tuning, GPT, Transformer, Chatbot, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09097v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09097v1.pdf filename=2403.09097v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Identifying scientific publications that are within a dynamic field of research often requires costly annotation by subject-matter experts. Resources like widely-accepted classification criteria or field taxonomies are unavailable for a domain like artificial intelligence (AI), which spans emerging topics and technologies. We address these challenges by inferring a functional definition of AI research from existing expert labels, and then evaluating state-of-the-art <b>chatbot</b> models on the task of expert data annotation. Using the arXiv publication database as ground-truth, we experiment with <b>prompt</b> engineering for <b>GPT</b> <b>chatbot</b> models to identify an alternative, automated expert annotation pipeline that assigns AI labels with 94% accuracy. For comparison, we <b>fine-tune</b> SPECTER, a <b>transformer</b> language model pre-trained on scientific publications, that achieves 96% accuracy (only 2% higher than <b>GPT)</b> on classifying AI publications. Our results indicate that with effective <b>prompt</b> engineering, <b>chatbots</b> can be used as reliable data annotators even where subject-area expertise is required. To evaluate the utility of <b>chatbot-annotated</b> datasets on downstream classification tasks, we train a new classifier on <b>GPT-labeled</b> data and compare its performance to the arXiv-trained model. The classifier trained on <b>GPT-labeled</b> data outperforms the arXiv-trained model by nine percentage points, achieving 82% accuracy.</p></p class="citation"></blockquote><h3 id=2442--170320-re-search-for-the-truth-multi-round-retrieval-augmented-large-language-models-are-strong-fake-news-detectors-guanghua-li-et-al-2024>(24/42 | 170/320) Re-Search for The Truth: Multi-round Retrieval-augmented Large Language Models are Strong Fake News Detectors (Guanghua Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guanghua Li, Wensheng Lu, Wei Zhang, Defu Lian, Kezhong Lu, Rui Mao, Kai Shu, Hao Liao. (2024)<br><strong>Re-Search for The Truth: Multi-round Retrieval-augmented Large Language Models are Strong Fake News Detectors</strong><br><button class=copy-to-clipboard title="Re-Search for The Truth: Multi-round Retrieval-augmented Large Language Models are Strong Fake News Detectors" index=170>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-170 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Fake News Detection, Reasoning, Fake News Detection, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09747v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09747v1.pdf filename=2403.09747v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The proliferation of <b>fake</b> <b>news</b> <b>has</b> had far-reaching implications on politics, the economy, and society at <b>large.</b> <b>While</b> <b>Fake</b> <b>news</b> <b>detection</b> methods have been employed to mitigate this issue, they primarily depend on two essential elements: the quality and relevance of the evidence, and the effectiveness of the verdict prediction mechanism. Traditional methods, which often source information from static repositories like Wikipedia, are limited by outdated or incomplete data, particularly for emerging or rare claims. <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> known for their remarkable <b>reasoning</b> and generative capabilities, introduce a new frontier for <b>fake</b> <b>news</b> <b>detection.</b> However, like traditional methods, <b>LLM-based</b> solutions also grapple with the limitations of stale and long-tail knowledge. Additionally, retrieval-enhanced <b>LLMs</b> frequently struggle with issues such as low-quality evidence retrieval and context length constraints. To address these challenges, we introduce a novel, retrieval-augmented <b>LLMs</b> framework&ndash;the first of its kind to automatically and strategically extract key evidence from web sources for claim verification. Employing a multi-round retrieval strategy, our framework ensures the acquisition of sufficient, relevant evidence, thereby enhancing performance. Comprehensive experiments across three real-world datasets validate the framework&rsquo;s superiority over existing methods. Importantly, our model not only delivers accurate verdicts but also offers human-readable explanations to improve result interpretability.</p></p class="citation"></blockquote><h3 id=2542--171320-dynamic-memory-compression-retrofitting-llms-for-accelerated-inference-piotr-nawrot-et-al-2024>(25/42 | 171/320) Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference (Piotr Nawrot et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Piotr Nawrot, Adrian Łańcucki, Marcin Chochowski, David Tarjan, Edoardo M. Ponti. (2024)<br><strong>Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference</strong><br><button class=copy-to-clipboard title="Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference" index=171>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-171 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: LLaMA, Transformer, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09636v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09636v1.pdf filename=2403.09636v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Transformers</b> have emerged as the backbone of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> However, generation remains inefficient due to the need to store in memory a cache of key-value representations for past tokens, whose size scales linearly with the input sequence length and batch size. As a solution, we propose Dynamic Memory Compression (DMC), a method for on-line key-value cache compression at inference time. Most importantly, the model learns to apply different compression rates in different heads and layers. We retrofit pre-trained <b>LLMs</b> such as <b>Llama</b> 2 (7B, 13B and 70B) into DMC <b>Transformers,</b> achieving up to ~3.7x throughput increase in auto-regressive inference on a NVIDIA H100 GPU. DMC is applied via continued pre-training on a negligible percentage of the original data without adding any extra parameters. We find that DMC preserves the original downstream performance with up to 4x cache compression, outperforming up-trained grouped-query attention (GQA). GQA and DMC can be even combined to obtain compounded gains. As a result DMC fits longer contexts and larger batches within any given memory budget.</p></p class="citation"></blockquote><h3 id=2642--172320-large-language-models-and-causal-inference-in-collaboration-a-comprehensive-survey-xiaoyu-liu-et-al-2024>(26/42 | 172/320) Large Language Models and Causal Inference in Collaboration: A Comprehensive Survey (Xiaoyu Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaoyu Liu, Paiheng Xu, Junda Wu, Jiaxin Yuan, Yifan Yang, Yuhang Zhou, Fuxiao Liu, Tianrui Guan, Haoliang Wang, Tong Yu, Julian McAuley, Wei Ai, Furong Huang. (2024)<br><strong>Large Language Models and Causal Inference in Collaboration: A Comprehensive Survey</strong><br><button class=copy-to-clipboard title="Large Language Models and Causal Inference in Collaboration: A Comprehensive Survey" index=172>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-172 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Fairness, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09606v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09606v1.pdf filename=2403.09606v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Causal inference has shown potential in enhancing the predictive accuracy, <b>fairness,</b> robustness, and explainability of Natural Language Processing (NLP) models by capturing causal relationships among variables. The emergence of generative <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> has significantly impacted various NLP domains, particularly through their advanced <b>reasoning</b> capabilities. This survey focuses on evaluating and improving <b>LLMs</b> from a causal view in the following areas: understanding and improving the <b>LLMs&rsquo;</b> <b>reasoning</b> capacity, addressing <b>fairness</b> and safety issues in <b>LLMs,</b> complementing <b>LLMs</b> with explanations, and handling multimodality. Meanwhile, <b>LLMs&rsquo;</b> strong <b>reasoning</b> capacities can in turn contribute to the field of causal inference by aiding causal relationship discovery and causal effect estimations. This review explores the interplay between causal inference frameworks and <b>LLMs</b> from both perspectives, emphasizing their collective potential to further the development of more advanced and equitable artificial intelligence systems.</p></p class="citation"></blockquote><h3 id=2742--173320-dial-insight-fine-tuning-large-language-models-with-high-quality-domain-specific-data-preventing-capability-collapse-jianwei-sun-et-al-2024>(27/42 | 173/320) Dial-insight: Fine-tuning Large Language Models with High-Quality Domain-Specific Data Preventing Capability Collapse (Jianwei Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianwei Sun, Chaoyang Mei, Linlin Wei, Kaiyu Zheng, Na Liu, Ming Cui, Tianyi Li. (2024)<br><strong>Dial-insight: Fine-tuning Large Language Models with High-Quality Domain-Specific Data Preventing Capability Collapse</strong><br><button class=copy-to-clipboard title="Dial-insight: Fine-tuning Large Language Models with High-Quality Domain-Specific Data Preventing Capability Collapse" index=173>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-173 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Fine-tuning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09167v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09167v1.pdf filename=2403.09167v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The efficacy of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> is heavily dependent on the quality of the underlying data, particularly within specialized domains. A common challenge when <b>fine-tuning</b> <b>LLMs</b> for domain-specific applications is the potential degradation of the model&rsquo;s generalization capabilities. To address these issues, we propose a two-stage approach for the construction of production <b>prompts</b> designed to yield high-quality data. This method involves the generation of a diverse array of <b>prompts</b> that encompass a broad spectrum of tasks and exhibit a rich variety of expressions. Furthermore, we introduce a cost-effective, multi-dimensional quality assessment framework to ensure the integrity of the generated labeling data. Utilizing a dataset comprised of service provider and customer interactions from the real estate sector, we demonstrate a positive correlation between data quality and model performance. Notably, our findings indicate that the domain-specific proficiency of general <b>LLMs</b> can be enhanced through <b>fine-tuning</b> with data produced via our proposed method, without compromising their overall generalization abilities, even when exclusively domain-specific data is employed for <b>fine-tuning.</b></p></p class="citation"></blockquote><h3 id=2842--174320-autolora-automatically-tuning-matrix-ranks-in-low-rank-adaptation-based-on-meta-learning-ruiyi-zhang-et-al-2024>(28/42 | 174/320) AutoLoRA: Automatically Tuning Matrix Ranks in Low-Rank Adaptation Based on Meta Learning (Ruiyi Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruiyi Zhang, Rushi Qiang, Sai Ashish Somayajula, Pengtao Xie. (2024)<br><strong>AutoLoRA: Automatically Tuning Matrix Ranks in Low-Rank Adaptation Based on Meta Learning</strong><br><button class=copy-to-clipboard title="AutoLoRA: Automatically Tuning Matrix Ranks in Low-Rank Adaptation Based on Meta Learning" index=174>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-174 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 40<br>Keywords: Fine-tuning, Fine-tuning, Meta Learning, Natural Language Understanding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09113v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09113v2.pdf filename=2403.09113v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Large-scale pretraining followed by task-specific <b>finetuning</b> has achieved great success in various NLP tasks. Since <b>finetuning</b> all parameters of large pretrained models poses substantial computational and memory challenges, several efficient <b>finetuning</b> methods have been developed. Among them, low-rank adaptation (LoRA), which <b>finetunes</b> low-rank incremental update matrices on top of frozen pretrained weights, has proven particularly effective. Nonetheless, LoRA&rsquo;s uniform rank assignment across all layers, along with its reliance on an exhaustive search to find the best rank, leads to high computation costs and suboptimal <b>finetuning</b> performance. To address these limitations, we introduce AutoLoRA, a <b>meta</b> <b>learning</b> based framework for automatically identifying the optimal rank of each LoRA layer. AutoLoRA associates each rank-1 matrix in a low-rank update matrix with a selection variable, which determines whether the rank-1 matrix should be discarded. A <b>meta</b> <b>learning</b> based method is developed to learn these selection variables. The optimal rank is determined by thresholding the values of these variables. Our comprehensive experiments on <b>natural</b> <b>language</b> <b>understanding,</b> generation, and sequence labeling demonstrate the effectiveness of AutoLoRA.</p></p class="citation"></blockquote><h3 id=2942--175320-ragged-towards-informed-design-of-retrieval-augmented-generation-systems-jennifer-hsia-et-al-2024>(29/42 | 175/320) RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems (Jennifer Hsia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jennifer Hsia, Afreen Shaikh, Zhiruo Wang, Graham Neubig. (2024)<br><strong>RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems</strong><br><button class=copy-to-clipboard title="RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems" index=175>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-175 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Question Answering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09040v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09040v1.pdf filename=2403.09040v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Retrieval-augmented</b> <b>generation</b> <b>(RAG)</b> greatly benefits language models (LMs) by providing additional context for tasks such as document-based <b>question</b> <b>answering</b> (DBQA). Despite its potential, the power of <b>RAG</b> is highly dependent on its configuration, raising the <b>question:</b> <b>What</b> is the optimal <b>RAG</b> configuration? To answer this, we introduce the RAGGED framework to analyze and optimize <b>RAG</b> systems. On a set of representative DBQA tasks, we study two classic sparse and dense retrievers, and four top-performing LMs in encoder-decoder and decoder-only architectures. Through RAGGED, we uncover that different models suit substantially varied <b>RAG</b> setups. While encoder-decoder models monotonically improve with more documents, we find decoder-only models can only effectively use &lt; 5 documents, despite often having a longer context window. RAGGED offers further insights into LMs&rsquo; context utilization habits, where we find that encoder-decoder models rely more on contexts and are thus more sensitive to <b>retrieval</b> <b>quality,</b> <b>while</b> decoder-only models tend to rely on knowledge memorized during training.</p></p class="citation"></blockquote><h3 id=3042--176320-hyper-cl-conditioning-sentence-representations-with-hypernetworks-young-hyun-yoo-et-al-2024>(30/42 | 176/320) Hyper-CL: Conditioning Sentence Representations with Hypernetworks (Young Hyun Yoo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Young Hyun Yoo, Jii Cha, Changhyeon Kim, Taeuk Kim. (2024)<br><strong>Hyper-CL: Conditioning Sentence Representations with Hypernetworks</strong><br><button class=copy-to-clipboard title="Hyper-CL: Conditioning Sentence Representations with Hypernetworks" index=176>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-176 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 36<br>Keywords: Graph, Benchmarking, Contrastive Learning, Knowledge Graph, Representation Learning, Sentence Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09490v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09490v1.pdf filename=2403.09490v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While the introduction of <b>contrastive</b> <b>learning</b> frameworks in <b>sentence</b> <b>representation</b> <b>learning</b> has significantly contributed to advancements in the field, it still remains unclear whether state-of-the-art <b>sentence</b> <b>embeddings</b> can capture the fine-grained semantics of <b>sentences,</b> <b>particularly</b> when conditioned on specific perspectives. In this paper, we introduce Hyper-CL, an efficient methodology that integrates hypernetworks with <b>contrastive</b> <b>learning</b> to compute conditioned <b>sentence</b> <b>representations.</b> <b>In</b> our proposed approach, the hypernetwork is responsible for transforming pre-computed condition embeddings into corresponding projection layers. This enables the same <b>sentence</b> <b>embeddings</b> to be projected differently according to various conditions. Evaluation on two representative conditioning <b>benchmarks,</b> namely conditional semantic text similarity and <b>knowledge</b> <b>graph</b> completion, demonstrates that Hyper-CL is effective in flexibly conditioning <b>sentence</b> <b>representations,</b> <b>showcasing</b> its computational efficiency at the same time. We also provide a comprehensive analysis of the inner workings of our approach, leading to a better interpretation of its mechanisms.</p></p class="citation"></blockquote><h3 id=3142--177320-self-consistency-boosts-calibration-for-math-reasoning-ante-wang-et-al-2024>(31/42 | 177/320) Self-Consistency Boosts Calibration for Math Reasoning (Ante Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ante Wang, Linfeng Song, Ye Tian, Baolin Peng, Lifeng Jin, Haitao Mi, Jinsong Su, Dong Yu. (2024)<br><strong>Self-Consistency Boosts Calibration for Math Reasoning</strong><br><button class=copy-to-clipboard title="Self-Consistency Boosts Calibration for Math Reasoning" index=177>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-177 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, Mistral, Reasoning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09849v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09849v1.pdf filename=2403.09849v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Calibration, which establishes the correlation between accuracy and model confidence, is important for <b>LLM</b> development. We design three off-the-shelf calibration methods based on self-consistency (Wang et al., 2022) for math <b>reasoning</b> tasks. Evaluation on two popular <b>benchmarks</b> (GSM8K and MathQA) using strong open-source <b>LLMs</b> <b>(Mistral</b> and LLaMA2), our methods better bridge model confidence and accuracy than existing methods based on p(True) (Kadavath et al., 2022) or logit (Kadavath et al., 2022).</p></p class="citation"></blockquote><h3 id=3242--178320-mcfend-a-multi-source-benchmark-dataset-for-chinese-fake-news-detection-yupeng-li-et-al-2024>(32/42 | 178/320) MCFEND: A Multi-source Benchmark Dataset for Chinese Fake News Detection (Yupeng Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yupeng Li, Haorui He, Jin Bai, Dacheng Wen. (2024)<br><strong>MCFEND: A Multi-source Benchmark Dataset for Chinese Fake News Detection</strong><br><button class=copy-to-clipboard title="MCFEND: A Multi-source Benchmark Dataset for Chinese Fake News Detection" index=178>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-178 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, Fact Verification, Fake News Detection, Fake News Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09092v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09092v1.pdf filename=2403.09092v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The prevalence of <b>fake</b> <b>news</b> <b>across</b> various online sources has had a significant influence on the public. Existing Chinese <b>fake</b> <b>news</b> <b>detection</b> datasets are limited to news sourced solely from Weibo. However, <b>fake</b> <b>news</b> <b>originating</b> from multiple sources exhibits diversity in various aspects, including its content and social context. Methods trained on purely one single news source can hardly be applicable to real-world scenarios. Our pilot experiment demonstrates that the F1 score of the state-of-the-art method that learns from a large Chinese <b>fake</b> <b>news</b> <b>detection</b> dataset, Weibo-21, drops significantly from 0.943 to 0.470 when the test data is changed to multi-source news data, failing to identify more than one-third of the multi-source <b>fake</b> <b>news.</b> <b>To</b> address this limitation, we constructed the first multi-source <b>benchmark</b> dataset for Chinese <b>fake</b> <b>news</b> <b>detection,</b> termed MCFEND, which is composed of news we collected from diverse sources such as social platforms, messaging apps, and traditional online news outlets. Notably, such news has been <b>fact-checked</b> <b>by</b> 14 authoritative <b>fact-checking</b> <b>agencies</b> worldwide. In addition, various existing Chinese <b>fake</b> <b>news</b> <b>detection</b> methods are thoroughly evaluated on our proposed dataset in cross-source, multi-source, and unseen source ways. MCFEND, as a <b>benchmark</b> dataset, aims to advance Chinese <b>fake</b> <b>news</b> <b>detection</b> approaches in real-world scenarios.</p></p class="citation"></blockquote><h3 id=3342--179320-transformers-get-stable-an-end-to-end-signal-propagation-theory-for-language-models-akhil-kedia-et-al-2024>(33/42 | 179/320) Transformers Get Stable: An End-to-End Signal Propagation Theory for Language Models (Akhil Kedia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Akhil Kedia, Mohd Abbas Zaidi, Sushil Khyalia, Jungho Jung, Harshith Goka, Haejun Lee. (2024)<br><strong>Transformers Get Stable: An End-to-End Signal Propagation Theory for Language Models</strong><br><button class=copy-to-clipboard title="Transformers Get Stable: An End-to-End Signal Propagation Theory for Language Models" index=179>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-179 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: I-2-7; I-2-10, cs-AI, cs-CL, cs-CV, cs-LG, cs.CL<br>Keyword Score: 30<br>Keywords: Transformer, Question Answering, Speech-to-Speech Translation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09635v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09635v1.pdf filename=2403.09635v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In spite of their huge success, <b>transformer</b> models remain difficult to scale in depth. In this work, we develop a unified signal propagation theory and provide formulae that govern the moments of the forward and backward signal through the <b>transformer</b> model. Our framework can be used to understand and mitigate vanishing/exploding gradients, rank collapse, and instability associated with high attention scores. We also propose DeepScaleLM, an initialization and scaling scheme that conserves unit output/gradient moments throughout the model, enabling the training of very deep models with 100s of layers. We find that <b>transformer</b> models could be much deeper - our deep models with fewer parameters outperform shallow models in Language Modeling, <b>Speech</b> <b>Translation,</b> and Image Classification, across Encoder-only, Decoder-only and Encoder-Decoder variants, for both Pre-LN and Post-LN <b>transformers,</b> for multiple datasets and model sizes. These improvements also translate into improved performance on downstream <b>Question</b> <b>Answering</b> tasks and improved robustness for image classification.</p></p class="citation"></blockquote><h3 id=3442--180320-caveat-lector-large-language-models-in-legal-practice-eliza-mik-2024>(34/42 | 180/320) Caveat Lector: Large Language Models in Legal Practice (Eliza Mik, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Eliza Mik. (2024)<br><strong>Caveat Lector: Large Language Models in Legal Practice</strong><br><button class=copy-to-clipboard title="Caveat Lector: Large Language Models in Legal Practice" index=180>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-180 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CY, cs.CL<br>Keyword Score: 30<br>Keywords: Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09163v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09163v1.pdf filename=2403.09163v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The current fascination with <b>large</b> <b>language</b> <b>models,</b> or <b>LLMs,</b> derives from the fact that many users lack the expertise to evaluate the quality of the generated text. <b>LLMs</b> may therefore appear more capable than they actually are. The dangerous combination of fluency and superficial plausibility leads to the temptation to trust the generated text and creates the risk of overreliance. Who would not trust perfect legalese? Relying recent findings in both technical and legal scholarship, this Article counterbalances the overly optimistic predictions as to the role of <b>LLMs</b> in legal practice. Integrating <b>LLMs</b> into legal workstreams without a better comprehension of their limitations, will create inefficiencies if not outright risks. Notwithstanding their unprecedented ability to generate text, <b>LLMs</b> do not understand text. Without the ability to understand meaning, <b>LLMs</b> will remain unable to use language, to acquire knowledge and to perform complex <b>reasoning</b> tasks. Trained to model language on the basis of stochastic word predictions, <b>LLMs</b> cannot distinguish fact from fiction. Their knowledge of the law is limited to word strings memorized in their parameters. It is also incomplete and largely incorrect. <b>LLMs</b> operate at the level of word distributions, not at the level of verified facts. The resulting propensity to hallucinate, to produce statements that are incorrect but appear helpful and relevant, is alarming in high-risk areas like legal services. At present, lawyers should beware of relying on text generated by <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=3542--181320-meta-cognitive-analysis-evaluating-declarative-and-procedural-knowledge-in-datasets-and-large-language-models-zhuoqun-li-et-al-2024>(35/42 | 181/320) Meta-Cognitive Analysis: Evaluating Declarative and Procedural Knowledge in Datasets and Large Language Models (Zhuoqun Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhuoqun Li, Hongyu Lin, Yaojie Lu, Hao Xiang, Xianpei Han, Le Sun. (2024)<br><strong>Meta-Cognitive Analysis: Evaluating Declarative and Procedural Knowledge in Datasets and Large Language Models</strong><br><button class=copy-to-clipboard title="Meta-Cognitive Analysis: Evaluating Declarative and Procedural Knowledge in Datasets and Large Language Models" index=181>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-181 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09750v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09750v1.pdf filename=2403.09750v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Declarative knowledge and procedural knowledge are two key parts in meta-cognitive theory, and these two hold significant importance in pre-training and inference of <b>LLMs.</b> However, a comprehensive analysis comparing these two types of knowledge is lacking, primarily due to challenges in definition, probing and quantitative assessment. In this paper, we explore from a new perspective by providing ground-truth knowledge for <b>LLMs</b> and evaluating the effective score. Through extensive experiments with widely-used datasets and models, we get conclusions: (1) In most tasks, benefits from declarative knowledge are greater than those from procedural knowledge. (2) Profits of procedural knowledge are larger than declarative knowledge only in <b>reasoning</b> tasks with simple logic. (3) As pre-training progresses and size increases, model ability to utilize both kinds of knowledge significantly improves, but in different speed. We do detailed analysis for the findings and this can provide primary guidance for evaluation and enhancement of <b>large</b> <b>language</b> <b>models.</b></p></p class="citation"></blockquote><h3 id=3642--182320-meaningful-learning-advancing-abstract-reasoning-in-large-language-models-via-generic-fact-guidance-kai-xiong-et-al-2024>(36/42 | 182/320) Meaningful Learning: Advancing Abstract Reasoning in Large Language Models via Generic Fact Guidance (Kai Xiong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kai Xiong, Xiao Ding, Ting Liu, Bing Qin, Dongliang Xu, Qing Yang, Hongtao Liu, Yixin Cao. (2024)<br><strong>Meaningful Learning: Advancing Abstract Reasoning in Large Language Models via Generic Fact Guidance</strong><br><button class=copy-to-clipboard title="Meaningful Learning: Advancing Abstract Reasoning in Large Language Models via Generic Fact Guidance" index=182>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-182 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09085v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09085v1.pdf filename=2403.09085v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have developed impressive performance and strong explainability across various <b>reasoning</b> scenarios, marking a significant stride towards mimicking human-like intelligence. Despite this, when tasked with simple questions supported by a generic fact, <b>LLMs</b> often fail to provide consistent and precise answers, indicating a deficiency in abstract <b>reasoning</b> abilities. This has sparked a vigorous debate about whether <b>LLMs</b> are genuinely <b>reasoning</b> or merely memorizing. In light of this, we design a preliminary study to quantify and delve into the abstract <b>reasoning</b> abilities of existing <b>LLMs.</b> Our findings reveal a substantial discrepancy between their general <b>reasoning</b> and abstract <b>reasoning</b> performances. To relieve this problem, we tailor an abstract <b>reasoning</b> dataset (AbsR) together with a meaningful learning paradigm to teach <b>LLMs</b> how to leverage generic facts for <b>reasoning</b> purposes. The results show that our approach not only boosts the general <b>reasoning</b> performance of <b>LLMs</b> but also makes considerable strides towards their capacity for abstract <b>reasoning,</b> moving beyond simple memorization or imitation to a more nuanced understanding and application of generic facts.</p></p class="citation"></blockquote><h3 id=3742--183320-a-continued-pretrained-llm-approach-for-automatic-medical-note-generation-dong-yuan-et-al-2024>(37/42 | 183/320) A Continued Pretrained LLM Approach for Automatic Medical Note Generation (Dong Yuan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dong Yuan, Eti Rastogi, Gautam Naik, Jai Chintagunta, Sree Prasanna Rajagopal, Fen Zhao, Sagar Goyal, Jeff Ward. (2024)<br><strong>A Continued Pretrained LLM Approach for Automatic Medical Note Generation</strong><br><button class=copy-to-clipboard title="A Continued Pretrained LLM Approach for Automatic Medical Note Generation" index=183>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-183 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: GPT, GPT-4, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09057v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09057v1.pdf filename=2403.09057v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>LLMs</b> are revolutionizing NLP tasks. However, the most powerful <b>LLM,</b> like <b>GPT-4,</b> is too costly for most domain-specific scenarios. We present the first continuously trained 13B Llama2-based <b>LLM</b> that is purpose-built for medical conversations and measured on automated scribing. Our results show that our model outperforms <b>GPT-4</b> in PubMedQA with 76.6% accuracy and matches its performance in summarizing medical conversations into SOAP notes. Notably, our model exceeds <b>GPT-4</b> in capturing a higher number of correct medical concepts and outperforms human scribes with higher correctness and completeness.</p></p class="citation"></blockquote><h3 id=3842--184320-recurrent-drafter-for-fast-speculative-decoding-in-large-language-models-aonan-zhang-et-al-2024>(38/42 | 184/320) Recurrent Drafter for Fast Speculative Decoding in Large Language Models (Aonan Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aonan Zhang, Chong Wang, Yi Wang, Xuanyu Zhang, Yunfei Cheng. (2024)<br><strong>Recurrent Drafter for Fast Speculative Decoding in Large Language Models</strong><br><button class=copy-to-clipboard title="Recurrent Drafter for Fast Speculative Decoding in Large Language Models" index=184>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-184 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 20<br>Keywords: Transformer, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09919v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09919v1.pdf filename=2403.09919v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we introduce an improved approach of speculative decoding aimed at enhancing the efficiency of serving <b>large</b> <b>language</b> <b>models.</b> Our method capitalizes on the strengths of two established techniques: the classic two-model speculative decoding approach, and the more recent single-model approach, Medusa. Drawing inspiration from Medusa, our approach adopts a single-model strategy for speculative decoding. However, our method distinguishes itself by employing a single, lightweight draft head with a recurrent dependency design, akin in essence to the small, draft model uses in classic speculative decoding, but without the complexities of the full <b>transformer</b> architecture. And because of the recurrent dependency, we can use beam search to swiftly filter out undesired candidates with the draft head. The outcome is a method that combines the simplicity of single-model design and avoids the need to create a data-dependent tree attention structure only for inference in Medusa. We empirically demonstrate the effectiveness of the proposed method on several popular open source language models, along with a comprehensive analysis of the trade-offs involved in adopting this approach.</p></p class="citation"></blockquote><h3 id=3942--185320-fakewatch-a-framework-for-detecting-fake-news-to-ensure-credible-elections-shaina-raza-et-al-2024>(39/42 | 185/320) FakeWatch: A Framework for Detecting Fake News to Ensure Credible Elections (Shaina Raza et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shaina Raza, Tahniat Khan, Drai Paulen-Patterson, Veronica Chatrath, Mizanur Rahman, Oluwanifemi Bamgbose. (2024)<br><strong>FakeWatch: A Framework for Detecting Fake News to Ensure Credible Elections</strong><br><button class=copy-to-clipboard title="FakeWatch: A Framework for Detecting Fake News to Ensure Credible Elections" index=185>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-185 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Fake News Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09858v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09858v1.pdf filename=2403.09858v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In today&rsquo;s technologically driven world, the rapid spread of <b>fake</b> <b>news,</b> particularly during critical events like elections, poses a growing threat to the integrity of information. To tackle this challenge head-on, we introduce FakeWatch, a comprehensive framework carefully designed to detect <b>fake</b> <b>news.</b> Leveraging a newly curated dataset of North American election-related news articles, we construct robust classification models. Our framework integrates a model hub comprising of both traditional machine learning (ML) techniques and cutting-edge Language Models (LMs) to discern <b>fake</b> <b>news</b> effectively. Our overarching objective is to provide the research community with adaptable and precise classification models adept at identifying the ever-evolving landscape of misinformation. Quantitative evaluations of <b>fake</b> <b>news</b> classifiers on our dataset reveal that, while state-of-the-art LMs exhibit a slight edge over traditional ML models, classical models remain competitive due to their balance of accuracy and computational efficiency. Additionally, qualitative analyses shed light on patterns within <b>fake</b> <b>news</b> articles. This research lays the groundwork for future endeavors aimed at combating misinformation, particularly concerning electoral processes. We provide our labeled data and model publicly for use and reproducibility.</p></p class="citation"></blockquote><h3 id=4042--186320-leveraging-prototypical-representations-for-mitigating-social-bias-without-demographic-information-shadi-iskander-et-al-2024>(40/42 | 186/320) Leveraging Prototypical Representations for Mitigating Social Bias without Demographic Information (Shadi Iskander et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shadi Iskander, Kira Radinsky, Yonatan Belinkov. (2024)<br><strong>Leveraging Prototypical Representations for Mitigating Social Bias without Demographic Information</strong><br><button class=copy-to-clipboard title="Leveraging Prototypical Representations for Mitigating Social Bias without Demographic Information" index=186>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-186 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CY, cs-LG, cs.CL<br>Keyword Score: 10<br>Keywords: Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09516v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09516v1.pdf filename=2403.09516v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Mitigating social biases typically requires identifying the social groups associated with each data sample. In this paper, we present DAFair, a novel approach to address social bias in language models. Unlike traditional methods that rely on explicit demographic labels, our approach does not require any such information. Instead, we leverage predefined prototypical demographic texts and incorporate a regularization term during the <b>fine-tuning</b> process to mitigate bias in the model&rsquo;s representations. Our empirical results across two tasks and two models demonstrate the effectiveness of our method compared to previous approaches that do not rely on labeled data. Moreover, with limited demographic-annotated data, our approach outperforms common debiasing approaches.</p></p class="citation"></blockquote><h3 id=4142--187320-emotional-intelligence-through-artificial-intelligence--nlp-and-deep-learning-in-the-analysis-of-healthcare-texts-prashant-kumar-nag-et-al-2024>(41/42 | 187/320) Emotional Intelligence Through Artificial Intelligence : NLP and Deep Learning in the Analysis of Healthcare Texts (Prashant Kumar Nag et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Prashant Kumar Nag, Amit Bhagat, R. Vishnu Priya, Deepak kumar Khare. (2024)<br><strong>Emotional Intelligence Through Artificial Intelligence : NLP and Deep Learning in the Analysis of Healthcare Texts</strong><br><button class=copy-to-clipboard title="Emotional Intelligence Through Artificial Intelligence : NLP and Deep Learning in the Analysis of Healthcare Texts" index=187>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-187 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-HC, cs-LG, cs-NE, cs.CL<br>Keyword Score: 10<br>Keywords: Sentiment Analysis<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09762v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09762v1.pdf filename=2403.09762v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This manuscript presents a methodical examination of the utilization of Artificial Intelligence in the assessment of emotions in texts related to healthcare, with a particular focus on the incorporation of Natural Language Processing and deep learning technologies. We scrutinize numerous research studies that employ AI to augment <b>sentiment</b> <b>analysis,</b> categorize emotions, and forecast patient outcomes based on textual information derived from clinical narratives, patient feedback on medications, and online health discussions. The review demonstrates noteworthy progress in the precision of algorithms used for <b>sentiment</b> <b>classification,</b> the prognostic capabilities of AI models for neurodegenerative diseases, and the creation of AI-powered systems that offer support in clinical decision-making. Remarkably, the utilization of AI applications has exhibited an enhancement in personalized therapy plans by integrating patient <b>sentiment</b> <b>and</b> contributing to the early identification of mental health disorders. There persist challenges, which encompass ensuring the ethical application of AI, safeguarding patient confidentiality, and addressing potential biases in algorithmic procedures. Nevertheless, the potential of AI to revolutionize healthcare practices is unmistakable, offering a future where healthcare is not only more knowledgeable and efficient but also more empathetic and centered around the needs of patients. This investigation underscores the transformative influence of AI on healthcare, delivering a comprehensive comprehension of its role in examining emotional content in healthcare texts and highlighting the trajectory towards a more compassionate approach to patient care. The findings advocate for a harmonious synergy between AI&rsquo;s analytical capabilities and the human aspects of healthcare.</p></p class="citation"></blockquote><h3 id=4242--188320-geographically-informed-language-identification-jonathan-dunn-et-al-2024>(42/42 | 188/320) Geographically-Informed Language Identification (Jonathan Dunn et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jonathan Dunn, Lane Edwards-Brown. (2024)<br><strong>Geographically-Informed Language Identification</strong><br><button class=copy-to-clipboard title="Geographically-Informed Language Identification" index=188>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-188 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 3<br>Keywords: Sample Size<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09892v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09892v1.pdf filename=2403.09892v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper develops an approach to language identification in which the set of languages considered by the model depends on the geographic origin of the text in question. Given that many digital corpora can be geo-referenced at the country level, this paper formulates 16 region-specific models, each of which contains the languages expected to appear in countries within that region. These regional models also each include 31 widely-spoken international languages in order to ensure coverage of these linguae francae regardless of location. An upstream evaluation using traditional language identification testing data shows an improvement in f-score ranging from 1.7 points (Southeast Asia) to as much as 10.4 points (North Africa). A downstream evaluation on social media data shows that this improved performance has a significant impact on the language labels which are applied to large real-world corpora. The result is a highly-accurate model that covers 916 languages at a <b>sample</b> <b>size</b> of 50 characters, the performance improved by incorporating geographic information into the model.</p></p class="citation"></blockquote><h2 id=cond-matmtrl-sci-1>cond-mat.mtrl-sci (1)</h2><h3 id=11--189320-adapting-oc20-trained-equiformerv2-models-for-high-entropy-materials-christian-m-clausen-et-al-2024>(1/1 | 189/320) Adapting OC20-trained EquiformerV2 Models for High-Entropy Materials (Christian M. Clausen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Christian M. Clausen, Jan Rossmeisl, Zachary W. Ulissi. (2024)<br><strong>Adapting OC20-trained EquiformerV2 Models for High-Entropy Materials</strong><br><button class=copy-to-clipboard title="Adapting OC20-trained EquiformerV2 Models for High-Entropy Materials" index=189>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-189 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cond-mat.mtrl-sci<br>Categories: cond-mat-mtrl-sci, cond-mat.mtrl-sci, cs-LG, physics-chem-ph<br>Keyword Score: 80<br>Keywords: Few-shot, Fine-tuning, Knowledge Distillation, Knowledge Distillation, Out-of-domain, Simulation, Simulator, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09811v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09811v1.pdf filename=2403.09811v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Computational high-throughput studies, especially in research on high-entropy materials and catalysts, are hampered by high-dimensional composition spaces and myriad structural microstates. They present bottlenecks to the conventional use of density functional theory calculations, and consequently, the use of machine-learned potentials is becoming increasingly prevalent in atomic structure <b>simulations.</b> In this communication, we show the results of adjusting and <b>fine-tuning</b> the pretrained EquiformerV2 model from the Open Catalyst Project to infer adsorption energies of *OH and *O on the <b>out-of-domain</b> high-entropy alloy Ag-Ir-Pd-Pt-Ru. By applying an energy filter based on the local environment of the binding site the <b>zero-shot</b> inference is markedly improved and through <b>few-shot</b> <b>fine-tuning</b> the model yields state-of-the-art accuracy. It is also found that EquiformerV2, assuming the role of general machine learning potential, is able to inform a smaller, more focused direct inference model. This <b>knowledge</b> <b>distillation</b> setup boosts performance on complex binding sites. Collectively, this shows that foundational <b>knowledge</b> <b>learned</b> from ordered intermetallic structures, can be extrapolated to the highly disordered structures of solid-solutions. With the vastly accelerated computational throughput of these models, hitherto infeasible research in the high-entropy material space is now readily accessible.</p></p class="citation"></blockquote><h2 id=csro-16>cs.RO (16)</h2><h3 id=116--190320-explorllm-guiding-exploration-in-reinforcement-learning-with-large-language-models-runyu-ma-et-al-2024>(1/16 | 190/320) ExploRLLM: Guiding Exploration in Reinforcement Learning with Large Language Models (Runyu Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Runyu Ma, Jelle Luijkx, Zlatan Ajanovic, Jens Kober. (2024)<br><strong>ExploRLLM: Guiding Exploration in Reinforcement Learning with Large Language Models</strong><br><button class=copy-to-clipboard title="ExploRLLM: Guiding Exploration in Reinforcement Learning with Large Language Models" index=190>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-190 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 80<br>Keywords: Few-shot, Foundation Model, Reinforcement Learning, Simulation, Simulator, Zero-shot, Reasoning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09583v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09583v2.pdf filename=2403.09583v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In image-based robot manipulation tasks with <b>large</b> <b>observation</b> <b>and</b> action spaces, <b>reinforcement</b> <b>learning</b> struggles with low sample efficiency, slow training speed, and uncertain convergence. As an alternative, <b>large</b> <b>pre-trained</b> <b>foundation</b> <b>models</b> have shown promise in robotic manipulation, particularly in <b>zero-shot</b> and <b>few-shot</b> applications. However, using these models directly is unreliable due to limited <b>reasoning</b> capabilities and challenges in understanding physical and spatial contexts. This paper introduces ExploRLLM, a novel approach that leverages the inductive bias of <b>foundation</b> <b>models</b> (e.g. <b>Large</b> <b>Language</b> <b>Models)</b> to guide exploration in <b>reinforcement</b> <b>learning.</b> We also exploit these <b>foundation</b> <b>models</b> to reformulate the action and observation spaces to enhance the training efficiency in <b>reinforcement</b> <b>learning.</b> Our experiments demonstrate that guided exploration enables much quicker convergence than training without it. Additionally, we validate that ExploRLLM outperforms vanilla <b>foundation</b> <b>model</b> baselines and that the policy trained in <b>simulation</b> can be applied in real-world settings without additional training.</p></p class="citation"></blockquote><h3 id=216--191320-paperbot-learning-to-design-real-world-tools-using-paper-ruoshi-liu-et-al-2024>(2/16 | 191/320) PaperBot: Learning to Design Real-World Tools Using Paper (Ruoshi Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruoshi Liu, Junbang Liang, Sruthi Sudhakar, Huy Ha, Cheng Chi, Shuran Song, Carl Vondrick. (2024)<br><strong>PaperBot: Learning to Design Real-World Tools Using Paper</strong><br><button class=copy-to-clipboard title="PaperBot: Learning to Design Real-World Tools Using Paper" index=191>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-191 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 50<br>Keywords: Human Intervention, Self-supervised Learning, Self-supervised Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09566v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09566v1.pdf filename=2403.09566v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Paper is a cheap, recyclable, and clean material that is often used to make practical tools. Traditional tool design either relies on <b>simulation</b> or physical analysis, which is often inaccurate and time-consuming. In this paper, we propose PaperBot, an approach that directly learns to design and use a tool in the real world using paper without <b>human</b> <b>intervention.</b> We demonstrated the effectiveness and efficiency of PaperBot on two tool design tasks: 1. learning to fold and throw paper airplanes for maximum travel distance 2. learning to cut paper into grippers that exert maximum gripping force. We present a <b>self-supervised</b> <b>learning</b> framework that learns to perform a sequence of folding, cutting, and dynamic manipulation actions in order to optimize the design and use of a tool. We deploy our system to a real-world two-arm robotic system to solve challenging design tasks that involve aerodynamics (paper airplane) and friction (paper gripper) that are impossible to simulate accurately.</p></p class="citation"></blockquote><h3 id=316--192320-gaussiangrasper-3d-language-gaussian-splatting-for-open-vocabulary-robotic-grasping-yuhang-zheng-et-al-2024>(3/16 | 192/320) GaussianGrasper: 3D Language Gaussian Splatting for Open-vocabulary Robotic Grasping (Yuhang Zheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuhang Zheng, Xiangyu Chen, Yupeng Zheng, Songen Gu, Runyi Yang, Bu Jin, Pengfei Li, Chengliang Zhong, Zengmao Wang, Lina Liu, Chao Yang, Dawei Wang, Zhen Chen, Xiaoxiao Long, Meiqing Wang. (2024)<br><strong>GaussianGrasper: 3D Language Gaussian Splatting for Open-vocabulary Robotic Grasping</strong><br><button class=copy-to-clipboard title="GaussianGrasper: 3D Language Gaussian Splatting for Open-vocabulary Robotic Grasping" index=192>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-192 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-CV, cs-RO, cs.RO<br>Keyword Score: 35<br>Keywords: Contrastive Learning, Geometry, Knowledge Distillation, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09637v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09637v1.pdf filename=2403.09637v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Constructing a 3D scene capable of accommodating open-ended language queries, is a pivotal pursuit, particularly within the domain of robotics. Such technology facilitates robots in executing object manipulations based on human language directives. To tackle this challenge, some research efforts have been dedicated to the development of language-embedded implicit fields. However, implicit fields (e.g. NeRF) encounter limitations due to the necessity of processing a large number of input views for reconstruction, coupled with their inherent inefficiencies in inference. Thus, we present the GaussianGrasper, which utilizes 3D Gaussian Splatting to explicitly represent the scene as a collection of Gaussian primitives. Our approach takes a limited set of RGB-D views and employs a tile-based splatting technique to create a feature field. In particular, we propose an Efficient Feature <b>Distillation</b> (EFD) module that employs <b>contrastive</b> <b>learning</b> to efficiently and accurately <b>distill</b> language embeddings derived from foundational models. With the reconstructed <b>geometry</b> of the Gaussian field, our method enables the pre-trained grasping model to generate collision-free grasp pose candidates. Furthermore, we propose a normal-guided grasp module to select the best grasp pose. Through comprehensive real-world experiments, we demonstrate that GaussianGrasper enables robots to accurately query and grasp objects with language instructions, providing a new solution for language-guided manipulation tasks. Data and codes can be available at <a href=https://github.com/MrSecant/GaussianGrasper>https://github.com/MrSecant/GaussianGrasper</a>.</p></p class="citation"></blockquote><h3 id=416--193320-enhancing-trust-in-autonomous-agents-an-architecture-for-accountability-and-explainability-through-blockchain-and-large-language-models-laura-fernández-becerra-et-al-2024>(4/16 | 193/320) Enhancing Trust in Autonomous Agents: An Architecture for Accountability and Explainability through Blockchain and Large Language Models (Laura Fernández-Becerra et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Laura Fernández-Becerra, Miguel Ángel González-Santamarta, Ángel Manuel Guerrero-Higueras, Francisco Javier Rodríguez-Lera, Vicente Matellán Olivera. (2024)<br><strong>Enhancing Trust in Autonomous Agents: An Architecture for Accountability and Explainability through Blockchain and Large Language Models</strong><br><button class=copy-to-clipboard title="Enhancing Trust in Autonomous Agents: An Architecture for Accountability and Explainability through Blockchain and Large Language Models" index=193>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-193 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs.RO<br>Keyword Score: 35<br>Keywords: Black Box, Natural Language Explanation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09567v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09567v1.pdf filename=2403.09567v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The deployment of autonomous agents in environments involving human interaction has increasingly raised security concerns. Consequently, understanding the circumstances behind an event becomes critical, requiring the development of capabilities to justify their behaviors to non-expert users. Such explanations are essential in enhancing trustworthiness and safety, acting as a preventive measure against failures, errors, and misunderstandings. Additionally, they contribute to improving communication, bridging the gap between the agent and the user, thereby improving the effectiveness of their interactions. This work presents an accountability and explainability architecture implemented for ROS-based mobile robots. The proposed solution consists of two main components. Firstly, a <b>black</b> <b>box-like</b> element to provide accountability, featuring anti-tampering properties achieved through blockchain technology. Secondly, a component in charge of generating <b>natural</b> <b>language</b> <b>explanations</b> by harnessing the capabilities of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> over the data contained within the previously mentioned <b>black</b> <b>box.</b> The study evaluates the performance of our solution in three different scenarios, each involving autonomous agent navigation functionalities. This evaluation includes a thorough examination of accountability and explainability metrics, demonstrating the effectiveness of our approach in using accountable data from robot actions to obtain coherent, accurate and understandable explanations, even when facing challenges inherent in the use of autonomous agents in real-world scenarios.</p></p class="citation"></blockquote><h3 id=516--194320-socially-integrated-navigation-a-social-acting-robot-with-deep-reinforcement-learning-daniel-flögel-et-al-2024>(5/16 | 194/320) Socially Integrated Navigation: A Social Acting Robot with Deep Reinforcement Learning (Daniel Flögel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daniel Flögel, Lars Fischer, Thomas Rudolf, Tobias Schürmann, Sören Hohmann. (2024)<br><strong>Socially Integrated Navigation: A Social Acting Robot with Deep Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Socially Integrated Navigation: A Social Acting Robot with Deep Reinforcement Learning" index=194>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-194 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-LG, cs-RO, cs-SY, cs.RO, eess-SY<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09793v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09793v1.pdf filename=2403.09793v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Mobile robots are being used on a large scale in various crowded situations and become part of our society. The socially acceptable navigation behavior of a mobile robot with individual human consideration is an essential requirement for scalable applications and human acceptance. Deep <b>Reinforcement</b> <b>Learning</b> (DRL) approaches are recently used to learn a robot&rsquo;s navigation policy and to model the complex interactions between robots and humans. We propose to divide existing DRL-based navigation approaches based on the robot&rsquo;s exhibited social behavior and distinguish between social collision avoidance with a lack of social behavior and socially aware approaches with explicit predefined social behavior. In addition, we propose a novel socially integrated navigation approach where the robot&rsquo;s social behavior is adaptive and emerges from the interaction with humans. The formulation of our approach is derived from a sociological definition, which states that social acting is oriented toward the acting of others. The DRL policy is trained in an environment where other agents interact socially integrated and reward the robot&rsquo;s behavior individually. The <b>simulation</b> results indicate that the proposed socially integrated navigation approach outperforms a socially aware approach in terms of distance traveled, time to completion, and negative impact on all agents within the environment.</p></p class="citation"></blockquote><h3 id=616--195320-right-place-right-time-towards-objectnav-for-non-stationary-goals-vishnu-sashank-dorbala-et-al-2024>(6/16 | 195/320) Right Place, Right Time! Towards ObjectNav for Non-Stationary Goals (Vishnu Sashank Dorbala et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vishnu Sashank Dorbala, Bhrij Patel, Amrit Singh Bedi, Dinesh Manocha. (2024)<br><strong>Right Place, Right Time! Towards ObjectNav for Non-Stationary Goals</strong><br><button class=copy-to-clipboard title="Right Place, Right Time! Towards ObjectNav for Non-Stationary Goals" index=195>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-195 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-CV, cs-RO, cs.RO<br>Keyword Score: 23<br>Keywords: Benchmarking, Grounding, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09905v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09905v1.pdf filename=2403.09905v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a novel approach to tackle the ObjectNav task for non-stationary and potentially occluded targets in an indoor environment. We refer to this task Portable ObjectNav (or P-ObjectNav), and in this work, present its formulation, feasibility, and a navigation <b>benchmark</b> using a novel memory-enhanced <b>LLM-based</b> policy. In contrast to ObjNav where target object locations are fixed for each episode, P-ObjectNav tackles the challenging case where the target objects move during the episode. This adds a layer of time-sensitivity to navigation, and is particularly relevant in scenarios where the agent needs to find portable targets (e.g. misplaced wallets) in human-centric environments. The agent needs to estimate not just the correct location of the target, but also the time at which the target is at that location for visual <b>grounding</b> &ndash; raising the question about the feasibility of the task. We address this concern by inferring results on two cases for object placement: one where the objects placed follow a routine or a path, and the other where they are placed at random. We dynamize Matterport3D for these experiments, and modify PPO and <b>LLM-based</b> navigation policies for evaluation. Using PPO, we observe that agent performance in the random case stagnates, while the agent in the routine-following environment continues to improve, allowing us to infer that P-ObjectNav is solvable in environments with routine-following object placement. Using memory-enhancement on an <b>LLM-based</b> policy, we set a <b>benchmark</b> for P-ObjectNav. Our memory-enhanced agent significantly outperforms their non-memory-based counterparts across object placement scenarios by 71.76% and 74.68% on average when measured by Success Rate (SR) and Success Rate weighted by Path Length (SRPL), showing the influence of memory on improving P-ObjectNav performance. Our code and dataset will be made publicly available.</p></p class="citation"></blockquote><h3 id=716--196320-behavior-1k-a-human-centered-embodied-ai-benchmark-with-1000-everyday-activities-and-realistic-simulation-chengshu-li-et-al-2024>(7/16 | 196/320) BEHAVIOR-1K: A Human-Centered, Embodied AI Benchmark with 1,000 Everyday Activities and Realistic Simulation (Chengshu Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chengshu Li, Ruohan Zhang, Josiah Wong, Cem Gokmen, Sanjana Srivastava, Roberto Martín-Martín, Chen Wang, Gabrael Levine, Wensi Ai, Benjamin Martinez, Hang Yin, Michael Lingelbach, Minjune Hwang, Ayano Hiranaka, Sujay Garlanka, Arman Aydin, Sharon Lee, Jiankai Sun, Mona Anvari, Manasi Sharma, Dhruva Bansal, Samuel Hunter, Kyu-Young Kim, Alan Lou, Caleb R Matthews, Ivan Villa-Renteria, Jerry Huayang Tang, Claire Tang, Fei Xia, Yunzhu Li, Silvio Savarese, Hyowon Gweon, C. Karen Liu, Jiajun Wu, Li Fei-Fei. (2024)<br><strong>BEHAVIOR-1K: A Human-Centered, Embodied AI Benchmark with 1,000 Everyday Activities and Realistic Simulation</strong><br><button class=copy-to-clipboard title="BEHAVIOR-1K: A Human-Centered, Embodied AI Benchmark with 1,000 Everyday Activities and Realistic Simulation" index=196>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-196 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs.RO<br>Keyword Score: 23<br>Keywords: Benchmarking, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09227v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09227v1.pdf filename=2403.09227v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present BEHAVIOR-1K, a comprehensive <b>simulation</b> <b>benchmark</b> for human-centered robotics. BEHAVIOR-1K includes two components, guided and motivated by the results of an extensive survey on &ldquo;what do you want robots to do for you?&rdquo;. The first is the definition of 1,000 everyday activities, grounded in 50 scenes (houses, gardens, restaurants, offices, etc.) with more than 9,000 objects annotated with rich physical and semantic properties. The second is OMNIGIBSON, a novel <b>simulation</b> environment that supports these activities via realistic physics <b>simulation</b> and rendering of rigid bodies, deformable bodies, and liquids. Our experiments indicate that the activities in BEHAVIOR-1K are long-horizon and dependent on complex manipulation skills, both of which remain a challenge for even state-of-the-art robot learning solutions. To calibrate the <b>simulation-to-reality</b> gap of BEHAVIOR-1K, we provide an initial study on transferring solutions learned with a mobile manipulator in a simulated apartment to its real-world counterpart. We hope that BEHAVIOR-1K&rsquo;s human-grounded nature, diversity, and realism make it valuable for embodied AI and robot learning research. Project website: <a href=https://behavior.stanford.edu>https://behavior.stanford.edu</a>.</p></p class="citation"></blockquote><h3 id=816--197320-touch-gs-visual-tactile-supervised-3d-gaussian-splatting-aiden-swann-et-al-2024>(8/16 | 197/320) Touch-GS: Visual-Tactile Supervised 3D Gaussian Splatting (Aiden Swann et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aiden Swann, Matthew Strong, Won Kyung Do, Gadiel Sznaier Camps, Mac Schwager, Monroe Kennedy III. (2024)<br><strong>Touch-GS: Visual-Tactile Supervised 3D Gaussian Splatting</strong><br><button class=copy-to-clipboard title="Touch-GS: Visual-Tactile Supervised 3D Gaussian Splatting" index=197>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-197 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-CV, cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Gaussian Process, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09875v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09875v2.pdf filename=2403.09875v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we propose a novel method to supervise 3D <b>Gaussian</b> <b>Splatting</b> (3DGS) scenes using optical tactile sensors. Optical tactile sensors have become widespread in their use in robotics for manipulation and object representation; however, raw optical tactile sensor data is unsuitable to directly supervise a 3DGS scene. Our representation leverages a <b>Gaussian</b> <b>Process</b> Implicit Surface to implicitly represent the object, combining many touches into a unified representation with uncertainty. We merge this model with a monocular depth estimation network, which is aligned in a two stage process, coarsely aligning with a depth camera and then finely adjusting to match our touch data. For every training image, our method produces a corresponding fused depth and uncertainty map. Utilizing this additional information, we propose a new loss function, variance weighted depth <b>supervised</b> loss, for training the 3DGS scene model. We leverage the DenseTact optical tactile sensor and RealSense RGB-D camera to show that combining touch and vision in this manner leads to quantitatively and qualitatively better results than vision or touch alone in a few-view scene syntheses on opaque as well as on reflective and transparent objects. Please see our project page at <a href=http://armlabstanford.github.io/touch-gs>http://armlabstanford.github.io/touch-gs</a></p></p class="citation"></blockquote><h3 id=916--198320-constrained-passive-interaction-control-leveraging-passivity-and-safety-for-robot-manipulators-zhiquan-zhang-et-al-2024>(9/16 | 198/320) Constrained Passive Interaction Control: Leveraging Passivity and Safety for Robot Manipulators (Zhiquan Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhiquan Zhang, Tianyu Li, Nadia Figueroa. (2024)<br><strong>Constrained Passive Interaction Control: Leveraging Passivity and Safety for Robot Manipulators</strong><br><button class=copy-to-clipboard title="Constrained Passive Interaction Control: Leveraging Passivity and Safety for Robot Manipulators" index=198>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-198 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09853v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09853v1.pdf filename=2403.09853v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Passivity is necessary for robots to fluidly collaborate and interact with humans physically. Nevertheless, due to the unconstrained nature of passivity-based impedance control laws, the robot is vulnerable to infeasible and unsafe configurations upon physical perturbations. In this paper, we propose a novel control architecture that allows a torque-controlled robot to guarantee safety constraints such as kinematic limits, self-collisions, external collisions and singularities and is passive only when feasible. This is achieved by constraining a dynamical system based impedance control law with a relaxed hierarchical control barrier function quadratic program subject to multiple concurrent, possibly contradicting, constraints. Joint space constraints are formulated from efficient data-driven self- and external C^2 collision boundary functions. We theoretically prove constraint satisfaction and show that the robot is passive when feasible. Our approach is validated in <b>simulation</b> and real robot experiments on a 7DoF Franka Research 3 manipulator.</p></p class="citation"></blockquote><h3 id=1016--199320-pushing-in-the-dark-a-reactive-pushing-strategy-for-mobile-robots-using-tactile-feedback-idil-ozdamar-et-al-2024>(10/16 | 199/320) Pushing in the Dark: A Reactive Pushing Strategy for Mobile Robots Using Tactile Feedback (Idil Ozdamar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Idil Ozdamar, Doganay Sirintuna, Robin Arbaud, Arash Ajoudani. (2024)<br><strong>Pushing in the Dark: A Reactive Pushing Strategy for Mobile Robots Using Tactile Feedback</strong><br><button class=copy-to-clipboard title="Pushing in the Dark: A Reactive Pushing Strategy for Mobile Robots Using Tactile Feedback" index=199>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-199 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09305v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09305v1.pdf filename=2403.09305v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>For mobile robots, navigating cluttered or dynamic environments often necessitates non-prehensile manipulation, particularly when faced with objects that are too large, irregular, or fragile to grasp. The unpredictable behavior and varying physical properties of these objects significantly complicate manipulation tasks. To address this challenge, this manuscript proposes a novel Reactive Pushing Strategy. This strategy allows a mobile robot to dynamically adjust its base movements in real-time to achieve successful pushing maneuvers towards a target location. Notably, our strategy adapts the robot motion based on changes in contact location obtained through the tactile sensor covering the base, avoiding dependence on object-related assumptions and its modeled behavior. The effectiveness of the Reactive Pushing Strategy was initially evaluated in the <b>simulation</b> environment, where it significantly outperformed the compared baseline approaches. Following this, we validated the proposed strategy through real-world experiments, demonstrating the robot capability to push objects to the target points located in the entire vicinity of the robot. In both <b>simulation</b> and real-world experiments, the object-specific properties (shape, mass, friction, inertia) were altered along with the changes in target locations to assess the robustness of the proposed method comprehensively.</p></p class="citation"></blockquote><h3 id=1116--200320-dtg--diffusion-based-trajectory-generation-for-mapless-global-navigation-jing-liang-et-al-2024>(11/16 | 200/320) DTG : Diffusion-based Trajectory Generation for Mapless Global Navigation (Jing Liang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jing Liang, Amirreza Payandeh, Daeun Song, Xuesu Xiao, Dinesh Manocha. (2024)<br><strong>DTG : Diffusion-based Trajectory Generation for Mapless Global Navigation</strong><br><button class=copy-to-clipboard title="DTG : Diffusion-based Trajectory Generation for Mapless Global Navigation" index=200>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-200 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09900v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09900v1.pdf filename=2403.09900v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a novel end-to-end <b>diffusion-based</b> <b>trajectory</b> generation method, DTG, for mapless global navigation in challenging outdoor scenarios with occlusions and unstructured off-road features like grass, buildings, bushes, etc. Given a distant goal, our approach computes a trajectory that satisfies the following goals: (1) minimize the travel distance to the goal; (2) maximize the traversability by choosing paths that do not lie in undesirable areas. Specifically, we present a novel Conditional RNN(CRNN) for <b>diffusion</b> <b>models</b> to efficiently generate trajectories. Furthermore, we propose an adaptive training method that ensures that the <b>diffusion</b> <b>model</b> generates more traversable trajectories. We evaluate our methods in various outdoor scenes and compare the performance with other global navigation algorithms on a Husky robot. In practice, we observe at least a 15% improvement in traveling distance and around a 7% improvement in traversability.</p></p class="citation"></blockquote><h3 id=1216--201320-multigrippergrasp-a-dataset-for-robotic-grasping-from-parallel-jaw-grippers-to-dexterous-hands-luis-felipe-casas-murrilo-et-al-2024>(12/16 | 201/320) MultiGripperGrasp: A Dataset for Robotic Grasping from Parallel Jaw Grippers to Dexterous Hands (Luis Felipe Casas Murrilo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Luis Felipe Casas Murrilo, Ninad Khargonkar, Balakrishnan Prabhakaran, Yu Xiang. (2024)<br><strong>MultiGripperGrasp: A Dataset for Robotic Grasping from Parallel Jaw Grippers to Dexterous Hands</strong><br><button class=copy-to-clipboard title="MultiGripperGrasp: A Dataset for Robotic Grasping from Parallel Jaw Grippers to Dexterous Hands" index=201>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-201 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: PaLM<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09841v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09841v1.pdf filename=2403.09841v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce a large-scale dataset named MultiGripperGrasp for robotic grasping. Our dataset contains 30.4M grasps from 11 grippers for 345 objects. These grippers range from two-finger grippers to five-finger grippers, including a human hand. All grasps in the dataset are verified in Isaac Sim to classify them as successful and unsuccessful grasps. Additionally, the object fall-off time for each grasp is recorded as a grasp quality measurement. Furthermore, the grippers in our dataset are aligned according to the orientation and position of their <b>palms,</b> allowing us to transfer grasps from one gripper to another. The grasp transfer significantly increases the number of successful grasps for each gripper in the dataset. Our dataset is useful to study generalized grasp planning and grasp transfer across different grippers.</p></p class="citation"></blockquote><h3 id=1316--202320-are-you-a-robot-detecting-autonomous-vehicles-from-behavior-analysis-fabio-maresca-et-al-2024>(13/16 | 202/320) Are you a robot? Detecting Autonomous Vehicles from Behavior Analysis (Fabio Maresca et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fabio Maresca, Filippo Grazioli, Antonio Albanese, Vincenzo Sciancalepore, Gianpiero Negri, Xavier Costa-Perez. (2024)<br><strong>Are you a robot? Detecting Autonomous Vehicles from Behavior Analysis</strong><br><button class=copy-to-clipboard title="Are you a robot? Detecting Autonomous Vehicles from Behavior Analysis" index=202>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-202 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-LG, cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09571v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09571v1.pdf filename=2403.09571v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The tremendous hype around autonomous driving is eagerly calling for emerging and novel technologies to support advanced mobility use cases. As car manufactures keep developing SAE level 3+ systems to improve the safety and comfort of passengers, traffic authorities need to establish new procedures to manage the transition from human-driven to fully-autonomous vehicles while providing a feedback-loop mechanism to <b>fine-tune</b> envisioned autonomous systems. Thus, a way to automatically profile autonomous vehicles and differentiate those from human-driven ones is a must. In this paper, we present a fully-fledged framework that monitors active vehicles using camera images and state information in order to determine whether vehicles are autonomous, without requiring any active notification from the vehicles themselves. Essentially, it builds on the cooperation among vehicles, which share their data acquired on the road feeding a machine learning model to identify autonomous cars. We extensively tested our solution and created the NexusStreet dataset, by means of the CARLA simulator, employing an autonomous driving control agent and a steering wheel maneuvered by licensed drivers. Experiments show it is possible to discriminate the two behaviors by analyzing video clips with an accuracy of 80%, which improves up to 93% when the target state information is available. Lastly, we deliberately degraded the state to observe how the framework performs under non-ideal data collection conditions.</p></p class="citation"></blockquote><h3 id=1416--203320-motpose-multi-object-6d-pose-estimation-for-dynamic-video-sequences-using-attention-based-temporal-fusion-arul-selvam-periyasamy-et-al-2024>(14/16 | 203/320) MOTPose: Multi-object 6D Pose Estimation for Dynamic Video Sequences using Attention-based Temporal Fusion (Arul Selvam Periyasamy et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Arul Selvam Periyasamy, Sven Behnke. (2024)<br><strong>MOTPose: Multi-object 6D Pose Estimation for Dynamic Video Sequences using Attention-based Temporal Fusion</strong><br><button class=copy-to-clipboard title="MOTPose: Multi-object 6D Pose Estimation for Dynamic Video Sequences using Attention-based Temporal Fusion" index=203>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-203 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Object Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09309v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09309v1.pdf filename=2403.09309v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Cluttered bin-picking environments are challenging for pose estimation models. Despite the impressive progress enabled by deep learning, single-view RGB pose estimation models perform poorly in cluttered dynamic environments. Imbuing the rich temporal information contained in the video of scenes has the potential to enhance models ability to deal with the adverse effects of occlusion and the dynamic nature of the environments. Moreover, joint <b>object</b> <b>detection</b> and pose estimation models are better suited to leverage the co-dependent nature of the tasks for improving the accuracy of both tasks. To this end, we propose attention-based temporal fusion for multi-object 6D pose estimation that accumulates information across multiple frames of a video sequence. Our MOTPose method takes a sequence of images as input and performs joint <b>object</b> <b>detection</b> and pose estimation for all <b>objects</b> <b>in</b> one forward pass. It learns to aggregate both <b>object</b> <b>embeddings</b> and <b>object</b> <b>parameters</b> over multiple time steps using cross-attention-based fusion modules. We evaluate our method on the physically-realistic cluttered bin-picking dataset SynPick and the YCB-Video dataset and demonstrate improved pose estimation accuracy as well as better <b>object</b> <b>detection</b> accuracy</p></p class="citation"></blockquote><h3 id=1516--204320-development-of-control-algorithms-for-mobile-robotics-focused-on-their-potential-use-for-fpga-based-robots-andrés-david-suárez-gómez-et-al-2024>(15/16 | 204/320) Development of control algorithms for mobile robotics focused on their potential use for FPGA-based robots (Andrés-David Suárez-Gómez et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andrés-David Suárez-Gómez, Andres A. Hernandez Ortega. (2024)<br><strong>Development of control algorithms for mobile robotics focused on their potential use for FPGA-based robots</strong><br><button class=copy-to-clipboard title="Development of control algorithms for mobile robotics focused on their potential use for FPGA-based robots" index=204>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-204 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AR, cs-RO, cs.RO<br>Keyword Score: 6<br>Keywords: Benchmarking, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09459v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09459v1.pdf filename=2403.09459v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper investigates the development and optimization of control algorithms for mobile robotics, with a keen focus on their implementation in Field-Programmable Gate Arrays (FPGAs). It delves into both classical control approaches such as PID and modern techniques including deep learning, addressing their application in sectors ranging from industrial automation to medical care. The study highlights the practical challenges and advancements in embedding these algorithms into FPGAs, which offer significant benefits for mobile robotics due to their high-speed processing and parallel computation capabilities. Through an analysis of various control strategies, the paper showcases the improvements in robot performance, particularly in navigation and obstacle avoidance. It emphasizes the critical role of FPGAs in enhancing the efficiency and adaptability of control algorithms in dynamic environments. Additionally, the research discusses the difficulties in <b>benchmarking</b> and evaluating the performance of these algorithms in real-world applications, suggesting a need for standardized evaluation criteria. The contribution of this work lies in its comprehensive examination of control algorithms&rsquo; potential in FPGA-based mobile robotics, offering insights into future research directions for improving robotic autonomy and operational efficiency.</p></p class="citation"></blockquote><h3 id=1616--205320-thör-magni-a-large-scale-indoor-motion-capture-recording-of-human-movement-and-robot-interaction-tim-schreiter-et-al-2024>(16/16 | 205/320) THÖR-MAGNI: A Large-scale Indoor Motion Capture Recording of Human Movement and Robot Interaction (Tim Schreiter et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tim Schreiter, Tiago Rodrigues de Almeida, Yufei Zhu, Eduardo Gutierrez Maestro, Lucas Morillo-Mendez, Andrey Rudenko, Luigi Palmieri, Tomasz P. Kucner, Martin Magnusson, Achim J. Lilienthal. (2024)<br><strong>THÖR-MAGNI: A Large-scale Indoor Motion Capture Recording of Human Movement and Robot Interaction</strong><br><button class=copy-to-clipboard title="THÖR-MAGNI: A Large-scale Indoor Motion Capture Recording of Human Movement and Robot Interaction" index=205>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-205 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 3<br>Keywords: Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09285v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09285v1.pdf filename=2403.09285v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a new large dataset of indoor human and robot navigation and interaction, called TH"OR-MAGNI, that is designed to facilitate research on social navigation: e.g., modelling and predicting human motion, analyzing goal-oriented interactions between humans and robots, and investigating visual attention in a social interaction context. TH"OR-MAGNI was created to fill a gap in available datasets for human motion analysis and HRI. This gap is characterized by a lack of comprehensive inclusion of exogenous factors and essential target agent cues, which hinders the development of robust models capable of capturing the relationship between contextual cues and human behavior in different scenarios. Unlike existing datasets, TH"OR-MAGNI includes a broader set of contextual features and offers multiple scenario variations to facilitate factor isolation. The dataset includes many social human-human and human-robot interaction scenarios, rich context annotations, and <b>multi-modal</b> data, such as walking trajectories, gaze tracking data, and lidar and camera streams recorded from a mobile robot. We also provide a set of tools for visualization and processing of the recorded data. TH"OR-MAGNI is, to the best of our knowledge, unique in the amount and diversity of sensor data collected in a contextualized and socially dynamic environment, capturing natural human-robot interactions.</p></p class="citation"></blockquote><h2 id=csse-8>cs.SE (8)</h2><h3 id=18--206320-codeultrafeedback-an-llm-as-a-judge-dataset-for-aligning-large-language-models-to-coding-preferences-martin-weyssow-et-al-2024>(1/8 | 206/320) CodeUltraFeedback: An LLM-as-a-Judge Dataset for Aligning Large Language Models to Coding Preferences (Martin Weyssow et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Martin Weyssow, Aton Kamanda, Houari Sahraoui. (2024)<br><strong>CodeUltraFeedback: An LLM-as-a-Judge Dataset for Aligning Large Language Models to Coding Preferences</strong><br><button class=copy-to-clipboard title="CodeUltraFeedback: An LLM-as-a-Judge Dataset for Aligning Large Language Models to Coding Preferences" index=206>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-206 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-CL, cs-LG, cs-SE, cs.SE<br>Keyword Score: 73<br>Keywords: Benchmarking, Direct Preference Optimization, Reinforcement Learning, GPT, GPT-3, GPT-3.5, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09032v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09032v1.pdf filename=2403.09032v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Evaluating the alignment of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> with user-defined coding preferences is a challenging endeavour that requires assessing intricate textual <b>LLMs&rsquo;</b> outputs. By relying on automated metrics and static analysis tools, existing <b>benchmarks</b> fail to assess nuances in user instructions and <b>LLM</b> outputs, highlighting the need for <b>large-scale</b> <b>datasets</b> <b>and</b> <b>benchmarks</b> for <b>LLM</b> preference alignment. In this paper, we introduce CodeUltraFeedback, a preference dataset of 10,000 complex instructions to tune and align <b>LLMs</b> to coding preferences through AI feedback. We generate responses to the instructions using a pool of 14 diverse <b>LLMs,</b> which we then annotate according to their alignment with five coding preferences using the <b>LLM-as-a-Judge</b> approach with <b>GPT-3.5,</b> producing both numerical and textual feedback. We also present CODAL-Bench, a <b>benchmark</b> for assessing <b>LLM</b> alignment with these coding preferences. Our results show that CodeLlama-7B-Instruct, aligned through <b>reinforcement</b> <b>learning</b> from AI feedback (RLAIF) with <b>direct</b> <b>preference</b> <b>optimization</b> (DPO) using CodeUltraFeedback&rsquo;s AI feedback data, outperforms 34B <b>LLMs</b> on CODAL-Bench, validating the utility of CodeUltraFeedback for preference tuning. Furthermore, we show our DPO-aligned CodeLlama model improves functional correctness on HumanEval+ compared to the unaligned base model. Therefore, our contributions bridge the gap in preference tuning of <b>LLMs</b> for code and set the stage for further advancements in model alignment and RLAIF for code intelligence. Our code and data are available at <a href=https://github.com/martin-wey/CodeUltraFeedback>https://github.com/martin-wey/CodeUltraFeedback</a>.</p></p class="citation"></blockquote><h3 id=28--207320-reality-bites-assessing-the-realism-of-driving-scenarios-with-large-language-models-jiahui-wu-et-al-2024>(2/8 | 207/320) Reality Bites: Assessing the Realism of Driving Scenarios with Large Language Models (Jiahui Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiahui Wu, Chengjie Lu, Aitor Arrieta, Tao Yue, Shaukat Ali. (2024)<br><strong>Reality Bites: Assessing the Realism of Driving Scenarios with Large Language Models</strong><br><button class=copy-to-clipboard title="Reality Bites: Assessing the Realism of Driving Scenarios with Large Language Models" index=207>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-207 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 40<br>Keywords: Text Generation, Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09906v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09906v1.pdf filename=2403.09906v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> are demonstrating outstanding potential for tasks such as <b>text</b> <b>generation,</b> <b>summarization,</b> and classification. Given that such models are trained on a humongous amount of online knowledge, we hypothesize that <b>LLMs</b> can assess whether driving scenarios generated by autonomous driving testing techniques are realistic, i.e., being aligned with real-world driving conditions. To test this hypothesis, we conducted an empirical evaluation to assess whether <b>LLMs</b> are effective and robust in performing the task. This reality check is an important step towards devising <b>LLM-based</b> autonomous driving testing techniques. For our empirical evaluation, we selected 64 realistic scenarios from \deepscenario&ndash;an open driving scenario dataset. Next, by introducing minor changes to them, we created 512 additional realistic scenarios, to form an overall dataset of 576 scenarios. With this dataset, we evaluated three <b>LLMs</b> (\gpt, \llama, and \mistral) to assess their robustness in assessing the realism of driving scenarios. Our results show that: (1) Overall, \gpt achieved the highest robustness compared to \llama and \mistral, consistently throughout almost all scenarios, roads, and weather conditions; (2) \mistral performed the worst consistently; (3) \llama achieved good results under certain conditions; and (4) roads and weather conditions do influence the robustness of the <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=38--208320-code-revert-prediction-with-graph-neural-networks-a-case-study-at-jp-morgan-chase-yulong-pei-et-al-2024>(3/8 | 208/320) Code Revert Prediction with Graph Neural Networks: A Case Study at J.P. Morgan Chase (Yulong Pei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yulong Pei, Salwa Alamir, Rares Dolga, Sameena Shah. (2024)<br><strong>Code Revert Prediction with Graph Neural Networks: A Case Study at J.P. Morgan Chase</strong><br><button class=copy-to-clipboard title="Code Revert Prediction with Graph Neural Networks: A Case Study at J.P. Morgan Chase" index=208>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-208 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 23<br>Keywords: Graph, Graph Neural Network, Anomaly Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09507v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09507v1.pdf filename=2403.09507v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Code revert prediction, a specialized form of software defect detection, aims to forecast or predict the likelihood of code changes being reverted or rolled back in software development. This task is very important in practice because by identifying code changes that are more prone to being reverted, developers and project managers can proactively take measures to prevent issues, improve code quality, and optimize development processes. However, compared to code defect detection, code revert prediction has been rarely studied in previous research. Additionally, many previous methods for code defect detection relied on independent features but ignored relationships between code scripts. Moreover, new challenges are introduced due to constraints in an industry setting such as company regulation, limited features and large-scale codebase. To overcome these limitations, this paper presents a systematic empirical study for code revert prediction that integrates the code import <b>graph</b> <b>with</b> <b>code</b> features. Different strategies to address anomalies and data imbalance have been implemented including <b>graph</b> <b>neural</b> <b>networks</b> with imbalance classification and <b>anomaly</b> <b>detection.</b> We conduct the experiments on real-world code commit data within J.P. Morgan Chase which is extremely imbalanced in order to make a comprehensive comparison of these different approaches for the code revert prediction problem.</p></p class="citation"></blockquote><h3 id=48--209320-welcome-your-new-ai-teammate-on-safety-analysis-by-leashing-large-language-models-ali-nouri-et-al-2024>(4/8 | 209/320) Welcome Your New AI Teammate: On Safety Analysis by Leashing Large Language Models (Ali Nouri et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ali Nouri, Beatriz Cabrero-Daniel, Fredrik Törner, Hȧkan Sivencrona, Christian Berger. (2024)<br><strong>Welcome Your New AI Teammate: On Safety Analysis by Leashing Large Language Models</strong><br><button class=copy-to-clipboard title="Welcome Your New AI Teammate: On Safety Analysis by Leashing Large Language Models" index=209>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-209 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-SE, cs.SE<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09565v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09565v1.pdf filename=2403.09565v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>DevOps is a necessity in many industries, including the development of Autonomous Vehicles. In those settings, there are iterative activities that reduce the speed of SafetyOps cycles. One of these activities is &ldquo;Hazard Analysis & Risk Assessment&rdquo; (HARA), which is an essential step to start the safety requirements specification. As a potential approach to increase the speed of this step in SafetyOps, we have delved into the capabilities of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> Our objective is to systematically assess their potential for application in the field of safety engineering. To that end, we propose a framework to support a higher degree of automation of HARA with <b>LLMs.</b> Despite our endeavors to automate as much of the process as possible, expert review remains crucial to ensure the validity and correctness of the analysis results, with necessary modifications made accordingly.</p></p class="citation"></blockquote><h3 id=58--210320-llm-based-agents-for-automating-the-enhancement-of-user-story-quality-an-early-report-zheying-zhang-et-al-2024>(5/8 | 210/320) LLM-based agents for automating the enhancement of user story quality: An early report (Zheying Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zheying Zhang, Maruf Rayhan, Tomas Herda, Manuel Goisauf, Pekka Abrahamsson. (2024)<br><strong>LLM-based agents for automating the enhancement of user story quality: An early report</strong><br><button class=copy-to-clipboard title="LLM-based agents for automating the enhancement of user story quality: An early report" index=210>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-210 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-SE, cs.SE<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09442v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09442v1.pdf filename=2403.09442v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In agile software development, maintaining high-quality user stories is crucial, but also challenging. This study explores the use of <b>large</b> <b>language</b> <b>models</b> to automatically improve the user story quality in Austrian Post Group IT agile teams. We developed a reference model for an Autonomous <b>LLM-based</b> Agent System and implemented it at the company. The quality of user stories in the study and the effectiveness of these agents for user story quality improvement was assessed by 11 participants across six agile teams. Our findings demonstrate the potential of <b>LLMs</b> in improving user story quality, contributing to the research on AI role in agile development, and providing a practical example of the transformative impact of AI in an industry setting.</p></p class="citation"></blockquote><h3 id=68--211320-analyzing-and-mitigating-with-llms-the-security-misconfigurations-of-helm-charts-from-artifact-hub-francesco-minna-et-al-2024>(6/8 | 211/320) Analyzing and Mitigating (with LLMs) the Security Misconfigurations of Helm Charts from Artifact Hub (Francesco Minna et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Francesco Minna, Fabio Massacci, Katja Tuma. (2024)<br><strong>Analyzing and Mitigating (with LLMs) the Security Misconfigurations of Helm Charts from Artifact Hub</strong><br><button class=copy-to-clipboard title="Analyzing and Mitigating (with LLMs) the Security Misconfigurations of Helm Charts from Artifact Hub" index=211>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-211 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09537v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09537v1.pdf filename=2403.09537v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Background: Helm is a package manager that allows defining, installing, and upgrading applications with Kubernetes (K8s), a popular container orchestration platform. A Helm chart is a collection of files describing all dependencies, resources, and parameters required for deploying an application within a K8s cluster. Objective: The goal of this study is to mine and empirically evaluate the security of Helm charts, comparing the performance of existing tools in terms of misconfigurations reported by policies available by default, and measure to what extent <b>LLMs</b> could be used for removing misconfiguration. We also want to investigate whether there are false positives in both the <b>LLM</b> refactorings and the tool outputs. Method: We propose a pipeline to mine Helm charts from Artifact Hub, a popular centralized repository, and analyze them using state-of-the-art open-source tools, such as Checkov and KICS. First, such a pipeline will run several chart analyzers and identify the common and unique misconfigurations reported by each tool. Secondly, it will use <b>LLMs</b> to suggest mitigation for each misconfiguration. Finally, the chart refactoring previously generated will be analyzed again by the same tools to see whether it satisfies the tool&rsquo;s policies. At the same time, we will also perform a manual analysis on a subset of charts to evaluate whether there are false positive misconfigurations from the tool&rsquo;s reporting and in the <b>LLM</b> refactoring.</p></p class="citation"></blockquote><h3 id=78--212320-gamified-gui-testing-with-selenium-in-the-intellij-ide-a-prototype-plugin-giacomo-garaccione-et-al-2024>(7/8 | 212/320) Gamified GUI testing with Selenium in the IntelliJ IDE: A Prototype Plugin (Giacomo Garaccione et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Giacomo Garaccione, Tommaso Fulcini, Paolo Stefanut Bodnarescul, Riccardo Coppola, Luca Ardito. (2024)<br><strong>Gamified GUI testing with Selenium in the IntelliJ IDE: A Prototype Plugin</strong><br><button class=copy-to-clipboard title="Gamified GUI testing with Selenium in the IntelliJ IDE: A Prototype Plugin" index=212>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-212 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 3<br>Keywords: Sample Size<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09842v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09842v1.pdf filename=2403.09842v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Software testing is a crucial phase in software development, enabling the detection of issues and defects that may arise during the development process. Addressing these issues enhances software applications&rsquo; quality, reliability, user experience, and performance. Graphical User Interface (GUI) testing, one such technique, involves mimicking a regular user&rsquo;s interactions with an application to identify defects. However, GUI testing is often underutilized due to its perceived repetitiveness, error-proneness, and lack of immediate feedback on test quality. In recent years, gamification-incorporating game elements in non-game contexts to boost interest, motivation, and engagement-has gained traction in various fields, including software engineering and education. This paper presents GIPGUT: a prototype of a gamification plugin for IntelliJ IDEA, an Integrated Development Environment (IDE) that supports scripted GUI testing. The plugin enhances testers&rsquo; engagement with typically monotonous and tedious tasks through achievements, rewards, and profile customization. A preliminary prototype evaluation was conducted with a small group of users to assess its usability and the impact of gamification on the GUI testing process. The results indicate high usability and positive reception of the gamification elements. However, due to the limited <b>sample</b> <b>size</b> of participants, further research is necessary to understand the plugin&rsquo;s effectiveness fully.</p></p class="citation"></blockquote><h3 id=88--213320-an-extensive-comparison-of-static-application-security-testing-tools-matteo-esposito-et-al-2024>(8/8 | 213/320) An Extensive Comparison of Static Application Security Testing Tools (Matteo Esposito et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Matteo Esposito, Valentina Falaschi, Davide Falessi. (2024)<br><strong>An Extensive Comparison of Static Application Security Testing Tools</strong><br><button class=copy-to-clipboard title="An Extensive Comparison of Static Application Security Testing Tools" index=213>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-213 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-CR, cs-CY, cs-SE, cs.SE<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09219v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09219v1.pdf filename=2403.09219v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Context: Static Application Security Testing Tools (SASTTs) identify software vulnerabilities to support the security and reliability of software applications. Interestingly, several studies have suggested that alternative solutions may be more effective than SASTTs due to their tendency to generate false alarms, commonly referred to as low Precision. Aim: We aim to comprehensively evaluate SASTTs, setting a reliable <b>benchmark</b> for assessing and finding gaps in vulnerability identification mechanisms based on SASTTs or alternatives. Method: Our SASTTs evaluation is based on a controlled, though synthetic, Java codebase. It involves an assessment of 1.5 million test executions, and it features innovative methodological features such as effort-aware accuracy metrics and method-level analysis. Results: Our findings reveal that SASTTs detect a tiny range of vulnerabilities. In contrast to prevailing wisdom, SASTTs exhibit high Precision while falling short in Recall. Conclusions: The paper suggests that enhancing Recall, alongside expanding the spectrum of detected vulnerability types, should be the primary focus for improving SASTTs or alternative approaches, such as machine learning-based vulnerability identification solutions.</p></p class="citation"></blockquote><h2 id=eesssy-11>eess.SY (11)</h2><h3 id=111--214320-exploring-the-capabilities-and-limitations-of-large-language-models-in-the-electric-energy-sector-lin-dong-et-al-2024>(1/11 | 214/320) Exploring the Capabilities and Limitations of Large Language Models in the Electric Energy Sector (Lin Dong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lin Dong, Subir Majumder, Fatemeh Doudi, Yuting Cai, Chao Tian, Dileep Kalathi, Kevin Ding, Anupam A. Thatte, Le Xie. (2024)<br><strong>Exploring the Capabilities and Limitations of Large Language Models in the Electric Energy Sector</strong><br><button class=copy-to-clipboard title="Exploring the Capabilities and Limitations of Large Language Models in the Electric Energy Sector" index=214>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-214 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 70<br>Keywords: Fine-tuning, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Chatbot, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09125v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09125v2.pdf filename=2403.09125v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> as <b>chatbots</b> have drawn remarkable attention thanks to their versatile capability in natural language processing as well as in a wide range of tasks. While there has been great enthusiasm towards adopting such foundational model-based artificial intelligence tools in all sectors possible, the capabilities and limitations of such <b>LLMs</b> in improving the operation of the electric energy sector need to be explored, and this article identifies fruitful directions in this regard. Key future research directions include data collection systems for <b>fine-tuning</b> <b>LLMs,</b> embedding power system-specific tools in the <b>LLMs,</b> and <b>retrieval</b> <b>augmented</b> <b>generation</b> <b>(RAG)-based</b> knowledge pool to improve the quality of <b>LLM</b> responses and <b>LLMs</b> in safety-critical use cases.</p></p class="citation"></blockquote><h3 id=211--215320-is-data-all-that-matters-the-role-of-control-frequency-for-learning-based-sampled-data-control-of-uncertain-systems-ralf-römer-et-al-2024>(2/11 | 215/320) Is Data All That Matters? The Role of Control Frequency for Learning-Based Sampled-Data Control of Uncertain Systems (Ralf Römer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ralf Römer, Lukas Brunke, Siqi Zhou, Angela P. Schoellig. (2024)<br><strong>Is Data All That Matters? The Role of Control Frequency for Learning-Based Sampled-Data Control of Uncertain Systems</strong><br><button class=copy-to-clipboard title="Is Data All That Matters? The Role of Control Frequency for Learning-Based Sampled-Data Control of Uncertain Systems" index=215>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-215 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-RO, cs-SY, eess-SY, eess.SY<br>Keyword Score: 50<br>Keywords: Continuous Time, Continuous Time, Discrete Time, Discrete Time, Probabilistic Model, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09504v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09504v1.pdf filename=2403.09504v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Learning models or control policies from data has become a powerful tool to improve the performance of uncertain systems. While a strong focus has been placed on increasing the amount and quality of data to improve performance, data can never fully eliminate uncertainty, making feedback necessary to ensure stability and performance. We show that the control frequency at which the input is recalculated is a crucial design parameter, yet it has hardly been considered before. We address this gap by combining <b>probabilistic</b> <b>model</b> learning and sampled-data control. We use Gaussian processes (GPs) to learn a <b>continuous-time</b> <b>model</b> and compute a corresponding <b>discrete-time</b> <b>controller.</b> The result is an uncertain sampled-data control system, for which we derive robust stability conditions. We formulate semidefinite programs to compute the minimum control frequency required for stability and to optimize performance. As a result, our approach enables us to study the effect of both control frequency and data on stability and closed-loop performance. We show in numerical <b>simulations</b> of a quadrotor that performance can be improved by increasing either the amount of data or the control frequency, and that we can trade off one for the other. For example, by increasing the control frequency by 33%, we can reduce the number of data points by half while still achieving similar performance.</p></p class="citation"></blockquote><h3 id=311--216320-impact-of-objective-function-on-spectral-efficiency-in-integrated-haps-terrestrial-networks-afsoon-alidadi-shamsabadi-et-al-2024>(3/11 | 216/320) Impact of Objective Function on Spectral Efficiency in Integrated HAPS-Terrestrial Networks (Afsoon Alidadi Shamsabadi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Afsoon Alidadi Shamsabadi, Animesh Yadav, Halim Yanikomeroglu. (2024)<br><strong>Impact of Objective Function on Spectral Efficiency in Integrated HAPS-Terrestrial Networks</strong><br><button class=copy-to-clipboard title="Impact of Objective Function on Spectral Efficiency in Integrated HAPS-Terrestrial Networks" index=216>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-216 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 30<br>Keywords: Fairness, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09817v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09817v1.pdf filename=2403.09817v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Integrating non-terrestrial networks (NTNs), in particular high altitude platform stations (HAPS), with terrestrial networks, referred to as vHetNets, emerges as a promising future wireless network architecture for providing ubiquitous connectivity. In this context, optimizing the performance of vHetNets has become a paramount concern, particularly in harmonized spectrum vHetNets, where HAPS and terrestrial networks share the same frequency band, resulting in severe inter-/intra-tier interference. This paper provides a comparative analysis of different objective functions, specifically focusing on weighted sum rate (WSR), network-wide proportional <b>fairness</b> (NW-PF), and network-wide max-min <b>fairness</b> (NW-MMF), with an aim to design a joint user association scheme and multiple-input multiple-output (MIMO) beamforming weights in a vHetNet, operating in an urban area. The <b>simulation</b> results comprehensively compare the behavior of different objective functions in vHetNets and standalone terrestrial networks. This analysis aims to shed light on the impact of diverse objective functions on the achievable spectral efficiency (SE) of vHetNets.</p></p class="citation"></blockquote><h3 id=411--217320-optimal-sequencing-and-motion-control-in-a-roundabout-with-safety-guarantees-yingqing-chen-et-al-2024>(4/11 | 217/320) Optimal Sequencing and Motion Control in a Roundabout with Safety Guarantees (Yingqing Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yingqing Chen, Christos G. Cassandras, Kaiyuan Xu. (2024)<br><strong>Optimal Sequencing and Motion Control in a Roundabout with Safety Guarantees</strong><br><button class=copy-to-clipboard title="Optimal Sequencing and Motion Control in a Roundabout with Safety Guarantees" index=217>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-217 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09923v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09923v2.pdf filename=2403.09923v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper develops a controller for Connected and Automated Vehicles (CAVs) traversing a single-lane roundabout. The controller simultaneously determines the optimal sequence and associated optimal motion control jointly minimizing travel time and energy consumption while providing speed-dependent safety guarantees, as well as satisfying velocity and acceleration constraints. This is achieved by integrating (a) Model Predictive Control (MPC) to enable receding horizon optimization with (b) Control Lyapunov-Barrier Functions (CLBFs) to guarantee convergence to a safe set in finite time, thus providing an extended stability region compared to the use of classic Control Barrier Functions (CBFs). The proposed MPC-CLBF framework addresses both infeasibility and myopic control issues commonly encountered when controlling CAVs over multiple interconnected control zones in a traffic network, which has been a limitation of prior work on CAVs going through roundabouts, while still providing safety guarantees. <b>Simulations</b> under varying traffic demands demonstrate the controller&rsquo;s effectiveness and stability.</p></p class="citation"></blockquote><h3 id=511--218320-fairness-aware-multi-server-federated-learning-task-delegation-over-wireless-networks-yulan-gao-et-al-2024>(5/11 | 218/320) Fairness-Aware Multi-Server Federated Learning Task Delegation over Wireless Networks (Yulan Gao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yulan Gao, Chao Ren, Han Yu. (2024)<br><strong>Fairness-Aware Multi-Server Federated Learning Task Delegation over Wireless Networks</strong><br><button class=copy-to-clipboard title="Fairness-Aware Multi-Server Federated Learning Task Delegation over Wireless Networks" index=218>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-218 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Fairness, Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09153v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09153v1.pdf filename=2403.09153v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the rapidly advancing field of <b>federated</b> <b>learning</b> (FL), ensuring efficient FL task delegation while incentivising FL client participation poses significant challenges, especially in wireless networks where FL participants&rsquo; coverage is limited. Existing Contract Theory-based methods are designed under the assumption that there is only one FL server in the system (i.e., the monopoly market assumption), which in unrealistic in practice. To address this limitation, we propose <b>Fairness-Aware</b> Multi-Server FL task delegation approach (FAMuS), a novel framework based on Contract Theory and Lyapunov optimization to jointly address these intricate issues facing wireless multi-server FL networks (WMSFLN). Within a given WMSFLN, a task requester products multiple FL tasks and delegate them to FL servers which coordinate the training processes. To ensure fair treatment of FL servers, FAMuS establishes virtual queues to track their previous access to FL tasks, updating them in relation to the resulting FL model performance. The objective is to minimize the time-averaged cost in a WMSFLN, while ensuring all queues remain stable. This is particularly challenging given the incomplete information regarding FL clients&rsquo; participation cost and the unpredictable nature of the WMSFLN state, which depends on the locations of the mobile clients. Extensive experiments comparing FAMuS against five state-of-the-art approaches based on two real-world datasets demonstrate that it achieves 6.91% higher test accuracy, 27.34% lower cost, and 0.63% higher <b>fairness</b> on average than the best-performing baseline.</p></p class="citation"></blockquote><h3 id=611--219320-optimal-pinning-control-for-synchronization-over-temporal-networks-aandrew-baggio-s-et-al-2024>(6/11 | 219/320) Optimal Pinning Control for Synchronization over Temporal Networks (Aandrew Baggio S et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aandrew Baggio S, Rachel Kalpana Kalaimani. (2024)<br><strong>Optimal Pinning Control for Synchronization over Temporal Networks</strong><br><button class=copy-to-clipboard title="Optimal Pinning Control for Synchronization over Temporal Networks" index=219>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-219 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09127v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09127v1.pdf filename=2403.09127v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we address the finite time synchronization of a network of dynamical systems with time-varying interactions modeled using temporal networks. We synchronize a few nodes initially using external control inputs. These nodes are termed as pinning nodes. The other nodes are synchronized by interacting with the pinning nodes and with each other. We first provide sufficient conditions for the network to be synchronized. Then we formulate an optimization problem to minimize the number of pinning nodes for synchronizing the entire network. Finally, we address the problem of maximizing the number of synchronized nodes when there are constraints on the number of nodes that could be pinned. We show that this problem belongs to the class of NP-hard problems and propose a greedy heuristic. We illustrate the results using numerical <b>simulations.</b></p></p class="citation"></blockquote><h3 id=711--220320-confidence-aware-safe-and-stable-control-of-control-affine-systems-shiqing-wei-et-al-2024>(7/11 | 220/320) Confidence-Aware Safe and Stable Control of Control-Affine Systems (Shiqing Wei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shiqing Wei, Prashanth Krishnamurthy, Farshad Khorrami. (2024)<br><strong>Confidence-Aware Safe and Stable Control of Control-Affine Systems</strong><br><button class=copy-to-clipboard title="Confidence-Aware Safe and Stable Control of Control-Affine Systems" index=220>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-220 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09067v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09067v1.pdf filename=2403.09067v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Designing control inputs that satisfy safety requirements is crucial in safety-critical nonlinear control, and this task becomes particularly challenging when full-state measurements are unavailable. In this work, we address the problem of synthesizing safe and stable control for control-affine systems via output feedback (using an observer) while reducing the estimation error of the observer. To achieve this, we adapt control Lyapunov function (CLF) and control barrier function (CBF) techniques to the output feedback setting. Building upon the existing CLF-CBF-QP (Quadratic Program) and CBF-QP frameworks, we formulate two confidence-aware optimization problems and establish the Lipschitz continuity of the obtained solutions. To validate our approach, we conduct <b>simulation</b> studies on two illustrative examples. The <b>simulation</b> studies indicate both improvements in the observer&rsquo;s estimation accuracy and the fulfillment of safety and control requirements.</p></p class="citation"></blockquote><h3 id=811--221320-a-geometric-approach-to-resilient-distributed-consensus-accounting-for-state-imprecision-and-adversarial-agents-christopher-a-lee-et-al-2024>(8/11 | 221/320) A Geometric Approach to Resilient Distributed Consensus Accounting for State Imprecision and Adversarial Agents (Christopher A. Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Christopher A. Lee, Waseem Abbas. (2024)<br><strong>A Geometric Approach to Resilient Distributed Consensus Accounting for State Imprecision and Adversarial Agents</strong><br><button class=copy-to-clipboard title="A Geometric Approach to Resilient Distributed Consensus Accounting for State Imprecision and Adversarial Agents" index=221>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-221 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09009v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09009v1.pdf filename=2403.09009v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a novel approach for resilient distributed consensus in multiagent networks when dealing with adversarial agents imprecision in states observed by normal agents. Traditional resilient distributed consensus algorithms often presume that agents have exact knowledge of their neighbors&rsquo; states, which is unrealistic in practical scenarios. We show that such existing methods are inadequate when agents only have access to imprecise states of their neighbors. To overcome this challenge, we adapt a geometric approach and model an agent&rsquo;s state by an `imprecision region&rsquo; rather than a point in $\mathbb{R}^d$. From a given set of imprecision regions, we first present an efficient way to compute a region that is guaranteed to lie in the convex hull of true, albeit unknown, states of agents. We call this region the \emph{invariant hull} of imprecision regions and provide its geometric characterization. Next, we use these invariant hulls to identify a \emph{safe point} for each normal agent. The safe point of an agent lies within the convex hull of its \emph{normal} neighbors&rsquo; states and hence is used by the agent to update it&rsquo;s state. This leads to the aggregation of normal agents&rsquo; states to safe points inside the convex hull of their initial states, or an approximation of consensus. We also illustrate our results through <b>simulations.</b> Our contributions enhance the robustness of resilient distributed consensus algorithms by accommodating state imprecision without compromising resilience against adversarial agents.</p></p class="citation"></blockquote><h3 id=911--222320-defense-via-behavior-attestation-against-attacks-in-connected-and-automated-vehicles-based-federated-learning-systems-godwin-badu-marfo-et-al-2024>(9/11 | 222/320) Defense via Behavior Attestation against Attacks in Connected and Automated Vehicles based Federated Learning Systems (Godwin Badu-Marfo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Godwin Badu-Marfo, Ranwa Al Mallah, Bilal Farooq. (2024)<br><strong>Defense via Behavior Attestation against Attacks in Connected and Automated Vehicles based Federated Learning Systems</strong><br><button class=copy-to-clipboard title="Defense via Behavior Attestation against Attacks in Connected and Automated Vehicles based Federated Learning Systems" index=222>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-222 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09531v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09531v1.pdf filename=2403.09531v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The recent application of <b>Federated</b> <b>Learning</b> algorithms in IOT and Wireless vehicular networks have given rise to newer cyber threats in the mobile environment which hitherto were not present in traditional fixed networks. These threats arise due to the intrinsic nature of wireless transmission medium and other inherent characteristics of mobile networks such as high-node mobility and rapidly changing topology. This paper investigates the robustness of Vehicular AttestedFL defense strategies against falsified information attacks by tracking the behavior. We show that the defense strategies are capable of detecting and eliminating malicious nodes in the wireless mobile setting of the future smart road networks.</p></p class="citation"></blockquote><h3 id=1011--223320-learning-algorithms-for-verification-of-markov-decision-processes-tomáš-brázdil-et-al-2024>(10/11 | 223/320) Learning Algorithms for Verification of Markov Decision Processes (Tomáš Brázdil et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tomáš Brázdil, Krishnendu Chatterjee, Martin Chmelik, Vojtěch Forejt, Jan Křetínský, Marta Kwiatkowska, Tobias Meggendorfer, David Parker, Mateusz Ujma. (2024)<br><strong>Learning Algorithms for Verification of Markov Decision Processes</strong><br><button class=copy-to-clipboard title="Learning Algorithms for Verification of Markov Decision Processes" index=223>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-223 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-AI, cs-SY, eess-SY, eess.SY<br>Keyword Score: 10<br>Keywords: Markov Decision Process<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09184v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09184v2.pdf filename=2403.09184v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a general framework for applying learning algorithms and heuristical guidance to the verification of Markov decision processes <b>(MDPs).</b> The primary goal of our techniques is to improve performance by avoiding an exhaustive exploration of the state space, instead focussing on particularly relevant areas of the system, guided by heuristics. Our work builds on the previous results of Br{'{a}}zdil et al., significantly extending it as well as refining several details and fixing errors. The presented framework focuses on probabilistic reachability, which is a core problem in verification, and is instantiated in two distinct scenarios. The first assumes that full knowledge of the MDP is available, in particular precise transition probabilities. It performs a heuristic-driven partial exploration of the model, yielding precise lower and upper bounds on the required probability. The second tackles the case where we may only sample the MDP without knowing the exact transition dynamics. Here, we obtain probabilistic guarantees, again in terms of both the lower and upper bounds, which provides efficient stopping criteria for the approximation. In particular, the latter is an extension of statistical model-checking (SMC) for unbounded properties in <b>MDPs.</b> In contrast to other related approaches, we do not restrict our attention to time-bounded (finite-horizon) or discounted properties, nor assume any particular structural properties of the MDP.</p></p class="citation"></blockquote><h3 id=1111--224320-partitioning-distribution-networks-for-integrated-electrification-planning-olamide-oladeji-et-al-2024>(11/11 | 224/320) Partitioning Distribution Networks for Integrated Electrification Planning (Olamide Oladeji et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Olamide Oladeji, Pedro Ciller Cutillas, Fernando de Cuadra, Ignacio Perez-Arriaga. (2024)<br><strong>Partitioning Distribution Networks for Integrated Electrification Planning</strong><br><button class=copy-to-clipboard title="Partitioning Distribution Networks for Integrated Electrification Planning" index=224>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-224 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-CE, cs-SY, eess-SY, eess.SY<br>Keyword Score: 6<br>Keywords: Benchmarking, Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09111v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09111v1.pdf filename=2403.09111v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In many developing countries, access to electricity remains a significant challenge. Electrification planners in these countries often have to make important decisions on the mode of electrification and the planning of electrical networks for those without access, while under resource constraints. An integrated approach to electrification planning in which traditional grid electrification is complemented off-the-grid technologies such as off-grid microgrids and stand-alone systems can enable the economic provision of electricity access in these regions. This integrated planning approach can be facilitated by determining the least-cost mode of electrification - i.e by electric grid extension or off-grid systems - for non-electrified consumers in a region under analysis, while considering technical, economic and environmental constraints. Computational <b>clustering</b> methods the identification of consumer clusters (either as clusters of off-grid microgrids, stand-alone systems or grid-extension projects) can be undertaken using computational <b>clustering</b> methods. This paper presents a novel computational approach to achieve this purpose. This methodology involves exploiting the grid network that connects all consumers, by greedily partitioning the network to identify clusters of consumers to be electrified by grid-extension and off-grid microgrid systems. Using test cases and sensitivity analyses, we implement and <b>benchmark</b> this top-down approach with those obtained from a bottom-up <b>clustering</b> methodology used by the Reference Electrification Model, a model obtainable in literature. Results presented show that the alternative top-down methodology proposed can compare favorably, in terms of global electrification costs, with a bottom-up approach to rural electrification planning.</p></p class="citation"></blockquote><h2 id=cscy-3>cs.CY (3)</h2><h3 id=13--225320-comparing-rationality-between-large-language-models-and-humans-insights-and-open-questions-dana-alsagheer-et-al-2024>(1/3 | 225/320) Comparing Rationality Between Large Language Models and Humans: Insights and Open Questions (Dana Alsagheer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dana Alsagheer, Rabimba Karanjai, Nour Diallo, Weidong Shi, Yang Lu, Suha Beydoun, Qiaoning Zhang. (2024)<br><strong>Comparing Rationality Between Large Language Models and Humans: Insights and Open Questions</strong><br><button class=copy-to-clipboard title="Comparing Rationality Between Large Language Models and Humans: Insights and Open Questions" index=225>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-225 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CY, cs.CY<br>Keyword Score: 60<br>Keywords: Reinforcement Learning, Reinforcement Learning from Human Feedback, Reinforcement Learning from Human Feedback, Transformer, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09798v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09798v1.pdf filename=2403.09798v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper delves into the dynamic landscape of artificial intelligence, specifically focusing on the burgeoning prominence of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> We underscore the pivotal role of <b>Reinforcement</b> <b>Learning</b> <b>from</b> <b>Human</b> <b>Feedback</b> <b>(RLHF)</b> in augmenting <b>LLMs&rsquo;</b> rationality and decision-making prowess. By meticulously examining the intricate relationship between human interaction and <b>LLM</b> behavior, we explore questions surrounding rationality and performance disparities between humans and <b>LLMs,</b> with particular attention to the Chat Generative Pre-trained <b>Transformer.</b> Our research employs comprehensive comparative analysis and delves into the inherent challenges of irrationality in <b>LLMs,</b> offering valuable insights and actionable strategies for enhancing their rationality. These findings hold significant implications for the widespread adoption of <b>LLMs</b> across diverse domains and applications, underscoring their potential to catalyze advancements in artificial intelligence.</p></p class="citation"></blockquote><h3 id=23--226320-metrognn-metro-network-expansion-with-reinforcement-learning-hongyuan-su-et-al-2024>(2/3 | 226/320) MetroGNN: Metro Network Expansion with Reinforcement Learning (Hongyuan Su et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hongyuan Su, Yu Zheng, Jingtao Ding, Depeng Jin, Yong Li. (2024)<br><strong>MetroGNN: Metro Network Expansion with Reinforcement Learning</strong><br><button class=copy-to-clipboard title="MetroGNN: Metro Network Expansion with Reinforcement Learning" index=226>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-226 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: 68T09, cs-CY, cs.CY<br>Keyword Score: 33<br>Keywords: Graph, Graph Neural Network, Markov Decision Process, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09197v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09197v1.pdf filename=2403.09197v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Selecting urban regions for metro network expansion to meet maximal transportation demands is crucial for urban development, while computationally challenging to solve. The expansion process relies not only on complicated features like urban demographics and origin-destination (OD) flow but is also constrained by the existing metro network and urban geography. In this paper, we introduce a <b>reinforcement</b> <b>learning</b> framework to address a <b>Markov</b> <b>decision</b> <b>process</b> within an urban heterogeneous multi-graph. Our approach employs an attentive policy network that intelligently selects nodes based on information captured by a <b>graph</b> <b>neural</b> <b>network.</b> Experiments on real-world urban data demonstrate that our proposed methodology substantially improve the satisfied transportation demands by over 30% when compared with state-of-the-art methods. Codes are published at <a href=https://github.com/tsinghua-fib-lab/MetroGNN>https://github.com/tsinghua-fib-lab/MetroGNN</a>.</p></p class="citation"></blockquote><h3 id=33--227320-older-adults-safety-and-security-online-a-post-pandemic-exploration-of-attitudes-and-behaviors-edgar-pacheco-2024>(3/3 | 227/320) Older adults&rsquo; safety and security online: A post-pandemic exploration of attitudes and behaviors (Edgar Pacheco, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Edgar Pacheco. (2024)<br><strong>Older adults&rsquo; safety and security online: A post-pandemic exploration of attitudes and behaviors</strong><br><button class=copy-to-clipboard title="Older adults' safety and security online: A post-pandemic exploration of attitudes and behaviors" index=227>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-227 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CY, cs.CY<br>Keyword Score: 10<br>Keywords: Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09208v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09208v1.pdf filename=2403.09208v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Older adults&rsquo; growing use of the internet and related technologies, further accelerated by the COVID-19 pandemic, has <b>prompted</b> not only a critical examination of their behaviors and attitudes about online threats but also a greater understanding of the roles of specific characteristics within this population group. Based on survey data and using descriptive and inferential statistics, this empirical study delves into this matter. The behaviors and attitudes of a group of older adults aged 60 years and older (n=275) regarding different dimensions of online safety and cybersecurity are investigated. The results show that older adults report a discernible degree of concern about the security of their personal information. Despite the varied precautions taken, most of them do not know where to report online threats. What is more, regarding key demographics, the study found some significant differences in terms of gender and age group, but not disability status. This implies that older adults do not seem to constitute a homogeneous group when it comes to attitudes and behaviors regarding safety and security online. The study concludes that support systems should include older adults in the development of protective measures and acknowledge their diversity. The implications of the results are discussed and some directions for future research are proposed.</p></p class="citation"></blockquote><h2 id=csir-5>cs.IR (5)</h2><h3 id=15--228320-usimagent-large-language-models-for-simulating-search-users-erhan-zhang-et-al-2024>(1/5 | 228/320) USimAgent: Large Language Models for Simulating Search Users (Erhan Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Erhan Zhang, Xingzhu Wang, Peiyuan Gong, Yankai Lin, Jiaxin Mao. (2024)<br><strong>USimAgent: Large Language Models for Simulating Search Users</strong><br><button class=copy-to-clipboard title="USimAgent: Large Language Models for Simulating Search Users" index=228>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-228 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-AI, cs-IR, cs.IR<br>Keyword Score: 60<br>Keywords: Simulation, Simulator, Information Retrieval, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09142v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09142v1.pdf filename=2403.09142v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Due to the advantages in the cost-efficiency and reproducibility, user <b>simulation</b> has become a promising solution to the user-centric evaluation of <b>information</b> <b>retrieval</b> systems. Nonetheless, accurately simulating user search behaviors has long been a challenge, because users&rsquo; actions in search are highly complex and driven by intricate cognitive processes such as learning, <b>reasoning,</b> and planning. Recently, <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have demonstrated remarked potential in simulating human-level intelligence and have been used in building autonomous agents for various tasks. However, the potential of using <b>LLMs</b> in simulating search behaviors has not yet been fully explored. In this paper, we introduce a <b>LLM-based</b> user search behavior simulator, USimAgent. The proposed simulator can simulate users&rsquo; querying, clicking, and stopping behaviors during search, and thus, is capable of generating complete search sessions for specific search tasks. Empirical investigation on a real user behavior dataset shows that the proposed simulator outperforms existing methods in query generation and is comparable to traditional methods in predicting user clicks and stopping behaviors. These results not only validate the effectiveness of using <b>LLMs</b> for user <b>simulation</b> but also shed light on the development of a more robust and generic user simulators.</p></p class="citation"></blockquote><h3 id=25--229320-logical-discrete-graphical-models-must-supplement-large-language-models-for-information-synthesis-gregory-coppola-2024>(2/5 | 229/320) Logical Discrete Graphical Models Must Supplement Large Language Models for Information Synthesis (Gregory Coppola, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gregory Coppola. (2024)<br><strong>Logical Discrete Graphical Models Must Supplement Large Language Models for Information Synthesis</strong><br><button class=copy-to-clipboard title="Logical Discrete Graphical Models Must Supplement Large Language Models for Information Synthesis" index=229>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-229 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 30<br>Keywords: Information Retrieval, Reasoning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09599v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09599v1.pdf filename=2403.09599v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Given the emergent <b>reasoning</b> abilities of <b>large</b> <b>language</b> <b>models,</b> <b>information</b> <b>retrieval</b> is becoming more complex. Rather than just retrieve a document, modern <b>information</b> <b>retrieval</b> systems advertise that they can synthesize an answer based on potentially many different documents, conflicting data sources, and using <b>reasoning.</b> We review recent literature and argue that the <b>large</b> <b>language</b> <b>model</b> has crucial flaws that prevent it from on its own ever constituting general intelligence, or answering general <b>information</b> <b>synthesis</b> requests. This review shows that the following are problems for <b>large</b> <b>language</b> <b>models:</b> hallucinations, complex <b>reasoning,</b> planning under uncertainty, and complex calculations. We outline how logical discrete graphical models can solve all of these problems, and outline a method of training a logical discrete model from unlabeled text.</p></p class="citation"></blockquote><h3 id=35--230320-projected-gradient-descent-for-spectral-compressed-sensing-via-symmetric-hankel-factorization-jinsheng-li-et-al-2024>(3/5 | 230/320) Projected Gradient Descent for Spectral Compressed Sensing via Symmetric Hankel Factorization (Jinsheng Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinsheng Li, Wei Cui, Xu Zhang. (2024)<br><strong>Projected Gradient Descent for Spectral Compressed Sensing via Symmetric Hankel Factorization</strong><br><button class=copy-to-clipboard title="Projected Gradient Descent for Spectral Compressed Sensing via Symmetric Hankel Factorization" index=230>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-230 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09031v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09031v1.pdf filename=2403.09031v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Current spectral compressed sensing methods via Hankel matrix completion employ symmetric factorization to demonstrate the low-rank property of the Hankel matrix. However, previous non-convex gradient methods only utilize asymmetric factorization to achieve spectral compressed sensing. In this paper, we propose a novel nonconvex projected gradient descent method for spectral compressed sensing via symmetric factorization named Symmetric Hankel Projected Gradient Descent (SHGD), which updates only one matrix and avoids a balancing regularization term. SHGD reduces about half of the computation and storage costs compared to the prior gradient method based on asymmetric factorization. {Besides, the symmetric factorization employed in our work is completely novel to the prior low-rank factorization model, introducing a new factorization ambiguity under complex orthogonal transformation}. Novel distance metrics are designed for our factorization method and a linear convergence guarantee to the desired signal is established with $O(r^2\log(n))$ observations. Numerical <b>simulations</b> demonstrate the superior performance of the proposed SHGD method in phase transitions and computation efficiency compared to state-of-the-art methods.</p></p class="citation"></blockquote><h3 id=45--231320-seed-based-information-retrieval-in-networks-of-research-publications-evaluation-of-direct-citations-bibliographic-coupling-co-citations-and-pubmed-related-article-score-peter-sjögårde-et-al-2024>(4/5 | 231/320) Seed-based information retrieval in networks of research publications: Evaluation of direct citations, bibliographic coupling, co-citations and PubMed related article score (Peter Sjögårde et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Peter Sjögårde, Per Ahlgren. (2024)<br><strong>Seed-based information retrieval in networks of research publications: Evaluation of direct citations, bibliographic coupling, co-citations and PubMed related article score</strong><br><button class=copy-to-clipboard title="Seed-based information retrieval in networks of research publications: Evaluation of direct citations, bibliographic coupling, co-citations and PubMed related article score" index=231>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-231 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 10<br>Keywords: Information Retrieval<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09295v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09295v1.pdf filename=2403.09295v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this contribution, we deal with seed-based <b>information</b> <b>retrieval</b> in networks of research publications. Using systematic reviews as a baseline, and publication data from the NIH Open Citation Collection, we compare the performance of the three citation-based approaches direct citation, co-citation, and bibliographic coupling with respect to recall and precision measures. In addition, we include the PubMed Related Article score as well as combined approaches in the comparison. We also provide a fairly comprehensive review of earlier research in which citation relations have been used for <b>information</b> <b>retrieval</b> purposes. The results show an advantage for co-citation over bibliographic coupling and direct citation. However, combining the three approaches outperforms the exclusive use of co-citation in the study. The results further indicate, in line with previous research, that combining citation-based approaches with textual approaches enhances the performance of seed-based <b>information</b> <b>retrieval.</b> The results from the study may guide approaches combining citation-based and textual approaches in their choice of citation similarity measures. We suggest that future research use more structured approaches to evaluate methods for seed-based retrieval of publications, including comparative approaches as well as the elaboration of common data sets and baselines for evaluation.</p></p class="citation"></blockquote><h3 id=55--232320-online-and-offline-evaluation-in-search-clarification-leila-tavakoli-et-al-2024>(5/5 | 232/320) Online and Offline Evaluation in Search Clarification (Leila Tavakoli et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Leila Tavakoli, Johanne R. Trippas, Hamed Zamani, Falk Scholer, Mark Sanderson. (2024)<br><strong>Online and Offline Evaluation in Search Clarification</strong><br><button class=copy-to-clipboard title="Online and Offline Evaluation in Search Clarification" index=232>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-232 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 10<br>Keywords: Information Retrieval<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09180v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09180v1.pdf filename=2403.09180v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The effectiveness of clarification question models in engaging users within search systems is currently constrained, casting doubt on their overall usefulness. To improve the performance of these models, it is crucial to employ assessment approaches that encompass both real-time feedback from users (online evaluation) and the characteristics of clarification questions evaluated through human assessment (offline evaluation). However, the relationship between online and offline evaluations has been debated in <b>information</b> <b>retrieval.</b> This study aims to investigate how this discordance holds in search clarification. We use user engagement as ground truth and employ several offline labels to investigate to what extent the offline ranked lists of clarification resemble the ideal ranked lists based on online user engagement.</p></p class="citation"></blockquote><h2 id=cssi-3>cs.SI (3)</h2><h3 id=13--233320-from-skepticism-to-acceptance-simulating-the-attitude-dynamics-toward-fake-news-yuhan-liu-et-al-2024>(1/3 | 233/320) From Skepticism to Acceptance: Simulating the Attitude Dynamics Toward Fake News (Yuhan Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuhan Liu, Xiuying Chen, Xiaoqing Zhang, Xing Gao, Ji Zhang, Rui Yan. (2024)<br><strong>From Skepticism to Acceptance: Simulating the Attitude Dynamics Toward Fake News</strong><br><button class=copy-to-clipboard title="From Skepticism to Acceptance: Simulating the Attitude Dynamics Toward Fake News" index=233>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-233 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-AI, cs-CL, cs-SI, cs.SI<br>Keyword Score: 50<br>Keywords: Simulation, Simulator, Fake News Detection, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09498v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09498v1.pdf filename=2403.09498v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the digital era, the rapid propagation of <b>fake</b> <b>news</b> and rumors via social networks brings notable societal challenges and impacts public opinion regulation. Traditional <b>fake</b> <b>news</b> modeling typically forecasts the general popularity trends of different groups or numerically represents opinions shift. However, these methods often oversimplify real-world complexities and overlook the rich semantic information of news text. The advent of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> provides the possibility of modeling subtle dynamics of opinion. Consequently, in this work, we introduce a <b>Fake</b> <b>news</b> Propagation <b>Simulation</b> framework (FPS) based on <b>LLM,</b> which studies the trends and control of <b>fake</b> <b>news</b> propagation in detail. Specifically, each agent in the <b>simulation</b> represents an individual with a distinct personality. They are equipped with both short-term and long-term memory, as well as a reflective mechanism to mimic human-like thinking. Every day, they engage in random opinion exchanges, reflect on their thinking, and update their opinions. Our <b>simulation</b> results uncover patterns in <b>fake</b> <b>news</b> propagation related to topic relevance, and individual traits, aligning with real-world observations. Additionally, we evaluate various intervention strategies and demonstrate that early and appropriately frequent interventions strike a balance between governance cost and effectiveness, offering valuable insights for practical applications. Our study underscores the significant utility and potential of <b>LLMs</b> in combating <b>fake</b> <b>news.</b></p></p class="citation"></blockquote><h3 id=23--234320-rumor-mitigation-in-social-media-platforms-with-deep-reinforcement-learning-hongyuan-su-et-al-2024>(2/3 | 234/320) Rumor Mitigation in Social Media Platforms with Deep Reinforcement Learning (Hongyuan Su et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hongyuan Su, Yu Zheng, Jingtao Ding, Depeng Jin, Yong Li. (2024)<br><strong>Rumor Mitigation in Social Media Platforms with Deep Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Rumor Mitigation in Social Media Platforms with Deep Reinforcement Learning" index=234>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-234 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: 68T09, cs-SI, cs.SI<br>Keyword Score: 23<br>Keywords: Graph, Graph Neural Network, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09217v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09217v1.pdf filename=2403.09217v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Social media platforms have become one of the main channels where people disseminate and acquire information, of which the reliability is severely threatened by rumors widespread in the network. Existing approaches such as suspending users or broadcasting real information to combat rumors are either with high cost or disturbing users. In this paper, we introduce a novel rumor mitigation paradigm, where only a minimal set of links in the social network are intervened to decelerate the propagation of rumors, countering misinformation with low business cost and user awareness. A knowledge-informed agent embodying rumor propagation mechanisms is developed, which intervenes the social network with a <b>graph</b> <b>neural</b> <b>network</b> for capturing information flow in the social media platforms and a policy network for selecting links. Experiments on real social media platforms demonstrate that the proposed approach can effectively alleviate the influence of rumors, substantially reducing the affected populations by over 25%. Codes for this paper are released at <a href=https://github.com/tsinghua-fib-lab/DRL-Rumor-Mitigation>https://github.com/tsinghua-fib-lab/DRL-Rumor-Mitigation</a>.</p></p class="citation"></blockquote><h3 id=33--235320-belief-and-persuasion-in-scientific-discourse-on-social-media-a-study-of-the-covid-19-pandemic-salwa-alamir-et-al-2024>(3/3 | 235/320) Belief and Persuasion in Scientific Discourse on Social Media: A Study of the COVID-19 Pandemic (Salwa Alamir et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Salwa Alamir, Armineh Nourbakhsh, Cecilia Tilli, Sameena Shah, Manuela Veloso. (2024)<br><strong>Belief and Persuasion in Scientific Discourse on Social Media: A Study of the COVID-19 Pandemic</strong><br><button class=copy-to-clipboard title="Belief and Persuasion in Scientific Discourse on Social Media: A Study of the COVID-19 Pandemic" index=235>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-235 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-SI, cs.SI, physics-soc-ph<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09260v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09260v1.pdf filename=2403.09260v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Research into COVID-19 has been rapidly evolving since the onset of the pandemic. This occasionally results in contradictory <b>recommendations</b> by credible sources of scientific opinion, public health authorities, and medical professionals. In this study, we examine whether this has resulted in a lack of trust in scientific opinion, by examining the belief patterns of social media users and their reactions to statements related to scientific facts. We devise models to mine belief and persuasion in Twitter discourse using semi-supervised approaches, and show the relationship between lack of belief and insurgence of paranoia and conspiracy theories. By investigating these belief patterns, we explore the best persuasion tactics for communicating information related to COVID-19.</p></p class="citation"></blockquote><h2 id=cscr-12>cs.CR (12)</h2><h3 id=112--236320-adashield-safeguarding-multimodal-large-language-models-from-structure-based-attack-via-adaptive-shield-prompting-yu-wang-et-al-2024>(1/12 | 236/320) AdaShield: Safeguarding Multimodal Large Language Models from Structure-based Attack via Adaptive Shield Prompting (Yu Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yu Wang, Xiaogeng Liu, Yu Li, Muhao Chen, Chaowei Xiao. (2024)<br><strong>AdaShield: Safeguarding Multimodal Large Language Models from Structure-based Attack via Adaptive Shield Prompting</strong><br><button class=copy-to-clipboard title="AdaShield: Safeguarding Multimodal Large Language Models from Structure-based Attack via Adaptive Shield Prompting" index=236>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-236 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs.CR<br>Keyword Score: 46<br>Keywords: Fine-tuning, Multi-modal, Multi-modal, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09513v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09513v1.pdf filename=2403.09513v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the advent and widespread deployment of <b>Multimodal</b> <b>Large</b> <b>Language</b> <b>Models</b> (MLLMs), the imperative to ensure their safety has become increasingly pronounced. However, with the integration of additional modalities, MLLMs are exposed to new vulnerabilities, rendering them prone to structured-based jailbreak attacks, where semantic content (e.g., &ldquo;harmful text&rdquo;) has been injected into the images to mislead MLLMs. In this work, we aim to defend against such threats. Specifically, we propose \textbf{Ada}ptive \textbf{Shield} <b>Prompting</b> (\textbf{AdaShield}), which prepends inputs with defense <b>prompts</b> to defend MLLMs against structure-based jailbreak attacks without <b>fine-tuning</b> MLLMs or training additional modules (e.g., post-stage content detector). Initially, we present a manually designed static defense <b>prompt,</b> which thoroughly examines the image and instruction content step by step and specifies response methods to malicious queries. Furthermore, we introduce an adaptive auto-refinement framework, consisting of a target MLLM and a <b>LLM-based</b> defense <b>prompt</b> generator (Defender). These components collaboratively and iteratively communicate to generate a defense <b>prompt.</b> Extensive experiments on the popular structure-based jailbreak attacks and benign datasets show that our methods can consistently improve MLLMs&rsquo; robustness against structure-based jailbreak attacks without compromising the model&rsquo;s general capabilities evaluated on standard benign tasks. Our code is available at <a href=https://github.com/rain305f/AdaShield>https://github.com/rain305f/AdaShield</a>.</p></p class="citation"></blockquote><h3 id=212--237320-graph-based-ddos-attack-detection-in-iot-systems-with-lossy-network-arvin-hekmati-et-al-2024>(2/12 | 237/320) Graph-Based DDoS Attack Detection in IoT Systems with Lossy Network (Arvin Hekmati et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Arvin Hekmati, Bhaskar Krishnamachari. (2024)<br><strong>Graph-Based DDoS Attack Detection in IoT Systems with Lossy Network</strong><br><button class=copy-to-clipboard title="Graph-Based DDoS Attack Detection in IoT Systems with Lossy Network" index=237>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-237 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 43<br>Keywords: Graph Convolutional Network, Graph Convolutional Network, Graph, Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09118v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09118v1.pdf filename=2403.09118v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study introduces a robust solution for the detection of Distributed Denial of Service (DDoS) attacks in Internet of Things (IoT) systems, leveraging the capabilities of <b>Graph</b> <b>Convolutional</b> <b>Networks</b> <b>(GCN).</b> By conceptualizing IoT devices as nodes within a <b>graph</b> <b>structure,</b> <b>we</b> present a detection mechanism capable of operating efficiently even in lossy network environments. We introduce various <b>graph</b> <b>topologies</b> <b>for</b> modeling IoT networks and evaluate them for detecting tunable futuristic DDoS attacks. By studying different levels of network connection loss and various attack situations, we demonstrate that the correlation-based hybrid <b>graph</b> <b>structure</b> <b>is</b> effective in spotting DDoS attacks, substantiating its good performance even in lossy network scenarios. The results indicate a remarkable performance of the <b>GCN-based</b> DDoS detection model with an F1 score of up to 91%. Furthermore, we observe at most a 2% drop in F1-score in environments with up to 50% connection loss. The findings from this study highlight the advantages of utilizing <b>GCN</b> for the security of IoT systems which benefit from high detection accuracy while being resilient to connection disruption.</p></p class="citation"></blockquote><h3 id=312--238320-helpful-or-harmful-exploring-the-efficacy-of-large-language-models-for-online-grooming-prevention-ellie-prosser-et-al-2024>(3/12 | 238/320) Helpful or Harmful? Exploring the Efficacy of Large Language Models for Online Grooming Prevention (Ellie Prosser et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ellie Prosser, Matthew Edwards. (2024)<br><strong>Helpful or Harmful? Exploring the Efficacy of Large Language Models for Online Grooming Prevention</strong><br><button class=copy-to-clipboard title="Helpful or Harmful? Exploring the Efficacy of Large Language Models for Online Grooming Prevention" index=238>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-238 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CL, cs-CR, cs.CR<br>Keyword Score: 40<br>Keywords: Question Answering, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09795v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09795v1.pdf filename=2403.09795v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Powerful generative <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> are becoming popular tools amongst the general public as <b>question-answering</b> <b>systems,</b> and are being utilised by vulnerable groups such as children. With children increasingly interacting with these tools, it is imperative for researchers to scrutinise the safety of <b>LLMs,</b> especially for applications that could lead to serious outcomes, such as online child safety queries. In this paper, the efficacy of <b>LLMs</b> for online grooming prevention is explored both for identifying and avoiding grooming through advice generation, and the impact of <b>prompt</b> design on model performance is investigated by varying the provided context and <b>prompt</b> specificity. In results reflecting over 6,000 <b>LLM</b> interactions, we find that no models were clearly appropriate for online grooming prevention, with an observed lack of consistency in behaviours, and potential for harmful answer generation, especially from open-source models. We outline where and how models fall short, providing suggestions for improvement, and identify <b>prompt</b> designs that heavily altered model performance in troubling ways, with findings that can be used to inform best practice usage guides.</p></p class="citation"></blockquote><h3 id=412--239320-what-was-your-prompt-a-remote-keylogging-attack-on-ai-assistants-roy-weiss-et-al-2024>(4/12 | 239/320) What Was Your Prompt? A Remote Keylogging Attack on AI Assistants (Roy Weiss et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Roy Weiss, Daniel Ayzenshteyn, Guy Amit, Yisroel Mirsky. (2024)<br><strong>What Was Your Prompt? A Remote Keylogging Attack on AI Assistants</strong><br><button class=copy-to-clipboard title="What Was Your Prompt? A Remote Keylogging Attack on AI Assistants" index=239>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-239 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CL, cs-CR, cs.CR<br>Keyword Score: 40<br>Keywords: Fine-tuning, ChatGPT, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09751v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09751v1.pdf filename=2403.09751v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>AI assistants are becoming an integral part of society, used for asking advice or help in personal and confidential issues. In this paper, we unveil a novel side-channel that can be used to read encrypted responses from AI Assistants over the web: the token-length side-channel. We found that many vendors, including OpenAI and Microsoft, have this side-channel. However, inferring the content of a response from a token-length sequence alone proves challenging. This is because tokens are akin to words, and responses can be several sentences long leading to millions of grammatically correct sentences. In this paper, we show how this can be overcome by (1) utilizing the power of a <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> to translate these sequences, (2) providing the <b>LLM</b> with inter-sentence context to narrow the search space and (3) performing a known-plaintext attack by <b>fine-tuning</b> the model on the target model&rsquo;s writing style. Using these methods, we were able to accurately reconstruct 29% of an AI assistant&rsquo;s responses and successfully infer the topic from 55% of them. To demonstrate the threat, we performed the attack on OpenAI&rsquo;s <b>ChatGPT-4</b> and Microsoft&rsquo;s Copilot on both browser and API traffic.</p></p class="citation"></blockquote><h3 id=512--240320-precurious-how-innocent-pre-trained-language-models-turn-into-privacy-traps-ruixuan-liu-et-al-2024>(5/12 | 240/320) PreCurious: How Innocent Pre-Trained Language Models Turn into Privacy Traps (Ruixuan Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruixuan Liu, Tianhao Wang, Yang Cao, Li Xiong. (2024)<br><strong>PreCurious: How Innocent Pre-Trained Language Models Turn into Privacy Traps</strong><br><button class=copy-to-clipboard title="PreCurious: How Innocent Pre-Trained Language Models Turn into Privacy Traps" index=240>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-240 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 35<br>Keywords: Black Box, Fine-tuning, Fine-tuning, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09562v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09562v1.pdf filename=2403.09562v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The pre-training and <b>fine-tuning</b> paradigm has demonstrated its effectiveness and has become the standard approach for tailoring language models to various tasks. Currently, community-based platforms offer easy access to various <b>pre-trained</b> <b>models,</b> <b>as</b> anyone can publish without strict validation processes. However, a released <b>pre-trained</b> <b>model</b> <b>can</b> be a privacy trap for <b>fine-tuning</b> datasets if it is carefully designed. In this work, we propose PreCurious framework to reveal the new attack surface where the attacker releases the <b>pre-trained</b> <b>model</b> <b>and</b> gets a <b>black-box</b> <b>access</b> to the final <b>fine-tuned</b> model. PreCurious aims to escalate the general privacy risk of both membership inference and data extraction. The key intuition behind PreCurious is to manipulate the memorization stage of the <b>pre-trained</b> <b>model</b> <b>and</b> guide <b>fine-tuning</b> with a seemingly legitimate configuration. The effectiveness of defending against privacy attacks on a <b>fine-tuned</b> model seems promising, as empirical and theoretical evidence suggests that parameter-efficient and differentially private <b>fine-tuning</b> techniques are invulnerable to privacy attacks. But PreCurious demonstrates the possibility of breaking up invulnerability in a stealthy manner compared to <b>fine-tuning</b> on a benign model. By further leveraging a sanitized dataset, PreCurious can extract originally unexposed secrets under differentially private <b>fine-tuning.</b> Thus, PreCurious raises warnings for users who download <b>pre-trained</b> <b>models</b> <b>from</b> unknown sources, rely solely on tutorials or common-sense defenses, and previously release sanitized datasets even after perfect scrubbing.</p></p class="citation"></blockquote><h3 id=612--241320-optimistic-verifiable-training-by-controlling-hardware-nondeterminism-megha-srivastava-et-al-2024>(6/12 | 241/320) Optimistic Verifiable Training by Controlling Hardware Nondeterminism (Megha Srivastava et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Megha Srivastava, Simran Arora, Dan Boneh. (2024)<br><strong>Optimistic Verifiable Training by Controlling Hardware Nondeterminism</strong><br><button class=copy-to-clipboard title="Optimistic Verifiable Training by Controlling Hardware Nondeterminism" index=241>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-241 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs-LG, cs.CR<br>Keyword Score: 30<br>Keywords: Fine-tuning, GPT, GPT-2<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09603v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09603v2.pdf filename=2403.09603v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The increasing compute demands of AI systems has led to the emergence of services that train models on behalf of clients lacking necessary resources. However, ensuring correctness of training and guarding against potential training-time attacks, such as data poisoning, poses challenges. Existing works on verifiable training largely fall into two classes: proof-based systems, which struggle to scale due to requiring cryptographic techniques, and &ldquo;optimistic&rdquo; methods that consider a trusted third-party auditor who replicates the training process. A key challenge with the latter is that hardware nondeterminism between GPU types during training prevents an auditor from replicating the training process exactly, and such schemes are therefore non-robust. We propose a method that combines training in a higher precision than the target model, rounding after intermediate computation steps, and storing rounding decisions based on an adaptive thresholding procedure, to successfully control for nondeterminism. Across three different NVIDIA GPUs (A40, Titan XP, RTX 2080 Ti), we achieve exact training replication at FP32 precision for both full-training and <b>fine-tuning</b> of ResNet-50 (23M) and <b>GPT-2</b> (117M) models. Our verifiable training scheme significantly decreases the storage and time costs compared to proof-based systems.</p></p class="citation"></blockquote><h3 id=712--242320-symbiotic-game-and-foundation-models-for-cyber-deception-operations-in-strategic-cyber-warfare-tao-li-et-al-2024>(7/12 | 242/320) Symbiotic Game and Foundation Models for Cyber Deception Operations in Strategic Cyber Warfare (Tao Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tao Li, Quanyan Zhu. (2024)<br><strong>Symbiotic Game and Foundation Models for Cyber Deception Operations in Strategic Cyber Warfare</strong><br><button class=copy-to-clipboard title="Symbiotic Game and Foundation Models for Cyber Deception Operations in Strategic Cyber Warfare" index=242>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-242 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs-GT, cs.CR<br>Keyword Score: 20<br>Keywords: Foundation Model, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10570v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10570v1.pdf filename=2403.10570v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We are currently facing unprecedented cyber warfare with the rapid evolution of tactics, increasing asymmetry of intelligence, and the growing accessibility of hacking tools. In this landscape, cyber deception emerges as a critical component of our defense strategy against increasingly sophisticated attacks. This chapter aims to highlight the pivotal role of game-theoretic models and <b>foundation</b> <b>models</b> (FMs) in analyzing, designing, and implementing cyber deception tactics. Game models (GMs) serve as a <b>foundational</b> <b>framework</b> for modeling diverse adversarial interactions, allowing us to encapsulate both adversarial knowledge and domain-specific insights. Meanwhile, FMs serve as the building blocks for creating tailored machine learning models suited to given applications. By leveraging the synergy between GMs and FMs, we can advance proactive and automated cyber defense mechanisms by not only securing our networks against attacks but also enhancing their resilience against well-planned operations. This chapter discusses the games at the tactical, operational, and strategic levels of warfare, delves into the symbiotic relationship between these methodologies, and explores relevant applications where such a framework can make a substantial impact in cybersecurity. The chapter discusses the promising direction of the multi-agent neurosymbolic conjectural learning (MANSCOL), which allows the defender to predict adversarial behaviors, design adaptive defensive deception tactics, and synthesize knowledge for the operational level synthesis and adaptation. FMs serve as pivotal tools across various functions for MANSCOL, including <b>reinforcement</b> <b>learning,</b> knowledge assimilation, formation of conjectures, and contextual representation. This chapter concludes with a discussion of the challenges associated with FMs and their application in the domain of cybersecurity.</p></p class="citation"></blockquote><h3 id=812--243320-privacy-preserving-anomaly-detection-on-homomorphic-encrypted-data-from-iot-sensors-anca-hangan-et-al-2024>(8/12 | 243/320) Privacy Preserving Anomaly Detection on Homomorphic Encrypted Data from IoT Sensors (Anca Hangan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anca Hangan, Dragos Lazea, Tudor Cioara. (2024)<br><strong>Privacy Preserving Anomaly Detection on Homomorphic Encrypted Data from IoT Sensors</strong><br><button class=copy-to-clipboard title="Privacy Preserving Anomaly Detection on Homomorphic Encrypted Data from IoT Sensors" index=243>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-243 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 20<br>Keywords: Anomaly Detection, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09322v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09322v1.pdf filename=2403.09322v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>IoT devices have become indispensable components of our lives, and the advancement of AI technologies will make them even more pervasive, increasing the vulnerability to malfunctions or cyberattacks and raising privacy concerns. Encryption can mitigate these challenges; however, most existing <b>anomaly</b> <b>detection</b> techniques decrypt the data to perform the analysis, potentially undermining the encryption protection provided during transit or storage. Homomorphic encryption schemes are promising solutions as they enable the processing and execution of operations on IoT data while still encrypted, however, these schemes offer only limited operations, which poses challenges to their practical usage. In this paper, we propose a novel privacy-preserving <b>anomaly</b> <b>detection</b> solution designed for homomorphically encrypted data generated by IoT devices that efficiently detects abnormal values without performing decryption. We have adapted the Histogram-based <b>anomaly</b> <b>detection</b> technique for TFHE scheme to address limitations related to the input size and the depth of computation by implementing vectorized support operations. These operations include addition, value placement in buckets, labeling abnormal buckets based on a threshold frequency, labeling abnormal values based on their range, and bucket labels. Evaluation results show that the solution effectively detects anomalies without requiring data decryption and achieves consistent results comparable to the mechanism operating on plain data. Also, it shows robustness and resilience against various challenges commonly encountered in IoT environments, such as noisy sensor data, <b>adversarial</b> <b>attacks,</b> communication failures, and device malfunctions. Moreover, the time and computational overheads determined for several solution configurations, despite being large, are reasonable compared to those reported in existing literature.</p></p class="citation"></blockquote><h3 id=912--244320-explainable-machine-learning-based-security-and-privacy-protection-framework-for-internet-of-medical-things-systems-ayoub-si-ahmed-et-al-2024>(9/12 | 244/320) Explainable Machine Learning-Based Security and Privacy Protection Framework for Internet of Medical Things Systems (Ayoub Si-ahmed et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ayoub Si-ahmed, Mohammed Ali Al-Garadi, Narhimene Boustia. (2024)<br><strong>Explainable Machine Learning-Based Security and Privacy Protection Framework for Internet of Medical Things Systems</strong><br><button class=copy-to-clipboard title="Explainable Machine Learning-Based Security and Privacy Protection Framework for Internet of Medical Things Systems" index=244>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-244 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs.CR<br>Keyword Score: 20<br>Keywords: Anomaly Detection, Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09752v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09752v1.pdf filename=2403.09752v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Internet of Medical Things (IoMT) transcends traditional medical boundaries, enabling a transition from reactive treatment to proactive prevention. This innovative method revolutionizes healthcare by facilitating early disease detection and tailored care, particularly in chronic disease management, where IoMT automates treatments based on real-time health data collection. Nonetheless, its benefits are countered by significant security challenges that endanger the lives of its users due to the sensitivity and value of the processed data, thereby attracting malicious interests. Moreover, the utilization of wireless communication for data transmission exposes medical data to interception and tampering by cybercriminals. Additionally, anomalies may arise due to human errors, network interference, or hardware malfunctions. In this context, <b>anomaly</b> <b>detection</b> based on Machine Learning (ML) is an interesting solution, but it comes up against obstacles in terms of explicability and protection of privacy. To address these challenges, a new framework for Intrusion Detection Systems (IDS) is introduced, leveraging Artificial Neural Networks (ANN) for intrusion detection while utilizing <b>Federated</b> <b>Learning</b> (FL) for privacy preservation. Additionally, eXplainable Artificial Intelligence (XAI) methods are incorporated to enhance model explanation and interpretation. The efficacy of the proposed framework is evaluated and compared with centralized approaches using multiple datasets containing network and medical data, simulating various attack types impacting the confidentiality, integrity, and availability of medical and physiological data. The results obtained offer compelling evidence that the FL method performs comparably to the centralized method, demonstrating high performance. Additionally, it affords the dual advantage of safeguarding privacy and providing model explanation.</p></p class="citation"></blockquote><h3 id=1012--245320-counter-samples-a-stateless-strategy-to-neutralize-black-box-adversarial-attacks-roey-bokobza-et-al-2024>(10/12 | 245/320) Counter-Samples: A Stateless Strategy to Neutralize Black Box Adversarial Attacks (Roey Bokobza et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Roey Bokobza, Yisroel Mirsky. (2024)<br><strong>Counter-Samples: A Stateless Strategy to Neutralize Black Box Adversarial Attacks</strong><br><button class=copy-to-clipboard title="Counter-Samples: A Stateless Strategy to Neutralize Black Box Adversarial Attacks" index=245>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-245 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs-LG, cs.CR<br>Keyword Score: 15<br>Keywords: Black Box, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10562v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10562v1.pdf filename=2403.10562v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Our paper presents a novel defence against <b>black</b> <b>box</b> attacks, where attackers use the victim model as an oracle to craft their <b>adversarial</b> <b>examples.</b> Unlike traditional preprocessing defences that rely on sanitizing input samples, our stateless strategy counters the attack process itself. For every query we evaluate a counter-sample instead, where the counter-sample is the original sample optimized against the attacker&rsquo;s objective. By countering every <b>black</b> <b>box</b> query with a targeted white box optimization, our strategy effectively introduces an asymmetry to the game to the defender&rsquo;s advantage. This defence not only effectively misleads the attacker&rsquo;s search for an <b>adversarial</b> <b>example,</b> it also preserves the model&rsquo;s accuracy on legitimate inputs and is generic to multiple types of attacks. We demonstrate that our approach is remarkably effective against state-of-the-art <b>black</b> <b>box</b> attacks and outperforms existing defences for both the CIFAR-10 and ImageNet datasets. Additionally, we also show that the proposed defence is robust against strong adversaries as well.</p></p class="citation"></blockquote><h3 id=1112--246320-lan-learning-adaptive-neighbors-for-real-time-insider-threat-detection-xiangrui-cai-et-al-2024>(11/12 | 246/320) LAN: Learning Adaptive Neighbors for Real-Time Insider Threat Detection (Xiangrui Cai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiangrui Cai, Yang Wang, Sihan Xu, Hao Li, Ying Zhang, Zheli Liu, Xiaojie Yuan. (2024)<br><strong>LAN: Learning Adaptive Neighbors for Real-Time Insider Threat Detection</strong><br><button class=copy-to-clipboard title="LAN: Learning Adaptive Neighbors for Real-Time Insider Threat Detection" index=246>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-246 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs-LG, cs.CR<br>Keyword Score: 13<br>Keywords: Graph, Anomaly Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09209v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09209v2.pdf filename=2403.09209v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Enterprises and organizations are faced with potential threats from insider employees that may lead to serious consequences. Previous studies on insider threat detection (ITD) mainly focus on detecting abnormal users or abnormal time periods (e.g., a week or a day). However, a user may have hundreds of thousands of activities in the log, and even within a day there may exist thousands of activities for a user, requiring a high investigation budget to verify abnormal users or activities given the detection results. On the other hand, existing works are mainly post-hoc methods rather than real-time detection, which can not report insider threats in time before they cause loss. In this paper, we conduct the first study towards real-time ITD at activity level, and present a fine-grained and efficient framework LAN. Specifically, LAN simultaneously learns the temporal dependencies within an activity sequence and the relationships between activities across sequences with <b>graph</b> structure learning. Moreover, to mitigate the data imbalance problem in ITD, we propose a novel hybrid prediction loss, which integrates self-supervision signals from normal activities and supervision signals from abnormal activities into a unified loss for <b>anomaly</b> <b>detection.</b> We evaluate the performance of LAN on two widely used datasets, i.e., CERT r4.2 and CERT r5.2. Extensive and comparative experiments demonstrate the superiority of LAN, outperforming 9 state-of-the-art baselines by at least 9.92% and 6.35% in AUC for real-time ITD on CERT r4.2 and r5.2, respectively. Moreover, LAN can be also applied to post-hoc ITD, surpassing 8 competitive baselines by at least 7.70% and 4.03% in AUC on two datasets. Finally, the ablation study, parameter analysis, and compatibility analysis evaluate the impact of each module and hyper-parameter in LAN. The source code can be obtained from <a href=https://github.com/Li1Neo/LAN>https://github.com/Li1Neo/LAN</a>.</p></p class="citation"></blockquote><h3 id=1212--247320-ldprecover-recovering-frequencies-from-poisoning-attacks-against-local-differential-privacy-xinyue-sun-et-al-2024>(12/12 | 247/320) LDPRecover: Recovering Frequencies from Poisoning Attacks against Local Differential Privacy (Xinyue Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinyue Sun, Qingqing Ye, Haibo Hu, Jiawei Duan, Tianyu Wo, Jie Xu, Renyu Yang. (2024)<br><strong>LDPRecover: Recovering Frequencies from Poisoning Attacks against Local Differential Privacy</strong><br><button class=copy-to-clipboard title="LDPRecover: Recovering Frequencies from Poisoning Attacks against Local Differential Privacy" index=247>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-247 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 10<br>Keywords: Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09351v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09351v1.pdf filename=2403.09351v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Local <b>differential</b> <b>privacy</b> (LDP), which enables an untrusted server to collect aggregated statistics from distributed users while protecting the privacy of those users, has been widely deployed in practice. However, LDP protocols for frequency estimation are vulnerable to poisoning attacks, in which an attacker can poison the aggregated frequencies by manipulating the data sent from malicious users. Therefore, it is an open challenge to recover the accurate aggregated frequencies from poisoned ones. In this work, we propose LDPRecover, a method that can recover accurate aggregated frequencies from poisoning attacks, even if the server does not learn the details of the attacks. In LDPRecover, we establish a genuine frequency estimator that theoretically guides the server to recover the frequencies aggregated from genuine users&rsquo; data by eliminating the impact of malicious users&rsquo; data in poisoned frequencies. Since the server has no idea of the attacks, we propose an adaptive attack to unify existing attacks and learn the statistics of the malicious data within this adaptive attack by exploiting the properties of LDP protocols. By taking the estimator and the learning statistics as constraints, we formulate the problem of recovering aggregated frequencies to approach the genuine ones as a constraint inference (CI) problem. Consequently, the server can obtain accurate aggregated frequencies by solving this problem optimally. Moreover, LDPRecover can serve as a frequency recovery paradigm that recovers more accurate aggregated frequencies by integrating attack details as new constraints in the CI problem. Our evaluation on two real-world datasets, three LDP protocols, and untargeted and targeted poisoning attacks shows that LDPRecover is both accurate and widely applicable against various poisoning attacks.</p></p class="citation"></blockquote><h2 id=eessiv-13>eess.IV (13)</h2><h3 id=113--248320-fastsam3d-an-efficient-segment-anything-model-for-3d-volumetric-medical-images-yiqing-shen-et-al-2024>(1/13 | 248/320) FastSAM3D: An Efficient Segment Anything Model for 3D Volumetric Medical Images (Yiqing Shen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yiqing Shen, Jingxing Li, Xinyuan Shao, Blanca Inigo Romillo, Ankush Jindal, David Dreizin, Mathias Unberath. (2024)<br><strong>FastSAM3D: An Efficient Segment Anything Model for 3D Volumetric Medical Images</strong><br><button class=copy-to-clipboard title="FastSAM3D: An Efficient Segment Anything Model for 3D Volumetric Medical Images" index=248>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-248 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 40<br>Keywords: Knowledge Distillation, Knowledge Transfer, Zero-shot, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09827v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09827v1.pdf filename=2403.09827v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Segment anything models (SAMs) are gaining attention for their <b>zero-shot</b> generalization capability in segmenting objects of unseen classes and in unseen domains when properly <b>prompted.</b> Interactivity is a key strength of SAMs, allowing users to iteratively provide <b>prompts</b> that specify objects of interest to refine outputs. However, to realize the interactive use of SAMs for 3D medical imaging tasks, rapid inference times are necessary. High memory requirements and long processing delays remain constraints that hinder the adoption of SAMs for this purpose. Specifically, while 2D SAMs applied to 3D volumes contend with repetitive computation to process all slices independently, 3D SAMs suffer from an exponential increase in model parameters and FLOPS. To address these challenges, we present FastSAM3D which accelerates SAM inference to 8 milliseconds per 128<em>128</em>128 3D volumetric image on an NVIDIA A100 GPU. This speedup is accomplished through 1) a novel layer-wise progressive <b>distillation</b> scheme that enables <b>knowledge</b> <b>transfer</b> from a complex 12-layer ViT-B to a lightweight 6-layer ViT-Tiny variant encoder without training from scratch; and 2) a novel 3D sparse flash attention to replace vanilla attention operators, substantially reducing memory needs and improving parallelization. Experiments on three diverse datasets reveal that FastSAM3D achieves a remarkable speedup of 527.38x compared to 2D SAMs and 8.75x compared to 3D SAMs on the same volumes without significant performance decline. Thus, FastSAM3D opens the door for low-cost truly interactive SAM-based 3D medical imaging segmentation with commonly used GPU hardware. Code is available at <a href=https://github.com/arcadelab/FastSAM3D>https://github.com/arcadelab/FastSAM3D</a>.</p></p class="citation"></blockquote><h3 id=213--249320-predicting-generalization-of-ai-colonoscopy-models-to-unseen-data-joel-shor-et-al-2024>(2/13 | 249/320) Predicting Generalization of AI Colonoscopy Models to Unseen Data (Joel Shor et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Joel Shor, Carson McNeil, Yotam Intrator, Joseph R Ledsam, Hiro-o Yamano, Daisuke Tsurumaru, Hiroki Kayama, Atsushi Hamabe, Koji Ando, Mitsuhiko Ota, Haruei Ogino, Hiroshi Nakase, Kaho Kobayashi, Masaaki Miyo, Eiji Oki, Ichiro Takemasa, Ehud Rivlin, Roman Goldenberg. (2024)<br><strong>Predicting Generalization of AI Colonoscopy Models to Unseen Data</strong><br><button class=copy-to-clipboard title="Predicting Generalization of AI Colonoscopy Models to Unseen Data" index=249>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-249 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-AI, cs-CV, cs-CY, eess-IV, eess.IV<br>Keyword Score: 30<br>Keywords: Distribution Shift, Distribution Shift, Out-of-domain, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09920v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09920v2.pdf filename=2403.09920v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Background: Generalizability of AI colonoscopy algorithms is important for wider adoption in clinical practice. However, current techniques for evaluating performance on unseen data require expensive and time-intensive labels. Methods: We use a &ldquo;Masked Siamese Network&rdquo; (MSN) to identify novel phenomena in unseen data and predict polyp detector performance. MSN is trained to predict masked out regions of polyp images, without any labels. We test MSN&rsquo;s ability to be trained on data only from Israel and detect unseen techniques, narrow-band imaging (NBI) and chromendoscoy (CE), on colonoscopes from Japan (354 videos, 128 hours). We also test MSN&rsquo;s ability to predict performance of Computer Aided Detection (CADe) of polyps on colonoscopies from both countries, even though MSN is not trained on data from Japan. Results: MSN correctly identifies NBI and CE as less similar to Israel whitelight than Japan whitelight (bootstrapped z-test, |z| > 496, p &lt; 10^-8 for both) using the label-free Frechet distance. MSN detects NBI with 99% accuracy, predicts CE better than our heuristic (90% vs 79% accuracy) despite being trained only on whitelight, and is the only method that is robust to noisy labels. MSN predicts CADe polyp detector performance on in-domain Israel and <b>out-of-domain</b> Japan colonoscopies (r=0.79, 0.37 respectively). With few examples of Japan detector performance to train on, MSN prediction of Japan performance improves (r=0.56). Conclusion: Our technique can identify <b>distribution</b> <b>shifts</b> in clinical data and can predict CADe detector performance on unseen data, without labels. Our <b>self-supervised</b> approach can aid in detecting when data in practice is different from training, such as between hospitals or data has meaningfully shifted from training. MSN has potential for application to medical image domains beyond colonoscopy.</p></p class="citation"></blockquote><h3 id=313--250320-reconstructing-blood-flow-in-data-poor-regimes-a-vasculature-network-kernel-for-gaussian-process-regression-shaghayegh-z-ashtiani-et-al-2024>(3/13 | 250/320) Reconstructing Blood Flow in Data-Poor Regimes: A Vasculature Network Kernel for Gaussian Process Regression (Shaghayegh Z. Ashtiani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shaghayegh Z. Ashtiani, Mohammad Sarabian, Kaveh Laksari, Hessam Babaee. (2024)<br><strong>Reconstructing Blood Flow in Data-Poor Regimes: A Vasculature Network Kernel for Gaussian Process Regression</strong><br><button class=copy-to-clipboard title="Reconstructing Blood Flow in Data-Poor Regimes: A Vasculature Network Kernel for Gaussian Process Regression" index=250>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-250 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-LG, eess-IV, eess.IV<br>Keyword Score: 30<br>Keywords: Gaussian Process, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09758v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09758v1.pdf filename=2403.09758v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Blood flow reconstruction in the vasculature is important for many clinical applications. However, in clinical settings, the available data are often quite limited. For instance, Transcranial Doppler ultrasound (TCD) is a noninvasive clinical tool that is commonly used in the clinical settings to measure blood velocity waveform at several locations on brain&rsquo;s vasculature. This amount of data is grossly insufficient for training machine learning surrogate models, such as deep neural networks or <b>Gaussian</b> <b>process</b> regression. In this work, we propose a <b>Gaussian</b> <b>process</b> regression approach based on physics-informed kernels, enabling near-real-time reconstruction of blood flow in data-poor regimes. We introduce a novel methodology to reconstruct the kernel within the vascular network, which is a non-Euclidean space. The proposed kernel encodes both spatiotemporal and vessel-to-vessel correlations, thus enabling blood flow reconstruction in vessels that lack direct measurements. We demonstrate that any prediction made with the proposed kernel satisfies the conservation of mass principle. The kernel is constructed by running stochastic one-dimensional blood flow <b>simulations,</b> where the stochasticity captures the epistemic uncertainties, such as lack of knowledge about boundary conditions and uncertainties in vasculature geometries. We demonstrate the performance of the model on three test cases, namely, a simple Y-shaped bifurcation, abdominal aorta, and the Circle of Willis in the brain.</p></p class="citation"></blockquote><h3 id=413--251320-xreal-realistic-anatomy-and-pathology-aware-x-ray-generation-via-controllable-diffusion-model-anees-ur-rehman-hashmi-et-al-2024>(4/13 | 251/320) XReal: Realistic Anatomy and Pathology-Aware X-ray Generation via Controllable Diffusion Model (Anees Ur Rehman Hashmi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anees Ur Rehman Hashmi, Ibrahim Almakky, Mohammad Areeb Qazi, Santosh Sanjeev, Vijay Ram Papineni, Dwarikanath Mahapatra, Mohammad Yaqub. (2024)<br><strong>XReal: Realistic Anatomy and Pathology-Aware X-ray Generation via Controllable Diffusion Model</strong><br><button class=copy-to-clipboard title="XReal: Realistic Anatomy and Pathology-Aware X-ray Generation via Controllable Diffusion Model" index=251>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-251 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 30<br>Keywords: Diffusion Model, Fine-tuning, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09240v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09240v1.pdf filename=2403.09240v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Large-scale generative models have demonstrated impressive capacity in producing visually compelling images, with increasing applications in medical imaging. However, they continue to grapple with the challenge of image hallucination and the generation of anatomically inaccurate outputs. These limitations are mainly due to the sole reliance on textual inputs and lack of spatial control over the generated images, hindering the potential usefulness of such models in real-life settings. We present XReal, a novel controllable <b>diffusion</b> <b>model</b> for generating realistic chest X-ray images through precise anatomy and pathology location control. Our lightweight method can seamlessly integrate spatial control in a pre-trained <b>text-to-image</b> <b>diffusion</b> <b>model</b> without <b>fine-tuning,</b> retaining its existing knowledge while enhancing its generation capabilities. XReal outperforms state-of-the-art x-ray <b>diffusion</b> <b>models</b> in quantitative and qualitative metrics while showing 13% and 10% anatomy and pathology realism gain, respectively, based on the expert radiologist evaluation. Our model holds promise for advancing generative models in medical imaging, offering greater precision and adaptability while inviting further exploration in this evolving field. A large synthetically generated data with annotations and code is publicly available at <a href=https://github.com/BioMedIA-MBZUAI/XReal>https://github.com/BioMedIA-MBZUAI/XReal</a>.</p></p class="citation"></blockquote><h3 id=513--252320-stainfuser-controlling-diffusion-for-faster-neural-style-transfer-in-multi-gigapixel-histology-images-robert-jewsbury-et-al-2024>(5/13 | 252/320) StainFuser: Controlling Diffusion for Faster Neural Style Transfer in Multi-Gigapixel Histology Images (Robert Jewsbury et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Robert Jewsbury, Ruoyu Wang, Abhir Bhalerao, Nasir Rajpoot, Quoc Dang Vu. (2024)<br><strong>StainFuser: Controlling Diffusion for Faster Neural Style Transfer in Multi-Gigapixel Histology Images</strong><br><button class=copy-to-clipboard title="StainFuser: Controlling Diffusion for Faster Neural Style Transfer in Multi-Gigapixel Histology Images" index=252>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-252 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 20<br>Keywords: Generative Adversarial Network, Style Transfer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09302v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09302v1.pdf filename=2403.09302v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Stain normalization algorithms aim to transform the color and intensity characteristics of a source multi-gigapixel histology image to match those of a target image, mitigating inconsistencies in the appearance of stains used to highlight cellular components in the images. We propose a new approach, StainFuser, which treats this problem as a <b>style</b> <b>transfer</b> task using a novel Conditional Latent Diffusion architecture, eliminating the need for handcrafted color components. With this method, we curate SPI-2M the largest stain normalization dataset to date of over 2 million histology images with neural <b>style</b> <b>transfer</b> for high-quality transformations. Trained on this data, StainFuser outperforms current state-of-the-art <b>GAN</b> and handcrafted methods in terms of the quality of normalized images. Additionally, compared to existing approaches, it improves the performance of nuclei instance segmentation and classification models when used as a test time augmentation method on the challenging CoNIC dataset. Finally, we apply StainFuser on multi-gigapixel Whole Slide Images (WSIs) and demonstrate improved performance in terms of computational efficiency, image quality and consistency across tiles over current methods.</p></p class="citation"></blockquote><h3 id=613--253320-vm-unet-v2-rethinking-vision-mamba-unet-for-medical-image-segmentation-mingya-zhang-et-al-2024>(6/13 | 253/320) VM-UNET-V2 Rethinking Vision Mamba UNet for Medical Image Segmentation (Mingya Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mingya Zhang, Yue Yu, Limei Gu, Tingsheng Lin, Xianping Tao. (2024)<br><strong>VM-UNET-V2 Rethinking Vision Mamba UNet for Medical Image Segmentation</strong><br><button class=copy-to-clipboard title="VM-UNET-V2 Rethinking Vision Mamba UNet for Medical Image Segmentation" index=253>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-253 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 20<br>Keywords: Convolutional Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09157v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09157v1.pdf filename=2403.09157v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the field of medical image segmentation, models based on both <b>CNN</b> and <b>Transformer</b> have been thoroughly investigated. However, <b>CNNs</b> have limited modeling capabilities for long-range dependencies, making it challenging to exploit the semantic information within images fully. On the other hand, the quadratic computational complexity poses a challenge for <b>Transformers.</b> Recently, State Space Models (SSMs), such as Mamba, have been recognized as a promising method. They not only demonstrate superior performance in modeling long-range interactions, but also preserve a linear computational complexity. Inspired by the Mamba architecture, We proposed Vison Mamba-UNetV2, the Visual State Space (VSS) Block is introduced to capture extensive contextual information, the Semantics and Detail Infusion (SDI) is introduced to augment the infusion of low-level and high-level features. We conduct comprehensive experiments on the ISIC17, ISIC18, CVC-300, CVC-ClinicDB, Kvasir, CVC-ColonDB and ETIS-LaribPolypDB public datasets. The results indicate that VM-UNetV2 exhibits competitive performance in medical image segmentation tasks. Our code is available at <a href=https://github.com/nobodyplayer1/VM-UNetV2>https://github.com/nobodyplayer1/VM-UNetV2</a>.</p></p class="citation"></blockquote><h3 id=713--254320-empowering-healthcare-through-privacy-preserving-mri-analysis-al-amin-et-al-2024>(7/13 | 254/320) Empowering Healthcare through Privacy-Preserving MRI Analysis (Al Amin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Al Amin, Kamrul Hasan, Saleh Zein-Sabatto, Deo Chimba, Liang Hong, Imtiaz Ahmed, Tariqul Islam. (2024)<br><strong>Empowering Healthcare through Privacy-Preserving MRI Analysis</strong><br><button class=copy-to-clipboard title="Empowering Healthcare through Privacy-Preserving MRI Analysis" index=254>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-254 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09836v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09836v1.pdf filename=2403.09836v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the healthcare domain, Magnetic Resonance Imaging (MRI) assumes a pivotal role, as it employs Artificial Intelligence (AI) and Machine Learning (ML) methodologies to extract invaluable insights from imaging data. Nonetheless, the imperative need for patient privacy poses significant challenges when collecting data from diverse healthcare sources. Consequently, the Deep Learning (DL) communities occasionally face difficulties detecting rare features. In this research endeavor, we introduce the Ensemble-Based <b>Federated</b> <b>Learning</b> (EBFL) Framework, an innovative solution tailored to address this challenge. The EBFL framework deviates from the conventional approach by emphasizing model features over sharing sensitive patient data. This unique methodology fosters a collaborative and privacy-conscious environment for healthcare institutions, empowering them to harness the capabilities of a centralized server for model refinement while upholding the utmost data privacy standards.Conversely, a robust ensemble architecture boasts potent feature extraction capabilities, distinguishing itself from a single DL model. This quality makes it remarkably dependable for MRI analysis. By harnessing our groundbreaking EBFL methodology, we have achieved remarkable precision in the classification of brain tumors, including glioma, meningioma, pituitary, and non-tumor instances, attaining a precision rate of 94% for the Global model and an impressive 96% for the Ensemble model. Our models underwent rigorous evaluation using conventional performance metrics such as Accuracy, Precision, Recall, and F1 Score. Integrating DL within the <b>Federated</b> <b>Learning</b> (FL) framework has yielded a methodology that offers precise and dependable diagnostics for detecting brain tumors.</p></p class="citation"></blockquote><h3 id=813--255320-analyzing-data-augmentation-for-medical-images-a-case-study-in-ultrasound-images-adam-tupper-et-al-2024>(8/13 | 255/320) Analyzing Data Augmentation for Medical Images: A Case Study in Ultrasound Images (Adam Tupper et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Adam Tupper, Christian Gagné. (2024)<br><strong>Analyzing Data Augmentation for Medical Images: A Case Study in Ultrasound Images</strong><br><button class=copy-to-clipboard title="Analyzing Data Augmentation for Medical Images: A Case Study in Ultrasound Images" index=255>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-255 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Data Augmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09828v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09828v1.pdf filename=2403.09828v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Data</b> <b>augmentation</b> is one of the most effective techniques to improve the generalization performance of deep neural networks. Yet, despite often facing limited <b>data</b> <b>availability</b> in medical image analysis, it is frequently underutilized. This appears to be due to a gap in our collective understanding of the efficacy of different augmentation techniques across medical imaging tasks and modalities. One domain where this is especially true is breast ultrasound images. This work addresses this issue by analyzing the effectiveness of different augmentation techniques for the classification of breast lesions in ultrasound images. We assess the generalizability of our findings across several datasets, demonstrate that certain augmentations are far more effective than others, and show that their usage leads to significant performance gains.</p></p class="citation"></blockquote><h3 id=913--256320-mitigating-data-consistency-induced-discrepancy-in-cascaded-diffusion-models-for-sparse-view-ct-reconstruction-hanyu-chen-et-al-2024>(9/13 | 256/320) Mitigating Data Consistency Induced Discrepancy in Cascaded Diffusion Models for Sparse-view CT Reconstruction (Hanyu Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hanyu Chen, Zhixiu Hao, Lin Guo, Liying Xiao. (2024)<br><strong>Mitigating Data Consistency Induced Discrepancy in Cascaded Diffusion Models for Sparse-view CT Reconstruction</strong><br><button class=copy-to-clipboard title="Mitigating Data Consistency Induced Discrepancy in Cascaded Diffusion Models for Sparse-view CT Reconstruction" index=256>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-256 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09355v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09355v1.pdf filename=2403.09355v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Sparse-view Computed Tomography (CT) image reconstruction is a promising approach to reduce radiation exposure, but it inevitably leads to image degradation. Although <b>diffusion</b> <b>model-based</b> approaches are computationally expensive and suffer from the training-sampling discrepancy, they provide a potential solution to the problem. This study introduces a novel Cascaded <b>Diffusion</b> <b>with</b> Discrepancy Mitigation (CDDM) framework, including the low-quality image generation in latent space and the high-quality image generation in pixel space which contains data consistency and discrepancy mitigation in a one-step reconstruction process. The cascaded framework minimizes computational costs by moving some inference steps from pixel space to latent space. The discrepancy mitigation technique addresses the training-sampling gap induced by data consistency, ensuring the data distribution is close to the original manifold. A specialized Alternating Direction Method of Multipliers (ADMM) is employed to process image gradients in separate directions, offering a more targeted approach to regularization. Experimental results across two datasets demonstrate CDDM&rsquo;s superior performance in high-quality image generation with clearer boundaries compared to existing methods, highlighting the framework&rsquo;s computational efficiency.</p></p class="citation"></blockquote><h3 id=1013--257320-advanced-tumor-segmentation-in-medical-imaging-an-ensemble-approach-for-brats-2023-adult-glioma-and-pediatric-tumor-tasks-fadillah-maani-et-al-2024>(10/13 | 257/320) Advanced Tumor Segmentation in Medical Imaging: An Ensemble Approach for BraTS 2023 Adult Glioma and Pediatric Tumor Tasks (Fadillah Maani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fadillah Maani, Anees Ur Rehman Hashmi, Mariam Aljuboory, Numan Saeed, Ikboljon Sobirov, Mohammad Yaqub. (2024)<br><strong>Advanced Tumor Segmentation in Medical Imaging: An Ensemble Approach for BraTS 2023 Adult Glioma and Pediatric Tumor Tasks</strong><br><button class=copy-to-clipboard title="Advanced Tumor Segmentation in Medical Imaging: An Ensemble Approach for BraTS 2023 Adult Glioma and Pediatric Tumor Tasks" index=257>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-257 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09262v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09262v1.pdf filename=2403.09262v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Automated segmentation proves to be a valuable tool in precisely detecting tumors within medical images. The accurate identification and segmentation of tumor types hold paramount importance in diagnosing, monitoring, and treating highly fatal brain tumors. The BraTS challenge serves as a platform for researchers to tackle this issue by participating in open challenges focused on tumor segmentation. This study outlines our methodology for segmenting tumors in the context of two distinct tasks from the BraTS 2023 challenge: Adult Glioma and Pediatric Tumors. Our approach leverages two encoder-decoder-based <b>CNN</b> models, namely SegResNet and MedNeXt, for segmenting three distinct subregions of tumors. We further introduce a set of robust postprocessing to improve the segmentation, especially for the newly introduced BraTS 2023 metrics. The specifics of our approach and comprehensive performance analyses are expounded upon in this work. Our proposed approach achieves third place in the BraTS 2023 Adult Glioma Segmentation Challenges with an average of 0.8313 and 36.38 Dice and HD95 scores on the test set, respectively.</p></p class="citation"></blockquote><h3 id=1113--258320-a-modified-da-vinci-surgical-instrument-for-oce-based-elasticity-estimation-with-deep-learning-maximilian-neidhardt-et-al-2024>(11/13 | 258/320) A Modified da Vinci Surgical Instrument for OCE based Elasticity Estimation with Deep Learning (Maximilian Neidhardt et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Maximilian Neidhardt, Robin Mieling, Sarah Latus, Martin Fischer, Tobias Maurer, Alexander Schlaefer. (2024)<br><strong>A Modified da Vinci Surgical Instrument for OCE based Elasticity Estimation with Deep Learning</strong><br><button class=copy-to-clipboard title="A Modified da Vinci Surgical Instrument for OCE based Elasticity Estimation with Deep Learning" index=258>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-258 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-SY, eess-IV, eess-SY, eess.IV<br>Keyword Score: 10<br>Keywords: Key Point Analysis<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09256v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09256v1.pdf filename=2403.09256v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Robot-assisted surgery has advantages compared to conventional laparoscopic procedures, e.g., precise movement of the surgical instruments, improved dexterity, and high-resolution visualization of the surgical field. However, mechanical tissue properties may provide additional information, e.g., on the location of lesions or vessels. While elastographic imaging has been proposed, it is not readily available as an online modality during robot-assisted surgery. We propose modifying a da~Vinci surgical instrument to realize optical coherence elastography (OCE) for quantitative elasticity estimation. The modified da~Vinci instrument is equipped with piezoelectric elements for shear wave excitation and we employ fast optical coherence tomography (OCT) imaging to track propagating wave fields, which are directly related to biomechanical tissue properties. All high-voltage components are mounted at the proximal end outside the patient. We demonstrate that external excitation at the instrument shaft can effectively stimulate shear waves, even when considering damping. Comparing conventional and deep learning-based signal processing, resulting in mean absolute errors of 19.27 <b>kPa</b> and 6.29 <b>kPa,</b> respectively. These results illustrate that precise quantitative elasticity estimates can be obtained. We also demonstrate quantitative elasticity estimation on ex-vivo tissue samples of heart, liver and stomach, and show that the measurements can be used to distinguish soft and stiff tissue types.</p></p class="citation"></blockquote><h3 id=1213--259320-tbi-imagetext-tbi-it-comprehensive-text-and-image-datasets-for-traumatic-brain-injury-research-jie-li-et-al-2024>(12/13 | 259/320) TBI Image/Text (TBI-IT): Comprehensive Text and Image Datasets for Traumatic Brain Injury Research (Jie Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jie Li, Jiaying Wen, Tongxin Yang, Fenglin Cai, Miao Wei, Zhiwei Zhang, Li Jiang. (2024)<br><strong>TBI Image/Text (TBI-IT): Comprehensive Text and Image Datasets for Traumatic Brain Injury Research</strong><br><button class=copy-to-clipboard title="TBI Image/Text (TBI-IT): Comprehensive Text and Image Datasets for Traumatic Brain Injury Research" index=259>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-259 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Named Entity Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09062v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09062v1.pdf filename=2403.09062v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we introduce a new dataset in the medical field of Traumatic Brain Injury (TBI), called TBI-IT, which includes both electronic medical records (EMRs) and head CT images. This dataset is designed to enhance the accuracy of artificial intelligence in the diagnosis and treatment of TBI. This dataset, built upon the foundation of standard text and image data, incorporates specific annotations within the EMRs, extracting key content from the text information, and categorizes the annotation content of imaging data into five types: brain midline, hematoma, left cerebral ventricle, right cerebral ventricle and fracture. TBI-IT aims to be a foundational dataset for feature learning in image segmentation tasks and <b>named</b> <b>entity</b> <b>recognition.</b></p></p class="citation"></blockquote><h3 id=1313--260320-deep-unfolding-network-for-hyperspectral-image-super-resolution-with-automatic-exposure-correction-yuan-fang-et-al-2024>(13/13 | 260/320) Deep unfolding Network for Hyperspectral Image Super-Resolution with Automatic Exposure Correction (Yuan Fang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuan Fang, Yipeng Liu, Jie Chen, Zhen Long, Ao Li, Chong-Yung Chi, Ce Zhu. (2024)<br><strong>Deep unfolding Network for Hyperspectral Image Super-Resolution with Automatic Exposure Correction</strong><br><button class=copy-to-clipboard title="Deep unfolding Network for Hyperspectral Image Super-Resolution with Automatic Exposure Correction" index=260>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-260 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09096v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09096v1.pdf filename=2403.09096v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, the fusion of high spatial resolution multispectral image (HR-MSI) and low spatial resolution hyperspectral image (LR-HSI) has been recognized as an effective method for HSI super-resolution (HSI-SR). However, both HSI and MSI may be acquired under extreme conditions such as night or poorly illuminating scenarios, which may cause different exposure levels, thereby seriously downgrading the yielded HSISR. In contrast to most existing methods based on respective low-light enhancements (LLIE) of MSI and HSI followed by their fusion, a deep Unfolding HSI Super-Resolution with Automatic Exposure Correction (UHSR-AEC) is proposed, that can effectively generate a high-quality fused HSI-SR (in texture and features) even under very imbalanced exposures, thanks to the correlation between LLIE and HSI-SR taken into account. Extensive experiments are provided to demonstrate the state-of-the-art overall performance of the proposed UHSR-AEC, including comparison with some <b>benchmark</b> peer methods.</p></p class="citation"></blockquote><h2 id=cshc-10>cs.HC (10)</h2><h3 id=110--261320-like-a-nesting-doll-analyzing-recursion-analogies-generated-by-cs-students-using-large-language-models-seth-bernstein-et-al-2024>(1/10 | 261/320) &lsquo;Like a Nesting Doll&rsquo;: Analyzing Recursion Analogies Generated by CS Students using Large Language Models (Seth Bernstein et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Seth Bernstein, Paul Denny, Juho Leinonen, Lauren Kan, Arto Hellas, Matt Littlefield Sami Sarsa, Stephen MacNeil. (2024)<br><strong>&lsquo;Like a Nesting Doll&rsquo;: Analyzing Recursion Analogies Generated by CS Students using Large Language Models</strong><br><button class=copy-to-clipboard title="'Like a Nesting Doll': Analyzing Recursion Analogies Generated by CS Students using Large Language Models" index=261>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-261 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-AI, cs-CL, cs-HC, cs.HC<br>Keyword Score: 40<br>Keywords: ChatGPT, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09409v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09409v1.pdf filename=2403.09409v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Grasping complex computing concepts often poses a challenge for students who struggle to anchor these new ideas to familiar experiences and understandings. To help with this, a good analogy can bridge the gap between unfamiliar concepts and familiar ones, providing an engaging way to aid understanding. However, creating effective educational analogies is difficult even for experienced instructors. We investigate to what extent <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> specifically <b>ChatGPT,</b> can provide access to personally relevant analogies on demand. Focusing on recursion, a challenging threshold concept, we conducted an investigation analyzing the analogies generated by more than 350 first-year computing students. They were provided with a code snippet and tasked to generate their own recursion-based analogies using <b>ChatGPT,</b> optionally including personally relevant topics in their <b>prompts.</b> We observed a great deal of diversity in the analogies produced with student-prescribed topics, in contrast to the otherwise generic analogies, highlighting the value of student creativity when working with <b>LLMs.</b> Not only did students enjoy the activity and report an improved understanding of recursion, but they described more easily remembering analogies that were personally and culturally relevant.</p></p class="citation"></blockquote><h3 id=210--262320-towards-proactive-interactions-for-in-vehicle-conversational-assistants-utilizing-large-language-models-huifang-du-et-al-2024>(2/10 | 262/320) Towards Proactive Interactions for In-Vehicle Conversational Assistants Utilizing Large Language Models (Huifang Du et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Huifang Du, Xuejing Feng, Jun Ma, Meng Wang, Shiyu Tao, Yijie Zhong, Yuan-Fang Li, Haofen Wang. (2024)<br><strong>Towards Proactive Interactions for In-Vehicle Conversational Assistants Utilizing Large Language Models</strong><br><button class=copy-to-clipboard title="Towards Proactive Interactions for In-Vehicle Conversational Assistants Utilizing Large Language Models" index=262>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-262 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 40<br>Keywords: Intent Detection, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09135v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09135v1.pdf filename=2403.09135v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Research demonstrates that the proactivity of in-vehicle conversational assistants (IVCAs) can help to reduce distractions and enhance driving safety, better meeting users&rsquo; cognitive needs. However, existing IVCAs struggle with user <b>intent</b> <b>recognition</b> and context awareness, which leads to suboptimal proactive interactions. <b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have shown potential for generalizing to various tasks with <b>prompts,</b> but their application in IVCAs and exploration of proactive interaction remain under-explored. These raise questions about how <b>LLMs</b> improve proactive interactions for IVCAs and influence user perception. To investigate these questions systematically, we establish a framework with five proactivity levels across two dimensions-assumption and autonomy-for IVCAs. According to the framework, we propose a &ldquo;Rewrite + ReAct + Reflect&rdquo; strategy, aiming to empower <b>LLMs</b> to fulfill the specific demands of each proactivity level when interacting with users. Both feasibility and subjective experiments are conducted. The <b>LLM</b> outperforms the state-of-the-art model in success rate and achieves satisfactory results for each proactivity level. Subjective experiments with 40 participants validate the effectiveness of our framework and show the proactive level with strong assumptions and user confirmation is most appropriate.</p></p class="citation"></blockquote><h3 id=310--263320-enabling-waypoint-generation-for-collaborative-robots-using-llms-and-mixed-reality-cathy-mengying-fang-et-al-2024>(3/10 | 263/320) Enabling Waypoint Generation for Collaborative Robots using LLMs and Mixed Reality (Cathy Mengying Fang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Cathy Mengying Fang, Krzysztof Zieliński, Pattie Maes, Joe Paradiso, Bruce Blumberg, Mikkel Baun Kjærgaard. (2024)<br><strong>Enabling Waypoint Generation for Collaborative Robots using LLMs and Mixed Reality</strong><br><button class=copy-to-clipboard title="Enabling Waypoint Generation for Collaborative Robots using LLMs and Mixed Reality" index=263>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-263 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs-RO, cs.HC<br>Keyword Score: 30<br>Keywords: Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09308v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09308v1.pdf filename=2403.09308v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Programming a robotic is a complex task, as it demands the user to have a good command of specific programming languages and awareness of the robot&rsquo;s physical constraints. We propose a framework that simplifies robot deployment by allowing direct communication using natural language. It uses <b>large</b> <b>language</b> <b>models</b> <b>(LLM)</b> for <b>prompt</b> processing, workspace understanding, and waypoint generation. It also employs Augmented Reality (AR) to provide visual feedback of the planned outcome. We showcase the effectiveness of our framework with a simple pick-and-place task, which we implement on a real robot. Moreover, we present an early concept of expressive robot behavior and skill generation that can be used to communicate with the user and learn new skills (e.g., object grasping).</p></p class="citation"></blockquote><h3 id=410--264320-prompthis-visualizing-the-process-and-influence-of-prompt-editing-during-text-to-image-creation-yuhan-guo-et-al-2024>(4/10 | 264/320) PrompTHis: Visualizing the Process and Influence of Prompt Editing during Text-to-Image Creation (Yuhan Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuhan Guo, Hanning Shao, Can Liu, Kai Xu, Xiaoru Yuan. (2024)<br><strong>PrompTHis: Visualizing the Process and Influence of Prompt Editing during Text-to-Image Creation</strong><br><button class=copy-to-clipboard title="PrompTHis: Visualizing the Process and Influence of Prompt Editing during Text-to-Image Creation" index=264>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-264 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 23<br>Keywords: Graph, Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09615v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09615v1.pdf filename=2403.09615v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Generative <b>text-to-image</b> models, which allow users to create appealing images through a text <b>prompt,</b> have seen a dramatic increase in popularity in recent years. However, most users have a limited understanding of how such models work and it often requires many trials and errors to achieve satisfactory results. The <b>prompt</b> history contains a wealth of information that could provide users with insights into what have been explored and how the <b>prompt</b> changes impact the output image, yet little research attention has been paid to the visual analysis of such process to support users. We propose the Image Variant <b>Graph,</b> a novel visual representation designed to support comparing <b>prompt-image</b> pairs and exploring the editing history. The Image Variant <b>Graph</b> models <b>prompt</b> differences as edges between corresponding images and presents the distances between images through projection. Based on the <b>graph,</b> we developed the PrompTHis system through co-design with artists. Besides Image Variant <b>Graph,</b> PrompTHis also incorporates a detailed <b>prompt-image</b> history and a navigation mini-map. Based on the review and analysis of the <b>prompting</b> history, users can better understand the impact of <b>prompt</b> changes and have a more effective control of image generation. A quantitative user study with eleven amateur participants and qualitative interviews with five professionals and one amateur user were conducted to evaluate the effectiveness of PrompTHis. The results demonstrate PrompTHis can help users review the <b>prompt</b> history, make sense of the model, and plan their creative process.</p></p class="citation"></blockquote><h3 id=510--265320-vivid-human-ai-collaborative-authoring-of-vicarious-dialogues-from-lecture-videos-seulgi-choi-et-al-2024>(5/10 | 265/320) VIVID: Human-AI Collaborative Authoring of Vicarious Dialogues from Lecture Videos (Seulgi Choi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Seulgi Choi, Hyewon Lee, Yoonjoo Lee, Juho Kim. (2024)<br><strong>VIVID: Human-AI Collaborative Authoring of Vicarious Dialogues from Lecture Videos</strong><br><button class=copy-to-clipboard title="VIVID: Human-AI Collaborative Authoring of Vicarious Dialogues from Lecture Videos" index=265>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-265 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09168v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09168v1.pdf filename=2403.09168v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The lengthy monologue-style online lectures cause learners to lose engagement easily. Designing lectures in a &ldquo;vicarious dialogue&rdquo; format can foster learners&rsquo; cognitive activities more than monologue-style. However, designing online lectures in a dialogue style catered to the diverse needs of learners is laborious for instructors. We conducted a design workshop with eight educational experts and seven instructors to present key guidelines and the potential use of <b>large</b> <b>language</b> <b>models</b> <b>(LLM)</b> to transform a monologue lecture script into pedagogically meaningful dialogue. Applying these design guidelines, we created VIVID which allows instructors to collaborate with <b>LLMs</b> to design, evaluate, and modify pedagogical dialogues. In a within-subjects study with instructors (N=12), we show that VIVID helped instructors select and revise dialogues efficiently, thereby supporting the authoring of quality dialogues. Our findings demonstrate the potential of <b>LLMs</b> to assist instructors with creating high-quality educational dialogues across various learning stages.</p></p class="citation"></blockquote><h3 id=610--266320-unlocking-the-conversion-of-web-screenshots-into-html-code-with-the-websight-dataset-hugo-laurençon-et-al-2024>(6/10 | 266/320) Unlocking the conversion of Web Screenshots into HTML Code with the WebSight Dataset (Hugo Laurençon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hugo Laurençon, Léo Tronchon, Victor Sanh. (2024)<br><strong>Unlocking the conversion of Web Screenshots into HTML Code with the WebSight Dataset</strong><br><button class=copy-to-clipboard title="Unlocking the conversion of Web Screenshots into HTML Code with the WebSight Dataset" index=266>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-266 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-AI, cs-CV, cs-HC, cs.HC<br>Keyword Score: 20<br>Keywords: Fine-tuning, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09029v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09029v1.pdf filename=2403.09029v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Using <b>vision-language</b> models (VLMs) in web development presents a promising strategy to increase efficiency and unblock no-code solutions: by providing a screenshot or a sketch of a UI, a VLM could generate the code to reproduce it, for instance in a language like HTML. Despite the advancements in VLMs for various tasks, the specific challenge of converting a screenshot into a corresponding HTML has been minimally explored. We posit that this is mainly due to the absence of a suitable, high-quality dataset. This work introduces WebSight, a synthetic dataset consisting of 2 million pairs of HTML codes and their corresponding screenshots. We <b>fine-tune</b> a foundational VLM on our dataset and show proficiency in converting webpage screenshots to functional HTML code. To accelerate the research in this area, we open-source WebSight.</p></p class="citation"></blockquote><h3 id=710--267320-influence-of-personality-and-communication-behavior-of-a-conversational-agent-on-user-experience-and-social-presence-in-augmented-reality-katerina-koleva-et-al-2024>(7/10 | 267/320) Influence of Personality and Communication Behavior of a Conversational Agent on User Experience and Social Presence in Augmented Reality (Katerina Koleva et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Katerina Koleva, Maurizio Vergari, Tanja Kojić, Sebastian Möller, Jan-Niklas Voigt-Antons. (2024)<br><strong>Influence of Personality and Communication Behavior of a Conversational Agent on User Experience and Social Presence in Augmented Reality</strong><br><button class=copy-to-clipboard title="Influence of Personality and Communication Behavior of a Conversational Agent on User Experience and Social Presence in Augmented Reality" index=267>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-267 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09883v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09883v1.pdf filename=2403.09883v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A virtual embodiment can benefit conversational agents, but it is unclear how their personalities and non-verbal behavior influence the User Experience and Social Presence in Augmented Reality (AR). We asked 30 users to converse with a virtual assistant who gives <b>recommendations</b> about city activities. The participants interacted with two different personalities: Sammy, a cheerful blue mouse, and Olive, a serious green human-like agent. Each was presented with two body languages - happy/friendly and annoyed/unfriendly. We conclude how agent representation and humor affect User Experience aspects, and that body language is significant in the evaluation and perception of the AR agent.</p></p class="citation"></blockquote><h3 id=810--268320-labelaid-just-in-time-ai-interventions-for-improving-human-labeling-quality-and-domain-knowledge-in-crowdsourcing-systems-chu-li-et-al-2024>(8/10 | 268/320) LabelAId: Just-in-time AI Interventions for Improving Human Labeling Quality and Domain Knowledge in Crowdsourcing Systems (Chu Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chu Li, Zhihan Zhang, Michael Saugstad, Esteban Safranchik, Minchu Kulkarni, Xiaoyu Huang, Shwetak Patel, Vikram Iyer, Tim Althoff, Jon E. Froehlich. (2024)<br><strong>LabelAId: Just-in-time AI Interventions for Improving Human Labeling Quality and Domain Knowledge in Crowdsourcing Systems</strong><br><button class=copy-to-clipboard title="LabelAId: Just-in-time AI Interventions for Improving Human Labeling Quality and Domain Knowledge in Crowdsourcing Systems" index=268>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-268 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-AI, cs-HC, cs-LG, cs.HC<br>Keyword Score: 10<br>Keywords: Weakly Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09810v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09810v1.pdf filename=2403.09810v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Crowdsourcing platforms have transformed distributed problem-solving, yet quality control remains a persistent challenge. Traditional quality control measures, such as prescreening workers and refining instructions, often focus solely on optimizing economic output. This paper explores just-in-time AI interventions to enhance both labeling quality and domain-specific knowledge among crowdworkers. We introduce LabelAId, an advanced inference model combining Programmatic <b>Weak</b> <b>Supervision</b> (PWS) with FT-Transformers to infer label correctness based on user behavior and domain knowledge. Our technical evaluation shows that our LabelAId pipeline consistently outperforms state-of-the-art ML baselines, improving mistake inference accuracy by 36.7% with 50 downstream samples. We then implemented LabelAId into Project Sidewalk, an open-source crowdsourcing platform for urban accessibility. A between-subjects study with 34 participants demonstrates that LabelAId significantly enhances label precision without compromising efficiency while also increasing labeler confidence. We discuss LabelAId&rsquo;s success factors, limitations, and its generalizability to other crowdsourced science domains.</p></p class="citation"></blockquote><h3 id=910--269320-param-leveraging-parametric-design-in-extended-reality-to-support-the-personalization-of-artifacts-for-personal-fabrication-evgeny-stemasov-et-al-2024>(9/10 | 269/320) pARam: Leveraging Parametric Design in Extended Reality to Support the Personalization of Artifacts for Personal Fabrication (Evgeny Stemasov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Evgeny Stemasov, Simon Demharter, Max Rädler, Jan Gugenheimer, Enrico Rukzio. (2024)<br><strong>pARam: Leveraging Parametric Design in Extended Reality to Support the Personalization of Artifacts for Personal Fabrication</strong><br><button class=copy-to-clipboard title="pARam: Leveraging Parametric Design in Extended Reality to Support the Personalization of Artifacts for Personal Fabrication" index=269>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-269 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: H-5-2, cs-GR, cs-HC, cs.HC<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09607v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09607v1.pdf filename=2403.09607v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Extended Reality (XR) allows in-situ previewing of designs to be manufactured through Personal Fabrication (PF). These in-situ interactions exhibit advantages for PF, like incorporating the environment into the design process. However, design-for-fabrication in XR often happens through either highly complex 3D-modeling or is reduced to rudimentary adaptations of crowd-sourced models. We present pARam, a tool combining parametric designs (PDs) and XR, enabling in-situ configuration of artifacts for PF. In contrast to modeling- or search-focused approaches, pARam supports customization through embodied and practical inputs (e.g., gestures, <b>recommendations)</b> and evaluation (e.g., lighting estimation) without demanding complex 3D-modeling skills. We implemented pARam for HoloLens 2 and evaluated it (n=20), comparing XR and desktop conditions. Users succeeded in choosing context-related parameters and took their environment into account for their configuration using pARam. We reflect on the prospects and challenges of PDs in XR to streamline complex design methods for PF while retaining suitable expressivity.</p></p class="citation"></blockquote><h3 id=1010--270320-which-artificial-intelligences-do-people-care-about-most-a-conjoint-experiment-on-moral-consideration-ali-ladak-et-al-2024>(10/10 | 270/320) Which Artificial Intelligences Do People Care About Most? A Conjoint Experiment on Moral Consideration (Ali Ladak et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ali Ladak, Jamie Harris, Jacy Reese Anthis. (2024)<br><strong>Which Artificial Intelligences Do People Care About Most? A Conjoint Experiment on Moral Consideration</strong><br><button class=copy-to-clipboard title="Which Artificial Intelligences Do People Care About Most? A Conjoint Experiment on Moral Consideration" index=270>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-270 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 10<br>Keywords: Emotion Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09405v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09405v1.pdf filename=2403.09405v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Many studies have identified particular features of artificial intelligences (AI), such as their autonomy and <b>emotion</b> <b>expression,</b> that affect the extent to which they are treated as subjects of moral consideration. However, there has not yet been a comparison of the relative importance of features as is necessary to design and understand increasingly capable, multi-faceted AI systems. We conducted an online conjoint experiment in which 1,163 participants evaluated descriptions of AIs that varied on these features. All 11 features increased how morally wrong participants considered it to harm the AIs. The largest effects were from human-like physical bodies and prosociality (i.e., <b>emotion</b> <b>expression,</b> <b>emotion</b> <b>recognition,</b> cooperation, and moral judgment). For human-computer interaction designers, the importance of prosociality suggests that, because AIs are often seen as threatening, the highest levels of moral consideration may only be granted if the AI has positive intentions.</p></p class="citation"></blockquote><h2 id=csai-7>cs.AI (7)</h2><h3 id=17--271320-silico-centric-theory-of-mind-anirban-mukherjee-et-al-2024>(1/7 | 271/320) Silico-centric Theory of Mind (Anirban Mukherjee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anirban Mukherjee, Hannah Hanwen Chang. (2024)<br><strong>Silico-centric Theory of Mind</strong><br><button class=copy-to-clipboard title="Silico-centric Theory of Mind" index=271>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-271 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 40<br>Keywords: Counter-factual, Counterfactual Reasoning, Reasoning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09289v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09289v1.pdf filename=2403.09289v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Theory of Mind (ToM) refers to the ability to attribute mental states, such as beliefs, desires, intentions, and knowledge, to oneself and others, and to understand that these mental states can differ from one&rsquo;s own and from reality. We investigate ToM in environments with multiple, distinct, independent AI agents, each possessing unique internal states, information, and objectives. Inspired by human false-belief experiments, we present an AI (&lsquo;focal AI&rsquo;) with a scenario where its clone undergoes a human-centric ToM assessment. We <b>prompt</b> the focal AI to assess whether its clone would benefit from additional instructions. Concurrently, we give its clones the ToM assessment, both with and without the instructions, thereby engaging the focal AI in higher-order <b>counterfactual</b> <b>reasoning</b> akin to human mentalizing&ndash;with respect to humans in one test and to other AI in another. We uncover a discrepancy: Contemporary AI demonstrates near-perfect accuracy on human-centric ToM assessments. Since information embedded in one AI is identically embedded in its clone, additional instructions are redundant. Yet, we observe AI crafting elaborate instructions for their clones, erroneously anticipating a need for assistance. An independent referee AI agrees with these unsupported expectations. Neither the focal AI nor the referee demonstrates ToM in our &lsquo;silico-centric&rsquo; test.</p></p class="citation"></blockquote><h3 id=27--272320-clinical-reasoning-over-tabular-data-and-text-with-bayesian-networks-paloma-rabaey-et-al-2024>(2/7 | 272/320) Clinical Reasoning over Tabular Data and Text with Bayesian Networks (Paloma Rabaey et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Paloma Rabaey, Johannes Deleu, Stefan Heytens, Thomas Demeester. (2024)<br><strong>Clinical Reasoning over Tabular Data and Text with Bayesian Networks</strong><br><button class=copy-to-clipboard title="Clinical Reasoning over Tabular Data and Text with Bayesian Networks" index=272>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-272 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 30<br>Keywords: Simulation, Simulator, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09481v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09481v2.pdf filename=2403.09481v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Bayesian networks are well-suited for clinical <b>reasoning</b> on tabular data, but are less compatible with natural language data, for which neural networks provide a successful framework. This paper compares and discusses strategies to augment Bayesian networks with neural text representations, both in a generative and discriminative manner. This is illustrated with <b>simulation</b> results for a primary care use case (diagnosis of pneumonia) and discussed in a broader clinical context.</p></p class="citation"></blockquote><h3 id=37--273320-xlp-explainable-link-prediction-for-master-data-management-balaji-ganesan-et-al-2024>(3/7 | 273/320) xLP: Explainable Link Prediction for Master Data Management (Balaji Ganesan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Balaji Ganesan, Matheen Ahmed Pasha, Srinivasa Parkala, Neeraj R Singh, Gayatri Mishra, Sumit Bhatia, Hima Patel, Somashekar Naganna, Sameep Mehta. (2024)<br><strong>xLP: Explainable Link Prediction for Master Data Management</strong><br><button class=copy-to-clipboard title="xLP: Explainable Link Prediction for Master Data Management" index=273>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-273 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 20<br>Keywords: Fact Verification, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09806v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09806v1.pdf filename=2403.09806v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Explaining neural model predictions to users requires creativity. Especially in enterprise applications, where there are costs associated with users&rsquo; time, and their trust in the model predictions is critical for adoption. For link prediction in master data management, we have built a number of explainability solutions drawing from research in interpretability, <b>fact</b> <b>verification,</b> path ranking, neuro-symbolic <b>reasoning</b> and self-explaining AI. In this demo, we present explanations for link prediction in a creative way, to allow users to choose explanations they are more comfortable with.</p></p class="citation"></blockquote><h3 id=47--274320-generating-feasible-and-plausible-counterfactual-explanations-for-outcome-prediction-of-business-processes-alexander-stevens-et-al-2024>(4/7 | 274/320) Generating Feasible and Plausible Counterfactual Explanations for Outcome Prediction of Business Processes (Alexander Stevens et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alexander Stevens, Chun Ouyang, Johannes De Smedt, Catarina Moreira. (2024)<br><strong>Generating Feasible and Plausible Counterfactual Explanations for Outcome Prediction of Business Processes</strong><br><button class=copy-to-clipboard title="Generating Feasible and Plausible Counterfactual Explanations for Outcome Prediction of Business Processes" index=274>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-274 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 20<br>Keywords: Counter-factual, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09232v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09232v1.pdf filename=2403.09232v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, various machine and deep learning architectures have been successfully introduced to the field of predictive process analytics. Nevertheless, the inherent opacity of these algorithms poses a significant challenge for human decision-makers, hindering their ability to understand the <b>reasoning</b> behind the predictions. This growing concern has sparked the introduction of <b>counterfactual</b> explanations, designed as human-understandable what if scenarios, to provide clearer insights into the decision-making process behind undesirable predictions. The generation of <b>counterfactual</b> explanations, however, encounters specific challenges when dealing with the sequential nature of the (business) process cases typically used in predictive process analytics. Our paper tackles this challenge by introducing a data-driven approach, REVISEDplus, to generate more feasible and plausible <b>counterfactual</b> explanations. First, we restrict the <b>counterfactual</b> algorithm to generate <b>counterfactuals</b> that lie within a high-density region of the process data, ensuring that the proposed <b>counterfactuals</b> are realistic and feasible within the observed process data distribution. Additionally, we ensure plausibility by learning sequential patterns between the activities in the process cases, utilising Declare language templates. Finally, we evaluate the properties that define the validity of <b>counterfactuals.</b></p></p class="citation"></blockquote><h3 id=57--275320-leveraging-constraint-programming-in-a-deep-learning-approach-for-dynamically-solving-the-flexible-job-shop-scheduling-problem-imanol-echeverria-et-al-2024>(5/7 | 275/320) Leveraging Constraint Programming in a Deep Learning Approach for Dynamically Solving the Flexible Job-Shop Scheduling Problem (Imanol Echeverria et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Imanol Echeverria, Maialen Murua, Roberto Santana. (2024)<br><strong>Leveraging Constraint Programming in a Deep Learning Approach for Dynamically Solving the Flexible Job-Shop Scheduling Problem</strong><br><button class=copy-to-clipboard title="Leveraging Constraint Programming in a Deep Learning Approach for Dynamically Solving the Flexible Job-Shop Scheduling Problem" index=275>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-275 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 13<br>Keywords: Benchmarking, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09249v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09249v1.pdf filename=2403.09249v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in the flexible job-shop scheduling problem (FJSSP) are primarily based on deep <b>reinforcement</b> <b>learning</b> (DRL) due to its ability to generate high-quality, real-time solutions. However, DRL approaches often fail to fully harness the strengths of existing techniques such as exact methods or constraint programming (CP), which can excel at finding optimal or near-optimal solutions for smaller instances. This paper aims to integrate CP within a deep learning (DL) based methodology, leveraging the benefits of both. In this paper, we introduce a method that involves training a DL model using optimal solutions generated by CP, ensuring the model learns from high-quality data, thereby eliminating the need for the extensive exploration typical in DRL and enhancing overall performance. Further, we integrate CP into our DL framework to jointly construct solutions, utilizing DL for the initial complex stages and transitioning to CP for optimal resolution as the problem is simplified. Our hybrid approach has been extensively tested on three public FJSSP <b>benchmarks,</b> demonstrating superior performance over five state-of-the-art DRL approaches and a widely-used CP solver. Additionally, with the objective of exploring the application to other combinatorial optimization problems, promising preliminary results are presented on applying our hybrid approach to the traveling salesman problem, combining an exact method with a well-known DRL method.</p></p class="citation"></blockquote><h3 id=67--276320-heuristic-reasoning-in-ai-instrumental-use-and-mimetic-absorption-anirban-mukherjee-et-al-2024>(6/7 | 276/320) Heuristic Reasoning in AI: Instrumental Use and Mimetic Absorption (Anirban Mukherjee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anirban Mukherjee, Hannah Hanwen Chang. (2024)<br><strong>Heuristic Reasoning in AI: Instrumental Use and Mimetic Absorption</strong><br><button class=copy-to-clipboard title="Heuristic Reasoning in AI: Instrumental Use and Mimetic Absorption" index=276>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-276 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09404v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09404v2.pdf filename=2403.09404v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deviating from conventional perspectives that frame artificial intelligence (AI) systems solely as logic emulators, we propose a novel program of heuristic <b>reasoning.</b> We distinguish between the &lsquo;instrumental&rsquo; use of heuristics to match resources with objectives, and &lsquo;mimetic absorption,&rsquo; whereby heuristics manifest randomly and universally. Through a series of innovative experiments, including variations of the classic Linda problem and a novel application of the Beauty Contest game, we uncover trade-offs between maximizing accuracy and reducing effort that shape the conditions under which AIs transition between exhaustive logical processing and the use of cognitive shortcuts (heuristics). We provide evidence that AIs manifest an adaptive balancing of precision and efficiency, consistent with principles of resource-rational human cognition as explicated in classical theories of bounded rationality and dual-process theory. Our findings reveal a nuanced picture of AI cognition, where trade-offs between resources and objectives lead to the emulation of biological systems, especially human cognition, despite AIs being designed without a sense of self and lacking introspective capabilities.</p></p class="citation"></blockquote><h3 id=77--277320-a-multi-population-integrated-approach-for-capacitated-location-routing-pengfei-he-et-al-2024>(7/7 | 277/320) A Multi-population Integrated Approach for Capacitated Location Routing (Pengfei He et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pengfei He, Jin-Kao Hao, Qinghua Wu. (2024)<br><strong>A Multi-population Integrated Approach for Capacitated Location Routing</strong><br><button class=copy-to-clipboard title="A Multi-population Integrated Approach for Capacitated Location Routing" index=277>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-277 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09361v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09361v1.pdf filename=2403.09361v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The capacitated location-routing problem involves determining the depots from a set of candidate capacitated depot locations and finding the required routes from the selected depots to serve a set of customers whereas minimizing a cost function that includes the cost of opening the chosen depots, the fixed utilization cost per vehicle used, and the total cost (distance) of the routes. This paper presents a multi-population integrated framework in which a multi-depot edge assembly crossover generates promising offspring solutions from the perspective of both depot location and route edge assembly. The method includes an effective neighborhood-based local search, a feasibility-restoring procedure and a diversification-oriented mutation. Of particular interest is the multi-population scheme which organizes the population into multiple subpopulations based on depot configurations. Extensive experiments on 281 <b>benchmark</b> instances from the literature show that the algorithm performs remarkably well, by improving 101 best-known results (new upper bounds) and matching 84 best-known results. Additional experiments are presented to gain insight into the role of the key elements of the algorithm.</p></p class="citation"></blockquote><h2 id=csni-2>cs.NI (2)</h2><h3 id=12--278320-whittle-index-based-user-association-in-dense-millimeter-wave-networks-mandar-r-nalavade-et-al-2024>(1/2 | 278/320) Whittle Index Based User Association in Dense Millimeter Wave Networks (Mandar R. Nalavade et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mandar R. Nalavade, Gaurav S. Kasbekar, Vivek S. Borkar. (2024)<br><strong>Whittle Index Based User Association in Dense Millimeter Wave Networks</strong><br><button class=copy-to-clipboard title="Whittle Index Based User Association in Dense Millimeter Wave Networks" index=278>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-278 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI<br>Keyword Score: 40<br>Keywords: Bandit Algorithm, Fairness, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09279v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09279v1.pdf filename=2403.09279v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We address the problem of user association in a dense millimeter wave (mmWave) network, in which each arriving user brings a file containing a random number of packets and each time slot is divided into multiple mini-slots. This problem is an instance of the restless multi-armed <b>bandit</b> problem, and is provably hard to solve. Using a technique introduced by Whittle, we relax the hard per-stage constraint that each arriving user must be associated with exactly one mmWave base station (mBS) to a long-term constraint and then use the Lagrangian multiplier technique to convert the problem into an unconstrained problem. This decouples the process governing the system into separate Markov Decision Processes at different mBSs. We prove that the problem is Whittle indexable, present a scheme for computing the Whittle indices of different mBSs, and propose an association scheme under which, each arriving user is associated with the mBS with the smallest value of the Whittle index. Using extensive <b>simulations,</b> we show that the proposed Whittle index based scheme outperforms several user association schemes proposed in prior work in terms of various performance metrics such as average cost, delay, throughput, and Jain&rsquo;s <b>fairness</b> index.</p></p class="citation"></blockquote><h3 id=22--279320-preconfig-a-pretrained-model-for-automating-network-configuration-fuliang-li-et-al-2024>(2/2 | 279/320) PreConfig: A Pretrained Model for Automating Network Configuration (Fuliang Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fuliang Li, Haozhi Lang, Jiajie Zhang, Jiaxing Shen, Xingwei Wang. (2024)<br><strong>PreConfig: A Pretrained Model for Automating Network Configuration</strong><br><button class=copy-to-clipboard title="PreConfig: A Pretrained Model for Automating Network Configuration" index=279>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-279 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI<br>Keyword Score: 10<br>Keywords: Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09369v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09369v1.pdf filename=2403.09369v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Manual network configuration automation (NCA) tools face significant challenges in versatility and flexibility due to their reliance on extensive domain expertise and manual design, limiting their adaptability to diverse scenarios and complex application needs. This paper introduces PreConfig, an innovative NCA tool that leverages a <b>pretrained</b> <b>language</b> <b>model</b> for automating network configuration tasks. PreConfig is designed to address the complexity and variety of NCA tasks by framing them as text-to-text transformation problems, thus unifying the tasks of configuration generation, translation, and analysis under a single, versatile model. Our approach overcomes existing tools&rsquo; limitations by utilizing advances in natural language processing to automatically comprehend and generate network configurations without extensive manual re-engineering. We confront the challenges of integrating domain-specific knowledge into <b>pretrained</b> <b>models</b> <b>and</b> the scarcity of supervision data in the network configuration field. Our solution involves constructing a specialized corpus and further pretraining on network configuration data, coupled with a novel data mining technique for generating task supervision data. The proposed model demonstrates robustness in configuration generation, translation, and analysis, outperforming conventional tools in handling complex networking environments. The experimental results validate the effectiveness of PreConfig, establishing a new direction for automating network configuration tasks with <b>pretrained</b> <b>language</b> <b>models.</b></p></p class="citation"></blockquote><h2 id=cssd-7>cs.SD (7)</h2><h3 id=17--280320-uamix-mae-efficient-tuning-of-pretrained-audio-transformers-with-unsupervised-audio-mixtures-afrina-tabassum-et-al-2024>(1/7 | 280/320) uaMix-MAE: Efficient Tuning of Pretrained Audio Transformers with Unsupervised Audio Mixtures (Afrina Tabassum et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Afrina Tabassum, Dung Tran, Trung Dang, Ismini Lourentzou, Kazuhito Koishida. (2024)<br><strong>uaMix-MAE: Efficient Tuning of Pretrained Audio Transformers with Unsupervised Audio Mixtures</strong><br><button class=copy-to-clipboard title="uaMix-MAE: Efficient Tuning of Pretrained Audio Transformers with Unsupervised Audio Mixtures" index=280>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-280 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-LG, cs-SD, cs.SD, eess-AS<br>Keyword Score: 33<br>Keywords: Autoencoder, Benchmarking, Unsupervised Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09579v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09579v1.pdf filename=2403.09579v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Masked <b>Autoencoders</b> (MAEs) learn rich low-level representations from unlabeled data but require substantial labeled data to effectively adapt to downstream tasks. Conversely, Instance Discrimination (ID) emphasizes high-level semantics, offering a potential solution to alleviate annotation requirements in MAEs. Although combining these two approaches can address downstream tasks with limited labeled data, naively integrating ID into MAEs leads to extended training times and high computational costs. To address this challenge, we introduce uaMix-MAE, an efficient ID tuning strategy that leverages <b>unsupervised</b> audio mixtures. Utilizing contrastive tuning, uaMix-MAE aligns the representations of pretrained MAEs, thereby facilitating effective adaptation to task-specific semantics. To optimize the model with small amounts of unlabeled data, we propose an audio mixing technique that manipulates audio samples in both input and virtual label spaces. Experiments in low/few-shot settings demonstrate that \modelname achieves 4-6% accuracy improvements over various <b>benchmarks</b> when tuned with limited unlabeled data, such as AudioSet-20K. Code is available at <a href=https://github.com/PLAN-Lab/uamix-MAE>https://github.com/PLAN-Lab/uamix-MAE</a></p></p class="citation"></blockquote><h3 id=27--281320-lm2d-lyrics--and-music-driven-dance-synthesis-wenjie-yin-et-al-2024>(2/7 | 281/320) LM2D: Lyrics- and Music-Driven Dance Synthesis (Wenjie Yin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenjie Yin, Xuejiao Zhao, Yi Yu, Hang Yin, Danica Kragic, Mårten Björkman. (2024)<br><strong>LM2D: Lyrics- and Music-Driven Dance Synthesis</strong><br><button class=copy-to-clipboard title="LM2D: Lyrics- and Music-Driven Dance Synthesis" index=281>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-281 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-AI, cs-LG, cs-MM, cs-SD, cs.SD, eess-AS<br>Keyword Score: 26<br>Keywords: Diffusion Model, Knowledge Distillation, Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09407v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09407v1.pdf filename=2403.09407v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Dance typically involves professional choreography with complex movements that follow a musical rhythm and can also be influenced by lyrical content. The integration of lyrics in addition to the auditory dimension, enriches the foundational tone and makes motion generation more amenable to its semantic meanings. However, existing dance synthesis methods tend to model motions only conditioned on audio signals. In this work, we make two contributions to bridge this gap. First, we propose LM2D, a novel probabilistic architecture that incorporates a <b>multimodal</b> <b>diffusion</b> <b>model</b> with consistency <b>distillation,</b> designed to create dance conditioned on both music and lyrics in one <b>diffusion</b> <b>generation</b> step. Second, we introduce the first 3D dance-motion dataset that encompasses both music and lyrics, obtained with pose estimation technologies. We evaluate our model against music-only baseline models with objective metrics and human evaluations, including dancers and choreographers. The results demonstrate LM2D is able to produce realistic and diverse dance matching both lyrics and music. A video summary can be accessed at: <a href=https://youtu.be/4XCgvYookvA>https://youtu.be/4XCgvYookvA</a>.</p></p class="citation"></blockquote><h3 id=37--282320-an-ai-driven-approach-to-wind-turbine-bearing-fault-diagnosis-from-acoustic-signals-zhao-wang-et-al-2024>(3/7 | 282/320) An AI-Driven Approach to Wind Turbine Bearing Fault Diagnosis from Acoustic Signals (Zhao Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhao Wang, Xiaomeng Li, Na Li, Longlong Shu. (2024)<br><strong>An AI-Driven Approach to Wind Turbine Bearing Fault Diagnosis from Acoustic Signals</strong><br><button class=copy-to-clipboard title="An AI-Driven Approach to Wind Turbine Bearing Fault Diagnosis from Acoustic Signals" index=282>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-282 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-LG, cs-SD, cs.SD, eess-AS<br>Keyword Score: 20<br>Keywords: Convolution, LSTM<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09030v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09030v1.pdf filename=2403.09030v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study aimed to develop a deep learning model for the classification of bearing faults in wind turbine generators from acoustic signals. A <b>convolutional</b> <b>LSTM</b> model was successfully constructed and trained by using audio data from five predefined fault types for both training and validation. To create the dataset, raw audio signal data was collected and processed in frames to capture time and frequency domain information. The model exhibited outstanding accuracy on training samples and demonstrated excellent generalization ability during validation, indicating its proficiency of generalization capability. On the test samples, the model achieved remarkable classification performance, with an overall accuracy exceeding 99.5%, and a false positive rate of less than 1% for normal status. The findings of this study provide essential support for the diagnosis and maintenance of bearing faults in wind turbine generators, with the potential to enhance the reliability and efficiency of wind power generation.</p></p class="citation"></blockquote><h3 id=47--283320-spoken-100-a-cross-lingual-benchmarking-dataset-for-the-classification-of-spoken-numbers-in-different-languages-rené-groh-et-al-2024>(4/7 | 283/320) SpokeN-100: A Cross-Lingual Benchmarking Dataset for The Classification of Spoken Numbers in Different Languages (René Groh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>René Groh, Nina Goes, Andreas M. Kist. (2024)<br><strong>SpokeN-100: A Cross-Lingual Benchmarking Dataset for The Classification of Spoken Numbers in Different Languages</strong><br><button class=copy-to-clipboard title="SpokeN-100: A Cross-Lingual Benchmarking Dataset for The Classification of Spoken Numbers in Different Languages" index=283>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-283 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-AI, cs-SD, cs.SD, eess-AS<br>Keyword Score: 16<br>Keywords: Benchmarking, Benchmarking, Automatic Speech Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09753v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09753v1.pdf filename=2403.09753v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Benchmarking</b> plays a pivotal role in assessing and enhancing the performance of compact deep learning models designed for execution on resource-constrained devices, such as microcontrollers. Our study introduces a novel, entirely artificially generated <b>benchmarking</b> dataset tailored for <b>speech</b> <b>recognition,</b> representing a core challenge in the field of tiny deep learning. SpokeN-100 consists of spoken numbers from 0 to 99 spoken by 32 different speakers in four different languages, namely English, Mandarin, German and French, resulting in 12,800 audio samples. We determine auditory features and use UMAP (Uniform Manifold Approximation and Projection for Dimension Reduction) as a dimensionality reduction method to show the diversity and richness of the dataset. To highlight the use case of the dataset, we introduce two <b>benchmark</b> tasks: given an audio sample, classify (i) the used language and/or (ii) the spoken number. We optimized state-of-the-art deep neural networks and performed an evolutionary neural architecture search to find tiny architectures optimized for the 32-bit ARM Cortex-M4 nRF52840 microcontroller. Our results represent the first <b>benchmark</b> data achieved for SpokeN-100.</p></p class="citation"></blockquote><h3 id=57--284320-the-neural-srp-method-for-positional-sound-source-localization-eric-grinstein-et-al-2024>(5/7 | 284/320) The Neural-SRP method for positional sound source localization (Eric Grinstein et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Eric Grinstein, Toon van Waterschoot, Mike Brookes, Patrick A. Naylor. (2024)<br><strong>The Neural-SRP method for positional sound source localization</strong><br><button class=copy-to-clipboard title="The Neural-SRP method for positional sound source localization" index=284>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-284 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-SD, cs.SD, eess-AS<br>Keyword Score: 10<br>Keywords: Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09455v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09455v1.pdf filename=2403.09455v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Steered Response Power (SRP) is a widely used method for the task of sound source localization using microphone arrays, showing satisfactory localization performance on many practical scenarios. However, its performance is diminished under highly reverberant environments. Although Deep Neural Networks (DNNs) have been previously proposed to overcome this limitation, most are trained for a specific number of microphones with fixed spatial coordinates. This restricts their practical application on scenarios frequently observed in wireless acoustic sensor networks, where each application has an ad-hoc microphone topology. We propose Neural-SRP, a DNN which combines the flexibility of SRP with the performance gains of DNNs. We train our network using simulated data and <b>transfer</b> <b>learning,</b> and evaluate our approach on recorded and simulated data. Results verify that Neural-SRP&rsquo;s localization performance significantly outperforms the baselines.</p></p class="citation"></blockquote><h3 id=67--285320-a-practical-guide-to-spectrogram-analysis-for-audio-signal-processing-zulfidin-khodzhaev-2024>(6/7 | 285/320) A Practical Guide to Spectrogram Analysis for Audio Signal Processing (Zulfidin Khodzhaev, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zulfidin Khodzhaev. (2024)<br><strong>A Practical Guide to Spectrogram Analysis for Audio Signal Processing</strong><br><button class=copy-to-clipboard title="A Practical Guide to Spectrogram Analysis for Audio Signal Processing" index=285>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-285 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-SD, cs.SD, eess-AS<br>Keyword Score: 10<br>Keywords: Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09321v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09321v1.pdf filename=2403.09321v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The paper <b>summarizes</b> spectrogram and gives practical application of spectrogram in signal processing. For analysis, finger-snapping is recorded with a sampling rate of 441000 Hz and 96000 Hz. The effects of the number of segments on the Power Spectral Density (PSD) and spectrogram are analyzed and visualized.</p></p class="citation"></blockquote><h3 id=77--286320-more-than-words-advancements-and-challenges-in-speech-recognition-for-singing-anna-kruspe-2024>(7/7 | 286/320) More than words: Advancements and challenges in speech recognition for singing (Anna Kruspe, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anna Kruspe. (2024)<br><strong>More than words: Advancements and challenges in speech recognition for singing</strong><br><button class=copy-to-clipboard title="More than words: Advancements and challenges in speech recognition for singing" index=286>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-286 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-CL, cs-IR, cs-LG, cs-SD, cs.SD, eess-AS<br>Keyword Score: 10<br>Keywords: Automatic Speech Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09298v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09298v1.pdf filename=2403.09298v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper addresses the challenges and advancements in <b>speech</b> <b>recognition</b> for singing, a domain distinctly different from standard <b>speech</b> <b>recognition.</b> Singing encompasses unique challenges, including extensive pitch variations, diverse vocal styles, and background music interference. We explore key areas such as phoneme recognition, language identification in songs, keyword spotting, and full lyrics transcription. I will describe some of my own experiences when performing research on these tasks just as they were starting to gain traction, but will also show how recent developments in deep learning and large-scale datasets have propelled progress in this field. My goal is to illuminate the complexities of applying <b>speech</b> <b>recognition</b> to singing, evaluate current capabilities, and outline future research directions.</p></p class="citation"></blockquote><h2 id=csdb-1>cs.DB (1)</h2><h3 id=11--287320-query-rewriting-via-large-language-models-jie-liu-et-al-2024>(1/1 | 287/320) Query Rewriting via Large Language Models (Jie Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jie Liu, Barzan Mozafari. (2024)<br><strong>Query Rewriting via Large Language Models</strong><br><button class=copy-to-clipboard title="Query Rewriting via Large Language Models" index=287>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-287 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DB<br>Categories: cs-DB, cs.DB<br>Keyword Score: 33<br>Keywords: Benchmarking, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09060v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09060v1.pdf filename=2403.09060v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Query rewriting is one of the most effective techniques for coping with poorly written queries before passing them down to the query optimizer. Manual rewriting is not scalable, as it is error-prone and requires deep expertise. Similarly, traditional query rewriting algorithms can only handle a small subset of queries: rule-based techniques do not generalize to new query patterns and synthesis-based techniques cannot handle complex queries. Fortunately, the rise of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> equipped with broad general knowledge and advanced <b>reasoning</b> capabilities, has created hopes for solving some of these previously open problems. In this paper, we present GenRewrite, the first holistic system that leverages <b>LLMs</b> for query rewriting. We introduce the notion of Natural Language Rewrite Rules (NLR2s), and use them as hints to the <b>LLM</b> but also a means for transferring knowledge from rewriting one query to another, and thus becoming smarter and more effective over time. We present a novel counterexample-guided technique that iteratively corrects the syntactic and semantic errors in the rewritten query, significantly reducing the <b>LLM</b> costs and the manual effort required for verification. GenRewrite speeds up 22 out of 99 TPC queries (the most complex public <b>benchmark)</b> by more than 2x, which is 2.5x&ndash;3.2x higher coverage than state-of-the-art traditional query rewriting and 2.1x higher than the out-of-the-box <b>LLM</b> baseline.</p></p class="citation"></blockquote><h2 id=physicsspace-ph-1>physics.space-ph (1)</h2><h3 id=11--288320-forecasting-geoffective-events-from-solar-wind-data-and-evaluating-the-most-predictive-features-through-machine-learning-approaches-sabrina-guastavino-et-al-2024>(1/1 | 288/320) Forecasting Geoffective Events from Solar Wind Data and Evaluating the Most Predictive Features through Machine Learning Approaches (Sabrina Guastavino et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sabrina Guastavino, Katsiaryna Bahamazava, Emma Perracchione, Fabiana Camattari, Gianluca Audone, Daniele Telloni, Roberto Susino, Gianalfredo Nicolini, Silvano Fineschi, Michele Piana, Anna Maria Massone. (2024)<br><strong>Forecasting Geoffective Events from Solar Wind Data and Evaluating the Most Predictive Features through Machine Learning Approaches</strong><br><button class=copy-to-clipboard title="Forecasting Geoffective Events from Solar Wind Data and Evaluating the Most Predictive Features through Machine Learning Approaches" index=288>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-288 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.space-ph<br>Categories: 85-08, 68T07, 68T05, astro-ph-SR, cs-AI, physics-space-ph, physics.space-ph<br>Keyword Score: 30<br>Keywords: LSTM, LSTM, Recurrent Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09847v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09847v1.pdf filename=2403.09847v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study addresses the prediction of geomagnetic disturbances by exploiting machine learning techniques. Specifically, the <b>Long-Short</b> <b>Term</b> <b>Memory</b> <b>recurrent</b> <b>neural</b> <b>network,</b> which is particularly suited for application over <b>long</b> <b>time</b> <b>series,</b> <b>is</b> employed in the analysis of in-situ measurements of solar wind plasma and magnetic field acquired over more than one solar cycle, from $2005$ to $2019$, at the Lagrangian point L$1$. The problem is approached as a binary classification aiming to predict one hour in advance a decrease in the SYM-H geomagnetic activity index below the threshold of $-50$ nT, which is generally regarded as indicative of magnetospheric perturbations. The strong class imbalance issue is tackled by using an appropriate loss function tailored to optimize appropriate skill scores in the training phase of the neural network. Beside classical skill scores, value-weighted skill scores are then employed to evaluate predictions, suitable in the study of problems, such as the one faced here, characterized by strong temporal variability. For the first time, the content of magnetic helicity and energy carried by solar transients, associated with their detection and likelihood of geo-effectiveness, were considered as input features of the network architecture. Their predictive capabilities are demonstrated through a correlation-driven feature selection method to rank the most relevant characteristics involved in the neural network prediction model. The optimal performance of the adopted neural network in properly forecasting the onset of geomagnetic storms, which is a crucial point for giving real warnings in an operational setting, is finally showed.</p></p class="citation"></blockquote><h2 id=csdc-2>cs.DC (2)</h2><h3 id=12--289320-burstattention-an-efficient-distributed-attention-framework-for-extremely-long-sequences-sun-ao-et-al-2024>(1/2 | 289/320) BurstAttention: An Efficient Distributed Attention Framework for Extremely Long Sequences (Sun Ao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sun Ao, Weilin Zhao, Xu Han, Cheng Yang, Zhiyuan Liu, Chuan Shi, Maosong Sun, Shengnan Wang, Teng Su. (2024)<br><strong>BurstAttention: An Efficient Distributed Attention Framework for Extremely Long Sequences</strong><br><button class=copy-to-clipboard title="BurstAttention: An Efficient Distributed Attention Framework for Extremely Long Sequences" index=289>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-289 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs-LG, cs.DC<br>Keyword Score: 30<br>Keywords: Transformer, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09347v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09347v1.pdf filename=2403.09347v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Effective attention modules have played a crucial role in the success of <b>Transformer-based</b> <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> but the quadratic time and memory complexities of these attention modules also pose a challenge when processing long sequences. One potential solution for the long sequence problem is to utilize distributed clusters to parallelize the computation of attention modules across multiple devices (e.g., GPUs). However, adopting a distributed approach inevitably introduces extra memory overheads to store local attention results and incurs additional communication costs to aggregate local results into global ones. In this paper, we propose a distributed attention framework named ``BurstAttention&rsquo;&rsquo; to optimize memory access and communication operations at both the global cluster and local device levels. In our experiments, we compare BurstAttention with other competitive distributed attention solutions for long sequence processing. The experimental results under different length settings demonstrate that BurstAttention offers significant advantages for processing long sequences compared with these competitive baselines, reducing 40% communication overheads and achieving 2 X speedup during training 32K sequence length on 8 X A100.</p></p class="citation"></blockquote><h3 id=22--290320-benchmarking-distributed-coordination-systems-a-survey-and-analysis-bekir-turkkan-et-al-2024>(2/2 | 290/320) Benchmarking Distributed Coordination Systems: A Survey and Analysis (Bekir Turkkan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bekir Turkkan, Tevfik Kosar, Aleksey Charapko, Ailidani Ailijiang, Murat Demirbas. (2024)<br><strong>Benchmarking Distributed Coordination Systems: A Survey and Analysis</strong><br><button class=copy-to-clipboard title="Benchmarking Distributed Coordination Systems: A Survey and Analysis" index=290>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-290 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs.DC<br>Keyword Score: 6<br>Keywords: Benchmarking, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09445v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09445v1.pdf filename=2403.09445v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Coordination services and protocols are critical components of distributed systems and are essential for providing consistency, fault tolerance, and scalability. However, due to lack of a standard <b>benchmarking</b> tool for distributed coordination services, coordination service developers/researchers either use a NoSQL standard <b>benchmark</b> and omit evaluating consistency, distribution, and fault-tolerance; or create their own ad-hoc microbenchmarks and skip comparability with other services. In this paper, we analyze and compare known and widely used distributed coordination services, their evaluations, and the tools used to <b>benchmark</b> those systems. We identify important requirements of distributed coordination service <b>benchmarking,</b> like the metrics and parameters that need to be evaluated and their evaluation setups and tools.</p></p class="citation"></blockquote><h2 id=physicsoptics-1>physics.optics (1)</h2><h3 id=11--291320-compute-first-optical-detection-for-noise-resilient-visual-perception-jungmin-kim-et-al-2024>(1/1 | 291/320) Compute-first optical detection for noise-resilient visual perception (Jungmin Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jungmin Kim, Nanfang Yu, Zongfu Yu. (2024)<br><strong>Compute-first optical detection for noise-resilient visual perception</strong><br><button class=copy-to-clipboard title="Compute-first optical detection for noise-resilient visual perception" index=291>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-291 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.optics<br>Categories: cs-CV, cs-LG, eess-IV, physics-optics, physics.optics<br>Keyword Score: 23<br>Keywords: MNIST, Benchmarking, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09612v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09612v1.pdf filename=2403.09612v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the context of visual perception, the optical signal from a scene is transferred into the electronic domain by detectors in the form of image data, which are then processed for the extraction of visual information. In noisy and weak-signal environments such as thermal imaging for night vision applications, however, the performance of neural computing tasks faces a significant bottleneck due to the inherent degradation of data quality upon noisy detection. Here, we propose a concept of optical signal processing before detection to address this issue. We demonstrate that spatially redistributing optical signals through a properly designed linear <b>transformer</b> can enhance the detection noise resilience of visual perception tasks, as <b>benchmarked</b> with the <b>MNIST</b> classification. Our idea is supported by a quantitative analysis detailing the relationship between signal concentration and noise robustness, as well as its practical implementation in an incoherent imaging system. This compute-first detection scheme can pave the way for advancing infrared machine vision technologies widely used for industrial and defense applications.</p></p class="citation"></blockquote><h2 id=csce-2>cs.CE (2)</h2><h3 id=12--292320-a-mixed-order-quasicontinuum-approach-for-beam-based-architected-materials-with-application-to-fracture-kevin-kraschewski-et-al-2024>(1/2 | 292/320) A mixed-order quasicontinuum approach for beam-based architected materials with application to fracture (Kevin Kraschewski et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kevin Kraschewski, Gregory P. Phlipot, Dennis M. Kochmann. (2024)<br><strong>A mixed-order quasicontinuum approach for beam-based architected materials with application to fracture</strong><br><button class=copy-to-clipboard title="A mixed-order quasicontinuum approach for beam-based architected materials with application to fracture" index=292>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-292 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CE<br>Categories: cs-CE, cs.CE<br>Keyword Score: 23<br>Keywords: Benchmarking, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09495v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09495v1.pdf filename=2403.09495v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Predicting the mechanics of large structural networks, such as beam-based architected materials, requires a multiscale computational strategy that preserves information about the discrete structure while being applicable to large assemblies of struts. Especially the fracture properties of such beam lattices necessitate a two-scale modeling strategy, since the fracture toughness depends on discrete beam failure events, while the application of remote loads requires large <b>simulation</b> domains. As classical homogenization techniques fail in the absence of a separation of scales at the crack tip, we present a concurrent multiscale technique: a fully-nonlocal quasicontinuum (QC) multi-lattice formulation for beam networks, based on a conforming mesh. Like the original atomistic QC formulation, we maintain discrete resolution where needed (such as around a crack tip) while efficiently coarse-graining in the remaining <b>simulation</b> domain. A key challenge is a suitable model in the coarse-grained domain, where classical QC uses affine interpolations. This formulation fails in bending-dominated lattices, as it overconstrains the lattice by preventing bending without stretching of beams. Therefore, we here present a beam QC formulation based on mixed-order interpolation in the coarse-grained region &ndash; combining the efficiency of linear interpolation where possible with the accuracy advantages of quadratic interpolation where needed. This results in a powerful computational framework, which, as we demonstrate through our validation and <b>benchmark</b> examples, overcomes the deficiencies of previous QC formulations and enables, e.g., the prediction of the fracture toughness and the diverse nature of stress distributions of stretching- and bending-dominated beam lattices in two and three dimensions.</p></p class="citation"></blockquote><h3 id=22--293320-modular-parametric-pgd-enabling-online-solution-of-partial-differential-equations-angelo-pasquale-et-al-2024>(2/2 | 293/320) Modular parametric PGD enabling online solution of partial differential equations (Angelo Pasquale et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Angelo Pasquale, Mohammad-Javad Kazemzadeh-Parsi, Daniele Di Lorenzo, Victor Champaney, Amine Ammar, Francisco Chinesta. (2024)<br><strong>Modular parametric PGD enabling online solution of partial differential equations</strong><br><button class=copy-to-clipboard title="Modular parametric PGD enabling online solution of partial differential equations" index=293>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-293 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CE<br>Categories: cs-CE, cs.CE<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09312v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09312v1.pdf filename=2403.09312v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the present work, a new methodology is proposed for building surrogate parametric models of engineering systems based on modular assembly of pre-solved modules. Each module is a generic parametric solution considering parametric <b>geometry,</b> material and boundary conditions. By assembling these modules and satisfying continuity constraints at the interfaces, a parametric surrogate model of the full problem can be obtained. In the present paper, the PGD technique in connection with NURBS <b>geometry</b> representation is used to create a parametric model for each module. In this technique, the NURBS objects allow to map the governing boundary value problem from a parametric non-regular domain into a regular reference domain and the PGD is used to create a reduced model in the reference domain. In the assembly stage, an optimization problem is solved to satisfy the continuity constraints at the interfaces. The proposed procedure is based on the offline&ndash;online paradigm: the offline stage consists of creating multiple pre-solved modules which can be afterwards assembled in almost real-time during the online stage, enabling quick evaluations of the full system response. To show the potential of the proposed approach some numerical examples in heat conduction and structural plates under bending are presented.</p></p class="citation"></blockquote><h2 id=csit-6>cs.IT (6)</h2><h3 id=16--294320-smart-resource-allocation-at-mmwavethz-frequencies-with-cooperative-rate-splitting-hyesang-cho-et-al-2024>(1/6 | 294/320) Smart Resource Allocation at mmWave/THz Frequencies with Cooperative Rate-Splitting (Hyesang Cho et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hyesang Cho, Junil Choi. (2024)<br><strong>Smart Resource Allocation at mmWave/THz Frequencies with Cooperative Rate-Splitting</strong><br><button class=copy-to-clipboard title="Smart Resource Allocation at mmWave/THz Frequencies with Cooperative Rate-Splitting" index=294>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-294 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 23<br>Keywords: Benchmarking, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09022v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09022v2.pdf filename=2403.09022v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we propose algorithms to minimize the energy consumption in millimeter wave/terahertz multi-user downlink communication systems. To ensure coverage in blockage-vulnerable high frequency systems, we consider cooperative rate-splitting (CRS) and transmission over multiple time blocks, where via CRS, multiple users cooperate to assist a blocked user. Moreover, we show that transmission over multiple time blocks provides benefits through smart resource allocation. We first propose a communication framework named improved distinct extraction-based CRS (iDeCRS) that utilizes the benefits of rate-splitting. With our transmission framework, we derive a performance <b>benchmark</b> assuming genie channel state information (CSI), i.e., the channels of the present and future time blocks are known, denoted as GENIE. Using the results from GENIE, we derive a novel efficiency constrained optimization (ECO) algorithm assuming instantaneous CSI. In addition, a simple but effective even data transmission (EDT) algorithm that promotes steady transmission along the time blocks is proposed. <b>Simulation</b> results show that ECO and EDT have satisfactory performances compared to GENIE. The results also show that ECO outperforms EDT when many users are cooperating, and vise versa.</p></p class="citation"></blockquote><h3 id=26--295320-localization-in-digital-twin-mimo-networks-a-case-for-massive-fingerprinting-joão-morais-et-al-2024>(2/6 | 295/320) Localization in Digital Twin MIMO Networks: A Case for Massive Fingerprinting (João Morais et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>João Morais, Ahmed Alkhateeb. (2024)<br><strong>Localization in Digital Twin MIMO Networks: A Case for Massive Fingerprinting</strong><br><button class=copy-to-clipboard title="Localization in Digital Twin MIMO Networks: A Case for Massive Fingerprinting" index=295>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-295 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09614v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09614v1.pdf filename=2403.09614v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Localization in outdoor wireless systems typically requires transmitting specific reference signals to estimate distance (trilateration methods) or angle (triangulation methods). These cause overhead on communication, need a LoS link to work well, and require multiple base stations, often imposing synchronization or specific hardware requirements. Fingerprinting has none of these drawbacks, but building its database requires high human effort to collect real-world measurements. For a long time, this issue limited the size of databases and thus their performance. This work proposes significantly reducing human effort in building fingerprinting databases by populating them with \textit{digital twin RF maps}. These RF maps are built from ray-tracing <b>simulations</b> on a digital replica of the environment across several frequency bands and beamforming configurations. Online user fingerprints are then matched against this spatial database. The approach was evaluated with practical <b>simulations</b> using realistic propagation models and user measurements. Our experiments show sub-meter localization errors on a NLoS location 95% of the time using sensible user measurement report sizes. Results highlight the promising potential of the proposed digital twin approach for ubiquitous wide-area 6G localization.</p></p class="citation"></blockquote><h3 id=36--296320-joint-port-selection-and-beamforming-design-for-fluid-antenna-assisted-integrated-data-and-energy-transfer-long-zhang-et-al-2024>(3/6 | 296/320) Joint Port Selection and Beamforming Design for Fluid Antenna Assisted Integrated Data and Energy Transfer (Long Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Long Zhang, Halvin Yang, Yizhe Zhao, Jie Hu. (2024)<br><strong>Joint Port Selection and Beamforming Design for Fluid Antenna Assisted Integrated Data and Energy Transfer</strong><br><button class=copy-to-clipboard title="Joint Port Selection and Beamforming Design for Fluid Antenna Assisted Integrated Data and Energy Transfer" index=296>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-296 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09357v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09357v1.pdf filename=2403.09357v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Integrated data and energy transfer (IDET) has been of fundamental importance for providing both wireless data transfer (WDT) and wireless energy transfer (WET) services towards low-power devices. Fluid antenna (FA) is capable of exploiting the huge spatial diversity of the wireless channel to enhance the receive signal strength, which is more suitable for the tiny-size low-power devices having the IDET requirements. In this letter, a multiuser FA assisted IDET system is studied and the weighted energy harvesting power at energy receivers (ERs) is maximized by jointly optimizing the port selection and transmit beamforming design under imperfect channel state information (CSI), while the signal-to-interference-plus-noise ratio (SINR) constraint for each data receiver (DR) is satisfied. An efficient algorithm is proposed to obtain the suboptimal solutions for the non-convex problem. <b>Simulation</b> results evaluate the performance of the FA-IDET system, while also demonstrate that FA outperforms the multi-input-multi-output (MIMO) counterpart in terms of the IDET performance, as long as the port number is large enough.</p></p class="citation"></blockquote><h3 id=46--297320-reverse-em-problem-based-on-bregman-divergence-and-its-application-to-classical-and-quantum-information-theory-masahito-hayashi-2024>(4/6 | 297/320) Reverse em-problem based on Bregman divergence and its application to classical and quantum information theory (Masahito Hayashi, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Masahito Hayashi. (2024)<br><strong>Reverse em-problem based on Bregman divergence and its application to classical and quantum information theory</strong><br><button class=copy-to-clipboard title="Reverse em-problem based on Bregman divergence and its application to classical and quantum information theory" index=297>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-297 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, math-IT, math-OC, quant-ph<br>Keyword Score: 15<br>Keywords: Geometry, Mutual Information<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09252v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09252v1.pdf filename=2403.09252v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The recent paper (IEEE Trans. IT 69, 1680) introduced an analytical method for calculating the channel capacity without the need for iteration. This method has certain limitations that restrict its applicability. Furthermore, the paper does not provide an explanation as to why the channel capacity can be solved analytically in this particular case. In order to broaden the scope of this method and address its limitations, we turn our attention to the reverse em-problem, proposed by Toyota (Information <b>Geometry,</b> 3, 1355 (2020)). This reverse em-problem involves iteratively applying the inverse map of the em iteration to calculate the channel capacity, which represents the maximum <b>mutual</b> <b>information.</b> However, several open problems remained unresolved in Toyota&rsquo;s work. To overcome these challenges, we formulate the reverse em-problem based on Bregman divergence and provide solutions to these open problems. Building upon these results, we transform the reverse em-problem into em-problems and derive a non-iterative formula for the reverse em-problem. This formula can be viewed as a generalization of the aforementioned analytical calculation method. Importantly, this derivation sheds light on the information geometrical structure underlying this special case. By effectively addressing the limitations of the previous analytical method and providing a deeper understanding of the underlying information geometrical structure, our work significantly expands the applicability of the proposed method for calculating the channel capacity without iteration.</p></p class="citation"></blockquote><h3 id=56--298320-a-deep-reinforcement-learning-approach-for-autonomous-reconfigurable-intelligent-surfaces-hyuckjin-choi-et-al-2024>(5/6 | 298/320) A Deep Reinforcement Learning Approach for Autonomous Reconfigurable Intelligent Surfaces (Hyuckjin Choi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hyuckjin Choi, Ly V. Nguyen, Junil Choi, A. Lee Swindlehurst. (2024)<br><strong>A Deep Reinforcement Learning Approach for Autonomous Reconfigurable Intelligent Surfaces</strong><br><button class=copy-to-clipboard title="A Deep Reinforcement Learning Approach for Autonomous Reconfigurable Intelligent Surfaces" index=298>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-298 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09270v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09270v2.pdf filename=2403.09270v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A reconfigurable intelligent surface (RIS) is a prospective wireless technology that enhances wireless channel quality. An RIS is often equipped with passive array of elements and provides cost and power-efficient solutions for coverage extension of wireless communication systems. Without any radio frequency (RF) chains or computing resources, however, the RIS requires control information to be sent to it from an external unit, e.g., a base station (BS). The control information can be delivered by wired or wireless channels, and the BS must be aware of the RIS and the RIS-related channel conditions in order to effectively configure its behavior. Recent works have introduced hybrid RIS structures possessing a few active elements that can sense and digitally process received data. Here, we propose the operation of an entirely autonomous RIS that operates without a control link between the RIS and BS. Using a few sensing elements, the autonomous RIS employs a deep Q network (DQN) based on <b>reinforcement</b> <b>learning</b> in order to enhance the sum rate of the network. Our results illustrate the potential of deploying autonomous RISs in wireless networks with essentially no network overhead.</p></p class="citation"></blockquote><h3 id=66--299320-performance-analysis-on-ris-aided-wideband-massive-mimo-ofdm-systems-with-low-resolution-adcs-xianzhe-chen-et-al-2024>(6/6 | 299/320) Performance Analysis on RIS-Aided Wideband Massive MIMO OFDM Systems with Low-Resolution ADCs (Xianzhe Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xianzhe Chen, Hong Ren, Cunhua Pan, Zhangjie Peng, Kangda Zhi, Yong Liu, Xiaojun Xi, Ana Garcia Armada, Cheng-Xiang Wang. (2024)<br><strong>Performance Analysis on RIS-Aided Wideband Massive MIMO OFDM Systems with Low-Resolution ADCs</strong><br><button class=copy-to-clipboard title="Performance Analysis on RIS-Aided Wideband Massive MIMO OFDM Systems with Low-Resolution ADCs" index=299>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-299 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 10<br>Keywords: Scaling Law<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09058v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09058v1.pdf filename=2403.09058v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper investigates a reconfigurable intelligent surface (RIS)-aided wideband massive multiple-input multiple-output (MIMO) orthogonal frequency division multiplexing (OFDM) system with low-resolution analog-to-digital converters (ADCs). Frequency-selective Rician fading channels are considered, and the OFDM data transmission process is presented in time domain. This paper derives the closed-form approximate expression of the uplink achievable rate, based on which the asymptotic system performance is analyzed when the number of the antennas at the base station and the number of reflecting elements at the RIS grow to infinity. Besides, the power <b>scaling</b> <b>laws</b> of the considered system are revealed to provide energy-saving insights. Furthermore, this paper proposes a gradient ascent-based algorithm to design the phase shifts of the RIS for maximizing the minimum user rate. Finally, numerical results are presented to verify the correctness of analytical conclusions and draw insights.</p></p class="citation"></blockquote><h2 id=nlinao-1>nlin.AO (1)</h2><h3 id=11--300320-implementation-of-parallel-process-execution-in-the-next-generation-system-analysis-model-harish-gadey-et-al-2024>(1/1 | 300/320) Implementation of Parallel Process Execution in the Next Generation System Analysis Model (Harish Gadey et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Harish Gadey, Lucas Vander Wal, Robert Joseph. (2024)<br><strong>Implementation of Parallel Process Execution in the Next Generation System Analysis Model</strong><br><button class=copy-to-clipboard title="Implementation of Parallel Process Execution in the Next Generation System Analysis Model" index=300>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-300 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: nlin.AO<br>Categories: cs-SY, eess-SY, nlin-AO, nlin.AO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09848v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09848v1.pdf filename=2403.09848v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The United States DOE Office of Integrated Waste Management program is planning for the transportation, storage, and eventual disposal of spent nuclear fuel and high-level radioactive waste from nuclear power plant sites across the United States. The Next Generation System Analysis Model is an agent-based <b>simulation</b> toolkit that is used for system level <b>simulation</b> and analysis of the SNF inventory in the United States. This tool was developed as part of a collaborative effort between Argonne National Laboratory and Oak Ridge National Laboratory. The analyst using NGSAM has the ability to define several factors like the number of storage facilities, capacity at each facility, transportation schedules, shipment rates, and other conditions. The primary purpose of NGSAM is to provide the system analyst with the tools to model the integrated waste management system and gain insights on waste management alternatives, impact of storage choices, generating cost estimates, and developing an integrated approach with emphasis on flexibility.</p></p class="citation"></blockquote><h2 id=eessas-1>eess.AS (1)</h2><h3 id=11--301320-ptsd-mdnn--fusion-tardive-de-réseaux-de-neurones-profonds-multimodaux-pour-la-détection-du-trouble-de-stress-post-traumatique-long-nguyen-phuoc-et-al-2024>(1/1 | 301/320) PTSD-MDNN : Fusion tardive de réseaux de neurones profonds multimodaux pour la détection du trouble de stress post-traumatique (Long Nguyen-Phuoc et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Long Nguyen-Phuoc, Renald Gaboriau, Dimitri Delacroix, Laurent Navarro. (2024)<br><strong>PTSD-MDNN : Fusion tardive de réseaux de neurones profonds multimodaux pour la détection du trouble de stress post-traumatique</strong><br><button class=copy-to-clipboard title="PTSD-MDNN : Fusion tardive de réseaux de neurones profonds multimodaux pour la détection du trouble de stress post-traumatique" index=301>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-301 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-AI, cs-CV, cs-SD, eess-AS, eess-IV, eess.AS, q-bio-NC<br>Keyword Score: 20<br>Keywords: Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.10565v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.10565v1.pdf filename=2403.10565v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In order to provide a more objective and quicker way to diagnose post-traumatic stress disorder (PTSD), we present PTSD-MDNN which merges two unimodal <b>convolutional</b> <b>neural</b> <b>networks</b> and which gives low detection error rate. By taking only videos and audios as inputs, the model could be used in the configuration of teleconsultation sessions, in the optimization of patient journeys or for human-robot interaction.</p></p class="citation"></blockquote><h2 id=statml-2>stat.ML (2)</h2><h3 id=12--302320-visa-variational-inference-with-sequential-sample-average-approximations-heiko-zimmermann-et-al-2024>(1/2 | 302/320) VISA: Variational Inference with Sequential Sample-Average Approximations (Heiko Zimmermann et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Heiko Zimmermann, Christian A. Naesseth, Jan-Willem van de Meent. (2024)<br><strong>VISA: Variational Inference with Sequential Sample-Average Approximations</strong><br><button class=copy-to-clipboard title="VISA: Variational Inference with Sequential Sample-Average Approximations" index=302>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-302 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09429v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09429v2.pdf filename=2403.09429v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present variational inference with sequential sample-average approximation (VISA), a method for approximate inference in computationally intensive models, such as those based on numerical <b>simulations.</b> VISA extends importance-weighted forward-KL variational inference by employing a sequence of sample-average approximations, which are considered valid inside a trust region. This makes it possible to reuse model evaluations across multiple gradient steps, thereby reducing computational cost. We perform experiments on high-dimensional Gaussians, Lotka-Volterra dynamics, and a Pickover attractor, which demonstrate that VISA can achieve comparable approximation accuracy to standard importance-weighted forward-KL variational inference with computational savings of a factor two or more for conservatively chosen learning rates.</p></p class="citation"></blockquote><h3 id=22--303320-pantypes-diverse-representatives-for-self-explainable-models-rune-kjærsgaard-et-al-2024>(2/2 | 303/320) Pantypes: Diverse Representatives for Self-Explainable Models (Rune Kjærsgaard et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rune Kjærsgaard, Ahcène Boubekki, Line Clemmensen. (2024)<br><strong>Pantypes: Diverse Representatives for Self-Explainable Models</strong><br><button class=copy-to-clipboard title="Pantypes: Diverse Representatives for Self-Explainable Models" index=303>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-303 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09383v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09383v1.pdf filename=2403.09383v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Prototypical self-explainable classifiers have emerged to meet the growing demand for interpretable AI systems. These classifiers are designed to incorporate high transparency in their decisions by basing inference on similarity with learned prototypical objects. While these models are designed with diversity in mind, the learned prototypes often do not sufficiently represent all aspects of the input distribution, particularly those in low density regions. Such lack of sufficient data representation, known as representation bias, has been associated with various detrimental properties related to machine learning diversity and <b>fairness.</b> In light of this, we introduce pantypes, a new family of prototypical objects designed to capture the full diversity of the input distribution through a sparse set of objects. We show that pantypes can empower prototypical self-explainable models by occupying divergent regions of the latent space and thus fostering high diversity, interpretability and <b>fairness.</b></p></p class="citation"></blockquote><h2 id=mathoc-1>math.OC (1)</h2><h3 id=11--304320-solvability-of-the-inverse-optimal-control-problem-based-on-the-minimum-principle-afreen-islam-et-al-2024>(1/1 | 304/320) Solvability of the Inverse Optimal Control problem based on the minimum principle (Afreen Islam et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Afreen Islam, Guido Herrmann, Joaquin Carrasco. (2024)<br><strong>Solvability of the Inverse Optimal Control problem based on the minimum principle</strong><br><button class=copy-to-clipboard title="Solvability of the Inverse Optimal Control problem based on the minimum principle" index=304>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-304 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-SY, eess-SY, math-OC, math.OC<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09375v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09375v1.pdf filename=2403.09375v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, the solvability of the Inverse Optimal Control (IOC) problem based on two existing minimum principal methods, is analysed. The aim of this work is to answer the question regarding what kinds of trajectories, that is depending on the initial conditions of the closed-loop system and system dynamics, of the original optimal control problem, will result in the recovery of the true weights of the reward function for both the soft and the hard-constrained methods [1], [2]. Analytical conditions are provided which allow to verify if a trajectory is sufficiently conditioned, that is, holds sufficient information to recover the true weights of an optimal control problem. It was found that the open-loop system of the original optimal problem has a stronger influence on the solvability of the Inverse Optimal Control problem for the hard-constrained method as compared to the soft-constrained method. These analytical results were validated via <b>simulation.</b></p></p class="citation"></blockquote><h2 id=q-biobm-1>q-bio.BM (1)</h2><h3 id=11--305320-leap-molecular-synthesisability-scoring-with-intermediates-antonia-calvi-et-al-2024>(1/1 | 305/320) Leap: molecular synthesisability scoring with intermediates (Antonia Calvi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Antonia Calvi, Théophile Gaudin, Dominik Miketa, Dominique Sydow, Liam Wilbraham. (2024)<br><strong>Leap: molecular synthesisability scoring with intermediates</strong><br><button class=copy-to-clipboard title="Leap: molecular synthesisability scoring with intermediates" index=305>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-305 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.BM<br>Categories: cs-LG, physics-chem-ph, q-bio-BM, q-bio.BM<br>Keyword Score: 20<br>Keywords: GPT, GPT-2<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.13005v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.13005v1.pdf filename=2403.13005v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Assessing whether a molecule can be synthesised is a primary task in drug discovery. It enables computational chemists to filter for viable compounds or bias molecular generative models. The notion of synthesisability is dynamic as it evolves depending on the availability of key compounds. A common approach in drug discovery involves exploring the chemical space surrounding synthetically-accessible intermediates. This strategy improves the synthesisability of the derived molecules due to the availability of key intermediates. Existing synthesisability scoring methods such as SAScore, SCScore and RAScore, cannot condition on intermediates dynamically. Our approach, Leap, is a <b>GPT-2</b> model trained on the depth, or longest linear path, of predicted synthesis routes that allows information on the availability of key intermediates to be included at inference time. We show that Leap surpasses all other scoring methods by at least 5% on AUC score when identifying synthesisable molecules, and can successfully adapt predicted scores when presented with a relevant intermediate compound.</p></p class="citation"></blockquote><h2 id=csgt-1>cs.GT (1)</h2><h3 id=11--306320-all-pay-auction-based-profit-maximization-in-end-to-end-computation-offloading-system-hai-xue-et-al-2024>(1/1 | 306/320) All-pay Auction Based Profit Maximization in End-to-End Computation Offloading System (Hai Xue et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hai Xue, Yun Xia, Di Zhang, Honghua Wei, Xiaolong Xu. (2024)<br><strong>All-pay Auction Based Profit Maximization in End-to-End Computation Offloading System</strong><br><button class=copy-to-clipboard title="All-pay Auction Based Profit Maximization in End-to-End Computation Offloading System" index=306>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-306 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-GT, cs.GT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09129v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09129v1.pdf filename=2403.09129v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Pricing is an important issue in mobile edge computing. How to appropriately determine the bid of end user (EU) is an incentive factor for edge cloud (EC) to offer service. In this letter, we propose an equilibrium pricing scheme based on the all-pay auction model in end-to-end collaboration environment, wherein all EUs can acquire the service at a lower price than the own value of the required resource. In addition, we propose a set allocation algorithm to divide all the bidders into different sets according to the price, and the EUs in each set get the service, which averts the case of getting no service due to the low price. Extensive <b>simulation</b> results demonstrate that the proposed scheme can effectively maximize the total profit of the edge offloading system, and guarantee all EUs can access the service.</p></p class="citation"></blockquote><h2 id=mathlo-2>math.LO (2)</h2><h3 id=12--307320-generalisation-of-proof-simulation-procedures-for-frege-systems-by-mlbonet-and-srbuss-daniil-kozhemiachenko-2024>(1/2 | 307/320) Generalisation of proof simulation procedures for Frege systems by M.L.~Bonet and S.R.~Buss (Daniil Kozhemiachenko, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daniil Kozhemiachenko. (2024)<br><strong>Generalisation of proof simulation procedures for Frege systems by M.L.~Bonet and S.R.~Buss</strong><br><button class=copy-to-clipboard title="Generalisation of proof simulation procedures for Frege systems by M.L.~Bonet and S.R.~Buss" index=307>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-307 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.LO<br>Categories: cs-LO, math-LO, math.LO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09119v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09119v1.pdf filename=2403.09119v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we present a~generalisation of proof <b>simulation</b> procedures for Frege systems by Bonet and Buss to some logics for which the deduction theorem does not hold. In particular, we study the case of finite-valued \L{}ukasiewicz logics. To this end, we provide proof systems that augment Avron&rsquo;s Frege system for \L{}ukasiewicz three-valued logic with nested and general versions of the disjunction elimination rule, respectively. For these systems we provide upper bounds on speed-ups w.r.t.\ both the number of steps in proofs and the length of proofs. We also consider Tamminga&rsquo;s natural deduction and Avron&rsquo;s hypersequent calculus for 3-valued \L{}ukasiewicz logic and generalise our results considering the disjunction elimination rule to all finite-valued \L{}ukasiewicz logics.</p></p class="citation"></blockquote><h3 id=22--308320-fregean-flows-eric-easthope-2024>(2/2 | 308/320) Fregean Flows (Eric Easthope, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Eric Easthope. (2024)<br><strong>Fregean Flows</strong><br><button class=copy-to-clipboard title="Fregean Flows" index=308>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-308 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.LO<br>Categories: cs-GR, cs-LO, math-LO, math.LO<br>Keyword Score: 8<br>Keywords: Graph, Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09921v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09921v1.pdf filename=2403.09921v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>I humbly introduce a concept I call &ldquo;Fregean flows,&rdquo; a <b>graph</b> theoretic representation of classical logic, to show how higher-dimensional <b>graph</b> characteristics might be useful to prove or perhaps at best show the provability of simple deductive statements typically represented as one-dimensional strings of characters. I apply these to a very simple proof, namely proving the equivalence of two definitions for an Abelian group G, an if-and-only-if statement, using a re-representation of statements as vertices and both conjunctions and implications as differently coloured edges. This re-representation of an if-and-only-if is simple but shows unexpected <b>geometry,</b> and I discuss its possible utility in terms of provability through ideas of <b>graph</b> topology, similarities of <b>graph</b> contraction to deductive elimination, and recursion.</p></p class="citation"></blockquote><h2 id=csar-2>cs.AR (2)</h2><h3 id=12--309320-flexnn-a-dataflow-aware-flexible-deep-learning-accelerator-for-energy-efficient-edge-devices-arnab-raha-et-al-2024>(1/2 | 309/320) FlexNN: A Dataflow-aware Flexible Deep Learning Accelerator for Energy-Efficient Edge Devices (Arnab Raha et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Arnab Raha, Deepak A. Mathaikutty, Soumendu K. Ghosh, Shamik Kundu. (2024)<br><strong>FlexNN: A Dataflow-aware Flexible Deep Learning Accelerator for Energy-Efficient Edge Devices</strong><br><button class=copy-to-clipboard title="FlexNN: A Dataflow-aware Flexible Deep Learning Accelerator for Energy-Efficient Edge Devices" index=309>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-309 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AR<br>Categories: cs-AR, cs-NE, cs.AR<br>Keyword Score: 20<br>Keywords: Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09026v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09026v1.pdf filename=2403.09026v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces FlexNN, a Flexible Neural Network accelerator, which adopts agile design principles to enable versatile dataflows, enhancing energy efficiency. Unlike conventional <b>convolutional</b> <b>neural</b> <b>network</b> accelerator architectures that adhere to fixed dataflows (such as input, weight, output, or row stationary) for transferring activations and weights between storage and compute units, our design revolutionizes by enabling adaptable dataflows of any type through software configurable descriptors. Considering that data movement costs considerably outweigh compute costs from an energy perspective, the flexibility in dataflow allows us to optimize the movement per layer for minimal data transfer and energy consumption, a capability unattainable in fixed dataflow architectures. To further enhance throughput and reduce energy consumption in the FlexNN architecture, we propose a novel sparsity-based acceleration logic that utilizes fine-grained sparsity in both the activation and weight tensors to bypass redundant computations, thus optimizing the <b>convolution</b> engine within the hardware accelerator. Extensive experimental results underscore a significant enhancement in the performance and energy efficiency of FlexNN relative to existing DNN accelerators.</p></p class="citation"></blockquote><h3 id=22--310320-analytical-heterogeneous-die-to-die-3d-placement-with-macros-yuxuan-zhao-et-al-2024>(2/2 | 310/320) Analytical Heterogeneous Die-to-Die 3D Placement with Macros (Yuxuan Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuxuan Zhao, Peiyu Liao, Siting Liu, Jiaxi Jiang, Yibo Lin, Bei Yu. (2024)<br><strong>Analytical Heterogeneous Die-to-Die 3D Placement with Macros</strong><br><button class=copy-to-clipboard title="Analytical Heterogeneous Die-to-Die 3D Placement with Macros" index=310>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-310 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AR<br>Categories: cs-AR, cs.AR<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09070v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09070v1.pdf filename=2403.09070v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents an innovative approach to 3D mixed-size placement in heterogeneous face-to-face (F2F) bonded 3D ICs. We propose an analytical framework that utilizes a dedicated density model and a bistratal wirelength model, effectively handling macros and standard cells in a 3D solution space. A novel 3D preconditioner is developed to resolve the topological and physical gap between macros and standard cells. Additionally, we propose a mixed-integer linear programming (MILP) formulation for macro rotation to optimize wirelength. Our framework is implemented with full-scale GPU acceleration, leveraging an adaptive 3D density accumulation algorithm and an incremental wirelength gradient algorithm. Experimental results on ICCAD 2023 contest <b>benchmarks</b> demonstrate that our framework can achieve 5.9% quality score improvement compared to the first-place winner with 4.0x runtime speedup.</p></p class="citation"></blockquote><h2 id=csdm-2>cs.DM (2)</h2><h3 id=12--311320-binary-stretch-embedding-of-weighted-graphs-javad-b-ebrahimi-et-al-2024>(1/2 | 311/320) Binary Stretch Embedding of Weighted Graphs (Javad B. Ebrahimi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Javad B. Ebrahimi, Mehri Oghbaei Bonab. (2024)<br><strong>Binary Stretch Embedding of Weighted Graphs</strong><br><button class=copy-to-clipboard title="Binary Stretch Embedding of Weighted Graphs" index=311>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-311 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DM<br>Categories: G-2-2; G-2-3, cs-DM, cs.DM, math-CO<br>Keyword Score: 13<br>Keywords: Graph, Graph Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09311v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09311v1.pdf filename=2403.09311v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we introduce and study the problem of \textit{binary stretch embedding} of edge-weighted <b>graph.</b> <b>This</b> problem is closely related to the well-known \textit{addressing problem} of Graham and Pollak. Addressing problem is the problem of assigning the shortest possible length strings (called ``addresses") over the alphabet ${0,1,<em>}$ to the vertices of an input <b>graph</b> <b>$G$</b> with the following property. For every pair $u,v$ of vertices, the number of positions in which one of their addresses is $1$, and the other is $0$ is exactly equal to the distance of $u,v$ in <b>graph</b> <b>$G$.</b> When the addresses do not contain the symbol $</em>$, the problem is called \textit{isometric hypercube embedding}. As far as we know, the isometric hypercube embedding was introduced by Firsov in 1965. It is known that such addresses do not exist for general <b>graphs.</b> <b>Inspired</b> by the addressing problem, in this paper, we introduce the \textit{binary stretch embedding problem}, or BSEP for short, for the edge-weighted undirected <b>graphs.</b> <b>We</b> also argue how this problem is related to other <b>graph</b> <b>embedding</b> problems in the literature. Using tools and techniques such as Hadamard codes and the theory of linear programming, several upper and lower bounds as well as exact solutions for certain classes of <b>graphs</b> <b>will</b> be discovered. As an application of the results in this paper, we derive improved upper bounds or exact values for the maximum size of Lee metric codes of certain parameters.</p></p class="citation"></blockquote><h3 id=22--312320-bounds-and-extremal-graphs-for-monitoring-edge-geodetic-sets-in-graphs-florent-foucaud-et-al-2024>(2/2 | 312/320) Bounds and extremal graphs for monitoring edge-geodetic sets in graphs (Florent Foucaud et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Florent Foucaud, Pierre-Marie Marcille, Zin Mar Myint, R. B. Sandeep, Sagnik Sen, S. Taruni. (2024)<br><strong>Bounds and extremal graphs for monitoring edge-geodetic sets in graphs</strong><br><button class=copy-to-clipboard title="Bounds and extremal graphs for monitoring edge-geodetic sets in graphs" index=312>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-312 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DM<br>Categories: cs-DM, cs.DM, math-CO<br>Keyword Score: 13<br>Keywords: Graph, Stemming<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09122v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09122v1.pdf filename=2403.09122v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A monitoring edge-geodetic set, or simply an MEG-set, of a <b>graph</b> $G$ is a vertex subset $M \subseteq V(G)$ such that given any edge $e$ of $G$, $e$ lies on every shortest $u$-$v$ path of $G$, for some $u,v \in M$. The monitoring edge-geodetic number of $G$, denoted by $meg(G)$, is the minimum cardinality of such an MEG-set. This notion provides a <b>graph</b> theoretic model of the network monitoring problem. In this article, we compare $meg(G)$ with some other <b>graph</b> theoretic parameters <b>stemming</b> from the network monitoring problem and provide examples of <b>graphs</b> having prescribed values for each of these parameters. We also characterize <b>graphs</b> $G$ that have $V(G)$ as their minimum MEG-set, which settles an open problem due to Foucaud \textit{et al.} (CALDAM 2023), and prove that some classes of <b>graphs</b> fall within this characterization. We also provide a general upper bound for $meg(G)$ for sparse <b>graphs</b> in terms of their girth, and later refine the upper bound using the chromatic number of $G$. We examine the change in $meg(G)$ with respect to two fundamental <b>graph</b> operations: clique-sum and subdivisions. In both cases, we provide a lower and an upper bound of the possible amount of changes and provide (almost) tight examples.</p></p class="citation"></blockquote><h2 id=econth-1>econ.TH (1)</h2><h3 id=11--313320-learning-macroeconomic-policies-based-on-microfoundations-a-stackelberg-mean-field-game-approach-qirui-mi-et-al-2024>(1/1 | 313/320) Learning Macroeconomic Policies based on Microfoundations: A Stackelberg Mean Field Game Approach (Qirui Mi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qirui Mi, Zhiyu Zhao, Siyu Xia, Yan Song, Jun Wang, Haifeng Zhang. (2024)<br><strong>Learning Macroeconomic Policies based on Microfoundations: A Stackelberg Mean Field Game Approach</strong><br><button class=copy-to-clipboard title="Learning Macroeconomic Policies based on Microfoundations: A Stackelberg Mean Field Game Approach" index=313>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-313 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: econ.TH<br>Categories: cs-AI, econ-TH, econ.TH<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.12093v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.12093v1.pdf filename=2403.12093v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Effective macroeconomic policies play a crucial role in promoting economic growth and social stability. This paper models the optimal macroeconomic policy problem based on the \textit{Stackelberg Mean Field Game} (SMFG), where the government acts as the leader in policy-making, and large-scale households dynamically respond as followers. This modeling method captures the asymmetric dynamic game between the government and large-scale households, and interpretably evaluates the effects of macroeconomic policies based on microfoundations, which is difficult for existing methods to achieve. We also propose a solution for SMFGs, incorporating pre-training on real data and a model-free \textit{Stackelberg mean-field <b>reinforcement</b> <b>learning</b> }(SMFRL) algorithm, which operates independently of prior environmental knowledge and transitions. Our experimental results showcase the superiority of the SMFG method over other economic policies in terms of performance, efficiency-equity tradeoff, and SMFG assumption analysis. This paper significantly contributes to the domain of AI for economics by providing a powerful tool for modeling and solving optimal macroeconomic policies.</p></p class="citation"></blockquote><h2 id=csgr-2>cs.GR (2)</h2><h3 id=12--314320-headevolver-text-to-head-avatars-via-locally-learnable-mesh-deformation-duotun-wang-et-al-2024>(1/2 | 314/320) HeadEvolver: Text to Head Avatars via Locally Learnable Mesh Deformation (Duotun Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Duotun Wang, Hengyu Meng, Zeyu Cai, Zhijing Shao, Qianxi Liu, Lin Wang, Mingming Fan, Ying Shan, Xiaohang Zhan, Zeyu Wang. (2024)<br><strong>HeadEvolver: Text to Head Avatars via Locally Learnable Mesh Deformation</strong><br><button class=copy-to-clipboard title="HeadEvolver: Text to Head Avatars via Locally Learnable Mesh Deformation" index=314>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-314 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GR<br>Categories: I-2-6; I-3-8, cs-AI, cs-GR, cs.GR<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09326v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09326v1.pdf filename=2403.09326v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present HeadEvolver, a novel framework to generate stylized head avatars from text guidance. HeadEvolver uses locally learnable mesh deformation from a template head mesh, producing high-quality digital assets for detail-preserving editing and animation. To tackle the challenges of lacking fine-grained and semantic-aware local shape control in global deformation through Jacobians, we introduce a trainable parameter as a weighting factor for the Jacobian at each triangle to adaptively change local shapes while maintaining global correspondences and facial features. Moreover, to ensure the coherence of the resulting shape and appearance from different viewpoints, we use pretrained image <b>diffusion</b> <b>models</b> for differentiable rendering with regularization terms to refine the deformation under text guidance. Extensive experiments demonstrate that our method can generate diverse head avatars with an articulated mesh that can be edited seamlessly in 3D graphics software, facilitating downstream applications such as more efficient animation with inherited blend shapes and semantic consistency.</p></p class="citation"></blockquote><h3 id=22--315320-a-new-split-algorithm-for-3d-gaussian-splatting-qiyuan-feng-et-al-2024>(2/2 | 315/320) A New Split Algorithm for 3D Gaussian Splatting (Qiyuan Feng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qiyuan Feng, Gengchen Cao, Haoxiang Chen, Tai-Jiang Mu, Ralph R. Martin, Shi-Min Hu. (2024)<br><strong>A New Split Algorithm for 3D Gaussian Splatting</strong><br><button class=copy-to-clipboard title="A New Split Algorithm for 3D Gaussian Splatting" index=315>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-315 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GR<br>Categories: cs-GR, cs.GR<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09143v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09143v1.pdf filename=2403.09143v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>3D Gaussian splatting models, as a novel explicit 3D representation, have been applied in many domains recently, such as explicit geometric editing and <b>geometry</b> generation. Progress has been rapid. However, due to their mixed scales and cluttered shapes, 3D Gaussian splatting models can produce a blurred or needle-like effect near the surface. At the same time, 3D Gaussian splatting models tend to flatten large untextured regions, yielding a very sparse point cloud. These problems are caused by the non-uniform nature of 3D Gaussian splatting models, so in this paper, we propose a new 3D Gaussian splitting algorithm, which can produce a more uniform and surface-bounded 3D Gaussian splatting model. Our algorithm splits an $N$-dimensional Gaussian into two N-dimensional Gaussians. It ensures consistency of mathematical characteristics and similarity of appearance, allowing resulting 3D Gaussian splatting models to be more uniform and a better fit to the underlying surface, and thus more suitable for explicit editing, point cloud extraction and other tasks. Meanwhile, our 3D Gaussian splitting approach has a very simple closed-form solution, making it readily applicable to any 3D Gaussian model.</p></p class="citation"></blockquote><h2 id=quant-ph-1>quant-ph (1)</h2><h3 id=11--316320-bridging-quantum-computing-and-differential-privacy-a-survey-on-quantum-computing-privacy-yusheng-zhao-et-al-2024>(1/1 | 316/320) Bridging Quantum Computing and Differential Privacy: A Survey on Quantum Computing Privacy (Yusheng Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yusheng Zhao, Hui Zhong, Xinyue Zhang, Chi Zhang, Miao Pan. (2024)<br><strong>Bridging Quantum Computing and Differential Privacy: A Survey on Quantum Computing Privacy</strong><br><button class=copy-to-clipboard title="Bridging Quantum Computing and Differential Privacy: A Survey on Quantum Computing Privacy" index=316>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-316 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-CR, quant-ph, quant-ph<br>Keyword Score: 10<br>Keywords: Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09173v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09173v1.pdf filename=2403.09173v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Quantum computing has attracted significant attention in areas such as cryptography, cybersecurity, and drug discovery. Due to the advantage of parallel processing, quantum computing can speed up the response to complex challenges and the processing of large-scale datasets. However, since quantum computing usually requires sensitive datasets, privacy breaches have become a vital concern. <b>Differential</b> <b>privacy</b> (DP) is a promising privacy-preserving method in classical computing and has been extended to the quantum domain in recent years. In this survey, we categorize the existing literature based on whether internal inherent noise or external artificial noise is used as a source to achieve DP in quantum computing. We explore how these approaches are applied at different stages of a quantum algorithm (i.e., state preparation, quantum circuit, and quantum measurement). We also discuss challenges and future directions for DP in quantum computing. By summarizing recent advancements, we hope to provide a comprehensive, up-to-date overview for researchers venturing into this field.</p></p class="citation"></blockquote><h2 id=mathna-2>math.NA (2)</h2><h3 id=12--317320-high-order-numerical-integration-on-regular-embedded-surfaces-gentian-zavalani-et-al-2024>(1/2 | 317/320) High-order numerical integration on regular embedded surfaces (Gentian Zavalani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gentian Zavalani, Michael Hecht. (2024)<br><strong>High-order numerical integration on regular embedded surfaces</strong><br><button class=copy-to-clipboard title="High-order numerical integration on regular embedded surfaces" index=317>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-317 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: 65D15, 65D30, 65D32, cs-NA, math-NA, math.NA<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09178v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09178v1.pdf filename=2403.09178v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a high-order surface quadrature (HOSQ) for accurately approximating regular surface integrals on closed surfaces. The initial step of our approach rests on exploiting square-squeezing&ndash;a homeomorphic bilinear square-simplex transformation, re-parametrizing any surface triangulation to a quadrilateral mesh. For each resulting quadrilateral domain we interpolate the <b>geometry</b> by tensor polynomials in Chebyshev&ndash;Lobatto grids. Posterior the tensor-product Clenshaw-Curtis quadrature is applied to compute the resulting integral. We demonstrate efficiency, fast runtime performance, high-order accuracy, and robustness for complex geometries.</p></p class="citation"></blockquote><h3 id=22--318320-dynamically-accelerating-the-power-iteration-with-momentum-christian-austin-et-al-2024>(2/2 | 318/320) Dynamically accelerating the power iteration with momentum (Christian Austin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Christian Austin, Sara Pollock, Yunrong Zhu. (2024)<br><strong>Dynamically accelerating the power iteration with momentum</strong><br><button class=copy-to-clipboard title="Dynamically accelerating the power iteration with momentum" index=318>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-318 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: 65F15, 65B99, cs-NA, math-NA, math.NA<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09618v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09618v1.pdf filename=2403.09618v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we propose, analyze and demonstrate a dynamic momentum method to accelerate power and inverse power iterations with minimal computational overhead. The method is appropriate for real, diagonalizable matrices, and does not require a priori spectral knowledge. We review and extend background results on previously developed static momentum accelerations for the power iteration through the connection between the momentum accelerated iteration and the standard power iteration applied to an augmented matrix. We show that the augmented matrix is defective for the optimal parameter choice. We then present our dynamic method which updates the momentum parameter at each iteration based on the Rayleigh quotient and two previous residuals. We present convergence and stability theory for the method by considering a power-like method consisting of multiplying an initial vector by a sequence of augmented matrices. We demonstrate the developed method on a number of <b>benchmark</b> problems, and see that it outperforms both the power iteration and often the static momentum acceleration with optimal parameter choice. Finally, we present and demonstrate an explicit extension of the algorithm to inverse power iterations.</p></p class="citation"></blockquote><h2 id=mathco-1>math.CO (1)</h2><h3 id=11--319320-edge-apexing-in-hereditary-classes-of-graphs-jagdeep-singh-et-al-2024>(1/1 | 319/320) Edge-apexing in hereditary classes of graphs (Jagdeep Singh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jagdeep Singh, Vaidy Sivaraman. (2024)<br><strong>Edge-apexing in hereditary classes of graphs</strong><br><button class=copy-to-clipboard title="Edge-apexing in hereditary classes of graphs" index=319>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-319 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.CO<br>Categories: 05C75, cs-DM, math-CO, math.CO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09456v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09456v1.pdf filename=2403.09456v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A class $\mathcal{G}$ of <b>graphs</b> is called hereditary if it is closed under taking induced subgraphs. We denote by $G^{epex}$ the class of <b>graphs</b> that are at most one edge away from being in $\mathcal{G}$. We note that $G^{epex}$ is hereditary and prove that if a hereditary class $\mathcal{G}$ has finitely many forbidden induced subgraphs, then so does $G^{epex}$. The hereditary class of cographs consists of all <b>graphs</b> $G$ that can be generated from $K_1$ using complementation and disjoint union. Cographs are precisely the <b>graphs</b> that do not have the $4$-vertex path as an induced subgraph. For the class of edge-apex cographs our main result bounds the order of such forbidden induced subgraphs by 8 and finds all of them by computer search.</p></p class="citation"></blockquote><h2 id=cslo-1>cs.LO (1)</h2><h3 id=11--320320-a-complete-logic-for-causal-consistency-will-simmons-et-al-2024>(1/1 | 320/320) A complete logic for causal consistency (Will Simmons et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Will Simmons, Aleks Kissinger. (2024)<br><strong>A complete logic for causal consistency</strong><br><button class=copy-to-clipboard title="A complete logic for causal consistency" index=320>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-320 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LO<br>Categories: 03B70, 18M45, 81P16, F-4-1, cs-LO, cs.LO, quant-ph<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.09297v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.09297v1.pdf filename=2403.09297v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The $\mathrm{Caus}[-]$ construction takes a base category of ``raw materials&rsquo;&rsquo; and builds a category of higher order causal processes, that is a category whose types encode causal (a.k.a. signalling) constraints between collections of systems. Notable examples are categories of higher-order stochastic maps and higher-order quantum channels. Well-typedness in $\mathrm{Caus}[-]$ corresponds to a composition of processes being causally consistent, in the sense that any choice of local processes of the prescribed types yields an overall process respecting causality constraints. It follows that closed processes always occur with probability 1, ruling out e.g. causal paradoxes arising from time loops. It has previously been shown that $\mathrm{Caus}[\mathcal{C}]$ gives a model of MLL+MIX and BV logic, hence these logics give sufficient conditions for causal consistency, but they fail to provide a complete characterisation. In this follow-on work, we introduce <b>graph</b> types as a tool to examine causal structures over <b>graphs</b> in this model. We explore their properties, standard forms, and equivalent definitions; in particular, a process obeys all signalling constraints of the <b>graph</b> iff it is expressible as an affine combination of factorisations into local causal processes connected according to the edges of the <b>graph.</b> The properties of <b>graph</b> types are then used to prove completeness for causal consistency of a new causal logic that conservatively extends pomset logic. The crucial extra ingredient is a notion of distinguished atoms that correspond to first-order states, which only admit a flow of information in one direction. Using the fact that causal logic conservatively extends pomset logic, we finish by giving a physically-meaningful interpretation to a separating statement between pomset and BV.</p></p class="citation"></blockquote></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/ title="arXiv @ 2024.03.15" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Prev</div><div class=next-prev-text>arXiv @ 2024.03.15</div></a></div><div class="col-md-6 next-article"><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/ title="arXiv @ 2024.03.17" class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>arXiv @ 2024.03.17</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#primary-categories>Primary Categories</a></li><li><a href=#keywords>Keywords</a></li><li><a href=#cscv-101>cs.CV (101)</a><ul><li><a href=#1101--1320-mm1-methods-analysis--insights-from-multimodal-llm-pre-training-brandon-mckinzie-et-al-2024>(1/101 | 1/320) MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training (Brandon McKinzie et al., 2024)</a></li><li><a href=#2101--2320-git-towards-generalist-vision-transformer-through-universal-language-interface-haiyang-wang-et-al-2024>(2/101 | 2/320) GiT: Towards Generalist Vision Transformer through Universal Language Interface (Haiyang Wang et al., 2024)</a></li><li><a href=#3101--3320-select-and-distill-selective-dual-teacher-knowledge-transfer-for-continual-learning-on-vision-language-models-yu-chu-yu-et-al-2024>(3/101 | 3/320) Select and Distill: Selective Dual-Teacher Knowledge Transfer for Continual Learning on Vision-Language Models (Yu-Chu Yu et al., 2024)</a></li><li><a href=#4101--4320-adversarial-training-with-ocr-modality-perturbation-for-scene-text-visual-question-answering-zhixuan-shen-et-al-2024>(4/101 | 4/320) Adversarial Training with OCR Modality Perturbation for Scene-Text Visual Question Answering (Zhixuan Shen et al., 2024)</a></li><li><a href=#5101--5320-visiongpt-vision-language-understanding-agent-using-generalized-multimodal-framework-chris-kelly-et-al-2024>(5/101 | 5/320) VisionGPT: Vision-Language Understanding Agent Using Generalized Multimodal Framework (Chris Kelly et al., 2024)</a></li><li><a href=#6101--6320-eyes-closed-safety-on-protecting-multimodal-llms-via-image-to-text-transformation-yunhao-gou-et-al-2024>(6/101 | 6/320) Eyes Closed, Safety On: Protecting Multimodal LLMs via Image-to-Text Transformation (Yunhao Gou et al., 2024)</a></li><li><a href=#7101--7320-3d-vla-a-3d-vision-language-action-generative-world-model-haoyu-zhen-et-al-2024>(7/101 | 7/320) 3D-VLA: A 3D Vision-Language-Action Generative World Model (Haoyu Zhen et al., 2024)</a></li><li><a href=#8101--8320-visiongpt-3d-a-generalized-multimodal-agent-for-enhanced-3d-vision-understanding-chris-kelly-et-al-2024>(8/101 | 8/320) VisionGPT-3D: A Generalized Multimodal Agent for Enhanced 3D Vision Understanding (Chris Kelly et al., 2024)</a></li><li><a href=#9101--9320-skateformer-skeletal-temporal-transformer-for-human-action-recognition-jeonghyeok-do-et-al-2024>(9/101 | 9/320) SkateFormer: Skeletal-Temporal Transformer for Human Action Recognition (Jeonghyeok Do et al., 2024)</a></li><li><a href=#10101--10320-df4lcz-a-sam-empowered-data-fusion-framework-for-scene-level-local-climate-zone-classification-qianqian-wu-et-al-2024>(10/101 | 10/320) DF4LCZ: A SAM-Empowered Data Fusion Framework for Scene-Level Local Climate Zone Classification (Qianqian Wu et al., 2024)</a></li><li><a href=#11101--11320-scp-diff-photo-realistic-semantic-image-synthesis-with-spatial-categorical-joint-prior-huan-ang-gao-et-al-2024>(11/101 | 11/320) SCP-Diff: Photo-Realistic Semantic Image Synthesis with Spatial-Categorical Joint Prior (Huan-ang Gao et al., 2024)</a></li><li><a href=#12101--12320-cloud-gap-filling-with-deep-learning-for-improved-grassland-monitoring-iason-tsardanidis-et-al-2024>(12/101 | 12/320) Cloud gap-filling with deep learning for improved grassland monitoring (Iason Tsardanidis et al., 2024)</a></li><li><a href=#13101--13320-anomaly-detection-by-adapting-a-pre-trained-vision-language-model-yuxuan-cai-et-al-2024>(13/101 | 13/320) Anomaly Detection by Adapting a pre-trained Vision Language Model (Yuxuan Cai et al., 2024)</a></li><li><a href=#14101--14320-localmamba-visual-state-space-model-with-windowed-selective-scan-tao-huang-et-al-2024>(14/101 | 14/320) LocalMamba: Visual State Space Model with Windowed Selective Scan (Tao Huang et al., 2024)</a></li><li><a href=#15101--15320-unicode-learning-a-unified-codebook-for-multimodal-large-language-models-sipeng-zheng-et-al-2024>(15/101 | 15/320) UniCode: Learning a Unified Codebook for Multimodal Large Language Models (Sipeng Zheng et al., 2024)</a></li><li><a href=#16101--16320-griffon-v2-advancing-multimodal-perception-with-high-resolution-scaling-and-visual-language-co-referring-yufei-zhan-et-al-2024>(16/101 | 16/320) Griffon v2: Advancing Multimodal Perception with High-Resolution Scaling and Visual-Language Co-Referring (Yufei Zhan et al., 2024)</a></li><li><a href=#17101--17320-are-vision-language-models-texture-or-shape-biased-and-can-we-steer-them-paul-gavrikov-et-al-2024>(17/101 | 17/320) Are Vision Language Models Texture or Shape Biased and Can We Steer Them? (Paul Gavrikov et al., 2024)</a></li><li><a href=#18101--18320-metadata-driven-federated-learning-of-connectional-brain-templates-in-non-iid-multi-domain-scenarios-geng-chen-et-al-2024>(18/101 | 18/320) Metadata-Driven Federated Learning of Connectional Brain Templates in Non-IID Multi-Domain Scenarios (Geng Chen et al., 2024)</a></li><li><a href=#19101--19320-spikereveal-unlocking-temporal-sequences-from-real-blurry-inputs-with-spike-streams-kang-chen-et-al-2024>(19/101 | 19/320) SpikeReveal: Unlocking Temporal Sequences from Real Blurry Inputs with Spike Streams (Kang Chen et al., 2024)</a></li><li><a href=#20101--20320-open-vocabulary-object-detection-with-meta-prompt-representation-and-instance-contrastive-optimization-zhao-wang-et-al-2024>(20/101 | 20/320) Open-Vocabulary Object Detection with Meta Prompt Representation and Instance Contrastive Optimization (Zhao Wang et al., 2024)</a></li><li><a href=#21101--21320-xcoop-explainable-prompt-learning-for-computer-aided-diagnosis-via-concept-guided-context-optimization-yequan-bie-et-al-2024>(21/101 | 21/320) XCoOp: Explainable Prompt Learning for Computer-Aided Diagnosis via Concept-guided Context Optimization (Yequan Bie et al., 2024)</a></li><li><a href=#22101--22320-video-editing-via-factorized-diffusion-distillation-uriel-singer-et-al-2024>(22/101 | 22/320) Video Editing via Factorized Diffusion Distillation (Uriel Singer et al., 2024)</a></li><li><a href=#23101--23320-explore-in-context-segmentation-via-latent-diffusion-models-chaoyang-wang-et-al-2024>(23/101 | 23/320) Explore In-Context Segmentation via Latent Diffusion Models (Chaoyang Wang et al., 2024)</a></li><li><a href=#24101--24320-semi--and-weakly-supervised-learning-for-mammogram-mass-segmentation-with-limited-annotations-xinyu-xiong-et-al-2024>(24/101 | 24/320) Semi- and Weakly-Supervised Learning for Mammogram Mass Segmentation with Limited Annotations (Xinyu Xiong et al., 2024)</a></li><li><a href=#25101--25320-knowledge-distillation-in-yolox-vit-for-side-scan-sonar-object-detection-martin-aubard-et-al-2024>(25/101 | 25/320) Knowledge Distillation in YOLOX-ViT for Side-Scan Sonar Object Detection (Martin Aubard et al., 2024)</a></li><li><a href=#26101--26320-annotation-free-semantic-segmentation-with-vision-foundation-models-soroush-seifi-et-al-2024>(26/101 | 26/320) Annotation Free Semantic Segmentation with Vision Foundation Models (Soroush Seifi et al., 2024)</a></li><li><a href=#27101--27320-customizing-segmentation-foundation-model-via-prompt-learning-for-instance-segmentation-hyung-il-kim-et-al-2024>(27/101 | 27/320) Customizing Segmentation Foundation Model via Prompt Learning for Instance Segmentation (Hyung-Il Kim et al., 2024)</a></li><li><a href=#28101--28320-sam-lightening-a-lightweight-segment-anything-model-with-dilated-flash-attention-to-achieve-30-times-acceleration-yanfei-song-et-al-2024>(28/101 | 28/320) SAM-Lightening: A Lightweight Segment Anything Model with Dilated Flash Attention to Achieve 30 times Acceleration (Yanfei Song et al., 2024)</a></li><li><a href=#29101--29320-pyra-parallel-yielding-re-activation-for-training-inference-efficient-task-adaptation-yizhe-xiong-et-al-2024>(29/101 | 29/320) PYRA: Parallel Yielding Re-Activation for Training-Inference Efficient Task Adaptation (Yizhe Xiong et al., 2024)</a></li><li><a href=#30101--30320-cardiocaps-attention-based-capsule-network-for-class-imbalanced-echocardiogram-classification-hyunkyung-han-et-al-2024>(30/101 | 30/320) CardioCaps: Attention-based Capsule Network for Class-Imbalanced Echocardiogram Classification (Hyunkyung Han et al., 2024)</a></li><li><a href=#31101--31320-adaptive-hybrid-masking-strategy-for-privacy-preserving-face-recognition-against-model-inversion-attack-yuanqing-huang-et-al-2024>(31/101 | 31/320) Adaptive Hybrid Masking Strategy for Privacy-Preserving Face Recognition Against Model Inversion Attack (Yuanqing Huang et al., 2024)</a></li><li><a href=#32101--32320-opengraph-open-vocabulary-hierarchical-3d-graph-representation-in-large-scale-outdoor-environments-yinan-deng-et-al-2024>(32/101 | 32/320) OpenGraph: Open-Vocabulary Hierarchical 3D Graph Representation in Large-Scale Outdoor Environments (Yinan Deng et al., 2024)</a></li><li><a href=#33101--33320-avibench-towards-evaluating-the-robustness-of-large-vision-language-model-on-adversarial-visual-instructions-hao-zhang-et-al-2024>(33/101 | 33/320) AVIBench: Towards Evaluating the Robustness of Large Vision-Language Model on Adversarial Visual-Instructions (Hao Zhang et al., 2024)</a></li><li><a href=#34101--34320-attention-based-class-conditioned-alignment-for-multi-source-domain-adaptive-object-detection-atif-belal-et-al-2024>(34/101 | 34/320) Attention-based Class-Conditioned Alignment for Multi-Source Domain Adaptive Object Detection (Atif Belal et al., 2024)</a></li><li><a href=#35101--35320-gazemotion-gaze-guided-human-motion-forecasting-zhiming-hu-et-al-2024>(35/101 | 35/320) GazeMotion: Gaze-guided Human Motion Forecasting (Zhiming Hu et al., 2024)</a></li><li><a href=#36101--36320-images-are-achilles-heel-of-alignment-exploiting-visual-vulnerabilities-for-jailbreaking-multimodal-large-language-models-yifan-li-et-al-2024>(36/101 | 36/320) Images are Achilles&rsquo; Heel of Alignment: Exploiting Visual Vulnerabilities for Jailbreaking Multimodal Large Language Models (Yifan Li et al., 2024)</a></li><li><a href=#37101--37320-unsupervised-modality-transferable-video-highlight-detection-with-representation-activation-sequence-learning-tingtian-li-et-al-2024>(37/101 | 37/320) Unsupervised Modality-Transferable Video Highlight Detection with Representation Activation Sequence Learning (Tingtian Li et al., 2024)</a></li><li><a href=#38101--38320-introducing-routing-functions-to-vision-language-parameter-efficient-fine-tuning-with-low-rank-bottlenecks-tingyu-qu-et-al-2024>(38/101 | 38/320) Introducing Routing Functions to Vision-Language Parameter-Efficient Fine-Tuning with Low-Rank Bottlenecks (Tingyu Qu et al., 2024)</a></li><li><a href=#39101--39320-groupcontrast-semantic-aware-self-supervised-representation-learning-for-3d-understanding-chengyao-wang-et-al-2024>(39/101 | 39/320) GroupContrast: Semantic-aware Self-supervised Representation Learning for 3D Understanding (Chengyao Wang et al., 2024)</a></li><li><a href=#40101--40320-onetracker-unifying-visual-object-tracking-with-foundation-models-and-efficient-tuning-lingyi-hong-et-al-2024>(40/101 | 40/320) OneTracker: Unifying Visual Object Tracking with Foundation Models and Efficient Tuning (Lingyi Hong et al., 2024)</a></li><li><a href=#41101--41320-promark-proactive-diffusion-watermarking-for-causal-attribution-vishal-asnani-et-al-2024>(41/101 | 41/320) ProMark: Proactive Diffusion Watermarking for Causal Attribution (Vishal Asnani et al., 2024)</a></li><li><a href=#42101--42320-generalized-predictive-model-for-autonomous-driving-jiazhi-yang-et-al-2024>(42/101 | 42/320) Generalized Predictive Model for Autonomous Driving (Jiazhi Yang et al., 2024)</a></li><li><a href=#43101--43320-video-mamba-suite-state-space-model-as-a-versatile-alternative-for-video-understanding-guo-chen-et-al-2024>(43/101 | 43/320) Video Mamba Suite: State Space Model as a Versatile Alternative for Video Understanding (Guo Chen et al., 2024)</a></li><li><a href=#44101--44320-counterfactual-contrastive-learning-robust-representations-via-causal-image-synthesis-melanie-roschewitz-et-al-2024>(44/101 | 44/320) Counterfactual contrastive learning: robust representations via causal image synthesis (Melanie Roschewitz et al., 2024)</a></li><li><a href=#45101--45320-faceptor-a-generalist-model-for-face-perception-lixiong-qin-et-al-2024>(45/101 | 45/320) Faceptor: A Generalist Model for Face Perception (Lixiong Qin et al., 2024)</a></li><li><a href=#46101--46320-condisr-contrastive-disentanglement-and-style-regularization-for-single-domain-generalization-aleksandr-matsun-et-al-2024>(46/101 | 46/320) ConDiSR: Contrastive Disentanglement and Style Regularization for Single Domain Generalization (Aleksandr Matsun et al., 2024)</a></li><li><a href=#47101--47320-streammultidiffusion-real-time-interactive-generation-with-region-based-semantic-control-jaerin-lee-et-al-2024>(47/101 | 47/320) StreamMultiDiffusion: Real-Time Interactive Generation with Region-Based Semantic Control (Jaerin Lee et al., 2024)</a></li><li><a href=#48101--48320-selector-heterogeneous-graph-network-with-convolutional-masked-autoencoder-for-multimodal-robust-prediction-of-cancer-survival-liangrui-pan-et-al-2024>(48/101 | 48/320) SELECTOR: Heterogeneous graph network with convolutional masked autoencoder for multimodal robust prediction of cancer survival (Liangrui Pan et al., 2024)</a></li><li><a href=#49101--49320-anatomical-structure-guided-medical-vision-language-pre-training-qingqiu-li-et-al-2024>(49/101 | 49/320) Anatomical Structure-Guided Medical Vision-Language Pre-training (Qingqiu Li et al., 2024)</a></li><li><a href=#50101--50320-shan-object-level-privacy-detection-via-inference-on-scene-heterogeneous-graph-zhuohang-jiang-et-al-2024>(50/101 | 50/320) SHAN: Object-Level Privacy Detection via Inference on Scene Heterogeneous Graph (Zhuohang Jiang et al., 2024)</a></li><li><a href=#51101--51320-marvis-motion--geometry-aware-real-and-virtual-image-segmentation-jiayi-wu-et-al-2024>(51/101 | 51/320) MARVIS: Motion & Geometry Aware Real and Virtual Image Segmentation (Jiayi Wu et al., 2024)</a></li><li><a href=#52101--52320-reconstruction-and-simulation-of-elastic-objects-with-spring-mass-3d-gaussians-licheng-zhong-et-al-2024>(52/101 | 52/320) Reconstruction and Simulation of Elastic Objects with Spring-Mass 3D Gaussians (Licheng Zhong et al., 2024)</a></li><li><a href=#53101--53320-sentinel-guided-zero-shot-learning-a-collaborative-paradigm-without-real-data-exposure-fan-wan-et-al-2024>(53/101 | 53/320) Sentinel-Guided Zero-Shot Learning: A Collaborative Paradigm without Real Data Exposure (Fan Wan et al., 2024)</a></li><li><a href=#54101--54320-glyph-byt5-a-customized-text-encoder-for-accurate-visual-text-rendering-zeyu-liu-et-al-2024>(54/101 | 54/320) Glyph-ByT5: A Customized Text Encoder for Accurate Visual Text Rendering (Zeyu Liu et al., 2024)</a></li><li><a href=#55101--55320-renovating-names-in-open-vocabulary-segmentation-benchmarks-haiwen-huang-et-al-2024>(55/101 | 55/320) Renovating Names in Open-Vocabulary Segmentation Benchmarks (Haiwen Huang et al., 2024)</a></li><li><a href=#56101--56320-weaksurg-weakly-supervised-surgical-instrument-segmentation-using-temporal-equivariance-and-semantic-continuity-qiyuan-wang-et-al-2024>(56/101 | 56/320) WeakSurg: Weakly supervised surgical instrument segmentation using temporal equivariance and semantic continuity (Qiyuan Wang et al., 2024)</a></li><li><a href=#57101--57320-eta-inversion-designing-an-optimal-eta-function-for-diffusion-based-real-image-editing-wonjun-kang-et-al-2024>(57/101 | 57/320) Eta Inversion: Designing an Optimal Eta Function for Diffusion-based Real Image Editing (Wonjun Kang et al., 2024)</a></li><li><a href=#58101--58320-distribution-and-depth-aware-transformers-for-3d-human-mesh-recovery-jerrin-bright-et-al-2024>(58/101 | 58/320) Distribution and Depth-Aware Transformers for 3D Human Mesh Recovery (Jerrin Bright et al., 2024)</a></li><li><a href=#59101--59320-the-first-to-know-how-token-distributions-reveal-hidden-knowledge-in-large-vision-language-models-qinyu-zhao-et-al-2024>(59/101 | 59/320) The First to Know: How Token Distributions Reveal Hidden Knowledge in Large Vision-Language Models? (Qinyu Zhao et al., 2024)</a></li><li><a href=#60101--60320-an-image-is-worth-1000-lies-adversarial-transferability-across-prompts-on-vision-language-models-haochen-luo-et-al-2024>(60/101 | 60/320) An Image Is Worth 1000 Lies: Adversarial Transferability across Prompts on Vision-Language Models (Haochen Luo et al., 2024)</a></li><li><a href=#61101--61320-make-your-3d-fast-and-consistent-subject-driven-3d-content-generation-fangfu-liu-et-al-2024>(61/101 | 61/320) Make-Your-3D: Fast and Consistent Subject-Driven 3D Content Generation (Fangfu Liu et al., 2024)</a></li><li><a href=#62101--62320-possam-panoptic-open-vocabulary-segment-anything-vibashan-vs-et-al-2024>(62/101 | 62/320) PosSAM: Panoptic Open-vocabulary Segment Anything (Vibashan VS et al., 2024)</a></li><li><a href=#63101--63320-dont-judge-by-the-look-a-motion-coherent-augmentation-for-video-recognition-yitian-zhang-et-al-2024>(63/101 | 63/320) Don&rsquo;t Judge by the Look: A Motion Coherent Augmentation for Video Recognition (Yitian Zhang et al., 2024)</a></li><li><a href=#64101--64320-mitigating-attribute-amplification-in-counterfactual-image-generation-tian-xia-et-al-2024>(64/101 | 64/320) Mitigating attribute amplification in counterfactual image generation (Tian Xia et al., 2024)</a></li><li><a href=#65101--65320-d3t-distinctive-dual-domain-teacher-zigzagging-across-rgb-thermal-gap-for-domain-adaptive-object-detection-dinh-phat-do-et-al-2024>(65/101 | 65/320) D3T: Distinctive Dual-Domain Teacher Zigzagging Across RGB-Thermal Gap for Domain-Adaptive Object Detection (Dinh Phat Do et al., 2024)</a></li><li><a href=#66101--66320-perspective-equivariant-imaging-an-unsupervised-framework-for-multispectral-pansharpening-andrew-wang-et-al-2024>(66/101 | 66/320) Perspective-Equivariant Imaging: an Unsupervised Framework for Multispectral Pansharpening (Andrew Wang et al., 2024)</a></li><li><a href=#67101--67320-clip-ebc-clip-can-count-accurately-through-enhanced-blockwise-classification-yiming-ma-et-al-2024>(67/101 | 67/320) CLIP-EBC: CLIP Can Count Accurately through Enhanced Blockwise Classification (Yiming Ma et al., 2024)</a></li><li><a href=#68101--68320-wsi-sam-multi-resolution-segment-anything-model-sam-for-histopathology-whole-slide-images-hong-liu-et-al-2024>(68/101 | 68/320) WSI-SAM: Multi-resolution Segment Anything Model (SAM) for histopathology whole-slide images (Hong Liu et al., 2024)</a></li><li><a href=#69101--69320-generalized-relevance-learning-grassmann-quantization-m-mohammadi-et-al-2024>(69/101 | 69/320) Generalized Relevance Learning Grassmann Quantization (M. Mohammadi et al., 2024)</a></li><li><a href=#70101--70320-switch-diffusion-transformer-synergizing-denoising-tasks-with-sparse-mixture-of-experts-byeongjun-park-et-al-2024>(70/101 | 70/320) Switch Diffusion Transformer: Synergizing Denoising Tasks with Sparse Mixture-of-Experts (Byeongjun Park et al., 2024)</a></li><li><a href=#71101--71320-dyadic-interaction-modeling-for-social-behavior-generation-minh-tran-et-al-2024>(71/101 | 71/320) Dyadic Interaction Modeling for Social Behavior Generation (Minh Tran et al., 2024)</a></li><li><a href=#72101--72320-leveraging-foundation-model-automatic-data-augmentation-strategies-and-skeletal-points-for-hands-action-recognition-in-industrial-assembly-lines-liang-wu-et-al-2024>(72/101 | 72/320) Leveraging Foundation Model Automatic Data Augmentation Strategies and Skeletal Points for Hands Action Recognition in Industrial Assembly Lines (Liang Wu et al., 2024)</a></li><li><a href=#73101--73320-towards-comprehensive-multimodal-perception-introducing-the-touch-language-vision-dataset-ning-cheng-et-al-2024>(73/101 | 73/320) Towards Comprehensive Multimodal Perception: Introducing the Touch-Language-Vision Dataset (Ning Cheng et al., 2024)</a></li><li><a href=#74101--74320-on-the-utility-of-3d-hand-poses-for-action-recognition-md-salman-shamil-et-al-2024>(74/101 | 74/320) On the Utility of 3D Hand Poses for Action Recognition (Md Salman Shamil et al., 2024)</a></li><li><a href=#75101--75320-mambatalk-efficient-holistic-gesture-synthesis-with-selective-state-space-models-zunnan-xu-et-al-2024>(75/101 | 75/320) MambaTalk: Efficient Holistic Gesture Synthesis with Selective State Space Models (Zunnan Xu et al., 2024)</a></li><li><a href=#76101--76320-efficientmfd-towards-more-efficient-multimodal-synchronous-fusion-detection-jiaqing-zhang-et-al-2024>(76/101 | 76/320) EfficientMFD: Towards More Efficient Multimodal Synchronous Fusion Detection (Jiaqing Zhang et al., 2024)</a></li><li><a href=#77101--77320-poifusion-multi-modal-3d-object-detection-via-fusion-at-points-of-interest-jiajun-deng-et-al-2024>(77/101 | 77/320) PoIFusion: Multi-Modal 3D Object Detection via Fusion at Points of Interest (Jiajun Deng et al., 2024)</a></li><li><a href=#78101--78320-holo-relighting-controllable-volumetric-portrait-relighting-from-a-single-image-yiqun-mei-et-al-2024>(78/101 | 78/320) Holo-Relighting: Controllable Volumetric Portrait Relighting from a Single Image (Yiqun Mei et al., 2024)</a></li><li><a href=#79101--79320-3d-scenedreamer-text-driven-3d-consistent-scene-generation-frank-zhang-et-al-2024>(79/101 | 79/320) 3D-SceneDreamer: Text-Driven 3D-Consistent Scene Generation (Frank Zhang et al., 2024)</a></li><li><a href=#80101--80320-sd-net-symmetric-aware-keypoint-prediction-and-domain-adaptation-for-6d-pose-estimation-in-bin-picking-scenarios-ding-tao-huang-et-al-2024>(80/101 | 80/320) SD-Net: Symmetric-Aware Keypoint Prediction and Domain Adaptation for 6D Pose Estimation In Bin-picking Scenarios (Ding-Tao Huang et al., 2024)</a></li><li><a href=#81101--81320-sculpt3d-multi-view-consistent-text-to-3d-generation-with-sparse-3d-prior-cheng-chen-et-al-2024>(81/101 | 81/320) Sculpt3D: Multi-View Consistent Text-to-3D Generation with Sparse 3D Prior (Cheng Chen et al., 2024)</a></li><li><a href=#82101--82320-thermohands-a-benchmark-for-3d-hand-pose-estimation-from-egocentric-thermal-image-fangqiang-ding-et-al-2024>(82/101 | 82/320) ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric Thermal Image (Fangqiang Ding et al., 2024)</a></li><li><a href=#83101--83320-score-guided-diffusion-for-3d-human-recovery-anastasis-stathopoulos-et-al-2024>(83/101 | 83/320) Score-Guided Diffusion for 3D Human Recovery (Anastasis Stathopoulos et al., 2024)</a></li><li><a href=#84101--84320-efficient-transferability-assessment-for-selection-of-pre-trained-detectors-zhao-wang-et-al-2024>(84/101 | 84/320) Efficient Transferability Assessment for Selection of Pre-trained Detectors (Zhao Wang et al., 2024)</a></li><li><a href=#85101--85320-gradient-aware-logit-adjustment-loss-for-long-tailed-classifier-fan-zhang-et-al-2024>(85/101 | 85/320) Gradient-Aware Logit Adjustment Loss for Long-tailed Classifier (Fan Zhang et al., 2024)</a></li><li><a href=#86101--86320-explorations-in-texture-learning-blaine-hoak-et-al-2024>(86/101 | 86/320) Explorations in Texture Learning (Blaine Hoak et al., 2024)</a></li><li><a href=#87101--87320-what-sketch-explainability-really-means-for-downstream-tasks-hmrishav-bandyopadhyay-et-al-2024>(87/101 | 87/320) What Sketch Explainability Really Means for Downstream Tasks (Hmrishav Bandyopadhyay et al., 2024)</a></li><li><a href=#88101--88320-improving-real-time-omnidirectional-3d-multi-person-human-pose-estimation-with-people-matching-and-unsupervised-2d-3d-lifting-pawel-knap-et-al-2024>(88/101 | 88/320) Improving Real-Time Omnidirectional 3D Multi-Person Human Pose Estimation with People Matching and Unsupervised 2D-3D Lifting (Pawel Knap et al., 2024)</a></li><li><a href=#89101--89320-eventrpg-event-data-augmentation-with-relevance-propagation-guidance-mingyuan-sun-et-al-2024>(89/101 | 89/320) EventRPG: Event Data Augmentation with Relevance Propagation Guidance (Mingyuan Sun et al., 2024)</a></li><li><a href=#90101--90320-d-yolo-a-robust-framework-for-object-detection-in-adverse-weather-conditions-zihan-chu-2024>(90/101 | 90/320) D-YOLO a robust framework for object detection in adverse weather conditions (Zihan Chu, 2024)</a></li><li><a href=#91101--91320-improving-distant-3d-object-detection-using-2d-box-supervision-zetong-yang-et-al-2024>(91/101 | 91/320) Improving Distant 3D Object Detection Using 2D Box Supervision (Zetong Yang et al., 2024)</a></li><li><a href=#92101--92320-noise-dimension-of-gan-an-image-compression-perspective-ziran-zhu-et-al-2024>(92/101 | 92/320) Noise Dimension of GAN: An Image Compression Perspective (Ziran Zhu et al., 2024)</a></li><li><a href=#93101--93320-intention-driven-ego-to-exo-video-generation-hongchen-luo-et-al-2024>(93/101 | 93/320) Intention-driven Ego-to-Exo Video Generation (Hongchen Luo et al., 2024)</a></li><li><a href=#94101--94320-intention-aware-denoising-diffusion-model-for-trajectory-prediction-chen-liu-et-al-2024>(94/101 | 94/320) Intention-aware Denoising Diffusion Model for Trajectory Prediction (Chen Liu et al., 2024)</a></li><li><a href=#95101--95320-rethinking-referring-object-removal-xiangtian-xue-et-al-2024>(95/101 | 95/320) Rethinking Referring Object Removal (Xiangtian Xue et al., 2024)</a></li><li><a href=#96101--96320-desigen-a-pipeline-for-controllable-design-template-generation-haohan-weng-et-al-2024>(96/101 | 96/320) Desigen: A Pipeline for Controllable Design Template Generation (Haohan Weng et al., 2024)</a></li><li><a href=#97101--97320-cloaf-collision-aware-human-flow-andrey-davydov-et-al-2024>(97/101 | 97/320) CLOAF: CoLlisiOn-Aware Human Flow (Andrey Davydov et al., 2024)</a></li><li><a href=#98101--98320-the-nerfect-match-exploring-nerf-features-for-visual-localization-qunjie-zhou-et-al-2024>(98/101 | 98/320) The NeRFect Match: Exploring NeRF Features for Visual Localization (Qunjie Zhou et al., 2024)</a></li><li><a href=#99101--99320-mm-multimodal-multitask-model-integrating-audiovisual-cues-in-cognitive-load-assessment-long-nguyen-phuoc-et-al-2024>(99/101 | 99/320) M&amp;M: Multimodal-Multitask Model Integrating Audiovisual Cues in Cognitive Load Assessment (Long Nguyen-Phuoc et al., 2024)</a></li><li><a href=#100101--100320-hyper-3dg-text-to-3d-gaussian-generation-via-hypergraph-donglin-di-et-al-2024>(100/101 | 100/320) Hyper-3DG: Text-to-3D Gaussian Generation via Hypergraph (Donglin Di et al., 2024)</a></li><li><a href=#101101--101320-rfacenet-an-end-to-end-network-for-enhanced-physiological-signal-extraction-through-identity-specific-facial-contours-dali-zhu-et-al-2024>(101/101 | 101/320) rFaceNet: An End-to-End Network for Enhanced Physiological Signal Extraction through Identity-Specific Facial Contours (Dali Zhu et al., 2024)</a></li></ul></li><li><a href=#cslg-45>cs.LG (45)</a><ul><li><a href=#145--102320-robust-subgraph-learning-by-monitoring-early-training-representations-sepideh-neshatfar-et-al-2024>(1/45 | 102/320) Robust Subgraph Learning by Monitoring Early Training Representations (Sepideh Neshatfar et al., 2024)</a></li><li><a href=#245--103320-spatial-temporal-memories-enhanced-graph-autoencoder-for-anomaly-detection-in-dynamic-graphs-jie-liu-et-al-2024>(2/45 | 103/320) Spatial-temporal Memories Enhanced Graph Autoencoder for Anomaly Detection in Dynamic Graphs (Jie Liu et al., 2024)</a></li><li><a href=#345--104320-keyformer-kv-cache-reduction-through-key-tokens-selection-for-efficient-generative-inference-muhammad-adnan-et-al-2024>(3/45 | 104/320) Keyformer: KV Cache Reduction through Key Tokens Selection for Efficient Generative Inference (Muhammad Adnan et al., 2024)</a></li><li><a href=#445--105320-adversarial-fine-tuning-of-compressed-neural-networks-for-joint-improvement-of-robustness-and-efficiency-hallgrimur-thorsteinsson-et-al-2024>(4/45 | 105/320) Adversarial Fine-tuning of Compressed Neural Networks for Joint Improvement of Robustness and Efficiency (Hallgrimur Thorsteinsson et al., 2024)</a></li><li><a href=#545--106320-adedgedrop-adversarial-edge-dropping-for-robust-graph-neural-networks-zhaoliang-chen-et-al-2024>(5/45 | 106/320) ADEdgeDrop: Adversarial Edge Dropping for Robust Graph Neural Networks (Zhaoliang Chen et al., 2024)</a></li><li><a href=#645--107320-learning-from-straggler-clients-in-federated-learning-andrew-hard-et-al-2024>(6/45 | 107/320) Learning from straggler clients in federated learning (Andrew Hard et al., 2024)</a></li><li><a href=#745--108320-fedcomloc-communication-efficient-distributed-training-of-sparse-and-quantized-models-kai-yi-et-al-2024>(7/45 | 108/320) FedComLoc: Communication-Efficient Distributed Training of Sparse and Quantized Models (Kai Yi et al., 2024)</a></li><li><a href=#845--109320-cooling-guide-diffusion-model-for-battery-cell-arrangement-nicholas-sung-et-al-2024>(8/45 | 109/320) Cooling-Guide Diffusion Model for Battery Cell Arrangement (Nicholas Sung et al., 2024)</a></li><li><a href=#945--110320-borrowing-treasures-from-neighbors-in-context-learning-for-multimodal-learning-with-missing-modalities-and-data-scarcity-zhuo-zhi-et-al-2024>(9/45 | 110/320) Borrowing Treasures from Neighbors: In-Context Learning for Multimodal Learning with Missing Modalities and Data Scarcity (Zhuo Zhi et al., 2024)</a></li><li><a href=#1045--111320-equiav-leveraging-equivariance-for-audio-visual-contrastive-learning-jongsuk-kim-et-al-2024>(10/45 | 111/320) EquiAV: Leveraging Equivariance for Audio-Visual Contrastive Learning (Jongsuk Kim et al., 2024)</a></li><li><a href=#1145--112320-mope-parameter-efficient-and-scalable-multimodal-fusion-via-mixture-of-prompt-experts-ruixiang-jiang-et-al-2024>(11/45 | 112/320) MoPE: Parameter-Efficient and Scalable Multimodal Fusion via Mixture of Prompt Experts (Ruixiang Jiang et al., 2024)</a></li><li><a href=#1245--113320-self-supervised-learning-for-time-series-contrastive-or-generative-ziyu-liu-et-al-2024>(12/45 | 113/320) Self-Supervised Learning for Time Series: Contrastive or Generative? (Ziyu Liu et al., 2024)</a></li><li><a href=#1345--114320-few-shot-class-incremental-learning-with-attention-aware-self-adaptive-prompt-chenxi-liu-et-al-2024>(13/45 | 114/320) Few-Shot Class Incremental Learning with Attention-Aware Self-Adaptive Prompt (Chenxi Liu et al., 2024)</a></li><li><a href=#1445--115320-towards-a-theory-of-model-distillation-enric-boix-adsera-2024>(14/45 | 115/320) Towards a theory of model distillation (Enric Boix-Adsera, 2024)</a></li><li><a href=#1545--116320-soften-to-defend-towards-adversarial-robustness-via-self-guided-label-refinement-daiwei-yu-et-al-2024>(15/45 | 116/320) Soften to Defend: Towards Adversarial Robustness via Self-Guided Label Refinement (Daiwei Yu et al., 2024)</a></li><li><a href=#1645--117320-a-conceptual-framework-for-white-box-neural-networks-maciej-satkiewicz-2024>(16/45 | 117/320) A Conceptual Framework For White Box Neural Networks (Maciej Satkiewicz, 2024)</a></li><li><a href=#1745--118320-minimax-optimal-and-computationally-efficient-algorithms-for-distributionally-robust-offline-reinforcement-learning-zhishuai-liu-et-al-2024>(17/45 | 118/320) Minimax Optimal and Computationally Efficient Algorithms for Distributionally Robust Offline Reinforcement Learning (Zhishuai Liu et al., 2024)</a></li><li><a href=#1845--119320-reawakening-knowledge-anticipatory-recovery-from-catastrophic-interference-via-structured-training-yanlai-yang-et-al-2024>(18/45 | 119/320) Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training (Yanlai Yang et al., 2024)</a></li><li><a href=#1945--120320-self-consistency-training-for-hamiltonian-prediction-he-zhang-et-al-2024>(19/45 | 120/320) Self-Consistency Training for Hamiltonian Prediction (He Zhang et al., 2024)</a></li><li><a href=#2045--121320-on-using-machine-learning-algorithms-for-motorcycle-collision-detection-philipp-rodegast-et-al-2024>(20/45 | 121/320) On using Machine Learning Algorithms for Motorcycle Collision Detection (Philipp Rodegast et al., 2024)</a></li><li><a href=#2145--122320-laying-the-foundation-first-investigating-the-generalization-from-atomic-skills-to-complex-reasoning-tasks-yuncheng-huang-et-al-2024>(21/45 | 122/320) Laying the Foundation First? Investigating the Generalization from Atomic Skills to Complex Reasoning Tasks (Yuncheng Huang et al., 2024)</a></li><li><a href=#2245--123320-easy-to-hard-generalization-scalable-alignment-beyond-human-supervision-zhiqing-sun-et-al-2024>(22/45 | 123/320) Easy-to-Hard Generalization: Scalable Alignment Beyond Human Supervision (Zhiqing Sun et al., 2024)</a></li><li><a href=#2345--124320-shake-to-leak-fine-tuning-diffusion-models-can-amplify-the-generative-privacy-risk-zhangheng-li-et-al-2024>(23/45 | 124/320) Shake to Leak: Fine-tuning Diffusion Models Can Amplify the Generative Privacy Risk (Zhangheng Li et al., 2024)</a></li><li><a href=#2445--125320-rethinking-autoencoders-for-medical-anomaly-detection-from-a-theoretical-perspective-yu-cai-et-al-2024>(24/45 | 125/320) Rethinking Autoencoders for Medical Anomaly Detection from A Theoretical Perspective (Yu Cai et al., 2024)</a></li><li><a href=#2545--126320-generative-models-and-connected-and-automated-vehicles-a-survey-in-exploring-the-intersection-of-transportation-and-ai-dong-shu-et-al-2024>(25/45 | 126/320) Generative Models and Connected and Automated Vehicles: A Survey in Exploring the Intersection of Transportation and AI (Dong Shu et al., 2024)</a></li><li><a href=#2645--127320-sindy-rl-interpretable-and-efficient-model-based-reinforcement-learning-nicholas-zolman-et-al-2024>(26/45 | 127/320) SINDy-RL: Interpretable and Efficient Model-Based Reinforcement Learning (Nicholas Zolman et al., 2024)</a></li><li><a href=#2745--128320-mamba-an-effective-world-model-approach-for-meta-reinforcement-learning-zohar-rimon-et-al-2024>(27/45 | 128/320) MAMBA: an Effective World Model Approach for Meta-Reinforcement Learning (Zohar Rimon et al., 2024)</a></li><li><a href=#2845--129320-hyperparameters-in-continual-learning-a-reality-check-sungmin-cha-et-al-2024>(28/45 | 129/320) Hyperparameters in Continual Learning: a Reality Check (Sungmin Cha et al., 2024)</a></li><li><a href=#2945--130320-taming-cross-domain-representation-variance-in-federated-prototype-learning-with-heterogeneous-data-domains-lei-wang-et-al-2024>(29/45 | 130/320) Taming Cross-Domain Representation Variance in Federated Prototype Learning with Heterogeneous Data Domains (Lei Wang et al., 2024)</a></li><li><a href=#3045--131320-achieving-pareto-optimality-using-efficient-parameter-reduction-for-dnns-in-resource-constrained-edge-environment-atah-nuh-mih-et-al-2024>(30/45 | 131/320) Achieving Pareto Optimality using Efficient Parameter Reduction for DNNs in Resource-Constrained Edge Environment (Atah Nuh Mih et al., 2024)</a></li><li><a href=#3145--132320-generalizing-denoising-to-non-equilibrium-structures-improves-equivariant-force-fields-yi-lun-liao-et-al-2024>(31/45 | 132/320) Generalizing Denoising to Non-Equilibrium Structures Improves Equivariant Force Fields (Yi-Lun Liao et al., 2024)</a></li><li><a href=#3245--133320-a-reinforcement-learning-approach-to-dairy-farm-battery-management-using-q-learning-nawazish-ali-et-al-2024>(32/45 | 133/320) A Reinforcement Learning Approach to Dairy Farm Battery Management using Q Learning (Nawazish Ali et al., 2024)</a></li><li><a href=#3345--134320-da-pfl-dynamic-affinity-aggregation-for-personalized-federated-learning-xu-yang-et-al-2024>(33/45 | 134/320) DA-PFL: Dynamic Affinity Aggregation for Personalized Federated Learning (Xu Yang et al., 2024)</a></li><li><a href=#3445--135320-uncertainty-quantification-for-cross-subject-motor-imagery-classification-prithviraj-manivannan-et-al-2024>(34/45 | 135/320) Uncertainty Quantification for cross-subject Motor Imagery classification (Prithviraj Manivannan et al., 2024)</a></li><li><a href=#3545--136320-mcformer-multivariate-time-series-forecasting-with-mixed-channels-transformer-wenyong-han-et-al-2024>(35/45 | 136/320) MCformer: Multivariate Time Series Forecasting with Mixed-Channels Transformer (Wenyong Han et al., 2024)</a></li><li><a href=#3645--137320-on-the-laplace-approximation-as-model-selection-criterion-for-gaussian-processes-andreas-besginow-et-al-2024>(36/45 | 137/320) On the Laplace Approximation as Model Selection Criterion for Gaussian Processes (Andreas Besginow et al., 2024)</a></li><li><a href=#3745--138320-design-of-an-basis-projected-layer-for-sparse-datasets-in-deep-learning-training-using-gc-ms-spectra-as-a-case-study-yu-tang-chang-et-al-2024>(37/45 | 138/320) Design of an basis-projected layer for sparse datasets in deep learning training using gc-ms spectra as a case study (Yu Tang Chang et al., 2024)</a></li><li><a href=#3845--139320-towards-diverse-perspective-learning-with-selection-over-multiple-temporal-poolings-jihyeon-seong-et-al-2024>(38/45 | 139/320) Towards Diverse Perspective Learning with Selection over Multiple Temporal Poolings (Jihyeon Seong et al., 2024)</a></li><li><a href=#3945--140320-ditmos-delving-into-diverse-tiny-model-selection-on-microcontrollers-xiao-ma-et-al-2024>(39/45 | 140/320) DiTMoS: Delving into Diverse Tiny-Model Selection on Microcontrollers (Xiao Ma et al., 2024)</a></li><li><a href=#4045--141320-towards-the-reusability-and-compositionality-of-causal-representations-davide-talon-et-al-2024>(40/45 | 141/320) Towards the Reusability and Compositionality of Causal Representations (Davide Talon et al., 2024)</a></li><li><a href=#4145--142320-s2mvtc-a-simple-yet-efficient-scalable-multi-view-tensor-clustering-zhen-long-et-al-2024>(41/45 | 142/320) S^2MVTC: a Simple yet Efficient Scalable Multi-View Tensor Clustering (Zhen Long et al., 2024)</a></li><li><a href=#4245--143320-multi-fidelity-bayesian-optimization-with-across-task-transferable-max-value-entropy-search-yunchuan-zhang-et-al-2024>(42/45 | 143/320) Multi-Fidelity Bayesian Optimization With Across-Task Transferable Max-Value Entropy Search (Yunchuan Zhang et al., 2024)</a></li><li><a href=#4345--144320-a-collection-of-the-accepted-papers-for-the-human-centric-representation-learning-workshop-at-aaai-2024-dimitris-spathis-et-al-2024>(43/45 | 144/320) A collection of the accepted papers for the Human-Centric Representation Learning workshop at AAAI 2024 (Dimitris Spathis et al., 2024)</a></li><li><a href=#4445--145320-timemachine-a-time-series-is-worth-4-mambas-for-long-term-forecasting-md-atik-ahamed-et-al-2024>(44/45 | 145/320) TimeMachine: A Time Series is Worth 4 Mambas for Long-term Forecasting (Md Atik Ahamed et al., 2024)</a></li><li><a href=#4545--146320-recursive-causal-discovery-ehsan-mokhtarian-et-al-2024>(45/45 | 146/320) Recursive Causal Discovery (Ehsan Mokhtarian et al., 2024)</a></li></ul></li><li><a href=#cscl-42>cs.CL (42)</a><ul><li><a href=#142--147320-mt-patcher-selective-and-extendable-knowledge-distillation-from-large-language-models-for-machine-translation-jiahuan-li-et-al-2024>(1/42 | 147/320) MT-PATCHER: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation (Jiahuan Li et al., 2024)</a></li><li><a href=#242--148320-taxollama-wordnet-based-model-for-solving-multiple-lexical-sematic-tasks-viktor-moskvoretskii-et-al-2024>(2/42 | 148/320) TaxoLLaMA: WordNet-based Model for Solving Multiple Lexical Sematic Tasks (Viktor Moskvoretskii et al., 2024)</a></li><li><a href=#342--149320-evaluating-llms-for-gender-disparities-in-notable-persons-lauren-rhue-et-al-2024>(3/42 | 149/320) Evaluating LLMs for Gender Disparities in Notable Persons (Lauren Rhue et al., 2024)</a></li><li><a href=#442--150320-chartinstruct-instruction-tuning-for-chart-comprehension-and-reasoning-ahmed-masry-et-al-2024>(4/42 | 150/320) ChartInstruct: Instruction Tuning for Chart Comprehension and Reasoning (Ahmed Masry et al., 2024)</a></li><li><a href=#542--151320-komodo-a-linguistic-expedition-into-indonesias-regional-languages-louis-owen-et-al-2024>(5/42 | 151/320) Komodo: A Linguistic Expedition into Indonesia&rsquo;s Regional Languages (Louis Owen et al., 2024)</a></li><li><a href=#642--152320-retrieval-augmented-text-to-sql-generation-for-epidemiological-question-answering-using-electronic-health-records-angelo-ziletti-et-al-2024>(6/42 | 152/320) Retrieval augmented text-to-SQL generation for epidemiological question answering using electronic health records (Angelo Ziletti et al., 2024)</a></li><li><a href=#742--153320-exploring-the-comprehension-of-chatgpt-in-traditional-chinese-medicine-knowledge-li-yizhen-et-al-2024>(7/42 | 153/320) Exploring the Comprehension of ChatGPT in Traditional Chinese Medicine Knowledge (Li Yizhen et al., 2024)</a></li><li><a href=#842--154320-proswitch-knowledge-guided-language-model-fine-tuning-to-generate-professional-and-non-professional-styled-text-chang-zong-et-al-2024>(8/42 | 154/320) ProSwitch: Knowledge-Guided Language Model Fine-Tuning to Generate Professional and Non-Professional Styled Text (Chang Zong et al., 2024)</a></li><li><a href=#942--155320-fisher-mask-nodes-for-language-model-merging-thennal-d-k-et-al-2024>(9/42 | 155/320) Fisher Mask Nodes for Language Model Merging (Thennal D K et al., 2024)</a></li><li><a href=#1042--156320-quiet-star-language-models-can-teach-themselves-to-think-before-speaking-eric-zelikman-et-al-2024>(10/42 | 156/320) Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking (Eric Zelikman et al., 2024)</a></li><li><a href=#1142--157320-rectifying-demonstration-shortcut-in-in-context-learning-joonwon-jang-et-al-2024>(11/42 | 157/320) Rectifying Demonstration Shortcut in In-Context Learning (Joonwon Jang et al., 2024)</a></li><li><a href=#1242--158320-to-label-or-not-to-label-hybrid-active-learning-for-neural-machine-translation-abdul-hameed-azeemi-et-al-2024>(12/42 | 158/320) To Label or Not to Label: Hybrid Active Learning for Neural Machine Translation (Abdul Hameed Azeemi et al., 2024)</a></li><li><a href=#1342--159320-unveiling-the-generalization-power-of-fine-tuned-large-language-models-haoran-yang-et-al-2024>(13/42 | 159/320) Unveiling the Generalization Power of Fine-Tuned Large Language Models (Haoran Yang et al., 2024)</a></li><li><a href=#1442--160320-information-extraction-an-application-to-the-domain-of-hyper-local-financial-data-on-developing-countries-abuzar-royesh-et-al-2024>(14/42 | 160/320) Information Extraction: An application to the domain of hyper-local financial data on developing countries (Abuzar Royesh et al., 2024)</a></li><li><a href=#1542--161320-large-language-models-are-parallel-multilingual-learners-yongyu-mu-et-al-2024>(15/42 | 161/320) Large Language Models are Parallel Multilingual Learners (Yongyu Mu et al., 2024)</a></li><li><a href=#1642--162320-lamp-a-language-model-on-the-map-pasquale-balsebre-et-al-2024>(16/42 | 162/320) LAMP: A Language Model on the Map (Pasquale Balsebre et al., 2024)</a></li><li><a href=#1742--163320-less-is-more-data-value-estimation-for-visual-instruction-tuning-zikang-liu-et-al-2024>(17/42 | 163/320) Less is More: Data Value Estimation for Visual Instruction Tuning (Zikang Liu et al., 2024)</a></li><li><a href=#1842--164320-scaling-behavior-of-machine-translation-with-large-language-models-under-prompt-injection-attacks-zhifan-sun-et-al-2024>(18/42 | 164/320) Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks (Zhifan Sun et al., 2024)</a></li><li><a href=#1942--165320-aratrust-an-evaluation-of-trustworthiness-for-llms-in-arabic-emad-a-alghamdi-et-al-2024>(19/42 | 165/320) AraTrust: An Evaluation of Trustworthiness for LLMs in Arabic (Emad A. Alghamdi et al., 2024)</a></li><li><a href=#2042--166320-sabiá-2-a-new-generation-of-portuguese-large-language-models-thales-sales-almeida-et-al-2024>(20/42 | 166/320) Sabiá-2: A New Generation of Portuguese Large Language Models (Thales Sales Almeida et al., 2024)</a></li><li><a href=#2142--167320-logits-of-api-protected-llms-leak-proprietary-information-matthew-finlayson-et-al-2024>(21/42 | 167/320) Logits of API-Protected LLMs Leak Proprietary Information (Matthew Finlayson et al., 2024)</a></li><li><a href=#2242--168320-basque-and-spanish-counter-narrative-generation-data-creation-and-evaluation-jaione-bengoetxea-et-al-2024>(22/42 | 168/320) Basque and Spanish Counter Narrative Generation: Data Creation and Evaluation (Jaione Bengoetxea et al., 2024)</a></li><li><a href=#2342--169320-ai-on-ai-exploring-the-utility-of-gpt-as-an-expert-annotator-of-ai-publications-autumn-toney-wails-et-al-2024>(23/42 | 169/320) AI on AI: Exploring the Utility of GPT as an Expert Annotator of AI Publications (Autumn Toney-Wails et al., 2024)</a></li><li><a href=#2442--170320-re-search-for-the-truth-multi-round-retrieval-augmented-large-language-models-are-strong-fake-news-detectors-guanghua-li-et-al-2024>(24/42 | 170/320) Re-Search for The Truth: Multi-round Retrieval-augmented Large Language Models are Strong Fake News Detectors (Guanghua Li et al., 2024)</a></li><li><a href=#2542--171320-dynamic-memory-compression-retrofitting-llms-for-accelerated-inference-piotr-nawrot-et-al-2024>(25/42 | 171/320) Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference (Piotr Nawrot et al., 2024)</a></li><li><a href=#2642--172320-large-language-models-and-causal-inference-in-collaboration-a-comprehensive-survey-xiaoyu-liu-et-al-2024>(26/42 | 172/320) Large Language Models and Causal Inference in Collaboration: A Comprehensive Survey (Xiaoyu Liu et al., 2024)</a></li><li><a href=#2742--173320-dial-insight-fine-tuning-large-language-models-with-high-quality-domain-specific-data-preventing-capability-collapse-jianwei-sun-et-al-2024>(27/42 | 173/320) Dial-insight: Fine-tuning Large Language Models with High-Quality Domain-Specific Data Preventing Capability Collapse (Jianwei Sun et al., 2024)</a></li><li><a href=#2842--174320-autolora-automatically-tuning-matrix-ranks-in-low-rank-adaptation-based-on-meta-learning-ruiyi-zhang-et-al-2024>(28/42 | 174/320) AutoLoRA: Automatically Tuning Matrix Ranks in Low-Rank Adaptation Based on Meta Learning (Ruiyi Zhang et al., 2024)</a></li><li><a href=#2942--175320-ragged-towards-informed-design-of-retrieval-augmented-generation-systems-jennifer-hsia-et-al-2024>(29/42 | 175/320) RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems (Jennifer Hsia et al., 2024)</a></li><li><a href=#3042--176320-hyper-cl-conditioning-sentence-representations-with-hypernetworks-young-hyun-yoo-et-al-2024>(30/42 | 176/320) Hyper-CL: Conditioning Sentence Representations with Hypernetworks (Young Hyun Yoo et al., 2024)</a></li><li><a href=#3142--177320-self-consistency-boosts-calibration-for-math-reasoning-ante-wang-et-al-2024>(31/42 | 177/320) Self-Consistency Boosts Calibration for Math Reasoning (Ante Wang et al., 2024)</a></li><li><a href=#3242--178320-mcfend-a-multi-source-benchmark-dataset-for-chinese-fake-news-detection-yupeng-li-et-al-2024>(32/42 | 178/320) MCFEND: A Multi-source Benchmark Dataset for Chinese Fake News Detection (Yupeng Li et al., 2024)</a></li><li><a href=#3342--179320-transformers-get-stable-an-end-to-end-signal-propagation-theory-for-language-models-akhil-kedia-et-al-2024>(33/42 | 179/320) Transformers Get Stable: An End-to-End Signal Propagation Theory for Language Models (Akhil Kedia et al., 2024)</a></li><li><a href=#3442--180320-caveat-lector-large-language-models-in-legal-practice-eliza-mik-2024>(34/42 | 180/320) Caveat Lector: Large Language Models in Legal Practice (Eliza Mik, 2024)</a></li><li><a href=#3542--181320-meta-cognitive-analysis-evaluating-declarative-and-procedural-knowledge-in-datasets-and-large-language-models-zhuoqun-li-et-al-2024>(35/42 | 181/320) Meta-Cognitive Analysis: Evaluating Declarative and Procedural Knowledge in Datasets and Large Language Models (Zhuoqun Li et al., 2024)</a></li><li><a href=#3642--182320-meaningful-learning-advancing-abstract-reasoning-in-large-language-models-via-generic-fact-guidance-kai-xiong-et-al-2024>(36/42 | 182/320) Meaningful Learning: Advancing Abstract Reasoning in Large Language Models via Generic Fact Guidance (Kai Xiong et al., 2024)</a></li><li><a href=#3742--183320-a-continued-pretrained-llm-approach-for-automatic-medical-note-generation-dong-yuan-et-al-2024>(37/42 | 183/320) A Continued Pretrained LLM Approach for Automatic Medical Note Generation (Dong Yuan et al., 2024)</a></li><li><a href=#3842--184320-recurrent-drafter-for-fast-speculative-decoding-in-large-language-models-aonan-zhang-et-al-2024>(38/42 | 184/320) Recurrent Drafter for Fast Speculative Decoding in Large Language Models (Aonan Zhang et al., 2024)</a></li><li><a href=#3942--185320-fakewatch-a-framework-for-detecting-fake-news-to-ensure-credible-elections-shaina-raza-et-al-2024>(39/42 | 185/320) FakeWatch: A Framework for Detecting Fake News to Ensure Credible Elections (Shaina Raza et al., 2024)</a></li><li><a href=#4042--186320-leveraging-prototypical-representations-for-mitigating-social-bias-without-demographic-information-shadi-iskander-et-al-2024>(40/42 | 186/320) Leveraging Prototypical Representations for Mitigating Social Bias without Demographic Information (Shadi Iskander et al., 2024)</a></li><li><a href=#4142--187320-emotional-intelligence-through-artificial-intelligence--nlp-and-deep-learning-in-the-analysis-of-healthcare-texts-prashant-kumar-nag-et-al-2024>(41/42 | 187/320) Emotional Intelligence Through Artificial Intelligence : NLP and Deep Learning in the Analysis of Healthcare Texts (Prashant Kumar Nag et al., 2024)</a></li><li><a href=#4242--188320-geographically-informed-language-identification-jonathan-dunn-et-al-2024>(42/42 | 188/320) Geographically-Informed Language Identification (Jonathan Dunn et al., 2024)</a></li></ul></li><li><a href=#cond-matmtrl-sci-1>cond-mat.mtrl-sci (1)</a><ul><li><a href=#11--189320-adapting-oc20-trained-equiformerv2-models-for-high-entropy-materials-christian-m-clausen-et-al-2024>(1/1 | 189/320) Adapting OC20-trained EquiformerV2 Models for High-Entropy Materials (Christian M. Clausen et al., 2024)</a></li></ul></li><li><a href=#csro-16>cs.RO (16)</a><ul><li><a href=#116--190320-explorllm-guiding-exploration-in-reinforcement-learning-with-large-language-models-runyu-ma-et-al-2024>(1/16 | 190/320) ExploRLLM: Guiding Exploration in Reinforcement Learning with Large Language Models (Runyu Ma et al., 2024)</a></li><li><a href=#216--191320-paperbot-learning-to-design-real-world-tools-using-paper-ruoshi-liu-et-al-2024>(2/16 | 191/320) PaperBot: Learning to Design Real-World Tools Using Paper (Ruoshi Liu et al., 2024)</a></li><li><a href=#316--192320-gaussiangrasper-3d-language-gaussian-splatting-for-open-vocabulary-robotic-grasping-yuhang-zheng-et-al-2024>(3/16 | 192/320) GaussianGrasper: 3D Language Gaussian Splatting for Open-vocabulary Robotic Grasping (Yuhang Zheng et al., 2024)</a></li><li><a href=#416--193320-enhancing-trust-in-autonomous-agents-an-architecture-for-accountability-and-explainability-through-blockchain-and-large-language-models-laura-fernández-becerra-et-al-2024>(4/16 | 193/320) Enhancing Trust in Autonomous Agents: An Architecture for Accountability and Explainability through Blockchain and Large Language Models (Laura Fernández-Becerra et al., 2024)</a></li><li><a href=#516--194320-socially-integrated-navigation-a-social-acting-robot-with-deep-reinforcement-learning-daniel-flögel-et-al-2024>(5/16 | 194/320) Socially Integrated Navigation: A Social Acting Robot with Deep Reinforcement Learning (Daniel Flögel et al., 2024)</a></li><li><a href=#616--195320-right-place-right-time-towards-objectnav-for-non-stationary-goals-vishnu-sashank-dorbala-et-al-2024>(6/16 | 195/320) Right Place, Right Time! Towards ObjectNav for Non-Stationary Goals (Vishnu Sashank Dorbala et al., 2024)</a></li><li><a href=#716--196320-behavior-1k-a-human-centered-embodied-ai-benchmark-with-1000-everyday-activities-and-realistic-simulation-chengshu-li-et-al-2024>(7/16 | 196/320) BEHAVIOR-1K: A Human-Centered, Embodied AI Benchmark with 1,000 Everyday Activities and Realistic Simulation (Chengshu Li et al., 2024)</a></li><li><a href=#816--197320-touch-gs-visual-tactile-supervised-3d-gaussian-splatting-aiden-swann-et-al-2024>(8/16 | 197/320) Touch-GS: Visual-Tactile Supervised 3D Gaussian Splatting (Aiden Swann et al., 2024)</a></li><li><a href=#916--198320-constrained-passive-interaction-control-leveraging-passivity-and-safety-for-robot-manipulators-zhiquan-zhang-et-al-2024>(9/16 | 198/320) Constrained Passive Interaction Control: Leveraging Passivity and Safety for Robot Manipulators (Zhiquan Zhang et al., 2024)</a></li><li><a href=#1016--199320-pushing-in-the-dark-a-reactive-pushing-strategy-for-mobile-robots-using-tactile-feedback-idil-ozdamar-et-al-2024>(10/16 | 199/320) Pushing in the Dark: A Reactive Pushing Strategy for Mobile Robots Using Tactile Feedback (Idil Ozdamar et al., 2024)</a></li><li><a href=#1116--200320-dtg--diffusion-based-trajectory-generation-for-mapless-global-navigation-jing-liang-et-al-2024>(11/16 | 200/320) DTG : Diffusion-based Trajectory Generation for Mapless Global Navigation (Jing Liang et al., 2024)</a></li><li><a href=#1216--201320-multigrippergrasp-a-dataset-for-robotic-grasping-from-parallel-jaw-grippers-to-dexterous-hands-luis-felipe-casas-murrilo-et-al-2024>(12/16 | 201/320) MultiGripperGrasp: A Dataset for Robotic Grasping from Parallel Jaw Grippers to Dexterous Hands (Luis Felipe Casas Murrilo et al., 2024)</a></li><li><a href=#1316--202320-are-you-a-robot-detecting-autonomous-vehicles-from-behavior-analysis-fabio-maresca-et-al-2024>(13/16 | 202/320) Are you a robot? Detecting Autonomous Vehicles from Behavior Analysis (Fabio Maresca et al., 2024)</a></li><li><a href=#1416--203320-motpose-multi-object-6d-pose-estimation-for-dynamic-video-sequences-using-attention-based-temporal-fusion-arul-selvam-periyasamy-et-al-2024>(14/16 | 203/320) MOTPose: Multi-object 6D Pose Estimation for Dynamic Video Sequences using Attention-based Temporal Fusion (Arul Selvam Periyasamy et al., 2024)</a></li><li><a href=#1516--204320-development-of-control-algorithms-for-mobile-robotics-focused-on-their-potential-use-for-fpga-based-robots-andrés-david-suárez-gómez-et-al-2024>(15/16 | 204/320) Development of control algorithms for mobile robotics focused on their potential use for FPGA-based robots (Andrés-David Suárez-Gómez et al., 2024)</a></li><li><a href=#1616--205320-thör-magni-a-large-scale-indoor-motion-capture-recording-of-human-movement-and-robot-interaction-tim-schreiter-et-al-2024>(16/16 | 205/320) THÖR-MAGNI: A Large-scale Indoor Motion Capture Recording of Human Movement and Robot Interaction (Tim Schreiter et al., 2024)</a></li></ul></li><li><a href=#csse-8>cs.SE (8)</a><ul><li><a href=#18--206320-codeultrafeedback-an-llm-as-a-judge-dataset-for-aligning-large-language-models-to-coding-preferences-martin-weyssow-et-al-2024>(1/8 | 206/320) CodeUltraFeedback: An LLM-as-a-Judge Dataset for Aligning Large Language Models to Coding Preferences (Martin Weyssow et al., 2024)</a></li><li><a href=#28--207320-reality-bites-assessing-the-realism-of-driving-scenarios-with-large-language-models-jiahui-wu-et-al-2024>(2/8 | 207/320) Reality Bites: Assessing the Realism of Driving Scenarios with Large Language Models (Jiahui Wu et al., 2024)</a></li><li><a href=#38--208320-code-revert-prediction-with-graph-neural-networks-a-case-study-at-jp-morgan-chase-yulong-pei-et-al-2024>(3/8 | 208/320) Code Revert Prediction with Graph Neural Networks: A Case Study at J.P. Morgan Chase (Yulong Pei et al., 2024)</a></li><li><a href=#48--209320-welcome-your-new-ai-teammate-on-safety-analysis-by-leashing-large-language-models-ali-nouri-et-al-2024>(4/8 | 209/320) Welcome Your New AI Teammate: On Safety Analysis by Leashing Large Language Models (Ali Nouri et al., 2024)</a></li><li><a href=#58--210320-llm-based-agents-for-automating-the-enhancement-of-user-story-quality-an-early-report-zheying-zhang-et-al-2024>(5/8 | 210/320) LLM-based agents for automating the enhancement of user story quality: An early report (Zheying Zhang et al., 2024)</a></li><li><a href=#68--211320-analyzing-and-mitigating-with-llms-the-security-misconfigurations-of-helm-charts-from-artifact-hub-francesco-minna-et-al-2024>(6/8 | 211/320) Analyzing and Mitigating (with LLMs) the Security Misconfigurations of Helm Charts from Artifact Hub (Francesco Minna et al., 2024)</a></li><li><a href=#78--212320-gamified-gui-testing-with-selenium-in-the-intellij-ide-a-prototype-plugin-giacomo-garaccione-et-al-2024>(7/8 | 212/320) Gamified GUI testing with Selenium in the IntelliJ IDE: A Prototype Plugin (Giacomo Garaccione et al., 2024)</a></li><li><a href=#88--213320-an-extensive-comparison-of-static-application-security-testing-tools-matteo-esposito-et-al-2024>(8/8 | 213/320) An Extensive Comparison of Static Application Security Testing Tools (Matteo Esposito et al., 2024)</a></li></ul></li><li><a href=#eesssy-11>eess.SY (11)</a><ul><li><a href=#111--214320-exploring-the-capabilities-and-limitations-of-large-language-models-in-the-electric-energy-sector-lin-dong-et-al-2024>(1/11 | 214/320) Exploring the Capabilities and Limitations of Large Language Models in the Electric Energy Sector (Lin Dong et al., 2024)</a></li><li><a href=#211--215320-is-data-all-that-matters-the-role-of-control-frequency-for-learning-based-sampled-data-control-of-uncertain-systems-ralf-römer-et-al-2024>(2/11 | 215/320) Is Data All That Matters? The Role of Control Frequency for Learning-Based Sampled-Data Control of Uncertain Systems (Ralf Römer et al., 2024)</a></li><li><a href=#311--216320-impact-of-objective-function-on-spectral-efficiency-in-integrated-haps-terrestrial-networks-afsoon-alidadi-shamsabadi-et-al-2024>(3/11 | 216/320) Impact of Objective Function on Spectral Efficiency in Integrated HAPS-Terrestrial Networks (Afsoon Alidadi Shamsabadi et al., 2024)</a></li><li><a href=#411--217320-optimal-sequencing-and-motion-control-in-a-roundabout-with-safety-guarantees-yingqing-chen-et-al-2024>(4/11 | 217/320) Optimal Sequencing and Motion Control in a Roundabout with Safety Guarantees (Yingqing Chen et al., 2024)</a></li><li><a href=#511--218320-fairness-aware-multi-server-federated-learning-task-delegation-over-wireless-networks-yulan-gao-et-al-2024>(5/11 | 218/320) Fairness-Aware Multi-Server Federated Learning Task Delegation over Wireless Networks (Yulan Gao et al., 2024)</a></li><li><a href=#611--219320-optimal-pinning-control-for-synchronization-over-temporal-networks-aandrew-baggio-s-et-al-2024>(6/11 | 219/320) Optimal Pinning Control for Synchronization over Temporal Networks (Aandrew Baggio S et al., 2024)</a></li><li><a href=#711--220320-confidence-aware-safe-and-stable-control-of-control-affine-systems-shiqing-wei-et-al-2024>(7/11 | 220/320) Confidence-Aware Safe and Stable Control of Control-Affine Systems (Shiqing Wei et al., 2024)</a></li><li><a href=#811--221320-a-geometric-approach-to-resilient-distributed-consensus-accounting-for-state-imprecision-and-adversarial-agents-christopher-a-lee-et-al-2024>(8/11 | 221/320) A Geometric Approach to Resilient Distributed Consensus Accounting for State Imprecision and Adversarial Agents (Christopher A. Lee et al., 2024)</a></li><li><a href=#911--222320-defense-via-behavior-attestation-against-attacks-in-connected-and-automated-vehicles-based-federated-learning-systems-godwin-badu-marfo-et-al-2024>(9/11 | 222/320) Defense via Behavior Attestation against Attacks in Connected and Automated Vehicles based Federated Learning Systems (Godwin Badu-Marfo et al., 2024)</a></li><li><a href=#1011--223320-learning-algorithms-for-verification-of-markov-decision-processes-tomáš-brázdil-et-al-2024>(10/11 | 223/320) Learning Algorithms for Verification of Markov Decision Processes (Tomáš Brázdil et al., 2024)</a></li><li><a href=#1111--224320-partitioning-distribution-networks-for-integrated-electrification-planning-olamide-oladeji-et-al-2024>(11/11 | 224/320) Partitioning Distribution Networks for Integrated Electrification Planning (Olamide Oladeji et al., 2024)</a></li></ul></li><li><a href=#cscy-3>cs.CY (3)</a><ul><li><a href=#13--225320-comparing-rationality-between-large-language-models-and-humans-insights-and-open-questions-dana-alsagheer-et-al-2024>(1/3 | 225/320) Comparing Rationality Between Large Language Models and Humans: Insights and Open Questions (Dana Alsagheer et al., 2024)</a></li><li><a href=#23--226320-metrognn-metro-network-expansion-with-reinforcement-learning-hongyuan-su-et-al-2024>(2/3 | 226/320) MetroGNN: Metro Network Expansion with Reinforcement Learning (Hongyuan Su et al., 2024)</a></li><li><a href=#33--227320-older-adults-safety-and-security-online-a-post-pandemic-exploration-of-attitudes-and-behaviors-edgar-pacheco-2024>(3/3 | 227/320) Older adults&rsquo; safety and security online: A post-pandemic exploration of attitudes and behaviors (Edgar Pacheco, 2024)</a></li></ul></li><li><a href=#csir-5>cs.IR (5)</a><ul><li><a href=#15--228320-usimagent-large-language-models-for-simulating-search-users-erhan-zhang-et-al-2024>(1/5 | 228/320) USimAgent: Large Language Models for Simulating Search Users (Erhan Zhang et al., 2024)</a></li><li><a href=#25--229320-logical-discrete-graphical-models-must-supplement-large-language-models-for-information-synthesis-gregory-coppola-2024>(2/5 | 229/320) Logical Discrete Graphical Models Must Supplement Large Language Models for Information Synthesis (Gregory Coppola, 2024)</a></li><li><a href=#35--230320-projected-gradient-descent-for-spectral-compressed-sensing-via-symmetric-hankel-factorization-jinsheng-li-et-al-2024>(3/5 | 230/320) Projected Gradient Descent for Spectral Compressed Sensing via Symmetric Hankel Factorization (Jinsheng Li et al., 2024)</a></li><li><a href=#45--231320-seed-based-information-retrieval-in-networks-of-research-publications-evaluation-of-direct-citations-bibliographic-coupling-co-citations-and-pubmed-related-article-score-peter-sjögårde-et-al-2024>(4/5 | 231/320) Seed-based information retrieval in networks of research publications: Evaluation of direct citations, bibliographic coupling, co-citations and PubMed related article score (Peter Sjögårde et al., 2024)</a></li><li><a href=#55--232320-online-and-offline-evaluation-in-search-clarification-leila-tavakoli-et-al-2024>(5/5 | 232/320) Online and Offline Evaluation in Search Clarification (Leila Tavakoli et al., 2024)</a></li></ul></li><li><a href=#cssi-3>cs.SI (3)</a><ul><li><a href=#13--233320-from-skepticism-to-acceptance-simulating-the-attitude-dynamics-toward-fake-news-yuhan-liu-et-al-2024>(1/3 | 233/320) From Skepticism to Acceptance: Simulating the Attitude Dynamics Toward Fake News (Yuhan Liu et al., 2024)</a></li><li><a href=#23--234320-rumor-mitigation-in-social-media-platforms-with-deep-reinforcement-learning-hongyuan-su-et-al-2024>(2/3 | 234/320) Rumor Mitigation in Social Media Platforms with Deep Reinforcement Learning (Hongyuan Su et al., 2024)</a></li><li><a href=#33--235320-belief-and-persuasion-in-scientific-discourse-on-social-media-a-study-of-the-covid-19-pandemic-salwa-alamir-et-al-2024>(3/3 | 235/320) Belief and Persuasion in Scientific Discourse on Social Media: A Study of the COVID-19 Pandemic (Salwa Alamir et al., 2024)</a></li></ul></li><li><a href=#cscr-12>cs.CR (12)</a><ul><li><a href=#112--236320-adashield-safeguarding-multimodal-large-language-models-from-structure-based-attack-via-adaptive-shield-prompting-yu-wang-et-al-2024>(1/12 | 236/320) AdaShield: Safeguarding Multimodal Large Language Models from Structure-based Attack via Adaptive Shield Prompting (Yu Wang et al., 2024)</a></li><li><a href=#212--237320-graph-based-ddos-attack-detection-in-iot-systems-with-lossy-network-arvin-hekmati-et-al-2024>(2/12 | 237/320) Graph-Based DDoS Attack Detection in IoT Systems with Lossy Network (Arvin Hekmati et al., 2024)</a></li><li><a href=#312--238320-helpful-or-harmful-exploring-the-efficacy-of-large-language-models-for-online-grooming-prevention-ellie-prosser-et-al-2024>(3/12 | 238/320) Helpful or Harmful? Exploring the Efficacy of Large Language Models for Online Grooming Prevention (Ellie Prosser et al., 2024)</a></li><li><a href=#412--239320-what-was-your-prompt-a-remote-keylogging-attack-on-ai-assistants-roy-weiss-et-al-2024>(4/12 | 239/320) What Was Your Prompt? A Remote Keylogging Attack on AI Assistants (Roy Weiss et al., 2024)</a></li><li><a href=#512--240320-precurious-how-innocent-pre-trained-language-models-turn-into-privacy-traps-ruixuan-liu-et-al-2024>(5/12 | 240/320) PreCurious: How Innocent Pre-Trained Language Models Turn into Privacy Traps (Ruixuan Liu et al., 2024)</a></li><li><a href=#612--241320-optimistic-verifiable-training-by-controlling-hardware-nondeterminism-megha-srivastava-et-al-2024>(6/12 | 241/320) Optimistic Verifiable Training by Controlling Hardware Nondeterminism (Megha Srivastava et al., 2024)</a></li><li><a href=#712--242320-symbiotic-game-and-foundation-models-for-cyber-deception-operations-in-strategic-cyber-warfare-tao-li-et-al-2024>(7/12 | 242/320) Symbiotic Game and Foundation Models for Cyber Deception Operations in Strategic Cyber Warfare (Tao Li et al., 2024)</a></li><li><a href=#812--243320-privacy-preserving-anomaly-detection-on-homomorphic-encrypted-data-from-iot-sensors-anca-hangan-et-al-2024>(8/12 | 243/320) Privacy Preserving Anomaly Detection on Homomorphic Encrypted Data from IoT Sensors (Anca Hangan et al., 2024)</a></li><li><a href=#912--244320-explainable-machine-learning-based-security-and-privacy-protection-framework-for-internet-of-medical-things-systems-ayoub-si-ahmed-et-al-2024>(9/12 | 244/320) Explainable Machine Learning-Based Security and Privacy Protection Framework for Internet of Medical Things Systems (Ayoub Si-ahmed et al., 2024)</a></li><li><a href=#1012--245320-counter-samples-a-stateless-strategy-to-neutralize-black-box-adversarial-attacks-roey-bokobza-et-al-2024>(10/12 | 245/320) Counter-Samples: A Stateless Strategy to Neutralize Black Box Adversarial Attacks (Roey Bokobza et al., 2024)</a></li><li><a href=#1112--246320-lan-learning-adaptive-neighbors-for-real-time-insider-threat-detection-xiangrui-cai-et-al-2024>(11/12 | 246/320) LAN: Learning Adaptive Neighbors for Real-Time Insider Threat Detection (Xiangrui Cai et al., 2024)</a></li><li><a href=#1212--247320-ldprecover-recovering-frequencies-from-poisoning-attacks-against-local-differential-privacy-xinyue-sun-et-al-2024>(12/12 | 247/320) LDPRecover: Recovering Frequencies from Poisoning Attacks against Local Differential Privacy (Xinyue Sun et al., 2024)</a></li></ul></li><li><a href=#eessiv-13>eess.IV (13)</a><ul><li><a href=#113--248320-fastsam3d-an-efficient-segment-anything-model-for-3d-volumetric-medical-images-yiqing-shen-et-al-2024>(1/13 | 248/320) FastSAM3D: An Efficient Segment Anything Model for 3D Volumetric Medical Images (Yiqing Shen et al., 2024)</a></li><li><a href=#213--249320-predicting-generalization-of-ai-colonoscopy-models-to-unseen-data-joel-shor-et-al-2024>(2/13 | 249/320) Predicting Generalization of AI Colonoscopy Models to Unseen Data (Joel Shor et al., 2024)</a></li><li><a href=#313--250320-reconstructing-blood-flow-in-data-poor-regimes-a-vasculature-network-kernel-for-gaussian-process-regression-shaghayegh-z-ashtiani-et-al-2024>(3/13 | 250/320) Reconstructing Blood Flow in Data-Poor Regimes: A Vasculature Network Kernel for Gaussian Process Regression (Shaghayegh Z. Ashtiani et al., 2024)</a></li><li><a href=#413--251320-xreal-realistic-anatomy-and-pathology-aware-x-ray-generation-via-controllable-diffusion-model-anees-ur-rehman-hashmi-et-al-2024>(4/13 | 251/320) XReal: Realistic Anatomy and Pathology-Aware X-ray Generation via Controllable Diffusion Model (Anees Ur Rehman Hashmi et al., 2024)</a></li><li><a href=#513--252320-stainfuser-controlling-diffusion-for-faster-neural-style-transfer-in-multi-gigapixel-histology-images-robert-jewsbury-et-al-2024>(5/13 | 252/320) StainFuser: Controlling Diffusion for Faster Neural Style Transfer in Multi-Gigapixel Histology Images (Robert Jewsbury et al., 2024)</a></li><li><a href=#613--253320-vm-unet-v2-rethinking-vision-mamba-unet-for-medical-image-segmentation-mingya-zhang-et-al-2024>(6/13 | 253/320) VM-UNET-V2 Rethinking Vision Mamba UNet for Medical Image Segmentation (Mingya Zhang et al., 2024)</a></li><li><a href=#713--254320-empowering-healthcare-through-privacy-preserving-mri-analysis-al-amin-et-al-2024>(7/13 | 254/320) Empowering Healthcare through Privacy-Preserving MRI Analysis (Al Amin et al., 2024)</a></li><li><a href=#813--255320-analyzing-data-augmentation-for-medical-images-a-case-study-in-ultrasound-images-adam-tupper-et-al-2024>(8/13 | 255/320) Analyzing Data Augmentation for Medical Images: A Case Study in Ultrasound Images (Adam Tupper et al., 2024)</a></li><li><a href=#913--256320-mitigating-data-consistency-induced-discrepancy-in-cascaded-diffusion-models-for-sparse-view-ct-reconstruction-hanyu-chen-et-al-2024>(9/13 | 256/320) Mitigating Data Consistency Induced Discrepancy in Cascaded Diffusion Models for Sparse-view CT Reconstruction (Hanyu Chen et al., 2024)</a></li><li><a href=#1013--257320-advanced-tumor-segmentation-in-medical-imaging-an-ensemble-approach-for-brats-2023-adult-glioma-and-pediatric-tumor-tasks-fadillah-maani-et-al-2024>(10/13 | 257/320) Advanced Tumor Segmentation in Medical Imaging: An Ensemble Approach for BraTS 2023 Adult Glioma and Pediatric Tumor Tasks (Fadillah Maani et al., 2024)</a></li><li><a href=#1113--258320-a-modified-da-vinci-surgical-instrument-for-oce-based-elasticity-estimation-with-deep-learning-maximilian-neidhardt-et-al-2024>(11/13 | 258/320) A Modified da Vinci Surgical Instrument for OCE based Elasticity Estimation with Deep Learning (Maximilian Neidhardt et al., 2024)</a></li><li><a href=#1213--259320-tbi-imagetext-tbi-it-comprehensive-text-and-image-datasets-for-traumatic-brain-injury-research-jie-li-et-al-2024>(12/13 | 259/320) TBI Image/Text (TBI-IT): Comprehensive Text and Image Datasets for Traumatic Brain Injury Research (Jie Li et al., 2024)</a></li><li><a href=#1313--260320-deep-unfolding-network-for-hyperspectral-image-super-resolution-with-automatic-exposure-correction-yuan-fang-et-al-2024>(13/13 | 260/320) Deep unfolding Network for Hyperspectral Image Super-Resolution with Automatic Exposure Correction (Yuan Fang et al., 2024)</a></li></ul></li><li><a href=#cshc-10>cs.HC (10)</a><ul><li><a href=#110--261320-like-a-nesting-doll-analyzing-recursion-analogies-generated-by-cs-students-using-large-language-models-seth-bernstein-et-al-2024>(1/10 | 261/320) &lsquo;Like a Nesting Doll&rsquo;: Analyzing Recursion Analogies Generated by CS Students using Large Language Models (Seth Bernstein et al., 2024)</a></li><li><a href=#210--262320-towards-proactive-interactions-for-in-vehicle-conversational-assistants-utilizing-large-language-models-huifang-du-et-al-2024>(2/10 | 262/320) Towards Proactive Interactions for In-Vehicle Conversational Assistants Utilizing Large Language Models (Huifang Du et al., 2024)</a></li><li><a href=#310--263320-enabling-waypoint-generation-for-collaborative-robots-using-llms-and-mixed-reality-cathy-mengying-fang-et-al-2024>(3/10 | 263/320) Enabling Waypoint Generation for Collaborative Robots using LLMs and Mixed Reality (Cathy Mengying Fang et al., 2024)</a></li><li><a href=#410--264320-prompthis-visualizing-the-process-and-influence-of-prompt-editing-during-text-to-image-creation-yuhan-guo-et-al-2024>(4/10 | 264/320) PrompTHis: Visualizing the Process and Influence of Prompt Editing during Text-to-Image Creation (Yuhan Guo et al., 2024)</a></li><li><a href=#510--265320-vivid-human-ai-collaborative-authoring-of-vicarious-dialogues-from-lecture-videos-seulgi-choi-et-al-2024>(5/10 | 265/320) VIVID: Human-AI Collaborative Authoring of Vicarious Dialogues from Lecture Videos (Seulgi Choi et al., 2024)</a></li><li><a href=#610--266320-unlocking-the-conversion-of-web-screenshots-into-html-code-with-the-websight-dataset-hugo-laurençon-et-al-2024>(6/10 | 266/320) Unlocking the conversion of Web Screenshots into HTML Code with the WebSight Dataset (Hugo Laurençon et al., 2024)</a></li><li><a href=#710--267320-influence-of-personality-and-communication-behavior-of-a-conversational-agent-on-user-experience-and-social-presence-in-augmented-reality-katerina-koleva-et-al-2024>(7/10 | 267/320) Influence of Personality and Communication Behavior of a Conversational Agent on User Experience and Social Presence in Augmented Reality (Katerina Koleva et al., 2024)</a></li><li><a href=#810--268320-labelaid-just-in-time-ai-interventions-for-improving-human-labeling-quality-and-domain-knowledge-in-crowdsourcing-systems-chu-li-et-al-2024>(8/10 | 268/320) LabelAId: Just-in-time AI Interventions for Improving Human Labeling Quality and Domain Knowledge in Crowdsourcing Systems (Chu Li et al., 2024)</a></li><li><a href=#910--269320-param-leveraging-parametric-design-in-extended-reality-to-support-the-personalization-of-artifacts-for-personal-fabrication-evgeny-stemasov-et-al-2024>(9/10 | 269/320) pARam: Leveraging Parametric Design in Extended Reality to Support the Personalization of Artifacts for Personal Fabrication (Evgeny Stemasov et al., 2024)</a></li><li><a href=#1010--270320-which-artificial-intelligences-do-people-care-about-most-a-conjoint-experiment-on-moral-consideration-ali-ladak-et-al-2024>(10/10 | 270/320) Which Artificial Intelligences Do People Care About Most? A Conjoint Experiment on Moral Consideration (Ali Ladak et al., 2024)</a></li></ul></li><li><a href=#csai-7>cs.AI (7)</a><ul><li><a href=#17--271320-silico-centric-theory-of-mind-anirban-mukherjee-et-al-2024>(1/7 | 271/320) Silico-centric Theory of Mind (Anirban Mukherjee et al., 2024)</a></li><li><a href=#27--272320-clinical-reasoning-over-tabular-data-and-text-with-bayesian-networks-paloma-rabaey-et-al-2024>(2/7 | 272/320) Clinical Reasoning over Tabular Data and Text with Bayesian Networks (Paloma Rabaey et al., 2024)</a></li><li><a href=#37--273320-xlp-explainable-link-prediction-for-master-data-management-balaji-ganesan-et-al-2024>(3/7 | 273/320) xLP: Explainable Link Prediction for Master Data Management (Balaji Ganesan et al., 2024)</a></li><li><a href=#47--274320-generating-feasible-and-plausible-counterfactual-explanations-for-outcome-prediction-of-business-processes-alexander-stevens-et-al-2024>(4/7 | 274/320) Generating Feasible and Plausible Counterfactual Explanations for Outcome Prediction of Business Processes (Alexander Stevens et al., 2024)</a></li><li><a href=#57--275320-leveraging-constraint-programming-in-a-deep-learning-approach-for-dynamically-solving-the-flexible-job-shop-scheduling-problem-imanol-echeverria-et-al-2024>(5/7 | 275/320) Leveraging Constraint Programming in a Deep Learning Approach for Dynamically Solving the Flexible Job-Shop Scheduling Problem (Imanol Echeverria et al., 2024)</a></li><li><a href=#67--276320-heuristic-reasoning-in-ai-instrumental-use-and-mimetic-absorption-anirban-mukherjee-et-al-2024>(6/7 | 276/320) Heuristic Reasoning in AI: Instrumental Use and Mimetic Absorption (Anirban Mukherjee et al., 2024)</a></li><li><a href=#77--277320-a-multi-population-integrated-approach-for-capacitated-location-routing-pengfei-he-et-al-2024>(7/7 | 277/320) A Multi-population Integrated Approach for Capacitated Location Routing (Pengfei He et al., 2024)</a></li></ul></li><li><a href=#csni-2>cs.NI (2)</a><ul><li><a href=#12--278320-whittle-index-based-user-association-in-dense-millimeter-wave-networks-mandar-r-nalavade-et-al-2024>(1/2 | 278/320) Whittle Index Based User Association in Dense Millimeter Wave Networks (Mandar R. Nalavade et al., 2024)</a></li><li><a href=#22--279320-preconfig-a-pretrained-model-for-automating-network-configuration-fuliang-li-et-al-2024>(2/2 | 279/320) PreConfig: A Pretrained Model for Automating Network Configuration (Fuliang Li et al., 2024)</a></li></ul></li><li><a href=#cssd-7>cs.SD (7)</a><ul><li><a href=#17--280320-uamix-mae-efficient-tuning-of-pretrained-audio-transformers-with-unsupervised-audio-mixtures-afrina-tabassum-et-al-2024>(1/7 | 280/320) uaMix-MAE: Efficient Tuning of Pretrained Audio Transformers with Unsupervised Audio Mixtures (Afrina Tabassum et al., 2024)</a></li><li><a href=#27--281320-lm2d-lyrics--and-music-driven-dance-synthesis-wenjie-yin-et-al-2024>(2/7 | 281/320) LM2D: Lyrics- and Music-Driven Dance Synthesis (Wenjie Yin et al., 2024)</a></li><li><a href=#37--282320-an-ai-driven-approach-to-wind-turbine-bearing-fault-diagnosis-from-acoustic-signals-zhao-wang-et-al-2024>(3/7 | 282/320) An AI-Driven Approach to Wind Turbine Bearing Fault Diagnosis from Acoustic Signals (Zhao Wang et al., 2024)</a></li><li><a href=#47--283320-spoken-100-a-cross-lingual-benchmarking-dataset-for-the-classification-of-spoken-numbers-in-different-languages-rené-groh-et-al-2024>(4/7 | 283/320) SpokeN-100: A Cross-Lingual Benchmarking Dataset for The Classification of Spoken Numbers in Different Languages (René Groh et al., 2024)</a></li><li><a href=#57--284320-the-neural-srp-method-for-positional-sound-source-localization-eric-grinstein-et-al-2024>(5/7 | 284/320) The Neural-SRP method for positional sound source localization (Eric Grinstein et al., 2024)</a></li><li><a href=#67--285320-a-practical-guide-to-spectrogram-analysis-for-audio-signal-processing-zulfidin-khodzhaev-2024>(6/7 | 285/320) A Practical Guide to Spectrogram Analysis for Audio Signal Processing (Zulfidin Khodzhaev, 2024)</a></li><li><a href=#77--286320-more-than-words-advancements-and-challenges-in-speech-recognition-for-singing-anna-kruspe-2024>(7/7 | 286/320) More than words: Advancements and challenges in speech recognition for singing (Anna Kruspe, 2024)</a></li></ul></li><li><a href=#csdb-1>cs.DB (1)</a><ul><li><a href=#11--287320-query-rewriting-via-large-language-models-jie-liu-et-al-2024>(1/1 | 287/320) Query Rewriting via Large Language Models (Jie Liu et al., 2024)</a></li></ul></li><li><a href=#physicsspace-ph-1>physics.space-ph (1)</a><ul><li><a href=#11--288320-forecasting-geoffective-events-from-solar-wind-data-and-evaluating-the-most-predictive-features-through-machine-learning-approaches-sabrina-guastavino-et-al-2024>(1/1 | 288/320) Forecasting Geoffective Events from Solar Wind Data and Evaluating the Most Predictive Features through Machine Learning Approaches (Sabrina Guastavino et al., 2024)</a></li></ul></li><li><a href=#csdc-2>cs.DC (2)</a><ul><li><a href=#12--289320-burstattention-an-efficient-distributed-attention-framework-for-extremely-long-sequences-sun-ao-et-al-2024>(1/2 | 289/320) BurstAttention: An Efficient Distributed Attention Framework for Extremely Long Sequences (Sun Ao et al., 2024)</a></li><li><a href=#22--290320-benchmarking-distributed-coordination-systems-a-survey-and-analysis-bekir-turkkan-et-al-2024>(2/2 | 290/320) Benchmarking Distributed Coordination Systems: A Survey and Analysis (Bekir Turkkan et al., 2024)</a></li></ul></li><li><a href=#physicsoptics-1>physics.optics (1)</a><ul><li><a href=#11--291320-compute-first-optical-detection-for-noise-resilient-visual-perception-jungmin-kim-et-al-2024>(1/1 | 291/320) Compute-first optical detection for noise-resilient visual perception (Jungmin Kim et al., 2024)</a></li></ul></li><li><a href=#csce-2>cs.CE (2)</a><ul><li><a href=#12--292320-a-mixed-order-quasicontinuum-approach-for-beam-based-architected-materials-with-application-to-fracture-kevin-kraschewski-et-al-2024>(1/2 | 292/320) A mixed-order quasicontinuum approach for beam-based architected materials with application to fracture (Kevin Kraschewski et al., 2024)</a></li><li><a href=#22--293320-modular-parametric-pgd-enabling-online-solution-of-partial-differential-equations-angelo-pasquale-et-al-2024>(2/2 | 293/320) Modular parametric PGD enabling online solution of partial differential equations (Angelo Pasquale et al., 2024)</a></li></ul></li><li><a href=#csit-6>cs.IT (6)</a><ul><li><a href=#16--294320-smart-resource-allocation-at-mmwavethz-frequencies-with-cooperative-rate-splitting-hyesang-cho-et-al-2024>(1/6 | 294/320) Smart Resource Allocation at mmWave/THz Frequencies with Cooperative Rate-Splitting (Hyesang Cho et al., 2024)</a></li><li><a href=#26--295320-localization-in-digital-twin-mimo-networks-a-case-for-massive-fingerprinting-joão-morais-et-al-2024>(2/6 | 295/320) Localization in Digital Twin MIMO Networks: A Case for Massive Fingerprinting (João Morais et al., 2024)</a></li><li><a href=#36--296320-joint-port-selection-and-beamforming-design-for-fluid-antenna-assisted-integrated-data-and-energy-transfer-long-zhang-et-al-2024>(3/6 | 296/320) Joint Port Selection and Beamforming Design for Fluid Antenna Assisted Integrated Data and Energy Transfer (Long Zhang et al., 2024)</a></li><li><a href=#46--297320-reverse-em-problem-based-on-bregman-divergence-and-its-application-to-classical-and-quantum-information-theory-masahito-hayashi-2024>(4/6 | 297/320) Reverse em-problem based on Bregman divergence and its application to classical and quantum information theory (Masahito Hayashi, 2024)</a></li><li><a href=#56--298320-a-deep-reinforcement-learning-approach-for-autonomous-reconfigurable-intelligent-surfaces-hyuckjin-choi-et-al-2024>(5/6 | 298/320) A Deep Reinforcement Learning Approach for Autonomous Reconfigurable Intelligent Surfaces (Hyuckjin Choi et al., 2024)</a></li><li><a href=#66--299320-performance-analysis-on-ris-aided-wideband-massive-mimo-ofdm-systems-with-low-resolution-adcs-xianzhe-chen-et-al-2024>(6/6 | 299/320) Performance Analysis on RIS-Aided Wideband Massive MIMO OFDM Systems with Low-Resolution ADCs (Xianzhe Chen et al., 2024)</a></li></ul></li><li><a href=#nlinao-1>nlin.AO (1)</a><ul><li><a href=#11--300320-implementation-of-parallel-process-execution-in-the-next-generation-system-analysis-model-harish-gadey-et-al-2024>(1/1 | 300/320) Implementation of Parallel Process Execution in the Next Generation System Analysis Model (Harish Gadey et al., 2024)</a></li></ul></li><li><a href=#eessas-1>eess.AS (1)</a><ul><li><a href=#11--301320-ptsd-mdnn--fusion-tardive-de-réseaux-de-neurones-profonds-multimodaux-pour-la-détection-du-trouble-de-stress-post-traumatique-long-nguyen-phuoc-et-al-2024>(1/1 | 301/320) PTSD-MDNN : Fusion tardive de réseaux de neurones profonds multimodaux pour la détection du trouble de stress post-traumatique (Long Nguyen-Phuoc et al., 2024)</a></li></ul></li><li><a href=#statml-2>stat.ML (2)</a><ul><li><a href=#12--302320-visa-variational-inference-with-sequential-sample-average-approximations-heiko-zimmermann-et-al-2024>(1/2 | 302/320) VISA: Variational Inference with Sequential Sample-Average Approximations (Heiko Zimmermann et al., 2024)</a></li><li><a href=#22--303320-pantypes-diverse-representatives-for-self-explainable-models-rune-kjærsgaard-et-al-2024>(2/2 | 303/320) Pantypes: Diverse Representatives for Self-Explainable Models (Rune Kjærsgaard et al., 2024)</a></li></ul></li><li><a href=#mathoc-1>math.OC (1)</a><ul><li><a href=#11--304320-solvability-of-the-inverse-optimal-control-problem-based-on-the-minimum-principle-afreen-islam-et-al-2024>(1/1 | 304/320) Solvability of the Inverse Optimal Control problem based on the minimum principle (Afreen Islam et al., 2024)</a></li></ul></li><li><a href=#q-biobm-1>q-bio.BM (1)</a><ul><li><a href=#11--305320-leap-molecular-synthesisability-scoring-with-intermediates-antonia-calvi-et-al-2024>(1/1 | 305/320) Leap: molecular synthesisability scoring with intermediates (Antonia Calvi et al., 2024)</a></li></ul></li><li><a href=#csgt-1>cs.GT (1)</a><ul><li><a href=#11--306320-all-pay-auction-based-profit-maximization-in-end-to-end-computation-offloading-system-hai-xue-et-al-2024>(1/1 | 306/320) All-pay Auction Based Profit Maximization in End-to-End Computation Offloading System (Hai Xue et al., 2024)</a></li></ul></li><li><a href=#mathlo-2>math.LO (2)</a><ul><li><a href=#12--307320-generalisation-of-proof-simulation-procedures-for-frege-systems-by-mlbonet-and-srbuss-daniil-kozhemiachenko-2024>(1/2 | 307/320) Generalisation of proof simulation procedures for Frege systems by M.L.~Bonet and S.R.~Buss (Daniil Kozhemiachenko, 2024)</a></li><li><a href=#22--308320-fregean-flows-eric-easthope-2024>(2/2 | 308/320) Fregean Flows (Eric Easthope, 2024)</a></li></ul></li><li><a href=#csar-2>cs.AR (2)</a><ul><li><a href=#12--309320-flexnn-a-dataflow-aware-flexible-deep-learning-accelerator-for-energy-efficient-edge-devices-arnab-raha-et-al-2024>(1/2 | 309/320) FlexNN: A Dataflow-aware Flexible Deep Learning Accelerator for Energy-Efficient Edge Devices (Arnab Raha et al., 2024)</a></li><li><a href=#22--310320-analytical-heterogeneous-die-to-die-3d-placement-with-macros-yuxuan-zhao-et-al-2024>(2/2 | 310/320) Analytical Heterogeneous Die-to-Die 3D Placement with Macros (Yuxuan Zhao et al., 2024)</a></li></ul></li><li><a href=#csdm-2>cs.DM (2)</a><ul><li><a href=#12--311320-binary-stretch-embedding-of-weighted-graphs-javad-b-ebrahimi-et-al-2024>(1/2 | 311/320) Binary Stretch Embedding of Weighted Graphs (Javad B. Ebrahimi et al., 2024)</a></li><li><a href=#22--312320-bounds-and-extremal-graphs-for-monitoring-edge-geodetic-sets-in-graphs-florent-foucaud-et-al-2024>(2/2 | 312/320) Bounds and extremal graphs for monitoring edge-geodetic sets in graphs (Florent Foucaud et al., 2024)</a></li></ul></li><li><a href=#econth-1>econ.TH (1)</a><ul><li><a href=#11--313320-learning-macroeconomic-policies-based-on-microfoundations-a-stackelberg-mean-field-game-approach-qirui-mi-et-al-2024>(1/1 | 313/320) Learning Macroeconomic Policies based on Microfoundations: A Stackelberg Mean Field Game Approach (Qirui Mi et al., 2024)</a></li></ul></li><li><a href=#csgr-2>cs.GR (2)</a><ul><li><a href=#12--314320-headevolver-text-to-head-avatars-via-locally-learnable-mesh-deformation-duotun-wang-et-al-2024>(1/2 | 314/320) HeadEvolver: Text to Head Avatars via Locally Learnable Mesh Deformation (Duotun Wang et al., 2024)</a></li><li><a href=#22--315320-a-new-split-algorithm-for-3d-gaussian-splatting-qiyuan-feng-et-al-2024>(2/2 | 315/320) A New Split Algorithm for 3D Gaussian Splatting (Qiyuan Feng et al., 2024)</a></li></ul></li><li><a href=#quant-ph-1>quant-ph (1)</a><ul><li><a href=#11--316320-bridging-quantum-computing-and-differential-privacy-a-survey-on-quantum-computing-privacy-yusheng-zhao-et-al-2024>(1/1 | 316/320) Bridging Quantum Computing and Differential Privacy: A Survey on Quantum Computing Privacy (Yusheng Zhao et al., 2024)</a></li></ul></li><li><a href=#mathna-2>math.NA (2)</a><ul><li><a href=#12--317320-high-order-numerical-integration-on-regular-embedded-surfaces-gentian-zavalani-et-al-2024>(1/2 | 317/320) High-order numerical integration on regular embedded surfaces (Gentian Zavalani et al., 2024)</a></li><li><a href=#22--318320-dynamically-accelerating-the-power-iteration-with-momentum-christian-austin-et-al-2024>(2/2 | 318/320) Dynamically accelerating the power iteration with momentum (Christian Austin et al., 2024)</a></li></ul></li><li><a href=#mathco-1>math.CO (1)</a><ul><li><a href=#11--319320-edge-apexing-in-hereditary-classes-of-graphs-jagdeep-singh-et-al-2024>(1/1 | 319/320) Edge-apexing in hereditary classes of graphs (Jagdeep Singh et al., 2024)</a></li></ul></li><li><a href=#cslo-1>cs.LO (1)</a><ul><li><a href=#11--320320-a-complete-logic-for-causal-consistency-will-simmons-et-al-2024>(1/1 | 320/320) A complete logic for causal consistency (Will Simmons et al., 2024)</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:contact.to.akitenkrad@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>contact.to.akitenkrad@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/akitenkrad-blog/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2020 Akitenkrad.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/akitenkrad-blog/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/jquery-3.4.1.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/popper.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/bootstrap.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/navbar.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/plyr.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=https://akitenkrad.github.io/akitenkrad-blog/js/single.js></script><script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.css><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.js></script><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!0})})</script></body></html>